- en: Speech Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading and plotting audio data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming audio signals into the frequency domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating audio signals with custom parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesizing music
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting frequency domain features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building **hidden Markov models** (**HMMs)**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a speech recognizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a **text-to-speech** (**TTS**) system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you will need the following files (which
    are available on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_plot.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_read.wav`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freq_transform.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_freq.wav`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synthesize_music.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extract_freq_features.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_freq.wav`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_recognizer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tts.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Speech recognition** refers to the process of recognizing and understanding
    spoken language. The input comes in the form of audio data, and the speech recognizers
    will process this data to extract meaningful information from it. This has a lot
    of practical uses, such as voice-controlled devices, the transcription of spoken
    language into words and security systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Speech signals are very versatile in nature. There are many variations of speech
    in the same language. There are different elements to speech, such as language,
    emotion, tone, noise, and accent. It's difficult to rigidly define a set of rules
    of what can constitute speech. Even with all these variations, humans are very
    good at understanding all of this with relative ease. Hence, we need machines
    to understand speech in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Over the last couple of decades, researchers have worked on various aspects
    of speech, such as identifying the speaker, understanding words, recognizing accents,
    and translating speech. Among all these tasks, automatic speech recognition has
    been the focal point for many researchers. In this chapter, we will learn how
    to build a **speech recognizer**.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and plotting audio data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at how to read an audio file and visualize the signal. This
    will be a good starting point, and it will give us a good understanding of the
    basic structure of audio signals. Before we start, we need to understand that
    audio files are digitized versions of actual audio signals. Actual audio signals
    are complex, continuous-valued waves. In order to save a digital version, we sample
    the signal and convert it into numbers. For example, speech is commonly sampled
    at 44,100 Hz. This means that each second of the signal is broken down into 44,100
    parts, and the values at these timestamps are stored. In other words, you store
    a value every 1/44,100 seconds. As the sampling rate is high, we feel that the
    signal is continuous when we listen to it on our media players.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `wavfile` package to read the audio file from
    a `.wav` input file. So, we will draw the signal with a diagram.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following steps to read and plot audio using the `wavfile`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `read_plot.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `wavfile` package to read the audio file from the `input_read.wav`
    input file that''s already provided for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print out the parameters of this signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The audio signal is stored as 16-bit signed integer data; we need to normalize
    these values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s extract the first 30 values to plot, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The *x* axis is the **time axis**. Let''s build this axis, considering the
    fact that it should be scaled using the sampling frequency factor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the units to seconds, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now plot this as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is in the `read_plot.py` file. If you run this code, you will
    see the following signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/45ca3d88-69b9-4028-b6b4-6a06819e1bb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will also see the following output printed on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wave audio files are uncompressed files. The format was introduced with Windows
    3.1 as a standard format for the sound used in multimedia applications. Its technical
    specifications and description can be found in the *Multimedia Programming Interface
    and Data Specifications 1.0* document ([https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf](https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf)).
    It is based on the **Resource Interchange File Format** (**RIFF**) specifications
    that were introduced in 1991, constituting a metaformat for multimedia files running
    in the Windows environment. The RIFF structure organizes blocks of data in sections
    called chunks, each of which describes a characteristic of the WAV file (such
    as the sample rate, the bit rate, and the number of audio channels), or contains
    the values of the samples (in this case, we are referring to chunk data). The
    chunks are 32 bit (with some exceptions).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To read the WAV file, the `scipy.io.wavfile.read()` function was used. This
    function returns data from a WAV file along with the sample rate. The returned
    sample rate is a Python integer, and the data is returned as a NumPy array with
    a datatype that corresponds to the file.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `scipy.io.wavfile.read()` function: [https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *WAV* (from Wikipedia): [https://en.wikipedia.org/wiki/WAV](https://en.wikipedia.org/wiki/WAV)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming audio signals into the frequency domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Audio signals consist of a complex mixture of sine waves of different frequencies,
    amplitudes, and phases. Sine waves are also referred to as **sinusoids**. There
    is a lot of information that is hidden in the frequency content of an audio signal.
    In fact, an audio signal is heavily characterized by its frequency content. The
    whole world of speech and music is based on this fact. Before you proceed further,
    you will need some knowledge of **Fourier transforms**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to transform an audio signal into the frequency
    domain. To do this, the `numpy.fft.fft()` function is used. This function computes
    the one-dimensional *n*-point **discrete Fourier transform** (**DFT**) with the
    efficient **fast Fourier transform** (**FFT**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to transform audio signals into the frequency domain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following package (the full code is
    in the `freq_transform.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the `input_freq.wav` file that is already provided for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the signal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The audio signal is just a NumPy array. So, you can extract the length using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the Fourier transform. The Fourier transform signal is mirrored
    along the center, so we just need to take the first half of the transformed signal.
    Our end goal is to extract the power signal, so we square the values in the signal
    in preparation for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the length of the signal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to double the signal according to the length of the signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The power signal is extracted using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The *x* axis is the time axis; we need to scale this according to the sampling
    frequency and then convert this into seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the signal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/70c4c4b5-2abc-46bc-b29b-9d57fba262ad.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sound spectrum is a graphical representation of the sound level, normally
    in **decibels** (**dB**), depending on the frequency in Hz. If the sound to be
    analyzed is a so-called pure sound (signal at a single frequency constant over
    time), for example, a perfect sine wave, the signal spectrum will have a single
    component at the sine wave frequency, with a certain level in dB. In reality,
    any real signal consists of a large number of sinusoidal components of amplitude
    that are continuously variable over time. For these signals, it is impossible
    to analyze pure tones because there are always fractions of the signal energy
    that are difficult to represent with sinusoids. In fact, the representation of
    a signal as the sum of sinusoidal harmonic components, according to the Fourier
    transform theorem, is only valid for stationary signals, which often do not correspond
    to real sounds.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The frequency analysis of the sounds is based on the Fourier transform theorem.
    That is, any periodic signal can be generated by summing together so many sinusoidal
    signals (called harmonics) having multiple whole frequencies of the frequency
    of the periodic signal (called fundamental frequency).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `numpy.fft.fft()` function: [https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.fft.fft.html](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.fft.fft.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *The Fourier Transform*: [http://www.thefouriertransform.com/](http://www.thefouriertransform.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Time-frequency representations* (from Aalto University): [https://mycourses.aalto.fi/pluginfile.php/145214/mod_resource/content/3/slides_05_time-frequency_representations.pdf](https://mycourses.aalto.fi/pluginfile.php/145214/mod_resource/content/3/slides_05_time-frequency_representations.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating audio signals with custom parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sound is a particular type of wave in which a variation of pressure that is
    induced by a vibrating body (that is, a sound source) propagates in the surrounding
    medium (usually air). Some examples of sound sources include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Musical instruments in which the vibrating part can be a struck string (such
    as a guitar), or rubbed with a bow (such as the violin).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our vocal cords that are made to vibrate from the air that comes out of the
    lungs and give rise to the voice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any phenomenon that causes a movement of air (such as the beating wings of a
    bird, an airplane that breaks down the supersonic barrier, a bomb that explodes,
    or a hammer beating on an anvil) having appropriate physical characteristics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reproduce sound through electronic equipment, it is necessary to transform
    it into an analogue sound that is an electric current that originates from the
    transformation by conversion of the mechanical energy of the sound wave into electrical
    energy. In order to be able to use the sound signals with the computer, it is
    necessary to transfigure the analogue in a digital signal originating from the
    transformation of the analog sound into an audio signal represented by a flow
    of 0 and 1 (bit).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use NumPy to generate audio signals. As we discussed
    earlier, audio signals are complex mixtures of sinusoids. So, we will bear this
    in mind when we generate our own audio signal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to generate audio signals with custom parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `generate.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to define the output file where the generated audio will be stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now specify the audio generation parameters. We want to generate a 3-second
    long signal with a sampling frequency of 44,100, and a tonal frequency of 587
    Hz. The values on the time axis will go from *-2*pi* to *2*pi*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s generate the time axis and the audio signal. The audio signal is a simple
    sinusoid with the previously mentioned parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add some noise to the signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to scale the values to 16-bit integers before we store them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Write this signal to the output file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the signal using the first 100 values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the time axis, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the time axis into seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the signal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6ada8825-3c7c-4b24-b6eb-a6a4227b51e0.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the NumPy library to generate audio signals. We have
    seen that a digital sound is a sequence of numbers, so generating a sound will
    be enough to build an array that represents a musical tone. First, we set the
    filename to where the output will be saved. Then, we specified the audio parameters.
    Thus, we generated audio using a sine wave. We then added some noise, so we resized
    to 16-bit integer values. In the end, we wrote the signal on the output file.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the coding of a signal, each value assigned to the single sample is represented
    in bits. Each bit corresponds to a dynamic range of 6 dB. The higher the number
    of bits used, the higher the range of dB that can be represented by the single
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the typical values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 8 bits per sample that correspond to 256 levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16 bits per sample (the number used for CDs) that correspond to 65,636 levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the NumPy library: [http://www.numpy.org/](http://www.numpy.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Sine wave* (from Wikipedia): [https://en.wikipedia.org/wiki/Sine_wave](https://en.wikipedia.org/wiki/Sine_wave)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesizing music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional musical instruments, sound is produced by the vibration of mechanical
    parts. In synthetic instruments, vibration is described by functions over time,
    called signals, which express the variation in the time of the acoustic pressure.
    Sound synthesis is a process that allows you to generate the sound artificially.
    The parameters by which the timbre of the sound is determined differ according
    to the type of synthesis that is used for the generation, and can be provided
    directly by the composer, or with actions on appropriate input devices, or derived
    from the analysis of pre-existing sounds.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to synthesize some music. To do this, we will
    use various notes, such as *A*, *G*, and *D*, along with their corresponding frequencies,
    to generate some simple music.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to synthesize some music:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `synthesize_music.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to synthesize a tone, based on input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the time axis values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Construct the audio sample using the input arguments, such as amplitude and
    frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the main function. You''ve been provided with a JSON file, called
    `tone_freq_map.json`, which contains some notes along with their frequencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Load that file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s assume that we want to generate a `G` note for a duration of two
    seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the function with the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the generated signal into the output file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: A single tone `.wav` file is generated (`output_tone.wav`). Open this file in
    a media player and listen to it. That's the *G* note!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s do something more interesting. Let''s generate some notes in sequence
    to give it a musical feel. Define a note sequence along with their durations in
    seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through this list and call the synthesizer function for each of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the signal to the output file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can now open the `output_tone_seq.wav` file in your media player and listen
    to it. You can feel the music!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Music is a work of ingenuity and creativity that is difficult to explain in
    a nutshell. Musicians read a piece of music recognizing the notes as they are
    placed on the stave. By analogy, we can regard the synthesis of sound as a sequence
    of the characteristic frequencies of the known ones. In this recipe, we have used
    this procedure to synthesize a short sequence of notes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To generate music artificially, the synthesizer is used. All synthesizers have
    the following basic components that work together to create a sound:'
  prefs: []
  type: TYPE_NORMAL
- en: An oscillator that generates the waveform and changes the tone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A filter that cuts out some frequencies in the wave to change the timbre
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An amplifier that controls the volume of the signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modulator to create effects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the NumPy library: [http://www.numpy.org/](http://www.numpy.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `scipy.io.wavfile.write()` function: [https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Frequencies of Musical Notes* (from Michigan Technological University):
    [http://pages.mtu.edu/~suits/notefreqs.html](http://pages.mtu.edu/~suits/notefreqs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Principles of Sound Synthesis* (from the University of Salford): [http://www.acoustics.salford.ac.uk/acoustics_info/sound_synthesis/](http://www.acoustics.salford.ac.uk/acoustics_info/sound_synthesis/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting frequency domain features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Transforming audio signals into the frequency domain* recipe, we discussed
    how to convert a signal into the frequency domain. In most modern speech recognition
    systems, people use frequency domain features. After you convert a signal into
    the frequency domain, you need to convert it into a usable form. **Mel Frequency
    Cepstral Coefficients** (**MFCC**) is a good way to do this. MFCC takes the power
    spectrum of a signal and then uses a combination of filter banks and **discrete
    cosine transform** (**DCT**) to extract the features.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use the `python_speech_features` package
    to extract frequency domain features. You can find the installation instructions
    at [http://python-speech-features.readthedocs.org/en/latest](http://python-speech-features.readthedocs.org/en/latest).
    So, let's take a look at how to extract MFCC features.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to extract frequency domain features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `extract_freq_features.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the `input_freq.wav` input file that is already provided for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the MFCC and filter bank features, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the parameters to see how many windows were generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now visualize the MFCC features. We need to transform the matrix so
    that the time domain is horizontal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s visualize the filter bank features. Again, we need to transform
    the matrix so that the time domain is horizontal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output for MFCC features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9f6de9bc-6a8b-4f22-b167-265da6b87b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The filter bank features will look like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c447803e-13e9-42d9-85f3-00e03bbe3f3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will get the following output on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **cepstrum** is the result of the Fourier transform applied to the dB spectrum
    of a signal. Its name is derived from the reversal of the first four letters of
    the word **spectrum**. It was defined in 1963 by Bogert et al. Thus, the cepstrum
    of a signal is the Fourier transform of the log value of the Fourier transform
    of the signal.
  prefs: []
  type: TYPE_NORMAL
- en: The graph of the cepstrum is used to analyze the rates of change of the spectral
    content of a signal. Originally, it was invented to analyze earthquakes, explosions,
    and the responses to radar signals. It is currently a very effective tool for
    discriminating the human voice in music informatics. For these applications, the
    spectrum is first transformed through the frequency bands of the Mel scale. The
    result is the spectral coefficient Mel, or MFCCs. It is used for voice identification
    and pitch detection algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cepstrum is used to separate the part of the signal that contains the excitation
    information from the transfer function performed by the larynx. The lifter action
    (filtering in the frequency domain) has as its objective the separation of the
    excitation signal from the transfer function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `python_speech_features` package: [https://python-speech-features.readthedocs.io/en/latest/](https://python-speech-features.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the MFCC tutorial: [http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building HMMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to discuss speech recognition. We will use HMMs to perform
    speech recognition; HMMs are great at modeling time series data. As an audio signal
    is a time series signal, HMMs perfectly suit our needs. An HMM is a model that
    represents probability distributions over sequences of observations. We assume
    that the outputs are generated by hidden states. So, our goal is to find these
    hidden states so that we can model the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to build an HMM using the `hmmlearn` package. Before
    you proceed, you will need to install the `hmmlearn` package. Let's take a look
    at how to build HMMs.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build HMMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and define a class to model HMMs (the full code is
    in the `speech_recognizer.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s initialize the class; we will use Gaussian HMMs to model our data. The
    `n_components` parameter defines the number of hidden states. `cov_type` defines
    the type of covariance in our transition matrix, and `n_iter` indicates the number
    of iterations it will go through before it stops training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The choice of the preceding parameters depends on the problem at hand. You need
    to have an understanding of your data in order to select these parameters in a
    smart way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the variables, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the model with the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The input data is a NumPy array, where each element is a feature vector consisting
    of *k *dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to extract the score, based on the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We built a class to handle HMM training and prediction, but we need some data
    to see it in action. We will use it in the next recipe to build a speech recognizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HMM is a model where the system is assumed to be a Markov process with unobserved
    states. A stochastic process is called Markovian when, having chosen a certain
    instance of *t* for observation, the evolution of the process, starting with *t*,
    depends only on *t*, and does not depend in any way on the previous instances.
    Thus, a process is Markovian when, given the moment of observation, only a particular
    instance determines the future evolution of the process, and that evolution does
    not depend on the past.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An HMM is, therefore, a Markov chain in which states are not directly observable.
    More precisely, it can be understood as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The chain has a number of states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The states evolve according to a Markov chain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each state generates an event with a certain probability distribution that depends
    only on the state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The event is observable, but the state is not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HMMs are particularly known for their applications in the recognition of the
    temporal pattern of spoken speeches, handwriting, texture recognition, and bioinformatics.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `hmmlearn` package: [https://hmmlearn.readthedocs.io/en/latest/](https://hmmlearn.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *A Tutorial on Hidden Markov Models (*by Lawrence R. Rabiner): [https://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf](https://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a speech recognizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speech recognition is the process by which human oral language is recognized,
    and subsequently processed through a computer, or, more specifically, through
    a special speech recognition system. Speech recognition systems are used for automated
    voice applications in the context of telephone applications (such as automatic
    call centers) for dictation systems, which allow the dictation of speeches to
    the computer, for control systems of the navigation system satellite, or for a
    phone in a car via voice commands.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need a database of speech files to build our speech recognizer. We will use
    the database available at [https://code.google.com/archive/p/hmm-speech-recognition/downloads](https://code.google.com/archive/p/hmm-speech-recognition/downloads).
    This contains 7 different words, where each word has 15 audio files associated
    with it. Download the ZIP file and extract the folder that contains the Python
    file (rename the folder that contains the data as `data`). This is a small dataset,
    but it is sufficient in understanding how to build a speech recognizer that can
    recognize 7 different words. We need to build an HMM model for each class. When
    we want to identify the word in a new input file, we need to run all the models
    on this file and pick the one with the best score. We will use the HMM class that
    we built in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a speech recognizer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `speech_recognizer.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to parse the input arguments in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the `HMMTrainer` class defined in the previous *Building HMMs* recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the main function, and parse the input arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Initiate the variable that will hold all the HMM models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the input directory that contains all the database''s audio files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the name of the subfolder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of the subfolder is the label of this class; extract it using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the variables for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through the list of audio files in each subfolder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Read each audio file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the MFCC features, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep appending this to the `X` variable, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Append the corresponding label too, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have extracted features from all the files in the current class, train
    and save the HMM model. As HMM is a generative model for unsupervised learning,
    we don''t need labels to build HMM models for each class. We explicitly assume
    that separate HMM models will be built for each class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a list of test files that were not used for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the input files, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in each audio file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the MFCC features, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the variables to store the maximum score and the output label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through all the models and run the input file through each of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the score and store the maximum score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the true and predicted labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is in the `speech_recognizer.py` file. Run this file using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we created a speech recognition system using an HMM. To do this,
    we first created a function to analyze input arguments. Then, a class was used
    to handle all HMM-related processing. Thus, we have classified the input data
    and then predicted the label of the test data. Finally, we printed the results.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A voice recognition system is based on a comparison of the input audio, which
    is appropriately processed, with a database created during system training. In
    practice, the software application tries to identify the word spoken by the speaker,
    looking for a similar sound in the database, and checking which word corresponds.
    Naturally, it is a very complex operation. Moreover, it is not done on whole words,
    but on the phonemes that compose them.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `hmmlearn` package: [https://hmmlearn.readthedocs.io/en/latest/](https://hmmlearn.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `python_speech_features` package: [https://python-speech-features.readthedocs.io/en/latest/](https://python-speech-features.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the *Argparse Tutorial*: [https://docs.python.org/3/howto/argparse.html](https://docs.python.org/3/howto/argparse.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Fundamentals of Speech Recognition: A Short Course* (from Mississippi
    State University): [http://www.iitg.ac.in/samudravijaya/tutorials/fundamentalOfASR_picone96.pdf](http://www.iitg.ac.in/samudravijaya/tutorials/fundamentalOfASR_picone96.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a TTS system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speech synthesis is the technique that is used for the artificial reproduction
    of the human voice. A system used for this purpose is called a speech synthesizer
    and can be implemented by software or hardware. Speech synthesis systems are also
    known as TTS systems due to their ability to convert text into speech. There are
    also systems that convert phonetic symbols into speech.
  prefs: []
  type: TYPE_NORMAL
- en: Speech synthesis can be achieved by concatenating recordings of vocals stored
    in a database. The various systems of speech synthesis differ according to the
    size of the stored voice samples. That is, a system that stores single phonemes
    or double phonemes allows you to obtain the maximum number of combinations at
    the expense of overall clarity, while other systems which are designed for a specific
    use repeat themselves, to record whole words or entire sentences in order to achieve
    a high-quality result.
  prefs: []
  type: TYPE_NORMAL
- en: A synthesizer can create a completely synthetic voice using vocal traits and
    other human characteristics. The quality of a speech synthesizer is evaluated
    on the basis of both the resemblance to the human voice and its level of comprehensibility.
    A TTS conversion program with good performance can play an important role in accessibility;
    for example, by allowing people with impaired vision or dyslexia to listen to
    documents written on the computer. For this type of application (since the early
    1980s), many operating systems have included speech synthesis functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will introduce the Python library that allows us to create
    TTS systems. We will run the `pyttsx` cross-platform TTS wrapper library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a TTS system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must install `pyttsx` for the Python 3 library (offline TTS for Python
    3) and its relative dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid possible errors, it is also necessary to install the `pypiwin32` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new Python file and import the `pyttsx3` package (the full code is
    in the `tts.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We create an engine instance that will use the specified driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'To change the speech rate, use the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'To change the voice of the speaker, use the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use the `say` method to queue a command to speak an utterance.
    The speech is output according to the properties set before this command in the
    queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will invoke the `runAndWait()` method. This method blocks while
    processing all currently queued commands and invokes callbacks for engine notifications
    appropriately. It returns when all commands queued before this call are emptied
    from the queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: At this point, a different voice will read the text supplied by us.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A speech synthesis system or engine is composed of two parts: a frontend and
    a backend. The frontend part deals with the conversion of the text into phonetic
    symbols, while the backend part interprets the phonetic symbols and reads them,
    thus, transforming them into an artificial voice. The frontend has two key functions;
    first, it performs an analysis of the written text to convert all numbers, abbreviations,
    and abbreviations into words in full. This preprocessing step is referred to as
    tokenization. The second function consists of converting each word into its corresponding
    phonetic symbols and performing the linguistic analysis of the revised text, subdividing
    it into prosodic units, that is, into prepositions, sentences, and periods. The
    process of assigning phonetic transcription to words is called conversion from
    text to phoneme, or from grapheme to phoneme.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An evolution of the classic TTS system is called `WaveNet`, and it seems to
    know how to speak, articulate accents, and pronounce a whole sentence fluently.
    WaveNet is a deep neural network that generates raw audio. It was created by researchers
    at the London-based artificial intelligence firm, DeepMind. WaveNet uses a deep
    generative model for sound waves that can imitate any human voice. The sentences
    pronounced by WaveNet sound 50% more similar to a human voice than the more advanced
    TTS. To demonstrate this, samples were created in English and Mandarin, and using
    the **Mean Opinion Scores** (**MOS**) system, which is now a standard in audio
    evaluation, samples of artificial intelligence were compared to those generated
    by normal TTS, parametric-TTS, and also with respect to the samples of real voices.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `pyttsx3` package: [https://pyttsx3.readthedocs.io/en/latest/index.html](https://pyttsx3.readthedocs.io/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Text to Speech: A Simple Tutorial* (by D. Sasirekha and E. Chandra):
    [https://pdfs.semanticscholar.org/e7ad/2a63458653ac965fe349fe375eb8e2b70b02.pdf](https://pdfs.semanticscholar.org/e7ad/2a63458653ac965fe349fe375eb8e2b70b02.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *WaveNet: A Generative Model for Raw Audio* (from Google DeepMind):
    [https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
