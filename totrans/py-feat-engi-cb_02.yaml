- en: <st c="0">2</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Encoding Categorical Variables</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`<st c="146">Home owner</st>` <st c="156">variable with the values of</st>
    `<st c="185">owner</st>` <st c="190">and</st> `<st c="195">non-owner</st>` <st
    c="204">is categorical, and so is the</st> `<st c="235">Marital status</st>` <st
    c="249">variable with the values of</st> `<st c="278">never married</st>`<st c="291">,</st>
    `<st c="293">married</st>`<st c="300">,</st> `<st c="302">divorced</st>`<st c="310">,
    and</st> `<st c="316">widowed</st>`<st c="323">. In some categorical variables,
    the labels have an intrinsic order; for example, in the</st> `<st c="412">Student''s
    grade</st>` <st c="427">variable, the values of</st> `<st c="452">A</st>`<st c="453">,</st>
    `<st c="455">B</st>`<st c="456">,</st> `<st c="458">C</st>`<st c="459">, and</st>
    `<st c="465">Fail</st>` <st c="469">are ordered, with</st> `<st c="488">A</st>`
    <st c="489">being the highest grade and</st> `<st c="518">Fail</st>` <st c="522">being
    the lowest.</st> <st c="541">These</st> <st c="547">are called</st> `<st c="708">City</st>`
    <st c="712">variable, with the values of</st> `<st c="742">London</st>`<st c="748">,</st>
    `<st c="750">Manchester</st>`<st c="760">,</st> `<st c="762">Bristol</st>`<st
    c="769">, and</st> <st c="775">so on.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="781">The values of categorical variables are often encoded as strings.</st>
    <st c="848">To train most machine learning models, we need to transform those
    strings into numbers.</st> <st c="936">The act of replacing strings</st> <st c="965">with
    numbers is</st> <st c="981">called</st> **<st c="988">categorical encoding</st>**<st
    c="1008">. In this chapter, we will discuss multiple categorical</st> <st c="1064">encoding
    methods.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1081">This chapter will cover the</st> <st c="1110">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1128">Creating binary variables through</st> <st c="1163">one-hot encoding</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1179">Performing one-hot encoding of</st> <st c="1211">frequent categories</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1230">Replacing categories with counts or the frequency</st> <st c="1281">of
    observations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1296">Replacing categories with</st> <st c="1323">ordinal numbers</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1338">Performing ordinal encoding based on the</st> <st c="1380">target
    value</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1392">Implementing target</st> <st c="1413">mean encoding</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1426">Encoding with the Weight</st> <st c="1452">of Evidence</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1463">Grouping rare or</st> <st c="1481">infrequent categories</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1502">Performing</st> <st c="1514">binary en</st><st c="1523">c</st><st
    c="1525">oding</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1530">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1553">In this chapter, we will use the</st> `<st c="1587">Matplotlib</st>`<st
    c="1597">,</st> `<st c="1599">pandas</st>`<st c="1605">,</st> `<st c="1607">NumPy</st>`<st
    c="1612">,</st> `<st c="1614">scikit-learn</st>`<st c="1626">,</st> `<st c="1628">feature-engine</st>`<st
    c="1642">, and Category Encoders Python libraries.</st> <st c="1684">If you need
    to install Python, the free Anaconda Python distribution (</st>[<st c="1754">https://www.anaconda.com/</st>](https://www.anaconda.com/)<st
    c="1780">) includes most numerical</st> <st c="1807">computing libraries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="1827">feature-engine</st>` <st c="1842">can be installed</st> <st c="1860">with</st>
    `<st c="1865">pip</st>`<st c="1868">:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="1897">If you use Anaconda, you can install</st> `<st c="1935">feature-engine</st>`
    <st c="1949">with</st> `<st c="1955">conda</st>`<st c="1960">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="2006">To install Category Encoders, use</st> `<st c="2041">pip</st>`
    <st c="2044">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="2086">We will use the</st> **<st c="2103">Credit Approval</st>** <st
    c="2118">dataset from the</st> *<st c="2136">UCI Machine Learning Repository</st>*
    <st c="2167">(</st>[<st c="2169">https://archive.ics.uci.edu/</st>](https://archive.ics.uci.edu/)<st
    c="2197">), licensed under the CC BY 4.0 creative commons attribution:</st> [<st
    c="2260">https://creativecommons.org/licenses/by/4.0/legalcode</st>](https://creativecommons.org/licenses/by/4.0/legalcode)<st
    c="2313">. You’ll find the dataset at this</st> <st c="2347">link:</st> [<st c="2353">http://archive.ics.uci.edu/dataset/27/credit+approval</st>](http://archive.ics.uci.edu/dataset/27/credit+approval)<st
    c="2406">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2407">I downloaded and modified the data as shown in this</st> <st c="2460">notebook:</st>
    [<st c="2470">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/credit-approval-dataset.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/credit-approval-dataset.ipynb)<st
    c="2620">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2621">You’ll find a copy of the modified data set in the accompanying
    GitHub</st> <st c="2693">repository:</st> [<st c="2705">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/)<st
    c="2826">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2827">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2832">Before encoding categorical variables, you might want to impute
    their missing data.</st> <st c="2917">Check out the imputation methods for categorical
    variables in</st> [*<st c="2979">Chapter 1</st>*](B22396_01.xhtml#_idTextAnchor020)<st
    c="2988">,</st> *<st c="2990">Imputing</st>* *<st c="2999">Missing</st> <st c="3007">Data</st>*<st
    c="3011">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3012">Creating binary variables through one-hot encoding</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`<st c="3223">1</st>` <st c="3224">if the category is present, or</st> `<st
    c="3256">0</st>` <st c="3257">otherwise.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3268">The following</st> <st c="3282">table shows the one-hot encoded
    representation of the</st> `<st c="3337">Smoker</st>` <st c="3343">variable with
    the categories of</st> `<st c="3376">Smoker</st>` <st c="3382">and</st> `<st c="3387">Non-Smoker</st>`<st
    c="3397">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – One-hot encoded representation of the Smoker variable](img/B22396_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="3475">Figure 2.1 – One-hot encoded representation of the Smoker variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3541">As shown in</st> *<st c="3554">Figure 2</st>**<st c="3562">.1</st>*<st
    c="3564">, from the</st> `<st c="3575">Smoker</st>` <st c="3581">variable, we
    can derive a binary variable for</st> `<st c="3628">Smoker</st>`<st c="3634">,
    which shows the value of</st> `<st c="3661">1</st>` <st c="3662">for smokers,</st>
    <st c="3675">or the binary variable for</st> `<st c="3703">Non-Smoker</st>`<st
    c="3713">, which takes the value of</st> `<st c="3740">1</st>` <st c="3741">for
    those who do</st> <st c="3759">not smoke.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3769">For the</st> `<st c="3778">Color</st>` <st c="3783">categorical
    variable with the values of</st> `<st c="3824">red</st>`<st c="3827">,</st> `<st
    c="3829">blue</st>`<st c="3833">, and</st> `<st c="3839">green</st>`<st c="3844">,
    we can create three variables called</st> `<st c="3883">red</st>`<st c="3886">,</st>
    `<st c="3888">blue</st>`<st c="3892">, and</st> `<st c="3898">green</st>`<st c="3903">.
    These variables will be assigned a value of</st> `<st c="3949">1</st>` <st c="3950">if
    the observation corresponds to the respective color, and</st> `<st c="4011">0</st>`
    <st c="4012">if it</st> <st c="4019">does not.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4028">A categorical variable with</st> *<st c="4057">k</st>* <st c="4058">unique
    categories can be encoded using</st> *<st c="4098">k-1</st>* <st c="4101">binary
    variables.</st> <st c="4120">For</st> `<st c="4124">Smoker</st>`<st c="4130">,</st>
    *<st c="4132">k</st>* <st c="4134">is</st> *<st c="4137">2</st>* <st c="4138">as
    it contains two labels (</st>`<st c="4166">Smoker</st>` <st c="4173">and</st>
    `<st c="4178">Non-Smoker</st>`<st c="4188">), so we only need one binary variable
    (</st>*<st c="4229">k - 1 = 1</st>*<st c="4239">) to capture all the information.</st>
    <st c="4274">For the</st> `<st c="4282">Color</st>` <st c="4287">variable, which
    has 3 categories (</st>*<st c="4322">k = 3</st>*<st c="4328">;</st> `<st c="4331">red</st>`<st
    c="4334">,</st> `<st c="4336">blue</st>`<st c="4340">, and</st> `<st c="4346">green</st>`<st
    c="4351">), we need 2 (</st>*<st c="4366">k - 1 = 2</st>*<st c="4376">) binary</st>
    <st c="4386">variables to capture all the information so that the</st> <st c="4439">following
    occurs:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4456">If the observation is red, it will be captured by the</st> `<st
    c="4511">red</st>` <st c="4514">variable (</st>`<st c="4525">red</st>` <st c="4529">=</st>
    `<st c="4532">1</st>`<st c="4533">,</st> `<st c="4535">blue</st>` <st c="4539">=</st>
    `<st c="4542">0</st>`<st c="4543">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4544">If the observation is blue, it will be captured by the</st> `<st
    c="4599">blue</st>` <st c="4603">variable (</st>`<st c="4614">red</st>` <st c="4618">=</st>
    `<st c="4621">0</st>`<st c="4622">,</st> `<st c="4624">blue</st>` <st c="4628">=</st>
    `<st c="4631">1</st>`<st c="4632">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4633">If the observation is green, it will be captured by the combination
    of</st> `<st c="4704">red</st>` <st c="4707">and</st> `<st c="4712">blue</st>`
    <st c="4716">(</st>`<st c="4718">red</st>` <st c="4721">=</st> `<st c="4724">0</st>`<st
    c="4725">,</st> `<st c="4727">blue</st>` <st c="4731">=</st> `<st c="4734">0</st>`<st
    c="4735">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4736">Encoding</st> <st c="4744">into</st> *<st c="4750">k-1</st>* <st
    c="4753">binary variables is well suited for linear models.</st> <st c="4805">There
    are a few occasions in which we may prefer to encode the categorical variables
    with</st> *<st c="4895">k</st>* <st c="4896">binary variables:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4913">When training decision trees, since they do not evaluate the entire
    feature space at the</st> <st c="5003">same time</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5012">When selecting</st> <st c="5028">features recursively</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5048">When determining the importance of each category within</st> <st
    c="5105">a variable</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5115">In this recipe, we will compare the one-hot encoding implementations
    of</st> `<st c="5188">pandas</st>`<st c="5194">,</st> `<st c="5196">scikit-learn</st>`<st
    c="5208">,</st> <st c="5210">and</st> `<st c="5214">featur</st><st c="5220">e-engine</st>`<st
    c="5229">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5230">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5246">Fir</st><st c="5250">st, let’s make a few imports an</st><st c="5282">d
    get the</st> <st c="5293">data ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5304">Import</st> `<st c="5312">pandas</st>` <st c="5318">and the</st>
    `<st c="5327">train_test_split</st>` <st c="5343">function</st> <st c="5353">from</st>
    `<st c="5358">scikit-learn</st>`<st c="5370">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5445">Let’s load the Credit</st> <st c="5468">Approval dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5531">Let’s separate the data into train and</st> <st c="5571">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5721">Let’s</st> <st c="5728">inspect the unique categories</st> <st
    c="5757">of the</st> `<st c="5765">A4</st>` <st c="5767">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5800">We can see the unique values of</st> `<st c="5833">A4</st>` <st
    c="5835">in the</st> <st c="5843">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: dummies = pd.get_dummies(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train["A4"], drop_first=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: dummies.head()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6102">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6107">With</st> `<st c="6113">pandas</st>`<st c="6119">’</st> `<st c="6122">get_dummies()</st>`<st
    c="6135">, we can either ignore or encode missing data through the</st> `<st c="6193">dummy_na</st>`
    <st c="6201">parameter.</st> <st c="6213">By setting</st> `<st c="6224">dummy_na=True</st>`<st
    c="6237">, missing data will be encoded in a new binary variable.</st> <st c="6294">To
    encode the variable into</st> *<st c="6322">k</st>* <st c="6323">dummies, use</st>
    `<st c="6337">drop_first=False</st>` <st c="6353">instead.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6362">Here, we can see the output of</st> *<st c="6394">Step 5</st>*<st
    c="6400">, where each label is now a</st> <st c="6428">binary variable:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="6593">Now, let’s</st> <st c="6605">encode</st> <st c="6612">all the categorical
    variables into</st> *<st c="6647">k-1</st>* <st c="6650">binaries:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6768">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="6773">pandas</st>`<st c="6780">’</st> `<st c="6783">get_dummies()</st>`<st
    c="6796">will encode all variables of the object, string, or category type by
    default.</st> <st c="6875">To encode a subset of the variables, pass the variable
    names in a list to the</st> `<st c="6953">columns</st>` <st c="6960">par</st><st
    c="6964">ameter.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6972">Let’s inspect the first</st> <st c="6996">five rows of the</st>
    <st c="7014">resulting DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7053">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7058">When encoding more than one variable,</st> `<st c="7097">get_dummies()</st>`
    <st c="7110">captures the variable name – say,</st> `<st c="7145">A1</st>` <st
    c="7147">– and places an underscore followed by the category name to identify
    the resulting</st> <st c="7231">binary variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7248">We can see the binary variables in the</st> <st c="7288">follo</st><st
    c="7293">wing output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – A transformed DataFrame showing the numerical variables followed
    by the one-hot encoded representation of the categorical variables](img/B22396_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="7967">Figure 2.2 – A transformed DataFrame showing the numerical variables
    followed by the one-hot encoded representation of the categorical variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8111">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="8116">pandas</st>`<st c="8123">’</st> `<st c="8126">get_dummies()</st>`<st
    c="8139">will create one binary variable per category seen in a DataFrame.</st>
    <st c="8206">Hence, if there are more categories in the train set than in the
    test set,</st> `<st c="8281">get_dummies()</st>` <st c="8294">will return more
    columns in the transformed train set than in the transformed test set, and vice
    versa.</st> <st c="8399">To avoid this, it is better to carry out one-hot encoding
    with</st> `<st c="8462">scikit-learn</st>` <st c="8474">or</st> `<st c="8478">feature-engine</st>`<st
    c="8492">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8493">Let’s do one-hot encoding using</st> `<st c="8526">scikit-learn</st>`
    <st c="8538">instead.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8547">Let’s</st> <st c="8554">import the encoder</st> <st c="8572">and</st>
    `<st c="8577">ColumnTransformer</st>` <st c="8594">from</st> `<st c="8600">scikit-learn</st>`<st
    c="8612">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8708">Let’s create a list with the names of the</st> <st c="8751">categorical
    variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8838">Let’s set up the encoder to create</st> *<st c="8874">k-1</st>*
    <st c="8877">binary variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8954">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8959">To encode variables into</st> *<st c="8985">k</st>* <st c="8986">dummies,
    set the</st> `<st c="9004">drop</st>` <st c="9008">parameter to</st> `<st c="9022">None</st>`<st
    c="9026">. To encode only binary variables into</st> *<st c="9065">k-1</st>*<st
    c="9068">, set the</st> `<st c="9078">drop</st>` <st c="9082">parameter to</st>
    `<st c="9096">if_binary</st>`<st c="9105">. The latter is useful because encoding
    binary variables into</st> *<st c="9167">k</st>* <st c="9168">dummies</st> <st
    c="9177">is redundant.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9190">Let’s</st> <st c="9197">restrict the encoding to the</st> <st c="9226">categorical
    variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="9396">Let’s fit</st> <st c="9407">the encoder so that it identifies the
    categories</st> <st c="9456">to encode:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="9482">Let’s inspect the categories that will be represented with</st>
    <st c="9542">binary variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="9605">The transformer will add binary variables for the</st> <st c="9656">follow</st><st
    c="9662">ing categories:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Arrays with the categories that will be encoded into binary
    variables (one array per variable)](img/B22396_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="10132">Figure 2.3 – Arrays with the categories that will be encoded into
    binary variables (one array per variable)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10239">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="10244">scikit-learn</st>`<st c="10257">’s</st> `<st c="10261">OneHotEncoder()</st>`
    <st c="10276">will only encode the categories learned from the train set.</st>
    <st c="10337">If there are new categories in the test set, we can instruct the
    encoder to ignore them, return an error, or replace them with an infrequent category,
    by setting the</st> `<st c="10503">handle_unknown</st>` <st c="10517">parameter
    to</st> `<st c="10531">ignore</st>`<st c="10537">,</st> `<st c="10539">error</st>`<st
    c="10544">,</st> <st c="10546">or</st> `<st c="10549">infrequent_if</st><st c="10562">_exists</st>`<st
    c="10570">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10571">Let’s</st> <st c="10578">encode the</st> <st c="10589">categorical
    variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10681">Make sure to inspect the result by</st> <st c="10717">executing</st>
    `<st c="10727">X_test_enc.head()</st>`<st c="10744">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="10745">To get</st> <st c="10752">familiar with the output, let’s print
    the variable names of the</st> <st c="10817">resulting DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10864">In the following image, we see the names of the variables in the</st>
    <st c="10930">transf</st><st c="10936">ormed DataFrame:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Arrays with the names of the variables in the resulting DataFrame](img/B22396_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11696">Figure 2.4 – Arrays with the names of the variables in the resulting
    DataFrame</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11774">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="11779">ColumnTransformer()</st>` <st c="11799">changes the name and
    order of the variables during the transformation.</st> <st c="11871">If the variable
    was encoded, it will append the</st> `<st c="11919">encoder</st>` <st c="11926">prefix
    and if the variable was not modified, it will append the</st> `<st c="11991">remainder</st>`
    <st c="12000">prefix.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12008">To wrap up the recipe, let’s perform one-hot encoding</st> <st
    c="12063">with</st> `<st c="12068">featu</st><st c="12073">re-engine</st>`<st
    c="12083">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12084">Let’s</st> <st c="12090">import</st> <st c="12098">the encoder</st>
    <st c="12110">from</st> `<st c="12115">f</st>``<st c="12116">eature-engine</st>`<st
    c="12129">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12182">Let’s set up the encoder so that it returns</st> *<st c="12227">k-1</st>*
    <st c="12230">binary variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12288">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="12293">feature-engine</st>`<st c="12308">’s</st> `<st c="12312">OneHotEncoder()</st>`
    <st c="12327">encodes all categorical variables by default.</st> <st c="12374">To
    encode a subset of the variables, pass the variable names in a list:</st> `<st
    c="12446">OneHotEncoder(variables=["A1", "A4"]</st>`<st c="12482">).</st> <st
    c="12486">To encode numerical variables, set the</st> `<st c="12525">ignore_format</st>`
    <st c="12538">parameter to</st> `<st c="12552">True</st>` <st c="12556">or cast
    the variables</st> <st c="12579">as objects.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12590">Let’s fit the encoder to the train set so that it learns the categories
    and variables</st> <st c="12677">to encode:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12708">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12713">To encode binary variables into</st> *<st c="12746">k-1</st>*<st
    c="12749">, and other categorical variables into</st> *<st c="12788">k</st>* <st
    c="12789">dummies, set the</st> `<st c="12807">drop_last_binary</st>` <st c="12823">parameter</st>
    <st c="12834">to</st> `<st c="12837">True</st>`<st c="12841">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12842">Let’s explore the variables that will</st> <st c="12881">be encoded:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12911">The transformer found and stored the variables of the object or
    categorical type, as shown in the</st> <st c="13010">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13086">Let’s</st> <st c="13092">explore the c</st><st c="13106">ategories
    for which dummy variables will</st> <st c="13148">be created:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13181">The</st> <st c="13185">following dictionary contains the categories
    that will be encoded in</st> <st c="13255">each variable:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13555">Let’s encode the categorical variables in train and</st> <st c="13608">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13698">If we execute</st> `<st c="13713">X_train_enc.head()</st>`<st
    c="13731">, we will see the</st> <st c="13749">f</st><st c="13750">ollowing DataFrame:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Transformed DataFrame with numerical variables followed by the
    one-hot encoded representation of the categorical variables](img/B22396_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14176">Figure 2.5 – Transformed DataFrame with numerical variables followed
    by the one-hot encoded representation of the categorical variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14311">Note</st> <st c="14317">how the</st> `<st c="14325">A4</st>` <st
    c="14327">cate</st><st c="14332">gorical</st> <st c="14341">variable was replaced
    with</st> `<st c="14368">A4_u</st>`<st c="14372">,</st> `<st c="14374">A4_y</st>`<st
    c="14378">, and</st> <st c="14384">so on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14390">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14395">We can get the names of all the variables in the transformed dataset
    by</st> <st c="14468">executing</st> `<st c="14478">ohe_enc.get</st><st c="14489">_feature_names_out()</st>`<st
    c="14510">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14511">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14527">In this recipe, we performed a one-hot encoding of categorical
    variables using</st> `<st c="14607">pandas</st>`<st c="14613">,</st> `<st c="14615">scikit-learn</st>`<st
    c="14627">,</st> <st c="14629">and</st> `<st c="14633">feature-engine</st>`<st
    c="14647">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="14648">pandas</st>`<st c="14655">’</st> `<st c="14658">get_dummies()</st>`
    <st c="14671">replaced the categorical variables with a set of binary variables
    representing each of the categories.</st> <st c="14775">When used on the entire
    dataset, it returned the numerical variables, followed by the one-hot encoded
    representation of each seen category in every variable of type object, string,</st>
    <st c="14956">or categorical.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14971">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="14976">pandas</st>` <st c="14983">will return binary variables for
    every category seen in a dataset.</st> <st c="15051">In practice, to avoid data
    leakage and anticipate deployment eventualities, we want to return dummy variables
    for categories seen in a training set only.</st> <st c="15205">So, it is safer
    to use</st> `<st c="15228">scikit-learn</st>` <st c="15240">and</st> `<st c="15245">feature-engine</st>`<st
    c="15259">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15260">OneHotEncoder()</st>` <st c="15276">from</st> `<st c="15282">scikit-learn</st>`
    <st c="15294">or</st> `<st c="15298">feature-engine</st>` <st c="15312">learned
    the categories that should be represented by binary variables from the train set
    when we applied</st> `<st c="15418">fit()</st>`<st c="15423">. With</st> `<st
    c="15430">transform()</st>`<st c="15441">,</st> `<st c="15443">scikit-learn</st>`
    <st c="15455">returned just the binary variables, whereas</st> `<st c="15500">feature-engine</st>`
    <st c="15514">returned the numerical variables followed by the one-hot encoded
    representation of the</st> <st c="15602">categorical ones.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15619">scikit-learn</st>`<st c="15632">’s</st> `<st c="15636">OneHotEncoder()</st>`
    <st c="15651">encodes all variables by default.</st> <st c="15686">To restrict
    the encoding to</st> <st c="15714">categorical variables, we</st> <st c="15740">used</st>
    `<st c="15745">ColumnTransformer()</st>`<st c="15764">. We set the output of</st>
    `<st c="15787">transform()</st>`<st c="15798">to</st> `<st c="15802">pandas</st>`
    <st c="15808">to obtain the resulting data as</st> <st c="15841">a DataFrame.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15853">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15858">One-hot encoding is suitable for linear models.</st> <st c="15907">It
    also expands the feature space.</st> <st c="15942">If your dataset contains many
    categorical variables or highly cardinal variables, you can restrict the number
    of binary variables by encoding the most frequent categories only.</st> <st c="16119">You
    can do this automatically with both</st> `<st c="16159">scikit-learn</st>` <st
    c="16171">and</st> `<st c="16176">feature-engine</st>` <st c="16190">as we describe
    in the</st> *<st c="16213">Performing one-hot encoding of freq</st><st c="16248">uent</st>*
    *<st c="16254">categories</st>* <st c="16264">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16272">There’s more...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16288">We can also perform one-hot encoding using the Category Encoders
    Python</st> <st c="16361">library:</st> [<st c="16370">https://</st><st c="16378">contrib.scikit-learn.org/category_encoders/onehot.html</st>](https://contrib.scikit-learn.org/category_encoders/onehot.html)<st
    c="16433">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16434">To limit the number of binary variables, we can choose which categories
    to encode and which to ignore; check out a Python demo in the following</st> <st
    c="16579">article:</st> <st c="16588">https://www.blog.trainindata.com/one-hot-encoding-</st><st
    c="16638">categorical-variables</st><st c="16660">/</st><st c="16662">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16663">Performing one-hot encoding of frequent categories</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="16714">One-hot encoding</st> <st c="16732">represents each variable’s
    category with a binary variable.</st> <st c="16791">Hence, one-hot encoding of
    highly cardinal variables or datasets with multiple categorical features can expand
    the feature space dramatically.</st> <st c="16935">This, in turn, may increase
    the computational cost of using machine learning models or deteriorate their performance.</st>
    <st c="17053">To reduce the number of binary variables, we can perform one-hot
    encoding of the most frequent categories.</st> <st c="17160">One-hot encoding</st>
    <st c="17177">the top categories is equivalent to treating the remaining, less
    frequent categories as a single,</st> <st c="17275">unique category.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17291">In this recipe, we will implement one-hot encoding of the most
    popular categories using</st> `<st c="17380">pandas</st>`<st c="17386">,</st>
    `<st c="17388">Scikit-l</st><st c="17396">earn</st>`<st c="17401">,</st> <st c="17403">and</st>
    `<st c="17407">feature-engine</st>`<st c="17421">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17422">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="17438">First, let’s import</st> <st c="17459">the necessary</st> <st
    c="17473">Python libraries and get the</st> <st c="17502">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17516">Import the required Python libraries, functions,</st> <st c="17566">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17670">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="17739">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17935">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17940">The most frequent categories need to be determined in the train
    set.</st> <st c="18010">This is to avoid</st> <st c="18027">data leakage.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18040">Let’s inspect the unique categories of the</st> `<st c="18084">A6</st>`
    <st c="18086">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18119">The unique values of</st> `<st c="18141">A6</st>` <st c="18143">are
    displayed in the</st> <st c="18165">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train["A6"].value_counts().sort_values(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ascending=False).head(5)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18610">A6</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18613">c      93</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18618">q      56</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18623">w      48</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18628">i      41</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18633">ff     38</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<st c="18717">A6</st> in a list by using the code in *<st c="18751">Step 4</st>*
    inside a list comprehension:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*****   <st c="18883">Let’s add a binary variable per top category to a copy
    of the train and</st> <st c="18956">test sets:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19178">Let’s display</st> <st c="19192">the top</st> `<st c="19201">10</st>`
    <st c="19203">rows</st> <st c="19209">of the original and encoded variable,</st>
    `<st c="19247">A6</st>`<st c="19249">, in the</st> <st c="19258">train set:</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19335">In the output of</st> *<st c="19353">Step 7</st>*<st c="19359">,
    we can see the</st> `<st c="19376">A6</st>` <st c="19378">variable, followed by
    the</st> <st c="19405">binary variables:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19690">Let’s import</st> <st c="19704">the encoder:</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19764">Let’s set</st> <st c="19775">up the encoder to encode categories
    shown in at least</st> `<st c="19829">39</st>` <st c="19831">observations and
    limit the number of categories to encode</st> <st c="19890">to</st> `<st c="19893">5</st>`<st
    c="19894">:</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20010">Finally, let’s</st> <st c="20025">fit the transformer to the two
    high cardinal variables and then transform</st> <st c="20100">the data:</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20222">If you execute</st> `<st c="20238">X_train_enc.head()</st>` <st
    c="20256">you’ll see the</st> <st c="20272">resulting DataFrame:</st>****
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '****![Figure 2.6 – Transformed DataFrame containing binary variables for those
    categories with at least 39 observations and an additional binary representing
    all remaining categories](img/B22396_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20596">Figure 2.6 – Transformed DataFrame containing binary variables
    for those categories with at least 39 observations and an additional binary representing
    all remaining categories</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20772">To wrap up the recipe, let’s encode the most frequent categories</st>
    <st c="20838">with</st> `<st c="20843">feature-engine</st>`<st c="20857">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20858">Let’s set up</st> <st c="20871">the one-hot encoder to encode
    the five most frequent categories of the</st> `<st c="20943">A6</st>` <st c="20945">and</st>
    `<st c="20950">A7</st>` <st c="20952">variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21081">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21086">The number of frequent categories to encode is arbitrarily dete</st><st
    c="21150">rmined by</st> <st c="21161">the user.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21170">Let’s fit</st> <st c="21180">the encoder to the train set so that
    it learns and stores the most frequent categories of</st> `<st c="21271">A6</st>`
    <st c="21273">and</st> `<st c="21278">A7</st>`<st c="21280">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21303">Finally, let’s encode</st> `<st c="21326">A6</st>` <st c="21328">and</st>
    `<st c="21333">A7</st>` <st c="21335">in the train and</st> <st c="21353">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21443">You can view the new binary variables in the transformed DataFrame
    by executing</st> `<st c="21524">X_train_enc.head()</st>`<st c="21542">. You can
    also find the top five categories learned by the encoder b</st><st c="21610">y</st>
    <st c="21613">executing</st> `<st c="21623">ohe_enc.encoder_dict_</st>`<st c="21644">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21645">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="21661">In the first</st> <st c="21675">part of this recipe, we worked
    with the</st> `<st c="21715">A6</st>` <st c="21717">categorical variable.</st>
    <st c="21740">We inspected its unique categories with</st> `<st c="21780">pandas</st>`<st
    c="21786">’</st> `<st c="21789">unique()</st>`<st c="21797">. Next, we counted
    the number of observations per category using</st> `<st c="21862">pandas</st>`<st
    c="21868">’</st> `<st c="21871">value_counts()</st>`<st c="21885">, which returned
    a</st> `<st c="21904">pandas</st>` <st c="21910">series with the categories as
    the index and the number of observations as values.</st> <st c="21993">Next, we
    sorted the categories from the one with the most to the one with the least observations
    using</st> `<st c="22096">pandas</st>`<st c="22102">’</st> `<st c="22104">sort_values()</st>`<st
    c="22117">. We then reduced the series to the five most popular categories by
    using</st> `<st c="22191">pandas</st>`<st c="22197">’</st> `<st c="22200">head()</st>`<st
    c="22206">. We used this series in a list comprehension to capture the names of
    the most frequent categories.</st> <st c="22306">After that, we looped over each
    category, and with NumPy’s</st> `<st c="22365">where()</st>`<st c="22372">, we
    created binary variables by placing a value of</st> `<st c="22424">1</st>` <st
    c="22425">if the observation showed the category, or</st> `<st c="22469">0</st>`
    <st c="22470">otherwise.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22481">We discussed how to use</st> `<st c="22506">OneHotEncoder()</st>`
    <st c="22521">from</st> `<st c="22527">scikit-learn</st>` <st c="22539">and</st>
    `<st c="22544">feature-engine</st>` <st c="22558">in the</st> *<st c="22566">Creating
    binary variables through one-hot encoding</st>* <st c="22616">recipe.</st> <st
    c="22625">Here, I will only highlight the parameters needed to encode the most</st>
    <st c="22694">frequent categories.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22714">To encode</st> <st c="22724">frequent categories with</st> `<st
    c="22750">scikit-learn</st>`<st c="22762">, we set the</st> `<st c="22775">min_frequency</st>`
    <st c="22788">parameter to</st> `<st c="22802">39</st>`<st c="22804">. Hence,
    categories shown in less than</st> `<st c="22843">39</st>` <st c="22845">observations
    were grouped into an additional binary variable</st> <st c="22907">called</st>
    `<st c="22914">infrequent_sklearn</st>`<st c="22932">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22933">To encode frequent categories with</st> `<st c="22969">feature-engine</st>`<st
    c="22983">, we set the</st> `<st c="22996">top_categories</st>` <st c="23010">parameter
    to</st> `<st c="23024">5</st>`<st c="23025">. Hence, the transformer created binary
    variables for the 5 most frequent categories only.</st> <st c="23116">Less frequent
    categories will s</st><st c="23147">how a</st> `<st c="23154">0</st>` <st c="23155">in
    all the</st> <st c="23167">binary variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23184">There’s more...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="23200">This recipe is based on the winning solution of the</st> **<st
    c="23253">Knowledge Discovery and Data</st>** <st c="23282">(</st>**<st c="23283">KDD</st>**<st
    c="23286">) 2009 mining cup,</st> *<st c="23306">Winning the KDD Cup Orange Challenge
    with Ensemble Selection</st>* <st c="23366">(http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf),
    where the author's limited one-hot encoding to the 10 most f</st><st c="23490">requent
    categories of</st> <st c="23513">each variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23527">Replacing categories with counts or th</st><st c="23566">e frequency
    of observations</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="23594">In count</st> <st c="23604">with counts or frequency of observatio</st><st
    c="23642">ns” or frequency encoding, we replace the categories with the count
    or the fraction of observations showing that category.</st> <st c="23766">That
    is, if 10 out of 100 observations show the</st> `<st c="23814">blue</st>` <st
    c="23818">category for the</st> `<st c="23836">Color</st>` <st c="23841">variable,
    we would replace</st> `<st c="23869">blue</st>` <st c="23873">with</st> `<st c="23879">10</st>`
    <st c="23881">when doing count encoding, or with</st> `<st c="23917">0.1</st>`
    <st c="23920">if performing frequency encoding.</st> <st c="23955">These encoding
    methods are useful when there is a relationship between the category frequency
    and the target.</st> <st c="24065">For example, in sales, the frequency of a product
    may indicate</st> <st c="24128">its popularity.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24143">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24148">If two different categories are present in the same number of
    observations, they will be replaced by the same value, which may lead to</st>
    <st c="24284">information loss.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24301">In this recipe, we will perform count and frequency enco</st><st
    c="24358">ding using</st> `<st c="24370">pandas</st>` <st c="24376">and</st> `<st
    c="24381">feature-engine</st>`<st c="24395">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24396">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="24412">We’ll start by encoding one variable with</st> `<st c="24455">pandas</st>`
    <st c="24461">and then we’ll automate the process</st> <st c="24498">with</st>
    `<st c="24503">feature-engine</st>`<st c="24517">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24519">Let’s start with</st> <st c="24536">the imports:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24679">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="24748">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24944">Let’s</st> <st c="24951">with counts or frequency of observations”
    capture the number of observations per category of the</st> `<st c="25048">A7</st>`
    <st c="25050">variable in</st> <st c="25063">a dictionary:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25124">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25129">To find the frequency instead,</st> <st c="25161">execute</st>
    `<st c="25169">X_train["A7"].value_counts(normalize=True).to_dict()</st>`<st c="25221">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25222">If we execute</st> `<st c="25237">print(counts)</st>`<st c="25250">,
    we’ll see the count of observations per category</st> <st c="25301">of</st> `<st
    c="25304">A7</st>`<st c="25306">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: <st c="25403">Let’s replace the categories in</st> `<st c="25436">A7</st>` <st
    c="25438">with the counts in a copy of the</st> <st c="25472">data sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25636">Go ahead and inspect the data by executing</st> `<st c="25680">X_train_enc.head()</st>`
    <st c="25698">to corroborate that the categories have been replaced by</st> <st
    c="25756">the counts.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="25767">To apply this procedure to multiple variables, we can</st> <st
    c="25822">use</st> `<st c="25826">feature-engine</st>`<st c="25840">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="25841">Let’s set up the encoder so that it encodes all categorical variables
    with the count</st> <st c="25927">of observations:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26021">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="26026">CountFrequencyEncoder()</st>` <st c="26050">will automatically
    find and encode all categorical variables in the train set.</st> <st c="26130">To
    encode only a subset of the variables, pass the variable names in a list to the</st>
    `<st c="26213">variables</st>` <st c="26222">argument.</st> <st c="26233">To encode
    with the frequency instead,</st> <st c="26271">use</st> `<st c="26275">encoding_method="frequency"</st>`<st
    c="26302">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26303">Let’s fit the</st> <st c="26317">encoder to the train set so that
    it stores the number of observations per category</st> <st c="26401">per variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26437">The encoder found the categorical variables automatically.</st>
    <st c="26497">Let’s check</st> <st c="26509">them out:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26539">The previous command returns the names of the categorical variables
    in the</st> <st c="26615">train set:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26683">Let’s print the count of observations per category</st> <st c="26735">per
    variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26772">The previous attribute stores the mappings that will be used to
    replace</st> <st c="26845">the categories:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Dictionary containing the number of observations per category,
    for each variable; these values will be used to encode the categorical variables](img/B22396_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27366">Figure 2.7 – Dictionary containing the number of observations
    per category, for each variable; these values will be used to encode the categorical
    variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27522">Finally, let’s</st> <st c="27538">with counts or frequency of
    observations” replace the categories with counts in the train and</st> <st c="27632">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27726">Check out the result by executing</st> `<st c="27761">X_train_enc.head()</st>`<st
    c="27779">. The encoder returns</st> `<st c="27801">pandas</st>` <st c="27807">DataFrames
    with the strings of the categorical variables replaced with the counts of observations,
    leaving the variables r</st><st c="27930">eady to use in machine</st> <st c="27954">learning
    models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27970">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="27986">In this recipe, we replaced categories with the count of observations
    using</st> `<st c="28063">pandas</st>` <st c="28069">and</st> `<st c="28074">feature-engine</st>`<st
    c="28088">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28089">Using</st> `<st c="28096">pandas</st>`<st c="28102">’</st> `<st
    c="28105">value_counts()</st>`<st c="28119">, we determined the number of observations
    per category of the</st> `<st c="28182">A7</st>` <st c="28184">variable, and with</st>
    `<st c="28204">pandas</st>`<st c="28210">’</st> `<st c="28213">to_dict()</st>`<st
    c="28222">, we captured these values in a</st> <st c="28254">with counts or frequency
    of observations” dictionary, where each key was a unique category, and each value
    the number of observations for that category.</st> <st c="28407">With</st> `<st
    c="28412">pandas</st>`<st c="28418">’</st> `<st c="28421">map()</st>` <st c="28426">and
    using this dictionary, we replaced the categories with the observation counts
    in both the train and</st> <st c="28531">test sets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28541">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28546">The count of observations for the encoding should be obtained
    from the train set to avoid data leakage.</st> <st c="28651">Note that new categories
    in the test set will not have a corresponding mapping and hence will be replaced
    by</st> `<st c="28760">nan</st>`<st c="28763">. To avoid this, use f</st>`<st
    c="28785">eature-engine</st>`<st c="28799">. Alternatively, you can replace the</st>
    `<st c="28836">nan</st>` <st c="28839">with</st> `<st c="28845">0</st>`<st c="28846">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28847">To perform count encoding with</st> `<st c="28879">feature-engine</st>`<st
    c="28893">, we used</st> `<st c="28903">CountFrequencyEncoder()</st>` <st c="28926">and
    set</st> `<st c="28935">encoding_method</st>` <st c="28950">to</st> `<st c="28954">'count'</st>`<st
    c="28961">. We left the</st> `<st c="28975">variables</st>` <st c="28984">argument
    set to</st> `<st c="29001">None</st>` <st c="29005">so that the encoder automatically
    finds all the categorical variables in the dataset.</st> <st c="29092">With</st>
    `<st c="29097">fit()</st>`<st c="29102">, the transformer found the categorical
    variables and stored the observation counts per category in the</st> `<st c="29206">encoder_dict_</st>`
    <st c="29219">attribute.</st> <st c="29231">With</st> `<st c="29236">transform()</st>`<st
    c="29247">, the transformer replaced the categories with the counts, returning
    a</st> `<st c="29318">pandas</st>` <st c="29324">DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29335">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29340">If there are categories in the test set that were not present
    in the train set, the encoder will raise an error by default.</st> <st c="29465">You
    can make it ignore them, in which case they will appear as</st> `<st c="29528">nan</st>`<st
    c="29531">, or encode them</st> <st c="29548">as</st> `<st c="29551">0</st>`<st
    c="29552">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29553">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="29562">You can also carry out count and frequency encoding with the Python
    library Category</st> <st c="29648">Encoders:</st> [<st c="29658">https://contrib.scikit-learn.org/category_encoders/count.html</st>](https://contrib.scikit-learn.org/category_encoders/count.html)<st
    c="29719">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29720">For some useful applications of count encoding, check out this</st>
    <st c="29784">article:</st> [<st c="29793">https://</st><st c="29801">letsdatascience.com/frequency-encoding/</st>](https://letsdatascience.com/frequency-encoding/)<st
    c="29841">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29842">Replacing categories with ordinal numbers</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="29884">Ordinal encoding</st> <st c="29902">consists of replacing the
    categories with digits from</st> *<st c="29956">1</st>* <st c="29957">to</st>
    *<st c="29961">k</st>* <st c="29962">(or</st> *<st c="29967">0</st>* <st c="29968">to</st>
    *<st c="29972">k-1</st>*<st c="29975">, depending on the implementation), where</st>
    *<st c="30017">k</st>* <st c="30018">is the number of distinct categories of the
    variable.</st> <st c="30073">The numbers are assigned arbitrarily.</st> <st c="30111">Ordinal
    encoding is better suited for non-linear machine learning models, which can navigate
    through arbitrarily assigned numbers to find patterns that relate to</st> <st
    c="30273">the target.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30284">In this recipe, we will</st> <st c="30309">perform ordinal encoding
    using</st> `<st c="30339">pandas</st>`<st c="30346">,</st> `<st c="30348">scikit-learn</st>`<st
    c="30360">,</st> <st c="30362">and</st> `<st c="30366">feature-engine</st>`<st
    c="30380">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30381">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30397">First, let’s make the import and prepare</st> <st c="30439">the
    dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30451">Import</st> `<st c="30459">pandas</st>` <st c="30465">and the
    data</st> <st c="30479">split function:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30567">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="30636">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30832">To</st> <st c="30836">encode the</st> `<st c="30847">A7</st>`
    <st c="30849">variable, let’s make a dictionary of</st> <st c="30887">category-to-integer
    pairs:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30989">If we execute</st> `<st c="31004">print(ordinal_mapping)</st>`<st
    c="31026">, we will see the digits that will replace</st> <st c="31069">each category:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31173">Now, let’s</st> <st c="31185">replace the categories in a copy
    of</st> <st c="31221">the DataFrames:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31408">Go ahead and execute</st> `<st c="31430">print(X_train["A7"].head())</st>`
    <st c="31457">to see the result of the</st> <st c="31483">previous operation.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="31502">Next, we’ll carry out ordinal encoding</st> <st c="31542">using</st>
    `<st c="31548">scikit-learn</st>`<st c="31560">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="31561">Let’s import the</st> <st c="31579">required classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31691">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31696">Do not confuse</st> `<st c="31712">OrdinalEncoder()</st>` <st
    c="31728">with</st> `<st c="31734">LabelEncoder()</st>` <st c="31748">from</st>
    `<st c="31754">scikit-learn</st>`<st c="31766">. The former is intended to encode
    predictive features, whereas the latter is intended to modify the</st> <st c="31867">target
    variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31883">Let’s set</st> <st c="31893">up</st> <st c="31897">the encoder:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31932">Let’s make a list containing the categorical variables</st> <st
    c="31988">to encode:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32062">Let’s restrict</st> <st c="32078">the encoding to the</st> <st
    c="32098">categorical variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32264">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32269">Remember to set</st> `<st c="32286">remainder</st>` <st c="32295">to</st>
    `<st c="32299">"passthrough"</st>` <st c="32312">to make the</st> `<st c="32325">ColumnTransformer()</st>`
    <st c="32344">return the un-transformed variables</st> <st c="32381">as well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32389">Let’s fit the encoder to the train set so that it creates and
    stores representations of categories</st> <st c="32489">to digits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32515">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32520">By executing</st> `<st c="32534">ct.named_transformers_["encoder"].categories_</st>`<st
    c="32579">, you can visualize the unique categories</st> <st c="32621">per variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32634">Now, let’s encode the categorical variables in the train and</st>
    <st c="32696">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32776">Go ahead and execute</st> `<st c="32798">X_train_enc.head()</st>`
    <st c="32816">to check out the</st> <st c="32834">resulting DataFrame.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="32854">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="32859">ColumnTransformer()</st>` <st c="32879">will mark the encoded
    variables by appending</st> `<st c="32925">encoder</st>` <st c="32932">to the
    variable name.</st> <st c="32955">The variables that were not modified show the</st>
    `<st c="33001">remainder</st>` <st c="33010">prefix.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33018">Now, let’s do</st> <st c="33032">ordinal encoding</st> <st c="33050">with</st>
    `<st c="33055">feature-engine</st>`<st c="33069">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33070">Let’s import</st> <st c="33084">the encoder:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33147">Let’s set up the encoder so that it replaces categories with arbitrary
    integers in the categorical variables specified in</st> *<st c="33270">Step 7</st>*<st
    c="33276">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33351">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="33356">feature-engine</st>`<st c="33371">’s</st> `<st c="33375">OrdinalEncoder()</st>`
    <st c="33391">automatically finds and encodes all categorical variables if the</st>
    `<st c="33457">variables</st>` <st c="33466">parameter is</st> `<st c="33480">None</st>`<st
    c="33484">. Alternatively, it will encode the variables indicated in the list.</st>
    <st c="33553">In addition, it can assign the integers according to the target
    mean value (see the</st> *<st c="33637">Performing ordinal encoding based on the
    target</st>* *<st c="33685">value</st>* <st c="33690">recipe).</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33699">Let’s fit the</st> <st c="33713">encoder to the train set so</st>
    <st c="33742">that it learns and stores the</st> <st c="33772">category-to-integer
    mappings:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33818">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33823">The category to integer mappings are stored in the</st> `<st c="33875">encoder_dict_</st>`
    <st c="33888">attribute and can be accessed by</st> <st c="33922">executing</st>
    `<st c="33932">enc.encoder_dict_</st>`<st c="33949">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33950">Finally, let’s encode the categorical variables in the train and</st>
    <st c="34016">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`<st c="34098">feature-engine</st>` <st c="34113">returns</st> `<st c="34122">pandas</st>`
    <st c="34128">DataFrames where the values of the original variables are replaced
    with numbers, leaving the</st> <st c="34222">DataFrame ready to use in machine</st>
    <st c="34256">learning models.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34272">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="34288">In this recipe, we replaced categories with integers</st> <st
    c="34342">assigned arbitrarily.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34363">We used</st> `<st c="34372">pandas</st>`<st c="34378">’</st> `<st
    c="34381">unique()</st>` <st c="34389">to find the unique categories of the</st>
    `<st c="34427">A7</st>` <st c="34429">variable.</st> <st c="34440">Next, we created
    a dictionary of category-to-integer and passed it to</st> `<st c="34510">pandas</st>`<st
    c="34516">’</st> `<st c="34519">map()</st>` <st c="34524">to replace the strings
    in</st> `<st c="34551">A7</st>` <st c="34553">with</st> <st c="34559">the integers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34572">Next, we carried out ordinal encoding using</st> `<st c="34617">scikit-learn</st>`<st
    c="34629">’s</st> `<st c="34633">OrdinalEncoder()</st>` <st c="34650">and used</st>
    `<st c="34659">ColumnTransformer()</st>` <st c="34678">to restrict the encoding
    to categorical variables.</st> <st c="34730">With</st> `<st c="34735">fit()</st>`<st
    c="34740">, the transformer created the category-to-integer mappings based on
    the categories in the train set.</st> <st c="34841">With</st> `<st c="34846">transform()</st>`<st
    c="34857">, the categories were replaced with integers.</st> <st c="34903">By
    setting the</st> `<st c="34918">remainder</st>` <st c="34927">parameter to</st>
    `<st c="34941">passthrough</st>`<st c="34952">, we made</st> `<st c="34962">ColumnTransformer()</st>`
    <st c="34981">concatenate the variables that are not encoded at the back of the</st>
    <st c="35048">encoded features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35065">To perform ordinal encoding with</st> `<st c="35099">feature-engine</st>`<st
    c="35113">, we used</st> `<st c="35123">OrdinalEncoder()</st>`<st c="35139">,
    indicating that the integers should be assigned arbitrarily through</st> `<st
    c="35209">encoding_method</st>`<st c="35224">, and passed a list with the variables
    to encode in the</st> `<st c="35280">variables</st>` <st c="35289">argument.</st>
    <st c="35300">With</st> `<st c="35305">fit()</st>`<st c="35310">, the encoder
    assigned integers to each variable’s categories, which</st> <st c="35379">were
    stored in the</st> `<st c="35398">encoder_dict_</st>` <st c="35411">attribute.</st>
    <st c="35423">These</st> <st c="35428">mappings were then used by the</st> `<st
    c="35460">transform()</st>` <st c="35471">method to replace the categories in
    the train and test sets,</st> <st c="35533">returning DataFrames.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35554">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35559">When a category in the test set is not present in the training
    set, it will not have a mapping to a digit.</st> `<st c="35667">OrdinalEncoder()</st>`
    <st c="35683">from</st> `<st c="35689">scikit-learn</st>` <st c="35701">and</st>
    `<st c="35706">feature-engine</st>` <st c="35720">will raise an error by default.</st>
    <st c="35753">However, they have the option to replace unseen categories with
    a user-defined value or</st> `<st c="35841">-</st>``<st c="35842">1</st>`<st c="35843">,
    respectively.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="35858">scikit-learn</st>`<st c="35871">’s</st> `<st c="35875">OrdinalEncoder()</st>`
    <st c="35891">can restrict the encoding to those categories with a minimum frequency.</st>
    `<st c="35964">feature-engine</st>`<st c="35978">’s</st> `<st c="35982">OrdinalEncoder()</st>`
    <st c="35998">can assign the numbers based on the target</st> <st c="36041">mean
    value, as we will see in the</st> <st c="36076">following recipe.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36093">There’s more...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="36109">You can also carry out ordinal encoding with</st> `<st c="36155">OrdinalEncoder()</st>`
    <st c="36171">from Category Encoders.</st> <st c="36196">Check it out</st> <st
    c="36209">at</st> [<st c="36212">http://cont</st><st c="36223">rib.scikit-l</st><st
    c="36236">earn.org/category_encoders/ordinal.html</st>](http://contrib.scikit-learn.org/category_encoders/ordinal.html)<st
    c="36276">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36277">Performing ordinal encoding based on the target value</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="36331">In the previous</st> <st c="36347">recipe, we replaced categories
    with integers, which were assigned arbitrarily.</st> <st c="36427">We can also
    assign integers to the categories given the target values.</st> <st c="36498">To
    do this, first, we calculate the mean value of the target per category.</st> <st
    c="36573">Next, we order the categories from the one with the lowest to the one
    with the highest target mean value.</st> <st c="36679">Finally, we assign digits
    to the ordered categories, starting with</st> *<st c="36746">0</st>* <st c="36747">to
    the first category up to</st> *<st c="36776">k-1</st>* <st c="36779">to the last
    category, where</st> *<st c="36808">k</st>* <st c="36809">is the number of</st>
    <st c="36827">distinct categories.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36847">This encoding method creates a monotonic relationship between
    the categorical variable and the response and therefore makes the variables more
    adequate for use in</st> <st c="37011">linear models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37025">In this recipe, we will</st> <st c="37049">encode categories while
    follo</st><st c="37079">wing the target value using</st> `<st c="37108">pandas</st>`
    <st c="37114">and</st> `<st c="37119">feature-engine</st>`<st c="37133">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37134">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="37150">First, let’s import the necessary Python libraries and get the</st>
    <st c="37214">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37228">Import the required Python libraries, functions,</st> <st c="37278">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37395">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="37464">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37660">Let’s determine the mean target value per category in</st> `<st
    c="37715">A7</st>`<st c="37717">, then sort the categories from that with the
    lowest to that with the highest</st> <st c="37795">target value:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37860">The following is the output of the</st> <st c="37896">preceding
    command:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38065">Now, let’s</st> <st c="38077">repeat the computation in</st> *<st
    c="38103">Step 3</st>*<st c="38109">, but this time, let’s retain the ordered</st>
    <st c="38151">category names:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38242">To display the output of the preceding command, we can execute</st>
    `<st c="38306">print(ordered_labels)</st>`<st c="38327">:</st> `<st c="38330">Index(['o',
    'ff', 'j', 'dd', 'v', 'bb', 'h', 'n', 'z', 'Missing'],</st>` `<st c="38397">dtype='object',
    name='A7')</st>`<st c="38423">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="38424">Let’s create a dictionary of category-to-integer pairs, using
    the ordered list we created in</st> *<st c="38518">Step 4</st>*<st c="38524">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38594">We can visualize the result of the preceding code by</st> <st
    c="38648">executing</st> `<st c="38658">print(ordinal_mapping)</st>`<st c="38680">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_enc = X_train.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc = X_test.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_enc["A7"] = X_train_enc["A7"].map(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ordinal_mapping)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc["A7"] = X_test_enc["A7"].map(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ordinal_mapping)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39050">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39055">If the test set contains a category that is not present in the
    train set, the preceding code will</st> <st c="39154">introduce</st> `<st c="39164">np.nan</st>`<st
    c="39170">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39171">To visualize the effect of this encoding, let’s plot the relationship
    of the categories of the</st> `<st c="39267">A7</st>` <st c="39269">variable with
    the target before and after</st> <st c="39312">the encoding.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39325">Let’s plot the mean target response per category of the</st> `<st
    c="39382">A7</st>` <st c="39384">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39531">We can see the non-monotonic relationship between</st> <st c="39581">categories
    of</st> `<st c="39596">A7</st>` <st c="39598">and the target in the</st> <st c="39621">following
    plot:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Mean target value per category of A7 before the encoding](img/B22396_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="39747">Figure 2.8 – Mean target value per category of A7 before the encoding</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39816">Let’s plot</st> <st c="39827">the mean target value per category
    in the</st> <st c="39870">encoded variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40028">The encoded variable shows a monotonic relationship with the target
    – the higher the mean targe</st><st c="40124">t value, the higher the digit assigned
    to</st> <st c="40167">the category:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Mean target value per category of A7 after the encoding.](img/B22396_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="40273">Figure 2.9 – Mean target value per category of A7 after the encoding.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40342">Now, let’s</st> <st c="40354">perform ordered ordinal encoding</st>
    <st c="40387">using</st> `<st c="40393">feature-engine</st>`<st c="40407">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40408">Let’s import</st> <st c="40422">the encoder:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40486">Next, let’s set up the encoder so that it assigns integers based
    on the target mean value to all categorical variables in</st> <st c="40609">the
    dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40694">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="40699">OrdinalEncoder()</st>` <st c="40716">will find and encode all
    categorical variables automatically.</st> <st c="40779">To restrict the encoding
    to a subset of variables, pass their names in a list to the</st> `<st c="40864">variables</st>`
    <st c="40873">argument.</st> <st c="40884">To encode numerical variables,</st>
    <st c="40915">set</st> `<st c="40919">ignore_format=True</st>`<st c="40937">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40938">Let’s fit the encoder to the train set so that it finds the categorical
    variables, and then stores the</st> <st c="41042">category and</st> <st c="41055">integer
    m</st><st c="41064">appings:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="41107">Finally, let’s replace the categories with numbers in the train
    and</st> <st c="41176">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="41274">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41279">You’ll find the digits that will replace each category in the</st>
    `<st c="41342">encoder_dict_</st>` <st c="41355">attribute.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41366">Check out the outp</st><st c="41385">ut of the transformation
    by</st> <st c="41414">executing</st> `<st c="41424">X_train_enc.head()</st>`<st
    c="41442">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41443">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="41459">In this recipe, we replaced the categories with integers according
    to the</st> <st c="41534">target mean.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41546">In the first part of this recipe, we worked with the</st> `<st
    c="41600">A7</st>` <st c="41602">categorical variable.</st> <st c="41625">With</st>
    `<st c="41630">pandas</st>`<st c="41636">’</st> `<st c="41639">groupby()</st>`<st
    c="41648">, we grouped the data based on the categories of</st> `<st c="41697">A7</st>`<st
    c="41699">, and with</st> `<st c="41710">pandas</st>`<st c="41716">’</st> `<st
    c="41719">mean()</st>`<st c="41725">, we determined the mean value of the target
    for each of those categories.</st> <st c="41800">Next, we ordered the categories
    with</st> `<st c="41837">pandas</st>`<st c="41843">’</st> `<st c="41846">sort_values()</st>`
    <st c="41859">from the ones with the lowest to the ones with the highest target
    mean response.</st> <st c="41941">The output of this operation was a</st> `<st
    c="41976">pandas</st>` <st c="41982">series, with the categories as indices and
    the target mean as values.</st> <st c="42053">With</st> `<st c="42058">pandas</st>`<st
    c="42064">’</st> `<st c="42067">index</st>`<st c="42072">, we captured the ordered
    categories in an array; then, with Python dictionary comprehension, we created
    a dictionary of category-to-integer pairs.</st> <st c="42220">Finally, we used
    this dictionary to replace the category with integers using</st> `<st c="42297">pandas</st>`<st
    c="42303">’</st> `<st c="42306">map()</st>`<st c="42311">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42312">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42317">To avoid data leakage, we determine the ca</st><st c="42360">tegory-to-integer
    mappings from the</st> <st c="42397">train set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42407">To perform the encoding with</st> `<st c="42437">feature-engine</st>`<st
    c="42451">, we used</st> `<st c="42461">OrdinalEncoder()</st>`<st c="42477">,
    setting the</st> `<st c="42491">encoding_method</st>` <st c="42506">to</st> `<st
    c="42510">ordered</st>`<st c="42517">. We left the argument variables set to</st>
    `<st c="42557">None</st>` <st c="42561">so that the encoder automatically detects
    all categorical variables in the dataset.</st> <st c="42646">With</st> `<st c="42651">fit()</st>`<st
    c="42656">, the encoder</st> <st c="42669">found the categorical variables and
    assigned digits to their categories according to the target mean value.</st> <st
    c="42778">The categorical variables’ names and dictionaries with category-to-digit
    pairs were stored in the</st> `<st c="42876">variables_</st>` <st c="42886">and</st>
    `<st c="42891">encoder_dict_</st>` <st c="42904">attributes, respectively.</st>
    <st c="42931">Finally, using</st> `<st c="42946">transform()</st>`<st c="42957">,
    we replaced the categories with digit</st><st c="42996">s in the train and test
    sets, returning</st> `<st c="43037">pandas</st>` <st c="43043">DataFrames.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43055">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43064">For an implementation of this recipe with Category Encoders, visit
    this book’s GitHub</st> <st c="43151">repository:</st> [<st c="43163">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-cate</st><st
    c="43267">gorical-encoding/Recipe-05-Ordered-ordinal-encod</st><st c="43316">ing.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-05-Ordered-ordinal-encoding.ipynb)<st
    c="43326">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43327">Implementing target mean encoding</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="43361">Mean encoding</st>** <st c="43375">or</st> **<st c="43379">target
    encoding</st>** <st c="43394">maps</st> <st c="43400">each category to the probability
    estimate of the target attribute.</st> <st c="43467">If the target is binary,
    the numerical mapping is the posterior probability of the target conditioned to
    the value of the category.</st> <st c="43599">If the target is continuous, the
    numerical representation is given by the expected value of the target given the
    value of</st> <st c="43721">the category.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43734">In its simplest form, the</st> <st c="43760">numerical representation
    for each category is given by the mean value of the target variable for a particular
    category group.</st> <st c="43887">For example, if we have a</st> `<st c="43913">City</st>`
    <st c="43917">variable, with the categories of</st> `<st c="43951">London</st>`<st
    c="43957">,</st> `<st c="43959">Manchester</st>`<st c="43969">, and</st> `<st
    c="43975">Bristol</st>`<st c="43982">, and we want to predict the default rate
    (the target takes values of</st> `<st c="44052">0</st>` <st c="44053">and</st>
    `<st c="44058">1</st>`<st c="44059">); if the default rate for</st> `<st c="44086">London</st>`
    <st c="44092">is 30%, we replace</st> `<st c="44112">London</st>` <st c="44118">with</st>
    `<st c="44124">0.3</st>`<st c="44127">; if the default rate for</st> `<st c="44154">Manchester</st>`
    <st c="44164">is 20%, we replace</st> `<st c="44184">Manchester</st>` <st c="44194">with</st>
    `<st c="44200">0.2</st>`<st c="44203">; and so on.</st> <st c="44217">If the target
    is continuous – say we want to predict income – then we would replace</st> `<st
    c="44301">London</st>`<st c="44307">,</st> `<st c="44309">Manchester</st>`<st
    c="44319">, and</st> `<st c="44325">Bristol</st>` <st c="44332">with the mean
    income earned in</st> <st c="44364">each city.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44374">In mathematical terms, if the target is binary, the replacement
    value,</st> *<st c="44446">S</st>*<st c="44447">, is determined</st> <st c="44463">like
    so:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="44480">Here, the numerator is the number of observations with a target
    value of</st> *<st c="44553">1</st>* <st c="44554">for category</st> *<st c="44568">i</st>*
    <st c="44569">and the denominator is the number of observations with a category
    value</st> <st c="44642">of</st> *<st c="44645">i</st>*<st c="44646">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44647">If the target is continuous,</st> *<st c="44677">S</st>*<st c="44678">,
    this is determined by the</st> <st c="44706">following formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>∑</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><msub><mi>n</mi><mi>i</mi></msub></mfrac></mrow></mrow></math>](img/2.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="44764">Here, the</st> <st c="44773">numerator is the sum of the target
    across observations in category</st> *<st c="44841">i</st>* <st c="44842">and</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/3.png)
    <st c="44847"><st c="44848">is the total number of observations in</st> <st c="44888">category</st>
    *<st c="44897">i</st>*<st c="44898">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44899">These formulas provide a good approximation of the target estimate
    if there is a sufficiently large number of observations with each category value
    – in other words, if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/4.png)
    <st c="45069"><st c="45080">is large.</st> <st c="45090">However, in many datasets,
    there will be categories present in a few observations.</st> <st c="45173">In
    these cases, target estimates derived from the precedent formulas can</st> <st
    c="45246">be unreliable.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="45260">To mitigate poor estimates returned for rare categories, the
    target estimates can be determined as a mixture of two probabilities: those returned
    by the preceding formulas and the prior probability of the target based on the
    entire training.</st> <st c="45503">The two probabilities are</st> *<st c="45529">blended</st>*
    <st c="45536">using a weighting factor, which is a function of the category</st>
    <st c="45599">group size:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mi>λ</mi><mfrac><msub><mi>n</mi><mrow><mi>i</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow></msub><msub><mi>n</mi><mi>i</mi></msub></mfrac><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>)</mo><mfrac><msub><mi>n</mi><mi>λ</mi></msub><mi>N</mi></mfrac></mrow></mrow></mrow></math>](img/5.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="45633">In this formula,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">λ</mml:mi></mml:mrow></mml:msub></mml:math>](img/6.png) <st
    c="45650"><st c="45675">is the total number of cases where the target takes a
    value of</st> *<st c="45738">1</st>*<st c="45739">,</st> *<st c="45741">N</st>*
    <st c="45742">is the size of the train set, and</st> *<st c="45777">𝜆</st>* <st
    c="45779">is the</st> <st c="45787">weighting factor.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45804">When the category group is large,</st> *<st c="45839">𝜆</st>*
    <st c="45841">tends to</st> *<st c="45851">1</st>*<st c="45852">, so more weight
    is given to the first term of the equation.</st> <st c="45913">When the category</st>
    <st c="45931">group size is small, then</st> *<st c="45957">𝜆</st>* <st c="45959">tends
    to</st> *<st c="45969">0</st>*<st c="45970">, so the estimate is mostly driven
    by the second term of the equation – that is, the target’s prior probability.</st>
    <st c="46083">In other words, if the group size is small, knowing the value of
    the category does not tell us anything about the value of</st> <st c="46206">the
    target.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46217">The weighting factor,</st> *<st c="46240">𝜆</st>*<st c="46242">,
    is determined differently in different open-source implementations.</st> <st c="46312">In
    Category Encoders,</st> *<st c="46334">𝜆</st>* <st c="46336">is a function of
    the group size,</st> *<st c="46370">k</st>*<st c="46371">, and a smoothing parameter,</st>
    *<st c="46400">f</st>*<st c="46401">, which controls the rate of transition between
    the first</st> <st c="46459">and second term of the</st> <st c="46482">preceding
    equation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>λ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mrow><mrow><mo>−</mo><mo>(</mo><mi>n</mi><mo>−</mo><mi>k</mi><mo>)</mo></mrow></mrow><mo>/</mo><mi>f</mi></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/7.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="46520">Here,</st> *<st c="46526">k</st>* <st c="46527">is half of the
    minimal size for which we</st> *<st c="46569">fully trust</st>* <st c="46580">the
    first term of the equation.</st> <st c="46613">The</st> *<st c="46617">f</st>*
    <st c="46618">parameter is selected by the user either arbitrarily or</st> <st
    c="46675">with optimization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46693">In</st> `<st c="46697">scikit-learn</st>` <st c="46709">and</st>
    `<st c="46714">feature-engine</st>`<st c="46728">,</st> *<st c="46730">𝜆</st>*
    <st c="46732">is a function of the target variance for the entire dataset and
    within the category, and is determined</st> <st c="46836">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="normal">λ</mi><mo>=</mo><mfrac><mrow><mi>n</mi><mi>i</mi><mo>×</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo>+</mo><mi>n</mi><mi>i</mi><mo>×</mo><mi>t</mi></mrow></mfrac></mrow></mrow></math>](img/8.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="46861">Here,</st> *<st c="46867">t</st>* <st c="46868">is the target</st>
    <st c="46882">variance in the entire dataset and</st> *<st c="46918">s</st>* <st
    c="46919">is the target variance within the category.</st> <st c="46964">Both
    implementations are equivalent, but it is important to know the equations because
    they will help you set up the parameters in</st> <st c="47095">the transformers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47112">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="47117">Mean encoding was designed to encode highly cardinal categorical
    variables without expanding the feature space.</st> <st c="47230">For more details,
    check out the following article: Micci-Barreca D.</st> <st c="47298">A.,</st>
    *<st c="47302">Preprocessing Scheme for High-Cardinality Categorical Attributes
    in Classification and Prediction Problems</st>*<st c="47408">. ACM SIGKDD Explorations</st>
    <st c="47434">Newsletter, 2001.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47451">In this recipe, we will perf</st><st c="47480">orm mean encoding
    using</st> `<st c="47505">scikit-learn</st>` <st c="47517">and</st> `<st c="47522">feature-engine</st>`<st
    c="47536">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47537">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="47553">Let’s begin with</st> <st c="47571">this recipe:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47583">Import</st> `<st c="47591">pandas</st>` <st c="47597">and the
    data</st> <st c="47611">split function:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47699">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="47768">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47964">Let’s import the transformers</st> <st c="47995">from</st> `<st
    c="48000">scikit-learn</st>`<st c="48012">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48108">Let’s</st> <st c="48115">make a list with the names of the</st>
    <st c="48149">categorical variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48236">Let’s set up the encoder to use the target variance to determine
    the weighting factor, as described at the beginning of</st> <st c="48357">the
    recipe:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48419">Let’s restrict the imputation to</st> <st c="48453">categorical
    variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48587">Let’s fit the encoder and transform</st> <st c="48624">the datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48720">Check out the result by</st> <st c="48745">executing</st> `<st
    c="48755">X_train_enc.head()</st>`<st c="48773">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="48774">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48779">The</st> `<st c="48784">fit_transform()</st>` <st c="48799">method
    of</st> `<st c="48810">scikit-learn</st>`<st c="48822">’s</st> `<st c="48826">TargetEncoder()</st>`
    <st c="48841">is not equivalent to applying</st> `<st c="48872">fit().transform()</st>`<st
    c="48889">. With</st> `<st c="48896">fit_transform()</st>`<st c="48911">, the
    resulting dataset is encoded based on partial fits over the training folds of
    a cross-validation scheme.</st> <st c="49022">This functionality was intentionally
    designed to prevent overfitting the machine learning model to the</st> <st c="49125">train
    set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49135">Now, let’s perform target encoding</st> <st c="49171">with</st>
    `<st c="49176">feature-engine</st>`<st c="49190">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49192">Let’s import</st> <st c="49204">the encoder:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49265">Let’s set up the target mean encoder to encode all categorical
    variables while</st> <st c="49345">applying smoothing:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49421">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="49426">MeanEncoder()</st>` <st c="49440">does not apply smoothing by
    default.</st> <st c="49478">Make sure you set it to</st> `<st c="49502">auto</st>`
    <st c="49506">or to an integer to control the blend b</st><st c="49546">etween
    prior and posterior</st> <st c="49574">target estimates.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49591">Let’s fit the transformer to the train set so that it learns and
    stores the mean target value per category</st> <st c="49699">per variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49743">Finally, let’s encode the train and</st> <st c="49780">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49872">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49877">The category-to-number pairs are stored as a dictionary of dictionaries
    in the</st> `<st c="49957">encoder_dict_</st>` <st c="49970">attribute.</st> <st
    c="49982">To disp</st><st c="49989">lay the stored parameters,</st> <st c="50017">execute</st>
    `<st c="50025">mean_enc.encoder_dict_.</st>`
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50048">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50062">In this recipe, we</st> <st c="50081">replaced the categories
    with the mean target value using</st> `<st c="50139">scikit-learn</st>` <st c="50151">and</st>
    `<st c="50156">feature-engine</st>`<st c="50170">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50171">To encode with</st> `<st c="50187">scikit-learn</st>`<st c="50199">,
    we used</st> `<st c="50209">TargetEncoder()</st>`<st c="50224">, leaving the</st>
    `<st c="50238">smooth</st>` <st c="50244">parameter to its default value of</st>
    `<st c="50279">auto</st>`<st c="50283">. Like this, the transformer used the target
    variance to determine the weighting factor for the blend of probabilities.</st>
    <st c="50403">With</st> `<st c="50408">fit()</st>`<st c="50413">, the transformer
    learned the value it should use to replace the categories, and with</st> `<st
    c="50499">transform()</st>`<st c="50510">, it replaced</st> <st c="50524">the
    categories.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50539">Note that for</st> `<st c="50554">TargetEncoder()</st>`<st c="50569">,
    the</st> `<st c="50575">fit()</st>` <st c="50580">method followed by</st> `<st
    c="50600">transform()</st>` <st c="50611">do not return the same dataset as the</st>
    `<st c="50650">fit_transform()</st>`<st c="50665">method.</st> <st c="50674">The
    latter encodes the training set based on mappings found with cross-validation.</st>
    <st c="50757">The idea is to use</st> `<st c="50776">fit_transform()</st>` <st
    c="50791">within a pipeline, so the machine learning model does not overfit.</st>
    <st c="50859">However, and here is where it gets confusing, the mappings stored
    in the</st> `<st c="50932">encodings_</st>` <st c="50942">attribute are the same
    after</st> `<st c="50972">fit()</st>` <st c="50977">and</st> `<st c="50982">fit_transform()</st>`<st
    c="50997">, and this is done intentionally so that when we apply</st> `<st c="51052">transform()</st>`
    <st c="51063">to a new dataset, we obtain the same result regardless of whether
    we apply</st> `<st c="51139">fit()</st>` <st c="51144">or</st> `<st c="51148">fit_transform()</st>`<st
    c="51163">to the</st> <st c="51171">training set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51184">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51189">Unseen categories are encoded with the target mean by</st> `<st
    c="51244">scikit-learn</st>`<st c="51256">’s</st> `<st c="51260">TargetEncoder()</st>`<st
    c="51275">.</st> `<st c="51277">feature-engine</st>`<st c="51291">’s</st> `<st
    c="51295">MeanEncoder()</st>` <st c="51308">can either return an error, replace
    the unseen categories with</st> `<st c="51372">nan</st>`<st c="51375">, or with
    the</st> <st c="51389">target mean.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51401">To perform the target encoding with</st> `<st c="51438">feature-engine</st>`<st
    c="51452">, we used</st> `<st c="51462">MeanEncoder(),</st>` <st c="51476">setting
    the</st> `<st c="51489">smoothing</st>` <st c="51498">parameter to</st> `<st c="51512">auto</st>`<st
    c="51516">. With</st> `<st c="51523">fit()</st>`<st c="51528">, the transformer
    found and stored the categorical variables and the values to encode each category.</st>
    <st c="51629">With</st> `<st c="51634">transform()</st>`<st c="51645">, it replace</st><st
    c="51657">d the categories with numbers, returning</st> `<st c="51699">pandas</st>`
    <st c="51705">DataFrames.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51717">There’s more…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="51731">If you want to implement target encoding with</st> `<st c="51778">pandas</st>`
    <st c="51784">or Category Encoders, check out the notebook in the accompanying
    GitHub</st> <st c="51857">repository:</st> [<st c="51869">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb)<st
    c="52026">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52027">There is an alternative way to return</st> *<st c="52066">better</st>*
    <st c="52072">target estimates when the category groups are small.</st> <st c="52126">The
    replacement valu</st><st c="52146">e for each category is determined</st> <st
    c="52181">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>n</mi><mrow><mi>i</mi><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow></mfenced></mrow></msub><mo>+</mo><mi>p</mi><mi>Y</mi><mi>x</mi><mi>m</mi></mrow><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mi>m</mi></mrow></mfrac></mrow></mrow></math>](img/9.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="52214">Here,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>n</mi><mrow><mi>i</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow></msub></mrow></math>](img/10.png)<st
    c="52220"><st c="52227">is the target mean for category</st> *<st c="52259">i</st>*
    <st c="52260">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/11.png)
    <st c="52265"><st c="52266">is the number of observations with category</st> *<st
    c="52311">i</st>*<st c="52312">. The</st> <st c="52318">target prior is given
    by</st> *<st c="52343">pY</st>* <st c="52345">and</st> *<st c="52350">m</st>*
    <st c="52351">is the weighting factor.</st> <st c="52377">With this adjustment,
    the only parameter that we have</st> <st c="52431">to set is the weight,</st>
    *<st c="52453">m</st>*<st c="52454">. If</st> *<st c="52459">m</st>* <st c="52460">is
    large, then more importance is given to the target’s prior probability.</st> <st
    c="52536">This adjustment affects target estimates for all categories but mostly
    for those with fewer observations because, in such cases,</st> *<st c="52665">m</st>*
    <st c="52666">could be much larger than</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/12.png)
    <st c="52693"><st c="52694">in the</st> <st c="52702">formula’s denominator.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52724">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52729">This method is a good alternative to Category Encoders’</st> `<st
    c="52786">TargetEncoder()</st>` <st c="52801">because, in Category Encoders’ implementation
    of target encoding, we need to optimize two parameters instead of one (as we did
    with</st> `<st c="52934">feature-engine</st>` <st c="52948">and</st> `<st c="52953">scikit-learn</st>`<st
    c="52965">) to control</st> <st c="52979">the smoothing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52993">For an implementation of this encoding method using</st> `<st
    c="53046">MEstimateEncoder()</st>`<st c="53064">, visit this book’s GitHub</st>
    <st c="53091">repository:</st> [<st c="53103">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb)<st
    c="53260">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53261">E</st><st c="53263">ncoding with Weight of E</st><st c="53287">vidence</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="53295">Weight of Evidence</st>** <st c="53314">(</st>**<st c="53316">WoE</st>**<st
    c="53319">) was</st> <st c="53325">developed primarily for credit and financial
    industries to facilitate variable screening and exploratory</st> <st c="53431">analysis
    and to build more predictive linear models to evaluate the risk of</st> <st c="53507">loan
    defaults.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53521">The WoE is computed from the basic</st> <st c="53557">odds ratio:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>WoE</mi><mo>=</mo><mi>log</mi><mrow><mrow><mo>(</mo><mfrac><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/13.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="53632">Here, positive and negative refer to the values of the target
    being</st> *<st c="53700">1</st>* <st c="53701">or</st> *<st c="53705">0</st>*<st
    c="53706">, respectively.</st> <st c="53722">The proportion of positive cases
    per category is determined as the sum of positive cases per category group divided
    by the total positive cases in the training set.</st> <st c="53887">The proportion
    of negative</st> <st c="53914">cases per category is determined as the sum of
    negative cases per category group divided by the total number of negative observations
    in the</st> <st c="54055">training set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54068">WoE has the</st> <st c="54081">following characteristics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54107">WoE =</st> *<st c="54114">0</st>* <st c="54115">if</st> *<st c="54119">p(positive)</st>*
    <st c="54130">/</st> *<st c="54133">p(negative)</st>* <st c="54144">=</st> *<st
    c="54147">1</st>*<st c="54148">; that is, if the outcome</st> <st c="54174">is
    random</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="54183">WoE ></st> *<st c="54190">0</st>* <st c="54191">if</st> *<st c="54195">p(positive)</st>*
    <st c="54206">></st> *<st c="54209">p(negative)</st>*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="54220">WoE <</st> *<st c="54227">0</st>* <st c="54228">if</st> *<st c="54232">p(negative)</st>*
    <st c="54243">></st> *<st c="54246">p(positive)</st>*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="54257">This allows us to directly visualize the predictive power of
    the category in the variable: the higher the WoE, the more likely the event will
    occur.</st> <st c="54407">If the WoE is positive, the event is likely</st> <st
    c="54451">to occur.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54460">Logistic regression models a binary response,</st> *<st c="54507">Y</st>*<st
    c="54508">, based on</st> *<st c="54519">X</st>* <st c="54520">predictor variables,
    assuming that there is a linear relationship between</st> *<st c="54595">X</st>*
    <st c="54596">and the log of odds</st> <st c="54617">of</st> *<st c="54620">Y</st>*<st
    c="54621">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mfenced
    open="(" close=")"><mfrac><mrow><mi>p</mi><mfenced open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow></mfenced></mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mn>0</mn></mrow></mfenced></mrow></mfrac></mfenced><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>b</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub></mrow></mrow></math>](img/14.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="54655">Here,</st> *<st c="54661">log (p(Y=1)/p(Y=0))</st>* <st c="54680">is
    the log of odds.</st> <st c="54701">As you can see, the WoE encodes the categories
    in the same scale – that is, the log of odds – as the outcome of the</st> <st
    c="54817">logistic regression.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54837">Therefore, by using WoE, the predictors are prepared and coded
    on the same scale, and the parameters in the logistic regression model – that
    is, the coefficients – can be</st> <st c="55009">directly compared.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55027">In this recipe</st><st c="55042">, we will perform WoE encoding
    using</st> `<st c="55079">pandas</st>` <st c="55085">and</st> `<st c="55090">feature-engine</st>`<st
    c="55104">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55105">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="55121">Let’s begin by making some imports and preparing</st> <st c="55171">the
    data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55180">Import the required libraries</st> <st c="55211">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="55317">Let’s load</st> <st c="55329">the Credit Approval dataset and
    divide it into train and</st> <st c="55386">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="55582">Let’s get the inverse of the target values to be able to calculate
    the</st> <st c="55654">negative cases:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="55746">Let’s determine the number of observations where the target variable
    takes a value of</st> `<st c="55833">1</st>` <st c="55834">or</st> `<st c="55837">0</st>`<st
    c="55838">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="55896">Now, let’s calculate the numerator and denominator of the WoE’s
    formula, which we discussed earlier in</st> <st c="56000">this recipe:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56128">Now, let’s</st> <st c="56139">calculate the WoE</st> <st c="56158">per
    category:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56193">We can display the series with the category to WoE pairs by</st>
    <st c="56254">executing</st> `<st c="56264">print(woe)</st>`<st c="56274">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_enc = X_train.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc = X_test.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_enc["A1"] = X_train_enc["A1"].map(woe)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc["A1"] = X_test_enc["A1"].map(woe)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56696">Let’s import</st> <st c="56710">the encoder:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56769">Next, let’s set up the encoder to encode three</st> <st c="56817">categorical
    variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56893">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56898">For rare categories, it might happen that</st> `<st c="56941">p(0)=0</st>`
    <st c="56947">or</st> `<st c="56951">p(1)=0</st>`<st c="56957">, and then the
    division or the logarithm is not defined.</st> <st c="57014">To avoid this, group
    infrequent categories as shown in the</st> *<st c="57073">Grouping rare or infrequent</st>*
    *<st c="57101">categories</st>* <st c="57111">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57119">Let’s fit the</st> <st c="57133">transformer to the train set
    so that it learns and stores the WoE of the</st> <st c="57207">different categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="57258">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57263">We can display the dictionaries with the categori</st><st c="57313">es
    to WoE pairs by</st> <st c="57333">executing</st> `<st c="57343">woe_enc.encoder_dict_</st>`<st
    c="57364">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57365">Finally, let’s encode the three categorical variables in the train
    and</st> <st c="57437">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`<st c="57527">feature-engine</st>` <st c="57542">returns</st> `<st c="57551">pandas</st>`
    <st c="57557">DataFrames, which contain the enco</st><st c="57592">ded categorical
    variables ready to use in machine</st> <st c="57643">learning models.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57659">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57675">In this recipe, we encoded categorical variables using the WoE
    with</st> `<st c="57744">pandas</st>` <st c="57750">and</st> `<st c="57755">feature-engine</st>`<st
    c="57769">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57770">We combined the use of</st> `<st c="57794">pandas</st>`<st c="57800">’</st>
    `<st c="57803">sum()</st>` <st c="57808">and</st> `<st c="57813">groupby()</st>`
    <st c="57822">and</st> `<st c="57827">numpy</st>`<st c="57832">’s</st> `<st c="57836">log()</st>`
    <st c="57841">to determine the WoE as we described at the beginning of</st> <st
    c="57899">this recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57911">Next, we automated the procedure with</st> `<st c="57950">feature-engine</st>`<st
    c="57964">. We used the</st> `<st c="57978">WoEEncoder()</st>`<st c="57990">,
    which learned the WoE per category with the</st> `<st c="58036">fit()</st>` <st
    c="58041">method, and then used</st> `<st c="58064">tra</st><st c="58067">nsform()</st>`
    <st c="58076">to replace the categories with the</st> <st c="58112">corresponding
    numbers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58134">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="58143">For an implementation of WoE with Category Encoders, visit this
    book’s GitHub</st> <st c="58222">repository:</st> [<st c="58234">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/m</st><st
    c="58325">ain/ch02-categorical-encoding/Recipe-07-Weight-of-evidence.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-07-Weight-of-evidence.ipynb)<st
    c="58390">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58391">Grouping rare or infrequent categories</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="58430">Rare categories are those</st> <st c="58457">presen</st><st c="58463">t
    only in a small fraction of the observations.</st> <st c="58512">There is no rule
    of thumb to determine how small a small fraction is, b</st><st c="58583">ut typically,
    any value below 5% can be</st> <st c="58624">considered rare.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58640">Infrequent labels often</st> <st c="58664">appear only on the
    train set or only on the test set, thus making the algorithms prone to overfitting
    or being unable to score an observation.</st> <st c="58808">In addition, when
    encoding categories to numbers, we only create mappings for those categories observed
    in the train set, so we won’t know how to encode new labels.</st> <st c="58973">T</st><st
    c="58974">o avoid these complications, we can group infrequent categories into
    a single category called</st> `<st c="59068">Rare</st>` <st c="59072">or</st>
    `<st c="59076">Other</st>`<st c="59081">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59082">In this recipe,</st> <st c="59099">we will group infrequent categories
    using</st> `<st c="59141">pandas</st>` <st c="59147">and</st> `<st c="59152">feature-engine</st>`<st
    c="59166">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59167">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="59183">First, let’s import the necessary Python libraries and get the</st>
    <st c="59247">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59261">Import the necessary Python libraries, functions,</st> <st c="59312">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59469">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="59538">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59735">Let’s</st> <st c="59742">capture the fraction of observations
    per category in</st> `<st c="59795">A7</st>` <st c="59797">in</st> <st c="59801">a
    variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59863">We can see the percentage of observations per category of</st>
    `<st c="59922">A7</st>`<st c="59924">, expressed as decimals, in the following
    output after</st> <st c="59979">executing</st> `<st c="59989">print(freqs)</st>`<st
    c="60001">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60282">Let’s create a list containing the names of the categories present
    in more than 5% of</st> <st c="60369">the observations:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60452">If we execute</st> `<st c="60467">print(frequent_cat)</st>`<st
    c="60486">, we will see the frequent categories</st> <st c="60524">of</st> `<st
    c="60527">A7</st>`<st c="60529">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_enc = X_train.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc = X_test.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_enc["A7"] = np.where(X_train["A7"].isin(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: frequent_cat), X_train["A7"], "Rare")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc["A7"] = np.where(X_test["A7"].isin(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: frequent_cat), X_test["A7"], "Rare")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60911">Let’s determine the percentage of observations in the</st> <st
    c="60966">encoded variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61026">We can see that the infrequent labels have now been re-grouped
    into the</st> `<st c="61099">Rare</st>` <st c="61103">category:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61251">Let’s</st> <st c="61257">create a rare label encoder that groups
    categories present in less than 5% of the observations, provided that the categorical
    variable has more than four</st> <st c="61412">distinct values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61486">Let’s fit the encoder so that it finds the categorical variables
    and then learns their most</st> <st c="61579">frequent categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61625">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61630">Upon fitting, the transformer will raise warnings, indicating
    that many categorical variables have less than four categories, thus their values
    will not be grouped.</st> <st c="61796">The transformer just lets you know that
    this</st> <st c="61841">is happening.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61854">We can display the frequent categories per variable by executing</st>
    `<st c="61920">rare_encoder.encoder_dict_</st>`<st c="61946">, as well as the
    variables that will be encoded by</st> <st c="61997">executing</st> `<st c="62007">rare_encoder.variables_</st>`<st
    c="62030">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62031">Finally, let’s group rare labels in the train and</st> <st c="62082">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="62182">Now that we have grouped rare labels, we are ready to encode the
    catego</st><st c="62254">rical variables, as we’ve done in the previous recipes
    in</st> <st c="62313">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62326">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62342">In this recipe, we grouped infrequent categories using</st> `<st
    c="62398">pandas</st>` <st c="62404">and</st> `<st c="62409">feature-engine</st>`<st
    c="62423">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62424">We determined</st> <st c="62438">the fraction of observations
    per category of the</st> `<st c="62488">A7</st>` <st c="62490">variable using</st>
    `<st c="62506">pandas</st>`<st c="62512">’</st> `<st c="62515">value_counts()</st>`
    <st c="62529">by setting the</st> `<st c="62545">normalize</st>` <st c="62554">parameter
    to</st> `<st c="62568">True</st>`<st c="62572">. Using list comprehension, we
    captured the names of the variables present in more than 5% of the observations.</st>
    <st c="62684">Finally, u</st><st c="62694">sing NumPy’s</st> `<st c="62708">where()</st>`<st
    c="62715">, we searched each row of</st> `<st c="62741">A7</st>`<st c="62743">,
    and if</st> <st c="62752">the observation was one of the frequent categories in
    the list, which we checked using</st> `<st c="62839">pandas</st>`<st c="62845">’</st>
    `<st c="62848">isin()</st>`<st c="62854">, its value was kept; otherwise, it was
    replaced</st> <st c="62903">with</st> `<st c="62908">Rare</st>`<st c="62912">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62913">We automated the preceding steps for multiple categorical variables
    using</st> `<st c="62988">feature-engine</st>`<st c="63002">’s</st> `<st c="63006">RareLabelEncoder()</st>`<st
    c="63024">. By setting</st> `<st c="63037">tol</st>` <st c="63040">to</st> `<st
    c="63044">0.05</st>`<st c="63048">, we retained categories present in more than
    5% of the observations.</st> <st c="63118">By setting</st> `<st c="63129">n_categories</st>`
    <st c="63141">to</st> `<st c="63145">4</st>`<st c="63146">, we only grouped categories
    in variables with more than four unique values.</st> <st c="63223">With</st> `<st
    c="63228">fit()</st>`<st c="63233">, the transformer identified the categorical
    variables and then learned and stored their frequent categories.</st> <st c="63343">With</st>
    `<st c="63348">transform</st><st c="63357">()</st>`<st c="63360">, the transformer
    replaced infrequent categories with the</st> `<st c="63418">Rare</st>` <st c="63422">string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63430">Performing b</st><st c="63443">inary encoding</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`<st c="63808">1</st>` <st c="63809">can be represented with the sequence of</st>
    `<st c="63850">1-0</st>`<st c="63853">, integer</st> `<st c="63863">2</st>` <st
    c="63864">with</st> `<st c="63870">0-1</st>`<st c="63873">, integer</st> `<st
    c="63883">3</st>` <st c="63884">with</st> `<st c="63890">1-1</st>`<st c="63893">,
    and integer</st> `<st c="63907">0</st>` <st c="63908">with</st> `<st c="63914">0-0</st>`<st
    c="63917">. The digits in the two positions of the binary string become the</st>
    <st c="63982">columns, which are the encoded representations of the</st> <st c="64037">original
    variable:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Table showing the steps required for binary encoding the color
    variable](img/B22396_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="64134">Figure 2.10 – Table showing the steps required for binary encoding
    the color variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64219">Binary encoding</st> <st c="64236">encodes the data in fewer dimensions
    than one-hot encoding.</st> <st c="64296">In our example, the</st> `<st c="64316">Color</st>`
    <st c="64321">variable would be encoded into</st> *<st c="64353">k-1</st>* <st
    c="64356">categories by one-hot encoding – that is, three variables – but with
    binary encoding, we can represent the variable with only two features.</st> <st
    c="64497">More generally, we determine the number of binary features needed to
    encode a variable as</st> *<st c="64587">log2(number of distinct</st> <st c="64610">categories)</st>*<st
    c="64622">; in our example,</st> *<st c="64641">log2(4) = 2</st>* <st c="64652">binary
    features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64669">Binary encoding is an alternative method to one-hot encoding where
    we do not lose information about the variable, yet we obtain fewer features after
    the encoding.</st> <st c="64833">This is particularly useful when we have highly
    cardinal variables.</st> <st c="64901">For example, if a variable contains 128
    unique categories, with one-hot encoding, we would need 127 features to encode
    the variable, whereas with binary encoding, we will only need</st> *<st c="65082">7
    (log2(128)=7)</st>*<st c="65097">. Thus, this encoding prevents the feature space
    from exploding.</st> <st c="65162">In addition, binary-encoded features are also
    suitable for linear models.</st> <st c="65236">On the downside, the derived binary
    features lack human interpretability, so if we need to interpret the decisions
    made by our models, this encoding method may not be a</st> <st c="65405">suitable
    option.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65421">In this rec</st><st c="65433">ipe, we will learn how to perform
    binary encoding using</st> <st c="65490">Category Encoders.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65508">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="65524">First, let’s import the necessary Python libraries and get the</st>
    <st c="65588">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65602">Import the required Python libraries, functions,</st> <st c="65652">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="65788">Let’s load the Credit Approval dataset and divide it into train
    and</st> <st c="65857">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="66053">Let’s inspect</st> <st c="66067">the unique categories</st> <st
    c="66090">in</st> `<st c="66093">A7</st>`<st c="66095">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="66120">In the following output, we can see that</st> `<st c="66162">A7</st>`
    <st c="66164">has 10</st> <st c="66172">different categories:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: encoder = BinaryEncoder(cols=["A7"],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: drop_invariant=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="66377">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="66382">BinaryEncoder()</st>`<st c="66398">, as well as other encoders
    from the Category Encoders package, allow us to select the variables to encode.</st>
    <st c="66506">We simply pass the column names in a list to the</st> `<st c="66555">cols</st>`
    <st c="66559">argument.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66569">Let’s fit the transformer to the train set so that it calculates
    how many binary variables it needs and creates the variable-to-binary</st> <st
    c="66705">code representations:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="66747">Finally, let’s encode</st> `<st c="66770">A7</st>` <st c="66772">in
    the train and</st> <st c="66790">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="66880">We can</st> <st c="66887">display the top rows of the transformed
    train set</st> <st c="66938">by executing</st> `<st c="66951">print(X_train_enc.head())</st>`<st
    c="66976">, which returns the</st> <st c="66996">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.11 – DataFrame with the variables after binary encoding](img/B22396_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="67333">Figure 2.11 – DataFrame with the variables after binary encoding</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67397">Binary encoding returned four binary variables for</st> `<st c="67449">A7</st>`<st
    c="67451">, which are</st> `<st c="67463">A7_0</st>`<st c="67467">,</st> `<st
    c="67469">A7_1</st>`<st c="67473">,</st> `<st c="67475">A7_2</st>`<st c="67479">,
    and</st> `<st c="67485">A</st><st c="67486">7_3</st>`<st c="67489">, instead of
    the nine that would have been returned by</st> <st c="67544">one-hot encoding.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67561">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="67577">In this recipe, we performed binary encoding using the Category
    Encoders package.</st> <st c="67660">We used</st> `<st c="67668">BinaryEncoder()</st>`
    <st c="67683">to encode the</st> `<st c="67698">A</st><st c="67699">7</st>` <st
    c="67700">variable.</st> <st c="67711">With the</st> `<st c="67720">fit()</st>`
    <st c="67725">method,</st> `<st c="67734">BinaryEncoder()</st>` <st c="67749">created
    a mapping from a category to a set of binary columns, and with the</st> `<st c="67825">transform()</st>`
    <st c="67836">method, the encoder encoded the</st> `<st c="67869">A7</st>` <st
    c="67871">variable in both the train and</st> <st c="67903">test sets.</st>****
  prefs: []
  type: TYPE_NORMAL
