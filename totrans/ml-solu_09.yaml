- en: Chapter 9. Building a Real-Time Object Recognition App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build an application that can detect objects. This
    application will help us recognize the object present in an image or a video feed.
    We will be using real-time input, such as a live video stream from our webcam,
    and our real-time object detection application will detect the objects present
    in the video stream. We will be using a live video stream, which is the main reason
    why this kind of object detection is called **Real-Time Object Detection**. In
    this chapter, we will be using the **Transfer Learning** methodology to build
    Real-Time Object Detection. I will explain Transfer Learning in detail during
    the course of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the coding environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features engineering for the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the **Machine Learning** (**ML**) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the process for optimization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be building an object detection application. We won't
    just be detecting objects, but we will be building the application that detects
    the objects in real time. This application can be used in self-driving cars, for
    segregation tasks in the agricultural field, or even in the robotics field. Let's
    understand our goal and what we are actually building.
  prefs: []
  type: TYPE_NORMAL
- en: We want to build an application in which we will provide the live webcam video
    stream or the live video stream as the input. Our application will use pre-trained
    Machine Learning models, which will help us predict the objects that appear in
    the video. This means that, if there is a person in the video, then our application
    can identify the person as a person. If the video contains a chair or a cup or
    a cell phone, then our application should identify all these objects in the correct
    manner. So, our main goal in this chapter is to build an application that can
    detect the objects in images and videos. In this chapter, you will also learn
    the concept of Transfer Learning. All our approaches are based on Deep Learning
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be discussing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the dataset on which the Deep Learning models
    have been trained. There are two datasets that are heavily used when we are trying
    to build the object detection application, and those datasets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The COCO dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PASCAL VOC dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at each of the datasets one by one.
  prefs: []
  type: TYPE_NORMAL
- en: The COCO dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'COCO stands for Common object in context. So, the short form for this dataset
    is the COCO dataset. Many tech giants, such as Google, Facebook, Microsoft, and
    so on are using COCO data to build amazing applications for object detection,
    object segmentation, and so on. You can find details regarding this dataset at
    this official web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://cocodataset.org/#home](http://cocodataset.org/#home)'
  prefs: []
  type: TYPE_NORMAL
- en: The COCO dataset is a large-scale object detection, segmentation, and captioning
    dataset. In this dataset, there are a total of 330,000 images, of which more than
    200,000 are labeled. These images contain 1.5 million object instances with 80
    object categories. All the labeled images have five different captions; so, our
    machine learning approach is able to generalize the object detection and segmentation
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using COCO Explorer, we can explore the COCO dataset. You can use the [http://cocodataset.org/#explore](http://cocodataset.org/#explore)
    URL to explore the dataset. COCO Explorer is great user interface. You just need
    to select objects tags such as *I want to see images with a person, a bicycle,
    and a bus in the picture* and the explorer provides you images with a person,
    a bicycle, and a bus in it. You can refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The COCO dataset](img/B08394_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: COCO Dataset explorer snippet'
  prefs: []
  type: TYPE_NORMAL
- en: In each image, the proper object boundary has been provided for each of the
    major objects. This is the main reason why this dataset is great if you want to
    build your own computer vision application from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are not downloading the dataset because if we need to train the models
    from scratch on this dataset, then it will require a lot of time and lots of GPUs
    in order to get good accuracy. So, we will be using a pre-trained model, and using
    Transfer Learning, we will implement real-time object detection. Now let's move
    on to the PASCAL VOC dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The PASCAL VOC dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PASCAL stands for Pattern Analysis, Statistical Modeling, and Computational
    Learning and VOC stands for Visual Object Classes. In this dataset, images are
    tagged for 20 classes for object detection. Action classes and person layout taster
    tagging are available as well. In the person layout taster tagging, the bounding
    box is all about the label of each part of a person (head, hands, feet). You can
    refer to the details of this dataset at [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: PASCAL VOC classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Images have been categorized into four major classes:'
  prefs: []
  type: TYPE_NORMAL
- en: Person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Animal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vehicle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indoor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each image is tagged with the preceding major classes, plus there are specific
    tags given to the objects in the images. Each of the preceding four categories
    has specific tags, which I have described in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Person**: person'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Animal**: bird, cat, cow, dog, horse, sheep'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vehicle**: aero plane, bicycle, boat, bus, car, motorbike, train'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indoor**: bottle, chair, dining table, potted plant, sofa, tv/monitor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the following figure, which will help you understand the tagging
    in this PASCAL VOC dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PASCAL VOC classes](img/B08394_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: PASCAL VOC tagged image example'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding figure, two major classes have been tagged:
    Person and Animal. Specific tagging has been given for objects appearing in the
    image, that is, the person and the sheep. We are not downloading this dataset;
    we will be using the pre-trained model, which was trained using this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: You have heard the terms Transfer Learning and pre-trained model a lot until
    now. Let's understand what they are.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at what Transfer Learning is and how it is going
    to be useful for us as we build real-time object detection. We divide this section
    into the following parts:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Transfer Learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a pre-trained model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why should we use a pre-trained model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we use the pre-trained model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the first question.
  prefs: []
  type: TYPE_NORMAL
- en: What is Transfer Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be looking at the intuition behind Transfer Learning first and, then,
    we will cover its technical definition. Let me explain this concept through a
    simple teacher-student analogy. A teacher has many years of experience in teaching
    certain specific topics or subjects. Whatever information the teacher has, they
    deliver it to their students. So, the process of teaching is all about transferring
    knowledge from the teacher to the student. You can refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Transfer Learning?](img/B08394_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: An overview of Transfer Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Now, remember this analogy; we will apply it to neural networks. When we train
    a neural network, it gains knowledge from the given dataset. This trained neural
    network has some weights that help it learn from the given dataset; after training,
    we can store these weights in a binary format. The weight that we have stored
    in the binary format can be extracted and then transferred to any other neural
    network. So, instead of training the neural network from scratch, we transfer
    the knowledge that the previously trained model gained. We are transferring the
    learned features to the new neural network and this will save a lot of our time.
    If we have an already trained model for a particular application, then we will
    apply it to the new but similar type of application, which in turn will help save
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to define Transfer Learning in more technical terms. Transfer
    Learning is a research problem in Machine Learning that focuses on storing knowledge
    gained while solving a particular problem and applying it to a different but related
    problem. Sometimes, Transfer Learning is also called inductive transfer. Let's
    take a solid example to solidify your vision. If we build a Machine Learning model
    that gained knowledge while learning to recognize cars, it can also be applied
    when we are trying to recognize trucks. In this chapter, we are using Transfer
    Learning in order to build this real-time object detection application.
  prefs: []
  type: TYPE_NORMAL
- en: What is a pre-trained model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I want to give you a simple explanation. A pre-trained model is one that is
    created and built by someone else to solve a specific problem. This means that
    we are using a model which has already been trained and plugging and playing with
    it. By using the pre-trained model, we can build new applications with similar
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me give you an example: suppose we want to create a self-driving car. In
    order to build it, our first step would be to build a decent object recognition
    system. You can spend a year or more to build the decent image and object recognition
    algorithm from scratch, or you can use a pre-trained model, such as the Google
    inception model or the YOLO model, which has been built using the PASCAL VOC dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some advantages of using a pre-trained model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A pre-trained model may not give you 100% accuracy, but it saves a lot of effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can optimize the accuracy of the real problem on which you are working rather
    than making an algorithm from scratch; as we say sometimes, there is no need to
    reinvent the wheel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many libraries available that can help us save trained models, so
    we can load and use them easily whenever we need them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the advantages the pre-trained models provide us, we need to understand
    other real reasons why we should use pre-trained models. So, let's discuss that
    in our next section.
  prefs: []
  type: TYPE_NORMAL
- en: Why should we use a pre-trained model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we are focusing on developing the existing algorithms in a different manner,
    our goal would be to build an algorithm that would outperform every other existing
    algorithm and making it more efficient. If we just focus on the research part,
    then this can be a nice approach to develop an efficient and accurate algorithm,
    but if your vision is to make an application and this algorithm is one part of
    the entire application, then you should focus on how quickly and efficiently you
    can build the application. Let's understand this through an example.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we want to build real-time object detection techniques. Now,
    my primary focus would be on building an application that would detect objects
    in real time. Not just that; I need to combine object detection and real-time
    tracking activity as well. If I ignore my primary goal and start making the new
    but effective object detection algorithm, then I will lose focus. My focus will
    be on building the entire real-time object detection application and not just
    a certain algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, if we lose focus and try to build the algorithm from scratch, then
    it will take a lot of time. We will also waste our efforts, because some smart
    people in the world may have already built it for us. When we use this already
    developed algorithm, we will save time and focus on our application. After building
    a prototype of our actual application, we will have time to improvise it so that
    it can be used by many other people.
  prefs: []
  type: TYPE_NORMAL
- en: Let me tell you my story. I started building an image detection application
    that could be used in the security domain a couple of months ago. At that time,
    I didn't want to use a pre-trained model, because I wanted to explore how much
    effort it would take to build an algorithm from scratch. So, I started making
    one on my own. I tried several different algorithms, such as SVM, **Multilayer
    Perceptron** (**MLP**), and **Convolution Neural Network** (**CNN**) models, but
    I got really low accuracy. I lost focus on building an image-detection algorithm
    application that could be used in the security domain and just started focusing
    on making the algorithm better. After some time, I realized that it would be better
    if I used a pre-trained model with an optimization technique that would save my
    time and enable me to build a better solution. After this experience, I tried
    to explore the option of using Transfer Learning in problem statements I was solving,
    and if I found that there would be no scope for Transfer Learning, then I would
    make the algorithms from scratch; otherwise, I preferred using the pre-trained
    model. So, always explore all the options before building the algorithm. Understand
    the application usage and build your solution based on that. Suppose you are building
    your own self-driving car; then, real-time object detection would become an important
    part of a self-driving car, but it would be just a part of the entire application;
    therefore, it would be better if we were to use a pre-trained model to detect
    objects, so that we could use our time to build a quality self-driving car.
  prefs: []
  type: TYPE_NORMAL
- en: How can we use a pre-trained model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generally, a pre-trained model is in the binary format, which can be downloaded
    and then used in our applications. Some libraries, such as Keras, TensorFlow,
    Darknet, and so on, already have those pre-models that you can load and use with
    certain available APIs. These pre-trained networks have the ability to generalize
    images that are not part of the PASCAL VOC or COCO dataset via Transfer Learning.
    We can modify the pre-existing model by fine-tuning the model. We don''t want
    to modify the weights too much, because it has been trained on a large dataset
    using lots of GPUs. The pre-trained model has the ability to generalize the prediction
    and classification of objects, so we know that this pre-trained model can be generalized
    enough to give us the best possible outcome. However, we can change some hyperparameters
    if we want to train the model from scratch. These parameters can be the learning
    rate, epochs, layer size, and so on. Let''s discuss some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning Rate: Learning rate basically controls how much we should update the
    weights of neurons. We can use fixed learning rate, decreassing learning rate,
    momentum-based methods, or adaptive learning rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of epochs: The number of epochs indicates the number of times the entire
    training dataset should pass through the neural network. We need to increase the
    number of epochs in order to decrease gap between the test error and the training
    error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch Size: For convolutional neural networks, mini-batch size is usually more
    preferable. A range of 16 to 128 is really a good choice to start from for convolutional
    neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activation function: As we know, activation functions introduce non-linearity
    to the model. ReLU activation function is the first choice for convolutional neural
    networks. You can use other activation functions, such as tanh, sigmoid, and so
    on, as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout for regularization: Regularization techniques are used to prevent overfitting
    problems. Dropout is the regularization technique for deep neural networks. In
    this technique, we are dropping out some of the neurons or units in neural networks.
    The drop out of neurons is based on probability value. The default value for this
    is 0.5, which is good choice to start with, but we can change the value for regularization
    after observing training error and testing error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we will be using pre-trained models such as the Caffe pre-trained model,
    the TensorFow object detraction model, and **You Only Look Once** (**YOLO**).
    For real-time streaming from our webcam, we are using OpenCV, which is also useful
    in order to draw bounding boxes. So first, let's set up the OpenCV environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the coding environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will list down the libraries and equipment you need in order
    to run the upcoming code. You need to have a webcam that can at least stream the
    video with good clarity. We will be using OpenCV, TensorFlow, YOLO, Darkflow,
    and Darknet libraries. I'm not going to explain how to install TensorFlow, because
    it is an easy process and you can find the documentation for the installation
    by clicking on [https://www.tensorflow.org/install/install_linux](https://www.tensorflow.org/install/install_linux).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be looking at how to set up OpenCV first and, in the
    upcoming sections, we will see how to set up YOLO, Darkflow, and DarkNet.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up and installing OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenCV stands for Open Source Computer Vision. It is designed for computational
    efficiency, with a strong focus on real-time applications. In this section, you
    will learn how to set up OpenCV. I''m using Ubuntu 16.04 and I have a GPU, so
    I have already installed CUDA and CUDNN. If you haven''t installed CUDA and CUDNN,
    then you can refer to this GitHub link: [https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64](https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64).
    Once you are done with that, start executing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will update the software and libraries: `$ sudo apt-get update`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will upgrade the OS and install OS-level updates: `$ sudo apt-get upgrade`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is for compiling the software: `$ sudo apt-get install build-essential`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This command installs prerequisites for OpenCV: `$ sudo apt-get install cmake
    git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This command installs optional prerequisites for OpenCV: `$ sudo apt-get install
    python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev
    libjasper-dev libdc1394-22-dev`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a directory using this command: `$ sudo mkdir ~/opencv`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jump to the directory that we just created: `$ cd ~/opencv`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the following OpenCV projects from GitHub inside the opencv directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$ sudo git clone https://github.com/opencv/opencv.git`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$ sudo git clone https://github.com/opencv/opencv_contrib.git`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the opencv folder, create another directory named build: `$ sudo mkdir
    ~/opencv/build`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jump to the build directory or folder: `$ cd ~/opencv/build`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you are in the build folder location, run this command. It may take some
    time. If you run this command without any error, then proceed to the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Identify how many CPU cores you have by using this command.: `$ nproc`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you know the number of cores of the CPU, you can use it to process multi-threading.
    I have allocated four CPU cores, which means there are four threads running simultaneously.
    The command for this is `$ make -j4` and it compiles all of the classes written
    in C for OpenCV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, execute this command for the actual installation of OpenCV: `$ sudo make
    install`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the path to the configuration file: `$ sudo sh -c ''echo "/usr/local/lib"
    >> /etc/ld.so.conf.d/opencv.conf''`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check for the proper configuration using this command: `$ sudo ldconfig`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have installed OpenCV successfully, we can use this library to stream
    real-time video. Now, we will start building our baseline model. Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Features engineering for the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build the baseline model, we will use the Caffe implementation of
    the Google MobileNet SSD detection network with pre-trained weights. This model
    has been trained on the PASCAL VOC dataset. So, in this section, we will look
    at the approach with which this model has been trained by Google. We will understand
    the basic approach behind MobileNet SSD and use the pre-trained model to help
    save time. To create this kind of accurate model, we need to have lots of GPUs
    and training time, so we are using a pre-trained model. This pre-trained MobileNet
    model uses **Convolution Neural Net** (**CNN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how the features have been extracted by the MobileNet using
    CNN. This will help us understand the basic idea behind CNN, as well as how MobileNet
    has been used. The CNN network is made of layers and, when we provide the images
    to CNN, it scans the region of the images and tries to extract the possible objects
    using the region proposal method. Then, it finds the region with objects, uses
    the warped region, and generates the CNN features. These features can be the position
    of the pixels, edges, the length of the edges, the texture of the images, the
    scale of the region, the lightness or darkness of the picture, object parts, and
    so on. The CNN network learns these kinds of features by itself. You can refer
    to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Features engineering for the baseline model](img/B08394_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Understanding features extraction in CNN'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding image, the region of the image has been scanned
    and the first CNN layer, made up of C1 and S1, will generate the features. These
    features can identify the representation of edges that eventually build the whole
    object. In the second stage, CNN layers learn the feature representation that
    can help the neural network identify parts of the objects. In the last stage,
    it learns all the features that are necessary in order to identify the objects
    present in the given input images. If you want to explore each and every aspect
    of the CNN network, you can refer to [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/).
    Don't worry; we will cover the overview of the CNN architecture in the upcoming
    section, so that you can understand how object detection will work. Now, it's
    time to explore CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the machine learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already know that we are using **Convolution Neural Networks** (**CNN**)
    for developing this application. You might wonder why we have chosen CNN and not
    another neural net. You might already know the answer to this question. There
    are three reasons why we have chosen CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: The amount of visual data present nowadays, which is carefully hand-labeled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The affordable computation machines through which GPUs open the door for optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The various kinds of architecture of CNN outperforms the other algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to these reasons, we have chosen the CNN with SSD. During the development
    of the baseline model, we will be using MobileNet, which uses CNN with **Single
    Shot Detector** (**SSD**) techniques underneath. So, in this section, we will
    look at the architecture of the CNN used during the development of the MobileNet.
    This will help us understand the pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the MobileNet SSD model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MobileNet SSD is fast and does the job of object detection in images and video
    well. This model is faster than **Region-based Convolution Neural Network** (**R-CNN**).
    SSD achieves this speed because it scans images and video frames quite differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R-CNN, models performed region proposals and region classifications in two
    different steps, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, they used a region proposal network in order to generate regions of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, fully connected layers or positive sensitive constitutional layers
    classified the objects in the selected regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These steps are at par with the R-CNN, but SSD performs them in a single shot,
    which means it simultaneously predicts the bounding boxes and the classes of the
    objects appearing in the bounding boxes. SDD performs the following steps when
    an image or video streams and set of basic truth labels has been given as the
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the images through the series of convolution layers that generate the sets
    of features map in the form of a different size of matrix. The output can be in
    the form of a 10×10 matrix, a 6×6 matrix, or a 3×3 matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each location in each of the feature maps, use the 3×3 convolution filter
    in order to evaluate the small set of default bound boxes. These generated default
    bounding boxes are equivalent to anchor boxes, which are generated using Faster
    R-CNN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each box, simultaneously predict the bounding box offset and the class probability
    for objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training, match the ground truth box with these predicted boxes.
    This matching is performed by using Intersection of Union (IoU). The best predicted
    box will be labeled as a positive bounding box. This happens to every boundary
    box. An IoU with more than 50% truth value has been considered here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a look at the following figure for a graphical representation. MobileNets
    have streamlined architecture that uses depth-wise separable convolutions in order
    to build light weight deep neural networks for mobile and embedded vision application.
    MobileNets are a more efficient ML model for computer vision application. You
    can also refer to the original paper for MobileNet at [https://arxiv.org/pdf/1704.04861.pdf](https://arxiv.org/pdf/1704.04861.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of the MobileNet SSD model](img/B08394_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Basic architectural building blocks for MobileNet SSD'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: https://arxiv.org/pdf/1704.04861.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding figure, we used the standard convolution network
    with the depth-wise convolution network. MobileNet SDD used the ReLU activation
    function. You can refer to the following figure to get an idea about what kind
    of filter shape this network has:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of the MobileNet SSD model](img/B08394_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6, MobileNet body architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Source: https://arxiv.org/pdf/1704.04861.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to interpret this table, then let''s consider an example. If we
    have an original image with a pixel sixe of 224×224, then this Mobilenet network
    shrinks the image down to 7×7 pixels; it also has 1,024 channels. After this,
    there is an average pooling layer that works on all the images and generates the
    vector of a 1×1×1,024 size, which is just a vector of 1,024 elements in reality.
    If you want to learn more about MobileNet SSD, refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19](https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/](http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's move on to the implementation part.
  prefs: []
  type: TYPE_NORMAL
- en: Building the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be looking at the coding part. You can refer to the
    code given at this GitHub link: [https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model](https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model).'
  prefs: []
  type: TYPE_NORMAL
- en: First, download the project from the given link and install OpenCV, as per the
    information given earlier in this chapter. When you download this project folder,
    there is a pre-trained MobileNet SSD that has been implemented using the caffe
    library, but here, we are using the pre-trained binary model. We are using OpenCV
    for loading the pre-trained model as well as streaming the video feeds from the
    webcam.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code, first, we specify the libraries that we need to import and define
    the command-line arguments that will be used to run the script. We need to provide
    the parameter file and the pre-trained model. The name of the parameter file is
    `MobileNetSSD_deploy.prototxt.txt` and the filename for the pre-trained model
    is `MobileNetSSD_deploy.caffemodel`. We have also defined the classes that can
    be identified by the model. After this, we will load the pre-trained model using
    OpenCV. You can refer to the coding up to this stage in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the baseline model](img/B08394_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Code snippet for the baseline model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how we can stream the video from our webcam. Here, we are
    using library `imutils` and its video API to stream the video from the webcam.
    Using the start function, we will start the streaming and, after that, we will
    define the frame size. We grab the frame size and convert it into a blob format.
    This code always verifies that the detected object confidence score will be higher
    than the minimum confidence score or the minimum threshold of the confidence score.
    Once we get a higher confidence score, we will draw the bounding box for those
    objects. We can see the objects that have been detected so far. You can refer
    to the following figure for the video streaming of the baseline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the baseline model](img/B08394_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Code snippet for the baseline model video streaming'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to stop the streaming, we need to break the loop by pressing Q or
    Ctrl + C and we need to take care that when we close the program, all windows
    and processes will stop appropriately. You can see this in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the baseline model](img/B08394_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Code snippet for ending the script'
  prefs: []
  type: TYPE_NORMAL
- en: Before we run the testing of the script, let's understand the testing metrics
    for the object detection application. Once we understand the testing metrics,
    we will run the code as well as checking how much accuracy we are getting.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the testing metrics. We will look at the two
    matrices that will help us understand how to test the object detection application.
    These testing matrices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Intersection over Union (IoU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mean Average Precision (mAP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intersection over Union (IoU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For detection, IoU is used in order to find out whether the object proposal
    is right or not. This is a regular way to determine whether object detection is
    done perfectly or not. IoU generally takes the set, A, of proposed object pixels
    and the set of true object pixels, B, and calculates IoU based on the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intersection over Union (IoU)](img/B08394_09_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally, IoU >0.5, which means that it was a hit or that it identified the
    object pixels or boundary box for the object; otherwise, it fails. This is a more
    formal understanding of the IoU. Now, let''s look at the intuition and the meaning
    behind it. Let''s take an image as reference to help us understand the intuition
    behind this matrix. You can refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intersection over Union (IoU)](img/B08394_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Understanding the intuition behind IoU'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot is an example of detecting a stop sign in an image.
    The predicted bounding box is drawn in red and pixels belonging to this red box
    are considered part of set A, while the ground-truth bounding box is drawn in
    green and pixels belong to this green box are considered part of set B. Our goal
    is to compute the Intersection of Union between these bounding boxes. So, when
    our application draws a boundary box, it should match the ground-truth boundary
    box at least more than 50%, which is considered a good prediction. The equation
    for IoU is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intersection over Union (IoU)](img/B08394_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Equation of IoU based on intuitive understanding'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are few chances in reality where the (x, y) coordinate of our predicted
    bounding box will exactly match the (x, y) coordinates of the ground-truth bounding
    box. In the following figure, you can see various examples for poor, good, and
    excellent IoUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intersection over Union (IoU)](img/B08394_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Various IoU boundary box examples'
  prefs: []
  type: TYPE_NORMAL
- en: IoUs help us to determine how well the application identifies the object boundaries
    and differentiates the various objects from each other. Now, it's time to understand
    the next testing metrics.
  prefs: []
  type: TYPE_NORMAL
- en: mean Average Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will cover the **mean Average Precision** (**mAP**). In
    object detection, first, we identify the object boundary box and then we classify
    it into a category. These categories have some labels, and we provide the appropriate
    label to the identified objects. Now, we need to test how well the application
    can assign these labels, which means how well we can classify the objects into
    different predefined categories. For each class, we will calculate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'True Positive TP(c): A predicted class was C and the object actually belongs
    to class C'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'False Positive FP(c): A predicted class was C but in reality, the object does
    not belong to class C'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average Precision for class C is given by the following equation:![mean Average
    Precision](img/B08394_09_31.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, for all the classes, we need to calculate the mAP and the equation for
    that is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![mean Average Precision](img/B08394_09_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want better prediction, then we need to increase the IoU from 0.5 to
    a higher value (up to 1.0, which would be perfect). We can denote this with this
    equation: mAP[@p], where p ∈ (0,1) is the IoU. mAP[@[0.5:0.95]] means that the
    mAP is calculated over multiple thresholds and then it is averaged again.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's test the baseline model and check the mAP for this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will run the baseline model. In order to run the script,
    we need to jump to the location where we put the script titled `real_time_object_detection.py`
    and, on Command Prompt, we need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the baseline model](img/B08394_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: Execution of the baseline approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following figure. Here, I have just placed example images,
    but you can see the entire video when you run the script. Here is the link to
    see the entire video for real-time object detection using the baseline approach:
    [https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the baseline model](img/B08394_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Output of the baseline approach (image is part of the video stream)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the mAP for the MobileNet SSD is 71.1% . You will learn how to optimize
    this approach in the upcoming section. First, we will list down the points that
    we can improve in the next iteration. So, let's jump to our next section.
  prefs: []
  type: TYPE_NORMAL
- en: Problem with existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the MobileNet SSD is fast and gives us good results, it still can't
    identify classes such as cup, pen, and so on. So, we need to use the pre-trained
    model that has been trained on a variety of objects. In this upcoming iteration,
    we need to use the pre-trained model, for example, the TensorFlow object detection
    API, which will able to identify the different objects compared to the baseline
    approach. So now, let's look at how we will optimize the existing approach.
  prefs: []
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, in order to optimize the existing approach, I will be
    using the TensorFlow Object Detection API. You can refer to Google''s TensorFlow
    GitHub repo for this API at the following link: [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).
    This API is trained using the COCO dataset as well as the PASCAL VOC dataset;
    so, it will have the capability of identifying the variety of classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the process for optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important part for us is how to use the various pre-trained models.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, pull the TensorFlow models repository using this link: [https://github.com/tensorflow/models](https://github.com/tensorflow/models)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you pull the repository, you can find the iPython Notebook that I have
    referred to in order to understand how to use the pre-trained model and to find
    the link for the iPython notebook at [https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, SSD with MobileNet has been used, but we are using the detection model
    zoo. This model is trained on the COCO dataset and their versions are given based
    on the speed and performance of the model. You can download the pre-trained model
    from this link: [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).
    I have already placed all these parts together, so it is easy for everyone to
    implement the code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main thing is that this model is trained using the SSD approach, but it
    has taken datasets such as the kitti dataset and the Open Image dataset. So, this
    model is able to detect more objects and is more generalized. The link for the
    Kitti dataset is [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    and the link for the Open Image dataset is [https://github.com/openimages/dataset](https://github.com/openimages/dataset).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we download the repository and the pre-trained model, we will load the
    pre-trained model. In TensorFlow, as we know, the models are saved as a .pb file.
    Once we load the model, we will be using OpenCV to stream the video. In the upcoming
    section, we will be implementing the code for the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will understand the implementation of the revised approach.
    You can refer to this GitHub link: [https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach](https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach),
    which has the pre-trained model and the TensorFlow''s Object detection folder.
    Before we even begin with the code, I will provide information regarding the folder
    structure of this approach. You can refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: Understanding the folder structure for the revised approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the object detection folder downloaded from the TensorFlow model repository:
    [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).
    In the `utils` folder, there are some helper functions to help us stream the video.
    The main script that helps us run the script is `object_detection_app.py`. The
    pre-trained model has been saved inside the object detection folder. The path
    for pre-trained model in this folder is this: `~/PycharmProjects/Real_time_object_detection/revised_approach/object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the coding implementation step by step. In the first step,
    we will import the dependency libraries and load the pre-trained model. You can
    refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Code snippet for loading the pre-trained model in the revised
    approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, there are 90 different types of objects that can be identified.
    Once we load the model, the next step is the `detect_objects()` function, which
    is used to identify the objects. Once the object is identified, the bounding boxes
    for that object are drawn and we simultaneously run the pre-trained model on those
    objects, so that we can get the identification labels for the object, whether
    it''s a cup, bottle, person, and so on. You can refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: Code snippet for the detect_objects() function'
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we have the `worker()` function, which helps us stream the video
    as well as perform some GPU memory management. You can refer to the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18: Code snippet for the worker() function'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have defined the GPU memory fraction, as well as the kind
    of colors used when it detects the objects. Now, let''s look at the main function
    of the script. In the main function, we define some optional arguments and their
    default values. The list of these arguments is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Device index of the camera: `--source=0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Width of the frames in the video stream `--width= 500`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Height of the frames in the video stream `--height= 500`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of workers `--num-workers=2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of the queue `--queue-size=5`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the implementation of the main function shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.19: Code snippet for the main function'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the script, we can see the output given in the upcoming figure.
    Here, we have placed the image, but you can see the video by using this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.20: Output of the revised approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we end the video, the resources and the process should end as well. For
    that, we will be using the code that has been provided to us through the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.21: Code snippet to release the resources once the script is terminated'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have executed this approach, we can identify objects such as cups, pens,
    and so on. So, we can say that our baseline approach is definitely improvised
    and there are some modified labels, such as a sofa (in this approach, identified
    as a couch). Apart from this, if we talk about the mAP of this pre-trained model,
    then as per the documentation, on the COCO dataset, this model gets around 52.4%
    accuracy. In our input, we will get around 73% accuracy. This approach identifies
    more objects with different categories, which is a great advantage.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will discuss the points that we can use to come
    up with the best possible solution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have tried approaches that are fast and accurate, but we need an approach
    that is fast as well as accurate and optimized. These are the points we need to
    keep in our mind when we develop the best possible solution:'
  prefs: []
  type: TYPE_NORMAL
- en: The SSD-based approach is great, but is not that accurate when you train your
    own model using COCO or the PASCAL VOC dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow detection model zoo will have a mAP score on the COCO test dataset
    from 20 to 40%. So, we need to explore other techniques that can help us give
    a better result in terms of processing and object detection accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in the upcoming section, we will look at the approach that can help us optimize
    the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be trying the approach named **YOLO**. YOLO stands
    for You Only Look Once. This technique gives us good accuracy, is fast, and its
    memory management is easy. This section will be divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding YOLO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach using YOLO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first section, we will understand the basics about YOLO. During the implementation,
    we will be use YOLO with the pre-trained YOLO model. So, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding YOLO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLO is a state-of-the-art, real-time object detection system. On GPU Titan
    X, it processes images at 40-90 FPS and has a mAP on the PASCAL VOC dataset of
    78.6% and a mAP of 48.1% on the coco test-dev dataset. So, now, we will look at
    how YOLO works and processes the images in order to identify the objects. We are
    using YOLOv2 (YOLO version 2) as it is a faster version.
  prefs: []
  type: TYPE_NORMAL
- en: The working of YOLO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLO reframes the object detection problem. It considers the object recognition
    task as single regression problem, right from the image pixels to the bounding
    box coordinates and class probabilities. A single convolutional network simultaneously
    predicts multiple bounding boxes and class probabilities for those boxes. YOLO
    trains on full images and directly optimizes detection performance. This approach
    has several advantages compared to traditional methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO is extremely fast. This is because, in YOLO, frame detection is a regression
    problem, and we do not need to use a complex pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can simply run our neural network on a new image at the time of testing to
    predict detection. Our base network runs at 45 frames per second with no batch
    processing on a Titan X GPU and a fast version runs at more than 150 fps. This
    means that we can process streaming video in real time with less than 25 milliseconds
    of latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO processes images globally when it makes a prediction regarding information
    about classes as well as their appearance. It also learns the generalization of
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLO divides the input image into an S×S grid. If the center of an object falls
    into a grid cell, then that grid cell is responsible for detecting the object.
    Each grid cell predicts the B bounding boxes and confidence scores for those boxes.
    These confidence scores reflect how confident the model is that the box contains
    an object and how accurate it thinks the box that it predicts is. So, formally,
    we can define the confidence mechanism for YOLO by using this notation. We define
    the confidence as Pr(object) * IoU. If no object exists in that cell, then the
    confidence scores should be zero; otherwise, we want the confidence score to equal
    the IoU between predicted box and the ground truth. Each bounding box consists
    of five predictions: x, y, w, h, and confidence. The (x, y) coordinates represent
    the center of the box relative to the bounds of the grid cell. The w and h represents
    the width and height. They are predicted in relativity to the whole image. Finally,
    the confidence score represents the IoU. For each grid cell, it predicts C conditional
    class probabilities, *Pr(Classi|Object)*. These probabilities are conditioned
    on the grid cell that contains an object. We predict only one set of class probabilities
    per grid cell regardless of the number of bounding boxes B. At the time of testing,
    we multiply the conditional class probabilities and the individual box confidence
    prediction that is defined by this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The working of YOLO](img/B08394_09_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation gives us the class-specific confidence score for each
    of the boxes. This score contains both the probability of the class appearing
    in the box and how well the predicted box fits the object. The pictorial representation
    of the whole process of YOLO is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The working of YOLO](img/B08394_09_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.22: Pictorial representation of YOLO object detection'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's get to basic knowledge about the YOLO architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of YOLO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will the learn basics about the YOLO architecture. In
    YOLO, there are 24 convolutional layers followed by two fully connected layers.
    Instead of the inception modules used by GoogleNet, YOLO simply uses 1×1 reduction
    layers followed by 3×3 convolutional layers. Fast YOLO uses a neural network with
    fewer convolutional layers. We use nine layers instead of 24 layers. We also use
    fewer filters in these layers. Apart from this, all parameters are the same for
    YOLO and Fast YOLO during training and testing. You can refer to the architecture
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture of YOLO](img/B08394_09_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.23: Architecture of YOLO'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the implementation part.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach using YOLO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement YOLO, we need to install the Cython module. Apart from
    that, you can use either Darknet or the Darkflow, which is the TensorFlow wrapper
    on Darknet. Darknet is written in C and CUDA, so it is quite fast. Here, we will
    be implementing both the options. Before implementation, we need to set up the
    environment. The implementation part should be divided into two sections here:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using Darknet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation using Darkflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to this GitHub repository for all the code: [https://github.com/jalajthanaki/Real_time_object_detection_with_YOLO](https://github.com/jalajthanaki/Real_time_object_detection_with_YOLO).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using Darknet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are following these steps in order to implement YOLO using Darknet:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup for Darknet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile the Darknet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the pre-trained weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run object detection for the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run object detection for the video stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment setup for Darknet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this step, we need to download the GitHub repository of Darknet. We can
    do that using the following command. You can download this repository at any path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once you run this command, the directory named Darknet is created. After that,
    you can jump to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the Darknet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we have downloaded the Darknet, we need to jump to the directory named
    Darknet. After that, we need to compile the Darknet. So, we need to execute the
    following commands sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the pre-trained weight
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Configuration files are already inside the `cfg/` subdirectory inside the darknet
    directory. So, by executing the following command, you can download the pre-trained
    weight for the YOLO model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This download may take some time. Once we have the pre-trained weight with us,
    we can run the Darknet.
  prefs: []
  type: TYPE_NORMAL
- en: Running object detection for the image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you want to identify the objects in the image, then you need to execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can refer to the output of this command in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running object detection for the image](img/B08394_09_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.24 : Output of object detection for the image using Darknet'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement YOLO on the video stream.
  prefs: []
  type: TYPE_NORMAL
- en: Running the object detection on the video stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can run YOLO on the video stream using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we need to pass the path of the video. For more information, you can
    refer to this Darknet documentation: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
    Now, let''s understand the implementation of Darkflow.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using Darkflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this implementation, you need to refer to the code given in the folder named
    Darkflow. We need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cython
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building the already provided setup file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the model and run object detection on the images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the model and run object detection on the video stream
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Cython
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to install Cython, we need to execute the following command. This
    Cython package is needed because the Darkflow is a Python wrapper that uses C
    code from Darknet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once Cython is installed, we can build the other setup.
  prefs: []
  type: TYPE_NORMAL
- en: Building the already provided setup file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this stage, we will be executing the command that will set up the necessary
    Cython environment for us. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When we execute this command, we will have to use `./flow` in the cloned Darkflow
    directory instead of flow, as Darkflow is not installed globally. Once this command
    runs successfully, we need to test whether we installed all the dependencies perfectly.
    You can download the pre-trained weight by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Testing the environment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this stage, we will test whether Darkflow runs perfectly or not. In order
    to check that, we need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the environment](img/B08394_09_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.25 : Successful testing outcome of Darkflow'
  prefs: []
  type: TYPE_NORMAL
- en: Once you can see the preceding output, you'll know that you have successfully
    configured Darkflow. Now, let's run it.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model and running object detection on images
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can run the Darkflow on images. For that, we need to load YOLO pre-trained
    weights, configuration files, and path of images so you can execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to save the object detection in the json format, then that is possible
    as well. You need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the output inside the `sample_img/out` folder; refer to the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on images](img/B08394_09_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.26 : Output image with predicted objects using Darkflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on images](img/B08394_09_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.27 : json output of the object detected in the image'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model and running object detection on the video stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we will run object detection on the video stream. First, we
    will see how to use the webcam and perform object detection. The command for that
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can refer to the following figure. You can see the video at this link:
    [https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on the video stream](img/B08394_09_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.28 : Output of the object detection using Darkflow for the webcam
    video stream'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also run the Darkflow for a prerecorded video. For that, you need to
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can refer to the following figure. You can see the video at this link:
    [https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on the video stream](img/B08394_09_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.29 : Output of the object detection using Darkflow for the prerecorded
    video'
  prefs: []
  type: TYPE_NORMAL
- en: In both commands we have used the—save Video flag to save the video and the—gpu
    0.60 flag, which use 60% memory of GPU. Using this approach, we will get an accuracy
    of 78%.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about Transfer Learning. We explored different
    libraries and approaches in order to build a real-time object detection application.
    You learned how to set up OpenCV and looked at how it is rather useful in building
    the baseline application. In this baseline approach, we used the model that is
    trained using the caffe deep learning library. After that, we used TensorFlow
    to build real-time object detection, but in the end, we used a pre-trained YOLO
    model, which outperformed every other approach. This YOLO-based approach gave
    us more generalized approach for object detection applications. If you are interested
    in building innovative solutions for computer vision, then you can enroll yourself
    in the VOC challenges. This boosts your skills and gives you a chance to learn.
    You can refer to this link for more information: [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/)
    (PASCAL VOC Challenges 2005-2012). You can also build your own algorithm and check
    the result and compare your result with the existing approach and, if it outperforms
    the existing approaches, you can definitely publish the paper in reputed journals.
    By using the YOLO approach, we get the Mean Average Precision of 78% on the PASCAL
    VOC dataset and it works pretty well when you apply this model to any video or
    image. The code credit for this chapter goes to Adrian Rosebrock, Dat Tran, and
    Trieu. We defined the mAP score based on the mAP it gets for either the COCO dataset
    or the PASCAL VOC dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will explore another application that belongs to the
    computer vision domain: face detection and facial expression detection. In order
    to build this application, we will be using Deep Learning techniques. So, keep
    reading!'
  prefs: []
  type: TYPE_NORMAL
