- en: Chapter 9. Building a Real-Time Object Recognition App
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：构建实时目标识别应用
- en: In this chapter, we will build an application that can detect objects. This
    application will help us recognize the object present in an image or a video feed.
    We will be using real-time input, such as a live video stream from our webcam,
    and our real-time object detection application will detect the objects present
    in the video stream. We will be using a live video stream, which is the main reason
    why this kind of object detection is called **Real-Time Object Detection**. In
    this chapter, we will be using the **Transfer Learning** methodology to build
    Real-Time Object Detection. I will explain Transfer Learning in detail during
    the course of the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个能够检测对象的应用。这个应用将帮助我们识别图像或视频流中存在的对象。我们将使用实时输入，例如来自网络摄像头的实时视频流，我们的实时目标检测应用将检测视频流中的对象。我们将使用实时视频流，这也是这种目标检测被称为**实时目标检测**的主要原因。在本章中，我们将使用**迁移学习**方法构建实时目标检测。我将在本章的讲解过程中详细解释迁移学习。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Transfer Learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Setting up the coding environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置编码环境
- en: Features engineering for the baseline model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: Selecting the **Machine Learning** (**ML**) algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择**机器学习**（**ML**）算法
- en: Building the baseline model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基线模型
- en: Understanding the testing metrics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试指标
- en: Testing the baseline model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: Problems with the existing approach
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有方法的问题
- en: How to optimize the existing approach
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何优化现有方法
- en: Understanding the process for optimization
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解优化过程
- en: Implementing the revised approach
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施改进方法
- en: Testing the revised approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试改进方法
- en: Understanding problems with the revised approach
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解改进方法的问题
- en: The best approach
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法
- en: Implementing the best approach
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: Summary
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述
- en: Introducing the problem statement
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: In this chapter, we will be building an object detection application. We won't
    just be detecting objects, but we will be building the application that detects
    the objects in real time. This application can be used in self-driving cars, for
    segregation tasks in the agricultural field, or even in the robotics field. Let's
    understand our goal and what we are actually building.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个目标检测应用。我们不仅会检测对象，还会构建一个实时检测对象的应用。这种应用可以用于自动驾驶汽车、农业领域的分离任务，甚至可以在机器人领域使用。让我们了解我们的目标和实际构建的内容。
- en: We want to build an application in which we will provide the live webcam video
    stream or the live video stream as the input. Our application will use pre-trained
    Machine Learning models, which will help us predict the objects that appear in
    the video. This means that, if there is a person in the video, then our application
    can identify the person as a person. If the video contains a chair or a cup or
    a cell phone, then our application should identify all these objects in the correct
    manner. So, our main goal in this chapter is to build an application that can
    detect the objects in images and videos. In this chapter, you will also learn
    the concept of Transfer Learning. All our approaches are based on Deep Learning
    techniques.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个应用，其中我们将提供实时网络摄像头视频流或实时视频流作为输入。我们的应用将使用预训练的机器学习模型，这将帮助我们预测视频中出现的对象。这意味着，如果视频中有一个人物，那么我们的应用可以将其识别为人物。如果视频包含椅子、杯子或手机，那么我们的应用应该以正确的方式识别所有这些对象。因此，本章的主要目标是构建一个能够检测图像和视频中的对象的应用。在本章中，你还将学习迁移学习的概念。我们所有的方法都是基于深度学习技术的。
- en: In the next section, we will be discussing the dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论数据集。
- en: Understanding the dataset
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'In this section, we will cover the dataset on which the Deep Learning models
    have been trained. There are two datasets that are heavily used when we are trying
    to build the object detection application, and those datasets are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍用于训练深度学习模型的那个数据集。当我们试图构建目标检测应用时，有两个数据集被广泛使用，这些数据集如下：
- en: The COCO dataset
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: COCO数据集
- en: The PASCAL VOC dataset
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PASCAL VOC数据集
- en: We will look at each of the datasets one by one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一查看每个数据集。
- en: The COCO dataset
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: COCO数据集
- en: 'COCO stands for Common object in context. So, the short form for this dataset
    is the COCO dataset. Many tech giants, such as Google, Facebook, Microsoft, and
    so on are using COCO data to build amazing applications for object detection,
    object segmentation, and so on. You can find details regarding this dataset at
    this official web page:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: COCO代表上下文中的常见对象。因此，这个数据集的简称是COCO数据集。许多科技巨头，如谷歌、Facebook、微软等，都在使用COCO数据来构建对象检测、对象分割等令人惊叹的应用程序。你可以在这个官方网页上找到有关此数据集的详细信息：
- en: '[http://cocodataset.org/#home](http://cocodataset.org/#home)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[COCO数据集](http://cocodataset.org/#home)'
- en: The COCO dataset is a large-scale object detection, segmentation, and captioning
    dataset. In this dataset, there are a total of 330,000 images, of which more than
    200,000 are labeled. These images contain 1.5 million object instances with 80
    object categories. All the labeled images have five different captions; so, our
    machine learning approach is able to generalize the object detection and segmentation
    effectively.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: COCO数据集是一个大规模的对象检测、分割和标题数据集。在这个数据集中，总共有330,000张图像，其中超过200,000张被标记。这些图像包含1.5百万个对象实例，分为80个对象类别。所有标记的图像都有五个不同的标题；因此，我们的机器学习方法能够有效地泛化对象检测和分割。
- en: 'By using COCO Explorer, we can explore the COCO dataset. You can use the [http://cocodataset.org/#explore](http://cocodataset.org/#explore)
    URL to explore the dataset. COCO Explorer is great user interface. You just need
    to select objects tags such as *I want to see images with a person, a bicycle,
    and a bus in the picture* and the explorer provides you images with a person,
    a bicycle, and a bus in it. You can refer to the following figure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用COCO探索器，我们可以探索COCO数据集。你可以使用[http://cocodataset.org/#explore](http://cocodataset.org/#explore)
    URL来探索数据集。COCO探索器是一个出色的用户界面。你只需选择对象标签，例如*我想看到图片中有人、自行车和公共汽车的图像*，探索器就会提供包含人、自行车和公共汽车的图像。你可以参考以下图示：
- en: '![The COCO dataset](img/B08394_09_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![COCO数据集](img/B08394_09_01.jpg)'
- en: 'Figure 9.1: COCO Dataset explorer snippet'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：COCO数据集探索片段
- en: In each image, the proper object boundary has been provided for each of the
    major objects. This is the main reason why this dataset is great if you want to
    build your own computer vision application from scratch.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在每张图像中，都为每个主要对象提供了适当的对象边界。这就是为什么如果你想要从头开始构建自己的计算机视觉应用，这个数据集非常棒的主要原因。
- en: Here, we are not downloading the dataset because if we need to train the models
    from scratch on this dataset, then it will require a lot of time and lots of GPUs
    in order to get good accuracy. So, we will be using a pre-trained model, and using
    Transfer Learning, we will implement real-time object detection. Now let's move
    on to the PASCAL VOC dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不是下载数据集，因为如果我们需要在这个数据集上从头开始训练模型，那么这将需要大量的时间和大量的GPU才能获得良好的准确率。因此，我们将使用预训练模型，并通过迁移学习实现实时对象检测。现在让我们继续讨论PASCAL
    VOC数据集。
- en: The PASCAL VOC dataset
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PASCAL VOC数据集
- en: PASCAL stands for Pattern Analysis, Statistical Modeling, and Computational
    Learning and VOC stands for Visual Object Classes. In this dataset, images are
    tagged for 20 classes for object detection. Action classes and person layout taster
    tagging are available as well. In the person layout taster tagging, the bounding
    box is all about the label of each part of a person (head, hands, feet). You can
    refer to the details of this dataset at [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL代表模式分析、统计建模和计算学习，VOC代表视觉对象类别。在这个数据集中，图像被标记为20个类别用于对象检测。动作类别和人物布局标签也是可用的。在人物布局标签中，边界框是关于人体每个部分的标签（头部、手、脚）。你可以在这个网页上找到有关此数据集的详细信息：[http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)。
- en: PASCAL VOC classes
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PASCAL VOC类别
- en: 'Images have been categorized into four major classes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已被分为四个主要类别：
- en: Person
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类
- en: Animal
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动物
- en: Vehicle
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆
- en: Indoor
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 室内
- en: 'Each image is tagged with the preceding major classes, plus there are specific
    tags given to the objects in the images. Each of the preceding four categories
    has specific tags, which I have described in the following list:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像都标记了前面的主要类别，并且还为图像中的对象提供了特定的标签。前四个类别都有特定的标签，以下列表中我将描述它们：
- en: '**Person**: person'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类**：人'
- en: '**Animal**: bird, cat, cow, dog, horse, sheep'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动物**：鸟、猫、牛、狗、马、羊'
- en: '**Vehicle**: aero plane, bicycle, boat, bus, car, motorbike, train'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**车辆**：飞机、自行车、船、公共汽车、汽车、摩托车、火车'
- en: '**Indoor**: bottle, chair, dining table, potted plant, sofa, tv/monitor'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**室内**：瓶子、椅子、餐桌、盆栽植物、沙发、电视/显示器'
- en: 'You can refer to the following figure, which will help you understand the tagging
    in this PASCAL VOC dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图示，它将帮助您理解这个PASCAL VOC数据集中的标签：
- en: '![PASCAL VOC classes](img/B08394_09_02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![PASCAL VOC类别](img/B08394_09_02.jpg)'
- en: 'Figure 9.2: PASCAL VOC tagged image example'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：PASCAL VOC标记图像示例
- en: 'As you can see in the preceding figure, two major classes have been tagged:
    Person and Animal. Specific tagging has been given for objects appearing in the
    image, that is, the person and the sheep. We are not downloading this dataset;
    we will be using the pre-trained model, which was trained using this dataset.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的图示中看到的，已经标记了两个主要类别：人和动物。对于图像中出现的特定对象，即人和羊，已经给出了特定的标记。我们不会下载这个数据集；我们将使用使用此数据集训练的预训练模型。
- en: You have heard the terms Transfer Learning and pre-trained model a lot until
    now. Let's understand what they are.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，您已经多次听说过迁移学习和预训练模型这两个术语。让我们了解它们是什么。
- en: Transfer Learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: 'In this section, we will look at what Transfer Learning is and how it is going
    to be useful for us as we build real-time object detection. We divide this section
    into the following parts:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨迁移学习是什么，以及它在我们构建实时目标检测时将如何对我们有用。我们将本节分为以下部分：
- en: What is Transfer Learning?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: What is a pre-trained model?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是预训练模型？
- en: Why should we use a pre-trained model?
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为什么要使用预训练模型？
- en: How can we use the pre-trained model?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用预训练模型？
- en: Let's start with the first question.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个问题开始。
- en: What is Transfer Learning?
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: 'We will be looking at the intuition behind Transfer Learning first and, then,
    we will cover its technical definition. Let me explain this concept through a
    simple teacher-student analogy. A teacher has many years of experience in teaching
    certain specific topics or subjects. Whatever information the teacher has, they
    deliver it to their students. So, the process of teaching is all about transferring
    knowledge from the teacher to the student. You can refer to the following figure:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将探讨迁移学习的直觉，然后我们将介绍其技术定义。让我通过一个简单的师生类比来解释这个概念。一位教师拥有多年教授某些特定主题或学科的经验。无论教师拥有什么信息，他们都会传授给学生。因此，教学的过程就是将知识从教师转移到学生的过程。您可以参考以下图示：
- en: '![What is Transfer Learning?](img/B08394_09_03.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![什么是迁移学习？](img/B08394_09_03.jpg)'
- en: 'Figure 9.3: An overview of Transfer Learning'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：迁移学习概述
- en: Now, remember this analogy; we will apply it to neural networks. When we train
    a neural network, it gains knowledge from the given dataset. This trained neural
    network has some weights that help it learn from the given dataset; after training,
    we can store these weights in a binary format. The weight that we have stored
    in the binary format can be extracted and then transferred to any other neural
    network. So, instead of training the neural network from scratch, we transfer
    the knowledge that the previously trained model gained. We are transferring the
    learned features to the new neural network and this will save a lot of our time.
    If we have an already trained model for a particular application, then we will
    apply it to the new but similar type of application, which in turn will help save
    time.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请记住这个类比；我们将将其应用于神经网络。当我们训练一个神经网络时，它会从给定的数据集中获得知识。这个经过训练的神经网络有一些权重，帮助它从给定的数据集中学习；训练后，我们可以将这些权重以二进制格式存储。我们以二进制格式存储的权重可以被提取出来，然后转移到任何其他神经网络。因此，我们不是从头开始训练神经网络，而是转移先前训练的模型获得的知识。我们将学习到的特征转移到新的神经网络中，这将节省我们大量的时间。如果我们已经为特定应用训练了一个模型，那么我们将将其应用于新的但类似类型的应用，这将反过来帮助我们节省时间。
- en: Now, it's time to define Transfer Learning in more technical terms. Transfer
    Learning is a research problem in Machine Learning that focuses on storing knowledge
    gained while solving a particular problem and applying it to a different but related
    problem. Sometimes, Transfer Learning is also called inductive transfer. Let's
    take a solid example to solidify your vision. If we build a Machine Learning model
    that gained knowledge while learning to recognize cars, it can also be applied
    when we are trying to recognize trucks. In this chapter, we are using Transfer
    Learning in order to build this real-time object detection application.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候用更技术性的术语来定义迁移学习了。迁移学习是机器学习中的一个研究问题，它关注于在解决特定问题时获得的知识存储，并将其应用于不同但相关的问题。有时，迁移学习也被称为归纳迁移。让我们用一个具体的例子来巩固你的理解。如果我们构建一个在识别汽车时获得知识的机器学习模型，它也可以在我们尝试识别卡车时应用。在本章中，我们使用迁移学习来构建这个实时物体检测应用程序。
- en: What is a pre-trained model?
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是预训练模型？
- en: I want to give you a simple explanation. A pre-trained model is one that is
    created and built by someone else to solve a specific problem. This means that
    we are using a model which has already been trained and plugging and playing with
    it. By using the pre-trained model, we can build new applications with similar
    domains.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我想给你一个简单的解释。预训练模型是由其他人创建和构建来解决特定问题的。这意味着我们正在使用一个已经训练好的模型，并对其进行插入和播放。通过使用预训练模型，我们可以构建具有相似领域的新的应用程序。
- en: 'Let me give you an example: suppose we want to create a self-driving car. In
    order to build it, our first step would be to build a decent object recognition
    system. You can spend a year or more to build the decent image and object recognition
    algorithm from scratch, or you can use a pre-trained model, such as the Google
    inception model or the YOLO model, which has been built using the PASCAL VOC dataset.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你举一个例子：假设我们想要创建一辆自动驾驶汽车。为了构建它，我们的第一步将是构建一个不错的物体识别系统。你可以花费一年或更长时间从头开始构建一个不错的图像和物体识别算法，或者你可以使用预训练模型，例如使用PASCAL
    VOC数据集构建的Google Inception模型或YOLO模型。
- en: 'There are some advantages of using a pre-trained model, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型有一些优势，如下所示：
- en: A pre-trained model may not give you 100% accuracy, but it saves a lot of effort.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型可能不会给你100%的准确率，但它可以节省大量的努力。
- en: You can optimize the accuracy of the real problem on which you are working rather
    than making an algorithm from scratch; as we say sometimes, there is no need to
    reinvent the wheel.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以优化你正在工作的真实问题的准确度，而不是从头开始制作一个算法；正如我们有时所说的，没有必要重新发明轮子。
- en: There are many libraries available that can help us save trained models, so
    we can load and use them easily whenever we need them.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在有众多库可以帮助我们保存训练好的模型，这样我们就可以在需要时轻松加载和使用它们。
- en: Apart from the advantages the pre-trained models provide us, we need to understand
    other real reasons why we should use pre-trained models. So, let's discuss that
    in our next section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预训练模型为我们提供的优势外，我们还需要了解其他真实的原因，为什么我们应该使用预训练模型。那么，让我们在下一节中讨论这个问题。
- en: Why should we use a pre-trained model?
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们应该使用预训练模型？
- en: When we are focusing on developing the existing algorithms in a different manner,
    our goal would be to build an algorithm that would outperform every other existing
    algorithm and making it more efficient. If we just focus on the research part,
    then this can be a nice approach to develop an efficient and accurate algorithm,
    but if your vision is to make an application and this algorithm is one part of
    the entire application, then you should focus on how quickly and efficiently you
    can build the application. Let's understand this through an example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们专注于以不同方式开发现有算法时，我们的目标将是构建一个能够超越所有其他现有算法并使其更高效的算法。如果我们只关注研究部分，那么这可以是一个开发高效且精确算法的好方法，但如果你的目标是开发一个应用程序，而这个算法是整个应用程序的一部分，那么你应该关注你如何快速且高效地构建应用程序。让我们通过一个例子来理解这一点。
- en: In this chapter, we want to build real-time object detection techniques. Now,
    my primary focus would be on building an application that would detect objects
    in real time. Not just that; I need to combine object detection and real-time
    tracking activity as well. If I ignore my primary goal and start making the new
    but effective object detection algorithm, then I will lose focus. My focus will
    be on building the entire real-time object detection application and not just
    a certain algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们想要构建实时目标检测技术。现在，我的主要焦点将是构建一个可以实时检测对象的应用程序。不仅如此；我还需要结合目标检测和实时跟踪活动。如果我忽略我的主要目标，开始制作新的但有效的目标检测算法，那么我会失去专注。我的焦点将是构建整个实时目标检测应用程序，而不仅仅是某个特定的算法。
- en: Sometimes, if we lose focus and try to build the algorithm from scratch, then
    it will take a lot of time. We will also waste our efforts, because some smart
    people in the world may have already built it for us. When we use this already
    developed algorithm, we will save time and focus on our application. After building
    a prototype of our actual application, we will have time to improvise it so that
    it can be used by many other people.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，如果我们失去了专注，试图从头开始构建算法，那么这将会花费很多时间。我们也会浪费我们的努力，因为世界上可能已经有聪明的人为我们构建了它。当我们使用这个已经开发的算法时，我们将节省时间并专注于我们的应用。在构建我们实际应用的原型之后，我们将有时间对其进行改进，以便它可以被许多人使用。
- en: Let me tell you my story. I started building an image detection application
    that could be used in the security domain a couple of months ago. At that time,
    I didn't want to use a pre-trained model, because I wanted to explore how much
    effort it would take to build an algorithm from scratch. So, I started making
    one on my own. I tried several different algorithms, such as SVM, **Multilayer
    Perceptron** (**MLP**), and **Convolution Neural Network** (**CNN**) models, but
    I got really low accuracy. I lost focus on building an image-detection algorithm
    application that could be used in the security domain and just started focusing
    on making the algorithm better. After some time, I realized that it would be better
    if I used a pre-trained model with an optimization technique that would save my
    time and enable me to build a better solution. After this experience, I tried
    to explore the option of using Transfer Learning in problem statements I was solving,
    and if I found that there would be no scope for Transfer Learning, then I would
    make the algorithms from scratch; otherwise, I preferred using the pre-trained
    model. So, always explore all the options before building the algorithm. Understand
    the application usage and build your solution based on that. Suppose you are building
    your own self-driving car; then, real-time object detection would become an important
    part of a self-driving car, but it would be just a part of the entire application;
    therefore, it would be better if we were to use a pre-trained model to detect
    objects, so that we could use our time to build a quality self-driving car.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我告诉你我的故事。几个月前，我开始构建一个可以用于安全领域的图像检测应用。当时，我不想使用预训练模型，因为我想要探索从头开始构建算法需要多少努力。所以，我开始自己动手做。我尝试了多种不同的算法，例如SVM、**多层感知器**（**MLP**）和**卷积神经网络**（**CNN**）模型，但准确率真的很低。我失去了构建一个可以用于安全领域的图像检测算法应用的专注，只是开始专注于使算法更好。过了一段时间，我意识到，如果使用一个带有优化技术的预训练模型，将节省我的时间并使我能够构建一个更好的解决方案，那会更好。这次经历之后，我试图探索在我解决的问题陈述中使用迁移学习的选项，如果我发现没有迁移学习的空间，那么我会从头开始构建算法；否则，我更愿意使用预训练模型。所以，在构建算法之前，总是探索所有选项。了解应用的使用情况，并基于此构建你的解决方案。假设你正在构建自己的自动驾驶汽车；那么，实时目标检测将成为自动驾驶汽车的一个重要组成部分，但它只是整个应用的一部分；因此，如果我们使用预训练模型来检测对象，那么我们可以用我们的时间来构建一个高质量的自动驾驶汽车。
- en: How can we use a pre-trained model?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何使用预训练模型？
- en: 'Generally, a pre-trained model is in the binary format, which can be downloaded
    and then used in our applications. Some libraries, such as Keras, TensorFlow,
    Darknet, and so on, already have those pre-models that you can load and use with
    certain available APIs. These pre-trained networks have the ability to generalize
    images that are not part of the PASCAL VOC or COCO dataset via Transfer Learning.
    We can modify the pre-existing model by fine-tuning the model. We don''t want
    to modify the weights too much, because it has been trained on a large dataset
    using lots of GPUs. The pre-trained model has the ability to generalize the prediction
    and classification of objects, so we know that this pre-trained model can be generalized
    enough to give us the best possible outcome. However, we can change some hyperparameters
    if we want to train the model from scratch. These parameters can be the learning
    rate, epochs, layer size, and so on. Let''s discuss some of them:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，预训练模型是二进制格式，可以下载后用于我们的应用程序。一些库，如Keras、TensorFlow、Darknet等，已经包含了那些可以加载和使用的预模型，你可以通过某些可用的API来使用这些预训练网络。这些预训练网络具有通过迁移学习泛化PASCAL
    VOC或COCO数据集之外图像的能力。我们可以通过微调模型来修改预存在的模型。我们不想修改权重太多，因为它是在大量数据集上使用大量GPU训练的。预训练模型具有泛化预测和对象分类的能力，因此我们知道这个预训练模型可以泛化到足够好的程度，以给出最佳的可能结果。然而，如果我们想从头开始训练模型，我们可以改变一些超参数。这些参数可以是学习率、周期数、层大小等。让我们讨论其中的一些：
- en: 'Learning Rate: Learning rate basically controls how much we should update the
    weights of neurons. We can use fixed learning rate, decreassing learning rate,
    momentum-based methods, or adaptive learning rates.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：学习率基本上控制了我们应该更新神经元权重多少。我们可以使用固定学习率、递减学习率、基于动量的方法或自适应学习率。
- en: 'Number of epochs: The number of epochs indicates the number of times the entire
    training dataset should pass through the neural network. We need to increase the
    number of epochs in order to decrease gap between the test error and the training
    error.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周期数：周期数表示整个训练数据集应该通过神经网络的次数。我们需要增加周期数以减少测试误差和训练误差之间的差距。
- en: 'Batch Size: For convolutional neural networks, mini-batch size is usually more
    preferable. A range of 16 to 128 is really a good choice to start from for convolutional
    neural networks.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小：对于卷积神经网络，小批量大小通常更可取。对于卷积神经网络来说，从16到128的范围确实是一个很好的起点。
- en: 'Activation function: As we know, activation functions introduce non-linearity
    to the model. ReLU activation function is the first choice for convolutional neural
    networks. You can use other activation functions, such as tanh, sigmoid, and so
    on, as well.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数：正如我们所知，激活函数向模型引入非线性。ReLU激活函数是卷积神经网络的第一个选择。你也可以使用其他激活函数，如tanh、sigmoid等。
- en: 'Dropout for regularization: Regularization techniques are used to prevent overfitting
    problems. Dropout is the regularization technique for deep neural networks. In
    this technique, we are dropping out some of the neurons or units in neural networks.
    The drop out of neurons is based on probability value. The default value for this
    is 0.5, which is good choice to start with, but we can change the value for regularization
    after observing training error and testing error.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout用于正则化：正则化技术用于防止过拟合问题。Dropout是深度神经网络的正则化技术。在这个技术中，我们会丢弃神经网络中的一些神经元或单元。神经元的丢弃基于概率值。默认值为0.5，这是一个很好的起点，但我们可以根据观察到的训练误差和测试误差来改变这个值以进行正则化。
- en: Here, we will be using pre-trained models such as the Caffe pre-trained model,
    the TensorFow object detraction model, and **You Only Look Once** (**YOLO**).
    For real-time streaming from our webcam, we are using OpenCV, which is also useful
    in order to draw bounding boxes. So first, let's set up the OpenCV environment.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用预训练模型，如Caffe预训练模型、TensorFlow目标检测模型和**你只需看一眼**（**YOLO**）。对于从我们的摄像头进行实时流，我们使用OpenCV，这对于绘制边界框也非常有用。所以，首先，让我们设置OpenCV环境。
- en: Setting up the coding environment
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置编码环境
- en: In this section, we will list down the libraries and equipment you need in order
    to run the upcoming code. You need to have a webcam that can at least stream the
    video with good clarity. We will be using OpenCV, TensorFlow, YOLO, Darkflow,
    and Darknet libraries. I'm not going to explain how to install TensorFlow, because
    it is an easy process and you can find the documentation for the installation
    by clicking on [https://www.tensorflow.org/install/install_linux](https://www.tensorflow.org/install/install_linux).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将列出运行即将到来的代码所需的库和设备。你需要有一个至少可以以良好清晰度流式传输视频的摄像头。我们将使用 OpenCV、TensorFlow、YOLO、Darkflow
    和 Darknet 库。我不会解释如何安装 TensorFlow，因为它是一个简单的过程，你可以通过点击[https://www.tensorflow.org/install/install_linux](https://www.tensorflow.org/install/install_linux)来找到安装文档。
- en: In this section, we will be looking at how to set up OpenCV first and, in the
    upcoming sections, we will see how to set up YOLO, Darkflow, and DarkNet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何首先设置 OpenCV，在接下来的章节中，我们将看到如何设置 YOLO、Darkflow 和 DarkNet。
- en: Setting up and installing OpenCV
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和安装 OpenCV
- en: 'OpenCV stands for Open Source Computer Vision. It is designed for computational
    efficiency, with a strong focus on real-time applications. In this section, you
    will learn how to set up OpenCV. I''m using Ubuntu 16.04 and I have a GPU, so
    I have already installed CUDA and CUDNN. If you haven''t installed CUDA and CUDNN,
    then you can refer to this GitHub link: [https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64](https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64).
    Once you are done with that, start executing the following steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 代表开源计算机视觉。它旨在提高计算效率，重点在于实时应用。在本节中，你将学习如何设置 OpenCV。我使用的是 Ubuntu 16.04，并且我有一个
    GPU，因此我已经安装了 CUDA 和 CUDNN。如果你还没有安装 CUDA 和 CUDNN，那么你可以参考这个 GitHub 链接：[https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64](https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64)。完成这些后，开始执行以下步骤：
- en: 'This will update the software and libraries: `$ sudo apt-get update`'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将更新软件和库：`$ sudo apt-get update`
- en: 'This will upgrade the OS and install OS-level updates: `$ sudo apt-get upgrade`'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将升级操作系统并安装操作系统级别的更新：`$ sudo apt-get upgrade`
- en: 'This is for compiling the software: `$ sudo apt-get install build-essential`'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是用于编译软件的：`$ sudo apt-get install build-essential`
- en: 'This command installs prerequisites for OpenCV: `$ sudo apt-get install cmake
    git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev`'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个命令安装 OpenCV 的先决条件：`$ sudo apt-get install cmake git libgtk2.0-dev pkg-config
    libavcodec-dev libavformat-dev libswscale-dev`
- en: 'This command installs optional prerequisites for OpenCV: `$ sudo apt-get install
    python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev
    libjasper-dev libdc1394-22-dev`'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个命令安装 OpenCV 的可选先决条件：`$ sudo apt-get install python-dev python-numpy libtbb2
    libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev`
- en: 'Create a directory using this command: `$ sudo mkdir ~/opencv`'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此命令创建一个目录：`$ sudo mkdir ~/opencv`
- en: 'Jump to the directory that we just created: `$ cd ~/opencv`'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳转到我们刚刚创建的目录：`$ cd ~/opencv`
- en: 'Clone the following OpenCV projects from GitHub inside the opencv directory:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在 opencv 目录内从 GitHub 克隆以下 OpenCV 项目:'
- en: '`$ sudo git clone https://github.com/opencv/opencv.git`'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`$ sudo git clone https://github.com/opencv/opencv.git`'
- en: '`$ sudo git clone https://github.com/opencv/opencv_contrib.git`'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`$ sudo git clone https://github.com/opencv/opencv_contrib.git`'
- en: 'Inside the opencv folder, create another directory named build: `$ sudo mkdir
    ~/opencv/build`'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在 opencv 文件夹内，创建一个名为 build 的子目录: `$ sudo mkdir ~/opencv/build`'
- en: 'Jump to the build directory or folder: `$ cd ~/opencv/build`'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '跳转到 build 目录或文件夹: `$ cd ~/opencv/build`'
- en: 'Once you are in the build folder location, run this command. It may take some
    time. If you run this command without any error, then proceed to the next step:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你处于 build 文件夹位置，运行此命令。这可能需要一些时间。如果你运行此命令没有错误，则继续下一步：
- en: '[PRE0]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Identify how many CPU cores you have by using this command.: `$ nproc`'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此命令识别你的 CPU 核心数：`$ nproc`
- en: Once you know the number of cores of the CPU, you can use it to process multi-threading.
    I have allocated four CPU cores, which means there are four threads running simultaneously.
    The command for this is `$ make -j4` and it compiles all of the classes written
    in C for OpenCV.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你知道 CPU 的核心数，你可以用它来处理多线程。我分配了四个 CPU 核心，这意味着有四个线程同时运行。这个命令是 `$ make -j4`，它会编译
    OpenCV 中用 C 编写的所有类。
- en: 'Now, execute this command for the actual installation of OpenCV: `$ sudo make
    install`'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '现在，执行以下命令以实际安装 OpenCV: `$ sudo make install`'
- en: 'Add the path to the configuration file: `$ sudo sh -c ''echo "/usr/local/lib"
    >> /etc/ld.so.conf.d/opencv.conf''`'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将配置文件的路径添加到配置文件中：`$ sudo sh -c 'echo "/usr/local/lib" >> /etc/ld.so.conf.d/opencv.conf'`
- en: 'Check for the proper configuration using this command: `$ sudo ldconfig`'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此命令检查适当的配置：`$ sudo ldconfig`
- en: Once you have installed OpenCV successfully, we can use this library to stream
    real-time video. Now, we will start building our baseline model. Let's begin!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功安装了OpenCV，我们就可以使用这个库来流式传输实时视频。现在，我们将开始构建我们的基线模型。让我们开始吧！
- en: Features engineering for the baseline model
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: In order to build the baseline model, we will use the Caffe implementation of
    the Google MobileNet SSD detection network with pre-trained weights. This model
    has been trained on the PASCAL VOC dataset. So, in this section, we will look
    at the approach with which this model has been trained by Google. We will understand
    the basic approach behind MobileNet SSD and use the pre-trained model to help
    save time. To create this kind of accurate model, we need to have lots of GPUs
    and training time, so we are using a pre-trained model. This pre-trained MobileNet
    model uses **Convolution Neural Net** (**CNN**).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建基线模型，我们将使用预训练权重的Google MobileNet SSD检测网络的Caffe实现。这个模型已经在PASCAL VOC数据集上进行了训练。因此，在本节中，我们将探讨谷歌如何训练这个模型的方法。我们将了解MobileNet
    SSD背后的基本方法，并使用预训练模型来帮助节省时间。为了创建这种精确的模型，我们需要大量的GPU和训练时间，所以我们使用预训练模型。这个预训练的MobileNet模型使用**卷积神经网络**（**CNN**）。
- en: 'Let''s look at how the features have been extracted by the MobileNet using
    CNN. This will help us understand the basic idea behind CNN, as well as how MobileNet
    has been used. The CNN network is made of layers and, when we provide the images
    to CNN, it scans the region of the images and tries to extract the possible objects
    using the region proposal method. Then, it finds the region with objects, uses
    the warped region, and generates the CNN features. These features can be the position
    of the pixels, edges, the length of the edges, the texture of the images, the
    scale of the region, the lightness or darkness of the picture, object parts, and
    so on. The CNN network learns these kinds of features by itself. You can refer
    to the following figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看MobileNet如何使用CNN提取特征。这将帮助我们理解CNN背后的基本思想，以及MobileNet是如何被使用的。CNN网络由层组成，当我们向CNN提供图像时，它会扫描图像的区域，并尝试使用区域提议方法提取可能的物体。然后，它找到包含物体的区域，使用扭曲的区域，并生成CNN特征。这些特征可以是像素的位置、边缘、边缘的长度、图像的纹理、区域的尺度、图片的明暗度、物体部分等等。CNN网络通过自身学习这些类型的特征。您可以参考以下图示：
- en: '![Features engineering for the baseline model](img/B08394_09_04.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![基线模型的特征工程](img/B08394_09_04.jpg)'
- en: 'Figure 9.4: Understanding features extraction in CNN'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：理解CNN中的特征提取
- en: As you can see in the preceding image, the region of the image has been scanned
    and the first CNN layer, made up of C1 and S1, will generate the features. These
    features can identify the representation of edges that eventually build the whole
    object. In the second stage, CNN layers learn the feature representation that
    can help the neural network identify parts of the objects. In the last stage,
    it learns all the features that are necessary in order to identify the objects
    present in the given input images. If you want to explore each and every aspect
    of the CNN network, you can refer to [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/).
    Don't worry; we will cover the overview of the CNN architecture in the upcoming
    section, so that you can understand how object detection will work. Now, it's
    time to explore CNN.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的图像中看到的，图像区域已经被扫描，由C1和S1组成的第一个CNN层将生成特征。这些特征可以识别构成整个物体的边缘表示。在第二阶段，CNN层学习有助于神经网络识别物体部分的特征表示。在最后阶段，它学习所有必要的特征，以便在给定的输入图像中识别存在的物体。如果您想探索CNN网络的每个方面，可以参考[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)。不用担心；我们将在下一节中概述CNN架构，以便您了解目标检测是如何工作的。现在，是时候探索CNN了。
- en: Selecting the machine learning algorithm
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择机器学习算法
- en: 'We already know that we are using **Convolution Neural Networks** (**CNN**)
    for developing this application. You might wonder why we have chosen CNN and not
    another neural net. You might already know the answer to this question. There
    are three reasons why we have chosen CNN:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道我们正在使用**卷积神经网络**（**CNN**）来开发这个应用程序。你可能想知道为什么我们选择了CNN而不是其他神经网络。你可能已经知道这个问题的答案。我们选择CNN有三个原因：
- en: The amount of visual data present nowadays, which is carefully hand-labeled
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现今存在的、经过精心手工标注的视觉数据量
- en: The affordable computation machines through which GPUs open the door for optimization
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过GPU打开优化大门的负担得起的计算机器
- en: The various kinds of architecture of CNN outperforms the other algorithms
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN的各种架构优于其他算法
- en: Due to these reasons, we have chosen the CNN with SSD. During the development
    of the baseline model, we will be using MobileNet, which uses CNN with **Single
    Shot Detector** (**SSD**) techniques underneath. So, in this section, we will
    look at the architecture of the CNN used during the development of the MobileNet.
    This will help us understand the pre-trained model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，我们选择了带有SSD的CNN。在开发基线模型的过程中，我们将使用MobileNet，它使用带有**单次检测器**（**SSD**）技术的CNN。因此，在本节中，我们将查看开发MobileNet时使用的CNN架构。这将帮助我们理解预训练模型。
- en: Architecture of the MobileNet SSD model
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MobileNet SSD模型的架构
- en: MobileNet SSD is fast and does the job of object detection in images and video
    well. This model is faster than **Region-based Convolution Neural Network** (**R-CNN**).
    SSD achieves this speed because it scans images and video frames quite differently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet SSD速度快，在图像和视频中执行对象检测任务很好。这个模型比**基于区域的卷积神经网络**（**R-CNN**）更快。SSD之所以能达到这种速度，是因为它以不同的方式扫描图像和视频帧。
- en: 'In R-CNN, models performed region proposals and region classifications in two
    different steps, given as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在R-CNN中，模型在两个不同的步骤中执行区域提议和区域分类，如下所示：
- en: First, they used a region proposal network in order to generate regions of interest
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，他们使用区域提议网络来生成感兴趣的区域
- en: After that, fully connected layers or positive sensitive constitutional layers
    classified the objects in the selected regions.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，全连接层或正敏感宪法层对所选区域中的对象进行分类。
- en: 'These steps are at par with the R-CNN, but SSD performs them in a single shot,
    which means it simultaneously predicts the bounding boxes and the classes of the
    objects appearing in the bounding boxes. SDD performs the following steps when
    an image or video streams and set of basic truth labels has been given as the
    input:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤与R-CNN相当，但SSD在单次操作中完成这些步骤，这意味着它同时预测边界框和边界框中出现的对象的类别。SDD在图像或视频流以及一组基本真实标签作为输入时执行以下步骤：
- en: Pass the images through the series of convolution layers that generate the sets
    of features map in the form of a different size of matrix. The output can be in
    the form of a 10×10 matrix, a 6×6 matrix, or a 3×3 matrix.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像通过一系列卷积层，生成不同尺寸矩阵形式的特征图集合。输出可以是10×10矩阵、6×6矩阵或3×3矩阵。
- en: For each location in each of the feature maps, use the 3×3 convolution filter
    in order to evaluate the small set of default bound boxes. These generated default
    bounding boxes are equivalent to anchor boxes, which are generated using Faster
    R-CNN.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个特征图中的每个位置，使用3×3卷积滤波器来评估一组默认边界框。这些生成的默认边界框相当于锚框，这些锚框是使用Faster R-CNN生成的。
- en: For each box, simultaneously predict the bounding box offset and the class probability
    for objects.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个框，同时预测对象的边界框偏移和类别概率。
- en: During the training, match the ground truth box with these predicted boxes.
    This matching is performed by using Intersection of Union (IoU). The best predicted
    box will be labeled as a positive bounding box. This happens to every boundary
    box. An IoU with more than 50% truth value has been considered here.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，将真实框与这些预测框进行匹配。这种匹配是通过使用交集并集（IoU）来完成的。最佳预测框将被标记为正边界框。这发生在每个边界框上。这里考虑的IoU的真实值超过50%。
- en: Take a look at the following figure for a graphical representation. MobileNets
    have streamlined architecture that uses depth-wise separable convolutions in order
    to build light weight deep neural networks for mobile and embedded vision application.
    MobileNets are a more efficient ML model for computer vision application. You
    can also refer to the original paper for MobileNet at [https://arxiv.org/pdf/1704.04861.pdf](https://arxiv.org/pdf/1704.04861.pdf).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下图，以图形方式表示。MobileNets具有简化的架构，使用深度可分离卷积来构建适用于移动和嵌入式视觉应用的超轻量级深度神经网络。MobileNets是计算机视觉应用中更高效的机器学习模型。您也可以参考MobileNet的原始论文，链接为[https://arxiv.org/pdf/1704.04861.pdf](https://arxiv.org/pdf/1704.04861.pdf)。
- en: '![Architecture of the MobileNet SSD model](img/B08394_09_05.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![MobileNet SSD模型架构](img/B08394_09_05.jpg)'
- en: 'Figure 9.5: Basic architectural building blocks for MobileNet SSD'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：MobileNet SSD的基本架构模块
- en: 'Image Source: https://arxiv.org/pdf/1704.04861.pdf'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://arxiv.org/pdf/1704.04861.pdf
- en: 'As you can see in the preceding figure, we used the standard convolution network
    with the depth-wise convolution network. MobileNet SDD used the ReLU activation
    function. You can refer to the following figure to get an idea about what kind
    of filter shape this network has:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们使用了标准的卷积网络和深度卷积网络。MobileNet SDD使用了ReLU激活函数。您可以通过以下图了解该网络具有什么样的滤波器形状：
- en: '![Architecture of the MobileNet SSD model](img/B08394_09_06.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![MobileNet SSD模型架构](img/B08394_09_06.jpg)'
- en: Figure 9.6, MobileNet body architecture
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6，MobileNet主体架构
- en: 'Image Source: https://arxiv.org/pdf/1704.04861.pdf'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://arxiv.org/pdf/1704.04861.pdf
- en: 'If you want to interpret this table, then let''s consider an example. If we
    have an original image with a pixel sixe of 224×224, then this Mobilenet network
    shrinks the image down to 7×7 pixels; it also has 1,024 channels. After this,
    there is an average pooling layer that works on all the images and generates the
    vector of a 1×1×1,024 size, which is just a vector of 1,024 elements in reality.
    If you want to learn more about MobileNet SSD, refer to the following resources:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想解释这个表格，那么让我们考虑一个例子。如果我们有一个原始图像，像素大小为224×224，那么这个Mobilenet网络将图像缩小到7×7像素；它还有1,024个通道。之后，有一个平均池化层作用于所有图像，生成一个1×1×1,024大小的向量，实际上就是一个包含1,024个元素的向量。如果您想了解更多关于MobileNet
    SSD的信息，请参考以下资源：
- en: '[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)'
- en: '[https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)'
- en: '[https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19](https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19](https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19)'
- en: '[http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/](http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/](http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)'
- en: Now, let's move on to the implementation part.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续到实现部分。
- en: Building the baseline model
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基线模型
- en: 'In this section, we will be looking at the coding part. You can refer to the
    code given at this GitHub link: [https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model](https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨编码部分。您可以参考以下GitHub链接中的代码：[https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model](https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model)。
- en: First, download the project from the given link and install OpenCV, as per the
    information given earlier in this chapter. When you download this project folder,
    there is a pre-trained MobileNet SSD that has been implemented using the caffe
    library, but here, we are using the pre-trained binary model. We are using OpenCV
    for loading the pre-trained model as well as streaming the video feeds from the
    webcam.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从提供的链接下载项目并安装OpenCV，如本章前面所述的信息。当您下载此项目文件夹时，其中包含一个使用Caffe库实现的预训练的MobileNet
    SSD，但在这里，我们使用预训练的二进制模型。我们使用OpenCV加载预训练模型以及从摄像头流式传输视频流。
- en: 'In the code, first, we specify the libraries that we need to import and define
    the command-line arguments that will be used to run the script. We need to provide
    the parameter file and the pre-trained model. The name of the parameter file is
    `MobileNetSSD_deploy.prototxt.txt` and the filename for the pre-trained model
    is `MobileNetSSD_deploy.caffemodel`. We have also defined the classes that can
    be identified by the model. After this, we will load the pre-trained model using
    OpenCV. You can refer to the coding up to this stage in the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，首先，我们指定需要导入的库和定义用于运行脚本的命令行参数。我们需要提供参数文件和预训练模型。参数文件的名称是`MobileNetSSD_deploy.prototxt.txt`，预训练模型的文件名是`MobileNetSSD_deploy.caffemodel`。我们还定义了模型可以识别的类别。之后，我们将使用OpenCV加载预训练模型。您可以在以下屏幕截图中参考此阶段的编码：
- en: '![Building the baseline model](img/B08394_09_07.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![构建基线模型](img/B08394_09_07.jpg)'
- en: 'Figure 9.7: Code snippet for the baseline model'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：基线模型的代码片段
- en: 'Now, let''s look at how we can stream the video from our webcam. Here, we are
    using library `imutils` and its video API to stream the video from the webcam.
    Using the start function, we will start the streaming and, after that, we will
    define the frame size. We grab the frame size and convert it into a blob format.
    This code always verifies that the detected object confidence score will be higher
    than the minimum confidence score or the minimum threshold of the confidence score.
    Once we get a higher confidence score, we will draw the bounding box for those
    objects. We can see the objects that have been detected so far. You can refer
    to the following figure for the video streaming of the baseline model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何从我们的网络摄像头中流式传输视频。在这里，我们使用`imutils`库及其视频API从网络摄像头流式传输视频。使用start函数，我们将开始流式传输，然后定义帧大小。我们获取帧大小并将其转换为blob格式。此代码始终验证检测到的对象置信度分数将高于最小置信度分数或置信度分数的最小阈值。一旦我们得到更高的置信度分数，我们将为这些对象绘制边界框。我们可以看到到目前为止已经检测到的对象。您可以参考以下图中的基线模型视频流：
- en: '![Building the baseline model](img/B08394_09_08.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![构建基线模型](img/B08394_09_08.jpg)'
- en: 'Figure 9.8: Code snippet for the baseline model video streaming'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：基线模型视频流的代码片段
- en: 'In order to stop the streaming, we need to break the loop by pressing Q or
    Ctrl + C and we need to take care that when we close the program, all windows
    and processes will stop appropriately. You can see this in the following screenshot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了停止流式传输，我们需要通过按Q键或Ctrl + C来中断循环，并且我们需要注意，当我们关闭程序时，所有窗口和进程都将适当地停止。您可以在以下屏幕截图中看到这一点：
- en: '![Building the baseline model](img/B08394_09_09.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![构建基线模型](img/B08394_09_09.jpg)'
- en: 'Figure 9.9: Code snippet for ending the script'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：结束脚本的代码片段
- en: Before we run the testing of the script, let's understand the testing metrics
    for the object detection application. Once we understand the testing metrics,
    we will run the code as well as checking how much accuracy we are getting.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行脚本测试之前，让我们了解对象检测应用程序的测试指标。一旦我们理解了测试指标，我们将运行代码并检查我们获得了多少精度。
- en: Understanding the testing metrics
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解测试指标
- en: 'In this section, we will cover the testing metrics. We will look at the two
    matrices that will help us understand how to test the object detection application.
    These testing matrices are as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍测试指标。我们将查看两个矩阵，这两个矩阵将帮助我们理解如何测试对象检测应用程序。这些测试矩阵如下：
- en: Intersection over Union (IoU)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交并比 (IoU)
- en: mean Average Precision (mAP)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均精度均值 (mAP)
- en: Intersection over Union (IoU)
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交并比 (IoU)
- en: 'For detection, IoU is used in order to find out whether the object proposal
    is right or not. This is a regular way to determine whether object detection is
    done perfectly or not. IoU generally takes the set, A, of proposed object pixels
    and the set of true object pixels, B, and calculates IoU based on the following
    formula:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于检测，使用IoU（交并比）是为了确定对象提议是否正确。这是确定对象检测是否完美完成的一种常规方法。IoU通常取提议的对象像素集A和真实对象像素集B，并基于以下公式计算IoU：
- en: '![Intersection over Union (IoU)](img/B08394_09_30.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![交并比 (IoU)](img/B08394_09_30.jpg)'
- en: 'Generally, IoU >0.5, which means that it was a hit or that it identified the
    object pixels or boundary box for the object; otherwise, it fails. This is a more
    formal understanding of the IoU. Now, let''s look at the intuition and the meaning
    behind it. Let''s take an image as reference to help us understand the intuition
    behind this matrix. You can refer to the following screenshot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，IoU > 0.5，这意味着这是一个命中或识别了对象的像素或边界框；否则，它失败。这是对IoU的更正式理解。现在，让我们看看其背后的直觉和含义。让我们以一个图像为参考，帮助我们理解这个矩阵的直觉。您可以参考以下截图：
- en: '![Intersection over Union (IoU)](img/B08394_09_10.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![交并比 (IoU)](img/B08394_09_10.jpg)'
- en: 'Figure 9.10: Understanding the intuition behind IoU'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：理解IoU背后的直觉
- en: 'The preceding screenshot is an example of detecting a stop sign in an image.
    The predicted bounding box is drawn in red and pixels belonging to this red box
    are considered part of set A, while the ground-truth bounding box is drawn in
    green and pixels belong to this green box are considered part of set B. Our goal
    is to compute the Intersection of Union between these bounding boxes. So, when
    our application draws a boundary box, it should match the ground-truth boundary
    box at least more than 50%, which is considered a good prediction. The equation
    for IoU is given in the following figure:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图是检测图像中停车标志的一个示例。预测的边界框用红色绘制，属于这个红色框的像素被认为是集合A的一部分，而真实边界框用绿色绘制，属于这个绿色框的像素被认为是集合B的一部分。我们的目标是计算这些边界框之间的交并比。因此，当我们的应用程序绘制边界框时，它应该至少与真实边界框匹配超过50%，这被认为是一个好的预测。IoU的方程如下所示：
- en: '![Intersection over Union (IoU)](img/B08394_09_11.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![交并比 (IoU)](img/B08394_09_11.jpg)'
- en: 'Figure 9.11: Equation of IoU based on intuitive understanding'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：基于直观理解的IoU方程
- en: 'There are few chances in reality where the (x, y) coordinate of our predicted
    bounding box will exactly match the (x, y) coordinates of the ground-truth bounding
    box. In the following figure, you can see various examples for poor, good, and
    excellent IoUs:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，我们的预测边界框的(x, y)坐标与真实边界框的(x, y)坐标完全匹配的机会很少。在下面的图中，您可以看到各种关于IoU较差、较好和优秀的示例：
- en: '![Intersection over Union (IoU)](img/B08394_09_12.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![交并比 (IoU)](img/B08394_09_12.jpg)'
- en: 'Figure 9.12: Various IoU boundary box examples'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：各种IoU边界框示例
- en: IoUs help us to determine how well the application identifies the object boundaries
    and differentiates the various objects from each other. Now, it's time to understand
    the next testing metrics.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: IoU帮助我们确定应用程序识别对象边界和区分不同对象的能力。现在，是时候了解下一个测试指标了。
- en: mean Average Precision
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均精度均值
- en: 'In this section, we will cover the **mean Average Precision** (**mAP**). In
    object detection, first, we identify the object boundary box and then we classify
    it into a category. These categories have some labels, and we provide the appropriate
    label to the identified objects. Now, we need to test how well the application
    can assign these labels, which means how well we can classify the objects into
    different predefined categories. For each class, we will calculate the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍**平均精度均值**（**mAP**）。在目标检测中，首先，我们识别目标边界框，然后将其分类到某个类别。这些类别有一些标签，我们将适当的标签提供给识别出的对象。现在，我们需要测试应用程序分配这些标签的效果如何，这意味着我们如何将对象分类到不同的预定义类别中。对于每个类别，我们将计算以下内容：
- en: 'True Positive TP(c): A predicted class was C and the object actually belongs
    to class C'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性TP(c)：预测的类别是C，且该对象实际上属于类别C
- en: 'False Positive FP(c): A predicted class was C but in reality, the object does
    not belong to class C'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性FP(c)：预测的类别是C，但实际上该对象不属于类别C
- en: Average Precision for class C is given by the following equation:![mean Average
    Precision](img/B08394_09_31.jpg)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类C的平均精度由以下方程给出：![平均精度均值](img/B08394_09_31.jpg)
- en: 'So, for all the classes, we need to calculate the mAP and the equation for
    that is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于所有类别，我们需要计算mAP，其计算公式如下：
- en: '![mean Average Precision](img/B08394_09_32.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![平均精度均值](img/B08394_09_32.jpg)'
- en: 'If we want better prediction, then we need to increase the IoU from 0.5 to
    a higher value (up to 1.0, which would be perfect). We can denote this with this
    equation: mAP[@p], where p ∈ (0,1) is the IoU. mAP[@[0.5:0.95]] means that the
    mAP is calculated over multiple thresholds and then it is averaged again.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要更好的预测，那么我们需要将IoU从0.5增加到更高的值（最高可达1.0，这将是最完美的）。我们可以用这个方程表示：mAP[@p]，其中p ∈
    (0,1)是IoU。mAP[@[0.5:0.95]]表示mAP是在多个阈值上计算，然后再次平均。
- en: Now, let's test the baseline model and check the mAP for this implementation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试基线模型并检查此实现的mAP值。
- en: Testing the baseline model
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: 'In this section, we will run the baseline model. In order to run the script,
    we need to jump to the location where we put the script titled `real_time_object_detection.py`
    and, on Command Prompt, we need to execute the following command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将运行基线模型。为了运行脚本，我们需要跳转到放置名为`real_time_object_detection.py`的脚本的位置，并在命令提示符中执行以下命令：
- en: '![Testing the baseline model](img/B08394_09_13.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![测试基线模型](img/B08394_09_13.jpg)'
- en: 'Figure 9.13: Execution of the baseline approach'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13：基线方法的执行
- en: 'Take a look at the following figure. Here, I have just placed example images,
    but you can see the entire video when you run the script. Here is the link to
    see the entire video for real-time object detection using the baseline approach:
    [https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的图。在这里，我刚刚放置了示例图像，但当你运行脚本时，你可以看到整个视频。这里是使用基线方法进行实时物体检测的整个视频链接：[https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing)。
- en: '![Testing the baseline model](img/B08394_09_14.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![测试基线模型](img/B08394_09_14.jpg)'
- en: 'Figure 9.14: Output of the baseline approach (image is part of the video stream)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14：基线方法的输出（图像是视频流的一部分）
- en: Here, the mAP for the MobileNet SSD is 71.1% . You will learn how to optimize
    this approach in the upcoming section. First, we will list down the points that
    we can improve in the next iteration. So, let's jump to our next section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，MobileNet SSD的mAP为71.1%。你将在下一节中学习如何优化这种方法。首先，我们将列出下一次迭代中我们可以改进的点。所以，让我们跳到下一节。
- en: Problem with existing approach
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有方法的弊端
- en: Although the MobileNet SSD is fast and gives us good results, it still can't
    identify classes such as cup, pen, and so on. So, we need to use the pre-trained
    model that has been trained on a variety of objects. In this upcoming iteration,
    we need to use the pre-trained model, for example, the TensorFlow object detection
    API, which will able to identify the different objects compared to the baseline
    approach. So now, let's look at how we will optimize the existing approach.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MobileNet SSD速度快且给出了良好的结果，但它仍然无法识别像杯子、钢笔等类别。因此，我们需要使用在多种物体上训练过的预训练模型。在即将到来的迭代中，我们需要使用预训练模型，例如TensorFlow对象检测API，它将能够识别与基线方法相比的不同物体。所以现在，让我们看看我们将如何优化现有方法。
- en: How to optimize the existing approach
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何优化现有方法
- en: 'As mentioned earlier, in order to optimize the existing approach, I will be
    using the TensorFlow Object Detection API. You can refer to Google''s TensorFlow
    GitHub repo for this API at the following link: [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).
    This API is trained using the COCO dataset as well as the PASCAL VOC dataset;
    so, it will have the capability of identifying the variety of classes.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了优化现有方法，我将使用TensorFlow对象检测API。你可以通过以下链接参考Google的TensorFlow GitHub仓库中的此API：[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)。此API使用COCO数据集以及PASCAL
    VOC数据集进行训练；因此，它将具有识别多种类别的功能。
- en: Understanding the process for optimization
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解优化过程
- en: 'The most important part for us is how to use the various pre-trained models.
    The steps are as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，最重要的部分是如何使用各种预训练模型。步骤如下：
- en: 'First, pull the TensorFlow models repository using this link: [https://github.com/tensorflow/models](https://github.com/tensorflow/models)'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用此链接拉取TensorFlow模型存储库：[https://github.com/tensorflow/models](https://github.com/tensorflow/models)
- en: Once you pull the repository, you can find the iPython Notebook that I have
    referred to in order to understand how to use the pre-trained model and to find
    the link for the iPython notebook at [https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb).
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您拉取了仓库，您就可以找到我提到的iPython Notebook，以了解如何使用预训练模型，并找到iPython笔记本的链接[https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)。
- en: 'Here, SSD with MobileNet has been used, but we are using the detection model
    zoo. This model is trained on the COCO dataset and their versions are given based
    on the speed and performance of the model. You can download the pre-trained model
    from this link: [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).
    I have already placed all these parts together, so it is easy for everyone to
    implement the code.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们使用了SSD与MobileNet，但我们使用的是检测模型库。这个模型是在COCO数据集上训练的，它们的版本基于模型的速度和性能。您可以从以下链接下载预训练模型：[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)。我已经将这些部分组合在一起，因此对每个人来说实现代码都很容易。
- en: The main thing is that this model is trained using the SSD approach, but it
    has taken datasets such as the kitti dataset and the Open Image dataset. So, this
    model is able to detect more objects and is more generalized. The link for the
    Kitti dataset is [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    and the link for the Open Image dataset is [https://github.com/openimages/dataset](https://github.com/openimages/dataset).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主要的是，这个模型是使用SSD方法训练的，但它使用了如kitti数据集和Open Image数据集等数据集。因此，这个模型能够检测更多的对象，并且更加通用。Kitti数据集的链接是[http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)，Open
    Image数据集的链接是[https://github.com/openimages/dataset](https://github.com/openimages/dataset)。
- en: Once we download the repository and the pre-trained model, we will load the
    pre-trained model. In TensorFlow, as we know, the models are saved as a .pb file.
    Once we load the model, we will be using OpenCV to stream the video. In the upcoming
    section, we will be implementing the code for the revised approach.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们下载了仓库和预训练模型，我们将加载预训练模型。在TensorFlow中，众所周知，模型以.pb文件保存。一旦我们加载了模型，我们将使用OpenCV来流式传输视频。在下一节中，我们将实现修订方法的代码。
- en: Implementing the revised approach
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现修订方法
- en: 'In this section, we will understand the implementation of the revised approach.
    You can refer to this GitHub link: [https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach](https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach),
    which has the pre-trained model and the TensorFlow''s Object detection folder.
    Before we even begin with the code, I will provide information regarding the folder
    structure of this approach. You can refer to the following figure:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解修订方法的实现。您可以参考以下GitHub链接：[https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach](https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach)，其中包含预训练模型和TensorFlow的物体检测文件夹。在我们开始编写代码之前，我将提供有关此方法文件夹结构的详细信息。您可以参考以下图示：
- en: '![Implementing the revised approach](img/B08394_09_15.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![实现修订方法](img/B08394_09_15.jpg)'
- en: 'Figure 9.15: Understanding the folder structure for the revised approach'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：理解修订方法的文件夹结构
- en: 'Here is the object detection folder downloaded from the TensorFlow model repository:
    [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).
    In the `utils` folder, there are some helper functions to help us stream the video.
    The main script that helps us run the script is `object_detection_app.py`. The
    pre-trained model has been saved inside the object detection folder. The path
    for pre-trained model in this folder is this: `~/PycharmProjects/Real_time_object_detection/revised_approach/object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb`.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the coding implementation step by step. In the first step,
    we will import the dependency libraries and load the pre-trained model. You can
    refer to the following figure:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_16.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Code snippet for loading the pre-trained model in the revised
    approach'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, there are 90 different types of objects that can be identified.
    Once we load the model, the next step is the `detect_objects()` function, which
    is used to identify the objects. Once the object is identified, the bounding boxes
    for that object are drawn and we simultaneously run the pre-trained model on those
    objects, so that we can get the identification labels for the object, whether
    it''s a cup, bottle, person, and so on. You can refer to the following figure:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_17.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: Code snippet for the detect_objects() function'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we have the `worker()` function, which helps us stream the video
    as well as perform some GPU memory management. You can refer to the following
    figure:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_18.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18: Code snippet for the worker() function'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have defined the GPU memory fraction, as well as the kind
    of colors used when it detects the objects. Now, let''s look at the main function
    of the script. In the main function, we define some optional arguments and their
    default values. The list of these arguments is as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Device index of the camera: `--source=0`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Width of the frames in the video stream `--width= 500`
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Height of the frames in the video stream `--height= 500`
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of workers `--num-workers=2`
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of the queue `--queue-size=5`
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the implementation of the main function shown in the following
    figure:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_19.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.19: Code snippet for the main function'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the script, we can see the output given in the upcoming figure.
    Here, we have placed the image, but you can see the video by using this link:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_20.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.20: Output of the revised approach'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we end the video, the resources and the process should end as well. For
    that, we will be using the code that has been provided to us through the following
    figure:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the revised approach](img/B08394_09_21.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.21: Code snippet to release the resources once the script is terminated'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have executed this approach, we can identify objects such as cups, pens,
    and so on. So, we can say that our baseline approach is definitely improvised
    and there are some modified labels, such as a sofa (in this approach, identified
    as a couch). Apart from this, if we talk about the mAP of this pre-trained model,
    then as per the documentation, on the COCO dataset, this model gets around 52.4%
    accuracy. In our input, we will get around 73% accuracy. This approach identifies
    more objects with different categories, which is a great advantage.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will discuss the points that we can use to come
    up with the best possible solution.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have tried approaches that are fast and accurate, but we need an approach
    that is fast as well as accurate and optimized. These are the points we need to
    keep in our mind when we develop the best possible solution:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The SSD-based approach is great, but is not that accurate when you train your
    own model using COCO or the PASCAL VOC dataset.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow detection model zoo will have a mAP score on the COCO test dataset
    from 20 to 40%. So, we need to explore other techniques that can help us give
    a better result in terms of processing and object detection accuracy.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in the upcoming section, we will look at the approach that can help us optimize
    the revised approach.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be trying the approach named **YOLO**. YOLO stands
    for You Only Look Once. This technique gives us good accuracy, is fast, and its
    memory management is easy. This section will be divided into two parts:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Understanding YOLO
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach using YOLO
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first section, we will understand the basics about YOLO. During the implementation,
    we will be use YOLO with the pre-trained YOLO model. So, let's begin!
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Understanding YOLO
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLO is a state-of-the-art, real-time object detection system. On GPU Titan
    X, it processes images at 40-90 FPS and has a mAP on the PASCAL VOC dataset of
    78.6% and a mAP of 48.1% on the coco test-dev dataset. So, now, we will look at
    how YOLO works and processes the images in order to identify the objects. We are
    using YOLOv2 (YOLO version 2) as it is a faster version.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: The working of YOLO
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLO reframes the object detection problem. It considers the object recognition
    task as single regression problem, right from the image pixels to the bounding
    box coordinates and class probabilities. A single convolutional network simultaneously
    predicts multiple bounding boxes and class probabilities for those boxes. YOLO
    trains on full images and directly optimizes detection performance. This approach
    has several advantages compared to traditional methods, as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: YOLO is extremely fast. This is because, in YOLO, frame detection is a regression
    problem, and we do not need to use a complex pipeline.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can simply run our neural network on a new image at the time of testing to
    predict detection. Our base network runs at 45 frames per second with no batch
    processing on a Titan X GPU and a fast version runs at more than 150 fps. This
    means that we can process streaming video in real time with less than 25 milliseconds
    of latency.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO processes images globally when it makes a prediction regarding information
    about classes as well as their appearance. It also learns the generalization of
    objects.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLO divides the input image into an S×S grid. If the center of an object falls
    into a grid cell, then that grid cell is responsible for detecting the object.
    Each grid cell predicts the B bounding boxes and confidence scores for those boxes.
    These confidence scores reflect how confident the model is that the box contains
    an object and how accurate it thinks the box that it predicts is. So, formally,
    we can define the confidence mechanism for YOLO by using this notation. We define
    the confidence as Pr(object) * IoU. If no object exists in that cell, then the
    confidence scores should be zero; otherwise, we want the confidence score to equal
    the IoU between predicted box and the ground truth. Each bounding box consists
    of five predictions: x, y, w, h, and confidence. The (x, y) coordinates represent
    the center of the box relative to the bounds of the grid cell. The w and h represents
    the width and height. They are predicted in relativity to the whole image. Finally,
    the confidence score represents the IoU. For each grid cell, it predicts C conditional
    class probabilities, *Pr(Classi|Object)*. These probabilities are conditioned
    on the grid cell that contains an object. We predict only one set of class probabilities
    per grid cell regardless of the number of bounding boxes B. At the time of testing,
    we multiply the conditional class probabilities and the individual box confidence
    prediction that is defined by this equation:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![The working of YOLO](img/B08394_09_33.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation gives us the class-specific confidence score for each
    of the boxes. This score contains both the probability of the class appearing
    in the box and how well the predicted box fits the object. The pictorial representation
    of the whole process of YOLO is shown in the following figure:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![The working of YOLO](img/B08394_09_22.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.22: Pictorial representation of YOLO object detection'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's get to basic knowledge about the YOLO architecture.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of YOLO
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will the learn basics about the YOLO architecture. In
    YOLO, there are 24 convolutional layers followed by two fully connected layers.
    Instead of the inception modules used by GoogleNet, YOLO simply uses 1×1 reduction
    layers followed by 3×3 convolutional layers. Fast YOLO uses a neural network with
    fewer convolutional layers. We use nine layers instead of 24 layers. We also use
    fewer filters in these layers. Apart from this, all parameters are the same for
    YOLO and Fast YOLO during training and testing. You can refer to the architecture
    in the following figure:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture of YOLO](img/B08394_09_23.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.23: Architecture of YOLO'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the implementation part.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach using YOLO
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement YOLO, we need to install the Cython module. Apart from
    that, you can use either Darknet or the Darkflow, which is the TensorFlow wrapper
    on Darknet. Darknet is written in C and CUDA, so it is quite fast. Here, we will
    be implementing both the options. Before implementation, we need to set up the
    environment. The implementation part should be divided into two sections here:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using Darknet
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation using Darkflow
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to this GitHub repository for all the code: [https://github.com/jalajthanaki/Real_time_object_detection_with_YOLO](https://github.com/jalajthanaki/Real_time_object_detection_with_YOLO).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using Darknet
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are following these steps in order to implement YOLO using Darknet:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup for Darknet
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile the Darknet
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the pre-trained weight
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run object detection for the image
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run object detection for the video stream
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment setup for Darknet
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this step, we need to download the GitHub repository of Darknet. We can
    do that using the following command. You can download this repository at any path:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you run this command, the directory named Darknet is created. After that,
    you can jump to the next step.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the Darknet
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we have downloaded the Darknet, we need to jump to the directory named
    Darknet. After that, we need to compile the Darknet. So, we need to execute the
    following commands sequentially:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Downloading the pre-trained weight
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Configuration files are already inside the `cfg/` subdirectory inside the darknet
    directory. So, by executing the following command, you can download the pre-trained
    weight for the YOLO model:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This download may take some time. Once we have the pre-trained weight with us,
    we can run the Darknet.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Running object detection for the image
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you want to identify the objects in the image, then you need to execute
    the following command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can refer to the output of this command in the following figure:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![Running object detection for the image](img/B08394_09_24.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.24 : Output of object detection for the image using Darknet'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement YOLO on the video stream.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Running the object detection on the video stream
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can run YOLO on the video stream using this command:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we need to pass the path of the video. For more information, you can
    refer to this Darknet documentation: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
    Now, let''s understand the implementation of Darkflow.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using Darkflow
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this implementation, you need to refer to the code given in the folder named
    Darkflow. We need to perform the following steps:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cython
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building the already provided setup file
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the environment
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the model and run object detection on the images
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the model and run object detection on the video stream
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Cython
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to install Cython, we need to execute the following command. This
    Cython package is needed because the Darkflow is a Python wrapper that uses C
    code from Darknet:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once Cython is installed, we can build the other setup.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Building the already provided setup file
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this stage, we will be executing the command that will set up the necessary
    Cython environment for us. The command is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When we execute this command, we will have to use `./flow` in the cloned Darkflow
    directory instead of flow, as Darkflow is not installed globally. Once this command
    runs successfully, we need to test whether we installed all the dependencies perfectly.
    You can download the pre-trained weight by using the following command:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Testing the environment
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this stage, we will test whether Darkflow runs perfectly or not. In order
    to check that, we need to execute the following command:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can refer to the following figure:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the environment](img/B08394_09_25.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.25 : Successful testing outcome of Darkflow'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Once you can see the preceding output, you'll know that you have successfully
    configured Darkflow. Now, let's run it.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model and running object detection on images
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can run the Darkflow on images. For that, we need to load YOLO pre-trained
    weights, configuration files, and path of images so you can execute the following
    command:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you want to save the object detection in the json format, then that is possible
    as well. You need to execute the following command:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can see the output inside the `sample_img/out` folder; refer to the following
    figure:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on images](img/B08394_09_26.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.26 : Output image with predicted objects using Darkflow'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also refer to the following figure:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on images](img/B08394_09_27.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.27 : json output of the object detected in the image'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Loading the model and running object detection on the video stream
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we will run object detection on the video stream. First, we
    will see how to use the webcam and perform object detection. The command for that
    is as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can refer to the following figure. You can see the video at this link:
    [https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on the video stream](img/B08394_09_28.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.28 : Output of the object detection using Darkflow for the webcam
    video stream'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also run the Darkflow for a prerecorded video. For that, you need to
    run the following command:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can refer to the following figure. You can see the video at this link:
    [https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing](https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the model and running object detection on the video stream](img/B08394_09_29.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.29 : Output of the object detection using Darkflow for the prerecorded
    video'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: In both commands we have used the—save Video flag to save the video and the—gpu
    0.60 flag, which use 60% memory of GPU. Using this approach, we will get an accuracy
    of 78%.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about Transfer Learning. We explored different
    libraries and approaches in order to build a real-time object detection application.
    You learned how to set up OpenCV and looked at how it is rather useful in building
    the baseline application. In this baseline approach, we used the model that is
    trained using the caffe deep learning library. After that, we used TensorFlow
    to build real-time object detection, but in the end, we used a pre-trained YOLO
    model, which outperformed every other approach. This YOLO-based approach gave
    us more generalized approach for object detection applications. If you are interested
    in building innovative solutions for computer vision, then you can enroll yourself
    in the VOC challenges. This boosts your skills and gives you a chance to learn.
    You can refer to this link for more information: [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/)
    (PASCAL VOC Challenges 2005-2012). You can also build your own algorithm and check
    the result and compare your result with the existing approach and, if it outperforms
    the existing approaches, you can definitely publish the paper in reputed journals.
    By using the YOLO approach, we get the Mean Average Precision of 78% on the PASCAL
    VOC dataset and it works pretty well when you apply this model to any video or
    image. The code credit for this chapter goes to Adrian Rosebrock, Dat Tran, and
    Trieu. We defined the mAP score based on the mAP it gets for either the COCO dataset
    or the PASCAL VOC dataset.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will explore another application that belongs to the
    computer vision domain: face detection and facial expression detection. In order
    to build this application, we will be using Deep Learning techniques. So, keep
    reading!'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨属于计算机视觉领域的另一个应用：人脸检测和面部表情检测。为了构建这个应用，我们将使用深度学习技术。所以，继续阅读吧！
