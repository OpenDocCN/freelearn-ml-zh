<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Bayes Intuition – Solving the Hit and Run Mystery and Performing Data Analysis</h1>
                </header>
            
            <article>
                
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/8e56062f-7bbb-4d3b-9bef-be0912cb3e70.png" style="width:9.58em;height:2.33em;"/></div>
<p>Notice how the start of this chapter was an in-your-face algorithm? I wanted to make sure that the first thing you see is this formula. This underscores just how important it will become in your machine learning career. Write it down, put it on a sticky note on your monitor, or commit it to memory!</p>
<p>In this chapter, we will:</p>
<ul>
<li>Apply the famous Bayes' theorem to solve a very famous problem in computer science</li>
<li>Show you how you can use Bayes' theorem and Naive Bayes to plot data, discover outliers from truth tables, and more</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overviewing Bayes' theorem</h1>
                </header>
            
            <article>
                
<p>To be honest, there are as many interpretations of Bayes' theorem as there are books about it. The one shown previously is the main one that we will be discussing. I would also encourage you to refer to <a href="https://brilliant.org/wiki/bayes-theorem/">https://brilliant.org/wiki/bayes-theorem/</a> for further reading.</p>
<p>To make this more concrete and formal, let's start off with a bit of intuition and formality; it will help us set the stage for what is to come.</p>
<p>When we use Bayes' theorem, we are measuring the degree of belief of something, the likelihood that an event will occur. Let's just keep it that simple for now:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/ad43f1e3-6227-4a34-a79b-26c76628b873.png" style="width:3.75em;height:1.33em;"/></div>
<p>The preceding formula means the probability of <em>A</em> given <em>B</em>.</p>
<p>Probability is usually quantified as a number between 0 and 1, inclusive of both; 0 would indicate impossibility and 1 would indicate absolute certainty. The higher the probability, the more the certainty. The odds of a dice rolling a 6 and the odds of a coin flip coming up heads are two examples of probability that you are no doubt very familiar with. There's also another example you are familiar with and encounter daily: spam.</p>
<p>All of us usually have our email open right beside us, all day long (some of us all night long too!). And with the messages that we are expecting also come the ones that we are not and do not care to receive. We all hate dealing with spam, that nasty email that has nothing to do with anything but Viagra; yet we somehow always seem to get it. What is the probability that any one of those emails I get each day is spam? What is the probability that I care about its content? How would we even know?</p>
<p>So let's talk a little bit about how a spam filter works because, you see, it's perhaps the best example of probability we can use! To be precise and more formal, we are dealing with <strong>conditional probability</strong>, which is the probability of event <em>A</em> given the occurrence of event <em>B</em>.</p>
<p>The way most spam filters work, at least at the very basic level, is by defining a list of words that are used to indicate emails that we do not want or did not ask to receive. If the email contains those words, it's considered spam and we deal with it accordingly. So, using Bayes' theorem, we look for the probability that an email is spam given a list of words, which would look like this in a formulaic view:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/b7eb54af-eab3-4059-8daa-8ddaf1db4f25.png" style="width:21.92em;height:2.75em;"/></div>
<p><strong>The probability that an email is spam, given a set of words</strong>: User Qniemiec in Wikipedia has an incredible visual diagram that explains in full force every combination of a probabilistic view, which is represented by the superposition of two event trees. If you are a visual person like I am, here is a complete visualization of Bayes' theorem represented by the superposition of two event tree diagrams:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5f346d0c-ec5b-4ccd-afc3-5ef501fa9ddd.png" style=""/></div>
<p>Now, let's move on to a very famous problem. It is called by many names, but the basic problem is what is known as the <strong>taxicab problem</strong>. Here's our scenario, which we will attempt <span>to solve </span>using probability and Bayes' theorem.</p>
<p>An Uber driver was involved in a hit-and-run accident. The famous yellow taxi cabs and Uber drivers are the two companies that operate in the city and can be seen everywhere. We are given the following data:</p>
<ul>
<li>85% of the cabs in the city are yellow and 15% are Uber.</li>
<li>A witness identified the car involved in the hit and run and stated that it had an Uber sticker on it. That being said, we know how reliable witness testimony is, so the court decided to test the user and determine their reliability. Using the same set of circumstances that existed on the night of the accident, the court concluded that the witness correctly identified each one of the two vehicles 80% of the time, but failed 20% of the time. This is going to be important, so stay with me on this!</li>
</ul>
<p><strong>Our dilemma</strong>: What is the probability that the car involved in the accident was an Uber driver versus a yellow cab?</p>
<p>Mathematically, here's how we get to the answer we need:</p>
<ul>
<li>The total number of Uber drivers identified correctly is:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><em>15 * 0.8 = 12</em></div>
<ul>
<li>The witness is incorrect 20% of the time, so the total number of vehicles incorrectly identified is:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><em>85 * 0.2 = 17</em></div>
<ul>
<li>Therefore, the total vehicles identified by the witness is <em>12 + 17 = 29</em>. The probability that they identified the Uber driver correctly is hence:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><em>12/29 = @41.3%</em><strong><br/></strong></div>
<p>Now, let's see whether we can develop a simple program that can help us arrive at that number to prove our solution works and is viable. To accomplish this, we are going to dive into our first open source toolkit: <strong>Encog</strong>. Encog is designed to handle problems exactly like this.</p>
<p>The Encog framework is a full-fledged machine learning framework and was developed by Mr. Jeff Heaton. Mr. Heaton has also published several books on the Encog framework, as well as other subjects, and I encourage you to seek them out if you plan to use this framework extensively. I persoally own every one of them and I consider them seminal works.</p>
<p>Let's look at the code it's going to take to solve our problem. As you'll notice, math, statistics, probability... it's all abstracted from you. Encog can allow you to focus on the business problem you are trying to solve.</p>
<p>The complete execution block looks like the following code. We'll begin to dissect it in a moment.</p>
<div class="mce-root CDPAlignLeft CDPAlign">
<pre>public void Execute(IExampleInterface app)<br/>{<br/>            // Create a Bayesian network<br/>            BayesianNetwork network = new BayesianNetwork();<br/>            // Create the Uber driver event<br/>            BayesianEvent UberDriver = network.CreateEvent("uber_driver");<br/>            // create the witness event<br/>            BayesianEvent WitnessSawUberDriver = network.CreateEvent("saw_uber_driver");<br/>            // Attach the two<br/>            network.CreateDependency(UberDriver, WitnessSawUberDriver);<br/>            network.FinalizeStructure();<br/>            // build the truth tables<br/>            UberDriver?.Table?.AddLine(0.85, true);<br/>            WitnessSawUberDriver?.Table?.AddLine(0.80, true, true);<br/>            WitnessSawUberDriver?.Table?.AddLine(0.20, true, false);<br/>            network.Validate();<br/>            Console.WriteLine(network.ToString());<br/>            Console.WriteLine($"Parameter count: {network.CalculateParameterCount()}");<br/>            EnumerationQuery query = new EnumerationQuery(network);<br/>            // The evidence is that someone saw the Uber driver hit the car<br/>            query.DefineEventType(WitnessSawUberDriver, EventType.<strong>Evidence</strong>);<br/>            // The result was the Uber driver did it<br/>            query.DefineEventType(UberDriver, EventType.<strong>Outcome</strong>);<br/>            query.SetEventValue(WitnessSawUberDriver, false);<br/>            query.SetEventValue(UberDriver, false);<br/>            query.Execute();<br/>            Console.WriteLine(query.ToString());<br/>}</pre></div>
<p>OK, let's break this down into more digestible pieces. The first thing we are going to do is create a Bayesian network. This object will be at the center of solving our mystery. The <kbd>BayesianNetwork</kbd> object is a wrapper around a probability and classification engine.</p>
<p>The Bayesian network is comprised of one or more <kbd>BayesianEvents</kbd>. An event will be one of three distinct types—<kbd>Evidence</kbd>, <kbd>Outcome</kbd>, or <kbd>Hidden</kbd><span>—</span>and will usually correspond to a number in the training data. An <kbd>Event</kbd> is always discrete, but continuous values (if present and desired) can be mapped to a range of discrete values.</p>
<p>After creating the initial network object, we create an event for the Uber driver as well as for the witness who claimed they saw the driver involved in the hit and run. We will create a dependency between the Uber driver and the witness, and then finalize the structure of our network:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">public void Execute(IExampleInterface app)<br/>         {<br/>             // Create a Bayesian network<br/>             BayesianNetwork network = new BayesianNetwork();<br/>             // Create the Uber driver event<br/>             BayesianEvent UberDriver = network.CreateEvent("uber_driver");<br/>             // create the witness event<br/>             BayesianEvent WitnessSawUberDriver = network.CreateEvent("saw_uber_driver");<br/>             // Attach the two<br/>             network.CreateDependency(UberDriver, WitnessSawUberDriver);<br/>             network.FinalizeStructure();<br/>             <br/>             // build the truth tables<br/>             UberDriver?.Table?.AddLine(0.85, true);<br/>             WitnessSawUberDriver?.Table?.AddLine(0.80, true, true);<br/>             WitnessSawUberDriver?.Table?.AddLine(0.20, true, false);<br/>             network.Validate();<br/> <br/>             Console.WriteLine(network.ToString());<br/>             Console.WriteLine($"Parameter count: {network.CalculateParameterCount()}");<br/>             <br/>             EnumerationQuery query = new EnumerationQuery(network);<br/> <br/>             // The evidence is that someone saw the Uber driver hit the car<br/>             query.DefineEventType(WitnessSawUberDriver, EventType.Evidence);<br/>             // The result was the Uber driver did it<br/>             query.DefineEventType(UberDriver, EventType.Outcome);<br/>             query.SetEventValue(WitnessSawUberDriver, false);<br/>             query.SetEventValue(UberDriver, false);<br/>             query.Execute();<br/>             Console.WriteLine(query.ToString());<br/>         }</pre>
<p>Next, we need to build the actual truth tables. A truth table is a listing of all possible values a function can have. There are one or more rows of increasing complexity, and the last row is the final function value. If you remember logic theory, there are basically three operations that you can have: <kbd>NOT</kbd>, <kbd>AND</kbd>, and <kbd>OR</kbd>. 0 usually represents <kbd>false</kbd>, and 1 usually represents <kbd>true</kbd>.</p>
<p>If we look just a little bit deeper, we will see that we end up with the following rules:</p>
<div class="CDPAlignCenter CDPAlign"><em>If A = 0, -A = 1</em></div>
<div class="CDPAlignCenter CDPAlign"><em>If A = 1, -A = 0</em></div>
<div class="CDPAlignCenter CDPAlign"><em>A+B = 1, except when A and B = 0</em></div>
<div class="CDPAlignCenter CDPAlign"><em>A+B = 0 if A and B = 0</em></div>
<div class="CDPAlignCenter CDPAlign"><em>A*B = 0, except when A and B = 1</em></div>
<div class="CDPAlignCenter CDPAlign"><em>A*B = 1 if A and B = 1</em></div>
<p>Now, back to our code.</p>
<p>To build the truth table, we will need to know the probability and the result value. In the case of our problem, the probability that an Uber driver was involved in the accident<span> </span><span>is 85%</span><span>. As for the witness, there is an 80% chance they are telling the truth and a 20% chance that they are mistaken. We will use the</span> <kbd>AddLine</kbd><span> function of the truth table to add this information:</span></p>
<pre class="mce-root CDPAlignLeft CDPAlign">// build the truth tables<br/>UberDriver?.Table?.AddLine(0.85, true);<br/>WitnessSawUberDriver?.Table?.AddLine(0.80, true, true);<br/>WitnessSawUberDriver?.Table?.AddLine(0.20, true, false);<br/>network.Validate();</pre>
<p>Let's talk a bit more about truth tables. Here is an extended truth table showing all the possible truth functions of two variables, <em>P</em> and <em>Q</em>.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ac69c6d3-1f8e-4ceb-ab07-3fcf0976fafe.png" style=""/></div>
<p>If we were to program our truth table more extensively, here is an example of how we could do so:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">a?.Table?.AddLine(0.5, true); // P(A) = 0.5<br/>x1?.Table?.AddLine(0.2, true, true); // p(x1|a) = 0.2<br/>x1?.Table?.AddLine(0.6, true, false);// p(x1|~a) = 0.6<br/>x2?.Table?.AddLine(0.2, true, true); // p(x2|a) = 0.2<br/>x2?.Table?.AddLine(0.6, true, false);// p(x2|~a) = 0.6<br/>x3?.Table?.AddLine(0.2, true, true); // p(x3|a) = 0.2<br/>x3?.Table?.AddLine(0.6, true, false);// p(x3|~a) = 0.6</pre>
<p>Now that our network and truth tables are built, it's time to define some events. As we mentioned earlier, events are any one of <kbd>Evidence</kbd>, <kbd>Hidden</kbd>, or <kbd>Outcome</kbd>. The <kbd>Hidden</kbd> event, which is neither <kbd>Evidence</kbd> nor <kbd>Outcome</kbd>, is still involved in the Bayesian graph itself. We will not be using <kbd>Hidden</kbd> but I wanted you to know that it does exist.</p>
<p>To solve our mystery, we must accumulate evidence. In our case, the evidence that we have is that the witness reported seeing an Uber driver involved in the hit and run. We will define an event type of <kbd>Evidence</kbd> and assign it to what the witness reported. The result, or outcome, is that it was an Uber driver, so we will assign an event type of outcome to that.</p>
<p>Finally, we must account for the fact that, at least some of the time, the witness's report of seeing an Uber driver involved was incorrect. So we must create event values for both probabilities—that the witness did not see an Uber driver, and that an Uber driver was not involved:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">EnumerationQuery query = new EnumerationQuery(network);<br/><br/>// The evidence is that someone saw the Uber driver hit the car<br/>query.DefineEventType(WitnessSawUberDriver, EventType.Evidence);<br/>// The result was the Uber driver did it<br/>query.DefineEventType(UberDriver, EventType.Outcome);<br/>query.SetEventValue(WitnessSawUberDriver, false);<br/>query.SetEventValue(UberDriver, false);<br/>query.Execute();</pre>
<p>Notice that the query we are going to execute is an <kbd>EnumerationQuery</kbd>. This object allows probabilistic queries against a Bayesian network. This is done by calculating every combination of hidden nodes and using total probability to find the result. The performance can be weak if our Bayesian network is large, but fortunately for us, it is not.</p>
<p>Finally, we execute our query against our Bayesian network definition and print the results:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/326f782d-f4b1-4e77-9e13-4b3026ca3c2a.png" style=""/></div>
<p>The result, as we had hoped for, was 41%.</p>
<p>As an exercise for you, see whether you can now use Encog to solve another very famous example. In this example, we wake up in the morning and find out that the grass was wet. Did it rain, was the sprinkler on, or both? Here's what our truth tables look like on pen and paper:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/99ee0c33-fdf3-44c3-83a3-8ed9aaa5cc95.png" style=""/></div>
<p>The probability that it rained:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/458aa8e9-2dee-4938-95bf-6feb70fd95d3.png" style=""/></div>
<p>The complete truth table:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3dafc76f-282d-468d-89c2-31f566fcad11.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overviewing Naive Bayes and plotting data</h1>
                </header>
            
            <article>
                
<p>Although we discussed Bayes' theorem, we would be doing a great disservice to you if we did not discuss Naive Bayes. It's everywhere, and for good reasons. It almost always works well (hence the name, Naive), and you will most certainly be exposed to it during your machine learning career. It is a simplistic technique based upon a premise that the value of any one feature is completely independent from the value of any other. For example, an orange is round, the color is orange, the skin is not smooth, and it's 10-20 cm in diameter. A Naive Bayes classifier would then consider each feature described previously to contribute independently that this is an orange versus an apple, lemon, and so on, even if there is some data relationship amongst its features.</p>
<p>As mentioned, Naïve Bayes is surprisingly efficient in resolving complex situations. Although there are scenarios where it can certainly be outperformed, it can be a great first-try algorithm to apply to your problem. We only need a very small amount of training data compared to many other models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting data</h1>
                </header>
            
            <article>
                
<p>In our next application, we will be using the fantastic Accord.NET machine learning framework to provide you with a tool with which you can enter data, watch it being plotted, and learn about false positives and negatives. We will be able to enter data for objects that exist in our data space and categorize them as either being green or blue. We will be able to change that data and see how it is classified and, more importantly, visually represented. Our objective is to learn which set new cases fall into as they arrive; they are either green or blue. In addition, we want to track false positives and false negatives. Naive Bayes will do this for us based upon the data that exists within our data space. Remember, after we train our Naive Bayes classifier, the end goal is that it can recognize new objects from data it has previously never seen. If it cannot, then we need to circle back to the training stage.</p>
<p>We briefly discussed truth tables, and now it's time to go back and put a bit more formality behind that definition. More concretely, let's talk in terms of a <strong>confusion matrix</strong>. In machine learning, a confusion matrix (error matrix or matching matrix) is a table layout that lets you visualize the performance of an algorithm. Each row represents predicted class instances, while each column represents actual class instances. It's called a confusion matrix because the visualization makes it easy to see whether you are confusing one with the other.</p>
<p>An abstract view of a truth table would look something like this:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td/>
<td>
<p><strong>X present</strong></p>
</td>
<td>
<p><strong>X absent</strong></p>
</td>
<td/>
</tr>
<tr>
<td>
<p>Test positive</p>
</td>
<td>
<p>True positive</p>
</td>
<td>
<p>False positive</p>
</td>
<td>
<p>Total positive</p>
</td>
</tr>
<tr>
<td>
<p>Test negative</p>
</td>
<td>
<p>False negative</p>
</td>
<td>
<p>True negative</p>
</td>
<td>
<p>Total negative</p>
</td>
</tr>
<tr>
<td/>
<td>
<p>Total with X</p>
</td>
<td>
<p>Total without X</p>
</td>
<td>
<p>Grand total</p>
</td>
</tr>
</tbody>
</table>
<p>A more visual view of the same truth table would look something like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fb4b97cd-5cfd-4804-aabf-2e0c0949798d.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Visual view of the truth table</div>
<p>And finally, a more formal view of a true confusion matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d0a00c1f-f694-4660-9dc7-cf0648e81e89.png" style=""/></div>
<p>In the field of machine learning, the truth table/confusion matrix allows you to visually assess the performance of your algorithm. As you will see in our following application, every time you add or change data, you will be able to see whether any of these false or negative conditions occur.</p>
<p>Currently, the test data we will start out with is split evenly between green and blue objects, so there's no reasonable probability that any new case is more likely to be one versus the other. This reasonable probability, sometimes called a <strong>belief</strong>, is more formally known as the <strong>prior probability</strong> (there's that word again!). Prior probabilities are based upon prior experience with what we've seen with the data and, in many cases, this information is used to predict outcomes prior to them happening. Given a prior probability or belief, we will formulate a conclusion which then becomes our <strong>posterior belief</strong>.</p>
<p>In our case, we are looking at:</p>
<ul>
<li>The prior probability of green objects being the total number of green objects/the total number of objects in our data space</li>
<li>The prior probability of blue objects being the total number of blue objects/the total number of objects in our data space</li>
</ul>
<p>Let's look a little bit further into what's happening.</p>
<p>You can see what our data looks like in the following screenshot. The <em>X</em> and <em>Y</em> columns indicate coordinates in our data space along an <em>x</em> and <em>y</em> axis, and the <em>G</em> column is a label as to whether or not the object is green. Remember, supervised learning should give us the objective we are trying to arrive at, and Naive Bayes should make it easy to see whether that's true.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7914ccf9-d845-4e8b-b919-e90892751181.png" style=""/></div>
<p class="mce-root">If we take the preceding data and create a scatter plot of it, it will look like the following screenshot. As you can see, all the points in our data space are plotted, and the ones with our <em>G</em> column having a value of 0 are plotted as blue, while those having a value of 1 are plotted as green.</p>
<p class="mce-root">Each data point is plotted against its <em>X</em>/<em>Y</em> location in our data space, represented by the <em>x</em>/<em>y</em> axis:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/828b307b-375c-40a0-8620-4babe7b20456.png" style=""/></div>
<p>But what happens when we add new objects to our data space that the Naive Bayes classifier cannot correctly classify? We end up with what is known as false negatives and false positives, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b6247b05-3d46-4daa-9079-23b34a5f0561.png" style=""/></div>
<p>As we <span>have </span>only two categories of data (green and blue), we need to determine how these new data objects will be correctly classified. As you can see, we have 14 new data points, and the color coding shows where they align to the <em>x</em> and <em>y</em> axis.</p>
<p>Now let's view our application in its full form. The following is a screenshot of our main screen. Under the <span class="packt_screen">Data Samples</span> tab on the left-hand side of the screen, we can see that we have our data space loaded. On the right-hand side of the screen, we can see that we have a scatter plot visual diagram that helps us visualize that data space. As you can see, all the data points have been plotted and color-coded correctly:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/56cebee2-9632-455e-b8ad-6740cffc1b11.png" style=""/></div>
<p>If we take a look at how the probabilities are classified and plotted, you can see that the data presents itself almost in two enclosed but overlapping clusters:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/076e34d3-9c73-453a-b6be-a52ce3a3e5ad.png" style=""/></div>
<p>When a data point in our space overlaps a different data point of a different color, that's where we need Naive Bayes to do its job for us.</p>
<p>If we switch to our <span class="packt_screen">Model Testing</span> tab, we can see the new data points we added.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b6247b05-3d46-4daa-9079-23b34a5f0561.png" style=""/></div>
<p>Next, let's modify some of the data points that we have added in order to show how any one data point can become a false negative or a false positive. Note that we start this exercise with seven false negatives and <span>seven</span> false positives.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/041a0a99-8d46-4295-bc5e-f948f664b1fb.png"/></div>
<p>The data modifications we made previously result in the following plot. As you can see, we have additional false positives now:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/79345884-955a-4004-a9f3-7970f160c9a8.png" style=""/></div>
<p>I will leave it up to you to experiment with the data and continue your Naive Bayes learning!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about probability theory, Bayes <span>theorem, N</span>aive Bayes, and how to apply it to real-world problems. We also learned how to develop a tool that will help us test out our classifier and see whether our data holds any false negatives or positives.</p>
<p>In our next chapter, we will dive deeper into the world of machine learning and talk about reinforcement learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Creative Commons Attribution—ShareAlike License</li>
<li>Heaton, J. (2015).<span> </span><em>Encog: Library of Interchangeable Machine Learning Models for Java and C#</em>, <em>Journal of Machine Learning Research</em>, 16, 1243-1247</li>
<li>Copyright 2008-2014, Heaton Research, Inc</li>
<li>Copyright (c) 2009-2017, Accord.NET authors<span> </span>authors@accord-framework.net</li>
<li><em>Case Study: The base rate fallacy reconsidered</em><span> </span>(Koehler (1996))</li>
</ul>


            </article>

            
        </section>
    </body></html>