- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multivariate Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The models we discussed in the previous chapter only depended on the previous
    values of the single variable of interest. Those models are appropriate when we
    only have a single variable in our time series. However, it is common to have
    multiple variables in time-series data. Often, these other variables in the series
    can improve forecasting of the variable of interest. We will discuss models for
    time series with multiple variables in this chapter. We will first discuss the
    correlation relationship between time-series variables, then discuss how we can
    model multivariate time series. While there are many models for multivariate time-series
    data, we will discuss two models that are both powerful and widely used: **autoregressive
    integrated moving average with exogenous variables** (**ARIMAX**) and **vector
    autoregressive** (**VAR**). Understanding these two models will extend the reader’s
    model toolbox and provide building blocks for the reader to learn more about multivariate
    time-series models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARIMAX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAR modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed models for **univariate time series**
    or a time series of one variable. However, in many modeling situations, it is
    common to have multiple time-varying variables that are measured together. A time
    series consisting of multiple time-varying variables is called a **multivariate
    time series**. Each variable in the time series is called a **covariate**. For
    example, a time series of weather data might include temperature, rain amount,
    wind speed, and relative humidity. Each of these variables, in the weather dataset,
    is a univariate time series, and together, a multivariate time series and each
    pair of variables are covariates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we typically represent a multivariate time-series as a vector-valued
    series, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: X = x 0,0 x 0,1 ⋮ , x 1,0 x 1,1 ⋮ , … , x t,0 x t,1 ⋮
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, each X instance consists of multiple values at each time step, and there
    are t steps in the series. The first index in the preceding equation represents
    the time step (0 through t), and the second index represents the individual series
    (0 through N variables). For example, if X were a time series of stock prices,
    at each time step of X, there would be a value of each stock price. In that case,
    X might look like the following equation if the first two stock symbols in the
    series were AAPL and GOOGL:'
  prefs: []
  type: TYPE_NORMAL
- en: X =  x 0, AAPL x 0, GOOGL ⋮ ,  x 1, AAPL x 1, GOOGL ⋮ , … ,  x t, APPL x t, GOOGL ⋮
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have a multivariate time series, a question arises: *What can we do
    with this additional* *information?* To answer that question, we need to understand
    whether these time-series variables are related. We can determine how time series
    are related by looking at their cross-correlation. Let’s discuss cross-correlation
    next.'
  prefs: []
  type: TYPE_NORMAL
- en: Time-series cross-correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to understand how time series can be related in order to determine
    whether covariate time series may be useful for forecasting a time series of interest.
    In a previous chapter, we discussed how to understand the relationship between
    two variables using plots and correlation coefficients; we will take a similar
    approach for the time-series data. Recall the following **cross-correlation function**
    (**CCF**) discussed in [*Chapter 10*](B18945_10.xhtml#_idTextAnchor160)*, Introduction
    to* *Time Series*:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ p  k(X, Y) =  ∑ t=1 n−k (X t −  _ X )(Y t−k −  _ Y )   ______________________   √ ____________  ∑ t=1 n  (X t
    −  _ X ) 2  √ ____________  ∑ t=1 n  (Y t −  _ Y ) 2
  prefs: []
  type: TYPE_NORMAL
- en: Here, X is a univariate time series and Y is a univariate time series. This
    function tells us how X and Y are related at k lags. In this equation, Y is k
    time steps behind X. In this situation, we say that X is leading Y by k steps
    or Y is lagging X by k steps. We will use this CCF to help us determine whether
    two time series are related and at which time steps they are related. Notice that
    if X = Y, then the CCF reduces to the **autocorrelation** **function** (**ACF**).
  prefs: []
  type: TYPE_NORMAL
- en: Cross-correlation significance
  prefs: []
  type: TYPE_NORMAL
- en: Just as with the ACF, cross-correlations have a threshold for statistical significance.
    We consider any cross-correlation value with an absolute value greater than 1.96
    / √ _ N  (where N is the sample size of the time series) to be significantly different
    from zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at cross-correlations with some data. This data [1] comes
    from the *UCI Machine Learning Repository* [2]. The data is weather data that
    was collected as part of a pollution study conducted in China. This data was sampled
    hourly over several years. We will subset the data to the first 1,000 data points.
    The weather data contained in this set consists of temperature (`TEMP`), pressure
    (`PRES`), dew point temperature (`DEWP`), rain precipitation (`RAIN`), and wind
    speed (`WSPM`). We will look at cross-correlations from the perspective of trying
    to forecast wind speed, meaning we will look at the cross-correlations of wind
    speed and the other variables. This may be an important problem for someone who
    wants to forecast power generation from windmills. The wind speed data and the
    ACF of the wind speed data are plotted in *Figure 12**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Plot of wind speed time series and its ACF](img/B18945_12_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Plot of wind speed time series and its ACF
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from the plot in *Figure 12**.1* that the wind speed could potentially
    be modeled by a stationary univariate model, which we discussed in the previous
    chapter. While there does not appear to be evidence of **autoregressive integrated
    moving average** (**ARIMA**) behavior, there does appear to be evidence of seasonality
    at lag 24\. Since the data is sampled hourly, the seasonality is likely a daily
    pattern. We have plotted the time series of the other variables in *Figure 12**.2*.
    These are the other variables we may want to use to help predict the value of
    wind speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Plot of wind speed time series and three other weather variables](img/B18945_12_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Plot of wind speed time series and three other weather variables
  prefs: []
  type: TYPE_NORMAL
- en: 'While we have not plotted the ACF for all the time series shown in *Figure
    12**.2*, these time series have characteristics that suggest they may be stationary.
    However, it is not clear from the time series alone whether these time series
    are related. This is where the cross-correlation of time series comes in. We can
    use a cross-correlation plot to determine whether any of the other time series
    are related to wind speed. We have plotted the cross-correlation of wind speed
    and the other variables in *Figure 12**.3*. The CCFs are plotted up to 48 lags.
    Since the data was sampled hourly, this gives us 2 days’ worth of lags to see
    the cross-correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – CCF plots of wind speed and three other variables](img/B18945_12_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – CCF plots of wind speed and three other variables
  prefs: []
  type: TYPE_NORMAL
- en: The plots in *Figure 12**.3* show that there are cross-correlations between
    wind speed and the other variables. While we are not too interested in the shapes
    of the cross-correlations, the patterns may provide some depth to how the variables
    are related. For example, the cross-correlation of wind speed and temperature
    shows an oscillating pattern with a period of about 12 hours; this may correspond
    to the effect of sunlight on wind speed. The primary finding is that these other
    variables share a relationship with wind speed and may help us forecast the values
    of wind speed. Let’s take a look at how we can use the additional variables for
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: ARIMAX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the ARIMA family of models and demonstrated
    how to model univariate time-series data. However, as we mentioned in the previous
    section, many time series are multivariate, such as stock data, weather data,
    or economic data. In this section, we will discuss how we can incorporate information
    from covariate variables when modeling time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we model a multivariate time series, we typically have a variable we are
    interested in forecasting. This variable is commonly called the **endogenous**
    variable. The other covariates in the multivariate time series are called **exogenous**
    variables. Recall from [*Chapter 11*](B18945_11.xhtml#_idTextAnchor174)*, ARIMA
    Models,* the equation representing the ARIMA model:'
  prefs: []
  type: TYPE_NORMAL
- en: y′ t = c + ϕ 1 y′ t−1 + … + ϕ p  y ′  t−p + ϵ t + θ 1 ϵ t−1 + … + ϕ q ϵ t−q
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, y′ t is the differenced series, differenced d times until it is stationary.
    In this model, y t is the variable we are interested in forecasting; it is the
    endogenous variable. Let us rewrite this equation in a more compact form. We collect
    the terms related to y′ t into one summation and the terms related to ϵ t into
    another summation. Let us start with the y′ t terms:'
  prefs: []
  type: TYPE_NORMAL
- en: ϕ 1 y′ t−1 + … + ϕ p  y ′  t−p = ∑ i=1 p ϕ i y′ t−i
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will also collect the ϵ t terms into different summations:'
  prefs: []
  type: TYPE_NORMAL
- en: ϵ t + θ 1 ϵ t−1 + … + ϕ q ϵ t−q = ϵ t + ∑ j=1 q ϕ j ϵ t−j
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting these summations together, we get the following more compact version
    of our ARIMA equation:'
  prefs: []
  type: TYPE_NORMAL
- en: y′ t = c + ∑ i=1 p ϕ i y′ t−i + ϵ t + ∑ j=1 q ϕ j ϵ t−j
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add the exogenous variables to the model. We will add the exogenous
    variables by adding a summation term to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: y′ t = c + ∑ i=1 p ϕ i y′ t−i + ϵ t + ∑ j=1 q ϕ j ϵ t−j + ∑ k=1 r β k X tk
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, there are r exogenous variables contained in the vector-valued variable
    X. Recall from earlier in the chapter, we can represent the vector-valued variable,
    as shown next, where the first index represents the time step, and the second
    index represents an individual series within the vector-valued variable:'
  prefs: []
  type: TYPE_NORMAL
- en: X =  x 0,0 ⋮ x 0,k,  x 1,0 ⋮ x 1,k, … ,  x t,0 ⋮ x t,k
  prefs: []
  type: TYPE_NORMAL
- en: This is how we will incorporate the exogenous variables into the ARIMA model.
    Now, let’s talk about how we would pick the exogenous variables. In [*Chapter
    11*](B18945_11.xhtml#_idTextAnchor174)*, ARIMA Models*, using an ACF plot, we
    saw that certain lags of a time series are more influential than others for modeling
    a univariate time series. We will make similar judgments about exogenous variables
    using CCF plots. Let’s take a look at example CCF plots and discuss how to make
    these judgments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CCF plot for wind speed and temperature is shown in *Figure 12**.4*. We
    have marked where the most significant cross-correlations occur within a lag period
    of 48 hours. Recall this data is sampled hourly; thus, each lagged time step corresponds
    to a lag of 1 hour. The plot shows that the most significant cross-correlations
    occur at approximately lag 13, lag 24, and lag 37:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – CCF plot of wind speed and temperature variables](img/B18945_12_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – CCF plot of wind speed and temperature variables
  prefs: []
  type: TYPE_NORMAL
- en: Based on the observation of the cross-correlation plot in *Figure 12**.4*, we
    would expect that temperature lags at 13, 24, and 37 could provide useful information
    for the current predicting wind speed. That is, the variables x t−13,temperature,
    x t−24,temperature, and x t−37,temperature may be useful for predicting y t. We
    can perform the same CCF analysis with the other two exogenous variables to determine
    which lags to use in the model. We found that pressure (`PRES`) had significant
    cross-correlations at lags 14 and 37, and dew point (`DEWP`) had significant cross-correlations
    at lags 2 and 20\. Now that we have assessed which lags of the exogenous variables
    we want to use in the model, let us go ahead and preprocess the data and fit the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the exogenous variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we fit the model, we need to preprocess the data to make the exogenous
    variables we mentioned previously. We need to create lagged versions of the time-series
    variables we described previously. We can lag a time series using the `shift()`
    method of a `pandas` `DataFrame`. Let’s take a look at a code example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows how to use the `shift()` method to lag the
    pressure variable by `14`. The first several lines of code import the library
    and load the data. The last line in the code sample shifts the `PRES` variable
    backward in time by 14 time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `shift()` method can be used to shift a series forward or backward in time.
    In this case, we want our exogenous variables to lag the variable we want to predict.
    So, we need to provide a negative shift value to move the series backward in time.
    We will use a similar preprocessing step for the other exogenous variables to
    create the other lagged variables. This is shown in the following code sample.
    All of these lagged variables created with the `shift()` method are collected
    into a new `DataFrame` called `X`, which we will use in the fit step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code sample, we are taking a subset of the lagged variables
    (the first `1,000` data points) to limit the computation time of the model. With
    the variables collected in the new `X` `DataFrame`, the preprocessing is complete,
    and we can move on to fitting and assessing the model.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in the previous chapter, we will use `auto_arima` to fit our ARIMA model.
    The main difference, in this case, is that we will need to provide the `auto_arima`
    function with the exogenous variables we created. The code to fit an ARIMA model
    with exogenous variables is shown in the following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should look very similar to the code shown in [*Chapter 11*](B18945_11.xhtml#_idTextAnchor174)*,
    ARIMA Models*. In fact, the only difference is the addition is the `X` variable
    in the `auto_arima` function. Running this code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this output should look very similar to the output we saw from `auto_arima`
    in [*Chapter 11*](B18945_11.xhtml#_idTextAnchor174)*, ARIMA Models*. It indicates
    we should use an AR(2) model based on the fit using the `auto_arima` only needs
    to fit a single coefficient for each exogenous variable in the model. It does
    not need to fit an `summary()` method on the object returned from `auto_arima`
    to see the coefficients and significance tests for the exogenous variables. Running
    the `summary()` method will output the following **seasonal autoregressive integrated
    moving average with eXogenous factors** (**SARIMAX**) results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – SARIMAX summary for AR(2) model](img/B18945_12_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – SARIMAX summary for AR(2) model
  prefs: []
  type: TYPE_NORMAL
- en: The output from the `summary()` method shown in the preceding snippet shows
    a lot of information. Let’s focus on the middle section of the output. The middle
    section shows the coefficients and significance tests for each exogenous variable
    and each ARIMA coefficient. The coefficients and test p-values are labeled `coef`
    and `P>|z|`, respectively. The significance test can help us determine whether
    variables should be included in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Recall back in [*Chapter 7*](B18945_07.xhtml#_idTextAnchor118)*, Multiple Linear
    Regression*, we discussed the idea of multicollinearity, which essentially means
    two or more predictor variables are highly correlated. We could encounter a similar
    situation here. For example, consider the `temp_lag_13`, `temp_lag_24`, and `temp_lag_37`
    variables. Since the temperature time series exhibits autocorrelation, it is possible,
    even likely, that these lagged versions of the temperature time series could be
    highly correlated. When we observe multicollinearity, it is a sign that we can
    remove variables from the model. We would recommend using one of the feature selection
    methods described in [*Chapter 7*](B18945_07.xhtml#_idTextAnchor118), such as
    recursive feature selection, performance-based selection, or selection by statistical
    significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding output shows only three of the exogenous variables in the current
    model, which are highlighted. This means we should consider reducing the variable
    in this model. We can do this with multiple methods; for this example, we will
    choose to eliminate features based on p-values. We will iteratively remove the
    feature with the highest p-value until all features have a p-value below 0.05\.
    The code for this selection process is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Performing this selection process will result in a model only containing the
    `temp_lag_24`, `temp_lag_37`, `pres_lag_24`, and `dewp_lag_2` exogenous variables.
    The ARIMA portion of the model is still an AR(2) model. With a model selection
    taken, let’s assess the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will assess the performance of the model by plotting the forecast versus
    the actual data points and calculating an error estimate. Let us start by looking
    at the forecasts from the ARIMAX model for the next `200` observations. The forecasts
    and test data points are plotted in *Figure 12**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Forecast of wind speed using an ARIMAX model](img/B18945_12_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Forecast of wind speed using an ARIMAX model
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the forecast for wind speed using the ARIMAX model we created in
    *Figure 12**.6*. Two observations stand out from the plotted forecast:'
  prefs: []
  type: TYPE_NORMAL
- en: The forecast captures the oscillation of the `windspeed` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The forecast fails to capture the high spikes in `windspeed`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand whether the exogenous variables provided material value in the
    model, we should compare the forecasts of the ARIMAX model, shown in *Figure 12**.6*,
    to forecasts from a univariate model. The forecasts from a univariate model are
    shown i*n* *Figure 12**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Forecast of wind speed using an ARIMA model](img/B18945_12_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Forecast of wind speed using an ARIMA model
  prefs: []
  type: TYPE_NORMAL
- en: The forecasts of the ARIMA model, shown in *Figure 12**.7*, do not appear to
    capture much variation in the time series as the forecast length grows. This is
    actually a property of ARMA models. It should be evident from the two plots that
    the ARIMAX model provides a better forecast than the ARIMA model. Finally, let
    us compare the **mean squared error** (**MSE**) of the forecasts of the two models.
    The MSE from the ARIMAX model is approximately 1.5, while the MSE from the ARIMA
    model is about 1.7\. The MSE value confirms what we saw in the plots—namely, the
    ARIMAX model provides a better forecast for this multivariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed an extension to the ARIMA model that allows us
    to incorporate information from covariates in a multivariate time series. This
    is only one of many methods for multivariate time series. In the next section,
    we will discuss another method for working with multivariate time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: VAR modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The AR(p), MA(q), ARMA(p,q), ARIMA(p,d,q)m, and SARIMA(p,d,q) models we looked
    at in the last chapter form the basis of multivariate VAR modeling. In this chapter,
    we have discussed ARIMA with exogenous variables (ARIMAX). We will now begin discussion
    on the VAR model. First, it is important to understand that while **ARIMAX requires
    leading (future) values of the exogenous variables**, **no future values of these
    variables are required for the VAR model** as they are all autoregressive to each
    other – hence the name vector autoregressive – and by definition not exogenous.
    To start, let us consider the two-variable, or bivariate, case. Consider a process
    y t that is the output of two different input variables, y t1 and y t2\. Note
    that in matrix form, we are discussing the case of an nxm matrix (y n,m) where
    *n* corresponds to the point in **time** and *m* corresponds to the **variables**
    involved (variables 1,2, … , m). We exclude the comma from notation going forward,
    but it is important to understand when we discuss multivariate autoregressive
    processes that we are discussing data within a multidimensional matrix rather
    than only the single-dimensional vector form we used for univariate time-series
    analysis. We can write our process, y t, in vector notation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y t = [y t1 y t2].
  prefs: []
  type: TYPE_NORMAL
- en: 'We can expand on the input variables’ definitions as a VAR(1) process, here,
    for the two variables in y t shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: y t1 = (1 − ϕ 11) μ 1 − ϕ 12 μ 2 + ϕ 11 y t−11 + ϕ 12 y t−12 + ε t1
  prefs: []
  type: TYPE_NORMAL
- en: y t2 = − ϕ 21 μ 1 + (1 − ϕ 22) μ 2 + ϕ 21 y t−11 + ϕ 22 y t−12 + ε t2.
  prefs: []
  type: TYPE_NORMAL
- en: The terms (1 − ϕ 11) μ 1 − ϕ 12 μ 2 and − ϕ 21 μ 1 − (1 − ϕ 22) μ 2 are our
    **model constants** (*β* coefficients).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reduce the previous processes (using a zero-mean form) into the matrix-reduced
    form, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: y t = Φ 1 y t−1 + ε t
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Φ 1 = [ϕ 11 ϕ 12 ϕ 21 ϕ 22]
  prefs: []
  type: TYPE_NORMAL
- en: 'Our forecast for a VAR(1) model is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  t 01(l) = (1 − ϕ 11)  _ y  1 − ϕ 12  _ y  2 + ϕ 11  ˆ y  t 01(l − 1) +
    ϕ 12  ˆ y  t 02(l − 1)
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  t 02(l) = − ϕ 21  _ y  1 + (1 − ϕ 22)  _ y  2 + ϕ 21  ˆ y  t 01(l − 1)
    + ϕ 22  ˆ y  t 02(l − 1)
  prefs: []
  type: TYPE_NORMAL
- en: 'It also includes this:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  t 0(l) = [ ˆ y  t 01(l)  ˆ y  t 02(l)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *l* represents the forward lag (t + 1, t + 2, … , t + n),  ˆ y  t 0m
    represents the forecasted value at time *t* for variable *m*, and t 0 corresponds
    to the most recent point in time (often, the length of the data). It is also important
    to note the mxm covariance matrix is this:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ Γ (k) =   ˆ γ  11(k) …  ˆ γ  1m(k)  ⋮ ⋱ ⋮   ˆ γ  m1(k) …  ˆ γ  mm(k)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, each covariance,  ˆ γ  ij(k), with *k* corresponding to the lag, is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ γ  ij(k) =  1 _ n  ∑ t=1 n−k (y ti −  _ y  i)(y t+k,j −  _ y  j).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our estimated cross-correlation between each variable y ti and y tj is therefore
    calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ ρ  ij(k) =   ˆ γ  ij(k) _ √ ___________  ˆ γ  ii(0)  ˆ γ  jj(0)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the calculations for cross-correlation and, especially, covariance
    helps with understanding the relationships between the variables and their impact
    on model fit. The method for combining the two models to generate one overall
    process forecast,  ˆ y  t, is through weighting the two input means and their
    correlations into one response. As we can see in our VAR(1) forecast, shown previously,
    the correlations between each given variable and the other variables across different
    points in time are able to be modeled using a VAR model. Notably, there are no
    truly independent and dependent variables in a VAR model; there is simply an interaction
    between each variable. It is important to note that all processes modeled with
    a VAR model should be **stationary**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the *United States Macroeconomic* dataset found at [https://www.statsmodels.org/dev/datasets/generated/macrodata.xhtml](https://www.statsmodels.org/dev/datasets/generated/macrodata.xhtml),
    we can observe the relationship between real gross private domestic investment
    (`realinv`), real personal consumption expenditures (`realcons`), and real private
    disposable income (`realdpi`) with a zero-mean form in the context of VAR modeling,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: realcons(t) = ϕ 11 realcons t−11 + ϕ 12 realinv t−12 + ϕ 13 realdpi t−13 + ε t1
  prefs: []
  type: TYPE_NORMAL
- en: realinv(t) = ϕ 21 realcons t−11 + ϕ 22 realinv t−12 + ϕ 23 realdpi t−13 + ε t2
  prefs: []
  type: TYPE_NORMAL
- en: realdpi(t) = ϕ 31 realcons t−11 + ϕ 32 realinv t−12 + ϕ 33 realdpi t−13 + ε t3
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at an example using this data in the VAR process in Python with
    the `statsmodels` VARMAX model. First, let’s load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see here from the output that 100% of our yearly quarters are unique
    and thus, no timestamp duplication exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '`100.0% of yearly quarters` `are unique`'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first things we always need to do when performing regression-based
    time-series modeling is to ensure stationarity. In VAR modeling, this means all
    variables must be stationary. The first step is to visualize the process realizations
    through line plotting by index. If data points are aggregated at an interval desired
    to be modeled, then we can leave the index as is, but if a different time indexing
    is desired, we need to specify this. Here, we are going to build a model to determine
    how `quarter` to the length of the data since that is the smallest denomination
    of time and there is one yearly quarter per index, as we checked previously. We
    then set the index to `quarter` (this is why `quarter` appears twice; once as
    the index and once as the column name):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the updated index and `quarter` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **quarter** | **year** | **quarter** | **realcons** | **realinv** | **realdpi**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1959 | 1 | 1707.4 | 286.898 | 1886.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1959 | 2 | 1733.7 | 310.859 | 1919.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1959 | 3 | 1751.8 | 289.226 | 1916.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1959 | 4 | 1753.7 | 299.356 | 1931.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1960 | 5 | 1770.5 | 331.722 | 1955.5 |'
  prefs: []
  type: TYPE_TB
- en: Figure 12.8 – First five rows of macrodata data set for VAR modeling
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will walk through six steps to produce a VAR model using the dataset
    we imported and prepared, shown partially in *Figure 12**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – visual inspection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the line plot of the data in *Figure 12**.9*, we can see what appears to
    be a strong linear trend. We need to check the ACF plots to assess this. After,
    we will run a **Dickey-Fuller test** to confirm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Line plots of realcons, realdpi, and realinv](img/B18945_12_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Line plots of *realcons*, *realdpi*, and *realinv*
  prefs: []
  type: TYPE_NORMAL
- en: 'The ACF plots in *Figure 12**.10* show a strong indication of a linear trend
    and at least one unit root, which likely gives us that trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – ACF plots for VARMAX input variables](img/B18945_12_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – ACF plots for VARMAX input variables
  prefs: []
  type: TYPE_NORMAL
- en: 'After running a Dickey-Fuller test, we can see that each variable contains
    a unit root and is thus trended. Therefore, we need to apply a first-order difference
    to all three variables so that we can work toward stationarizing the data. Recall
    that the **null hypothesis for the Dickey-Fuller is that a unit root exists**,
    and the alternative hypothesis is that one does not. We’ll execute the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see the Dickey-Fuller results for each variable. Recall the null
    hypothesis is the presence of a single unit root. Therefore, since the p-values
    are not low, we will not reject the null hypothesis. This gives us statistical
    evidence to assume the presence of trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ADF p-value for` `realcons: 0.9976992503412904`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ADF p-value for` `realinv: 0.6484956579101141`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ADF p-value for` `realdpi: 1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s difference the data here with a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Step 2 – selecting the order of AR(p)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we can rerun the ACF plots to check whether we still see trending autocorrelation.
    Because the original line plots did not appear to have seasonality, we will go
    ahead and plot the **partial ACFs** (**PACFs**) to get an idea of what kind of
    AR(p) ordering might be useful. Had we suspected seasonality, it might have been
    useful to check the **spectral density of the sample autocorrelations to quantify
    seasonal periodicity**, a process we demonstrated in [*Chapter 11*](B18945_11.xhtml#_idTextAnchor174)*,
    ARIMA Models*. When checking the PACF plots, we will use the **Yule-Walker method**.
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the ACF plots there is no apparent seasonality. When using a VAR
    model, all terms are used to estimate all other terms as the estimates for a forecast
    of any term are developed. In other words, to forecast one, we must forecast all.
    The reason this matters is that when looking at the PACFs in *Figure 12**.11*,
    we can see one variable could arguably be fit with an AR(1) (real investment)
    model, one with an AR(2) (real disposable income) model, and another with an AR(3)
    (real consumption) model. Using orders that are significant for each variable
    is one important aspect of making VAR modeling work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect of selecting ordering in a VAR model is cross-correlation.
    For example, we may have a model of variables whose highest order is order 3,
    but if the cross-correlation between the target variable and the input variable
    is at lag 5, we would most likely need to use lag 5\. However, a model using lag
    3 or 4 may also be useful. As with any regression model, however, we will always
    want to check the significance of coefficient terms following model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Differenced VARMAX variables; ACFs and PACFs](img/B18945_12_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – Differenced VARMAX variables; ACFs and PACFs
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – assessing cross-correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a VAR model, cross-correlation is most important between the variables we
    consider as inputs and the variable(s) we consider our target. This can be done
    using multiple lags or with the most significant lag if multiple iterations of
    the same variable using different shifts are a concern for overfitting. We must
    confirm cross-correlation analysis used in a forecasting model is done with respect
    to each input variable and the dependent variable. A cross-correlation that shows
    the input variable as a leading indicator for the target variable is not of major
    concern; it means historical values of the input are used to predict the target.
    However, if we have a scenario where the input is a lagging indicator—meaning
    the target predicts the input—and we want to use the input to predict the target,
    we will need to shift the input forward so that the correlation is at lag 0 and
    trim the data by the amount shifted.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, there is significant leading and lagging cross-correlation; this
    may indicate seasonality. In the case of seasonality with a VAR model, an indicator
    variable should be included with the data that provides a value of 1 associated
    with the peak in spectral density and a 0 value otherwise. Another special case
    with cross-correlation is that the input variable has prominent statistical significance
    but only at a lag far back in time. In this case, it may be appropriate to shift
    the data forward so that the model does not include terms (autocorrelations) that
    are not significant at the risk of overfitting. To demonstrate shifting based
    on the CCF, we will incorporate this into our process, next.
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe in *Figure 12**.12* that the strongest correlation between `realdpi`
    and `realinv` is at lag 0, but there are at least two other similarly correlated
    lags at *x=-27* and *x=-37* in the plot. As we saw in the *ARIMAX* section, we
    can include multiple shifted versions of the same variables as new variables to
    improve the predictive power of the model. This would enable us to take advantage
    of multiple correlations. However, for the purpose of model demonstration, we
    will not include that as the output from the VAR summary can become large. Also,
    as we mentioned, including too many lags of the same variable can result in overfitting.
    It is up to the practitioner to ultimately decide how many lags to use. This must
    be done by assessing model fit and bias/variance trade-offs. The function code
    given here is also in [*Chapter 10*](B18945_10.xhtml#_idTextAnchor160)*, Introduction
    to* *Time Series*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.12 – Cross-correlation for realdpi and realinv](img/B18945_12_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – Cross-correlation for *realdpi* and *realinv*
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 12**.13*, we can see `realcons` is a leading indicator for `realinv`
    with a positive correlation. This means an increase in consumption results in
    an increase in investment. Nonetheless, we could still use a downshifted version
    of `realinv` to predict `realcons`—if this were our objective—since both variables
    are expected to continue indefinitely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.13 – Cross-correlation for realcons and realinv](img/B18945_12_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 – Cross-correlation for *realcons* and *realinv*
  prefs: []
  type: TYPE_NORMAL
- en: To address the one-lag lead `realcons` has on `realinv`, let us shift `realcons`
    forward one index so that the strongest correlation is at lag 0\. This is not
    required for a VAR model but can be useful in scenarios where the most significant
    lag is **much farther back** in time and could result in including an unreasonably
    high order for our model to include the necessary variable relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Note on shifting data
  prefs: []
  type: TYPE_NORMAL
- en: When shifting data, null values will be included in the dataset. Consequently,
    the dataset must be trimmed by the length of the shift. This may come at the cost
    of model performance, so it is worth assessing the need to shift by comparing
    model errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying a forward shift to `realcons` in the following code snippet
    and rerunning the CCF, we can see in *Figure 12**.14* the variables’ strongest
    correlation is now at lag 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.14 – Cross-correlation for realinv and forward-shifted realcons](img/B18945_12_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.14 – Cross-correlation for *realinv* and forward-shifted *realcons*
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to run a function to append possible values of p and q (AR(p)
    and MA(q)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see the AIC error for VAR(1,0) and VAR(2,0) models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(p,q): (1,0),` `AIC: 6092.740258665657`'
  prefs: []
  type: TYPE_NORMAL
- en: '`(p,q): (2,0),` `AIC: 6050.444583720871`'
  prefs: []
  type: TYPE_NORMAL
- en: We have two possible models through the AIC selection method. First, we built
    a model with `p=2` and `q=0`, but after running, we observed that of seven coefficients
    predicting for `realinv`, only two were significant (even at the 0.10 level of
    significance), which is not good. After rerunning using a selection of `p=1` and
    `q=0`, two of the four coefficients were found significant at the 0.05 level of
    significance and all at the 0.10 level of significance. Therefore, we can conclude
    the `realinv`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – building the VAR(p,q) model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will skip the model validation steps we walked through in the last chapter
    as the process would be the same here. What we will do here is re-order the columns
    so that when we run the `get_prediction()` function, the results are for the first
    column. The model will stay the same since all variables are used to predict all
    other variables. After re-indexing, we will run the `VARMAX` model with `realcons`
    shifted up one, and predict `realinv`. For our model, we used AR(1) and MA(0).
    Note that we use `trend='c'` for *constant* as we removed `trend` when performing
    differencing; the `trend` argument is for trend determinism. Trend differencing
    should be handled outside of the model since not all variables may have unit root
    trends. Handling trends outside of the model also helps us identify components
    such as seasonality or significance of the true autocorrelation structure following
    the removal of `trend`. It is worthwhile to note that seasonality is not addressed
    directly with the VAR (and VARMAX) model; indicator (dummy) variables for seasonality
    should be included.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding model fit, we can refer to [*Chapter 11*](B18945_11.xhtml#_idTextAnchor174)*,
    ARIMA Models,* for information on `Dep. Variable` list, shown next. Each model
    variable has listed the coefficients of the other variables’ terms used for predicting
    each variable. For example, we can observe in `Results for equation realcons`
    that `realinv` does not contribute much predictive capability to `realcons`, but
    `realdpi` does. We can also observe that `realcons` is significant for predicting
    `realinv`. We can also see based on the p-value and confidence interval in the
    `realinv` and `realdpi`. We can expect to see larger coefficient values if we
    have variance in one feature explained by variance in the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.15 – VAR(1) model output](img/B18945_12_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 – VAR(1) model output
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – testing the forecast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is important to note that the prediction for the VAR model is actually quite
    stable compared to the long-running historical performance. The reason we see
    such extreme differences at the end of the forecast is that this time period corresponds
    to the **Great Recession**, which lasted from 2007 to 2009\. The last five points
    in our test forecast correspond to the last two quarters of 2008 and the first
    three quarters of 2009:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.16 – Test forecast for VARMAX with p=1](img/B18945_12_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 – Test forecast for VARMAX with p=1
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compare our forecast (*mean*) to actuals (`realinv`) for the investment
    variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here in *Figure 12**.17*, we can see the VAR (from the `VARMAX` function) model
    test forecast:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `mean` | `mean_se` | `mean_ci_lower` | `mean_ci_upper` | `realinv` |'
  prefs: []
  type: TYPE_TB
- en: '| 196 | -12.4348 | 40.79603 | -92.3935 | 67.52398 | -56.368 |'
  prefs: []
  type: TYPE_TB
- en: '| 197 | -6.26154 | 40.79603 | -86.2203 | 73.69722 | -35.825 |'
  prefs: []
  type: TYPE_TB
- en: '| 198 | -33.5406 | 40.79603 | -113.499 | 46.41821 | -133.032 |'
  prefs: []
  type: TYPE_TB
- en: '| 199 | -53.6481 | 40.79603 | -133.607 | 26.31066 | -299.167 |'
  prefs: []
  type: TYPE_TB
- en: '| 200 | -81.514 | 40.79603 | -161.473 | -1.55526 | -101.816 |'
  prefs: []
  type: TYPE_TB
- en: '| 201 | -10.1039 | 40.79603 | -90.0627 | 69.85483 | 29.72 |'
  prefs: []
  type: TYPE_TB
- en: Figure 12.17 – VAR(1) model test forecast
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – building the forecast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can observe that because this is a model with an **autoregressive order
    p=1**, the forecast **tends very quickly toward the mean**. Including a higher
    order would likely result in more model variance, which we could see as a more
    confident, yet risky forecast. The multivariate AR(1) model produces more bias,
    which we can observe in *Figure 12**.18* as the tendency to the mean we mentioned.
    We can run the following code to generate the data and plot for the forecast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.18 – VAR(1) model forecast, h=7](img/B18945_12_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.18 – VAR(1) model forecast, h=7
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the forecasted points and the confidence interval for the
    forecast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|  | `mean` | `mean_se` | `mean_ci_lower` | `mean_ci_upper` |'
  prefs: []
  type: TYPE_TB
- en: '| 202 | -16.3442 | 40.79603 | -96.3029 | 63.6146 |'
  prefs: []
  type: TYPE_TB
- en: '| 203 | -10.9571 | 43.69828 | -96.6041 | 74.68998 |'
  prefs: []
  type: TYPE_TB
- en: '| 204 | -3.24668 | 44.4028 | -90.2746 | 83.7812 |'
  prefs: []
  type: TYPE_TB
- en: '| 205 | 1.301495 | 44.56735 | -86.0489 | 88.65189 |'
  prefs: []
  type: TYPE_TB
- en: '| 206 | 3.576619 | 44.60344 | -83.8445 | 90.99775 |'
  prefs: []
  type: TYPE_TB
- en: '| 207 | 4.650973 | 44.61108 | -82.7851 | 92.08709 |'
  prefs: []
  type: TYPE_TB
- en: '| 208 | 5.147085 | 44.61268 | -82.2922 | 92.58633 |'
  prefs: []
  type: TYPE_TB
- en: Figure 12.19 – VAR(1) model output data
  prefs: []
  type: TYPE_NORMAL
- en: Note that our results are based on the differenced data. This model tells us
    the level of statistical significance between variables regarding their respective
    signals but does not give us forecast values in terms of the original data. To
    reverse differenced data, we need to estimate a constant of integration and then
    reverse-difference the data according to that constant or use a different transformation
    method that allows us to back-transform, such as a logarithmic transformation
    that can be exponentiated to reverse. Often, the VAR model is used for identifying
    potential causation between variables identified as highly correlated in a cross-correlation
    analysis as well as in economic shock analysis.
  prefs: []
  type: TYPE_NORMAL
- en: It can be argued that depending on the level of detail needed in the forecast,
    differencing and shifting of variables may not be required. However, because this
    is a fully endogenous model with no exogenous variables, it is important that
    if we apply differencing to any variables, we must also apply differencing to
    all variables containing non-stationary behavior. With respect to shifting, it
    may be more useful to simply apply a higher autoregressive lag order than to apply
    a shift; the drawback of shifting is we lose samples early on in the process.
    However, the drawback of using a higher lag order is the inclusion of more variables
    in the time dimension, which can increase the likelihood of overfitting. Logically,
    as more variables are included in the model, we must also increase sample size.
    As with any model, we must apply rigorous cross-validation to ensure performance
    stability and minimize risk.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided an overview of multivariate time-series and how
    they differ from the univariate case. We then covered the math and intuition behind
    two popular approaches to solving problems using multivariate time-series models—ARIMAX
    and the VAR model framework. We walked through examples for each model using a
    step-by-step approach. This chapter concludes our discussions on time-series analysis
    and forecasting. At this point, you should be able to identify and assess the
    statistical properties of time series, transform them as needed, and construct
    models that are useful for fitting and forecasting both univariate and multivariate
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin our discussion on survival analysis with
    an introduction to **time-to-event** (**TTE**) variables.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] *Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and
    Chen, S. X.* (*2015*). *Assessing Beijing’s PM2.5 pollution: severity, weather
    impact, APEC and winter heating*. *Proceedings of the Royal Society A,* *471,
    20150257*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] *Dua, D. and Graff, C.* (*2019*). *UCI Machine Learning Repository* [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    *Irvine, CA: University of California, School of Information and* *Computer Science*.'
  prefs: []
  type: TYPE_NORMAL
- en: Part 5:Survival Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will cover another statistical approach named survival analysis by
    analyzing a time to event outcome variable. After an introduction of survival
    analysis and censored data, we will study models with survival responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'It includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B18945_13.xhtml#_idTextAnchor206), *Time to Event variables
    - An introduction*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B18945_14.xhtml#_idTextAnchor217), *Survival Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
