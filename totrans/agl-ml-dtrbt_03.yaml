- en: '*Chapter 2*: Machine Learning Basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：机器学习基础'
- en: This chapter covers some basic concepts of machine learning that will be used
    and referenced in this book. This is the bare minimum you need to know in order
    to use DataRobot effectively. Experienced data scientists can safely skip this
    chapter. It is not the intention of this chapter to give you a comprehensive understanding
    of statistics or machine learning, but just a refresher of some key ideas and
    concepts. Also, the focus is on practical aspects of what you need to know in
    order to understand the core ideas without going into too much detail. It might
    be tempting to jump in and let DataRobot automatically build the models, but doing
    that without a basic understanding could backfire. If you are leading a data science
    team, please make sure that you have experienced data scientists in your teams
    who are mentoring others and that there are other governance processes in place.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了本书中将使用和引用的一些机器学习基本概念。这是使用DataRobot有效所需的最基本知识。经验丰富的数据科学家可以安全地跳过本章。本章的目的不是让你对统计学或机器学习有一个全面的理解，而是对一些关键思想和概念进行复习。此外，重点是了解你需要了解的实践方面，以便理解核心思想，而不深入细节。你可能想跳进去，让DataRobot自动构建模型，但如果没有基本理解，这样做可能会适得其反。如果你是数据科学团队的领导者，请确保你的团队中有经验丰富的数据科学家，他们正在指导他人，并且已经建立了其他治理流程。
- en: Some of these concepts will come up again during the hands-on examples, but
    we are covering many concepts here that might not come up during a specific example,
    but might come up in relation to your project at some point. The topics listed
    here can be used as a guide to determine some of the basic knowledge that you
    require in order to start using powerful tools such as DataRobot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一些这些概念将在实际操作示例中再次出现，但我们在这里涵盖了可能不会在特定示例中出现，但可能在某个时候与你的项目相关的许多概念。这里列出的主题可以用作指南，以确定你开始使用像DataRobot这样的强大工具所需的一些基本知识。
- en: 'By the end of this chapter, you will have learned some of the core concepts
    you need to know to use DataRobot effectively. In this chapter, we''re going to
    cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学习到一些使用DataRobot有效所需的核心概念。在本章中，我们将涵盖以下主要主题：
- en: Data preparation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Data visualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Machine learning algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: Performance metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标
- en: Understanding the results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解结果
- en: Data preparation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Before an algorithm can be applied to a dataset, the dataset needs to fit a
    certain pattern. The dataset also needs to be free of errors. Certain methods
    and techniques are used to ensure that the dataset is ready for the algorithms,
    and this will be the focus of this section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法可以应用于数据集之前，数据集需要符合一定的模式。数据集还需要没有错误。某些方法和技术被用来确保数据集为算法做好准备，这将是本节的重点。
- en: Supervised learning dataset
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习数据集
- en: 'Since DataRobot mostly works with supervised learning problems, we will only
    focus on datasets for supervised machine learning (other types will be covered
    in a later section). In a supervised machine learning problem, we provide all
    the answers as part of the dataset. Imagine a table of data where each row represents
    a set of clues with their corresponding answers (*Figure 2.1*):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DataRobot主要处理监督学习问题，因此我们只会关注监督机器学习的数据集（其他类型将在后面的章节中介绍）。在监督机器学习问题中，我们将所有答案作为数据集的一部分提供。想象一下一张数据表，其中每一行代表一组线索及其相应的答案（*图2.1*）：
- en: '![Figure 2.1 – Supervised learning dataset'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 – 监督学习数据集'
- en: '](img/Figure_2.1_B17159.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.1_B17159.jpg)'
- en: Figure 2.1 – Supervised learning dataset
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 监督学习数据集
- en: This dataset is made up of columns that contain clues (these are called **features**),
    and there is a column with the answers (this is called **target**). Given a dataset
    that looks like this, the algorithm learns how to produce the right answer given
    a set of clues. No matter what form your data is in, your task is to first transform
    it to make it look like the table in *Figure 2.1*. Note that the clues that you
    have might be spread across multiple databases or Excel files. You will have to
    compile all of that information into one table. If the datasets you have are complex,
    you will need to use languages such as SQL, tools such as **Python** **Pandas**,
    or **Excel**, or tools such as **Paxata**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Time series datasets
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time series or forecasting problems have time as a key component of their datasets.
    They are similar to the supervised learning datasets, with slight differences,
    as shown in *Figure 2.2*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Time series dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.2_B17159.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – Time series dataset
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: You need to make sure that your time series datasets appear as shown in the
    preceding diagram. It should have a date or time-based column, and a column with
    the series values you are trying to forecast, and a set of clues as needed. You
    can also add columns that help to categorize different series, if there are multiple
    time series that you need to forecast. For example, you might be interested in
    forecasting units sold for dates 5 and 6\. If your data is in some other form,
    it needs to be transformed to look like the preceding diagram.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data that comes to you will typically have errors in it. For example, you
    might have text in a field that is supposed to contain numbers. You might see
    a price column where the values may contain a $ sign on occasion, but no sign
    at other times. DataRobot can catch some of these, but there are times when an
    automated tool will not catch these, so you need to look and analyze the dataset
    carefully. It is useful to sometimes upload your data to DataRobot to see what
    it finds, and then use its analysis to determine the next steps. Some of this
    cleansing will need to be performed outside DataRobot, so be prepared to iterate
    a few times to get the data set up correctly. Common issues to watch out for include
    the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Wrong data type in a column
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed data types in a column
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spaces or other characters in numeric columns that make them look like text
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synonyms or misspelled words
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dates encoded as strings
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dates with differing formats
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data normalization and standardization
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When different data features have varying scales and ranges, it becomes harder
    to compare their impacts on the target values. Also, many algorithms have difficulty
    in dealing with different scales of values, sometimes leading to stability issues.
    One method for avoiding these problems is to normalize (not to be confused with
    database normal forms) or standardize the values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'In normalization (also known as scaling), you scale the values such that they
    range from 0 to 1:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Xnormalized = (X – Xmin) / (Xmax – Xmin)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Standardization, on the other hand, centers the data such that the mean becomes
    zero and scales it such that the standard deviation becomes 1\. This is also known
    as **z-scoring** the data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Xstandardized = (X – Xmean) / XSD
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Here, Xmean is the mean of all X values, and XSD is the standard deviation of
    X values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: In general, you will not need to worry about this because DataRobot automatically
    does this for the datasets as required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outliers are values that seem to be out of place compared to the rest of the
    dataset. These values can be very large or very small. In general, values that
    are more than three standard deviations from the mean are considered outliers,
    but this only applies to features where values are expected to be normally distributed.
    Outliers typically come from data quality issues or some unusual situations that
    are not considered relevant enough to be trained on. The data points deemed to
    be outliers are typically removed from the dataset to prevent them from overpowering
    your models. The rules of thumb are only for highlighting the candidates. You
    will have to use your judgment to determine whether any values are outliers and
    whether they need to be removed. Once again, DataRobot will highlight potential
    outliers, but you will have to review those data points and determine whether
    to remove them or leave them in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Missing values
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a very common problem in datasets. Your dataset may contain many missing
    values, marked as **NULL** or **NaN**. In some cases, you will see a **?**, or
    you might see an unusual value, such as **-999**, that an organization might be
    using to represent a missing or unknown value. How you choose to handle such values
    depends a lot on the problem you are trying to solve and what the dataset represents.
    Many times, you might choose to remove the row of data that contains a missing
    value. Sometimes, that is not possible because you might not have enough data,
    and removing such rows might lead to the removal of a significant portion of your
    dataset. Sometimes, you will see a large number of values in a feature (or column)
    that might be missing. In those situations, you might want to remove that feature
    from the dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Another possible way of dealing with this situation is to fill the missing values
    with a reasonable guess. This could take the form of a zero value, or the mean
    value for that feature, or a median value of that feature. For categorical data,
    missing values are typically treated as their own separate category.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated methods use the k-nearest neighbor algorithm to compute missing
    values based on other similar data points. No one answer will be appropriate every
    time, so you will need to use your judgment and understanding of the problem to
    make a decision. One final option is to leave it as it is and let DataRobot figure
    out how to deal with the situation. DataRobot has many imputation strategies as
    well as algorithms to handle missing values. But you have to be careful, as that
    might not always lead to the best solution. Talk to an experienced data scientist
    and use your understanding of the business problem to plot a course of action.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Category encoding
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many problems, you have to transform your features into numeric values. This
    is because many algorithms cannot handle categorical data. There are many ways
    to encode categorical values and DataRobot has many of these methods built in.
    Some of these techniques are one-hot encoding, leave one out encoding, and target
    encoding. We will not get into the details, as normally you would let DataRobot
    handle this for you, but there might be cases where you will want to encode it
    yourself in a specific way due to your understanding of the business problem.
    This feature of DataRobot is a great time saver and typically works very well
    for most problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Consolidate categories
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, you have categorical data that contains a large number of categories.
    Although there are methods for dealing with large category counts (as discussed
    in the preceding section), many times, it is advisable to consolidate the categories.
    For example, you might have many categories that contain very few data points,
    but are very similar to one another. In this case, you can combine them into a
    single category. In other cases, it might just be that someone used a different
    spelling, a synonym, or an abbreviation. In such cases, it is better to combine
    them into a single category as well. Sometimes, you might want to split up a numerical
    feature into bins that have a business meaning for your users or stakeholders.
    This is an example of data preparation that you will need to do on your own based
    on your understanding of the problem. You should do this prior to uploading the
    data into DataRobot.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Target leakage
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, the dataset contains features that are derived from the target itself.
    These are not known in advance or are not known at the time of prediction. Inadvertently
    using these features to build a model causes problems downstream. This issue is
    called target leakage. The dataset should be inspected carefully and such features
    should be removed from the training features. DataRobot will also analyze the
    features automatically and try to flag any features that might lead to target
    leakage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Term-document matrix
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your dataset may contain features that contain text or notes. These notes frequently
    contain important information that is useful for making decisions. Many of the
    algorithms, however, cannot make use of this text directly. This text has to be
    parsed into numeric values for it to become useful to modeling algorithms. There
    are several methods for doing that, with the most common one being the term-document
    matrices. Document here refers to a single text or notes entry. Each of these
    documents can be parsed to split it up into terms. Now you can count how many
    times a term showed up in a document. This result can be stored in a matrix called
    a **Term Frequency** (**TF**) matrix. Some of this information can also be visualized
    in word clouds. DataRobot will automatically build these word clouds for you.
    While TF is useful, it can be limiting because some terms might be very common
    in all the documents, hence they are not very useful in distinguishing between
    them. This leads to another idea, whereby perhaps we should look for terms that
    are somewhat unique to a document. This concept of giving more weight to a term
    that is present in some documents only is called **Inverse Document Frequency**
    (**IDF**). The combination of a term showing up multiple times in a document (TF)
    and it being somewhat rare (IDF) is called **TFIDF**. TFIDF is something that
    DataRobot will compute automatically for you and gets applied to features that
    contain text.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集中可能包含包含文本或注释的特征。这些注释通常包含对做出决策有用的重要信息。然而，许多算法却无法直接利用这些文本。这些文本必须被解析成数值，以便对建模算法变得有用。为此有几种方法，其中最常见的是术语-文档矩阵。这里的文档指的是单个文本或注释条目。每个文档都可以被解析以分割成术语。现在您可以计算一个术语在文档中出现的次数。这个结果可以存储在一个称为**词频（TF**）矩阵中。一些信息也可以在词云中进行可视化。DataRobot会自动为您构建这些词云。虽然TF很有用，但它可能有限制，因为某些术语可能在所有文档中都非常常见，因此它们在区分它们之间不是非常有用。这导致另一个想法，即我们可能应该寻找仅对某些文档独特的术语。这种给仅在某些文档中出现的术语赋予更多权重的概念被称为**逆文档频率（IDF**）。一个术语在文档中多次出现（TF）并且相对罕见（IDF）的组合被称为**TFIDF**。TFIDF是DataRobot会自动为您计算并应用于包含文本的特征。
- en: Data transformations
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'While DataRobot will do many data transformations for you (and it keeps adding
    more all the time), there are many transformations that will impact your model
    but that DataRobot will not be able to catch. You will have to do these on your
    own. Examples of these are mathematical transformations such as log, square, square
    root, absolute values, and differences. Some of the simple ones can be set up
    inside DataRobot, but for more complex ones, you will have to perform the operations
    outside of DataRobot or in tools such as Paxata. Sometimes, you will do a transformation
    to linearize your problem or to deal with features that have long-tailed data.
    Some of the transformations that DataRobot does automatically are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DataRobot会为您执行许多数据转换（并且它一直在添加更多），但有许多转换会影响您的模型，但DataRobot将无法捕捉到。您将不得不自己执行这些操作。这些操作的例子包括数学转换，如对数、平方、平方根、绝对值和差值。其中一些简单的可以在DataRobot内部设置，但对于更复杂的转换，您必须在DataRobot之外或使用Paxata等工具执行操作。有时，您会进行转换以线性化问题或处理具有长尾数据的特征。DataRobot自动执行的一些转换如下：
- en: Computing aggregates such as counts, min, max, average, median, most frequent,
    and entropy
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算聚合数据，如计数、最小值、最大值、平均值、中位数、最频繁值和熵
- en: An extensive list of time-based features, such as change over time, max over
    time, and averages over time
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个广泛的时间相关特征列表，例如随时间变化、随时间最大值和随时间平均值
- en: Some text extraction features, such as word counts, extracted tokens, and term-document
    matrices
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些文本提取特征，例如词数、提取的标记和术语-文档矩阵
- en: Geospatial features from geospatial data
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自地理空间数据的地理空间特征
- en: We will discuss this topic again in more detail in [*Chapter 4*](B17159_04_Final_NM_ePub.xhtml#_idTextAnchor087),
    *Preparing Data for DataRobot*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第4章*](B17159_04_Final_NM_ePub.xhtml#_idTextAnchor087)中更详细地讨论这个主题，*为DataRobot准备数据*。
- en: Collinearity checks
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共线性检查
- en: In any given dataset, there will be features that are highly correlated to other
    features. In essence, they carry the same information as some other features.
    It is generally desirable to remove such features that are highly duplicative
    of some other features in the dataset. DataRobot performs these checks automatically
    for you and will flag these collinear features. This is especially critical for
    linear models, but some of the newer methods can deal with this issue better.
    What thresholds to use varies based on the modeling algorithms and your business
    problem. It is fairly easy in DataRobot to remove these features from your feature
    sets to be used for modeling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: DataRobot also produces a correlation matrix that shows how the different features
    are correlated to one another. This helps identify collinear features as well
    as key candidate features to be used in the model. You can gain a lot of insight
    into your data and the problem by analyzing the correlation matrix. In [*Chapter
    5*](B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097), *Exploratory Data Analysis
    with DataRobot*, we will discuss examples of how this is done.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Data partitioning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start building the models, you need to partition your dataset into
    three parts. These parts are called training, validation, and holdout. These three
    parts are used for different purposes during the model building process. It is
    common to split 10-20% of the dataset into the holdout set. The remaining portion
    is split up further, with 70-80% going to training and 20-30% going to the validation
    set. This splitting is done to make sure that the models are not overfitted and
    that the expected results in deployment are in line with results seen during model
    building.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Only the training dataset is used to train the model. The validation set is
    designed to tune the algorithms in order to optimize the results by performing
    multiple cross-validation tests. Finally, the holdout set is used after the models
    are built to test the model on data that it has never seen before. If the results
    on the holdout set are acceptable, then the model can be considered for deployment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: DataRobot automates most of this process, but it does allow the user to customize
    the split percentages, as well as how the partitioning should be done. It also
    performs a similar function for time series or forecasting problems by automatically
    splitting the data for time-based backtests.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important tasks a data analyst or data scientist needs to do
    is to understand the dataset. Data visualization is key to this understanding.
    DataRobot provides various ways to visualize the datasets to help you understand
    the dataset. These visualizations are built automatically for you so that you
    can spend your time analyzing them instead of preparing them. Let's look at what
    these are and how to use them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'When you go to the data page (*Figure 1.20*) for your project, you will see
    high-level profile information for your dataset. Inspect this information carefully
    to understand your dataset in totality. If you click on the **Feature Association**
    menu (top left), you will see how the features are related to one another (*Figure
    2.3*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Feature associations using mutual information'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.3_B17159.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – Feature associations using mutual information
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: This diagram shows the interrelationships using the mutual information metric.
    **Mutual Information** (**MI**) uses information theory to determine the amount
    of information you obtain about one feature from the other feature. The benefit
    of using MI compared to the Pearson correlation coefficient is that it can be
    used for any type of feature. The value goes from 0 (the two features are independent)
    to 1 (they carry the same information). This is useful in determining which features
    will be good candidates for the model and which features will not provide any
    useful information or are redundant. This view is extremely important to understand
    and use before model building starts, even though DataRobot automatically uses
    this information to make modeling decisions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another metric that is also used in a similar capacity. If you click
    on the metric dropdown at the bottom of the preceding screenshot, you can select
    the other metric called **Cramer''s V**. Once you select Cramer''s V, you will
    see a similar graphical view (*Figure 2.4*):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Feature associations using Cramer''s V'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.4_B17159.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Feature associations using Cramer's V
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Cramer's V is an alternative metric to MI, and it is used similarly. Its value
    also ranges from 0 (no relationship) to 1 (the features are highly correlated).
    Cramer's V is often used with categorical variables as an alternative to the Pearson
    correlation coefficient.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Notice that DataRobot automatically found clusters of interrelated features.
    Each cluster is color-coded in a different color, and the features are sorted
    by clusters in *Figure 2.4*. You can zoom into specific clusters to inspect them
    further. This is an important feature of the DataRobot environment as very few
    data scientists know about this idea or make use of it. The clusters are important
    because they highlight groups of interrelated features. These complex interdependencies
    are typically very important for understanding the business problem. Normally,
    the only people who know about these complex interdependencies are people with
    a lot of domain experience. Most others will not even be aware of these complexities.
    If you are new to a domain, then understanding these will give you an equivalent
    of multiple years of experience. Study these carefully, discuss them with your
    business experts to fully understand what they are trying to highlight, and then
    use these insights to improve your models as well as your business processes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that DataRobot provides a list of the top 10 strongest associations.
    It is important to note these associations and spend some time thinking about
    what they mean for your problem. Are these consistent with what you know about
    your domain, or are there some surprises? It is the surprises that often result
    in key insights that could prove to be valuable insights for your business. In
    the following list, you see a **View Feature Association Pairs** button. If you
    click on that button, you will see *Figure 2.5*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Feature association details'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.5_B17159.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – Feature association details
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: This graphic shows the relationship between two selected features in detail.
    In this example, one feature is categorical while the other is numeric. The diagram
    shows how the two are related and could provide additional insights into the problem.
    Be sure to investigate the relationships, especially the ones that might be counterintuitive.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can click on the specific features to see how they are distributed
    (*Figure 2.6*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Feature details'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.6_B17159.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – Feature details
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: This view shows a histogram of how the values are distributed and how they are
    related to the target values. Key things to focus on are ranges where you do not
    have enough data and where you have non-linearities. These could give you ideas
    about feature engineering. These are also areas where you ask the question why
    does the system exhibit this behavior?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: With this background work done, you are now ready to dive into modeling algorithms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are now hundreds of machine learning algorithms available to be used
    for a machine learning project, and more are being invented every day. DataRobot
    supports a wide array of open source machine learning algorithms, including several
    deep learning algorithms – Prophet, SparkML-based algorithms, and H2O algorithms.
    Let''s now take a look at what types of algorithms exist and what they are used
    for (*Figure 2.7*):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Machine learning algorithms'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.7_B17159-DESKTOP-C2VUV36.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – Machine learning algorithms
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Our focus will mostly be on the algorithm types that DataRobot supports. These
    algorithm types are described in the following sub-sections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Supervised learning algorithms are used when you can provide an answer (also
    called a label) as part of the training dataset. For supervised learning, you
    have to assign a feature of your dataset to be the answer, and the algorithm tries
    to learn to predict the answer by seeing multiple examples and learning from these
    examples. See *Figure 2.8* for the different types of answers:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Targets for supervised learning algorithms'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.8_B17159.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Targets for supervised learning algorithms
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'DataRobot functionality is primarily focused on supervised learning algorithms.
    Included in the set are deep learning algorithms as well as big data algorithms
    from SparkML and H2O. DataRobot has built-in best practices to select the best-suited
    algorithms for your problem and dataset. There are four major types of supervised
    learning problems:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regression problems are the ones where the answer (target) takes a numeric form
    (see *Figure 2.8*). Regression models try to fit a curve such that the error between
    the prediction and the actual value is minimized for the entire training dataset.
    Sometimes, even a classification problem can be set up as a numeric regression
    problem. In such cases, the answer is a number that can then be turned into a
    bin by using thresholds. Logistic regression is one such method that produces
    a value between zero and one. You can mark all answers below a certain threshold
    to be zero, and all above as ones. There are linear as well as non-linear regression
    algorithms that are used based on the problem. The models are assessed based on
    how well the regression line matches the data. Typical metrics used are **RMSE**,
    **MAPE**, **LogLoss**, and **Rsquared**. Typical algorithms used are **XGBoost**,
    **Elastic Net**, **Random Forest**, and **GA2M**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Binary classification problems have answers that can only take two distinct
    values (called classes). These could be in the form of 0 or 1, Yes or No, and
    so on. Please refer to *Figure 2.8* for an example of the target feature for binary
    classification. A typical issue that you commonly face is the problem of class
    imbalance. This happens when most of the dataset is biased toward one class. These
    are typically addressed by downsampling the overrepresented class when sufficient
    training data is present. When this is not possible, you can try oversampling
    the underrepresented class or use other methods. None of these methods is perfect,
    and sometimes you have to try different approaches to see what works best. DataRobot
    provides mechanisms to specify downsampling if needed. Some of the algorithms
    that are commonly used for binary classification are **logistic regression**,
    **k-nearest neighbors**, **tree-based algorithms**, **SVM**, and **Naïve Bayes**.
    In the case of classification problems, it is best to avoid using accuracy as
    a metric to assess results. The results are often shown in the form of a confusion
    matrix (described later in this chapter). DataRobot will automatically select
    an appropriate metric to use in such cases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiclass classification problems are the ones where you are trying to predict
    more than two classes or categories. For a simple example of what the target might
    look like, see *Figure 2.8*. Multiclass capability was added recently and many
    of the DataRobot features might not work with such problems. Since downsampling
    is not available, you might want to adjust your sampling prior to uploading your
    dataset into DataRobot. Also, note that you can frequently collapse your problem
    into a binary classification problem by collapsing the classes into two classes.
    That may or may not work for your use case, but it is an option if required. Also,
    not all algorithms are appropriate for multiclass problems. DataRobot will automatically
    select the appropriate algorithms to build the models for multiclass problems.
    Typical metrics to use are AUC, LogLoss, or Balanced Accuracy. The results are
    often shown in the form of a confusion matrix (described later in this chapter).
    Typical algorithms used are XGBoost, Random Forest, and TensorFlow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Time series/forecasting
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Time series or forecasting models are also referred to as time-aware models
    in DataRobot. In these problems, you have data that is changing over time and
    you are interested in predicting/forecasting a target value in the future (*Figure
    2.2*). DataRobot not only supports the usual algorithms for time series such as
    ARIMA, but can also adapt these problems to machine learning regression problems
    and then apply algorithms such as XGBoost to solve them. These problems require
    that the series should be transformed into stationary series and require extensive
    feature engineering to create time-based features. The problems also require that
    you take into account important events in the past that may repeat (such as holidays
    or major shopping days). Time series models also require special ways of handling
    validation and testing via a method called backtests:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Backtesting for time series problems'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.9_B17159.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 – Backtesting for time series problems
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: In backtesting, models are built using past data, and then tested using holdout
    data that is newer and has never been seen by the model. This time-based slicing
    of holdout data is also referred to as out-of-time validation. DataRobot automates
    many of these tasks for you, as we will see in more detail later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s review some of the main algorithms used in DataRobot. Here, we only
    provide a high-level overview of these algorithms These algorithms can be tuned
    for a given problem by changing their hyperparameters. For a more detailed understanding
    of any specific algorithm, you can refer to a machine learning book or the DataRobot
    documentation. Some of the important algorithms are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**. A random forest model is built by creating multiple decision
    tree models and then uses the mean of the output. This is done by creating bootstrap
    samples of the training data and building decision trees (*Figure 2.10*) on these
    samples:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Random forest'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.10_B17159.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – Random forest
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Random forest models handle missing data and non-linearities and have proven
    to work great in many situations. A random forest model can be used for regression
    as well as classification problems:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '**XGBoost**: Also known as **eXtreme** gradient boosted trees, are decision
    tree-based algorithms that have become very popular because they tend to produce
    very effective predictions and can handle missing values. They can handle non-linear
    problems and interactions between features. XGBoost builds upon random forest
    models by creating a random forest and then creating trees on the residuals of
    the previous trees. This way, every new set of trees is able to produce a better
    result. XGBoost can be used for regression as well as classification problems.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rulefit**: Rulefit models are ensembles of simple rules. You can think of
    these rules as being chained together like a decision tree. Rulefit models are
    much easier to understand as most people can relate to a combination of rules
    being applied to solve a problem. DataRobot typically builds this model to help
    you understand a problem and provide insights. You can go to the insights section
    of your **Models** tab and see the insights generated from a Rulefit model and
    how effective a given rule is for the problem. They can be used for classification
    as well as regression problems.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ElasticNet**, **Ridge regressor**, **Lasso regressor**: These models use
    regularization to make sure that the models are not overfitting and are not unnecessarily
    complex. Regularization is done by adding a penalty for adding more features,
    which in turn forces the models to either drop some features or reduce their relative
    impact. Lasso regressor (also known as **L1 regressor**) uses penalty weights
    that are the absolute values of the coefficients. The effect of using Lasso is
    that it tries to reduce the coefficients to zero, thereby selecting important
    features and removing the ones that do not contribute much. Ridge regressor (also
    known as **L2 regressor**) uses penalty weights that are squared coefficients.
    The impact of this is to reduce the magnitude of coefficients. **ElasticNet**
    is used to refer to linear models that use both Lasso and Ridge regularization
    to produce models that are simpler as well as regularized. This comes in handy
    when you have a lot of features that are correlated with each other.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic Regression**: Logistic regression is a non-linear regression model
    that is used for binary classification. The output is in the form of a probability
    with a value ranging from 0 to 1\. This is then typically used with a threshold
    to assign the value to be a 0 or a 1.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SVM** (**Support Vector Machine**): This is a classification algorithm that
    tries to find a vector that best separates classes. It is easy to see what this
    looks like in a two-dimensional space (*Figure 2.11*), but the algorithm is known
    to work well in high dimension spaces. Another benefit of SVM is its ability to
    handle non-linearity by using non-linear kernel functions, which can be used to
    linearize the problem:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Targets for supervised learning algorithms'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.11_B17159.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Targets for supervised learning algorithms
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**GA2M** (**Generalized Additive Model**): This is one of those rare algorithms
    that offers understandability, while also offering high accuracy even in a non-linear
    problem. The number "2" in the name represents its ability to model interactions
    between features. GAM model output is a summation of outputs of the effects of
    individual features that have been binned. Since GAM allows these effects to be
    non-linear, it can capture the non-linear nature of the problem. The results of
    the model can be represented as a simple table that shows you the contribution
    of each feature to the overall answer. This type of table representation is easily
    understandable by most people. For industries or use cases where understandability
    and explainability are very important, this is perhaps one of the best options
    you can choose.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbors**: This is a very straightforward algorithm that finds
    the k closest data points (based on a specific way of computing distance). Now
    it finds the classification answers for these k points. It then determines the
    answer with the most votes and then assigns that as the answer. The default distance
    metric used is **Euclidian** distance, but DataRobot chooses the appropriate metric
    based on the dataset. A user can also specify a specific distance metric to be
    used.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**. TensorFlow is a deep learning model that is based on deep neural
    networks. A deep neural network is one that has hidden deep layers made up of
    ensembles of artificial neurons. The neurons carry highly non-linear activation
    functions that allow them to fit highly non-linear problems. These models are
    very good at producing high accuracy without the need for feature engineering,
    but they do require a lot more training data as compared to other algorithms.
    These models are generally considered very opaque and are prone to overfitting
    and are therefore not suitable for some applications. They are especially successful
    for applications where the features and feature engineering are hard to extract,
    for example, image processing. These models can be used for regression as well
    as classification problems.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras Neural Network**: Keras is a high-level deep learning library built
    on top of TensorFlow that allows many types of deep learning models to be incorporated
    into DataRobot. Being a higher-level library, it makes building a TensorFlow model
    a lot easier. Everything described in the preceding section applies to Keras.
    The particular implementation in DataRobot is well suited for sparse datasets
    and is particularly useful for text processing and classification problems.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning problems are those where you are not provided with an
    answer or a label. Examples of such problems are clustering or anomaly detection.
    DataRobot does not offer much for these problems, but it does have some capability
    for anomaly or outlier detection. These are problems where you have data points
    that are unusual in a way that happens very rarely. Examples include fraud detection,
    cybersecurity breach detection, failure detection, and data outlier detection.
    DataRobot allows you to set up a project without a target and it will then attempt
    to identify anomalous data points. For any clustering problems, you should try
    to use Python or R to create clustering models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning problems are where you want to learn a series of decisions
    to be taken by an agent such that you achieve a certain goal. This goal is associated
    with a reward that is given to the agent for achieving the goal either completely
    or partially. There is no dataset available for this training, so the agent must
    try multiple times (with different strategies) and learn something on each attempt.
    Over many attempts, the agent will learn the strategy or rules that produce the
    best reward. As you can now guess, these algorithms work best when you do not
    have data, but you can experiment repeatedly in the real world (or a synthetic
    world). As we discussed before, DataRobot is not a suitable tool for such problems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble/blended models
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensembling is a technique for creating a model that aggregates or blends predictions
    of other models. Different algorithms are sometimes able to exploit different
    aspects of the problem or dataset better. This means that many times, you can
    increase prediction accuracy by combining several good models. This, of course,
    comes with increasing complexity and cost. DataRobot offers many blending approaches
    and, in most circumstances, builds the blended model automatically for your project.
    You can then evaluate whether the increase in accuracy is enough to justify the
    additional complexity.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Blueprints
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In DataRobot, every model is associated with a blueprint. A blueprint is a
    step-by-step recipe used by DataRobot to train a specific model. See *Figure 2.12*
    for an example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Model blueprint'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.12_B17159.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – Model blueprint
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The blueprint shows all the steps taken by DataRobot to build that specific
    model, including any data preparation and feature engineering done by DataRobot.
    Clicking on any specific box will show more details on the actions taken, parameters
    used, and documentation of the particular algorithm used. This also serves as
    great documentation for your modeling project that is automatically created for
    you.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how to determine how well an algorithm did. For this, we
    will require some performance metrics.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataRobot offers a wide range of performance metrics for the models. You have
    to specify the metric you want to use to optimize the models for your project.
    Typically, the best metric to use is the one recommended by DataRobot. DataRobot
    does compute the other metrics as well once the model is built, so you can review
    the results of your model across multiple metrics. Please keep in mind that no
    metric is perfect for every situation, and you should be careful in selecting
    the metric for evaluating your results. Listed here are some details regarding
    commonly used metrics:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**RMSE** (**Root Mean Squared Error**): RMSE is a metric that first computes
    the square of errors (the difference between actual and predicted). These are
    then averaged over the entire dataset and then we compute a square root of that
    average. Given that this metric is dependent on the scale of the values, its interpretation
    is dependent on the problem. You cannot compare RMSE for two different datasets.
    This metric is frequently used for regression problems when the data is not highly
    skewed.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAPE** (**Mean Absolute Percentage Error**): MAPE is somewhat similar to
    RMSE in the sense that it first computes the absolute value of the percentage
    error. Then, these values are averaged over the dataset. Given that this metric
    is scaled in terms of percentage, it is easier to compare MAPE for different datasets.
    However, you have to be mindful of the fact that the percentage error for very
    small values (or zero values) tends to look very big.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SMAPE** (**Symmetric MAPE**): SMAPE is similar to MAPE, but addresses some
    of the shortcomings discussed above. SMAPE bounds the upper percentage value so
    that errors from small values do not overpower the metric. This makes SMAPE a
    good metric that you can easily compare across different problems.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: Accuracy is one of the metrics used for classification problems.
    It can be represented as follows:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accuracy = number of correct predictions/number of total predictions*'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is essentially the ratio of the number of correct predictions and all predictions.
    For unbalanced problems, this metric can be misleading, hence it is never used
    by itself to determine how well a model did. It is typically used in combination
    with other metrics.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Balanced Accuracy**: Balanced accuracy overcomes the issues with accuracy
    by normalizing the accuracy across the two classes being predicted. Let''s say
    that the two classes are A and B:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (a) *Accuracy rate for A = number of correct A predictions/total number of As*
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b) *Accuracy rate for B = number of correct B predictions/total number of Bs*
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (c) *Balanced accuracy = accuracy rate for A + accuracy rate for B/2*
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Balanced accuracy is essentially the average of the accuracy rate for A and
    the accuracy rate for B.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**AUC** (**Area Under the ROC Curve**): AUC is the area under the **ROC** (**Received
    Operator Characteristic**) curve. This metric is frequently used for classification
    problems as this also overcomes the deficiencies associated with the accuracy
    metric. The ROC curve represents the relationship between the true positive rate
    and the false positive rate. The AUC goes from 0 to 1 and it shows how well the
    model discriminates between the two classes. A value of 0.5 represents a random
    model, so you would want the AUC for your model to be greater than 0.5.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma Deviance**: Gamma deviance is used for regression problems when the
    target values are gamma-distributed. For such targets, gamma deviance measures
    twice the average deviance (using the log-likelihood function) of the predictions
    from the actuals. A model that fits perfectly will have a deviance of zero.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poisson Deviance**: Poisson deviance is used for regression problems when
    the aim is to count data that is skewed. It works in a way that is very similar
    to gamma deviance.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LogLoss**: LogLoss (also known as cross-entropy loss) is a measure of the
    inaccuracy of predicted probabilities for a classification problem. A value of
    0 indicates a perfect model, and as the model becomes worse, the logloss value
    increases.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rsquared**: Rsquared is a metric used for regression problems that tells
    how well the fitted line represents the dataset. Its value ranges between 0 and
    1\. 0 indicates a poor model that explains none of the variation, while a value
    of 1 indicates a perfect model that explains 100% of the variation. It is one
    of the most commonly used metrics, but it can suffer from the problem that you
    can increase it by adding more variables without necessarily improving the model.
    It is also not suitable for non-linear problems.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have discussed some of the commonly used metrics, let's look at
    how to look at other results to assess the quality of your model, and the effects
    of different features on your model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the results
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss various visualizations of metrics and other
    information to understand the results of the modeling exercise. These are important
    visualizations that need to be inspected carefully in addition to looking at the
    model metrics discussed in the previous section. These visualizations are generated
    automatically by DataRobot for any model that it trains.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Lift chart
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The lift chart shows how effective the model is at predicting the target values.
    As the number of data points is typically very large to show in one graphic, the
    lift chart sorts the output and aggregates the data into multiple bins. It then
    compares the averages of predictions and actuals in each bin (*Figure 2.13*):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Lift chart'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.13_B17159.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – Lift chart
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The preceding lift chart shows how the predictions have been sorted from low
    to high and then binned (60 bins in this case). You can now see the average prediction
    and average actual value in each bin. This gives you a sense of how well the model
    is doing across the entire spectrum. You can see whether there are ranges where
    the model is doing worse. If the model is not doing well in a range that is important
    to your business, you can then investigate further to see how you can improve
    the model in that range. You can also inspect different models to see whether
    there is a model that does better in the region that is more important. Lift charts
    are more meaningful for regression problems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix (binary and multiclass)
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For classification problems, one of the best ways to assess model results is
    by looking at the confusion matrix and its associated metrics (*Figure 2.14*).
    This tab is available for multiclass problems:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Confusion matrix'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.14_B17159.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 – Confusion matrix
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix maps predicted versus actual counts (frequency) for each
    class. Let's look at the sedan column. The big green circle indicates how many
    times we correctly classified a sedan as a sedan. In that column, you will also
    see red dots where the model predicted it to be a sedan, but it is a different
    type. You can see these for all classes. The relative scales should give you an
    idea of how well your model did and where it is having difficulty.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'If you select a specific class, you can look at the class-specific confusion
    matrix on the right. You can see two columns (+ for predicting a sedan, - for
    predicting something that isn''t a sedan). Similarly, you see two rows (+ where
    it is a sedan, and - for when it is not a sedan). You also see some critical definitions
    and metrics:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives** (**TP**) = Where it is a sedan and is predicted as a sedan'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives** (**FP**) = Where it is not a sedan but is predicted as
    a sedan'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives** (**TN**) = Where it is not a sedan and is predicted as not
    being a sedan'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives** (**FN**) = Where it is a sedan but is predicted as not
    being a sedan'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these, we can now compute some specific metrics for this class:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision = correct fraction of predictions = TP/All Positive Predictions
    = TP/(TP+FP)*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recall = correct fraction of actuals = TP/All Positive Actuals = TP/(TP+FN)*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1 Score = harmonic mean of precision and recall. So, 1/F1 = 1/Precision +
    1/Recall*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tab is available for binary classification problems. The **ROC** (**Receiver
    Operator Characteristic**) curve is the relationship between the true positive
    rate and the false positive rate. The area under this curve is known as AUC. It
    goes from 0 to 1 and it shows how well the model discriminates between the two
    classes (*Figure 2.15*):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – ROC curve and confusion matrix'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.15_B17159.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 – ROC curve and confusion matrix
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: You can also see the confusion matrix (described earlier) and the associated
    metrics for the two classes. You can move the thresholds and assess the resulting
    trade-offs and cumulative gains. Since most problems are not symmetric in the
    sense that true positives have different business values compared to true negatives,
    you should select the threshold that makes sense for your business problem.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy over time
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tab is available for time series problems (*Figure 2.16*) and compares
    the actual versus predicted values over time for a series:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Model accuracy over time'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.16_B17159.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 – Model accuracy over time
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: You can view these values for the backtests or the holdout datasets. The diagram
    will clearly show where the model is not performing well and what you might want
    to focus on to improve your model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Feature impacts
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides model performance, one of the first things you want to understand is
    how impactful the features are in terms of your model''s performance. The **Feature
    Impacts** tab (*Figure 2.17*) is perhaps the most critical for understanding your
    model:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Feature impacts'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.17_B17159.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 – Feature impacts
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The graphic shows a sorted list of the most important features. For each feature,
    you can see the relative impact that a feature has on this model. You can see
    which features contribute very little; this can be used to create new feature
    lists by removing some of the features that have very little impact.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Feature Fit
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Feature Fit** tab (*Figure 2.18*) shows an alternative view of the contribution
    of a feature. The graphic shows the features ranked by their importance:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Feature Fit'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.18_B17159.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18 – Feature Fit
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: For the selected feature, it shows how the predictions compare to actuals for
    the range of values of a feature. Reviewing these graphs for the key features
    can provide a lot of insight about how a feature impacts the results and range
    of values that perform better and ranges where it performs the worst. This could
    sometimes highlight the regions where you might need to collect more data to improve
    your model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Feature Effects
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Feature Effects** show information that is very similar to **Feature Fit**
    (*Figure 2.19*). In this graphic, the features are sorted by **Feature Impacts**.
    Also, **Feature Effects** are focused on partial dependence:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Feature Effects and Partial Dependence'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.19_B17159.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19 – Feature Effects and Partial Dependence
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence plots are one of the most important plots that you want to
    study carefully. These plots tell you how a change in the value of a feature impacts
    the change in the average value of the target over a range of values for the other
    features. This insight is critical to understanding the business problem, understanding
    what the model is doing, and, more importantly, what aspects of the model are
    actionable and what range of values will produce the maximum impact.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Explanations
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Prediction Explanations** describe the reasons for a specific prediction
    in terms of feature values for the specific instance or row that is being scored
    (*Figure 2.20*). Note that this is different from **Feature Impacts**, which tell
    you the importance of a feature at a global level:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Prediction Explanations'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.20_B17159.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.20 – Prediction Explanations
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Explanations** can be generated for an entire dataset or a subset
    of data, as shown in the preceding screenshot. For example, it will provide the
    top three reasons why the model predicted a specific value. These explanations
    are sometimes required for regulatory reasons in certain use cases, but it is
    a good idea to produce these explanations as they do help in understanding why
    a model predicts a certain way and can be very useful in validating or catching
    errors in a model. DataRobot uses two algorithms for computing the explanations:
    **XEMP** (**exemplar-based explanations**) or **Shapley values**. XEMP is supported
    for a broader range of models and is selected by default. Shapley values are described
    in the next section.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Shapley** **values** (**SHAP**) are an alternative mechanism for producing
    prediction explanations (*Figure 2.21*). If you want to use SHAP for explanations,
    you have to specify this in the advanced options during the project setup before
    you press the **Start** button. Once DataRobot starts building the models, you
    cannot switch to SHAP. SHAP values are only available for linear or tree-based
    models and are not available for ensemble models:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – SHAP-based explanations'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.21_B17159.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21 – SHAP-based explanations
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: SHAP values are based on cooperative game theory, which tries to assign values
    to contributions of a team member in a collaborative project. In the context of
    machine learning, it tries to assign the value contribution of a specific feature
    when there is a team of features collaborating to make a prediction. SHAP values
    are additive and you can easily see how much of the final answer is due to a specific
    feature value.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered some of the basic machine learning concepts that
    will come in handy as we go through the remaining chapters, and they will also
    be useful in your data science journey. Please note that we have only covered
    concepts at a high level, and depending on your job role, you might want to explore
    some areas in more detail. We have also related this material to how DataRobot
    performs certain functions and where you need to pay closer attention.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, this has given you some insights into what DataRobot will be displaying
    and where to focus your attention in different stages of your project. Since DataRobot
    automates a good chunk of model building and prediction tasks, it might be tempting
    to ignore many of the outputs that DataRobot is automatically producing for you.
    Please resist that temptation. DataRobot software is taking considerable pains
    and resources to produce those outputs for a very good reason. It is also doing
    much of the grunt work for you, so please take advantage of those capabilities.
    Specifically, we have covered the following: What are the things to watch out
    for during data preparation? What data visualizations are important for gaining
    an understanding of your dataset? What are the key machine learning algorithms,
    and when do you use them? How do you measure the goodness of your model results?
    How do you assess model performance and understand what the model is telling you
    about your problem?'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the basics, we will start our data science journey in the next
    chapter by learning how to understand the business problem and how to turn it
    into a specification that can be solved by using machine learning.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
