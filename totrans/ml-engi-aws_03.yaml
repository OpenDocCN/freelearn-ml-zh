- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041), *Deep Learning AMIs*, we
    used **AWS Deep Learning AMIs** (**DLAMIs**) to set up an environment inside an
    EC2 instance where we could train and evaluate a deep learning model. In this
    chapter, we will take a closer look at **AWS Deep Learning Containers** (**DLCs**),
    which can run consistently across multiple environments and services. In addition
    to this, we will discuss the similarities and differences between DLAMIs and DLCs.
  prefs: []
  type: TYPE_NORMAL
- en: The hands-on solutions in this chapter focus on the different ways we can use
    DLCs to solve several pain points when working on **machine learning** (**ML**)
    requirements in the cloud. For example, container technologies such as **Docker**
    allow us to make the most of our running EC2 instances since we’ll be able to
    run different types of applications inside containers, without having to worry
    about whether their dependencies would conflict or not. In addition to this, we
    would have more options and solutions available when trying to manage and reduce
    costs. For one thing, if we were to use the container image support of **AWS Lambda**
    (a serverless compute service that lets us run our custom backend code) to deploy
    our deep learning model inside a serverless function, we would be able to significantly
    reduce the infrastructure costs associated with having an inference endpoint running
    24/7\. At the same time, with a serverless function, all we need to worry about
    is the custom code inside the function since AWS will take care of the infrastructure
    where this function would run.
  prefs: []
  type: TYPE_NORMAL
- en: In the scenario discussed in the *Understanding how AWS pricing works for EC2
    instances* section of the previous chapter, we were able to reduce the cost of
    running a 24/7 inference endpoint to about *$69.12 per month* using an `m6i.large`
    instance. It is important to note that this value would more or less remain constant,
    even if this inference endpoint is not receiving any traffic. In other words,
    we might be paying *$69.12 per month* for a resource that could be either underutilized
    or unused. If we were to set up a staging environment that is configured the same
    as the production environment, this cost would double and it’s pretty much guaranteed
    that the staging environment resources would be severely underutilized. At this
    point, you might be wondering, *Is it possible for us to reduce this cost further?*
    The good news is that this is possible, so long as we can design a more optimal
    architecture using the right set of tools, services, and frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: We will start the hands-on section of this chapter by training a **PyTorch**
    model inside a DLC. This model will be uploaded to a custom container image that
    will then be used to create an **AWS Lambda** function. After that, we will create
    an **API Gateway** HTTP API that accepts an HTTP request and triggers the AWS
    Lambda function with an event containing the input request data. The AWS Lambda
    function will then load the model we trained to perform ML predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with AWS Deep Learning Containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essential prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AWS Deep Learning Containers to train an ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless ML deployment with Lambda’s container image support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While working on the hands-on solutions of this chapter, we will cover several
    *serverless* services such as AWS Lambda and Amazon API Gateway, which allow us
    to run applications without having to manage the infrastructure ourselves. At
    the same time, the cost of using these resources scales automatically, depending
    on the usage of these resources. In a typical setup, we may have an EC2 instance
    running 24/7 where we will be paying for the running resource, regardless of whether
    it is being used. With AWS Lambda, we only need to pay when the function code
    runs. If it only runs for a few seconds per month, then we may pay close to zero
    for that month!
  prefs: []
  type: TYPE_NORMAL
- en: With these points in mind, let’s begin this chapter with a quick introduction
    to how AWS DLCs work.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we must have the following ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (preferably Chrome or Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account that was used in the first two chapters of this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the Cloud9 environment that you prepared in the *Creating your Cloud9
    environment* and *Increasing the Cloud9 storage* sections of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Jupyter notebooks, source code, and other files used for each chapter are
    available in this book’s GitHub repository at https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended that you use an IAM user with limited permissions instead
    of the root account when running the examples in this book. We will discuss this,
    along with other security best practices, in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*. If you are just starting using
    AWS, you may proceed with using the root account in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with AWS Deep Learning Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers allow developers, engineers, and system administrators to run processes,
    scripts, and applications inside consistent isolated environments. This consistency
    is guaranteed since these containers are launched from container images, similar
    to how EC2 instances are launched from **Amazon Machine Images** (**AMIs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that we can run different isolated containers at the
    same time inside an instance. This allows engineering teams to make the most of
    the computing power available to the existing instances and run different types
    of processes and workloads, similar to what we have in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Running multiple containers inside a single EC2 instance ](img/B18638_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Running multiple containers inside a single EC2 instance
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular container management solutions available is **Docker**.
    It is an open source containerization platform that allows developers and engineers
    to easily build, run, and manage containers. It involves the usage of a **Dockerfile**,
    which is a text document containing instructions on how to build container images.
    These container images are then managed and stored inside container registries
    so that they can be used at a later time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Docker images are used to create containers. Docker images are like ZIP files
    that package everything needed to run an application. When a Docker container
    is run from a container image (using the `docker run` command), the container
    acts like a virtual machine, with its environment isolated and separate from the
    server where the container is running.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better idea of how containers and container images work,
    let’s proceed by discussing what DLCs are and how these are used to speed up the
    training and deployment of ML models. One of the key benefits when using AWS DLCs
    is that most of the relevant ML packages, frameworks, and libraries are installed
    in the container images already. This means that ML engineers and data scientists
    no longer need to worry about installing and configuring the ML frameworks, libraries,
    and packages. This allows them to proceed with preparing the custom scripts used
    for training and deploying their deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Since DLC images are simply prebuilt container images, these can be used in
    any AWS service where containers and container images can be used. These AWS services
    include **Amazon EC2**, **Amazon Elastic Container Service** (**ECS**), **Amazon
    Elastic Kubernetes Service (EKS)**, **Amazon SageMaker**, **AWS CodeBuild**, **AWS
    Lambda**, and more.
  prefs: []
  type: TYPE_NORMAL
- en: With these in mind, let’s proceed with training and deploying a deep learning
    model using AWS Deep Learning Containers!
  prefs: []
  type: TYPE_NORMAL
- en: Essential prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will ensure that the following prerequisites are ready
    before proceeding with the training steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We will prepare a Cloud9 environment and ensure it has been set up so that we
    can train the model and build the custom container image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will prepare a training dataset that will be used when training the deep
    learning model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preparing the Cloud9 environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first part of this chapter, we will run our Deep Learning Container
    inside an EC2 instance, similar to what’s shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Running a Deep Learning Container inside an EC2 instance ](img/B18638_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Running a Deep Learning Container inside an EC2 instance
  prefs: []
  type: TYPE_NORMAL
- en: This container will serve as the environment where the ML model is trained using
    a script that utilizes the **PyTorch** framework. Even if PyTorch is not installed
    in the EC2 instance, the training script will still run successfully since it
    will be executed inside the container environment where PyTorch is preinstalled.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering what PyTorch is, it is one of the most popular open source
    ML frameworks available. You may check out [https://pytorch.org/](https://pytorch.org/)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will make sure that our Cloud9 environment is
    ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type `cloud9` in the search bar. Select **Cloud9** from the list of results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Navigating to the Cloud9 console ](img/B18638_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Navigating to the Cloud9 console
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the region is currently set to `us-west-2`). Make sure
    that you change this to where you created the Cloud9 instance in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: Open the Cloud9 environment you created in the *Creating your Cloud9 environment*
    section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML
    Engineering on AWS*, by clicking the `us-west-2`) where the Cloud9 environment
    was created in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction
    to ML Engineering on AWS*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you skipped the first chapter, make sure that you complete the *Creating
    your Cloud9 environment* and *Increasing the Cloud9 storage* sections of that
    chapter before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Terminal of the Cloud9 environment, run the following `bash` commands
    to create the `ch03` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will use this directory as our current working directory for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our Cloud9 environment ready, let’s proceed with downloading
    the training dataset so that we can train our deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the sample dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training dataset we will use in this chapter is the same dataset we used
    in [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041), *Deep Learning AMIs*. It has
    two columns that correspond to the continuous *x* and *y* variables. Later in
    this chapter, we will also generate a regression model using this dataset. The
    regression model is expected to accept an input *x* value and return a predicted
    *y* value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will download the training dataset into our Cloud9
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to create the `data` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s download the training data CSV file by using the `wget` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `head` command to inspect what our training data looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should give us rows of *(x,y) pairs*, similar to what is shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The first few rows of the training_data.csv file ](img/B18638_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The first few rows of the training_data.csv file
  prefs: []
  type: TYPE_NORMAL
- en: Since we started this section inside the `ch03` directory, it is important to
    note that the `training_data.csv` file should be inside the `ch03/data` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the prerequisites ready, we can proceed with the training step.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS Deep Learning Containers to train an ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, you might be wondering what makes a deep learning model different
    from other ML models. Deep learning models are networks of interconnected nodes
    that communicate with each other, similar to how networks of neurons communicate
    in a human brain. These models make use of multiple layers in the network, similar
    to what we have in the following diagram. Having more layers and more neurons
    per layer gives deep learning models the ability to process and learn complex
    non-linear patterns and relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Deep learning model ](img/B18638_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Deep learning model
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning has several practical applications in **natural language processing**
    (**NLP**), **computer vision**, and **fraud detection**. In addition to these,
    here are some of its other applications and examples as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**): These can be used to generate
    realistic examples from the original dataset, similar to what we had in the *Generating
    a synthetic dataset using a deep learning model* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Reinforcement Learning**: This utilizes deep neural networks and reinforcement
    learning techniques to solve complex problems in industries such as robotics and
    gaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These past couple of years, the training and deployment of deep learning models
    have been greatly simplified with deep learning frameworks such as **PyTorch**,
    **TensorFlow**, and **MXNet**. AWS DLCs speed things up further by providing container
    images that already come preinstalled with everything you need to run these ML
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the list of available DLC images here: [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    Note that these container images are categorized by (1) the installed ML framework
    (**PyTorch**, **TensorFlow**, or **MXNet**), (2) the job type (*training* or *inference*),
    and (3) the installed Python version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will use the DLC image that’s been optimized to
    train PyTorch models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s download the `train.py` file by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before we proceed, let’s check the contents of the `train.py` file by opening
    it from the `File` tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Opening the train.py file from the File tree ](img/B18638_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Opening the train.py file from the File tree
  prefs: []
  type: TYPE_NORMAL
- en: 'We should see a script that makes use of the training data stored in the `data`
    directory to train a deep learning model. This model gets saved in the `model`
    directory after the training step has been completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – The main() function of the train.py script file ](img/B18638_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – The main() function of the train.py script file
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the `main()` function of our `train.py` script performs
    the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) defines the model using the `prepare_model()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) loads the training data using the `load_data()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (3) performs the training step using the `fit()` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (4) saves the model artifacts using the `torch.save()` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last block of code in the preceding screenshot simply runs the `main()`
    function if `train.py` is being executed directly as a script.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the complete `train.py` script here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/train.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/train.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create the `model` directory using the `mkdir` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Later, we will see that the model output gets saved inside this directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `tree` utility by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the `tree` utility we just installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a tree-like structure, similar to what we have in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Results after using the tree command ](img/B18638_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Results after using the tree command
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the `train.py` script is in the `ch03` directory,
    which is where the `data` and `model` directories are located as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `train.sh` file using the `wget` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we check the contents of the `train.sh` file, we should see the following
    lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `train.sh` script first authenticates with **Amazon Elastic Container Registry**
    (a fully managed Docker container registry where we can store our container images)
    so that we can successfully download the training container image. This container
    image has *PyTorch 1.8.1* and *Python 3.6* preinstalled already.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The code in the `train.sh` script assumes that we will run the training experiment
    inside an EC2 instance (where the Cloud9 environment is running) in the *Oregon*
    (`us-west-2`) region. Make sure that you replace `us-west-2` with the appropriate
    region code. For more information on this topic, feel free to check out https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.xhtml.
  prefs: []
  type: TYPE_NORMAL
- en: The `docker run` command first downloads the specified container image and creates
    a running container process using that image. After that, the contents of the
    current working directory are “copied” to the container after the current working
    directory (`ch03`) is mounted to the container using the `-v` flag when running
    the `docker run` command. We then set the working directory to where our files
    were mounted (`/env`) inside the container using the `-w` flag. Once all the steps
    are complete, the `train.py` script is executed inside the environment of the
    running container.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://docs.docker.com/engine/reference/run/](https://docs.docker.com/engine/reference/run/)
    for more information on how to use the `docker run` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better idea of what will happen when we execute the `train.sh`
    file, let’s run it using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a set of logs, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Logs generated while running the train.sh script ](img/B18638_03_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Logs generated while running the train.sh script
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `train.sh` script ran a container that invoked the `train.py` (Python)
    script to train the deep learning model. In the preceding screenshot, we can see
    the logs that were generated by the `train.py` script as it iteratively updates
    the weights of the neural network to improve the quality of the output model (that
    is, reducing the loss per iteration so that we can minimize the error). It is
    important to note that this `train.py` script makes use of **PyTorch** to prepare
    and train a sample deep learning model using the data provided.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason why we’re using a deep learning container image that has
    *PyTorch 1.8.1* and *Python 3.6* preinstalled already.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take 5 to 10 minutes to complete. Feel free to get a cup of coffee
    or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training script has finished running, let’s check whether the `model`
    directory contains a `model.pth` file using the `tree` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a tree-like structure, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Verifying whether the model was saved successfully ](img/B18638_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Verifying whether the model was saved successfully
  prefs: []
  type: TYPE_NORMAL
- en: This `model.pth` file contains the serialized model we have trained using the
    `train.py` script. This file was created using the `torch.save()` method after
    the model training step was completed. Feel free to check out [https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml](https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The generated `model.pth` file allows us to use the parameters of the model
    to make predictions (after the model has been loaded from the file). For example,
    if our model makes use of an equation such as *ax^2 + bxy + cy^2 = 0*, the *a*,
    *b*, and *c* values are the model parameters. With this, if we have *x* (which
    is the independent variable), we can easily compute the value of *y*. That said,
    we can say that determining *a*, *b*, and *c* is the task of the training phase,
    and that determining *y* given *x* (and given *a*, *b*, and *c*) is the task of
    the inference phase. By loading the `model.pth` file, we can proceed with the
    inference phase and compute for the predicted value of *y* given an input *x*
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Wasn’t that easy? With the training step complete, we will proceed with the
    deployment step in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless ML deployment with Lambda’s container image support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the `model.pth` file, what do we do with it? The answer is
    simple: we will deploy this model in a serverless API using an **AWS Lambda**
    function and an **Amazon API Gateway** HTTP API, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Serverless ML deployment with an API Gateway and AWS Lambda
    ](img/B18638_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Serverless ML deployment with an API Gateway and AWS Lambda
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the HTTP API should be able to accept *GET* requests from “clients”
    such as mobile apps and other web servers that interface with end users. These
    requests then get passed to the AWS Lambda function as input event data. The Lambda
    function then loads the model from the `model.pth` file and uses it to compute
    the predicted *y* value using the *x* value from the input event data.
  prefs: []
  type: TYPE_NORMAL
- en: Building the custom container image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our AWS Lambda function code needs to utilize **PyTorch** functions and utilities
    to load the model. To get this setup working properly, we will build a custom
    container image from an existing DLC image optimized for **PyTorch** inference
    requirements. This custom container image will be used for the environment where
    our AWS Lambda function code will run through AWS Lambda’s container image support.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on AWS Lambda’s container image support, check out https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that a variety of DLC images are available for us to
    choose from. These images are categorized based on their job type (*training versus
    inference*), installed framework (*PyTorch versus TensorFlow versus MXNet versus
    other options*), and installed Python version (*3.8 versus 3.7 versus 3.6 versus
    other options*). Since we are planning to use a container where a **PyTorch**
    model can be loaded and used to perform predictions, we will be choosing a **PyTorch**
    DLC *optimized for inference* as the base image when building the custom Docker
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps focus on building a custom container image from an existing
    DLC image:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you are inside the `ch03` directory by running the `pwd` command in
    the Terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, run the following commands to download `dlclambda.zip` and extract its
    contents inside the `ch03` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This ZIP file contains the files and scripts needed to build the custom container
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `tree` command to see what the `ch03` directory looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a tree-like structure, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Results after running the tree command ](img/B18638_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Results after running the tree command
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, several new files have been extracted from the `dlclambda.zip` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Dockerfile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`app/app.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`build.sh`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`download-rie.sh`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invoke.sh`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run.sh`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss each of these files in detail as we go through the steps in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the File tree, locate and open the `app.py` file located inside the `ch03/app`
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.13 – app.py Lambda handler implementation ](img/B18638_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – app.py Lambda handler implementation
  prefs: []
  type: TYPE_NORMAL
- en: This file contains the AWS Lambda handler implementation code, which (1) loads
    the model, (2) extracts the input *x* value from the event data, (3) computes
    for the predicted *y* value using the model, and (4) returns the output *y* value
    as a string.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Completing and testing the serverless API setup* section near the end
    of this chapter, we will set up an HTTP API that accepts a value for `x` via the
    URL query string (for example, `https://<URL>/predict?x=42`). Once the request
    comes in, Lambda will call a handler function that contains the code to handle
    the incoming request. It will load the deep learning model and use it to predict
    the value of `y` using the value of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the complete `app/app.py` file here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/app/app.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/app/app.p).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the `model.pth` file from the `model` directory into the `app/model` directory
    using the `cp` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you only load ML models from trusted sources. Inside `app/app.py`,
    we are loading the model using `torch.load()`, which can be exploited by attackers
    with a model containing a malicious payload. Attackers can easily prepare a model
    (with a malicious payload) that, when loaded, would give the attacker access to
    your server or resource running the ML scripts (for example, through a **reverse
    shell**). For more information on this topic, you may check the author’s talk
    on how to hack and secure ML environments and systems: [https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=8](https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=8).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s make the `build.sh`, `download-rie.sh`, `invoke.sh`, and `run.sh`
    script files executable using the `chmod` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before running the `build.sh` command, let’s check the script’s contents using
    the `cat` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a single line of code, similar to what we have in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `docker build` command builds a Docker container image using the instructions
    specified in the Dockerfile in the current directory. *What does this mean?* This
    means that we are building a container image using the relevant files in the directory
    and we’re using the instructions in the Dockerfile to install the necessary packages
    as well. This process is similar to preparing the *DNA* of a container, which
    can be used to create new containers with an environment configured with the desired
    set of tools and packages.
  prefs: []
  type: TYPE_NORMAL
- en: Since we passed `dlclambda` as the argument to the `-t` flag, our custom container
    image will have the `dlclambda:latest` name and tag after the build process completes.
    Note that we can replace the latest tag with a specific version number (for example,
    `dlclambda:3`), but we will stick with using the `latest` tag for now.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the `docker build` command, feel free to check out https://docs.docker.com/engine/reference/commandline/build/.
  prefs: []
  type: TYPE_NORMAL
- en: We must check the contents of the Dockerfile as well. What happens when we build
    the container image using this Dockerfile?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following DLC image is used as the base image for the two stages of the
    build: `https://763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.8.1-cpu-py36-ubuntu18.04`.
    It is important to note that this Dockerfile makes use of **multi-stage builds**
    to ensure that the final container does not contain the unused artifacts and files
    from the previous build stages.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the **Lambda Runtime Interface Client** is installed. This allows any
    custom container image to be compatible for use with AWS Lambda.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The `/function` directory is created. The contents of the `app/` directory (inside
    the `ch03` directory of the Cloud9 environment) are then copied to the `/function`
    directory inside the container.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ENTRYPOINT` is set to `/opt/conda/bin/python -m awslambdaric`. `CMD` is then
    set to `app.handler`. The `ENTRYPOINT` and `CMD` instructions define which command
    is executed when the container starts to run.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A `FROM` instructions within a single Dockerfile. Each of these `FROM` instructions
    corresponds to a new build stage where artifacts and files from previous stages
    can be copied. With a multi-stage build, the last build stage produces the final
    image (which ideally does not include the unused files from the previous build
    stages).
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected final output would be a container image that can be used to launch
    a container, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Lambda Runtime Interface Client ](img/B18638_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Lambda Runtime Interface Client
  prefs: []
  type: TYPE_NORMAL
- en: 'If this container is launched without any additional parameters, the following
    command will execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This will run the `handler()` function of our `app.py` file to process AWS Lambda
    events. This `handler()` function will then use the deep learning model we trained
    in the *Using AWS Deep Learning Containers to train an ML model* section to make
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the Dockerfile here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/Dockerfile](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/Dockerfile).'
  prefs: []
  type: TYPE_NORMAL
- en: Before running the `build.sh` script, make sure that you replace all instances
    of `us-west-2` in the Dockerfile with the appropriate region code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run the `build.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we need to check whether the size of the custom container image exceeds
    10 GB using the `docker images` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We should see that the container image size of `dlclambda` is `4.61GB`. It is
    important to note that there is a 10 GB limit when using container images for
    Lambda functions. The image size of our custom container image needs to be below
    10 GB if we want these to be used in AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, our custom container image is ready. The next step is to test
    the container image locally before using it to create an AWS Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the container image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can test the container image locally using the **Lambda Runtime Interface
    Emulator**. This will help us check whether our container image will run properly
    when it is deployed to AWS Lambda later.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next couple of steps, we will download and use the Lambda Runtime Interface
    Emulator to check our container image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `cat` command to check the contents of `download-rie.sh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should print the following block of code as output in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `download-rie.sh` script simply downloads the Lambda Runtime Interface Emulator
    binary and makes it executable using the `chmod` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the `download-rie.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `cat` command to check the contents of `run.sh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should see a `docker run` command with several parameter values, similar
    to what we have in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s quickly check the parameter values that were passed to each of the flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-v`: `~/.aws-lambda-rie` is a directory outside of the running Docker container
    to be mounted to `/aws-lambda` (which is inside the container).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-p`: This binds port `8080` of the container to port `9000` of the instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--entrypoint`: This will override the default `ENTRYPOINT` command that gets
    executed when the container starts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[IMAGE]`: `dlclambda:latest.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[COMMAND]` `[ARG…]`: `/opt/conda/bin/python -m awslambdaric app.handler.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This `docker run` command overrides the default `ENTRYPOINT` command and uses
    the `aws-lambda-rie`, instead of using the `--entrypoint` flag. This will then
    start a local endpoint at `http://localhost:9000/2015-03-31/functions/function/invocations`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the `docker run` command, feel free to check out [https://docs.docker.com/engine/reference/commandline/run/](https://docs.docker.com/engine/reference/commandline/run/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s invoke the `run.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new Terminal tab by clicking the plus (**+**) button, as shown in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Creating a new Terminal tab ](img/B18638_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Creating a new Terminal tab
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `run.sh` script should be kept running while we are opening a
    **New Terminal** tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `invoke.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should show us what is inside the `invoke.sh` script file. It should contain
    a one-liner script, similar to what we have in the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This script simply makes use of the `curl` command to send a sample `POST` request
    containing the `x` input value to the local endpoint that was started by the `run.sh`
    script earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run the `invoke.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield a value close to `"42.4586"`. Feel free to change the input
    `x` value in the `invoke.sh` script to see how the output value changes as well.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the first tab and press *Ctrl* + *C* to stop the running `run.sh`
    script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that we were able to successfully invoke the `app.py` Lambda function
    handler inside the custom container image using the **Lambda Runtime Interface
    Emulator**, we can now proceed with pushing our container image to Amazon ECR
    and using it to create an AWS Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the container image to Amazon ECR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Elastic Container Registry** (**ECR**) is a container registry service
    that allows us to store and manage Docker container images. In this section, we
    will create an ECR repository and then push our custom container image to this
    ECR repository.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating an ECR repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the top right-hand corner of the Cloud9 environment, locate and click the
    circle beside the **Share** button, as shown in the following screenshot. Select
    **Go To Dashboard** from the list of options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Navigating to the Cloud9 console ](img/B18638_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Navigating to the Cloud9 console
  prefs: []
  type: TYPE_NORMAL
- en: This should open the Cloud9 console, where we can find all the created Cloud9
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Type `registry` in the search bar. Select **Elastic Container Registry** from
    the list of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate and click the **Create repository** button in the top right-hand corner
    of the ECR console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the `dlclambda`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Creating an ECR repository ](img/B18638_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Creating an ECR repository
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can enable **Tag immutability**, similar to what is shown in
    the preceding screenshot. This will help ensure that we do not accidentally overwrite
    existing container image tags.
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page and then click **Create Repository**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should see a success notification, along with the **View push commands**
    button, similar to what we have in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.18 – View push commands ](img/B18638_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – View push commands
  prefs: []
  type: TYPE_NORMAL
- en: Click the **View push commands** button to open the **Push commands for <ECR
    repository name>** popup window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the `bash` command inside the gray box under *Step 1*. Copy the command
    to the clipboard by clicking the box button highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Push commands ](img/B18638_03_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Push commands
  prefs: []
  type: TYPE_NORMAL
- en: This command will be used to authenticate the Docker client in our Cloud9 environment
    to Amazon ECR. This will give us permission to push and pull container images
    to Amazon ECR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate back to the `bash` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Running the client authentication command ](img/B18638_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Running the client authentication command
  prefs: []
  type: TYPE_NORMAL
- en: We should get a **Login Succeeded** message. Without this step, we wouldn’t
    be able to push and pull container images from Amazon ECR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate back to the browser tab with the ECR push commands and copy the command
    under *Step 3*, as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Copying the docker tag command ](img/B18638_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Copying the docker tag command
  prefs: []
  type: TYPE_NORMAL
- en: This time, we will be copying the `docker tag` command from the `docker tag`
    command is used to create and map named references to Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `docker tag` command is used to specify and add metadata (such as the name
    and the version) to a container image. A container image repository stores different
    versions of a specific image, and the `docker tag` command helps the repository
    identify which version of the image will be updated (or uploaded) when the `docker
    push` command is used. For more information, feel free to check out https://docs.docker.com/engine/reference/commandline/tag/.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the browser tab that contains the Cloud9 environment, paste the copied
    `docker tag` command in the Terminal window. Locate the `latest` tag value at
    the end of the command and replace it with `1` instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The command should be similar to what we have in the following code block after
    the `latest` tag has been replaced with `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that the `<ACCOUNT ID>` value is correctly set to the account ID of
    the AWS account you are using. The `docker tag` command that you copied from the
    `<ACCOUNT ID>` value set correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker images` command to quickly check the container images in our
    Cloud9 environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should return all the container images, including the `dlclambda` container
    images, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Running the docker images command ](img/B18638_03_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Running the docker images command
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that both container image tags shown in the preceding
    screenshot have the same image ID. This means that they point to the same image,
    even if they have different names and tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Push the container image to the Amazon ECR repository using the `docker push`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure that you replace the value of `<ACCOUNT ID>` with the account ID of
    the AWS account you are using. You can get the value for `<ACCOUNT ID>` by checking
    the numerical value before `.dkr.ecr.us-west-2.amazonaws.com/dlclambda` after
    running the `docker images` command in the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the image tag value is a `1` (one) instead of the letter *l* after
    the container image name and the colon.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the browser tab that contains the ECR push commands and click
    the **Close** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate and click the name of the ECR repository we created (that is, `dlclambda`)
    under the list of **Private repositories**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Private repositories ](img/B18638_03_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Private repositories
  prefs: []
  type: TYPE_NORMAL
- en: 'This should redirect us to the details page, where we can see the different
    image tags, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Repository details page ](img/B18638_03_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Repository details page
  prefs: []
  type: TYPE_NORMAL
- en: Once our container image with the specified image tag has been reflected in
    the corresponding Amazon ECR repository details page, we can use it to create
    AWS Lambda functions using Lambda’s container image support.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our custom container image has been pushed to **Amazon ECR**, we can
    prepare and configure the serverless API setup!
  prefs: []
  type: TYPE_NORMAL
- en: Running ML predictions on AWS Lambda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**AWS Lambda** is a serverless compute service that allows developers and engineers
    to run event-driven code without having to provision or manage infrastructure.
    Lambda functions can be invoked by resources from other AWS services such as **API
    Gateway** (a fully managed service for configuring and managing APIs), **Amazon
    S3** (an object storage service where we can upload and download files), **Amazon
    SQS** (a fully managed message queuing service), and more. These functions are
    executed inside isolated runtime environments that have a defined max execution
    time and max memory limits, similar to what we have in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – AWS Lambda isolated runtime environment ](img/B18638_03_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – AWS Lambda isolated runtime environment
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to deploy Lambda function code and its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a container image as the deployment package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a `.zip` file as the deployment package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using a container image as the deployment package, the custom Lambda function
    code can use what is installed and configured inside the container image. That
    said, if we were to use the custom container image that was built from AWS DLC,
    we would be able to use the installed ML framework (that is, **PyTorch**) in our
    function code and run ML predictions inside an AWS Lambda execution environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding of how AWS Lambda’s container image
    support works, let’s proceed with creating our AWS Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: Type `lambda` in the search bar. Select **Lambda** from the list of results
    to navigate to the AWS Lambda console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate and click the **Create function** button found at the top-right of the
    page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the `dlclambda`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Using the container image support of AWS Lambda ](img/B18638_03_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Using the container image support of AWS Lambda
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the **Container image** option means that we will use a custom container
    image as the deployment package. This deployment package is expected to contain
    the Lambda code, along with its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Container image URI**, click the **Browse Images** button. This will
    open a popup window, similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Selecting the container image ](img/B18638_03_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – Selecting the container image
  prefs: []
  type: TYPE_NORMAL
- en: Under `dlclambda:1`).
  prefs: []
  type: TYPE_NORMAL
- en: Click the `dlclambda` container image will be used for the deployment package
    of our Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, click **Create function**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take 3 to 5 minutes to complete. Feel free to get a cup of coffee
    or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the **Configuration > General configuration** tab and click **Edit**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Editing the general configuration ](img/B18638_03_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – Editing the general configuration
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the AWS Lambda function is configured with a default max
    memory limit of 128 MB and a timeout of 3 seconds. An error is raised if the Lambda
    function exceeds one or more of the configured limits during execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, update the `10240` MB since we’re expecting our `1` min and `0` seconds
    as well since the inference step may take longer than the default value of 3 seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.29 – Modifying the memory and timeout settings ](img/B18638_03_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – Modifying the memory and timeout settings
  prefs: []
  type: TYPE_NORMAL
- en: Note that increasing the memory and timeout limits here will influence the compute
    power and total running time available for the Lambda function, as well as the
    overall cost of running predictions using the service. For now, let’s focus on
    getting the **AWS Lambda** function to work using these current configuration
    values for **Memory** and **Timeout**. Once we can get the initial setup running,
    we can play with different combinations of configuration values to manage the
    performance and cost of our setup.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can use the **AWS Compute Optimizer** to help us optimize the overall performance
    and cost of AWS Lambda functions. For more information on this topic, check out
    [https://aws.amazon.com/blogs/compute/optimizing-aws-lambda-cost-and-performance-using-aws-compute-optimizer/](https://aws.amazon.com/blogs/compute/optimizing-aws-lambda-cost-and-performance-using-aws-compute-optimizer/).
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Save** button afterward. We should see a notification similar to
    **Updating the function <function name>** while the changes are being applied.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the **Test** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under `test`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Configuring the test event ](img/B18638_03_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.30 – Configuring the test event
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you specify the following test event value inside the code editor,
    similar to what is shown in the preceding screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This test event value gets passed to the `event` (first) parameter of the AWS
    Lambda `handler()` function when a test execution is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s test our setup by clicking the **Test** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Successful execution result ](img/B18638_03_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.31 – Successful execution result
  prefs: []
  type: TYPE_NORMAL
- en: After a few seconds, we should see that the execution results succeeded, similar
    to what we have in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: In the `x` to `41` and then click the `41.481697`) almost right away.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: During an AWS Lambda function’s first invocation, it may take a few seconds
    for its function code to be downloaded and for its execution environment to be
    prepared. This phenomenon is commonly referred to as a *cold start*. When it is
    invoked a second time (within the same minute, for example), the Lambda function
    runs immediately without the delay associated with the cold start. For example,
    a Lambda function may take around 30 to 40 seconds for its first invocation to
    complete. After that, all succeeding requests would take a second or less. The
    Lambda function completes its execution significantly faster since the execution
    environment that was prepared during the first invocation is frozen and reused
    for succeeding invocations. If the AWS Lambda function is not invoked after some
    time (for example, around 10 to 30 minutes of inactivity), the execution environment
    is deleted and a new one needs to be prepared again the next time the function
    gets invoked. There are different ways to manage this and ensure that the AWS
    Lambda function performs consistently without experiencing the effects of a cold
    start. One of the strategies is to utilize **Provisioned Concurrency**, which
    helps ensure predictable function start times. Check out [https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/](https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/)
    for more information on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: With our AWS Lambda function ready to perform ML predictions, we can proceed
    with creating the serverless HTTP API that will trigger our Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Completing and testing the serverless API setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The AWS Lambda function we created needs to be triggered by an event source.
    One of the possible event sources is an API Gateway HTTP API configured to receive
    an HTTP request. After receiving the request, the HTTP API will pass the request
    data to the AWS Lambda function as an event. Once the Lambda function receives
    the event, it will use the deep learning model to perform inference, and then
    return the predicted output value to the HTTP API. After that, the HTTP API will
    return the HTTP response to the requesting resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to create an API Gateway HTTP API. In the next couple
    of steps, we will create this HTTP API directly from the AWS Lambda console:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the **Function overview** pane and click **Add trigger**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.32 – Add trigger ](img/B18638_03_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.32 – Add trigger
  prefs: []
  type: TYPE_NORMAL
- en: The **Add trigger** button should be on the left-hand side of the **Function
    overview** pane, as shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a new AWS Lambda trigger using the following trigger configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.33 – Trigger configuration ](img/B18638_03_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 – Trigger configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the trigger configuration that we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Select a trigger**: **API Gateway**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create a new API or use an existing one**: **Create an API**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API type**: **HTTP API**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: **Open**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will create and configure an HTTP API that accepts a request and sends
    the request data as an event to the AWS Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that this configuration needs to be secured once we have configured our
    setup for production use. For more information on this topic, check out [https://docs.aws.amazon.com/apigateway/latest/developerguide/security.xhtml](https://docs.aws.amazon.com/apigateway/latest/developerguide/security.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have finished configuring the new trigger, click the **Add** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate the API Gateway trigger we just created under the **Triggers** pane.
    Click the **API Gateway** link (for example, **dlclambda-API**) which should open
    a new tab. Under **Develop** (in the sidebar), click **Integrations**. Under Routes
    for **dlclambda-API**, click **ANY**. Click **Manage Integration** and then click
    **Edit** (located in the **Integration Details** pane). In the **Edit Integration**
    page, update the value of **Payload format version** (under **Advanced Settings**)
    to **2.0** similar to what we have in *Figure 3.34*. Click **Save** afterwards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.34 – Updating the Payload format version ](img/B18638_03_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 – Updating the Payload format version
  prefs: []
  type: TYPE_NORMAL
- en: After updating the `x` value in the URL, the Lambda function will use `0` as
    the default `x` value when performing a test inference.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You may want to trigger an exception instead if there is no `x` value specified
    when a request is sent to the API Gateway endpoint. Feel free to change this behavior
    by modifying *line 44* of `app.py`: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/app/app.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter03/app/app.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Append `?x=42` to the end of the browser URL, similar to what we have in the
    following URL string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that you press the `42` as the input `x` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.35 – Testing the API endpoint ](img/B18638_03_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.35 – Testing the API endpoint
  prefs: []
  type: TYPE_NORMAL
- en: This should return a value close to `42.4586`, as shown in the preceding screenshot.
    Feel free to test different values for `x` to see how the predicted *y* value
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you delete the AWS Lambda and API Gateway resources once you
    are done configuring and testing the API setup.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should be proud of ourselves as we were able to successfully
    deploy our deep learning model in a serverless API using **AWS Lambda** and **Amazon
    API Gateway**! Before the release of AWS Lambda’s container image support, it
    was tricky to set up and maintain serverless ML inference APIs using the same
    tech stack we used in this chapter. Now that we have this initial setup working,
    it should be easier to prepare and configure similar serverless ML-powered APIs.
    Note that we also have the option to create a Lambda function URL to generate
    a unique URL endpoint for the Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.36 – Cost of running the serverless API versus an API running inside
    an EC2 instance ](img/B18638_03_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.36 – Cost of running the serverless API versus an API running inside
    an EC2 instance
  prefs: []
  type: TYPE_NORMAL
- en: Before we end this chapter, let’s quickly check what the costs would look like
    if we were to use **AWS Lambda** and **API Gateway** for the ML inference endpoint.
    As shown in the preceding diagram, the expected cost of running this serverless
    API depends on the traffic passing through it. This means that the cost would
    be minimal if no traffic is passing through the API. Once more traffic passes
    through this HTTP API endpoint, the cost would gradually increase as well. Comparing
    this to the chart on the right, the expected cost will be the same, regardless
    of whether there’s traffic passing through the HTTP API that was deployed inside
    an EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the architecture and setup to use for your API depends on a variety
    of factors. We will not discuss this topic in detail, so feel free to check out
    the resources available here: [https://aws.amazon.com/lambda/resources/](https://aws.amazon.com/lambda/resources/).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were able to take a closer look at **AWS Deep Learning Containers**
    (**DLCs**). Similar to **AWS Deep Learning AMIs** (**DLAMIs**), AWS DLCs already
    have the relevant ML frameworks, libraries, and packages installed. This significantly
    speeds up the process of building and deploying deep learning models. At the same
    time, container environments are guaranteed to be consistent since these are run
    from pre-built container images.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key differences between DLAMIs and DLCs is that multiple AWS DLCs
    can run inside a single EC2 instance. These containers can also be used in other
    AWS services that support containers. These services include **AWS Lambda**, **Amazon
    ECS**, **Amazon EKS**, and **Amazon EC2**, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we were able to train a deep learning model using a DLC. We
    then deployed this model to an AWS Lambda function through Lambda’s container
    image support. After that, we tested the Lambda function to see whether it’s able
    to successfully load the deep learning model to perform predictions. To trigger
    this Lambda function from an HTTP endpoint, we created an API Gateway HTTP API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus on **serverless data management** and use
    a variety of services to set up and configure a data warehouse and a data lake.
    We will be working with the following AWS services, capabilities, and features:
    **Redshift Serverless**, **AWS Lake Formation**, **AWS Glue**, and **Amazon Athena**.'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics covered in this chapter, feel free to check
    out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What are Deep Learning Containers?* ([https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.xhtml](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security in Amazon API Gateway* (https://docs.aws.amazon.com/apigateway/latest/developerguide/security.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*New for AWS Lambda – Container Image Support* (https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Issues to Avoid When Implementing Serverless Architecture with AWS Lambda*
    ([https://aws.amazon.com/blogs/architecture/mistakes-to-avoid-when-implementing-serverless-architecture-with-lambda/)](https://aws.amazon.com/blogs/architecture/mistakes-to-avoid-when-implementing-serverless-architecture-with-lambda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2:Solving Data Engineering and Analysis Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, readers will learn how to perform data engineering using a
    variety of solutions and services on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management
    on AWS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18638_05.xhtml#_idTextAnchor105), *Pragmatic Data Processing
    and Analysis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
