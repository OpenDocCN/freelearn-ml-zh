<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Locating Objects in the World</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far, we have limited ourselves to recognizing the single most dominant object within an image using a <strong>convolutional neural network</strong> (<strong>CNN</strong>). We have seen how a model can be trained to take in a image and extract a series of feature maps that are then fed into a <strong>fully connected layer </strong>to output a probability distribution of a set of classes. This is then interpreted to classify the object within the image, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/046899f6-8490-4d77-b184-fa56fff667b3.png"/></div>
<p class="mce-root">In this chapter, we will build on this and explore how we can detect and locate multiple objects within a single image. We will start by building up our understanding of how this works and then walk through <span>implementing a image search for a photo gallery application. This application allows the user to filter and sort images not only based on what objects are present in the image, but also on their position relative to one another (object composition). Along the way, we will also get hands-on experience with</span> Core ML Tools, th<span>e tool set Apple released to support converting models from popular <strong>machine learning</strong> (<strong>ML</strong>) frameworks to Core ML. </span></p>
<p>Let's begin by first understanding what it takes to detect multiple objects in an image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object localization and object detection </h1>
                </header>
            
            <article>
                
<p>As mentioned in the introduction of this chapter, we have already been introduced to the concepts behind object recognition using CNNs. For this case, we used a trained model to perform classification; it achieved this by learning a set of feature maps using convolutional layers <span>that are</span> fed into fully connected (or dense) layers and, finally, their output, through an activation layer which gave us the probability for each of the classes. The class was inferred by selecting the one with the largest probability. </p>
<div class="packt_infobox">Let's differentiate between object recognition, object localization, and object detection. Object recognition is the task of classifying the most dominant object in a image while object localization performs classification and predicts an object's bounding box. Object detection further extends this and allows multiple classes to be detected and located, and that's the topic of this chapter.</div>
<p>This process is known as <strong>object recognition</strong> and is a classification problem, but here we don't get the full picture (pun intended). What about the location of the detected object? That would be useful for increasing the perception capabilities of robotic systems or increasing the scope for intelligent interfaces, such as intelligent cropping and image enhancements. What about detecting multiple objects and their locations? The former, detecting the location of a single object, is known as <strong>object localization,</strong> while the later is generally known as <strong>object detection</strong>, illustrated as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b2e938f9-2cd5-4c6f-8657-ee51140ad2e4.png" style="width:45.25em;height:17.17em;"/></div>
<p>We will start by introducing object localization and then work our way to object detection. The concepts are complementary and the former can be seen as an extension of object recognition, which you are already familiar with. </p>
<p>When training a model for classification, we tune the weights so that they achieve minimum loss for predicting a single class. For object localization, we essentially want to extend this to <span>predict</span> <span>not only the class, but also the location of the recognized object. Let's work through a concrete example to help illustrate the concept. Imagine we are training a model to recognize and locate a cat, dog, or person. For this, our model would need to output the probabilities for each class (cat, dog, or person) as we have already seen, and also their location. This can be described using the</span> center <em>x</em> and <em>y</em> position <span>and</span> the width and height of the ob<span>ject. To simplify the task of training, we also include a value indicating whether an object exists or not. The following figure illustrates two input images and their associated outputs. Let's assume here that our one-hot encoded classes are in the order of cat, dog, and person. That is, cat would be encoded as</span> <em>(1,0,0)</em><span>, dog as</span> <em>(0,1,0)</em><span>, and person as <em>(</em></span><em>0,0,1)</em><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f10dbb68-c666-46d6-9ed2-3756bc9b113f.png"/></div>
<p>The structure of the output, shown in the preceding image, consists of these elements:</p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><img src="assets/da6eafe8-786a-45b6-8466-bb1c4dc9ad83.png" style="width:16.25em;height:11.67em;"/></div>
<p>Here, we have three classes, but this can be generalized to include any arbitrary number of classes. What is of note is that if there is no object detected (the first element in our output), then we ignore the remaining elements. The other important point to highlight is that the bounding box is described in units rather than absolute values.</p>
<p>For example, a value of <em>0.5</em> for <em>b<sub>x</sub></em> would indicate half of the width of the image, where the top left is <strong>(0, 0)</strong> and bottom right is <strong>(1, 1)</strong>, as illustrated here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/50b813d6-eae3-463f-baa0-ce445442a090.png" style="width:33.50em;height:20.17em;"/></div>
<p>We can borrow a lot of the structure of a typical CNN used for classification. Therein, the image is fed through a series of convolutional layers and their output, a feature vector, through a series of fully connected layers, before being squeezed through a softmax activation for multi-class classification (giving us the probability distribution across all classes). Instead of just passing the feature vector from the convolutional layers to a single set of fully connected layers, we can also pass them to a layer (or layers) for binary classification (the fourth element: object present or not) and another layer (or series of layers) for predicting the bounding box using regression.</p>
<p>The structure of these modifications can be seen in the following figure, with the amendments in bold:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/45b9bfc5-e790-4474-aa98-6ae1f66c1a50.png" style="width:45.50em;height:32.83em;"/></div>
<p>This is a good start but, more often than not, our images consist of many objects. So, let's briefly describe how we can approach this problem.</p>
<p>We are now moving into object detection, where we are interested in detecting and locating multiple objects (of different classes) within a single image. So far, we have seen how we can detect a single object and its location from an image, so a logical progression from this is reshaping our problem around this architecture.</p>
<p>By this, I mean we can use this or a similar approach, but rather than passing the full image, we can pass in cropped regions of the image; the regions are selected by sliding a window across the image, as follows (and in keeping with our cat theme):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/cbbdab03-082b-405d-9e22-396a48f3cd4d.png" style="width:40.42em;height:24.83em;"/></div>
<p>This, for obvious reasons, is called the <strong>sliding window detection algorithm</strong> and should be familiar to those who have experience in computer vision (used in template matching, among many others). It's important to also emphasize the difference in training; in object localization, we trained the network by passing the full image along with the associated output vector (<em>b<sub>x</sub>, b<sub>y</sub>, b<sub>w</sub>, h<sub>y</sub>, <span>p<sub>c</sub>, </span>c<sub>1</sub>, c<sub>2</sub>, c<sub>3</sub>, ...</em>), while here the network is trained on <strong>tightly cropped images</strong> for each of the objects, which may occupy our window size.</p>
<div class="packt_infobox">For the inquisitive reader wondering how this algorithm detects objects that don't fit nicely into the window size, one approach could be to simply resize your image (both decreasing and increasing) or, similarly, to use different-sized windows, that is, a set of small, medium, and large windows.</div>
<p>This approach has two major drawbacks; the first is that it is computationally expensive, and the second is that it doesn't allow for very accurate bounding boxes due to the dependency on window sizes and stride size. The former can be resolved by rearchitecting the CNN so that it performs the sliding window algorithm in a single pass, but we are still left with inaccurate bounding boxes.</p>
<p>Fortunately, for us, in 2015, J. Redmon, S. Divvala, R. Girshick, and A. Farhadi released their paper <em>You Only Look Once (YOLO): Unified, Real-Time Object Detection</em>. It describes an approach that requires just a single network capable of predicting bounding boxes and probabilities from a full image in a single pass. And because it is a unified pipeline, the whole process is efficient enough to be performed in real time, hence the network we use in this chapter. </p>
<div class="packt_infobox">The paper <em>You Only Look Once: Unified, Real-Time Object Detection</em> is available here: <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a>.</div>
<p>Let's spend some time getting acquainted with the algorithm YOLO, where we will briefly look at the general concepts of the algorithm and interpreting the output to make use of it later in the example application for this chapter.</p>
<p>One of the major differences of YOLO compared to the previous approaches we have discussed in this chapter is how the model is trained. Similar to the first, when object localization was introduced, the model was trained on a image and label pair, and the elements of the label consisted of (<em>b<sub>x</sub></em>, <em>b<sub>y</sub></em>, <em>b<sub>w</sub></em>, <em>b<sub>h</sub></em>, <span><em>p<sub>c</sub></em>, </span><em>c<sub>1</sub></em>, <em>c<sub>2</sub></em>, ...), so too is the case with the YOLO network. But instead of training on the whole image, the image is broken down into a grid, with each cell having an associated label, as outlined before. In the next figure, you can see this. The grid illustrated here is a 3 x 3 for legibility; these are typically more dense, as you'll soon see:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1020 image-border" src="assets/80e6dea6-0a0c-4bf7-9caa-0a3c57addce3.png" style="width:27.92em;height:14.83em;"/></div>
<p>When training, we feed in the image and the network is structured so that it outputs a <strong>vector</strong> (as shown before) for <strong>each grid cell</strong>. To make this more concrete, the following figure illustrates how some of these outputs would look for the cells within this grid:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1021 image-border" src="assets/c0b85d59-a5c9-41fd-b971-dd44c3e5378a.png" style="width:165.00em;height:89.50em;"/></div>
<p>This should look familiar to you as it is, as previously mentioned, very similar to the approach we first introduced for object localization. The only major difference is that it is performed on each grid cell as opposed to the whole image.</p>
<p>It's worth noting that despite the object spanning across multiple cells, the training samples only label an object using a single cell (normally the cell in the center of the image), with the other encompassing cells having no object assigned to them.</p>
<p>Before we move on, let's unpack the bounding box variables. In the preceding figure, I have entered approximate values for each of the objects; like object localization, these values are normalized between <em>0.0</em> and <em>1.0</em> for values that fall within the cell. But, unlike object localization, these values are <strong>local</strong> to the cell itself rather than the image. Using the first cat as an example, we can see that the central position is <em>0.6</em> in the <em>x</em> axis and <em>0.35</em> in the <em>y</em> axis; this can be interpreted as being at a position 60% along the <em>x</em> axis of the cell and 35% along the <em>y</em> axis of the cell. Because our bounding box extends beyond the cell, there are assigned values greater than one, as we saw in the preceding figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2cd83249-1015-49b0-8408-ff2ad8748e31.png" style="width:17.83em;height:15.83em;"/></div>
<p>Previously, we highlighted that the training sample only assigns a single cell per object but, given that we are running object detection and localization on each cell, it is likely that we will end up with multiple predictions. To manage this, YOLO uses something called <strong>non-max <span>suppression</span></strong>, which we will introduce over the next few paragraphs, and you'll get a chance to implement it in the upcoming example.</p>
<p>As mentioned before, because we are performing object detection and localization on each grid cell, it is likely that we will end up with multiple bounding boxes. This is shown in the following figure. To simplify the illustration, we will just concentrate on a single object, but this of course applies to all detected objects:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/2c42812f-b4cd-4d60-8543-2461ec466366.png" style="width:18.08em;height:18.00em;"/></div>
<p>In the previous figure, we can see that the network has predicted and located the cat in three cells. For each cell, I have added a fictional confidence value, located at the top-right, to each of their associated bounding boxes.</p>
<div class="packt_infobox">The <strong>confidence value</strong> is calculated by multiplying the object present probability (<em>p<sub>c</sub></em>) with the most likely class, that is, the class with the highest probability.</div>
<p>The first step in n<span>on-max suppression is simply to filter out predictions that don't meet a certain threshold; for all intents and purposes, let's set our object threshold to <em>0.3</em>. With this value, we can see that we filter out one of the predictions (with a confidence value of <em>0.2</em>), leaving us with just two, as shown in the following figure:</span></p>
<div class="CDPAlignCenter CDPAlign"><span><img src="assets/fbcb36de-1806-49f1-a12b-9f95bdd3a72c.png" style="width:22.75em;height:22.58em;"/></span></div>
<p><span>In the next step, we iterate over all detected boxes, from the one with the largest confidence to the least, removing any other bounding boxes that occupy the same space. In the preceding figure, we can clearly see that both bounding boxes are essentially occupying the same area, but how do we determine this programmatically?</span></p>
<p>To determine this, we calculate what is called the <strong>intersection over union</strong> (<strong>IoU</strong>) and test the returned value against a threshold. We calculate this, as the name implies, <span>by</span> dividing the <strong>intersection area</strong> of the two bounding boxes by their <strong>union area</strong>. A value of <em>1.0</em> tells us that the two bounding boxes occupy exactly the same space (a value you would get if you performed this calculation using a single bounding box with itself). Anything below this gives us a ratio of overlapped occupancy; a typical threshold is <em>0.5</em>. T<span>he following figure</span> illustrates the intersection and union areas of our example:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/830b7548-9fcf-4473-a286-252b13806d68.png" style="width:34.00em;height:19.17em;"/></div>
<p>Because these two bounding boxes would return a relatively high IoU, we would end up pruning the least probable one (the one with the lower confidence score) and end up with a single bounding box, as shown in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/23a6b3ce-185a-4eee-9bdb-15093c7adb22.png" style="width:23.08em;height:23.00em;"/></div>
<p>We repeat this process until we have iterated over all of the model's predictions. There is just one more concept to introduce before moving on to the example project for this chapter and starting to write some code.</p>
<p>Up until this point, we have assumed <span>(or have been constrained by the idea) </span><span>that</span> each cell is associated with either no object or a single object. But what about the case of objects overlapping, where two objects' center positions occupy the same grid cell? To handle circumstances like this, the YOLO algorithm implements something called <strong>anchor boxes</strong>. Anchor boxes allow multiple objects to occupy a single grid cell given that their bounding shape differs. Let's make this more concrete by explaining it visually. In the next figure, we have a image where the centers of two objects occupy the same grid cell. With our current output vector, we would need to label the cell as either a person or a bike, as shown in the following figure:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1111 image-border" src="assets/46fb4511-7627-440b-a97e-394862e35787.png" style="width:37.58em;height:19.92em;"/></div>
<p>The idea with anchor boxes is that we extend our output vector to include different variations of anchor boxes (illustrated here as two, but these can be of any number). This allows each cell to encode multiple objects so long as they have different bounding shapes.</p>
<p>From the previous figure, we see that <span>we</span> can use two anchor boxes, one for the person and the other for the bike, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/2eb10ad6-92a7-4029-8aa6-d76c5f03b343.png" style="width:21.25em;height:11.00em;"/></div>
<p>With our anchor boxes now defined, we extend our vector output such that for each grid cell we can encode the output for both anchor boxes, as illustrated in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/803ec1ef-98a0-4285-847e-b5f0b5b1b1cd.png" style="width:26.25em;height:18.42em;"/></div>
<p><span>Each anchor box can be treated</span> independent<span>ly of each other output of the same cell and other cells; that is, we handle it exactly as we did previously, the only addition now being that we have more bounding boxes to process.</span></p>
<div class="packt_infobox"><span>Just to clarify, anchor</span> boxes are not constrained to a <span>specific class even though some shapes are more suitable than others. They are typically generalized shapes found using some type of unsupervised learning</span> algorithm, such as <strong>k-means</strong>, <span>within an existing dataset to find the dominant shapes.</span></div>
<p>This concludes all the concepts we need to understand for this chapter's example and what we will be implementing in the coming sections but, before we do, let's walk through converting a Keras model to Core ML using the Core ML Tools Python package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting Keras Tiny YOLO to Core ML</h1>
                </header>
            
            <article>
                
<p>In the previous section, we discussed the concepts of the model and algorithm we will be using in this chapter. In this section, we will be moving one step closer to realizing the example project for this chapter by converting a trained Keras model of Tiny YOLO to Core ML using Apple's Core ML Tools <span>Python</span> package; but, before doing so, we will quickly discuss the model and the data it was trained on.</p>
<p>YOLO was conceived on a neural network framework called <strong>darknet</strong>, which is currently not supported by the default Core ML Tools package; fortunately, the authors of YOLO and darknet have made the architecture and weights of the trained model publicly available on their website at <a href="https://pjreddie.com/darknet/yolov2/">https://pjreddie.com/darknet/yolov2/</a>. There are a few variations of YOLO that have been trained on either the dataset from <strong><span>Common Objects in Context </span></strong>(<strong>COCO</strong>), which consists of 80 classes, or The PASCAL <strong>Visual Object Classes</strong> (<strong>VOC</strong>) Challenge 2007, which consists of 20 classes.</p>
<div class="packt_infobox">The official website can be found at <a href="http://cocodataset.org">http://cocodataset.org</a>, and <em>The PASCAL VOC Challenge 2007</em> at <a href="http://host.robots.ox.ac.uk/pascal/VOC/index.html">http://host.robots.ox.ac.uk/pascal/VOC/index.html</a>.</div>
<p>In this chapter, we will be using the Tiny version of YOLOv2 and the weights from the model trained on the <em>The PASCAL VOC Challenge 2007</em> dataset. The Keras model we will be using was modeled on the configuration file and weights available on the official site (link presented previously).</p>
<p>As usual, we will be omitting a lot of details of the model and will instead provide the model in its diagrammatic form, shown next. We'll then discuss some of the relevant parts before moving on to convert it into a Core ML model:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/68b12837-9f34-460f-b84f-72051d5898e0.png"/></div>
<p>The first thing to notice is the shape of the input and output; this indicates what our model will be expecting to be fed and what it will be outputting for us to use. As shown before, the input size is 416 x 416 x 3, which, as you might suspect, is a 416 x 416 RGB image. The output shape needs a little more explanation and it will become more apparent when we arrive at coding the example for this chapter.</p>
<p>The output shape is 13 x 13 x 125. The 13 x 13 tells us the size of the grid being applied, that is, the 416 x 416 image is broken into a grid of 13 x 13 cells, as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/f7f5353d-1f9f-4c88-864f-09eaa71d0960.png" style="width:28.75em;height:28.75em;"/></div>
<p>As discussed previously, each cell has a 125<em>-</em>vector encoding of the probability of an object being present and, if so, the bounding box and probabilities across all 20 classes; visually, this is explained as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e83bddf8-2a30-4205-a45f-36d08e13191c.png" style="width:27.42em;height:7.58em;"/></div>
<p>The final point about the model that I want to highlight is its simplicity; the bulk of the network is made up of convolutional blocks consisting of a convolutional layer: <strong>Batch Normalization</strong>, <strong>LeakyReLU</strong> activation, and finally a <strong>MaxPooling</strong> layer. This progressively increases the filter size (depth) of the network until it has reached the desired grid size and then transforms the data using only the convolutional layer, <strong>Batch Normalization</strong>, and <strong>LeakyReLU</strong> activation, dropping the <strong>MaxPooling</strong>.</p>
<div class="packt_infobox">Now, we have introduced the terms batch normalization and leaky ReLU, which may be unfamiliar to some; here I will provide a brief description of each, starting with batch normalization. It's considered best practice to normalize the input layer before feeding it into a network. For example, we normally divide pixel values by 255 to force them into a range of 0 to 1. We do this to make it easier for the network to learn by removing any large values (or large variance in values), which may cause our network to oscillate when adjusting the weights during training. Batch normalization performs this same adjustment for the outputs of the hidden layers rather than the inputs.<br/>
ReLU is an activation function that sets anything below 0 to 0, that is, it doesn't allow non-positive values to propagate through the network. Leaky ReLU provides a less strict implementation of ReLU, allowing a small non-zero gradient to slip through when the neuron is not active.</div>
<p><span>This concludes our brief overview of the model. You can learn more about it from the official paper <em>YOLO9000: Better, Faster, Stronger</em> by J. Redmon and A. Farhadi, available at <a href="https://arxiv.org/abs/1612.08242">https://arxiv.org/abs/1612.08242</a>. Let's now turn our attention to converting the trained Keras model of the Tiny YOLO to Core ML.</span></p>
<p><span>As alluded to in <a href="7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Machine Learning</em>, Core ML is more of a suite of tools than a single framework. One part of this suite is the Core ML Tools Python package, which assists in converting trained models from other frameworks to Core ML for easy and rapid integration. Currently, official converters are available for Caffe, Keras, LibSVM, scikit-learn, and XGBoost, but the package is open source, with many other converters being made available for other popular ML frameworks, such as TensorFlow.</span></p>
<p><span>At its core, the conversion process generates a model specification that is a machine-interpretable representation of the learning models and is used by Xcode to generate the Core ML models, consisting of the following:</span></p>
<ul>
<li><strong>Model description</strong>: Encodes names and type information of the inputs and outputs of the model</li>
<li><strong>Model parameters</strong>: The set of parameters required to represent a specific instance of the model (model weights/coefficients)</li>
<li><strong>Additional metadata</strong>: Information about the origin, license, and author of the model</li>
</ul>
<p>In this chapter, we present the most simplistic of flows, but we will revisit the Core ML Tools package in <a href="40971e0d-b260-42e1-a9fb-5c4a56b0ebb2.xhtml">Chapter 6</a>, <em>Creating Art with Style Transfer</em>, to see how to deal with custom layers.</p>
<p>To avoid any complications when setting up the environment on your local or remote machine, we will leverage the free Jupyter cloud service provided by Microsoft. Head over to <a href="https://notebooks.azure.com">https://notebooks.azure.com</a> and log in, or register if you haven't already.</p>
<p class="mce-root">Once logged in, click on the <span class="packt_screen">Libraries</span> menu link from the navigation bar, which will take you to a page containing a list of all your libraries, similar to what is shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/9c8f12d6-2b5b-4c41-a724-6903324fa026.png" style="width:47.25em;height:31.25em;"/></div>
<p>Next, click on the <span class="packt_screen">+ New Library</span> link to bring up the <span class="packt_screen">Create New Library</span> dialog:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1025 image-border" src="assets/19bb5167-2674-4f8c-bc58-c8b64bab75b9.png" style="width:35.33em;height:21.83em;"/></div>
<p>Then click on the <span class="packt_screen">From GitHub</span> tab and enter <kbd>https://github.com/packtpublishing/machine-learning-with-core-ml</kbd> in the <span class="packt_screen">GitHub repository</span> field. After that, give your library a meaningful name and click on the <span class="packt_screen">Import</span> button to begin the process of cloning the repository and creating the library.</p>
<p class="mce-root">Once the library has been created, you will be redirected to the root; from here, click on the <kbd>Chapter5/Notebooks</kbd> folder to open up the relevant folder for this chapter. Finally, click on the Notebook <kbd>Tiny YOLO_Keras2CoreML.ipynb</kbd>. To help ensure that we are all on the same page (pun intended), here is a screenshot of what you should see after clicking on the <kbd>Chapter5/Notebooks</kbd> folder:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1026 image-border" src="assets/3d07e169-339e-41dd-904d-d5db7e83baff.png" style="width:37.92em;height:14.50em;"/><br/></div>
<p>With our Notebook now loaded, it's time to walk through each of the cells to create our Core ML model; all of the required code exists and all that remains is executing each of the cells sequentially. To execute a cell, you can either use the shortcut keys <em>Shift</em> + <em>Enter</em> or click on the <span class="packt_screen">Run</span> button in the toolbar (which will run the currently selected cell), as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c956b413-3944-4894-9ebb-915ccba65bfd.png" style="width:60.00em;height:45.50em;"/></div>
<p>I will provide a brief explanation of what each cell does; ensure that you execute each cell as we walk through them so that we all end up with the converted model, which we will then download and use in the next section for our iOS project.</p>
<p><span>We start by ensuring that the Core ML Tools Python package is available in the environment, by running the following cell:</span></p>
<pre><strong>!pip install coremltools</strong></pre>
<p>Once installed, we make the package available by importing it:</p>
<pre><strong>import coremltools</strong></pre>
<p>The model architecture and weights have been serialized and saved to the file <kbd>tinyyolo_voc2007_modelweights.h5</kbd>; in the following cell, we will pass this into the convert function of the Keras converter, which will return the converted Core ML model (if no errors occur). Along with the file, we also pass in values for the parameters <kbd>input_names</kbd>, <kbd>image_input_names</kbd>, <kbd>output_names</kbd>, and <kbd>image_scale</kbd>. The <kbd>input_names</kbd> <span>parameter</span> takes in a single string, or a list of strings for multiple inputs, and is used to explicitly set the names that will be used in the interface of the Core ML model to refer to the inputs of the Keras model.</p>
<p>We also pass this input name to the <kbd>image_input_names</kbd> parameter so that the converter treats the input as an image rather than an N-dimensional array. Similar to <kbd>input_names</kbd>, values passed to <kbd>output_names</kbd> will be used in the interface of the Core ML model to refer to the outputs of the Keras model. The last parameter, <kbd>image_scale</kbd>, allows us to add a scaling factor to our input before being passed to the model. Here, we are dividing each pixel by 255, which forces each pixel to be in a range of <em>0.0</em> to <em>1.0</em>, a typical preprocessing task when working with images. There are plenty more parameters available, allowing you to tune and tweak the inputs and outputs of your model. You can learn more about these at the official documentation site here at <a href="https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html">https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html</a>. In the next snippet, we perform the actual conversion using what we have just discussed:</p>
<pre>coreml_model = coremltools.converters.keras.convert(<br/>    'tinyyolo_voc2007_modelweights.h5',<br/>    input_names='image',<br/>    image_input_names='image',<br/>    output_names='output',<br/>    image_scale=1./255.)</pre>
<p>With reference to the converted model, <kbd>coreml_model</kbd>, we add metadata, which will be made available and displayed in Xcode's ML model views:</p>
<pre>coreml_model.author = 'Joshua Newnham'<br/>coreml_model.license = 'BSD'<br/>coreml_model.short_description = 'Keras port of YOLOTiny VOC2007 by Joseph Redmon and Ali Farhadi'<br/>coreml_model.input_description['image'] = '416x416 RGB Image'<br/>coreml_model.output_description['output'] = '13x13 Grid made up of: [cx, cy, w, h, confidence, 20 x classes] * 5 bounding boxes'</pre>
<p>We are now ready to save our model; run the final cell to save the converted model:</p>
<pre>coreml_model.save('tinyyolo_voc2007.mlmodel')</pre>
<p>With our model now saved, we return to the previous tab showing the contents of the <kbd>Chapter5</kbd> directory and download the <kbd>tinyyolo_voc2007.mlmodel</kbd> file. We do so by either right-clicking on it and selecting the <span class="packt_screen">Download</span> menu item, or by clicking on the <span class="packt_screen">Download</span> toolbar item, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1027 image-border" src="assets/4e089f81-195a-4c68-ac48-937d747bb77e.png" style="width:37.17em;height:17.75em;"/></div>
<p>With our converted model in hand, it's now time to jump into Xcode and work through the example project for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making it easier to find photos</h1>
                </header>
            
            <article>
                
<p>In this section, we will put our model to work in an intelligent search application; we'll start off by quickly introducing the application, giving us a clear vision of what we intend to build. Then, we'll work through implementing the functionality related to interpreting the model's output and search heuristic for the desired functionality. We will be omitting a lot of the usual iOS functionality so that we can stay focused on the intelligent aspect of the application.</p>
<p>Over the past few years, we have seen a surge of intelligence <span>being</span> embedded in photo gallery applications, providing us with efficient ways of surfacing those cat photos hidden deep in hundreds (if not thousands) of photos we have accumulated over the years. In this section, we want to continue with this theme but push that level of intelligence a little bit further by taking advantage of the semantic information gained through object detection. Our users <span>will</span> be able to search for n<span>ot only</span> specific objects within a image, but also photos based on the objects and their relative positioning. For example, they can search for an image, or images, with two people standing side by side and in front of a car.</p>
<p>The user interface allows the user to draw the objects as they would like them positioned and their relative sizes. It will be our job in this section to implement the intelligence that returns relevant images based on this search criteria.</p>
<p>The next figure shows the user interface; the first two screenshots show the search screen, where the user can visually articulate what they are looking for. By using labeled bounding boxes, the user is able to describe what they are looking for, how they would like these objects arranged, and relative object sizes. The last two screenshots show the result of a search, and when expanded (last screenshot), the image will be overlaid with the detected objects and associated bounding boxes:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c2d18585-97a8-4c18-980f-5106f4e79979.png" style="width:45.83em;height:22.17em;"/></div>
<p>Let's start by taking a tour of the existing project before importing the model we have just converted and downloaded.</p>
<p>If you haven't already, pull down the latest code from the accompanying repository:  <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>. Once downloaded, navigate to the directory <kbd>Chapter5/Start/</kbd> and open the project <kbd>ObjectDetection.xcodeproj</kbd>. Once loaded, you will see the project for this chapter, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e23df0d7-0d32-4113-a52d-9d5d45500745.png" style="width:45.92em;height:33.33em;"/></div>
<p>I will leave exploring the full project as an exercise for you, and I'll just concentrate on the files <kbd>PhotoSearcher.swift</kbd> and <kbd>YOLOFacade.swift</kbd> for this section. <kbd>PhotoSearcher.swift</kbd> is where we will implement the cost functions responsible for filtering and sorting the photos based on the search criteria and detected objects from <kbd>YOLOFacade.swift</kbd>, whose sole purpose is to wrap the Tiny YOLO model and implement the functionality to interpret its output. But before jumping into the code, let's quickly review the flow and data structures we will be working with.</p>
<p>The following diagram illustrates the general flow of the application; the user first defines the search criteria via <kbd>SearchViewController</kbd>, which is described as an array of normalized <kbd>ObjectBounds</kbd>. We'll cover more details on these later. When the user initiates the search (top-right search icon) these are passed to <kbd>SearchResultsViewController</kbd>, which delegates the task of finding suitable images to <kbd>PhotoSearcher</kbd>.</p>
<p><kbd>PhotoSearcher</kbd> proceeds to iterate through all of our photos, passing each of them through to <kbd>YOLOFacade</kbd> to perform object detection using the model we converted in the previous section. The results of these are passed back to <kbd>PhotoSearcher</kbd>, which evaluates the cost of each with respect to the search criteria and then filters and orders the results, before passing them back to <kbd>SearchResultsViewController</kbd> to be displayed:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d31059c4-70ff-44fe-b7bf-c737a34299ab.png"/></div>
<p>Each component communicates with the another using either the data object <kbd>ObjectBounds</kbd> or <kbd>SearchResult</kbd>. Because we will be working with them throughout the rest of this chapter, let's quickly introduce them here, all of which are defined in the <kbd>DataObjects.swift</kbd> file. Let's start with <kbd>ObjectBounds</kbd>, the structure shown in the following snippet:</p>
<pre>struct ObjectBounds {<br/>    public var object : DetectableObject<br/>    public var origin : CGPoint<br/>    public var size : CGSize<br/>    <br/>    var bounds : CGRect{<br/>        return CGRect(origin: self.origin, size: self.size)<br/>    }<br/>}    </pre>
<p>As the name suggests, <kbd>ObjectBounds</kbd> is just that—it encapsulates the boundary of an object using the variables <kbd>origin</kbd> and <kbd>size</kbd>. The <kbd>object</kbd> itself is of type <kbd>DetectableObject</kbd>, which provides a structure to store both the class index and its associated label. It also provides a static array of objects that are available in our search, as follows:</p>
<pre>struct DetectableObject{<br/>    public var classIndex : Int<br/>    public var label : String<br/>    <br/>    static let objects = [<br/>        DetectableObject(classIndex:19, label:"tvmonitor"),<br/>        DetectableObject(classIndex:18, label:"train"),<br/>        DetectableObject(classIndex:17, label:"sofa"),<br/>        DetectableObject(classIndex:14, label:"person"),<br/>        DetectableObject(classIndex:11, label:"dog"),<br/>        DetectableObject(classIndex:7, label:"cat"),<br/>        DetectableObject(classIndex:6, label:"car"),<br/>        DetectableObject(classIndex:5, label:"bus"),<br/>        DetectableObject(classIndex:4, label:"bottle"),<br/>        DetectableObject(classIndex:3, label:"boat"),<br/>        DetectableObject(classIndex:2, label:"bird"),<br/>        DetectableObject(classIndex:1, label:"bicycle")<br/>    ]<br/>}</pre>
<p><kbd>ObjectBounds</kbd> are used for both the search criteria defined by the user and search results returned by <kbd>YOLOFacade</kbd>; in the former, they describe where and which objects the user is interested in finding (search criteria), and the latter encapsulates the results from object detection.</p>
<p><kbd>SearchResult</kbd> doesn't get any more complex; it's intended to encapsulate the result of a search with the addition of the image and cost, which is set during the cost evaluation stage (<em>step 8</em>), as shown in the previous diagram. For the complete code, the structure is as follows:</p>
<pre>struct SearchResult{<br/>    public var image : UIImage  <br/>    public var detectedObjects : [ObjectBounds]<br/>    public var cost : Float<br/>}</pre>
<p>It's worth noting that the <kbd>ObjectBounds</kbd> messages, in the previous diagram, annotated with the word <strong>Normalized</strong>, refer to the values being in unit values based on the source or target size; that is, an origin of <em>x = 0.5</em> and <em>y = 0.5</em> defines the center of the source image it was defined on. The reason for this to ensure that the bounds are invariant to changes in the images they are operating on. Y<span>ou will soon see</span> that, before passing images to our model, we need to resize and crop to a size of 416 x 416 (the expected input to our model), but we need to transform them back to the original for rendering the results.</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Now, we have a better idea of what objects we will be consuming and generating; let's proceed with implementing the <kbd>YOLOFacade</kbd> and work our way up the stack.</p>
<p>Let's start by importing the model we have just converted in the previous section; locate the downloaded <kbd>.mlmodel</kbd> file and drag it onto Xcode. Once imported, select it from the left-hand panel to inspect the metadata to remind ourselves what we need to implement. It should resemble this screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c09a525c-cff6-494f-b272-cd1d0681949f.png" style="width:45.83em;height:28.83em;"/></div>
<p>With our model now imported, let's walk through implementing the functionality <kbd>YOLOFacade</kbd> is responsible for; this includes preprocessing the image, passing it to our model for inference, and then parsing the model's output, including performing <strong>non-max supression</strong>. Select <kbd>YOLOFacade.swift</kbd> from the left-hand panel to bring up the code in the main window.</p>
<p>The class is broken into three parts, via an extension, with the first including the variables and entry point; the second including the functionality for performing inference and parsing the models outputs; and the third part including the non-max supression algorithm we discussed at the start of this chapter. Let's start at the beginning which currently looks like this:</p>
<pre>class YOLOFacade{<br/>    <br/>    <strong>// TODO add input size (of image)</strong><br/>    <strong>// TODO add grid size</strong><br/>    <strong>// TODO add number of classes</strong><br/>    <strong>// TODO add number of anchor boxes</strong><br/>    <strong>// TODO add anchor shapes (describing aspect ratio)</strong><br/>    <br/>    lazy var model : VNCoreMLModel? = {<br/>        do{<br/>            <strong>// TODO add model</strong><br/>            return nil<br/>        } catch{<br/>            fatalError("Failed to obtain tinyyolo_voc2007")<br/>        }<br/>    }()<br/>    <br/>    func asyncDetectObjects(<br/>        photo:UIImage,<br/>        completionHandler:@escaping (_ result:[ObjectBounds]?) -&gt; Void){<br/>        <br/>        DispatchQueue.global(qos: .background).sync {<br/>            <br/>            self.detectObjects(photo: photo, completionHandler: { (result) -&gt; Void in<br/>                DispatchQueue.main.async {<br/>                    completionHandler(result)<br/>                }<br/>            })<br/>        }<br/>    }<br/>    <br/>}  </pre>
<p>The <kbd>asyncDetectObjects</kbd> method is the entry point of the class and is called by <kbd>PhotoSearcher</kbd> for each image it receives from the Photos framework; when called, this method simply delegates the task to the method <kbd>detectObject</kbd> in the background and waits for the results, before passing them back to the caller on the main thread. I have annotated the class with <kbd>TODO</kbd> to help you keep focused.</p>
<p>Let's start by declaring the target size required by our model; this will be used for preprocessing of the input of our model and transforming the normalized bounds to those of the source image. Add the following code:</p>
<pre>// TODO add input size (of image)<br/><strong>var targetSize = CGSize(width: 416, height: 416)</strong></pre>
<p>Next, we define properties of our model that are used during parsing of the output; these include grid size, number of classes, number of anchor boxes, and finally, the dimensions for each of the anchor boxes (each pair describes the width and height, respectively). Make the following amendments to your <kbd>YOLOFacade</kbd> class:</p>
<pre>// TODO add grid size<br/><strong>let gridSize = CGSize(width: 13, height: 13)</strong><br/>// TODO add number of classes<br/><strong>let numberOfClasses = 20</strong><br/>// TODO add number of anchor boxes<br/><strong>let numberOfAnchorBoxes = 5</strong><br/>// TODO add anchor shapes (describing aspect ratio)<br/><strong>let anchors : [Float] = [1.08, 1.19, 3.42, 4.41, 6.63, 11.38, 9.42, 5.11, 16.62, 10.52]</strong></pre>
<p>Let's now implement the model property; in this example, we will take advantage of the Vision framework for handling the preprocessing. For this, we will need to wrap our model in an instance of <kbd>VNCoreMLModel</kbd> so that we can pass it into a <kbd>VNCoreMLRequest</kbd>; make the following amendments, as shown in bold:</p>
<pre>lazy var model : VNCoreMLModel = {<br/>    do{<br/>        // TODO add model<br/>        <strong>let model = try VNCoreMLModel(</strong><br/><strong>            for: tinyyolo_voc2007().model)</strong><br/><strong>        return model</strong><br/>    } catch{<br/>        fatalError("Failed to obtain tinyyolo_voc2007")<br/>    }<br/>}()</pre>
<p>Let's now turn our attention to the <kbd>detectObjects</kbd> method. It will be responsible for performing inference via <kbd>VNCoreMLRequest</kbd> and <kbd>VNImageRequestHandler</kbd>, passing the model's output to the <kbd>detectObjectsBounds</kbd> method (which we will come to next), and finally transforming the normalized bounds to the dimensions of the original (source) image.</p>
<div class="packt_infobox">In this chapter, we will postpone the discussion around the Vision framework classes (<kbd>VNCoreMLModel</kbd>, <kbd>VNCoreMLRequest</kbd>, and <kbd>VNImageRequestHandler</kbd>) until the next chapter, where we will elaborate a little on what each does and how they work together.</div>
<p>Within the <kbd>detectObjects</kbd> method of <kbd>YOLOFacade</kbd>, replace the comment <kbd>// TODO preprocess image and pass to model</kbd> with the following code:</p>
<pre>let request = VNCoreMLRequest(model: self.model)<br/>request.imageCropAndScaleOption = .centerCrop<br/><br/>let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])<br/><br/>do {<br/>    try handler.perform([request])<br/>} catch {<br/>    print("Failed to perform classification.\n\(error.localizedDescription)")<br/>    completionHandler(nil)<br/>    return<br/>}</pre>
<p>In the preceding snippet, we start by creating an instance of <kbd>VNCoreMLRequest</kbd>, passing in our model, which itself has been wrapped with an instance of <kbd>VNCoreMLModel</kbd>. This request performs the heavy lifting, including preprocessing (inferred by the model's metadata) and performing inference. We set its <kbd>imageCropAndScaleOption</kbd> <span>property </span>to <kbd>centerCrop</kbd>, which determines, as you might expect, how the image is resized to fit into the model's input. The request itself doesn't actually execute the task; this is the responsibility of <kbd>VNImageRequestHandler</kbd>, which we declare next by passing in our source image and then executing the request via the handler's <kbd>perform</kbd> method.</p>
<p>If all goes to plan, we should expect to have the model's output available via the request's results property. Let's move on to the last snippet for this method; replace the comment <kbd>// TODO pass models results to detectObjectsBounds(::)</kbd> and the following statement, <kbd>completionHandler(nil)</kbd>, with this code:</p>
<pre>guard let observations = request.results as? [VNCoreMLFeatureValueObservation] else{<br/>    completionHandler(nil)<br/>    return<br/>}<br/><br/>var detectedObjects = [ObjectBounds]()<br/><br/>for observation in observations{<br/>    guard let multiArray = observation.featureValue.multiArrayValue else{<br/>        continue<br/>    }<br/>    <br/>    if let observationDetectedObjects = self.detectObjectsBounds(array: multiArray){<br/>        <br/>        for detectedObject in observationDetectedObjects.map(<br/>            {$0.transformFromCenteredCropping(from: photo.size, to: self.targetSize)}){<br/>                detectedObjects.append(detectedObject)<br/>        }<br/>    }<br/>}<br/><br/>completionHandler(detectedObjects) </pre>
<p>We begin by trying to cast the results to an array of <kbd>VNCoreMLFeatureValueObservation</kbd>, a type of image analysis observation that provides key-value pairs. One of them is <kbd>multiArrayValue</kbd>, which we then pass to the <span><kbd>detectObjectsBounds</kbd></span> method to parse the output and return the detected objects and their bounding boxes. Once <kbd>detectObjectsBounds</kbd> returns, we map each of the results with the <kbd>ObjectBounds</kbd> method <kbd>transformFromCenteredCropping</kbd>, which is responsible for transforming the normalized bounds into the space of the source image. Once each of the bounds has been transformed, we call the completion handler, passing in the detected bounds.</p>
<p>The next two methods encapsulate the bulk of the YOLO algorithm and the bulk of the code for this class. Let's start with the <kbd>detectObjectsBounds</kbd> method, making our way through it in small chunks.</p>
<p>This method will receive an <kbd>MLMultiArray</kbd> with the shape of <em>(125, 13, 13)</em>; this will hopefully look familiar to you (although reversed) where the <em>(13, 13)</em> is the size of our grid and the 125 encodes five blocks (coinciding with our five anchor boxes) each containing <span>the bounding box, the</span> probability of an object being present (or not), and the probability distribution across 20 classes. For your convenience, I have again added the diagram illustrating this structure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ca1f312e-358e-4a58-98f1-80da52f41fc8.png" style="width:20.50em;height:5.67em;"/><br/></div>
<p>To improve performance, we will access the <kbd>MLMultiArray</kbd>'s raw data directly, rather than through the <kbd>MLMultiArray</kbd> subscript. Although having direct access gives us a performance boost, it does have a trade-off of requiring us to correctly calculate the index for each value. Let's define the constants that we will use when calculating these indexes, as well as obtaining access to the raw data buffer and some arrays to store the intermediate results; add the following code within your <kbd>detectObjectsBounds</kbd> method:</p>
<pre>let gridStride = array.strides[0].intValue<br/>let rowStride = array.strides[1].intValue<br/>let colStride = array.strides[2].intValue<br/><br/>let arrayPointer = UnsafeMutablePointer&lt;Double&gt;(OpaquePointer(array.dataPointer))<br/><br/>var objectsBounds = [ObjectBounds]()<br/>var objectConfidences = [Float]() </pre>
<p>As mentioned before, we start by defining constants of stride values for the grid, row, and column—each used to calculate the current value. These values are obtained through the <kbd>strides</kbd> property of <kbd>MLMultiArray</kbd>, which gives us the number of data elements in each dimension. In this case, this would be 125, 13, and 13 respectively. Next, we get a reference to the underlying buffer of the <kbd>MLMultiArray</kbd> and, finally, we create two arrays to store the bounds and associated confidence value.</p>
<p>Next, we want to iterate through the model's output and process each of the grid cells and their subsequent anchor boxes independently; we do this by using three nested loops and then calculating the relevant index. Let's do that by adding the following snippet:</p>
<pre>for row in 0..&lt;Int(gridSize.height) {<br/>    for col in 0..&lt;Int(gridSize.width) {<br/>        for b in 0..&lt;numberOfAnchorBoxes {<br/>            <br/>            let gridOffset = row * rowStride + col * colStride<br/>            let anchorBoxOffset = b * (numberOfClasses + numberOfAnchorBoxes)<br/>            <strong>// TODO calculate the confidence of each class, ignoring if under threshold</strong> <br/>        }<br/>    }<br/>}</pre>
<p>The important values here are <kbd>gridOffset</kbd> and <kbd>anchorBoxOffset</kbd>; <kbd>gridOffset</kbd> gives us the relevant offset for the specific grid cell (as the name implies), while <kbd>anchorBoxOffset</kbd> gives us the index of the associated anchor box. Now that we have these values, we can access each of the elements using the <kbd>[(anchorBoxOffset + INDEX_TO_VALUE) * gridStride + gridOffset</kbd> index, where <kbd>INDEX_TO_VALUE</kbd> is the relevant value within the anchor box vector we want to access, as illustrated in this diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7f430c34-c32a-4b53-bfad-50114ad51c93.png" style="width:29.42em;height:20.00em;"/></div>
<p>Now we know how to access each bounding box for each grid cell in our buffer, let's use it to find the most probable class and put in our first test of ignoring any prediction if it doesn't meet our threshold (defined as a method parameter with the default value of 0.3<em>:</em> <kbd>objectThreshold:Float = 0.3</kbd>). Add the following code, replacing the comment <kbd>// TODO calculate the confidence of each class, ignoring if under threshold</kbd>, as seen previously:</p>
<pre>let confidence = sigmoid(x: Float(arrayPointer[(anchorBoxOffset + 4) * gridStride + gridOffset]))<br/><br/>var classes = Array&lt;Float&gt;(repeating: 0.0, count: numberOfClasses)<br/>for c in 0..&lt;numberOfClasses{<br/>    classes[c] = Float(arrayPointer[(anchorBoxOffset + 5 + c) * gridStride + gridOffset])<br/>}<br/>classes = softmax(z: classes)<br/><br/>let classIdx = classes.argmax<br/>let classScore = classes[classIdx]<br/>let classConfidence = classScore * confidence<br/><br/>if classConfidence &lt; objectThreshold{<br/>    continue<br/>}<br/><br/><strong>// TODO obtain bounding box and transform to image dimensions  </strong></pre>
<p>In the preceding code snippet, we first obtain the probability of an object being present and store it in the constant <kbd>confidence</kbd>. Then, we populate an array with the probabilities of all the classes, before applying a softmax across them all. This will squash the values so that the accumulated value of them equals <em>1.0,</em> essentially providing us with our probability distribution across all classes.</p>
<p>We then find the class index with the largest probability and multiply it with our <kbd>confidence</kbd> constant, which gives us the class confidence we will threshold against and use during non-max suppression, ignoring the prediction if it doesn't meet our threshold.</p>
<p>Before continuing with the procedure, I want to take a quick detour to highlight and explain a couple of the methods used in the preceding snippet, namely the <kbd>softmax</kbd> method and <kbd>argmax</kbd> property of the classes array. Softmax is a logistic function that essentially squashes a vector of numbers so that all values in the vector add up to 1; it's an activation function commonly used when dealing with multi-class classification problems where the result is interpreted as the likelihood of each class, typically taking the class with the largest value as the predicted class (within a threshold). The implementation can be found in the <kbd>Math.swift</kbd> file, which makes use of the Accelerate framework to improve performance. The equation and implementation are shown here for completeness, but the details are omitted and left for you to explore:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/afcd28b0-c339-49fc-a38a-f254a0f0c909.png" style="width:13.00em;height:3.00em;"/></div>
<p>Here, we use a slightly modified version of the equation shown previously; in practice, calculating the softmax values can be problematic if any of the values are very large. Applying an exponential operation on it will make it explode, and dividing any value by a huge value can cause arithmetic computation problems. To avoid this, it is often best practice to subtract the maximum value from all elements.<br/></p>
<p>Because there are quite a few functions for this operation, let's build it up piece by <span>piece, from the inside out. The following is the function that performs element-wise subtraction:</span></p>
<pre>/**<br/> Subtract a scalar c from a vector x<br/> @param x Vector x.<br/> @param c Scalar c.<br/> @return A vector containing the difference of the scalar and the vector<br/> */<br/>public func sub(x: [Float], c: Float) -&gt; [Float] {<br/>    var result = (1...x.count).map{_ in c} <br/>    catlas_saxpby(Int32(x.count), 1.0, x, 1, -1.0, &amp;amp;result, 1) <br/>    return result<br/>}</pre>
<p>Next, the function that computes the element-wise exponential for an array:</p>
<pre>/**<br/> Perform an elementwise exponentiation on a vector <br/> @param x Vector x.<br/> @returns A vector containing x exponentiated elementwise.<br/> */<br/>func exp(x: [Float]) -&gt; [Float] {<br/>    var results = [Float](repeating: 0.0, count: x.count) <br/>    vvexpf(&amp;amp;results, x, [Int32(x.count)]) <br/>    return results<br/>}</pre>
<p>Now, the function to perform summation on an array, as follows:</p>
<pre>/**<br/> Compute the vector sum of a vector<br/> @param x Vector.<br/> @returns A single precision vector sum.<br/> */<br/>public func sum(x: [Float]) -&gt; Float {<br/>    return cblas_sasum(Int32(x.count), x, 1)<br/>}</pre>
<p>This is the last function used by the softmax function! This will be responsible for performing element-wise division for a given scalar, as follows:</p>
<pre> /**<br/> Divide a vector x by a scalar y<br/> @param x Vector x.<br/> @parame c Scalar c.<br/> @return A vector containing x dvidided elementwise by vector c.<br/> */<br/>public func div(x: [Float], c: Float) -&gt; [Float] {<br/>    let divisor = [Float](repeating: c, count: x.count)<br/>    var result = [Float](repeating: 0.0, count: x.count) <br/>    vvdivf(&amp;amp;result, x, divisor, [Int32(x.count)]) <br/>    return result<br/>}</pre>
<p>Finally, we the softmax function (using the max trick as described previously):</p>
<pre>/**<br/> Softmax function<br/> @param z A vector z.<br/> @return A vector y = (e^z / sum(e^z))<br/> */<br/>func softmax(z: [Float]) -&gt; [Float] {<br/>    let x = exp(x:sub(x:z, c: z.maxValue))    <br/>    return div(x:x, c: sum(x:x))<br/>}</pre>
<p>In addition to the <span>preceding</span> functions, it uses an extension property, <kbd>maxValue</kbd>, of Swift's array class; this extension also includes the <kbd>argmax</kbd> property alluded to previously. So, we will present both together in the following snippet, found in the <kbd>Array+Extension.swift</kbd> file. Before presenting the code, just a reminder about the function of the <kbd>argmax</kbd> property—its purpose is to return the index of the largest value within the array, a common method available in the Python package NumPy:</p>
<pre>extension Array where Element == Float{<br/>    <br/>    /**<br/>     @return index of the largest element in the array<br/>     **/<br/>    var argmax : Int {<br/>        get{<br/>            precondition(self.count &gt; 0)<br/>            <br/>            let maxValue = self.maxValue<br/>            for i in 0..&lt;self.count{<br/>                if self[i] == maxValue{<br/>                    return i<br/>                }<br/>            }<br/>            return -1<br/>        }<br/>    }<br/>    <br/>    /**<br/>     Find the maximum value in array<br/>     */<br/>    var maxValue : Float{<br/>        get{<br/>            let len = vDSP_Length(self.count)<br/>            <br/>            var max: Float = 0<br/>            vDSP_maxv(self, 1, &amp;amp;max, len)<br/>            <br/>            return max<br/>        }<br/>    }<br/>}</pre>
<p>Let's now turn our attention back to the parsing of the model's output and extracting the detected objects and associated bounding boxes. Within the loop, we now have a prediction we are somewhat confident with, having passed our threshold filter. The next task is to extract and transform the bounding box of the predicted object. Add the following code, replacing the line <kbd>// TODO obtain bounding box and transform to image dimensions</kbd>:</p>
<pre>let tx = CGFloat(arrayPointer[anchorBoxOffset * gridStride + gridOffset])<br/>let ty = CGFloat(arrayPointer[(anchorBoxOffset + 1) * gridStride + gridOffset])<br/>let tw = CGFloat(arrayPointer[(anchorBoxOffset + 2) * gridStride + gridOffset])<br/>let th = CGFloat(arrayPointer[(anchorBoxOffset + 3) * gridStride + gridOffset])<br/><br/>let cx = (sigmoid(x: tx) + CGFloat(col)) / gridSize.width <br/>let cy = (sigmoid(x: ty) + CGFloat(row)) / gridSize.height<br/>let w = CGFloat(anchors[2 * b + 0]) * exp(tw) / gridSize.width <br/>let h = CGFloat(anchors[2 * b + 1]) * exp(th) / gridSize.height<br/><br/>// TODO create a ObjectBounds instance and store it in our array of candidates  </pre>
<p>We start by getting the first four values of from the grid cell's anchor box segment; this returns the center position and size relative to the grid. The next block is responsible for transforming these values from the grid coordinate system to the image coordinate system. For the center position, we pass the returned value through a <kbd>sigmoid</kbd> function, keeping it between <em>0.0 - 1.0</em>, and offset based on the relevant column (or row). Finally we divide it by the grid size (13). Similarly with the dimensions, we first get the associated anchor box, multiplying it by the exponential of the predicted dimension and then dividing it by the grid size.</p>
<p>As we have done previously, I now present the implementations for the function <kbd>sigmoid</kbd> for reference, which can found in the <kbd>Math.swift</kbd> file. The equation is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1fb2ae26-e3be-4ffa-9f64-beffc326387d.png" style="width:12.92em;height:3.17em;"/></div>
<pre>/**<br/> A sigmoid function <br/> @param x Scalar<br/> @return 1 / (1 + exp(-x))<br/> */<br/>public func sigmoid(x: CGFloat) -&gt; CGFloat {<br/>    return 1 / (1 + exp(-x))<br/>}</pre>
<p>The final chunk of code simply creates an instance <kbd>ObjectBounds</kbd>, passing in the transformed bounding box and the associated <kbd>DetectableObject</kbd> class (filtering on the class index). Add the following code, replacing the comment <kbd>// TODO create a ObjectBounds instance and store it in our array of candidates</kbd>:</p>
<pre>guard let detectableObject = DetectableObject.objects.filter(<br/>    {$0.classIndex == classIdx}).first else{<br/>    continue<br/>}<br/><br/>let objectBounds = ObjectBounds(<br/>    object: detectableObject,<br/>    origin: CGPoint(x: cx - w/2, y: cy - h/2),<br/>    size: CGSize(width: w, height: h))<br/><br/>objectsBounds.append(objectBounds)<br/>objectConfidences.append(classConfidence)</pre>
<p>In addition to storing the <kbd>ObjectBounds</kbd>, we also store <kbd>confidence</kbd>, which will be used when we get to implementing non-max suppression.</p>
<p>This completes the functionality required within the nested loops; by the end of this process, we have an array populated with our candidate detected objects. Our next task will be to filter them. Near the end of the <kbd>detectObjectsBounds</kbd> method, add the following statement (outside any loops):</p>
<pre>return self.filterDetectedObjects(<br/>    objectsBounds: objectsBounds,<br/>    objectsConfidence: objectConfidences)</pre>
<p>Here, we are simply returning the results from the <kbd>filterDetectedObjects</kbd> method, which we will now turn our attention to. The method has been blocked out but is vacant of functionality, as follows:</p>
<pre>func filterDetectedObjects(<br/>    objectsBounds:[ObjectBounds],<br/>    objectsConfidence:[Float],<br/>    nmsThreshold : Float = 0.3) -&gt; [ObjectBounds]?{<br/>    <br/>    // If there are no bounding boxes do nothing<br/>    guard objectsBounds.count &gt; 0 else{<br/>        return []<br/>    }<br/>        <strong>// TODO implement Non-Max Supression</strong><br/>    <br/>    return nil<br/>}</pre>
<p>Our job will be to implement the non-max suppression algorithm; just to recap, the algorithm can be described as follows:</p>
<ol>
<li>Order the detected boxes from most confident to least</li>
<li>While valid boxes remain, do the following:
<ol>
<li>Pick the box with the highest confidence value (the top of our ordered array)</li>
<li>Iterate through all the remaining boxes, discarding any with an IoU value greater than a predefined threshold</li>
</ol>
</li>
</ol>
<p>Let's start by creating a clone of the confidence array passed into the method; we will use this to obtain an array of sorted indices, as well as to flag any boxes that are sufficiently overlapped by the preceding box. This is done by simply setting its confidence value to 0. Add the following statement to do just this, along with creating the sorted array of <span>indices, replacing the comment <kbd>// TODO implement Non-Max Suppression</kbd></span>:</p>
<pre>var detectionConfidence = objectsConfidence.map{<br/>    (confidence) -&gt; Float in<br/>    return confidence<br/>}<br/><br/>let sortedIndices = detectionConfidence.indices.sorted {<br/>    detectionConfidence[$0] &gt; detectionConfidence[$1]<br/>}<br/><br/>var bestObjectsBounds = [ObjectBounds]()<br/><br/><strong>// TODO iterate through each box</strong> </pre>
<p>As mentioned previously, we start by cloning the confidence array, assigning it to the variable <kbd>detectionConfidence</kbd>. Then, we sort the indices in descending order and, finally, create an array to store the boxes we want to keep and return.</p>
<p>Next, we will create the loops that embody the bulk of the algorithm, including picking the next box with the highest confidence and storing it in our <kbd>bestObjectsBounds</kbd> array. Add the following code, replacing the comment <kbd>// TODO iterate through each box</kbd>:</p>
<pre>for i in 0..&lt;sortedIndices.count{<br/>    let objectBounds = objectsBounds[sortedIndices[i]]<br/>    <br/>    guard detectionConfidence[sortedIndices[i]] &gt; 0 else{<br/>        continue<br/>    }<br/><br/>    bestObjectsBounds.append(objectBounds)<br/>    <br/>    for j in (i+1)..&lt;sortedIndices.count{<br/>        guard detectionConfidence[sortedIndices[j]] &gt; 0 else {<br/>            continue<br/>        }<br/>        let otherObjectBounds = objectsBounds[sortedIndices[j]]<br/>        <br/>        <strong>// TODO calculate IoU and compare against our threshold</strong>        <br/>    }<br/>}</pre>
<p>Most of the code should be self-explanatory; what's worth noting is that within each loop, we test that the associated boxes confidence is greater than 0. As mentioned before, we use this to indicate that an object has been discarded due to being sufficiently overlapped by a box with higher confidence.</p>
<p>What remains is calculating the IoU between <kbd>objectBounds</kbd> and <kbd>otherObjectBounds</kbd>, and invaliding <kbd>otherObjectBounds</kbd> if it doesn't meet our IoU threshold, <kbd>nmsThreshold</kbd>. Replace the comment <kbd>// TODO calculate IoU and compare against our threshold</kbd>, with this:</p>
<pre>if Float(objectBounds.bounds.computeIOU(<br/>    other: otherObjectBounds.bounds)) &gt; nmsThreshold{<br/>    detectionConfidence[sortedIndices[j]] = 0.0<br/>}</pre>
<p>Here, we are using a <kbd>CGRect</kbd> extension method, <kbd>computeIOU</kbd>, to handle the calculation. Let's have a peek at this, implemented in the file <kbd>CGRect+Extension.swift</kbd>:</p>
<pre>extension CGRect{<br/>    <br/>    ...<br/>    <br/>    var area : CGFloat{<br/>        get{<br/>            return self.size.width * self.size.height<br/>        }<br/>    }<br/>    <br/>    func computeIOU(other:CGRect) -&gt; CGFloat{<br/>        return self.intersection(other).area / self.union(other).area<br/>    }    <br/>}</pre>
<p>Thanks to the existing <kbd>intersection</kbd> and <kbd>union</kbd> of the <kbd>CGRect</kbd> structure, this method is nice and concise.</p>
<p>One final thing to do before we finish with the <kbd>YOLOFacade</kbd> class as well as the YOLO algorithm is to return the results. At the bottom of the <kbd>filterDetectedObjects</kbd> method, return the array <kbd>bestObjectsBounds</kbd>; with that done, we can now turn our attention to the last piece of functionality before implementing our intelligent search photo application.</p>
<div class="packt_infobox">This chapter does a good job highlighting that most of the effort integrating ML into your applications surrounds the <strong>preprocessing</strong> of the data before feeding it into the model and <strong>interpreting</strong> the output of the model. The <strong>Vision framework</strong> does a good job alleviating the preprocessing tasks, but there is still significant effort handling the output. Fortunately, no doubt because object detection is compelling for many applications, Apple has added a new observation type explicitly for object detection called <strong>VNRecognizedObjectObservation</strong>. Although we don't cover it here; I encourage you to review the official documentation <a href="https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation">https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation</a>.</div>
<p>The next piece of functionality is concerned with evaluating a cost on each of the returned detected objects with respect to the user's search criteria; by this, I mean filtering and sorting the photos so that the results are relevant to what the user sought. As a reminder, the search criteria is defined by an array of <kbd>ObjectBounds</kbd>, collectively describing the objects the user wants within a image, their relative positions, as well as the sizes <span>relative</span> <span>to each other and to the image itself. The following figure shows how the user defines their search within our application:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/334ca2eb-c83f-4a10-8aa7-a224e2a3e50e.png" style="width:22.00em;height:22.42em;"/></div>
<p>Here, we will implement <span>only</span> <span>two of the four evaluations, but it should provide a sufficient base for you to implement the remaining two yourself.</span></p>
<p>The cost evaluation is performed within the <kbd>PhotoSearcher</kbd> class once the <kbd>YOLOFacade</kbd> has returned the detected objects for all of the images. This code resides in the <kbd>asyncSearch</kbd> method (within the <kbd>PhotoSearcher.swift</kbd> file), highlighted in the following code snippet:</p>
<pre>public func asyncSearch(<br/>    searchCriteria : [ObjectBounds]?,<br/>    costThreshold : Float = 5){<br/>    DispatchQueue.global(qos: .background).async {<br/>        let photos = self.getPhotosFromPhotosLibrary()<br/>        <br/>        let unscoredSearchResults = self.detectObjects(photos: photos)<br/>        <br/>        var sortedSearchResults : [SearchResult]?<br/>        <br/>        if let unscoredSearchResults = unscoredSearchResults{<br/>            <strong>sortedSearchResults = self.calculateCostForObjects(</strong><br/><strong>                detectedObjects:unscoredSearchResults,</strong><br/><strong>                searchCriteria: searchCriteria).filter({</strong><br/><strong>                    (searchResult) -&gt; Bool in</strong><br/><strong>                    return searchResult.cost &lt; costThreshold</strong><br/><strong>                }).sorted(by: { (a, b) -&gt; Bool in</strong><br/><strong>                    return a.cost &lt; b.cost</strong><br/><strong>                })</strong><br/>        }<br/>        <br/>        DispatchQueue.main.sync {<br/>            self.delegate?.onPhotoSearcherCompleted(<br/>                status: 1,<br/>                result: sortedSearchResults)<br/>        }<br/>    }<br/>} </pre>
<p><kbd>calculateCostForObjects</kbd> takes in the search criteria and results from the <kbd>YOLOFacade</kbd> and returns an array of <kbd>SearchResults</kbd> from the <kbd>detectObjects</kbd> with their cost properties set, after which they are filtered and sorted before being returned to the delegate.</p>
<p>Let's jump into the <kbd>calculateCostForObjects</kbd> <span>method</span> <span>and discuss what we mean by cost; the code of the method</span> <kbd>calculateCostForObjects</kbd> <span>is as follows:</span></p>
<pre> private func calculateCostForObjects(<br/>    detectedObjects:[SearchResult],<br/>    searchCriteria:[ObjectBounds]?) -&gt; [SearchResult]{<br/>    <br/>    guard let searchCriteria = searchCriteria else{<br/>        return detectedObjects<br/>    }<br/>    <br/>    var result = [SearchResult]()<br/>    <br/>    for searchResult in detectedObjects{<br/>        let cost = self.<strong>costForObjectPresences</strong>(<br/>            detectedObject: searchResult,<br/>            searchCriteria: searchCriteria) +<br/>            self.<strong>costForObjectRelativePositioning</strong>(<br/>                detectedObject: searchResult,<br/>                searchCriteria: searchCriteria) +<br/>            self.<strong>costForObjectSizeRelativeToImageSize</strong>(<br/>                detectedObject: searchResult,<br/>                searchCriteria: searchCriteria) +<br/>            self.<strong>costForObjectSizeRelativeToOtherObjects</strong>(<br/>                detectedObject: searchResult,<br/>                searchCriteria: searchCriteria)<br/>        <br/>        let searchResult = SearchResult(<br/>            image: searchResult.image,<br/>            detectedObjects:searchResult.detectedObjects,<br/>            cost: cost)<br/>        <br/>        result.append(searchResult)<br/>    }<br/>    <br/>    return result<br/>}</pre>
<p>A <kbd>SearchResult</kbd> incurs a cost each time it differs from the user's search criteria, meaning that the results with the least cost are those that better match the search criteria. We perform cost evaluation on four different heuristics; each method will be responsible for adding the calculated cost to each result. Here we will only implement <kbd>costForObjectPresences</kbd> and <kbd>costForObjectRelativePositioning</kbd>, leaving the remaining two as an exercise for your.</p>
<p>Let's jump straight in and start implementing the <kbd>costForObjectPresences</kbd> method; at the moment, it's nothing more than a stub, as follows:</p>
<pre>private func costForObjectPresences(<br/>    detectedObject:SearchResult,<br/>    searchCriteria:[ObjectBounds],<br/>    weight:Float=2.0) -&gt; Float{<br/>    <br/>    var cost : Float = 0.0<br/>    <br/>    <strong>// TODO implement cost function for object presence</strong><br/>    <br/>    return cost * weight<br/>}</pre>
<p>Before writing the code, let's quickly discuss what we are evaluating for. Maybe, a better name for this function would have been <kbd>costForDifference</kbd> as we <span>not only</span> <span>want to assess that the image has objects declared in the search criteria, but also we <span>equally</span> want to increase the cost for additional objects. That is, if the user searches for just two dogs but a photo has three dogs or two dogs and a cat, we want to increase the cost for these additional objects such that we are favoring the one that is most similar to the search criteria (just two dogs).</span></p>
<p>To calculate this, we simply need to find the absolute difference between the two arrays; to do this, we first create a dictionary of counts for all classes in both <kbd>detectedObject</kbd> and <kbd>searchCriteria</kbd>. The directory's key will be the object's label and the corresponding value will be the count of objects within the array. The following figure illustrates these arrays and formula used to calculate:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a4ce80a3-a157-4be3-b033-2c3e76108094.png" style="width:35.25em;height:13.17em;"/></div>
<p>Let's now implement it; add the following code to do this, replacing the comment <kbd>// TODO implement cost function for object presence</kbd>:</p>
<pre>var searchObjectCounts = searchCriteria.map {<br/>    (detectedObject) -&gt; String in<br/>    return detectedObject.object.label<br/>    }.reduce([:]) {<br/>        (counter:[String:Float], label) -&gt; [String:Float] in<br/>        var counter = counter<br/>        counter[label] = counter[label]?.advanced(by: 1) ?? 1<br/>        return counter<br/>}<br/><br/>var detectedObjectCounts = detectedObject.detectedObjects.map {<br/>    (detectedObject) -&gt; String in<br/>    return detectedObject.object.label<br/>    }.reduce([:]) {<br/>        (counter:[String:Float], label) -&gt; [String:Float] in<br/>        var counter = counter<br/>        counter[label] = counter[label]?.advanced(by: 1) ?? 1<br/>        return counter<br/>}<br/><br/><strong>// TODO accumulate cost based on the difference</strong></pre>
<p>Now, with our count dictionaries created and populated, it's simply a matter of iterating over all available classes (using the items in <kbd>DetectableObject.objects</kbd>) and calculating the cost based on the absolute difference between the two. Add the following code, which does this, by replacing the comment <kbd>// TODO accumulate cost based on the difference</kbd>:</p>
<pre>for detectableObject in DetectableObject.objects{<br/>    let label = detectableObject.label<br/>    <br/>    let searchCount = searchObjectCounts[label] ?? 0<br/>    let detectedCount = detectedObjectCounts[label] ?? 0<br/>    <br/>    cost += abs(searchCount - detectedCount)<br/>}</pre>
<p>The result of this is a cost that is larger for images that differ the most from the search criteria; the last thing worth noting is that the cost is multiplied by a weight before being returned (function parameter). Each evaluation method has a weight parameter which allows for easy tuning (<span>during</span> either design time or runtime) of the search, giving preference to one evaluation over another.</p>
<p>The next, and last, cost evaluation function we are going to implement is the method <kbd>costForObjectRelativePositioning</kbd>; the stub of this method is as follows:</p>
<pre> private func costForObjectRelativePositioning(<br/>    detectedObject:SearchResult,<br/>    searchCriteria:[ObjectBounds],<br/>    weight:Float=1.5) -&gt; Float{<br/>    <br/>    var cost : Float = 0.0<br/>    <br/>    <strong>// TODO implement cost function for relative positioning</strong><br/>    <br/>    return cost * weight<br/>} </pre>
<p>As we did before, let's quickly discuss the motivation behind this evaluation and how we plan to implement it. This method is used to favor items that match the composition of the user's search; this allows our search to surface images that closely resemble the arrangement the user is searching for. For example, the user may be looking for an image or images where two dogs are sitting next to each other, side by side, or they may want an image with two dogs sitting next to each other on a sofa.</p>
<p>There are no doubt many approaches you could take for this, and it's perhaps a use case for a neural network, but the approach taken here is the simplest I could think of to avoid having to explain complicated code; the algorithm used is described as follows:</p>
<ol>
<li>For each object (<kbd>a</kbd>) of type <kbd>ObjectBounds</kbd> within <kbd>searchCriteria</kbd>
<ol>
<li>Find the closest object (<kbd>b</kbd>) in proximity (still within <kbd>searchCriteria</kbd>)</li>
<li>Create a normalized direction vector from <kbd>a</kbd> to <kbd>b</kbd></li>
<li>Find the matching object <kbd>a'</kbd> (the same class) within the <kbd>detectedObject</kbd>
<ol>
<li>Search all other objects (<kbd>b'</kbd>) in <kbd>detectedObject</kbd> that have the same class as <kbd>b</kbd>
<ol>
<li>Create a normalized direction vector from <kbd>a'</kbd> to <kbd>b'</kbd></li>
<li>Calculate the dot product between the two vectors (angle); in this case, our vectors are <kbd>a-&gt;b</kbd> and <kbd>a'-&gt;b'</kbd></li>
</ol>
</li>
</ol>
</li>
<li>Using <kbd>a'</kbd> and <kbd>b'</kbd>, which have the lowest dot product, increment the cost by how much the angle differs from the search criteria and images</li>
</ol>
</li>
</ol>
<p>Essentially, what we are doing is finding two matching pairs from the <kbd>searchCriteria</kbd> and <kbd>detectedObject</kbd> arrays, and calculating the cost based on the difference in the angles.</p>
<div class="packt_infobox">A direction vector of two objects is calculated by subtracting one's position from the other and then normalizing it. The dot product can <span>then</span> be used on two (normalized) vectors to find their angle, where <em>1.0</em> would be returned if the vectors are pointing in the same direction, <em>0.0</em> if they are perpendicular, and <em>-1.0</em> if pointing in opposite directions.</div>
<p>The following figure presents part of this process; we first find an object pair in close proximity within the search criteria. After calculating the dot product, we iterate over all the objects detected in the image and find the most suitable pair; "suitable" here means the same object type and the closest angle to the search criteria within the possible matching pairs:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b6556913-07e2-45cd-90be-1fe8e929762b.png" style="width:55.25em;height:36.00em;"/></div>
<p>Once comparable pairs are found, we calculate the cost based on the difference in angle, as we will soon see. But we are getting a little ahead of ourselves; we first need a way to find the closest object. Let's do this using a nested function we can call within our <kbd>costForObjectRelativePositioning</kbd> method. Add the following code, replacing the comment <kbd>// TODO implement cost function for relative positioning</kbd>:</p>
<pre>func indexOfClosestObject(<br/>    objects:[ObjectBounds],<br/>    forObjectAtIndex i:Int) -&gt; Int{<br/>    <br/>    let searchACenter = objects[i].bounds.center<br/>    <br/>    var closestDistance = Float.greatestFiniteMagnitude<br/>    var closestObjectIndex : Int = -1<br/>    <br/>    for j in 0..&lt;objects.count{<br/>        guard i != j else{<br/>            continue<br/>        }<br/>        <br/>        let searchBCenter = objects[j].bounds.center<br/>        let distance = Float(searchACenter.distance(other: searchBCenter))<br/>        if distance &lt; closestDistance{<br/>            closestObjectIndex = j<br/>            closestDistance = distance<br/>        }<br/>    }<br/>    <br/>    return closestObjectIndex<br/>}<br/><br/><strong>// TODO Iterate over all items in the searchCriteria array</strong> </pre>
<p>The <span>preceding</span> function will be used to find the closest object, given an array of <kbd>ObjectBounds</kbd> and index of the object we are searching against. From there, it simply iterates over all of the items in the array, returning the one that is, well, closest.</p>
<p>With our helper function now implemented, let's create the loop that will inspect the search item pair from the user's search criteria. Append the following code to the <kbd>costForObjectRelativePositioning</kbd> method, replacing the comment <kbd>// TODO Iterate over all items in the searchCriteria array</kbd>:</p>
<pre>for si in 0..&lt;searchCriteria.count{<br/>    let closestObjectIndex = indexOfClosestObject(<br/>        objects: searchCriteria,<br/>        forObjectAtIndex: si)<br/>    <br/>    if closestObjectIndex &lt; 0{<br/>        continue<br/>    }<br/>    <br/>    // Get object types<br/>    let searchAClassIndex = searchCriteria[si].object.classIndex<br/>    let searchBClassIndex = searchCriteria[closestObjectIndex].object.classIndex<br/>    <br/>    // Get centers of objects<br/>    let searchACenter = searchCriteria[si].bounds.center<br/>    let searchBCenter = searchCriteria[closestObjectIndex].bounds.center<br/>    <br/>    // Calcualte the normalised vector from A -&gt; B<br/>    let searchDirection = (searchACenter - searchBCenter).normalised<br/>    <br/>    <strong>// TODO Find matching pair</strong><br/>}  </pre>
<p>We start by searching for the closest object to the current object, jumping to the next item if nothing is found. Once we have our search pair, we proceed to calculate the direction by subtracting the first bound's center from its pair and normalizing the result.</p>
<p>We now need to find all objects of both classes, whereby we will proceed to evaluate each of them to find the best match. Before that, let's get all the classes with the index of <kbd>searchAClassIndex</kbd> and <kbd>searchBClassIndex</kbd>; add the following code, replacing the comment <kbd>// TODO Find matching pair</kbd>:</p>
<pre>// Find comparable objects in detected objects<br/>let detectedA = detectedObject.detectedObjects.filter {<br/>    (objectBounds) -&gt; Bool in<br/>    objectBounds.object.classIndex == searchAClassIndex<br/>}<br/><br/>let detectedB = detectedObject.detectedObjects.filter {<br/>    (objectBounds) -&gt; Bool in<br/>    objectBounds.object.classIndex == searchBClassIndex<br/>}<br/><br/>// Check that we have matching pairs<br/>guard detectedA.count &gt; 0, detectedB.count &gt; 0 else{<br/>    continue<br/>}<br/><br/><strong>// TODO Search for the most suitable pair</strong></pre>
<p>If we are unable to find a matching pair, we continue to the next item, knowing that a cost has already been added for the mismatch in objects of both arrays. Next, we iterate over all pairs. For each pair, we calculate the normalized direction vector and then the dot product against our <kbd>searchDirection</kbd> vector, taking the one that has the closest dot product (closest in angle). Add the following code in place of the comment <kbd>// TODO Search for the most suitable pair</kbd>:</p>
<pre>var closestDotProduct : Float = Float.greatestFiniteMagnitude<br/>for i in 0..&lt;detectedA.count{<br/>    for j in 0..&lt;detectedB.count{<br/>        if detectedA[i] == detectedB[j]{<br/>            continue<br/>        }<br/>        <br/>        let detectedDirection = (detectedA[i].bounds.center - detectedB[j].bounds.center).normalised<br/>        let dotProduct = Float(searchDirection.dot(other: detectedDirection))<br/>        if closestDotProduct &gt; 10 ||<br/>            (dotProduct &lt; closestDotProduct &amp;amp;&amp;amp;<br/>                dotProduct &gt;= 0) {<br/>            closestDotProduct = dotProduct<br/>        }<br/>    }<br/>}<br/><br/><strong>// TODO Add cost </strong></pre>
<p>Similar to what we did with our search pair, we calculate the direction vector by subtracting the pair's center positions and then normalize the result. Then, with the two vectors <kbd>searchDirection</kbd> and <kbd>detectedDirection</kbd>, we calculate the dot product, keeping reference to it if it is the first or lowest dot product so far.</p>
<p>There is just one last thing we need to do for this method, and this project. But before doing so, let's take a little detour and look at a couple of extensions made to <kbd>CGPoint</kbd>, specifically the <kbd>dot</kbd> and <kbd>normalize</kbd> used previously. You can find these extensions in the <kbd>CGPoint+Extension.swift</kbd> file. As I did previously, I will list the code for reference rather than describing the details, most of which we have already touched upon:</p>
<pre>extension CGPoint{<br/>    <br/>    var length : CGFloat{<br/>        get{<br/>            return sqrt(<br/>                self.x * self.x + self.y * self.y<br/>            )<br/>        }<br/>    }<br/>    <br/>    var normalised : CGPoint{<br/>        get{<br/>            return CGPoint(<br/>                x: self.x/self.length,<br/>                y: self.y/self.length)<br/>        }<br/>    }<br/>    <br/>    func distance(other:CGPoint) -&gt; CGFloat{<br/>        let dx = (self.x - other.x)<br/>        let dy = (self.y - other.y)<br/>        <br/>        return sqrt(dx*dx + dy*dy)<br/>    }<br/>    <br/>    func dot(other:CGPoint) -&gt; CGFloat{<br/>        return (self.x * other.x) + (self.y * other.y)<br/>    }<br/>    <br/>    static func -(left: CGPoint, right: CGPoint) -&gt; CGPoint{<br/>        return CGPoint(<br/>            x: left.x - right.x,<br/>            y: left.y - right.y)<br/>    }<br/>}   </pre>
<p>Now, back to the <kbd>costForObjectRelativePositioning</kbd> method to finish our method and project. Our final task is to add to the cost; this is done simply by subtracting the stored <kbd>closestDotProduct</kbd> from <kbd>1.0</kbd> (remembering that we want to increase the cost for larger differences where the dot product of two normalized vectors pointing in the same direction is <kbd>1.0</kbd>) and ensuring that the value is positive by wrapping it in an <kbd>abs</kbd> function. Let's do that now; add the following code, replacing the comment <kbd>// TODO add cost</kbd>:</p>
<pre>cost += abs((1.0-closestDotProduct))</pre>
<p>With that done, we have finished this method, and the coding for this chapter. Well done! It's time to test it out; build and run the project to see your hard work in action. Shown here are a few searches and their results:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/02d7b7ca-c6d5-44b5-86b0-a4b7242dc890.png" style="width:52.33em;height:32.17em;"/></div>
<p>Although the YOLO algorithm is performant and feasible for near real-time use, our example is far from optimized and unlikely to perform well on large sets of photos. With the release of Core ML 2, Apple provides one avenue we can use to make our process more efficient. This will be the topic of the next section before wrapping up.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing with batches</h1>
                </header>
            
            <article>
                
<p>At the moment, our process involves iterating over each photo and performing inference on each one individually. With the release of Core ML 2, we now have the option to create a batch and pass this batch to our model for inference. As with efficiencies gained with economies of scale, here, we also gain significant improvements; so let's walk through adapting our project to process our photos in a single batch rather than individually.</p>
<p>Let's work our way up the stack, starting in our <kbd>YOLOFacade</kbd> class and moving up to the <kbd>PhotoSearcher</kbd>. For this we will be using our model directly rather than proxying through Vision, so our first task is to replace the <kbd>model</kbd> property of our <kbd>YOLOFacade</kbd> class with the following declaration:</p>
<pre>let model = tinyyolo_voc2007().model</pre>
<p>Now, let's rewrite the <kbd>detectObjects</kbd> method to handle an array of photos rather than a single instance; because this is where most of the changes reside, we will start from scratch. So, go ahead and delete the method from your <kbd>YOLOFacade</kbd> class and replace it with the following stub:</p>
<pre>func detectObjects(<strong>photos:[UIImage]</strong>, completionHandler:(_ <strong>result:[[ObjectBounds]]</strong>?) -&gt; Void){<br/>    <br/>    <strong>// TODO batch items (array of MLFeatureProvider)<br/><br/>    // TODO Wrap our items in an instance of MLArrayBatchProvider <br/><br/>    // TODO Perform inference on the batch </strong><br/>    <br/><strong>    // TODO (As we did before) Process the outputs of the model</strong> <br/><br/><strong>    // TODO Return results via the callback handler</strong> <br/>}</pre>
<p>I have made the changes to the signature of the method bold<strong> </strong>and listed our remaining tasks. The first is to create an array of <kbd>MLFeatureProvider</kbd>. If you recall from <a href="5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml" target="_blank">Chapter 3</a>, <em>Recognizing Objects in the World</em>, when we import a Core ML model into Xcode, it generates interfaces for the model, its input and output. The input and output are subclasses of <kbd>MLFeatureProvider</kbd>, so here we want to create an array of <kbd>tinyyolo_voc2007Input</kbd>, which can be instantiated with instances of <kbd>CVPixelBuffer</kbd>.</p>
<p>To create this, we will transform the array of photos passed into the method, including the required preprocessing steps (resizing to 416 x 416). Replace the comment <kbd>// TODO batch items (array of MLFeatureProvider)</kbd> with the following code:</p>
<pre>let X = photos.map({ (photo) -&gt; tinyyolo_voc2007Input in<br/>    guard let ciImage = CIImage(image: photo) else{<br/>        fatalError("\(#function) Failed to create CIImage from UIImage")<br/>    }<br/>    let cropSize = CGSize(<br/>        width:min(ciImage.extent.width, ciImage.extent.height),<br/>        height:min(ciImage.extent.width, ciImage.extent.height))<br/>    <br/>    let targetSize = CGSize(width:416, height:416)<br/>    <br/>    guard let pixelBuffer = ciImage<br/>        .centerCrop(size:cropSize)?<br/>        .resize(size:targetSize)<br/>        .toPixelBuffer() else{<br/>        fatalError("\(#function) Failed to create CIImage from UIImage")<br/>    }<br/>    <br/>    return tinyyolo_voc2007Input(image:pixelBuffer)<br/>    <br/>}) <br/><br/><strong>// TODO Wrap our items in an instance of MLArrayBatchProvider </strong><br/><br/><strong>// TODO Perform inference on the batch </strong><br/><strong>    </strong><br/><strong>// TODO (As we did before) Process the outputs of the model </strong><br/><br/><strong>// TODO Return results via the callback handler </strong></pre>
<div class="packt_infobox">For reasons of simplicity and readability, we are omitting any kind of error handling; obviously, in production you want to handle exceptions appropriately.</div>
<p>To perform inference on a batch, we need to have our input conform to the <kbd>MLBatchProvider</kbd> interface. Fortunately, Core ML provides a concrete implementation that conveniently wraps array. Let's do this now; replace the comment <kbd>// TODO Wrap our items in an instance of MLArrayBatchProvider</kbd> with the following code:</p>
<pre>let batch = MLArrayBatchProvider(array:X)<br/><br/><strong>// TODO Perform inference on the batch </strong><br/><strong>    </strong><br/><strong>// TODO (As we did before) Process the outputs of the model </strong><br/><br/><strong>// TODO Return results via the callback handler</strong> </pre>
<p>To perform inference, it's simply a matter of calling the <kbd>predictions</kbd> method on our model; as usual, replace the comment <kbd>// TODO Perform inference on the batch</kbd> with the following code:</p>
<pre>guard let batchResults = try? self.model.predictions(<br/>    from: batch,<br/>    options: MLPredictionOptions()) else{<br/>        completionHandler(nil)<br/>        return<br/>}<br/><br/><strong>// TODO (As we did before) Process the outputs of the model <br/></strong><br/><strong>// TODO Return results via the callback handler</strong><span> <br/></span></pre>
<p>What we get back is an instance of <kbd>MLBatchProvider</kbd> (if successful); this is more or less a collection of results for each of our samples (inputs). We can access an specific result via the batch providers <kbd>features(at: Int)</kbd> <span>method,</span> which returns an instance of <kbd>MLFeatureProvider</kbd> (in our case, an in <kbd>tinyyolo_voc2007Output</kbd>).</p>
<p>Here we simply process each result as we had done before to obtain the most salient; replace the comment <kbd>// TODO (As we did before) Process the outputs of the model</kbd> with the following code:</p>
<pre>var results = [[ObjectBounds]]()<br/><br/>for i in 0..&lt;batchResults.count{<br/>    var iResults = [ObjectBounds]()<br/>    <br/>    if let features = batchResults.features(at: i)<br/>        as? tinyyolo_voc2007Output{<br/>        <br/>        if let observationDetectObjects = self.detectObjectsBounds(<br/>            array: features.output){<br/>            <br/>            for detectedObject in observationDetectObjects.map(<br/>                {$0.transformFromCenteredCropping(<br/>                    from: photos[i].size,<br/>                    to: self.targetSize)}){<br/>                        <br/>                    iResults.append(detectedObject)<br/>            }<br/>            <br/>        }<br/>    }<br/>    results.append(iResults)<br/>}<br/><br/><strong>// TODO Return results via the callback handler</strong> </pre>
<p>The only difference here, than before, is that we are iterating over a batch of outputs rather than a single one. The last thing we need to do is call the handler; replace the comment <kbd>// TODO Return results via the callback handler</kbd> with the following statement:</p>
<pre>completionHandler(results)</pre>
<p>This now completes the changes required to our <kbd>YOLOFacade</kbd> class; let's jump into the <kbd>PhotoSearcher</kbd> and make the necessary, and final, changes.</p>
<p>The big change here is that we now need to pass in all photos at once rather than passing each one individually. Locate the <kbd>detectObjects</kbd> method and replace its body with the following code:</p>
<pre>var results = [SearchResult]()<br/><br/>yolo.detectObjects(photos: photos) { (photosObjectBounds) in<br/>    if let photosObjectBounds = photosObjectBounds,<br/>        photos.count == photosObjectBounds.count{<br/>        for i in 0..&lt;photos.count{<br/>            results.append(SearchResult(<br/>                image: photos[i],<br/>                detectedObjects: photosObjectBounds[i],<br/>                cost: 0.0))<br/>        }<br/>    }<br/>}<br/><br/>return results</pre>
<p>Same code but organised a little differently to handle batch inputs and output from and to the <kbd>YOLOFacade</kbd> class. Now is a good time to build, deploy and run the application; paying particular attention to efficiencies gained from adapting batch inference. When you return; we will conclude this chapter with a quick summary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the concept of object detection, comparing it with object recognition and <span>object</span> localization. While the other two are limited to a single dominant object, object detection allows multi-object classification, including predicting their bounding boxes. We then spent some time introducing one particular algorithm, YOLO, before getting acquainted with Apple's Core ML Tools Python package, walking through converting a trained Keras model to Core ML. Once we had the model in hand, we moved on to implementing YOLO in Swift with the goal of creating an intelligent search application.</p>
<p>Despite this being a fairly lengthy chapter, I hope you found it valuable and gained deeper intuition into how deep neural networks learn and understand images and how they can be applied in novel ways to create new experiences. It's helpful to remind ourselves that using the same architecture, we can create devise new applications by simply swapping the data we train it on. For example, you could train this model on a dataset of hands and their corresponding bounding boxes to create a more immersive <strong>augmented reality</strong> (<strong>AR</strong>) experience by allowing the user to interact with digital content through touch.</p>
<p>But for now, let's continue our journey of understanding Core ML and explore how else we can apply it. In the next chapter, you will see how the popular Prisma creates those stunning photos with style transform.</p>


            </article>

            
        </section>
    </body></html>