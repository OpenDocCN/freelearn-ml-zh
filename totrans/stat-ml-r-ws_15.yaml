- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Linear Regression in R
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R中的线性回归
- en: 'In this chapter, we will introduce linear regression, a fundamental statistical
    approach that’s used to model the relationship between a target variable and multiple
    explanatory (also called independent) variables. We will cover the basics of linear
    regression, starting with simple linear regression and then extending the concepts
    to multiple linear regression. We will learn how to estimate the model coefficients,
    evaluate the goodness of fit, and test the significance of the coefficients using
    hypothesis testing. Additionally, we will discuss the assumptions underlying linear
    regression and explore techniques to address potential issues, such as nonlinearity,
    interaction effect, multicollinearity, and heteroskedasticity. We will also introduce
    two widely used regularization techniques: the ridge and **Least Absolute Shrinkage
    and Selection Operator** (**lasso**) penalties.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍线性回归，这是一种基本的统计方法，用于建模目标变量与多个解释变量（也称为独立变量）之间的关系。我们将从简单线性回归开始，然后扩展到多元线性回归的概念。我们将学习如何估计模型系数，评估拟合优度，并使用假设检验测试系数的显著性。此外，我们还将讨论线性回归背后的假设，并探讨解决潜在问题（如非线性、交互效应、多重共线性异方差性）的技术。我们还将介绍两种广泛使用的正则化技术：岭回归和**最小绝对收缩和选择算子**（**lasso**）惩罚。
- en: By the end of this chapter, you will learn the core principles of linear regression,
    its extensions to regularized linear regression, and the implementation details
    involved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将学习线性回归的核心原则，其扩展到正则化线性回归，以及涉及的实现细节。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing linear regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍线性回归
- en: Introducing penalized linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍惩罚线性回归
- en: Working with ridge regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用岭回归进行工作
- en: Working with lasso regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用lasso回归进行工作
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的代码，您需要拥有以下包的最新版本：
- en: '`ggplot2`, 3.4.0'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ggplot2`, 3.4.0'
- en: '`tidyr`, 1.2.1'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyr`, 1.2.1'
- en: '`dplyr`, 1.0.10'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dplyr`, 1.0.10'
- en: '`car`, 3.1.1'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`car`, 3.1.1'
- en: '`lmtest`, 0.9.40'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lmtest`, 0.9.40'
- en: '`glmnet`, 4.1.7'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glmnet`, 4.1.7'
- en: Please note that the versions of the packages mentioned in the preceding list
    are the latest ones at the time of writing this chapter. All the code and data
    for this chapter are available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面提到的包的版本是在编写本章时的最新版本。本章的所有代码和数据均可在[https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R)找到。
- en: Introducing linear regression
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍线性回归
- en: At the core of linear regression is the concept of fitting a straight line –
    or more generally, a hyperplane – to the data points. Such fitting aims to minimize
    the deviation between the observed and predicted values. When it comes to simple
    linear regression, one target variable is regressed by one predictor, and the
    goal is to fit a straight line that best mimics the relationship between the two
    variables. For multiple linear regression, there is more than one predictor, and
    the goal is to fit a hyperplane that best describes the relationship among the
    variables. Both tasks can be achieved by minimizing a measure of deviation between
    the predictions and the corresponding targets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的核心是拟合一条直线——或者更一般地说，一个超平面——到数据点。这种拟合旨在最小化观察值和预测值之间的偏差。对于简单线性回归，一个目标变量由一个预测变量回归，目标是拟合一条最佳模拟两个变量之间关系的直线。对于多元线性回归，存在多个预测变量，目标是拟合一个最佳描述变量之间关系的超平面。这两个任务都可以通过最小化预测值和相应目标之间的偏差度量来实现。
- en: In linear regression, obtaining an optimal model means identifying the best
    coefficients that define the relationship between the target variable and the
    input predictors. These coefficients represent the change in the target associated
    with a single unit change in the associated predictor, assuming all other variables
    are constant. This allows us to quantify the magnitude (size of the coefficient)
    and direction (sign of the coefficient) of the relationship between the variables,
    which can be used for inference (highlighting explainability) and prediction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，获得最佳模型意味着识别定义目标变量和输入预测变量之间关系的最佳系数。这些系数代表与相关预测变量的单位变化相关的目标变量的变化，假设其他所有变量保持不变。这使我们能够量化变量之间关系的大小（系数的大小）和方向（系数的符号），这些可以用于推断（强调可解释性）和预测。
- en: When it comes to inference, we often look at the relative impact on the target
    variable given a unit change to the input variable. Examples of such explanatory
    modeling include how marketing spend affects quarterly sales, how smoker status
    affects insurance premiums, and how education affects income. On the other hand,
    predictive modeling focuses on predicting a target quantity. Examples include
    predicting quarterly sales given the marketing spend, predicting the insurance
    premium given a policyholder’s profile information, such as age and gender, and
    predicting income given someone’s education, age, work experience, and industry.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到推断时，我们通常会观察输入变量单位变化对目标变量的相对影响。此类解释建模的例子包括营销支出如何影响季度销售额、吸烟状态如何影响保险费率、以及教育如何影响收入。另一方面，预测建模侧重于预测目标量。例如，根据营销支出预测季度销售额、根据投保人的个人资料信息（如年龄和性别）预测保险费率，以及根据某人的教育、年龄、工作经验和行业预测收入。
- en: In linear regression, the expected outcome is modeled as a weighted sum of all
    the input variables. It also assumes that the change in the output is linearly
    proportional to the change in any input variable. This is the simplest form of
    the regression method.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，预期的结果被建模为所有输入变量的加权总和。它还假设输出的变化与任何输入变量的变化成线性比例。这是回归方法的最简单形式。
- en: Let’s start with **simple linear** **regression** (**SLR**).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**简单线性** **回归**（**SLR**）开始。
- en: Understanding simple linear regression
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解简单线性回归
- en: SLR is a powerful and widely used statistical model that specifies the relationship
    between two continuous variables, including one input and one output. It allows
    us to understand how a response variable (also referred to as the dependent or
    target variable) changes as the explanatory variable (also called the independent
    variable or the input variable) varies. By fitting a straight line to the observed
    data, SLR quantifies the strength and direction of the linear association between
    the two variables. This straight line is called the SLR **model**. It enables
    us to make predictions and infer the impact of the predictor on the target variable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SLR（简单线性回归）是一种强大且广泛使用的统计模型，它规定了两个连续变量之间的关系，包括一个输入和一个输出。它使我们能够理解响应变量（也称为因变量或目标变量）如何随着解释变量（也称为自变量或输入变量）的变化而变化。通过将一条直线拟合到观察到的数据，SLR量化了两个变量之间线性关联的强度和方向。这条直线被称为SLR
    **模型**。它使我们能够进行预测并推断预测变量对目标变量的影响。
- en: 'Specifically, in an SLR model, we assume a linear relationship between the
    target variable (y) and the input variable (x). The model can be represented mathematically
    as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在SLR模型中，我们假设目标变量（y）和输入变量（x）之间存在线性关系。该模型可以用以下数学公式表示：
- en: y = β 0 + β 1 x + ϵ
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: y = β 0 + β 1 x + ϵ
- en: Here, y is called the response variable, dependent variable, explained variable,
    predicted variable, target variable, or regressand. x is called the explanatory
    variable, independent variable, control variable, predictor variable, or regressor.
    β 0 is the intercept of the linear line that represents the expected value of
    y when x is 0\. β 1 is the slope that represents the change in y for a one-unit
    increase in x. Finally, ϵ is the random error term that accounts for the variability
    in the target, y, that the predictor, x, cannot explain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，y被称为响应变量、因变量、解释变量、预测变量、目标变量或回归量。x被称为解释变量、自变量、控制变量、预测变量或回归变量。β 0是线性线的截距，表示当x为0时y的期望值。β 1是斜率，表示x增加一个单位时y的变化。最后，ϵ是随机误差项，它解释了目标变量y中预测变量x无法解释的变异性。
- en: 'The main objective of SLR is to estimate the β 0 and β 1 parameters. An optimal
    set of β 0 and β 1 would minimize the total squared deviations between the observed
    target values, y, and the predicted values,  ˆ y , using the model. This is called
    the **least squares method**, where we seek the optimal β 0 and β 1 parameters
    that correspond to the minimum **sum of squared** **error** (**SSR**):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型（SLR）的主要目标是估计 β₀ 和 β₁ 参数。一组最优的 β₀ 和 β₁ 参数将最小化观测目标值 y 和预测值 ˆy 之间的总平方偏差。这被称为**最小二乘法**，我们寻求最优的
    β₀ 和 β₁ 参数，它们对应于最小**平方误差和**（SSR）：
- en: minSSR = min∑ i=1 n u i 2 = min(y i − ˆ y i) 2
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: minSSR = min∑i=1nui² = min(yi − ˆyi)²
- en: 'Here, each residual, u i, is the difference between the observation, y i, and
    its fitted value,  ˆ y  i. In simple terms, the objective is to locate the straight
    line that is closest to the data points given. *Figure 12**.1* illustrates a collection
    of data points (in blue) and the linear model (in red):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个残差 ui 是观测值 yi 和其拟合值 ˆyi 之间的差异。简单来说，目标是找到最接近数据点的直线。*图 12**.1* 展示了一组数据点（蓝色）和线性模型（红色）：
- en: '![Figure 12.1 – The SLR model, where the linear model appears as a line and
    is trained by minimizing the SSR](img/B18680_12_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 线性回归模型，其中线性模型以线的形式出现，并通过最小化 SSR 进行训练](img/B18680_12_001.jpg)'
- en: Figure 12.1 – The SLR model, where the linear model appears as a line and is
    trained by minimizing the SSR
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 线性回归模型，其中线性模型以线的形式出现，并通过最小化 SSR 进行训练
- en: 'Once we have estimated the model coefficients, β 0 and β 1, we can use the
    model to make predictions and inferences on the intensity of the linear relationship
    between the variables. Such a linear relationship indicates the goodness of fit,
    which is often measured using the coefficient of determination, or R 2\. R 2 ranges
    from `0` to `1` and quantifies the proportion of the total variation in y that
    can be explained by x. It is defined as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们估计了模型系数 β₀ 和 β₁，我们就可以使用该模型对变量之间线性关系的强度进行预测和推断。这种线性关系表明了拟合优度，通常使用确定系数，或 R²
    来衡量。R² 的范围从 `0` 到 `1`，并量化了 y 的总变异中可以由 x 解释的部分。它定义如下：
- en: R 2 = 1 −  ∑ i (y i − ˆ y i) 2 _ ∑ i (y i −  _ y ) 2
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: R² = 1 − ∑i(yi − ˆyi)² / ∑i(yi −  _y)²
- en: Here,  _ y  denotes the average value of the observed target variable, y.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，_y 表示观测目标变量 y 的平均值。
- en: Besides this, we can also use hypothesis testing to test the significance of
    the resulting coefficients, β 0 and β 1, thus helping us determine whether the
    observed relationship between the variables is statistically significant.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以使用假设检验来检验得到的系数 β₀ 和 β₁ 的显著性，从而帮助我们确定变量之间的观测关系是否具有统计学意义。
- en: Let’s go through an example of building a simple linear model using a simulated
    dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用模拟数据集构建简单线性模型的例子来进行分析。
- en: Exercise 12.1 – building an SLR model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 12.1 – 构建线性回归模型
- en: 'In this exercise, we will demonstrate the implementation of an SLR model in
    R. We’ll be using a combination of built-in functions and packages to accomplish
    this task using a simulated dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将演示在 R 中实现线性回归模型。我们将使用内置函数和包的组合，使用模拟数据集来完成这项任务：
- en: 'Simulate a dataset such that the response variable, `Y`, is linearly dependent
    on the explanatory variable, `X`, with some added noise:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟一个数据集，使得响应变量 `Y` 线性依赖于解释变量 `X`，并添加了一些噪声：
- en: '[PRE0]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we use the `runif()` function to generate the independent variable, `X`,
    which is a vector of random uniform numbers. Then, we add some “noise” to the
    dependent variable, `Y`, making the observed data more realistic and less perfectly
    linear. This is achieved using the `rnorm()` function, which creates a vector
    of random normal numbers. The target variable, `Y`, is then created as a function
    of `X`, plus the noise.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `runif()` 函数生成自变量 `X`，它是一个随机均匀数的向量。然后，我们在因变量 `Y` 上添加一些“噪声”，使观测数据更真实，且不那么完美地线性。这是通过使用
    `rnorm()` 函数实现的，它创建了一个随机正态数的向量。然后，目标变量 `Y` 被创建为 `X` 的函数，加上噪声。
- en: Besides this, we use a seed (`set.seed(123)`) at the beginning to ensure reproducibility.
    This means that we will get the same set of random numbers every time we run this
    code. Each run will produce a different list of random numbers if we don’t set
    a seed.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们在开始时使用了一个种子（`set.seed(123)`）来确保可重复性。这意味着每次运行此代码时，我们都会得到相同的一组随机数。如果我们不设置种子，每次运行都会产生不同的随机数列表。
- en: In this simulation, the true intercept (β 0) is 5, the true slope (β 1) is 0.5,
    and the noise is normally distributed with a mean of 0 and a standard deviation
    of 10.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个模拟中，真实的截距（β 0）是5，真实的斜率（β 1）是0.5，噪声是均值为0、标准差为10的正态分布。
- en: 'Train a linear regression model based on the simulated dataset using the `lm()`
    function:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`lm()`函数基于模拟数据集训练线性回归模型：
- en: '[PRE1]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we use the `lm()` function to fit the data, where `lm` stands for “linear
    model.” This function creates our SLR model. The `Y ~ X` syntax is how we specify
    our model: it tells the function that `Y` is being modeled as a function of `X`.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`lm()`函数来拟合数据，其中`lm`代表“线性模型”。此函数创建我们的简单线性回归（SLR）模型。`Y ~ X`语法是我们指定模型的方式：它告诉函数`Y`被建模为`X`的函数。
- en: The `summary()` function provides a comprehensive overview of the model, including
    the estimated coefficients, the standard errors, the t-values, and the p-values,
    among other statistics. Since the resulting p-value is extremely low, we can conclude
    that the input variable is predictive with strong statistical significance.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`summary()`函数提供了模型的全面概述，包括估计系数、标准误差、t值和p值等统计量。由于得到的p值极低，我们可以得出结论，输入变量具有强烈的统计显著性。'
- en: 'Use the `plot()` and `abline()` functions to visualize the data and the fitted
    regression line:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot()`和`abline()`函数可视化数据和拟合的回归线：
- en: '[PRE2]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Running this code generates *Figure 12**.2*:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码生成*图12.2*：
- en: '![Figure 12.2 – Visualizing the data and the fitted regression line](img/B18680_12_002.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – 可视化数据和拟合的回归线](img/B18680_12_002.jpg)'
- en: Figure 12.2 – Visualizing the data and the fitted regression line
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 可视化数据和拟合的回归线
- en: Here, the `plot()` function creates a scatter plot of our data, and the `abline()`
    function adds the regression line to this plot. Such a visual representation is
    very useful for understanding the quality of the fitting.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`plot()`函数创建了我们数据的散点图，而`abline()`函数将回归线添加到该图上。这种视觉表示对于理解拟合的质量非常有用。
- en: We’ll move on to the **multiple linear regression** (**MLR**) model in the next
    section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续介绍**多元线性回归**（**MLR**）模型。
- en: Introducing multiple linear regression
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍多元线性回归
- en: MLR expands the single predictor in SLR to predict the target outcome based
    on multiple predictor variables. Here, the term “multiple” in MLR refers to the
    multiple predictors in the model, where each feature is given a coefficient. A
    specific coefficient, β, represents the change in the outcome variable for a single
    unit change in the associated predictor variable, assuming all other predictors
    are held constant.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MLR将SLR中的单个预测因子扩展到基于多个预测变量预测目标结果。在这里，MLR中的“多个”一词指的是模型中的多个预测因子，其中每个特征都被赋予一个系数。特定的系数β代表在相关预测变量单位变化的情况下，假设所有其他预测变量保持不变，结果变量的变化。
- en: 'One of the great advantages of MLR is its ability to include multiple predictors,
    allowing for a more complex and realistic (linear) representation of the real
    world. It can provide a holistic view of the connection between the target and
    all input variables. This is particularly useful in fields where the outcome variable
    is likely influenced by more than one predictor variable. It is modeled via the
    following formula:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MLR（多元线性回归）的一个显著优点是它能够包含多个预测因子，从而允许对现实世界进行更复杂和真实的（线性）表示。它可以提供对目标变量与所有输入变量之间关系的整体视角。这在结果变量可能受多个预测变量影响的领域中特别有用。它通过以下公式进行建模：
- en: y = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p + ϵ
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: y = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p + ϵ
- en: 'Here, we have a total of p features, and therefore, (p + 1) coefficients due
    to the intercept term. ϵ is the usual noise term that represents the unexplained
    part. In other words, our prediction using MLR is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总共有p个特征，因此由于截距项，有(p + 1)个系数。ϵ是表示未解释部分的常规噪声项。换句话说，我们使用MLR的预测如下：
- en: ˆ y  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ˆ y  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
- en: We can perform ceteris paribus analysis with this formulation, which is a Latin
    way of saying all other things are equal, and we only change one input variable
    to assess its impact on the outcome variable. In other words, MLR allows us to
    explicitly control (that is, keep unchanged) many other factors that simultaneously
    affect the target variable and observe the impact of only one factor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此公式进行ceteris paribus分析，这是一种拉丁语表达方式，意思是所有其他事物都相等，我们只改变一个输入变量以评估其对结果变量的影响。换句话说，MLR允许我们明确控制（即保持不变）许多同时影响目标变量的其他因素，并观察仅一个因素的影响。
- en: 'For example, suppose we add a small increment, Δ x j, to the feature, x j,
    and keep all other features unchanged. The new prediction,  ˆ y  new, is obtained
    by the following formula:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们向特征x_j添加一个小的增量Δx_j，并保持所有其他特征不变。新的预测ˆy_new是通过以下公式获得的：
- en: ˆ y  new = β 0 + β 1 x 1 + … + β j( x j + Δ x j) + … + β p x p
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ˆy_new = β_0 + β_1 x_1 + … + β_j( x_j + Δx_j) + … + β_p x_p
- en: 'We know that the original prediction is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道原始预测如下：
- en: ˆ y  old = β 0 + β 1 x 1 + … + β j x j + … + β p x p
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ˆy_old = β_0 + β_1 x_1 + … + β_j x_j + … + β_p x_p
- en: 'The difference between these two gives us the change in the output variable:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个之间的差异给我们带来了输出变量的变化：
- en: Δ ˆ y  =  ˆ y  new −  ˆ y  old = β j Δ x j
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Δˆy = ˆy_new − ˆy_old = β_j Δx_j
- en: What we are doing here is essentially controlling all other input variables
    but only bumping x j to see the impact on the prediction,  ˆ y . So, the coefficient,
    β j, measures the sensitivity of the outcome to a specific feature. When we have
    a unit change, with Δ x j = 1, the change is exactly the coefficient itself, giving
    us Δ ˆ y  = β j.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的实质上是控制所有其他输入变量，但只提高x_j以观察对预测ˆy的影响。因此，系数β_j衡量结果对特定特征的敏感性。当我们有一个单位变化，Δx_j
    = 1时，变化正好是系数本身，给我们Δˆy = β_j。
- en: The next section discusses the measure of the predictiveness of the MLR model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节讨论了MLR模型的预测性度量。
- en: Seeking a higher coefficient of determination
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求更高的确定系数
- en: MLR tends to perform better than SLR due to the multiple predictors used in
    the model, such as a higher coefficient of determination (R 2). However, a regression
    model with more input variables and a higher R 2 does not necessarily mean that
    the model is a better fit and can predict better for the test set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: MLR由于模型中使用的多个预测因子（例如更高的确定系数R²）而往往比SLR表现更好。然而，具有更多输入变量和更高R²的回归模型并不一定意味着模型拟合得更好，并且可以更好地预测测试集。
- en: A higher R 2, as a result of more input features, could likely be due to overfitting.
    Overfitting occurs when a model is excessively complex, including too many predictors
    or even interaction terms between predictors. In such cases, the model may fit
    the observed data well (thus leading to a high R 2), but it may perform poorly
    when applied to new, unseen test data. This is because the model might have learned
    not only the underlying structure of the training data but also the random noise
    specific to the dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更多输入特征，更高的R²可能是因为过拟合。过拟合发生在模型过于复杂，包括过多的预测因子甚至预测因子之间的交互项。在这种情况下，模型可能很好地拟合观察到的数据（从而导致高R²），但当应用于新的、未见过的测试数据时，它可能表现不佳。这是因为模型可能不仅学会了训练数据的潜在结构，还学会了特定于数据集的随机噪声。
- en: 'Let’s look at the metric of R 2 more closely. While R 2 measures how well the
    model explains the variance in the outcome variable, it has a major limitation:
    it tends to get bigger as more predictors enter the model, even if those predictors
    are irrelevant. As a remedy, we can use the adjusted R 2\. Unlike R 2, the adjusted
    R 2 explicitly considers the number of predictors and adjusts the resulting statistic
    accordingly. If a predictor improves the model substantially, the adjusted R 2
    will increase, but if a predictor does not improve the model by a significant
    amount, the adjusted R 2 may even decrease.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看R²的度量。虽然R²衡量模型解释结果变量方差的好坏，但它有一个主要限制：随着更多预测因子的进入，它往往会变得更大，即使这些预测因子是不相关的。作为一种补救措施，我们可以使用调整后的R²。与R²不同，调整后的R²明确考虑了预测因子的数量，并相应地调整了结果统计量。如果一个预测因子显著提高了模型，调整后的R²将增加，但如果一个预测因子没有通过显著的数量提高模型，调整后的R²甚至可能减少。
- en: When building statistical models, simpler models are usually preferred when
    they perform similarly to more complex models. This principle of parsimony, also
    known as Occam’s razor, suggests that among models with similar predictive power,
    the simplest one should be chosen. In other words, adding more predictors to the
    model makes it more complex, harder to interpret, and more likely to overfit.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建统计模型时，当简单模型与更复杂的模型表现相似时，通常更倾向于选择简单模型。这个简约原则，也称为奥卡姆剃刀，表明在具有相似预测能力的模型中，应该选择最简单的一个。换句话说，向模型中添加更多的预测器会使模型更加复杂，更难以解释，并且更有可能过拟合。
- en: More on adjusted R 2
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于调整后的R²
- en: The adjusted R 2 improves upon R 2 by adjusting for the number of features in
    the selected model. Specifically, the value of the adjusted R 2 only increases
    if adding this feature is worth more than what would have been expected from adding
    a random feature. Essentially, the additional predictors that are added to the
    model must be meaningful and predictive to lead to a higher adjusted R 2\. These
    additional predictors, however, would always increase R 2 when added to the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R²通过调整所选模型中的特征数量来改进R²。具体来说，只有当添加这个特征所带来的价值超过添加随机特征所期望的价值时，调整后的R²的值才会增加。本质上，添加到模型中的额外预测器必须是具有意义和预测性的，以导致调整后的R²更高。然而，这些额外的预测器在添加到模型中时，总是会提高R²。
- en: 'The adjusted R 2 addresses this issue by incorporating the model’s degree of
    freedom. Here, the degree of freedom refers to the number of values in a statistical
    calculation that is free to vary. In the context of regression models, this typically
    means the number of predictors. The adjusted R 2 can be expressed as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R²通过包含模型的自由度来解决这一问题。在这里，自由度指的是在统计计算中可以自由变化的值的数量。在回归模型的背景下，这通常意味着预测器的数量。调整后的R²可以表示如下：
- en: Adjusted R 2 = 1 − (1 − R 2)  (n − 1) _ n − p − 1
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R² = 1 − (1 − R²) × (n − 1) × (n − p − 1)
- en: Here, n denotes the number of observations and p represents the number of features
    in the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，n表示观测值的数量，p表示模型中的特征数量。
- en: The formula works by adjusting the scale of R 2 based on the count of observations
    and predictors. The term  (n − 1) _ n − p − 1 is a ratio that reflects the degrees
    of freedom in the model, where (n − 1) represents the total degrees of freedom
    in the model. We subtract by 1 because we are estimating the mean of the dependent
    variable from the data. (n − p − 1) represents the degrees of freedom for the
    error, which, in turn, represents the number of observations left over after estimating
    the model parameters. The whole term, (1 − R 2)  (n − 1) _ n − p − 1, denotes
    the error variance that’s been adjusted for the count of predictors.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式通过根据观测值和预测器的数量调整R²的尺度来工作。术语(n − 1) × (n − p − 1)是一个反映模型中自由度的比率，其中(n − 1)代表模型中的总自由度。我们减去1是因为我们是从数据中估计因变量的均值。而(n
    − p − 1)代表误差的自由度，它反过来代表在估计模型参数后剩余的观测值数量。整个术语(1 − R²) × (n − 1) × (n − p − 1)表示调整了预测器数量的误差方差。
- en: Subtracting this from 1 results in the proportion of the total variance explained
    by the model, after adjusting for the number of predictors in the model. In other
    words, it’s a version of R 2 that penalizes the addition of unnecessary predictors.
    This helps to prevent overfitting and makes adjusted R 2 a more balanced measure
    of a model’s explanatory power when comparing models with different numbers of
    predictors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从1中减去这个值，得到的是模型解释的总方差的比例，这是在调整了模型中预测器的数量之后的结果。换句话说，它是一种R²的版本，惩罚了不必要的预测器的添加。这有助于防止过拟合，使得调整后的R²在比较具有不同预测器数量的模型时，成为一个更平衡的解释力度量。
- en: Let’s look at how to develop an MLR model in R.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在R中开发一个多元线性回归（MLR）模型。
- en: Developing an MLR model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发一个MLR模型
- en: In this section, we will develop an MLR model using the same `lm()` function
    in R based on the `mtcars` dataset, which comes preloaded with R and was used
    in previous exercises. Again, the `mtcars` dataset contains measurements for 32
    vehicles from a 1974 Motor Trend issue. These measurements include attributes
    such as miles per gallon (`mpg`), number of cylinders (`cyl`), horsepower (`hp`),
    and weight (`wt`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用R中的相同`lm()`函数，基于预加载的`mtcars`数据集来开发一个MLR模型，该数据集在之前的练习中使用过。再次强调，`mtcars`数据集包含了1974年《汽车趋势》杂志中32辆汽车的测量数据。这些测量包括诸如每加仑英里数（`mpg`）、汽缸数（`cyl`）、马力（`hp`）和重量（`wt`）等属性。
- en: Exercise 12.2 – building an MLR model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.2 – 构建一个MLR模型
- en: 'In this exercise, we will develop an MLR model to predict `mpg` using `cyl`,
    `hp`, and `wt`. We will then interpret the model results:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将开发一个MLR模型，使用`cyl`、`hp`和`wt`预测`mpg`。然后我们将解释模型结果：
- en: 'Load the `mtcars` dataset and build an MLR that predicts `mpg` based on `cyl`,
    `hp`, and `wt`:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`mtcars`数据集并构建一个基于`cyl`、`hp`和`wt`预测`mpg`的MLR模型：
- en: '[PRE3]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we first load the `mtcars` dataset using the `data()` function, then construct
    the MLR model using the `lm()` function. The `mpg ~ cyl + hp + wt` formula is
    used to specify the model. This formula tells R that we want to model `mpg` as
    a function of `cyl`, `hp`, and `wt`. The `data = mtcars` argument tells R to look
    for these variables in the `mtcars` dataset. The `lm()` function fits the model
    to the data and returns a model object, which we store in the variable model.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先使用`data()`函数加载`mtcars`数据集，然后使用`lm()`函数构建MLR模型。`mpg ~ cyl + hp + wt`公式用于指定模型。这个公式告诉R，我们想要将`mpg`建模为`cyl`、`hp`和`wt`的函数。`data
    = mtcars`参数告诉R在`mtcars`数据集中寻找这些变量。`lm()`函数将模型拟合到数据，并返回一个模型对象，我们将其存储在变量model中。
- en: 'View the summary of the model:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看模型的摘要：
- en: '[PRE4]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The summary includes the model’s coefficients (the intercept and the slopes
    for each predictor), the residuals (differences between the actual observations
    and predicted values for the target), and several statistics that tell us how
    well the model fits the data, including R 2 and the adjusted R 2.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要包括模型的系数（每个预测变量的截距和斜率）、残差（实际观察值与目标预测值之间的差异），以及一些统计量，告诉我们模型如何拟合数据，包括R²和调整R²。
- en: Let’s interpret the output. Each coefficient represents the expected change
    in `mpg` for a single unit increase in the associated predictor, assuming all
    other predictors are constant. The R 2 value, which is `0.8431`, denotes the proportion
    of variance (over 84%) in `mpg` that can be explained by the predictors together.
    Again, the adjusted R 2 value, which is `0.8263`, is a modified R 2 that accounts
    for the number of features in the model.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们解释输出结果。每个系数代表在相关预测变量增加一个单位时，`mpg`的预期变化，假设其他所有预测变量保持不变。R²值，即`0.8431`，表示`mpg`中可由预测变量共同解释的方差比例（超过84%）。再次强调，调整R²值，即`0.8263`，是一个经过修改的R²，它考虑了模型中的特征数量。
- en: In addition, the p-values for each predictor test the null hypothesis that the
    true value of the coefficient is zero. If a predictor’s p-value is smaller than
    a preset significance level (such as 0.05), we would reject this null hypothesis
    and conclude that the predictor is statistically significant. In this case, `wt`
    is the only statistically significant factor compared with others using a significance
    level of 5%.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，每个预测变量的p值测试了系数真实值为零的零假设。如果一个预测变量的p值小于预设的显著性水平（如0.05），我们会拒绝这个零假设，并得出结论说该预测变量具有统计学意义。在这种情况下，与使用5%显著性水平比较的其他因素相比，`wt`是唯一具有统计学意义的因素。
- en: In the MLR model, all coefficients are negative, indicating a reverse direction
    of travel between the input variable and the target. However, we cannot conclude
    that all the predictors negatively correlate with the target variable. The correlation
    between the individual predictor and the target variable could be positive or
    negative in SLR.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLR模型中，所有系数都是负数，表明输入变量和目标变量之间存在反向的旅行方向。然而，我们不能得出所有预测变量都与目标变量负相关的结论。在SLR中，单个预测变量与目标变量的相关性可能是正的或负的。
- en: The next section provides more context on this phenomenon.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将提供更多关于这一现象的背景信息。
- en: Introducing Simpson’s Paradox
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍辛普森悖论
- en: Simpson’s Paradox says that a trend appears in different data groups but disappears
    or changes when combined. In the context of regression analysis, Simpson’s Paradox
    can appear when a variable that seems positively correlated with the outcome might
    be negatively correlated when we control other variables.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 辛普森悖论指出，趋势在不同数据组中显现，但在合并时消失或改变。在回归分析的情况下，当控制其他变量时，一个看似与结果正相关的变量可能具有负相关性，辛普森悖论就可能出现。
- en: Essentially, this paradox illustrates the importance of considering confounding
    variables and not drawing conclusions from aggregated data without understanding
    the context. The confounding variables are those not among the explanatory variables
    under consideration but are related to both the target variable and the predictors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，这个悖论说明了考虑混杂变量的重要性，以及在未理解背景的情况下，不要从汇总数据中得出结论。混杂变量是那些不在考虑的解释变量中，但与目标变量和预测变量都相关的变量。
- en: Let’s consider a simple example through the following exercise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下练习来考虑一个简单的例子。
- en: Exercise 12.3 – illustrating Simpson’s Paradox
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.3 – 说明辛普森悖论
- en: 'In this exercise, we will look at two scenarios with opposite signs of coefficient
    values for the same feature in both SLR and MLR:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将观察两个场景，在SLR和MLR中，相同特征的系数值符号相反：
- en: 'Create a dummy dataset with two predictors and one output variable:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含两个预测变量和一个输出变量的虚拟数据集：
- en: '[PRE5]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `x1` is a set of 100 numbers randomly generated from a standard normal
    distribution. `x2` is a linear function of `x1` but with a negative correlation,
    and some random noise is added (via `rnorm(100)`). `y` is then generated as a
    linear function of `x1` and `x2`, again with some random noise added. All three
    variables are compiled into a DataFrame, `df`.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`x1`是一组从标准正态分布中随机生成的100个数字。`x2`是`x1`的线性函数，但具有负相关性，并添加了一些随机噪声（通过`rnorm(100)`）。然后，`y`作为`x1`和`x2`的线性函数生成，同样添加了一些随机噪声。所有三个变量都编译到一个DataFrame，`df`中。
- en: 'Train an SLR model with `y` as the outcome and `x1` as the input features.
    Check the summary of the model:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`y`作为结果变量和`x1`作为输入特征来训练一个简单线性回归（SLR）模型。检查模型的摘要：
- en: '[PRE6]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result shows that `x1` is negatively correlated with `y` due to a negative
    coefficient of `-2.1869`.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，由于系数为`-2.1869`，`x1`与`y`呈负相关。
- en: 'Train an SLR model with `y` as the target and `x1` and `x2` as the input features.
    Check the summary of the model:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`y`作为目标变量，`x1`和`x2`作为输入特征来训练一个SLR模型。检查模型的摘要：
- en: '[PRE7]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The result shows that the estimated coefficient for `x1` is now a positive quantity.
    Does this suggest that `x1` is suddenly positively correlated with `y`? No, since
    there are likely other confounding variables that lead to a positive coefficient.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，`x1`的估计系数现在是一个正数。这表明`x1`突然与`y`呈正相关吗？不，因为可能存在其他导致系数为正的混杂变量。
- en: The key takeaway is that we can only make inferences on the (positive or negative)
    correlation between a predictor and a target outcome in an SLR setting. For example,
    if we build an SLR model to regress `y` against `x`, we can conclude that `x`
    and `y` are positively correlated if the resulting coefficient is positive (β
    > 0). Similarly, if β > 0, we can conclude that `x` and `y` are positively correlated.
    The same applies to the case of negative correlation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的启示是，我们只能在SLR设置中（正或负）对预测变量和目标结果之间的相关性进行推断。例如，如果我们构建一个SLR模型来对`y`进行回归，如果得到的系数是正的（β
    > 0），我们可以得出结论，`x`和`y`是正相关的。同样，如果β > 0，我们可以得出结论，`x`和`y`是正相关的。同样适用于负相关的情况。
- en: However, such inference breaks in an MLR setting – that is, we cannot conclude
    a positive correlation if β > 0, and vice versa.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多元线性回归（MLR）设置中，这样的推断会失效 – 也就是说，如果我们有β > 0，我们不能得出正相关的结论，反之亦然。
- en: Let’s take this opportunity to interpret the results. The `Estimate` column
    shows the estimated regression coefficients. These values indicate how much the
    `y` variable is expected to increase when the corresponding predictor variable
    increases by one unit while holding all other features constant. In this case,
    for each unit increase in `x1`, `y` is expected to increase by approximately `0.93826`
    units, and for each unit increase in `x2`, `y` is expected to increase by approximately
    `1.02381` units. The `(Intercept)` row shows the estimated value of `y` when all
    predictor variables in the model are zero. In this model, the estimated intercept
    is `2.13507`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用这个机会来解释结果。`Estimate`列显示了估计的回归系数。这些值表示当相应的预测变量增加一个单位，同时保持所有其他特征不变时，`y`变量预期会增加多少。在这种情况下，对于`x1`每增加一个单位，`y`预计会增加大约`0.93826`个单位，对于`x2`每增加一个单位，`y`预计会增加大约`1.02381`个单位。`(Intercept)`行显示了当模型中的所有预测变量都为零时`y`的估计值。在这个模型中，估计的截距是`2.13507`。
- en: '`Std. Error` represents the standard errors for the estimates. Smaller values
    here indicate more precise estimates. The `t value` column shows the t-statistics
    for the hypothesis test that the corresponding population regression coefficient
    is zero, given that the other predictors are in the model. A larger absolute value
    of the t-statistic indicates stronger evidence against the null hypothesis. The
    `Pr(>|t|)` column gives the p-values for the hypothesis tests. In this case, both
    `x1` and `x2` have p-values below `0.05`, indicating that both are statistically
    significant predictors of `y` at the 5% significance level.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the multiple R-squared and adjusted R-squared values provide measures
    of how well the model fits the data. The multiple R-squared value is `0.8484`,
    indicating that this model explains approximately 84.84% of the variability in
    `y`. The adjusted R-squared value adjusts this measure for the number of features
    in the model. As discussed, it is a better measure when comparing models with
    different numbers of predictors. Here, the adjusted R-squared value is `0.8453`.
    The F-statistic and its associated p-value are used to test the hypothesis that
    all population regression coefficients are zero. A small p-value (less than `0.05`)
    indicates that we can reject this hypothesis, and conclude that at least one of
    the predictors is useful in predicting `y`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at the situation when we have a categorical predictor
    in the MLR model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Working with categorical variables
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In MLR, the process of including a binary predictor is similar to including
    a numeric predictor. However, the interpretation differs. Consider a dataset where
    y is the target variable, x 1 is a numeric predictor, and x 2 is a binary predictor:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: y = β 0 + β 1 x 1 + β 2 x 2 + ϵ
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In this model, x 2 is coded as 0 and 1, and its corresponding coefficient, β 2,
    represents the difference in the mean values of y between the two groups identified
    by x 2.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: For example, if x 2 is a binary variable representing sex (0 for males, 1 for
    females), and y is salary, then β 2 represents the average difference in salary
    between females and males, after accounting for the value of x 1.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Note that the interpretation of the coefficient of a binary predictor is dependent
    on the other variables in the model. So, in the preceding example, β 2 is the
    difference in salary between females and males for given values of x 1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: On the implementation side, R automatically creates dummy variables when a factor
    is used in a regression model. So, if x 2 were a factor with levels of “male”
    and “female,” R would handle the conversion to 0 and 1 internally when fitting
    the model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a concrete example. In the following code, we’re building an
    MLR model to predict `mpg` using `qsec` and `am`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that the `am` variable is treated as a numeric variable. Since it represents
    the type of transmission in the car (`0` = automatic, `1` = manual), it should
    have been treated as a categorical variable. This can be achieved by converting
    it into a factor, as shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that only one variable, `am_cat1`, is created for the categorical variable,
    `am_cat`. This is because `am_cat` is binary, thus we only need one dummy column
    (keeping only `am_cat1` and removing `am_cat0` in this case) to represent the
    original categorical variable. In general, for a categorical variable with k categorical,
    R will automatically create (k − 1) dummy variables in the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: This process is called `am` was equal to the corresponding level, and 0 otherwise.
    This essentially creates a set of indicators that capture the presence or absence
    of each category. Finally, since we can infer the last dummy variable based on
    the values of the previous (`k-1`) dummy variables, we can remove the last dummy
    variable in the resulting one-hot encoded set of variables.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a categorical variable introduces a vertical shift to the model estimate,
    as discussed in the following section. To see this, let’s look more closely at
    the impact of the categorical variable, `am_cat1`. Our MLR model now assumes the
    following form:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x qsec + β 2 x am_cat
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that x am_cat is a binary variable. When x am_cat = 0, the prediction
    becomes as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x qsec
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'When x am_cat = 1, the prediction is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x qsec + β 2 = (β 0 + β 2) + β 1 x qsec
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: By comparing these two quantities, we can see that they are two linear models
    parallel to each other since the slope is the same and the only difference is
    β 2 in the intercept term.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'A visual illustration helps here. In the following code snippet, we first create
    a new DataFrame, `newdata`, that covers the range of `qsec` values in the original
    data, for each of the `am_cat` values (0 and 1). Then, we use the `predict()`
    function to get the predicted `mpg` values from the model for this new data. Next,
    we plot the original data points with `geom_point()` and add two regression lines
    with `geom_line()`, where the lines are based on the predicted values in `newdata`.
    The `color = am_cat` aesthetic setting adds different colors to the points and
    lines for the different `am_cat` values, and the labels are adjusted in `scale_color_discrete()`
    so that 0 corresponds to “Automatic” and 1 corresponds to “Manual”:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running this code generates *Figure 12**.3*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Visualizing the two linear regression models based on different
    transmission types. These two lines are parallel to each other due to a shift
    in the intercept term](img/B18680_12_003.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Visualizing the two linear regression models based on different
    transmission types. These two lines are parallel to each other due to a shift
    in the intercept term
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: What this figure suggests is that manual transmission cars have the same miles
    per gallon (`mpg`) more than automatic transmission cars given the same quarter-mile
    time (`qsec`). However, this is unlikely in practice since different car types
    (manual versus automatic) will likely have different quarter-mile times. In other
    words, there is an interaction effect between these two variables.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The following section introduces the interaction term as a remedy to this situation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the interaction term
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In regression analysis, an interaction occurs when the effect of one predictor
    on the target variable differs depending on the level of another predictor variable.
    In our running example, we are essentially looking at whether the relationship
    between `mpg` and `qsec` is different for different values of `am`. In other words,
    we are testing whether the slope of the line relating `mpg` and `qsec` differs
    for manual (`am`=1) and automatic (`am`=0) transmissions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: For example, if there is no interaction effect, then the impact of `qsec` on
    `mpg` is the same, regardless of whether the car has a manual or automatic transmission.
    This would mean that the lines depicting the relationship between `mpg` and `qsec`
    for manual and automatic cars would be parallel.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: If there is an interaction effect, then the effect of `qsec` on `mpg` differs
    for manual and automatic cars. This would mean that the lines depicting the relationship
    between `mpg` and `qsec` for manual and automatic cars would not be parallel.
    They could either cross or, more commonly, just have different slopes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'To depict these differences in relationships, we can add an interaction term
    to the model, which is done using the `*` operator. For example, the formula for
    a regression model with an interaction between `qsec` and `am_cat` would be `mpg
    ~ qsec * am_cat`. This is equivalent to `mpg ~ qsec + am``_cat + qsec:am_cat`,
    where `qsec:am_cat` represents the interaction term. The following code shows
    the details:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s also plot the updated model, which consists of two intersecting lines
    due to the interaction effect. In the following code snippet, `geom_smooth(method
    =""l"", se = FALSE)` is used to fit different linear lines to each group of points
    (automatic and manual cars). `as.factor(am_cat)` is used to treat `am_cat` as
    a factor (categorical) variable so that a separate line is fit for each category:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Running this code generates *Figure 12**.4*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Two intersecting lines due to the intersection term between
    quarter-mile time and transmission type](img/B18680_12_004.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Two intersecting lines due to the intersection term between quarter-mile
    time and transmission type
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section focuses on another related topic: nonlinear terms.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Handling nonlinear terms
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression is a widely used model for understanding the linear relationships
    between a response and explanatory variables. However, not all underlying relationships
    in the data are linear. In many situations, a feature and a response variable
    may not have a straight-line relationship, thus necessitating the handling of
    nonlinear terms in the linear regression model to increase its flexibility.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to incorporate nonlinearity, and therefore build a curve instead
    of a straight line, is by including polynomial terms of predictors in the regression
    model. In polynomial regression, some or all predictors are raised to a specific
    polynomial term – for example, transforming a feature, x, into x 2 or x 3.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to understand the impact of adding polynomial features
    to a linear regression model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.4 – adding polynomial features to a linear regression model
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will create a simple dataset where the relationship between
    the input feature, x, and the target variable, y, is not linear, but quadratic.
    First, we will fit an SLR model, then add a quadratic term and compare the results:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a sequence of `x` values ranging from -10 to 10\. For each `x`, compute
    the corresponding `y` as the square of `x`, plus some random noise to show a (noisy)
    quadratic relationship. Put `x` and `y` in a DataFrame:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result suggests a not-so-good model fitting to the data, which possesses
    a nonlinear relationship.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit a quadratic model to the data by including x 2 as a predictor using the
    `I()` function. Print the summary of the model:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result shows that the polynomial model fits the data better than the simple
    linear model. Thus, adding nonlinear terms can improve model fit when the relationship
    between predictors and the response is not strictly linear.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the linear and quadratic models together with the data:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we first calculate the predicted values for both models and add them to
    the DataFrame. Then, we create a scatter plot of the data and add two lines representing
    the predicted values from the linear model (in blue) and the quadratic model (in
    red).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Running this code generates *Figure 12**.5*. The result suggests that adding
    a polynomial feature could extend the flexibility of a linear model:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear
    data](img/B18680_12_005.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear data
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Other common ways to introduce nonlinearity include the logarithmic transformation
    (logx) or a square root transformation (√ _ x ). These transformations can also
    be applied to the target variable, y, and we can have multiple transformed features
    in the same linear model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Note that the model with transformed features remains a linear model. If there
    is a nonlinear transformation to the coefficients, the model would be a nonlinear
    one.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section sheds more light on a widely used type of transformation:
    the logarithmic transformation.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: More on the logarithmic transformation
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logarithmic transformation, or log transformation, maps an input to the
    corresponding output based on the logarithmic function, giving y = logx. A popular
    reason behind using such a transformation is to introduce nonlinearity in the
    linear regression model. When the relationship between the input features and
    the target output is nonlinear, applying a transformation can sometimes linearize
    the relationship, making it possible to model the relationship with a linear regression
    model. For the logarithmic transformation, it can help when the rate of change
    in the outcome variable increases or decreases as the value of the predictor increases.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, the rate of change decreases as the input becomes more extreme.
    The natural consequence of such a transformation is that potential outliers in
    the input data are squeezed so that they appear less extreme in the transformed
    column. In other words, the resulting linear regression model will be less sensitive
    to the original outliers due to the log transformation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Another side benefit of using log transformation is its ability to deal with
    heteroscedasticity. Heteroscedasticity is when the variability of the error term
    in a regression model is not constant across all levels of the predictors. This
    violates one of the assumptions of linear regression models and can lead to inefficient
    and biased estimates. In this case, log transformations can stabilize the variance
    of the error term by shrinking the potential big error terms, making it more constant
    across different levels of the predictors.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, when the relationship between predictors and the outcome is multiplicative
    rather than additive, taking the log of the predictors and/or the outcome variable
    can convert the relationship into an additive one, which can be modeled using
    linear regression.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example where we predict the miles per gallon (`mpg`) from
    horsepower (`hp`). We’ll compare the model where we predict `mpg` directly from
    `hp` and another model where we predict the log of `mpg` from `hp`, as shown in
    the following code snippet:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running this code generates *Figure 12**.6*, where we can see a slight curvature
    in the blue line:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Visualizing the original and log-transformed model](img/B18680_12_006.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Visualizing the original and log-transformed model
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Note that the log transformation can only be applied to positive data. In the
    case of `mtcars$mpg`, all values are positive, so we can safely apply the log
    transformation. If the variable included zero or negative values, we would need
    to consider a different transformation or approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The next section focuses on deriving and using the closed-form solution to the
    linear regression model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Working with the closed-form solution
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When developing a linear regression model, the available training set (X, y)
    is given, and the only unknown parameters are the coefficients, β. Here, a bold
    lowercase letter means a vector (such as β and y), and a bold uppercase letter
    denotes a matrix (such as X). It turns out that the closed-form solution to a
    linear regression model can be derived using the concept of the **ordinary least
    squares** (**OLS**) estimator, which aims to minimize the sum of the squared residuals
    in the model. Having the closed-form solution means we can simply plug in the
    required elements (in this case, X and y) and perform the calculation to obtain
    the solution, without resorting to any optimization procedure.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given a data matrix, X (which includes a column of ones for the
    intercept term and is in bold to indicate more than one feature), of dimensions
    n × p (where n is the number of observations and p is the number of predictors)
    and a response vector, y, of length n, the OLS estimator for the coefficient vector,
    β, is given by the following formula:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: β = (X T X) −1 X T y
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: This solution assumes that the term (X T X) is invertible, meaning it should
    be a full-rank matrix. If this is not the case, the solution either does not exist
    or is not unique.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at how to derive this solution. We start with the minimization
    problem for the least squares: minimizing (y − Xβ) T(y − Xβ) over β. This quadratic
    form can be expanded to y T y − β T X T y − y T Xβ + β T X T Xβ. Note that β T
    X T y = y T Xβ since both terms are scalars and therefore are equal to each other
    after the transpose operation. We can write the **residual sum of squares** (**RSS**)
    expression as y T y − 2 β T X T y + β T X T Xβ.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we apply the first-order condition to solve for the value of β that minimizes
    this expression (recall that the point that either minimizes or maximizes a graph
    has a derivative of 0). This means that we would set its first derivative to zero,
    leading to the following formula:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: ∂ ( y T y − 2 β T X T y + β T X T Xβ)  ____________________  ∂ β  = − 2 X T
    y + 2 X T Xβ = 0
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: X T Xβ = X T y
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: β = (X T X) −1 X T y
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have derived the closed-form solution of β that minimizes the sum of
    the squared residuals. Let’s go through an example to see how it can be implemented.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the closed-form solution
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at implementing the OLS estimation in R for an SLR model. An example
    that uses synthetic data is shown in the following code snippet:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we generate 100 observations with a single input feature, where the observation
    is noise-perturbed and follows a process given by y = β 0 + β 1 x + ϵ. The error
    term assumes a normal distribution that’s parameterized by a mean of 0 and a standard
    deviation of 2.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding to the estimation, note that we also appended a column of
    1s on the left of the input feature, `x`, to form a matrix, `X`. This column of
    1s is used to indicate the intercept term and is often referred to as the bias
    trick. That is, the coefficient, β 0, for the intercept term will be part of the
    coefficient vector, and there is no need to create a separate coefficient just
    for the intercept.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the result using the closed-form solution:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, `%*%` is used for matrix multiplication, `t(X)` is the transpose of `X`,
    and `solve()` is used to calculate the inverse of a matrix.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also run the linear regression procedure using the `lm()` function for
    comparison:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The results are the same as the ones that we obtained via the manual approach.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two sections cover two common issues in linear regression settings:
    multicollinearity and heteroskedasticity.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with multicollinearity
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multicollinearity refers to the case when two (or more) predictors are highly
    correlated in a multiple regression model. This means that one independent variable
    can be linearly predicted from the others with a high degree of accuracy. This
    is a situation that we do not want to fall into. In other words, we would like
    to see a high degree of correlation between the predictors and the target variable,
    while a low degree of correlation among these predictors themselves.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In the face of multicollinearity in a linear regression model, the resultant
    model tends to generate unreliable and unstable estimates of the regression coefficients.
    It can inflate the coefficients of the parameters, making them statistically insignificant,
    even though they might be substantively important. In addition, multicollinearity
    makes it difficult to assess the effect of each independent variable on the dependent
    variable as the effects are intertwined. However, it does not affect the predictive
    power or interpretability of the model; instead, it only changes the calculations
    for individual features.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Detecting any potential multicollinearity among the predictors can be performed
    by examining the pair-wise correlation. Alternatively, we can resort to a particular
    test statistic called the **variance inflation factor** (**VIF**), which quantifies
    how much the variance is increased due to multicollinearity. A VIF of 1 indicates
    that two variables are not correlated, while a VIF greater than 5 (in many fields)
    would suggest a problematic amount of multicollinearity.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: When multicollinearity exists in the linear regression model, we could choose
    to keep one predictor only and remove all other highly correlated predictors.
    We can also combine these correlated variables into a few uncorrelated ones via
    **principle component analysis** (**PCA**), a widely used technique for dimension
    reduction. Besides this, we can resort to ridge regression to control the magnitude
    of the coefficients; this will be introduced later in this chapter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'To check multicollinearity using VIF, we can use the `vif()` function from
    the `car` package, as shown in the following code snippet:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Looking at the result, `disp` seems to have high multicollinearity (VIF = 7.32
    > 5), suggesting that it has a strong correlation with `hp` and `wt`. This implies
    that `disp` is not providing much information that is not already contained in
    the other two predictors.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: To handle the multicollinearity here, we can consider removing `disp` from the
    model since it has the highest VIF, applying PCA to combine the three predictors,
    or using ridge or lasso regression (more on this in the last two sections of this
    chapter).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The next section focuses on the issue of heteroskedasticity.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with heteroskedasticity
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heteroskedasticity (or heteroscedasticity) refers to the situation in which
    the variability of the error term, or residuals, is not the same across all levels
    of the independent variables. This violates one of the key assumptions of OLS
    linear regression, which assumes that the residuals have a constant variance –
    in other words, the residuals are homoskedastic. Violating this assumption could
    lead to incorrect inferences on the statistical significance of the coefficients
    since the resulting standard errors of the regression coefficients could be larger
    or smaller than they should be.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: There are a few ways to handle heteroskedasticity. First, we can transform the
    outcome variable using the logarithmic function, as introduced earlier. Other
    functions, such as taking the square root or inverse of the original outcome variable,
    could also help reduce heteroskedasticity. Advanced regression models such as
    **weighted least squares** (**WLS**) or **generalized least squares** (**GLS**)
    may also be explored to reduce the impact of heteroskedasticity.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'To formally test for heteroskedasticity, we can conduct a Breusch-Pagan test
    using the `bptest()` function from the `lmtest` package. In the following code
    snippet, we fit an MLR model to predict `mpg` using `wt` and `hp`, followed by
    performing the Breusch-Pagan test:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since the p-value (`0.6438`) is greater than 0.05, we do not reject the null
    hypothesis of the Breusch-Pagan test. This suggests that there is not enough evidence
    to say that heteroskedasticity is present in the regression model. So, we would
    conclude that the variances of the residuals are not significantly different from
    being constant, or homoskedastic.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The next section shifts to looking at regularized linear regression models and
    ridge and lasso penalties.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Introducing penalized linear regression
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Penalized regression models, such as ridge and lasso, are techniques that are
    used to handle problems such as multicollinearity, reduce overfitting, and even
    perform variable selection, especially when dealing with high-dimensional data
    with multiple input features.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression (also called L2 regularization) is a method that adds a penalty
    equivalent to the square of the magnitude of coefficients. We would add this term
    to the loss function after weighting it by an additional hyperparameter, often
    denoted as λ, to control the strength of the penalty term.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression (L1 regularization), on the other hand, is a method that, similar
    to ridge regression, adds a penalty for non-zero coefficients, but unlike ridge
    regression, it can force some coefficients to be exactly equal to zero when the
    penalty tuning parameter is large enough. The larger the value of the hyperparameter,
    λ, the greater the amount of shrinkage. The penalty on the size of coefficients
    helps reduce model complexity and multicollinearity, leading to a model that can
    generalize better on unseen data. However, ridge regression includes all the features
    in the final model, so it doesn’t induce any sparsity. Therefore, it’s not particularly
    useful for variable selection.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: In summary, ridge and lasso regression are both penalized linear regression
    methods that add a constraint regarding the magnitude of the estimated coefficients
    to the model optimization process, which helps prevent overfitting, manage multicollinearity,
    and reduce model complexity. However, ridge tends to include all predictors in
    the model and helps reduce their effect, while lasso can exclude predictors from
    the model altogether, leading to a simpler and more interpretable model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with ridge regression and look at its loss function more closely.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Working with ridge regression
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ridge regression, also referred to as L2 regularization, is a commonly used
    technique to alleviate overfitting in linear regression models by penalizing the
    magnitude of the estimated coefficients in the resulting model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in an SLR model, we seek to minimize the sum of the squared differences
    between our predicted and actual values, which we refer to as the least squares
    method. The loss function we wish to minimize is the RSS:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: RSS = ∑ i=1 n (y i − (β 0 + ∑ j=1 p β j x ij)) 2
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Here, y i is the actual target value, β 0 is the intercept term, {β j} are the
    coefficient estimates for each predictor, x ij, and the summations are overall
    observations and predictors.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Purely minimizing the RSS would give us an overfitting model, as represented
    by the high magnitude of the resulting coefficients. As a remedy, we could apply
    ridge regression by adding a penalty term to this loss function. This penalty
    term is the sum of the squares of each coefficient multiplied by a tuning parameter,
    λ. So, the ridge regression loss function (also known as the **cost function**)
    is as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: L ridge = RSS + λ∑ j=1 p β j 2
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Here, the λ parameter is a user-defined tuning parameter. A larger λ means a
    higher penalty and a smaller λ means less regularization effect. λ = 0 gives the
    ordinary least squares regression result, while as λ approaches infinity, the
    impact of the penalty term dominates, and the coefficient estimates approach zero.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: By adding this penalty term, ridge regression tends to decrease the size of
    the coefficients, which can help mitigate the problem of multicollinearity (where
    predictors are highly correlated). It does this by spreading the coefficient estimates
    of correlated predictors across each other, which can lead to a more stable and
    interpretable model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that ridge regression does not typically produce
    sparse solutions and does not perform variable selection. In other words, it will
    not result in a model where some coefficients are exactly zero (unless λ is infinite),
    thus all predictors are included in the model. If feature selection is important,
    methods such as lasso (L1 regularization) or elastic net (a combination of L1
    and L2 regularization) might be more appropriate.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Note that we would often penalize the intercept, β 0.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to learn how to develop a ridge regression model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.5 – implementing ridge regression
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement a ridge regression model and compare the
    estimated coefficients with the OLS model. Our implementation will be based on
    the `glmnet` package:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the `glmnet` package:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we use an `if-else` statement to detect if the `glmnet` package is installed.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Store all columns other than `mpg` as predictors in `X` and `mpg` as the target
    variable in `y`:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit a ridge regression model using the `glmnet()` function:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, the `alpha` parameter controls the type of model we fit. `alpha = 0` fits
    a ridge regression model, `alpha = 1` fits a lasso model, and any value in between
    fits an elastic net model.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use cross-validation to choose the best `lambda` value:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, we use the cross-validation approach to identify the optimal `lambda`
    that gives the lowest error on the cross-validation set on average. All repeated
    cross-validation steps are completed via the `cv.glmnet()` function.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit a new ridge regression model using the optimal `lambda` and extract its
    coefficients without the intercept:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit a linear regression model and extract its coefficients:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the coefficients of both models on the same graph:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Running this code generates *Figure 12**.7*:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS
    models](img/B18680_12_007.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS
    models
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: This plot shows that the estimated coefficients from the ridge regression model
    are, in general, smaller than those from the OLS model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The next section focuses on lasso regression.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Working with lasso regression
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lasso regression is another type of regularized linear regression. It is similar
    to ridge regression but differs in terms of the specific process of calculating
    the magnitude of the coefficients. Specifically, it uses the L1 norm of the coefficients,
    which consists of the total sum of absolute values of the coefficients, as the
    penalty that’s added to the OLS loss function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The lasso regression cost function can be written as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: L lasso = RSS + λ∑ j=1 p | β j|
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The key characteristic of lasso regression is that it can reduce some coefficients
    exactly to 0, effectively performing variable selection. This is a consequence
    of the L1 penalty term and is not the case for ridge regression, which can only
    shrink coefficients close to 0\. Therefore, lasso regression is particularly useful
    when we believe that only a subset of the predictors matters when it comes to
    predicting the outcome.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in contrast to ridge regression, which can’t perform variable selection
    and therefore may be less interpretable, lasso regression automatically selects
    the most important features and discards the rest, which can make the final model
    easier to interpret.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Like ridge regression, the lasso regression penalty term is also subject to
    a tuning parameter, λ. The optimal λ parameter is typically chosen via cross-validation
    or a more intelligent search policy such as Bayesian optimization.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to understand how to develop a lasso regression
    model.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.6 – implementing lasso regression
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To implement a lasso regression model, we can follow a similar process as we
    did for the ridge regression model:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'To fit a lasso regression model, set `alpha = 1` in the `glmnet()` function:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use the same cross-validation procedure to identify the optimal value of `lambda`:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Fit a new lasso regression model using the optimal `lambda`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The resulting coefficients can also be extracted using the `coef()` function,
    followed by `[-1]` to remove the intercept term:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the estimated coefficients together with the previous two models:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Running this code generates *Figure 12**.8*, which suggests that only two variables
    are kept in the resultant model. So, the lasso regression model can produce a
    sparse solution by setting the coefficients of some features equal to zero:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso,
    and OLS regression models](img/B18680_12_008.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso,
    and OLS regression models
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the lasso regression model gives us a sparse model by setting the
    coefficients of non-significant variables to zero, thus achieving feature selection
    and model estimation at the same time.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the nuts and bolts of the linear regression model.
    We started by introducing the SLR model, which consists of only one input variable
    and one target variable, and then extended to the MLR model with two or more predictors.
    Both models can be assessed using R 2, or more preferably, the adjusted R 2 metric.
    Next, we discussed specific scenarios, such as working with categorical variables
    and interaction terms, handling nonlinear terms via transformations, working with
    the closed-form solution, and dealing with multicollinearity and heteroskedasticity.
    Lastly, we introduced widely used regularization techniques such as ridge and
    lasso penalties, which can be incorporated into the loss function as a penalty
    term and generate a regularized model, and, additionally, a sparse solution in
    the case of lasso regression.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will cover another type of widely used linear model:
    the logistic regression model.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
