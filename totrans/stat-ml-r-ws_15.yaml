- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Regression in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce linear regression, a fundamental statistical
    approach that’s used to model the relationship between a target variable and multiple
    explanatory (also called independent) variables. We will cover the basics of linear
    regression, starting with simple linear regression and then extending the concepts
    to multiple linear regression. We will learn how to estimate the model coefficients,
    evaluate the goodness of fit, and test the significance of the coefficients using
    hypothesis testing. Additionally, we will discuss the assumptions underlying linear
    regression and explore techniques to address potential issues, such as nonlinearity,
    interaction effect, multicollinearity, and heteroskedasticity. We will also introduce
    two widely used regularization techniques: the ridge and **Least Absolute Shrinkage
    and Selection Operator** (**lasso**) penalties.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will learn the core principles of linear regression,
    its extensions to regularized linear regression, and the implementation details
    involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing penalized linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with ridge regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with lasso regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ggplot2`, 3.4.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tidyr`, 1.2.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dplyr`, 1.0.10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`car`, 3.1.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lmtest`, 0.9.40'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glmnet`, 4.1.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the versions of the packages mentioned in the preceding list
    are the latest ones at the time of writing this chapter. All the code and data
    for this chapter are available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the core of linear regression is the concept of fitting a straight line –
    or more generally, a hyperplane – to the data points. Such fitting aims to minimize
    the deviation between the observed and predicted values. When it comes to simple
    linear regression, one target variable is regressed by one predictor, and the
    goal is to fit a straight line that best mimics the relationship between the two
    variables. For multiple linear regression, there is more than one predictor, and
    the goal is to fit a hyperplane that best describes the relationship among the
    variables. Both tasks can be achieved by minimizing a measure of deviation between
    the predictions and the corresponding targets.
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, obtaining an optimal model means identifying the best
    coefficients that define the relationship between the target variable and the
    input predictors. These coefficients represent the change in the target associated
    with a single unit change in the associated predictor, assuming all other variables
    are constant. This allows us to quantify the magnitude (size of the coefficient)
    and direction (sign of the coefficient) of the relationship between the variables,
    which can be used for inference (highlighting explainability) and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to inference, we often look at the relative impact on the target
    variable given a unit change to the input variable. Examples of such explanatory
    modeling include how marketing spend affects quarterly sales, how smoker status
    affects insurance premiums, and how education affects income. On the other hand,
    predictive modeling focuses on predicting a target quantity. Examples include
    predicting quarterly sales given the marketing spend, predicting the insurance
    premium given a policyholder’s profile information, such as age and gender, and
    predicting income given someone’s education, age, work experience, and industry.
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, the expected outcome is modeled as a weighted sum of all
    the input variables. It also assumes that the change in the output is linearly
    proportional to the change in any input variable. This is the simplest form of
    the regression method.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with **simple linear** **regression** (**SLR**).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding simple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SLR is a powerful and widely used statistical model that specifies the relationship
    between two continuous variables, including one input and one output. It allows
    us to understand how a response variable (also referred to as the dependent or
    target variable) changes as the explanatory variable (also called the independent
    variable or the input variable) varies. By fitting a straight line to the observed
    data, SLR quantifies the strength and direction of the linear association between
    the two variables. This straight line is called the SLR **model**. It enables
    us to make predictions and infer the impact of the predictor on the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in an SLR model, we assume a linear relationship between the
    target variable (y) and the input variable (x). The model can be represented mathematically
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = β 0 + β 1 x + ϵ
  prefs: []
  type: TYPE_NORMAL
- en: Here, y is called the response variable, dependent variable, explained variable,
    predicted variable, target variable, or regressand. x is called the explanatory
    variable, independent variable, control variable, predictor variable, or regressor.
    β 0 is the intercept of the linear line that represents the expected value of
    y when x is 0\. β 1 is the slope that represents the change in y for a one-unit
    increase in x. Finally, ϵ is the random error term that accounts for the variability
    in the target, y, that the predictor, x, cannot explain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main objective of SLR is to estimate the β 0 and β 1 parameters. An optimal
    set of β 0 and β 1 would minimize the total squared deviations between the observed
    target values, y, and the predicted values,  ˆ y , using the model. This is called
    the **least squares method**, where we seek the optimal β 0 and β 1 parameters
    that correspond to the minimum **sum of squared** **error** (**SSR**):'
  prefs: []
  type: TYPE_NORMAL
- en: minSSR = min∑ i=1 n u i 2 = min(y i − ˆ y i) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, each residual, u i, is the difference between the observation, y i, and
    its fitted value,  ˆ y  i. In simple terms, the objective is to locate the straight
    line that is closest to the data points given. *Figure 12**.1* illustrates a collection
    of data points (in blue) and the linear model (in red):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – The SLR model, where the linear model appears as a line and
    is trained by minimizing the SSR](img/B18680_12_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – The SLR model, where the linear model appears as a line and is
    trained by minimizing the SSR
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have estimated the model coefficients, β 0 and β 1, we can use the
    model to make predictions and inferences on the intensity of the linear relationship
    between the variables. Such a linear relationship indicates the goodness of fit,
    which is often measured using the coefficient of determination, or R 2\. R 2 ranges
    from `0` to `1` and quantifies the proportion of the total variation in y that
    can be explained by x. It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: R 2 = 1 −  ∑ i (y i − ˆ y i) 2 _ ∑ i (y i −  _ y ) 2
  prefs: []
  type: TYPE_NORMAL
- en: Here,  _ y  denotes the average value of the observed target variable, y.
  prefs: []
  type: TYPE_NORMAL
- en: Besides this, we can also use hypothesis testing to test the significance of
    the resulting coefficients, β 0 and β 1, thus helping us determine whether the
    observed relationship between the variables is statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an example of building a simple linear model using a simulated
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.1 – building an SLR model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will demonstrate the implementation of an SLR model in
    R. We’ll be using a combination of built-in functions and packages to accomplish
    this task using a simulated dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulate a dataset such that the response variable, `Y`, is linearly dependent
    on the explanatory variable, `X`, with some added noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `runif()` function to generate the independent variable, `X`,
    which is a vector of random uniform numbers. Then, we add some “noise” to the
    dependent variable, `Y`, making the observed data more realistic and less perfectly
    linear. This is achieved using the `rnorm()` function, which creates a vector
    of random normal numbers. The target variable, `Y`, is then created as a function
    of `X`, plus the noise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides this, we use a seed (`set.seed(123)`) at the beginning to ensure reproducibility.
    This means that we will get the same set of random numbers every time we run this
    code. Each run will produce a different list of random numbers if we don’t set
    a seed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this simulation, the true intercept (β 0) is 5, the true slope (β 1) is 0.5,
    and the noise is normally distributed with a mean of 0 and a standard deviation
    of 10.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train a linear regression model based on the simulated dataset using the `lm()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we use the `lm()` function to fit the data, where `lm` stands for “linear
    model.” This function creates our SLR model. The `Y ~ X` syntax is how we specify
    our model: it tells the function that `Y` is being modeled as a function of `X`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `summary()` function provides a comprehensive overview of the model, including
    the estimated coefficients, the standard errors, the t-values, and the p-values,
    among other statistics. Since the resulting p-value is extremely low, we can conclude
    that the input variable is predictive with strong statistical significance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `plot()` and `abline()` functions to visualize the data and the fitted
    regression line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 12**.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Visualizing the data and the fitted regression line](img/B18680_12_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Visualizing the data and the fitted regression line
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `plot()` function creates a scatter plot of our data, and the `abline()`
    function adds the regression line to this plot. Such a visual representation is
    very useful for understanding the quality of the fitting.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll move on to the **multiple linear regression** (**MLR**) model in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing multiple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLR expands the single predictor in SLR to predict the target outcome based
    on multiple predictor variables. Here, the term “multiple” in MLR refers to the
    multiple predictors in the model, where each feature is given a coefficient. A
    specific coefficient, β, represents the change in the outcome variable for a single
    unit change in the associated predictor variable, assuming all other predictors
    are held constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the great advantages of MLR is its ability to include multiple predictors,
    allowing for a more complex and realistic (linear) representation of the real
    world. It can provide a holistic view of the connection between the target and
    all input variables. This is particularly useful in fields where the outcome variable
    is likely influenced by more than one predictor variable. It is modeled via the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: y = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p + ϵ
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have a total of p features, and therefore, (p + 1) coefficients due
    to the intercept term. ϵ is the usual noise term that represents the unexplained
    part. In other words, our prediction using MLR is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  prefs: []
  type: TYPE_NORMAL
- en: We can perform ceteris paribus analysis with this formulation, which is a Latin
    way of saying all other things are equal, and we only change one input variable
    to assess its impact on the outcome variable. In other words, MLR allows us to
    explicitly control (that is, keep unchanged) many other factors that simultaneously
    affect the target variable and observe the impact of only one factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we add a small increment, Δ x j, to the feature, x j,
    and keep all other features unchanged. The new prediction,  ˆ y  new, is obtained
    by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  new = β 0 + β 1 x 1 + … + β j( x j + Δ x j) + … + β p x p
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the original prediction is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  old = β 0 + β 1 x 1 + … + β j x j + … + β p x p
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between these two gives us the change in the output variable:'
  prefs: []
  type: TYPE_NORMAL
- en: Δ ˆ y  =  ˆ y  new −  ˆ y  old = β j Δ x j
  prefs: []
  type: TYPE_NORMAL
- en: What we are doing here is essentially controlling all other input variables
    but only bumping x j to see the impact on the prediction,  ˆ y . So, the coefficient,
    β j, measures the sensitivity of the outcome to a specific feature. When we have
    a unit change, with Δ x j = 1, the change is exactly the coefficient itself, giving
    us Δ ˆ y  = β j.
  prefs: []
  type: TYPE_NORMAL
- en: The next section discusses the measure of the predictiveness of the MLR model.
  prefs: []
  type: TYPE_NORMAL
- en: Seeking a higher coefficient of determination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLR tends to perform better than SLR due to the multiple predictors used in
    the model, such as a higher coefficient of determination (R 2). However, a regression
    model with more input variables and a higher R 2 does not necessarily mean that
    the model is a better fit and can predict better for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: A higher R 2, as a result of more input features, could likely be due to overfitting.
    Overfitting occurs when a model is excessively complex, including too many predictors
    or even interaction terms between predictors. In such cases, the model may fit
    the observed data well (thus leading to a high R 2), but it may perform poorly
    when applied to new, unseen test data. This is because the model might have learned
    not only the underlying structure of the training data but also the random noise
    specific to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the metric of R 2 more closely. While R 2 measures how well the
    model explains the variance in the outcome variable, it has a major limitation:
    it tends to get bigger as more predictors enter the model, even if those predictors
    are irrelevant. As a remedy, we can use the adjusted R 2\. Unlike R 2, the adjusted
    R 2 explicitly considers the number of predictors and adjusts the resulting statistic
    accordingly. If a predictor improves the model substantially, the adjusted R 2
    will increase, but if a predictor does not improve the model by a significant
    amount, the adjusted R 2 may even decrease.'
  prefs: []
  type: TYPE_NORMAL
- en: When building statistical models, simpler models are usually preferred when
    they perform similarly to more complex models. This principle of parsimony, also
    known as Occam’s razor, suggests that among models with similar predictive power,
    the simplest one should be chosen. In other words, adding more predictors to the
    model makes it more complex, harder to interpret, and more likely to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: More on adjusted R 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The adjusted R 2 improves upon R 2 by adjusting for the number of features in
    the selected model. Specifically, the value of the adjusted R 2 only increases
    if adding this feature is worth more than what would have been expected from adding
    a random feature. Essentially, the additional predictors that are added to the
    model must be meaningful and predictive to lead to a higher adjusted R 2\. These
    additional predictors, however, would always increase R 2 when added to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The adjusted R 2 addresses this issue by incorporating the model’s degree of
    freedom. Here, the degree of freedom refers to the number of values in a statistical
    calculation that is free to vary. In the context of regression models, this typically
    means the number of predictors. The adjusted R 2 can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted R 2 = 1 − (1 − R 2)  (n − 1) _ n − p − 1
  prefs: []
  type: TYPE_NORMAL
- en: Here, n denotes the number of observations and p represents the number of features
    in the model.
  prefs: []
  type: TYPE_NORMAL
- en: The formula works by adjusting the scale of R 2 based on the count of observations
    and predictors. The term  (n − 1) _ n − p − 1 is a ratio that reflects the degrees
    of freedom in the model, where (n − 1) represents the total degrees of freedom
    in the model. We subtract by 1 because we are estimating the mean of the dependent
    variable from the data. (n − p − 1) represents the degrees of freedom for the
    error, which, in turn, represents the number of observations left over after estimating
    the model parameters. The whole term, (1 − R 2)  (n − 1) _ n − p − 1, denotes
    the error variance that’s been adjusted for the count of predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting this from 1 results in the proportion of the total variance explained
    by the model, after adjusting for the number of predictors in the model. In other
    words, it’s a version of R 2 that penalizes the addition of unnecessary predictors.
    This helps to prevent overfitting and makes adjusted R 2 a more balanced measure
    of a model’s explanatory power when comparing models with different numbers of
    predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to develop an MLR model in R.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an MLR model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will develop an MLR model using the same `lm()` function
    in R based on the `mtcars` dataset, which comes preloaded with R and was used
    in previous exercises. Again, the `mtcars` dataset contains measurements for 32
    vehicles from a 1974 Motor Trend issue. These measurements include attributes
    such as miles per gallon (`mpg`), number of cylinders (`cyl`), horsepower (`hp`),
    and weight (`wt`).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.2 – building an MLR model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will develop an MLR model to predict `mpg` using `cyl`,
    `hp`, and `wt`. We will then interpret the model results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `mtcars` dataset and build an MLR that predicts `mpg` based on `cyl`,
    `hp`, and `wt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first load the `mtcars` dataset using the `data()` function, then construct
    the MLR model using the `lm()` function. The `mpg ~ cyl + hp + wt` formula is
    used to specify the model. This formula tells R that we want to model `mpg` as
    a function of `cyl`, `hp`, and `wt`. The `data = mtcars` argument tells R to look
    for these variables in the `mtcars` dataset. The `lm()` function fits the model
    to the data and returns a model object, which we store in the variable model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'View the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The summary includes the model’s coefficients (the intercept and the slopes
    for each predictor), the residuals (differences between the actual observations
    and predicted values for the target), and several statistics that tell us how
    well the model fits the data, including R 2 and the adjusted R 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s interpret the output. Each coefficient represents the expected change
    in `mpg` for a single unit increase in the associated predictor, assuming all
    other predictors are constant. The R 2 value, which is `0.8431`, denotes the proportion
    of variance (over 84%) in `mpg` that can be explained by the predictors together.
    Again, the adjusted R 2 value, which is `0.8263`, is a modified R 2 that accounts
    for the number of features in the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition, the p-values for each predictor test the null hypothesis that the
    true value of the coefficient is zero. If a predictor’s p-value is smaller than
    a preset significance level (such as 0.05), we would reject this null hypothesis
    and conclude that the predictor is statistically significant. In this case, `wt`
    is the only statistically significant factor compared with others using a significance
    level of 5%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the MLR model, all coefficients are negative, indicating a reverse direction
    of travel between the input variable and the target. However, we cannot conclude
    that all the predictors negatively correlate with the target variable. The correlation
    between the individual predictor and the target variable could be positive or
    negative in SLR.
  prefs: []
  type: TYPE_NORMAL
- en: The next section provides more context on this phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Simpson’s Paradox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simpson’s Paradox says that a trend appears in different data groups but disappears
    or changes when combined. In the context of regression analysis, Simpson’s Paradox
    can appear when a variable that seems positively correlated with the outcome might
    be negatively correlated when we control other variables.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this paradox illustrates the importance of considering confounding
    variables and not drawing conclusions from aggregated data without understanding
    the context. The confounding variables are those not among the explanatory variables
    under consideration but are related to both the target variable and the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a simple example through the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.3 – illustrating Simpson’s Paradox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at two scenarios with opposite signs of coefficient
    values for the same feature in both SLR and MLR:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a dummy dataset with two predictors and one output variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `x1` is a set of 100 numbers randomly generated from a standard normal
    distribution. `x2` is a linear function of `x1` but with a negative correlation,
    and some random noise is added (via `rnorm(100)`). `y` is then generated as a
    linear function of `x1` and `x2`, again with some random noise added. All three
    variables are compiled into a DataFrame, `df`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train an SLR model with `y` as the outcome and `x1` as the input features.
    Check the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that `x1` is negatively correlated with `y` due to a negative
    coefficient of `-2.1869`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train an SLR model with `y` as the target and `x1` and `x2` as the input features.
    Check the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the estimated coefficient for `x1` is now a positive quantity.
    Does this suggest that `x1` is suddenly positively correlated with `y`? No, since
    there are likely other confounding variables that lead to a positive coefficient.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The key takeaway is that we can only make inferences on the (positive or negative)
    correlation between a predictor and a target outcome in an SLR setting. For example,
    if we build an SLR model to regress `y` against `x`, we can conclude that `x`
    and `y` are positively correlated if the resulting coefficient is positive (β
    > 0). Similarly, if β > 0, we can conclude that `x` and `y` are positively correlated.
    The same applies to the case of negative correlation.
  prefs: []
  type: TYPE_NORMAL
- en: However, such inference breaks in an MLR setting – that is, we cannot conclude
    a positive correlation if β > 0, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take this opportunity to interpret the results. The `Estimate` column
    shows the estimated regression coefficients. These values indicate how much the
    `y` variable is expected to increase when the corresponding predictor variable
    increases by one unit while holding all other features constant. In this case,
    for each unit increase in `x1`, `y` is expected to increase by approximately `0.93826`
    units, and for each unit increase in `x2`, `y` is expected to increase by approximately
    `1.02381` units. The `(Intercept)` row shows the estimated value of `y` when all
    predictor variables in the model are zero. In this model, the estimated intercept
    is `2.13507`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Std. Error` represents the standard errors for the estimates. Smaller values
    here indicate more precise estimates. The `t value` column shows the t-statistics
    for the hypothesis test that the corresponding population regression coefficient
    is zero, given that the other predictors are in the model. A larger absolute value
    of the t-statistic indicates stronger evidence against the null hypothesis. The
    `Pr(>|t|)` column gives the p-values for the hypothesis tests. In this case, both
    `x1` and `x2` have p-values below `0.05`, indicating that both are statistically
    significant predictors of `y` at the 5% significance level.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the multiple R-squared and adjusted R-squared values provide measures
    of how well the model fits the data. The multiple R-squared value is `0.8484`,
    indicating that this model explains approximately 84.84% of the variability in
    `y`. The adjusted R-squared value adjusts this measure for the number of features
    in the model. As discussed, it is a better measure when comparing models with
    different numbers of predictors. Here, the adjusted R-squared value is `0.8453`.
    The F-statistic and its associated p-value are used to test the hypothesis that
    all population regression coefficients are zero. A small p-value (less than `0.05`)
    indicates that we can reject this hypothesis, and conclude that at least one of
    the predictors is useful in predicting `y`.
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at the situation when we have a categorical predictor
    in the MLR model.
  prefs: []
  type: TYPE_NORMAL
- en: Working with categorical variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In MLR, the process of including a binary predictor is similar to including
    a numeric predictor. However, the interpretation differs. Consider a dataset where
    y is the target variable, x 1 is a numeric predictor, and x 2 is a binary predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: y = β 0 + β 1 x 1 + β 2 x 2 + ϵ
  prefs: []
  type: TYPE_NORMAL
- en: In this model, x 2 is coded as 0 and 1, and its corresponding coefficient, β 2,
    represents the difference in the mean values of y between the two groups identified
    by x 2.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if x 2 is a binary variable representing sex (0 for males, 1 for
    females), and y is salary, then β 2 represents the average difference in salary
    between females and males, after accounting for the value of x 1.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the interpretation of the coefficient of a binary predictor is dependent
    on the other variables in the model. So, in the preceding example, β 2 is the
    difference in salary between females and males for given values of x 1.
  prefs: []
  type: TYPE_NORMAL
- en: On the implementation side, R automatically creates dummy variables when a factor
    is used in a regression model. So, if x 2 were a factor with levels of “male”
    and “female,” R would handle the conversion to 0 and 1 internally when fitting
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a concrete example. In the following code, we’re building an
    MLR model to predict `mpg` using `qsec` and `am`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `am` variable is treated as a numeric variable. Since it represents
    the type of transmission in the car (`0` = automatic, `1` = manual), it should
    have been treated as a categorical variable. This can be achieved by converting
    it into a factor, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that only one variable, `am_cat1`, is created for the categorical variable,
    `am_cat`. This is because `am_cat` is binary, thus we only need one dummy column
    (keeping only `am_cat1` and removing `am_cat0` in this case) to represent the
    original categorical variable. In general, for a categorical variable with k categorical,
    R will automatically create (k − 1) dummy variables in the model.
  prefs: []
  type: TYPE_NORMAL
- en: This process is called `am` was equal to the corresponding level, and 0 otherwise.
    This essentially creates a set of indicators that capture the presence or absence
    of each category. Finally, since we can infer the last dummy variable based on
    the values of the previous (`k-1`) dummy variables, we can remove the last dummy
    variable in the resulting one-hot encoded set of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a categorical variable introduces a vertical shift to the model estimate,
    as discussed in the following section. To see this, let’s look more closely at
    the impact of the categorical variable, `am_cat1`. Our MLR model now assumes the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x qsec + β 2 x am_cat
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that x am_cat is a binary variable. When x am_cat = 0, the prediction
    becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x qsec
  prefs: []
  type: TYPE_NORMAL
- en: 'When x am_cat = 1, the prediction is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = β 0 + β 1 x qsec + β 2 = (β 0 + β 2) + β 1 x qsec
  prefs: []
  type: TYPE_NORMAL
- en: By comparing these two quantities, we can see that they are two linear models
    parallel to each other since the slope is the same and the only difference is
    β 2 in the intercept term.
  prefs: []
  type: TYPE_NORMAL
- en: 'A visual illustration helps here. In the following code snippet, we first create
    a new DataFrame, `newdata`, that covers the range of `qsec` values in the original
    data, for each of the `am_cat` values (0 and 1). Then, we use the `predict()`
    function to get the predicted `mpg` values from the model for this new data. Next,
    we plot the original data points with `geom_point()` and add two regression lines
    with `geom_line()`, where the lines are based on the predicted values in `newdata`.
    The `color = am_cat` aesthetic setting adds different colors to the points and
    lines for the different `am_cat` values, and the labels are adjusted in `scale_color_discrete()`
    so that 0 corresponds to “Automatic” and 1 corresponds to “Manual”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code generates *Figure 12**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Visualizing the two linear regression models based on different
    transmission types. These two lines are parallel to each other due to a shift
    in the intercept term](img/B18680_12_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Visualizing the two linear regression models based on different
    transmission types. These two lines are parallel to each other due to a shift
    in the intercept term
  prefs: []
  type: TYPE_NORMAL
- en: What this figure suggests is that manual transmission cars have the same miles
    per gallon (`mpg`) more than automatic transmission cars given the same quarter-mile
    time (`qsec`). However, this is unlikely in practice since different car types
    (manual versus automatic) will likely have different quarter-mile times. In other
    words, there is an interaction effect between these two variables.
  prefs: []
  type: TYPE_NORMAL
- en: The following section introduces the interaction term as a remedy to this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the interaction term
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In regression analysis, an interaction occurs when the effect of one predictor
    on the target variable differs depending on the level of another predictor variable.
    In our running example, we are essentially looking at whether the relationship
    between `mpg` and `qsec` is different for different values of `am`. In other words,
    we are testing whether the slope of the line relating `mpg` and `qsec` differs
    for manual (`am`=1) and automatic (`am`=0) transmissions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if there is no interaction effect, then the impact of `qsec` on
    `mpg` is the same, regardless of whether the car has a manual or automatic transmission.
    This would mean that the lines depicting the relationship between `mpg` and `qsec`
    for manual and automatic cars would be parallel.
  prefs: []
  type: TYPE_NORMAL
- en: If there is an interaction effect, then the effect of `qsec` on `mpg` differs
    for manual and automatic cars. This would mean that the lines depicting the relationship
    between `mpg` and `qsec` for manual and automatic cars would not be parallel.
    They could either cross or, more commonly, just have different slopes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To depict these differences in relationships, we can add an interaction term
    to the model, which is done using the `*` operator. For example, the formula for
    a regression model with an interaction between `qsec` and `am_cat` would be `mpg
    ~ qsec * am_cat`. This is equivalent to `mpg ~ qsec + am``_cat + qsec:am_cat`,
    where `qsec:am_cat` represents the interaction term. The following code shows
    the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also plot the updated model, which consists of two intersecting lines
    due to the interaction effect. In the following code snippet, `geom_smooth(method
    =""l"", se = FALSE)` is used to fit different linear lines to each group of points
    (automatic and manual cars). `as.factor(am_cat)` is used to treat `am_cat` as
    a factor (categorical) variable so that a separate line is fit for each category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code generates *Figure 12**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Two intersecting lines due to the intersection term between
    quarter-mile time and transmission type](img/B18680_12_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Two intersecting lines due to the intersection term between quarter-mile
    time and transmission type
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section focuses on another related topic: nonlinear terms.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling nonlinear terms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression is a widely used model for understanding the linear relationships
    between a response and explanatory variables. However, not all underlying relationships
    in the data are linear. In many situations, a feature and a response variable
    may not have a straight-line relationship, thus necessitating the handling of
    nonlinear terms in the linear regression model to increase its flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to incorporate nonlinearity, and therefore build a curve instead
    of a straight line, is by including polynomial terms of predictors in the regression
    model. In polynomial regression, some or all predictors are raised to a specific
    polynomial term – for example, transforming a feature, x, into x 2 or x 3.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to understand the impact of adding polynomial features
    to a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.4 – adding polynomial features to a linear regression model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will create a simple dataset where the relationship between
    the input feature, x, and the target variable, y, is not linear, but quadratic.
    First, we will fit an SLR model, then add a quadratic term and compare the results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a sequence of `x` values ranging from -10 to 10\. For each `x`, compute
    the corresponding `y` as the square of `x`, plus some random noise to show a (noisy)
    quadratic relationship. Put `x` and `y` in a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result suggests a not-so-good model fitting to the data, which possesses
    a nonlinear relationship.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit a quadratic model to the data by including x 2 as a predictor using the
    `I()` function. Print the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the polynomial model fits the data better than the simple
    linear model. Thus, adding nonlinear terms can improve model fit when the relationship
    between predictors and the response is not strictly linear.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the linear and quadratic models together with the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first calculate the predicted values for both models and add them to
    the DataFrame. Then, we create a scatter plot of the data and add two lines representing
    the predicted values from the linear model (in blue) and the quadratic model (in
    red).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Running this code generates *Figure 12**.5*. The result suggests that adding
    a polynomial feature could extend the flexibility of a linear model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear
    data](img/B18680_12_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear data
  prefs: []
  type: TYPE_NORMAL
- en: Other common ways to introduce nonlinearity include the logarithmic transformation
    (logx) or a square root transformation (√ _ x ). These transformations can also
    be applied to the target variable, y, and we can have multiple transformed features
    in the same linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the model with transformed features remains a linear model. If there
    is a nonlinear transformation to the coefficients, the model would be a nonlinear
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section sheds more light on a widely used type of transformation:
    the logarithmic transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: More on the logarithmic transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logarithmic transformation, or log transformation, maps an input to the
    corresponding output based on the logarithmic function, giving y = logx. A popular
    reason behind using such a transformation is to introduce nonlinearity in the
    linear regression model. When the relationship between the input features and
    the target output is nonlinear, applying a transformation can sometimes linearize
    the relationship, making it possible to model the relationship with a linear regression
    model. For the logarithmic transformation, it can help when the rate of change
    in the outcome variable increases or decreases as the value of the predictor increases.
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, the rate of change decreases as the input becomes more extreme.
    The natural consequence of such a transformation is that potential outliers in
    the input data are squeezed so that they appear less extreme in the transformed
    column. In other words, the resulting linear regression model will be less sensitive
    to the original outliers due to the log transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Another side benefit of using log transformation is its ability to deal with
    heteroscedasticity. Heteroscedasticity is when the variability of the error term
    in a regression model is not constant across all levels of the predictors. This
    violates one of the assumptions of linear regression models and can lead to inefficient
    and biased estimates. In this case, log transformations can stabilize the variance
    of the error term by shrinking the potential big error terms, making it more constant
    across different levels of the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, when the relationship between predictors and the outcome is multiplicative
    rather than additive, taking the log of the predictors and/or the outcome variable
    can convert the relationship into an additive one, which can be modeled using
    linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example where we predict the miles per gallon (`mpg`) from
    horsepower (`hp`). We’ll compare the model where we predict `mpg` directly from
    `hp` and another model where we predict the log of `mpg` from `hp`, as shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code generates *Figure 12**.6*, where we can see a slight curvature
    in the blue line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Visualizing the original and log-transformed model](img/B18680_12_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Visualizing the original and log-transformed model
  prefs: []
  type: TYPE_NORMAL
- en: Note that the log transformation can only be applied to positive data. In the
    case of `mtcars$mpg`, all values are positive, so we can safely apply the log
    transformation. If the variable included zero or negative values, we would need
    to consider a different transformation or approach.
  prefs: []
  type: TYPE_NORMAL
- en: The next section focuses on deriving and using the closed-form solution to the
    linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the closed-form solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When developing a linear regression model, the available training set (X, y)
    is given, and the only unknown parameters are the coefficients, β. Here, a bold
    lowercase letter means a vector (such as β and y), and a bold uppercase letter
    denotes a matrix (such as X). It turns out that the closed-form solution to a
    linear regression model can be derived using the concept of the **ordinary least
    squares** (**OLS**) estimator, which aims to minimize the sum of the squared residuals
    in the model. Having the closed-form solution means we can simply plug in the
    required elements (in this case, X and y) and perform the calculation to obtain
    the solution, without resorting to any optimization procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given a data matrix, X (which includes a column of ones for the
    intercept term and is in bold to indicate more than one feature), of dimensions
    n × p (where n is the number of observations and p is the number of predictors)
    and a response vector, y, of length n, the OLS estimator for the coefficient vector,
    β, is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: β = (X T X) −1 X T y
  prefs: []
  type: TYPE_NORMAL
- en: This solution assumes that the term (X T X) is invertible, meaning it should
    be a full-rank matrix. If this is not the case, the solution either does not exist
    or is not unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at how to derive this solution. We start with the minimization
    problem for the least squares: minimizing (y − Xβ) T(y − Xβ) over β. This quadratic
    form can be expanded to y T y − β T X T y − y T Xβ + β T X T Xβ. Note that β T
    X T y = y T Xβ since both terms are scalars and therefore are equal to each other
    after the transpose operation. We can write the **residual sum of squares** (**RSS**)
    expression as y T y − 2 β T X T y + β T X T Xβ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we apply the first-order condition to solve for the value of β that minimizes
    this expression (recall that the point that either minimizes or maximizes a graph
    has a derivative of 0). This means that we would set its first derivative to zero,
    leading to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: ∂ ( y T y − 2 β T X T y + β T X T Xβ)  ____________________  ∂ β  = − 2 X T
    y + 2 X T Xβ = 0
  prefs: []
  type: TYPE_NORMAL
- en: X T Xβ = X T y
  prefs: []
  type: TYPE_NORMAL
- en: β = (X T X) −1 X T y
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have derived the closed-form solution of β that minimizes the sum of
    the squared residuals. Let’s go through an example to see how it can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the closed-form solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at implementing the OLS estimation in R for an SLR model. An example
    that uses synthetic data is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we generate 100 observations with a single input feature, where the observation
    is noise-perturbed and follows a process given by y = β 0 + β 1 x + ϵ. The error
    term assumes a normal distribution that’s parameterized by a mean of 0 and a standard
    deviation of 2.
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding to the estimation, note that we also appended a column of
    1s on the left of the input feature, `x`, to form a matrix, `X`. This column of
    1s is used to indicate the intercept term and is often referred to as the bias
    trick. That is, the coefficient, β 0, for the intercept term will be part of the
    coefficient vector, and there is no need to create a separate coefficient just
    for the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the result using the closed-form solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, `%*%` is used for matrix multiplication, `t(X)` is the transpose of `X`,
    and `solve()` is used to calculate the inverse of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also run the linear regression procedure using the `lm()` function for
    comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The results are the same as the ones that we obtained via the manual approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two sections cover two common issues in linear regression settings:
    multicollinearity and heteroskedasticity.'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with multicollinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multicollinearity refers to the case when two (or more) predictors are highly
    correlated in a multiple regression model. This means that one independent variable
    can be linearly predicted from the others with a high degree of accuracy. This
    is a situation that we do not want to fall into. In other words, we would like
    to see a high degree of correlation between the predictors and the target variable,
    while a low degree of correlation among these predictors themselves.
  prefs: []
  type: TYPE_NORMAL
- en: In the face of multicollinearity in a linear regression model, the resultant
    model tends to generate unreliable and unstable estimates of the regression coefficients.
    It can inflate the coefficients of the parameters, making them statistically insignificant,
    even though they might be substantively important. In addition, multicollinearity
    makes it difficult to assess the effect of each independent variable on the dependent
    variable as the effects are intertwined. However, it does not affect the predictive
    power or interpretability of the model; instead, it only changes the calculations
    for individual features.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting any potential multicollinearity among the predictors can be performed
    by examining the pair-wise correlation. Alternatively, we can resort to a particular
    test statistic called the **variance inflation factor** (**VIF**), which quantifies
    how much the variance is increased due to multicollinearity. A VIF of 1 indicates
    that two variables are not correlated, while a VIF greater than 5 (in many fields)
    would suggest a problematic amount of multicollinearity.
  prefs: []
  type: TYPE_NORMAL
- en: When multicollinearity exists in the linear regression model, we could choose
    to keep one predictor only and remove all other highly correlated predictors.
    We can also combine these correlated variables into a few uncorrelated ones via
    **principle component analysis** (**PCA**), a widely used technique for dimension
    reduction. Besides this, we can resort to ridge regression to control the magnitude
    of the coefficients; this will be introduced later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check multicollinearity using VIF, we can use the `vif()` function from
    the `car` package, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the result, `disp` seems to have high multicollinearity (VIF = 7.32
    > 5), suggesting that it has a strong correlation with `hp` and `wt`. This implies
    that `disp` is not providing much information that is not already contained in
    the other two predictors.
  prefs: []
  type: TYPE_NORMAL
- en: To handle the multicollinearity here, we can consider removing `disp` from the
    model since it has the highest VIF, applying PCA to combine the three predictors,
    or using ridge or lasso regression (more on this in the last two sections of this
    chapter).
  prefs: []
  type: TYPE_NORMAL
- en: The next section focuses on the issue of heteroskedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with heteroskedasticity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heteroskedasticity (or heteroscedasticity) refers to the situation in which
    the variability of the error term, or residuals, is not the same across all levels
    of the independent variables. This violates one of the key assumptions of OLS
    linear regression, which assumes that the residuals have a constant variance –
    in other words, the residuals are homoskedastic. Violating this assumption could
    lead to incorrect inferences on the statistical significance of the coefficients
    since the resulting standard errors of the regression coefficients could be larger
    or smaller than they should be.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few ways to handle heteroskedasticity. First, we can transform the
    outcome variable using the logarithmic function, as introduced earlier. Other
    functions, such as taking the square root or inverse of the original outcome variable,
    could also help reduce heteroskedasticity. Advanced regression models such as
    **weighted least squares** (**WLS**) or **generalized least squares** (**GLS**)
    may also be explored to reduce the impact of heteroskedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: 'To formally test for heteroskedasticity, we can conduct a Breusch-Pagan test
    using the `bptest()` function from the `lmtest` package. In the following code
    snippet, we fit an MLR model to predict `mpg` using `wt` and `hp`, followed by
    performing the Breusch-Pagan test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Since the p-value (`0.6438`) is greater than 0.05, we do not reject the null
    hypothesis of the Breusch-Pagan test. This suggests that there is not enough evidence
    to say that heteroskedasticity is present in the regression model. So, we would
    conclude that the variances of the residuals are not significantly different from
    being constant, or homoskedastic.
  prefs: []
  type: TYPE_NORMAL
- en: The next section shifts to looking at regularized linear regression models and
    ridge and lasso penalties.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing penalized linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Penalized regression models, such as ridge and lasso, are techniques that are
    used to handle problems such as multicollinearity, reduce overfitting, and even
    perform variable selection, especially when dealing with high-dimensional data
    with multiple input features.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression (also called L2 regularization) is a method that adds a penalty
    equivalent to the square of the magnitude of coefficients. We would add this term
    to the loss function after weighting it by an additional hyperparameter, often
    denoted as λ, to control the strength of the penalty term.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression (L1 regularization), on the other hand, is a method that, similar
    to ridge regression, adds a penalty for non-zero coefficients, but unlike ridge
    regression, it can force some coefficients to be exactly equal to zero when the
    penalty tuning parameter is large enough. The larger the value of the hyperparameter,
    λ, the greater the amount of shrinkage. The penalty on the size of coefficients
    helps reduce model complexity and multicollinearity, leading to a model that can
    generalize better on unseen data. However, ridge regression includes all the features
    in the final model, so it doesn’t induce any sparsity. Therefore, it’s not particularly
    useful for variable selection.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, ridge and lasso regression are both penalized linear regression
    methods that add a constraint regarding the magnitude of the estimated coefficients
    to the model optimization process, which helps prevent overfitting, manage multicollinearity,
    and reduce model complexity. However, ridge tends to include all predictors in
    the model and helps reduce their effect, while lasso can exclude predictors from
    the model altogether, leading to a simpler and more interpretable model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with ridge regression and look at its loss function more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Working with ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ridge regression, also referred to as L2 regularization, is a commonly used
    technique to alleviate overfitting in linear regression models by penalizing the
    magnitude of the estimated coefficients in the resulting model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in an SLR model, we seek to minimize the sum of the squared differences
    between our predicted and actual values, which we refer to as the least squares
    method. The loss function we wish to minimize is the RSS:'
  prefs: []
  type: TYPE_NORMAL
- en: RSS = ∑ i=1 n (y i − (β 0 + ∑ j=1 p β j x ij)) 2
  prefs: []
  type: TYPE_NORMAL
- en: Here, y i is the actual target value, β 0 is the intercept term, {β j} are the
    coefficient estimates for each predictor, x ij, and the summations are overall
    observations and predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Purely minimizing the RSS would give us an overfitting model, as represented
    by the high magnitude of the resulting coefficients. As a remedy, we could apply
    ridge regression by adding a penalty term to this loss function. This penalty
    term is the sum of the squares of each coefficient multiplied by a tuning parameter,
    λ. So, the ridge regression loss function (also known as the **cost function**)
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: L ridge = RSS + λ∑ j=1 p β j 2
  prefs: []
  type: TYPE_NORMAL
- en: Here, the λ parameter is a user-defined tuning parameter. A larger λ means a
    higher penalty and a smaller λ means less regularization effect. λ = 0 gives the
    ordinary least squares regression result, while as λ approaches infinity, the
    impact of the penalty term dominates, and the coefficient estimates approach zero.
  prefs: []
  type: TYPE_NORMAL
- en: By adding this penalty term, ridge regression tends to decrease the size of
    the coefficients, which can help mitigate the problem of multicollinearity (where
    predictors are highly correlated). It does this by spreading the coefficient estimates
    of correlated predictors across each other, which can lead to a more stable and
    interpretable model.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that ridge regression does not typically produce
    sparse solutions and does not perform variable selection. In other words, it will
    not result in a model where some coefficients are exactly zero (unless λ is infinite),
    thus all predictors are included in the model. If feature selection is important,
    methods such as lasso (L1 regularization) or elastic net (a combination of L1
    and L2 regularization) might be more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we would often penalize the intercept, β 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to learn how to develop a ridge regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.5 – implementing ridge regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement a ridge regression model and compare the
    estimated coefficients with the OLS model. Our implementation will be based on
    the `glmnet` package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the `glmnet` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use an `if-else` statement to detect if the `glmnet` package is installed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Store all columns other than `mpg` as predictors in `X` and `mpg` as the target
    variable in `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit a ridge regression model using the `glmnet()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `alpha` parameter controls the type of model we fit. `alpha = 0` fits
    a ridge regression model, `alpha = 1` fits a lasso model, and any value in between
    fits an elastic net model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use cross-validation to choose the best `lambda` value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the cross-validation approach to identify the optimal `lambda`
    that gives the lowest error on the cross-validation set on average. All repeated
    cross-validation steps are completed via the `cv.glmnet()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit a new ridge regression model using the optimal `lambda` and extract its
    coefficients without the intercept:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit a linear regression model and extract its coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the coefficients of both models on the same graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 12**.7*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS
    models](img/B18680_12_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS
    models
  prefs: []
  type: TYPE_NORMAL
- en: This plot shows that the estimated coefficients from the ridge regression model
    are, in general, smaller than those from the OLS model.
  prefs: []
  type: TYPE_NORMAL
- en: The next section focuses on lasso regression.
  prefs: []
  type: TYPE_NORMAL
- en: Working with lasso regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lasso regression is another type of regularized linear regression. It is similar
    to ridge regression but differs in terms of the specific process of calculating
    the magnitude of the coefficients. Specifically, it uses the L1 norm of the coefficients,
    which consists of the total sum of absolute values of the coefficients, as the
    penalty that’s added to the OLS loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lasso regression cost function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: L lasso = RSS + λ∑ j=1 p | β j|
  prefs: []
  type: TYPE_NORMAL
- en: The key characteristic of lasso regression is that it can reduce some coefficients
    exactly to 0, effectively performing variable selection. This is a consequence
    of the L1 penalty term and is not the case for ridge regression, which can only
    shrink coefficients close to 0\. Therefore, lasso regression is particularly useful
    when we believe that only a subset of the predictors matters when it comes to
    predicting the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in contrast to ridge regression, which can’t perform variable selection
    and therefore may be less interpretable, lasso regression automatically selects
    the most important features and discards the rest, which can make the final model
    easier to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Like ridge regression, the lasso regression penalty term is also subject to
    a tuning parameter, λ. The optimal λ parameter is typically chosen via cross-validation
    or a more intelligent search policy such as Bayesian optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise to understand how to develop a lasso regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.6 – implementing lasso regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To implement a lasso regression model, we can follow a similar process as we
    did for the ridge regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To fit a lasso regression model, set `alpha = 1` in the `glmnet()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the same cross-validation procedure to identify the optimal value of `lambda`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit a new lasso regression model using the optimal `lambda`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting coefficients can also be extracted using the `coef()` function,
    followed by `[-1]` to remove the intercept term:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the estimated coefficients together with the previous two models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 12**.8*, which suggests that only two variables
    are kept in the resultant model. So, the lasso regression model can produce a
    sparse solution by setting the coefficients of some features equal to zero:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso,
    and OLS regression models](img/B18680_12_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso,
    and OLS regression models
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the lasso regression model gives us a sparse model by setting the
    coefficients of non-significant variables to zero, thus achieving feature selection
    and model estimation at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the nuts and bolts of the linear regression model.
    We started by introducing the SLR model, which consists of only one input variable
    and one target variable, and then extended to the MLR model with two or more predictors.
    Both models can be assessed using R 2, or more preferably, the adjusted R 2 metric.
    Next, we discussed specific scenarios, such as working with categorical variables
    and interaction terms, handling nonlinear terms via transformations, working with
    the closed-form solution, and dealing with multicollinearity and heteroskedasticity.
    Lastly, we introduced widely used regularization techniques such as ridge and
    lasso penalties, which can be incorporated into the loss function as a penalty
    term and generate a regularized model, and, additionally, a sparse solution in
    the case of lasso regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will cover another type of widely used linear model:
    the logistic regression model.'
  prefs: []
  type: TYPE_NORMAL
