- en: Working with PyCUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA进行工作
- en: In the last chapter, we saw the procedure to install PyCUDA for Windows and
    Linux operating systems. In this chapter, we will start by developing the first
    PyCUDA program that displays a string on the console. It is very important to
    know and access the device properties of the GPU on which PyCUDA is running; the
    method for doing this will be discussed in detail in this chapter. We will also
    look at the execution of threads and blocks for a kernel in PyCUDA. The important
    programming concepts for any CUDA programming, such as allocating and deallocating
    the memory on the device, transferring data from host to device and vice versa,
    and the kernel call will be discussed in detail, using an example of the vector
    addition program. The method to measure the performance of PyCUDA programs using
    CUDA events and to compare it with the CPU program will also be discussed. These
    programming concepts will be used to develop some complex PyCUDA programs, such
    as the squaring of elements in an array and matrix multiplication. The last part
    of the chapter describes some advanced methods to define kernel functions in PyCUDA.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了为Windows和Linux操作系统安装PyCUDA的步骤。在本章中，我们将首先开发第一个在控制台上显示字符串的PyCUDA程序。了解和访问PyCUDA运行的GPU的设备属性非常重要；这一方法将在本章中详细讨论。我们还将查看PyCUDA中内核的线程和块执行。任何CUDA编程的重要编程概念，如分配和释放设备上的内存、从主机到设备以及相反的数据传输，以及内核调用，将使用向量加法程序示例进行详细讨论。还将讨论使用CUDA事件测量PyCUDA程序性能的方法，并将其与CPU程序进行比较。这些编程概念将被用于开发一些复杂的PyCUDA程序，例如数组元素的平方和矩阵乘法。本章的最后部分描述了在PyCUDA中定义内核函数的一些高级方法。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Writing the first "Hello, PyCUDA!" program in PyCUDA
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyCUDA中编写第一个“Hello, PyCUDA!”程序
- en: Accessing device properties from a PyCUDA program
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从PyCUDA程序访问设备属性
- en: Thread and block execution in PyCUDA
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyCUDA中的线程和块执行
- en: Basic PyCUDA programming concepts using a vector addition program
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用向量加法程序的基本PyCUDA编程概念
- en: Measuring the performance of PyCUDA programs using CUDA events
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA事件测量PyCUDA程序的性能
- en: Some complex programs in PyCUDA
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyCUDA中的一些复杂程序
- en: Advanced kernel functions in PyCUDA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyCUDA中的高级内核函数
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires a good understanding of the Python programming language.
    It also requires any computer or laptop with the Nvidia GPU onboard. All the code
    used in this chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要良好的Python编程语言理解。它还需要任何带有Nvidia GPU的计算机或笔记本电脑。本章中使用的所有代码都可以从以下GitHub链接下载：[https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA)。
- en: 'Check out the following video to see the code in action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际运行情况：
- en: '[http://bit.ly/2QPWojV](http://bit.ly/2QPWojV)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2QPWojV](http://bit.ly/2QPWojV)'
- en: Writing the first program in PyCUDA
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyCUDA中编写第一个程序
- en: This section describes the procedure for writing a simple "Hello, PyCUDA!" program
    using PyCUDA. It will demonstrate the workflow for writing any PyCUDA programs.
    As Python is an interpreted language, the code can also be run line by line from
    the Python terminal, or it can be saved with the `.py` extension and executed
    as a file.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了使用PyCUDA编写简单“Hello, PyCUDA!”程序的步骤。它将演示编写任何PyCUDA程序的工作流程。由于Python是一种解释型语言，代码也可以从Python终端逐行运行，或者保存为`.py`扩展名并作为文件执行。
- en: 'The program for displaying a simple string from the kernel using PyCUDA is
    shown as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyCUDA显示从内核中简单字符串的程序如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first step while developing PyCUDA code is to include all libraries needed
    for the code. The `import` directive is used to include a library, module, class,
    or function in a file. This is similar to including a directive in C or C++, and
    it can be done in three different ways, as shown in the following steps. The use
    of three imported modules is also shown as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发PyCUDA代码时的第一步是包含代码所需的所有库。使用`import`指令来包含一个库、模块、类或函数。这与在C或C++中包含指令类似，并且可以通过以下三种不同的方式完成，如下面的步骤所示。以下也展示了使用三个导入模块的示例：
- en: Import `pycuda.driver` as `drv`
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pycuda.driver`为`drv`
- en: This indicates that the driver submodule of the pymodule is imported and it
    is given a short notation `drv` , so wherever functions from the `pycuda.driver`
    module are to be used then they can be used as `drv.functionname`. This module
    contains memory management functions, device properties, data direction functions,
    and so on.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表示导入了pymodule的驱动子模块，并给它一个简写符号`drv`，所以当需要使用`pycuda.driver`模块中的函数时，可以使用`drv.functionname`。此模块包含内存管理函数、设备属性、数据方向函数等。
- en: Import `pycuda.autoinit`
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pycuda.autoinit`
- en: This command indicates the `autoint` module from `pycuda` is imported. It is
    not given any shorthand notation. The `autoint` module is used for device initialization,
    context creation, and memory cleanup. This module is not mandatory, and all the
    above functions can also be done manually.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个命令表示从`pycuda`的`autoint`模块导入了。没有给出任何缩写符号。`autoint`模块用于设备初始化、上下文创建和内存清理。此模块不是必需的，上述所有功能也可以手动完成。
- en: From `pycuda.compiler` import `SourceModule`
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`pycuda.compiler`导入`SourceModule`
- en: This command indicates that only the `SourceModule` class from the `pycuda.compiler`
    module is imported. This is important when you only want to use one class of a
    large module. The `SourceModule` class is used to define C-like kernel functions
    in PyCUDA.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个命令表示只从`pycuda.compiler`模块导入了`SourceModule`类。当你只想使用一个大模块中的一个类时，这是很重要的。`SourceModule`类用于在PyCUDA中定义类似C的内核函数。
- en: The C or C++ kernel code is fed as a constructor to the `Sourcemodule` class
    and the mod object is created. The kernel code is very simple as it is just printing
    a `Hello, PyCUDA!` string on the console. As the `printf` function is used inside
    kernel code, it is very important to include the `stdio.h` header file. The `myfirst_kernel`
    function is defined inside the kernel code using the `__global__` directive to
    indicate that the function will be executed on the GPU. The function does not
    take any arguments. It just prints a string on the console. This kernel function
    will be compiled by the `nvcc` compiler.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: C或C++内核代码作为构造函数传递给`Sourcemodule`类，并创建mod对象。内核代码非常简单，因为它只是在控制台上打印一个`Hello, PyCUDA!`字符串。由于内核代码中使用了`printf`函数，因此包含`stdio.h`头文件非常重要。`myfirst_kernel`函数在内核代码中使用`__global__`指令定义，表示该函数将在GPU上执行。该函数不接受任何参数。它只是在控制台上打印一个字符串。此内核函数将由`nvcc`编译器编译。
- en: This function can be used inside Python code by creating a pointer to the function
    using the `get_function` method of the `mod` object. The name of the kernel function
    is given as arguments in quotes. The pointer variable can be given any name. This
    pointer variable is used to call the kernel in the last line of the code. The
    arguments to the kernel function can be specified here, but the `myfirst_kernel`
    function does not have any arguments, so no arguments are specified. The number
    of threads per block and blocks per grid to be launched for a kernel can also
    be provided as an argument by using optional block and grid arguments. The block
    argument is given a value of (1,1,1) which is a 1 x 3 Python tuple, which indicates
    a block size of 1 x 1 x 1\. So one thread will be launched that will print the
    string on the console.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以通过使用`mod`对象的`get_function`方法创建一个指向该函数的指针在Python代码内部使用。内核函数的名称作为引号内的参数给出。指针变量可以取任何名字。这个指针变量用于在代码的最后一行调用内核。内核函数的参数也可以在这里指定，但由于`myfirst_kernel`函数没有参数，所以没有指定参数。内核可以提供的线程数和每个网格要启动的块数也可以通过使用可选的块和网格参数作为参数提供。块参数被赋予值（1,1,1），这是一个1
    x 3的Python元组，表示块大小为1 x 1 x 1。因此，将启动一个线程，该线程将在控制台上打印字符串。
- en: 'The output of the program is shown as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的输出如下所示：
- en: '![](img/c5ea51cf-3057-4c87-8f71-b868bd39c37e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5ea51cf-3057-4c87-8f71-b868bd39c37e.png)'
- en: To summarize, this section demonstrated the procedure to develop a simple PyCUDA
    program step by step.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，本节逐步展示了开发一个简单的PyCUDA程序的步骤。 '
- en: A kernel call
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核调用
- en: 'The device code that is written using ANSI C keywords along with CUDA extension
    keywords is called a **kernel**. It is launched from a Python code by a method
    called **Kernel Call**. Basically, the meaning of a kernel call is that we are
    launching a device code from the host code. A kernel call typically generates
    a large number of blocks and threads to exploit data parallelism on a GPU. Kernel
    code is very similar to that of normal C functions; it is just that this code
    is executed by several threads in parallel. It has a very simple syntax in Python,
    which can be shown as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ANSI C 关键字以及 CUDA 扩展关键字编写的设备代码被称为 **内核**。它通过一种名为 **内核调用** 的方法从 Python 代码中启动。基本上，内核调用的意义是我们从主机代码中启动设备代码。内核代码与正常
    C 函数非常相似；只是这段代码是由多个线程并行执行的。它在 Python 中的语法非常简单，如下所示：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It starts with the pointer of the kernel function that we want to launch. You
    should make sure that this kernel pointer is created using the `get_function`
    method. Then, it can include parameters of the kernel function separated by a
    comma. The block parameter indicates the number of threads to be launched, and
    the grid parameter indicates the number of blocks in the grid. The block and grid
    parameters are specified using a 1 x 3 Python tuple, which indicates blocks and
    threads in three dimensions. The total number of threads started by a kernel launch
    will be the multiplication of these numbers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这从我们要启动的内核函数的指针开始。你应该确保这个内核指针是通过 `get_function` 方法创建的。然后，它可以包括用逗号分隔的内核函数参数。块参数表示要启动的线程数，而网格参数表示网格中的块数。块和网格参数使用一个
    1 x 3 的 Python 元组指定，表示三维空间中的块和线程。内核启动启动的线程总数将是这些数字的乘积。
- en: Accessing GPU device properties from PyCUDA program
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 PyCUDA 程序访问 GPU 设备属性
- en: PyCUDA provides a simple API to find information such as, which CUDA-enabled
    GPU devices (if any) are present and which capabilities each device supports.
    It is important to find out the properties of a GPU device that is being used
    before writing PyCUDA programs so that the optimal resources of the device can
    be used.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA 提供了一个简单的 API 来查找信息，例如，哪些 CUDA 兼容的 GPU 设备（如果有）存在，以及每个设备支持哪些功能。在编写 PyCUDA
    程序之前，了解正在使用的 GPU 设备的属性非常重要，这样就可以使用设备的最佳资源。
- en: 'The program for displaying all properties of CUDA-enabled devices on a system
    by using PyCUDA is shown as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyCUDA 显示系统上所有 CUDA 兼容设备属性的程序如下所示：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, it is important to get a count of how many CUDA-enabled devices are present
    on the system, as a system may contain more than one GPU-enabled device. This
    count can be determined by the `drv.Device.count()` function of a driver class
    in PyCUDA. All the devices present on a system are iterated to determine the properties
    of each device. A pointer object to each device is created using the `drv.Device`
    function. This pointer is used to determine all properties of a particular device.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重要的是要获取系统上存在的 CUDA 兼容设备数量，因为一个系统可能包含多个启用 GPU 的设备。这个数量可以通过 PyCUDA 中驱动类 `drv.Device.count()`
    函数确定。系统上所有设备都会被迭代以确定每个设备的属性。使用 `drv.Device` 函数为每个设备创建一个指针对象。这个指针用于确定特定设备的所有属性。
- en: The `name` function will give the name of a particular device and `total_memory`
    will give the size of the GPU global memory available on the device. The other
    properties are stored as a Python dictionary that can be fetched by the `get_attributes().items()`
    function. This is converted to a list of tuples by using list comprehension in
    Python. All the rows of this list contain the 2 x 1 tuple, which has the name
    of the property and its value.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`name` 函数将给出特定设备的名称，而 `total_memory` 将给出设备上可用的 GPU 全局内存的大小。其他属性存储为 Python 字典，可以通过
    `get_attributes().items()` 函数检索。这通过 Python 中的列表推导式转换为元组列表。这个列表的所有行都包含一个 2 x 1
    的元组，其中包含属性的名称和其值。'
- en: 'This list is iterated using the `for` loop to display all the properties on
    the console. This program is executed on the laptop with a GeForce 940 GPU and
    CUDA 9\. The output of the program is as shown:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `for` 循环迭代此列表以在控制台上显示所有属性。此程序在配备 GeForce 940 GPU 和 CUDA 9 的笔记本电脑上执行。程序输出如下：
- en: '![](img/f52a087b-5ec6-4407-9326-48effac47ab0.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f52a087b-5ec6-4407-9326-48effac47ab0.png)'
- en: The properties were discussed in detail in earlier chapters of the book, so
    we won't discuss them again; however, to summarize, this section demonstrated
    the method to access GPU device properties from a PyCUDA program.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性在本书的早期章节中已经详细讨论过，所以我们不再重复讨论；然而，为了总结，本节展示了从 PyCUDA 程序中访问 GPU 设备属性的方法。
- en: Thread and block execution in PyCUDA
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA 中的线程和块执行
- en: 'We saw in the *A k**ernel call* section that we can start multiple blocks and
    multiple threads in parallel. So, in which order do these blocks and threads start
    and finish their execution? It is important to know this if we want to use the
    output of one thread in other threads. To understand this, we have modified the
    kernel in the `hello,PyCUDA!` program, seen in the earlier section, by including
    a print statement in a kernel call, which prints the block number. The modified
    code is shown as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *内核调用* 部分看到，我们可以并行启动多个块和多个线程。那么，这些块和线程以什么顺序开始和结束它们的执行？如果我们想在其他线程中使用一个线程的输出，了解这一点很重要。为了理解这一点，我们修改了前面章节中看到的
    `hello,PyCUDA!` 程序中的内核，通过在内核调用中包含一个打印语句来打印块号。修改后的代码如下所示：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As can be seen from the code, we are launching a kernel with 10 blocks in parallel,
    with each block having a single thread. In the kernel code, we are printing the
    block ID of the kernel execution. We can think of that as 10 copies of the same
    `myfirstkernel` start execution in parallel. Each of these copies will have a
    unique block ID, which can be accessed by the `blockIdx.x` directive, and unique
    thread ID, which can be accessed by `threadIdx.x`. These IDs will tell us which
    block and thread are executing the kernel. When you run the program many times,
    you will find that each time, blocks execute in different orders. One sample output
    can be shown as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中可以看出，我们以并行方式启动了 10 个块，每个块有一个线程。在内核代码中，我们正在打印内核执行的块 ID。我们可以将其视为 10 个相同的 `myfirstkernel`
    并行开始执行。这些副本中的每一个都将有一个唯一的块 ID，可以通过 `blockIdx.x` 指令访问，以及唯一的线程 ID，可以通过 `threadIdx.x`
    访问。这些 ID 将告诉我们哪个块和线程正在执行内核。当你多次运行程序时，你会发现每次块执行的顺序都不同。一个示例输出如下所示：
- en: '![](img/2aa20d82-b407-4168-8d84-f97a577bd75a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2aa20d82-b407-4168-8d84-f97a577bd75a.png)'
- en: It can produce *n* factorial number of different outputs, where *n* indicates
    the number of blocks started in parallel. So, whenever you are writing the program
    in PyCUDA, you should be careful that blocks execute in random order.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以产生 *n* 的阶乘数量的不同输出，其中 *n* 表示并行启动的块的数量。因此，每次你在 PyCUDA 中编写程序时，都应该小心，因为块会以随机顺序执行。
- en: Basic programming concepts in PyCUDA
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA 中的基本编程概念
- en: We will start developing some useful stuff using PyCUDA in this section. The
    section will also demonstrate some useful functions and directives of PyCUDA,
    using a simple example of adding two numbers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始使用 PyCUDA 开发一些有用的功能。本节还将通过一个简单的加法示例展示 PyCUDA 的一些有用的函数和指令。
- en: Adding two numbers in PyCUDA
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA 中的加法
- en: 'Python provides a very fast library for numerical operations which is called
    **numpy (Numeric Python)**. It is developed in C or C++ and is very useful for
    array manipulations in Python. It is used frequently in PyCUDA programs as arguments
    to PyCUDA kernel functions are passed as numpy arrays. This section explains how
    to add two numbers using PyCUDA. The basic kernel code for adding two numbers
    is shown as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了一个用于数值运算的非常快速的库，称为 **numpy (Numeric Python)**。它是用 C 或 C++ 开发的，并且对于
    Python 中的数组操作非常有用。它在 PyCUDA 程序中经常被用作 PyCUDA 内核函数的参数，这些参数作为 numpy 数组传递。本节解释了如何使用
    PyCUDA 添加两个数字。添加两个数字的基本内核代码如下所示：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `SourceModule` class and driver class are imported as explained earlier.
    The `numpy` library is also imported as it will be required for passing arguments
    to the kernel code. The `add_num` kernel function is defined as a constructor
    to the `SourceModule` class. The function takes two device pointers as input and
    one device pointer that points to the answer of the addition as output. It is
    important to note that, though we are adding two numbers, the kernel function
    is defined so that it can work on two array additions as well. Two single numbers
    are nothing but two arrays with one element each. If there aren’t any errors,
    this code will be compiled and loaded onto the device. The code to call this kernel
    code from Python is shown as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，导入 `SourceModule` 类和驱动类。`numpy` 库也被导入，因为它将用于将参数传递给内核代码。`add_num` 内核函数被定义为
    `SourceModule` 类的构造函数。该函数接受两个设备指针作为输入，一个设备指针指向加法的结果作为输出。需要注意的是，尽管我们在添加两个数字，但内核函数被定义为可以同时处理两个数组加法。两个单个数字不过是每个只有一个元素的数组。如果没有错误，此代码将被编译并加载到设备上。从
    Python 调用此内核代码的代码如下所示：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The pointer reference to the kernel function is created using `get_function`.
    Two random numbers are created using the `numpy.random.randn(1)` function, which
    is used to create a random number in the normal distribution. These numbers are
    converted to single precision floating point numbers, using the `astype(numpy.float32)`
    method. The numpy array to store the result on the host is initialized to zero.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `get_function` 创建内核函数的指针引用。使用 `numpy.random.randn(1)` 函数创建两个随机数，该函数用于创建正态分布中的随机数。这些数字使用
    `astype(numpy.float32)` 方法转换为单精度浮点数。用于在主机上存储结果的 numpy 数组被初始化为零。
- en: The memory on the device can be allocated using the `mem_alloc` function of
    a driver class in PyCUDA. The size of memory is passed as an argument to the function.
    The size for input is found using the `h_a.nbytes` function. PyCUDA provides a
    `memcpy` function in the driver class to copy data from host memory to the device
    memory and vice versa.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 PyCUDA 中驱动类的 `mem_alloc` 函数在设备上分配内存。内存的大小作为函数的参数传递。使用 `h_a.nbytes` 函数找到输入的大小。PyCUDA
    在驱动类中提供了一个 `memcpy` 函数，用于从主机内存到设备内存以及相反方向的复制数据。
- en: 'The `drv.memcpy_htod` function copies data from the host memory to the device
    memory. The pointer to the device memory is passed as the first argument and the
    host memory pointer is passed as the second argument. The `add_num` kernel is
    called by passing device pointers as arguments along with numbers that specify
    the number of blocks and threads to be launched. In the code given before, one
    block is launched with one thread. The result computed by the kernel is copied
    back to the host by using the `drv.memcpy_dtoh` function. The result is displayed
    on the console, which is shown as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`drv.memcpy_htod` 函数用于将数据从主机内存复制到设备内存。设备内存的指针作为第一个参数传递，主机内存指针作为第二个参数传递。通过传递设备指针以及指定要启动的块和线程数量的数字作为参数，调用
    `add_num` 内核。在前面给出的代码中，使用一个线程启动了一个块。内核计算的结果通过使用 `drv.memcpy_dtoh` 函数复制回主机。结果在控制台上显示，如下所示：'
- en: '![](img/9455fd86-f37e-406b-a16f-e7e86be45476.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9455fd86-f37e-406b-a16f-e7e86be45476.png)'
- en: To summarize, this section demonstrated the structure of a PyCUDA program. It
    started with a kernel definition code. Then inputs are defined in Python. The
    memory is allocated on the device and inputs are transferred to the device memory.
    This is followed by a kernel call, which will compute the result. This result
    is transferred to the host for further processing. PyCUDA provides even simpler
    APIs to do this operation, which is explained in the next section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本节展示了 PyCUDA 程序的结构。它从内核定义代码开始。然后在 Python 中定义输入。在设备上分配内存并将输入传输到设备内存。接着是内核调用，它将计算结果。然后将结果传输到主机进行进一步处理。PyCUDA
    提供了更简单的 API 来执行此操作，这将在下一节中解释。
- en: Simplifying the addition program using driver class
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用驱动类简化加法程序
- en: 'PyCUDA provides an even simpler API for kernel calling that does not require
    memory allocation and memory copying. It is done implicitly by the API. This can
    be accomplished by using the `In` and `Out` functions of the driver class in PyCUDA.
    The modified array addition code is shown as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA 提供了一个更简单的内核调用 API，它不需要内存分配和内存复制。这是通过 API 隐式完成的。这可以通过使用 PyCUDA 中驱动类中的
    `In` 和 `Out` 函数来实现。修改后的数组加法代码如下所示：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Ten elements of an array are added instead of single elements in the preceding
    code. The kernel function is exactly the same as the code seen previously. Two
    arrays of ten random numbers are created on the host. Now instead of creating
    the memory of them and transferring that to the device, the kernel is called directly.
    The kernel call is modified by specifying the direction of data using `drv.Out`
    or `drv.In`. It simplifies the PyCUDA code and reduces the size of the code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，数组中的十个元素被添加，而不是单个元素。内核函数与之前看到的代码完全相同。在主机上创建了两个包含十个随机数的数组。现在，不是创建它们的内存并将它们传输到设备，而是直接调用内核。通过指定数据方向使用
    `drv.Out` 或 `drv.In` 来修改内核调用。这简化了 PyCUDA 代码并减少了代码的大小。
- en: 'The kernel is called with one block and *N* threads per block. These *N* threads
    add *N* elements of the array in parallel, which accelerates the addition operation.
    The result of the kernel is automatically downloaded to the host memory by using
    the `drv.out` directive so this result is directly printed on the console using
    the `for` loop. The result for an addition of ten elements using PyCUDA is shown
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内核函数使用一个块和每个块 *N* 个线程。这 *N* 个线程并行地添加数组中的 *N* 个元素，从而加速了加法操作。使用 `drv.out` 指令，内核的结果会自动下载到主机内存，因此这个结果可以直接通过
    `for` 循环打印到控制台。使用 PyCUDA 进行十个元素加法的结果如下所示：
- en: '![](img/82f501bd-24c0-4e17-84e7-bec4ddd25f74.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82f501bd-24c0-4e17-84e7-bec4ddd25f74.png)'
- en: To summarize, this section described the important concepts and functions of
    PyCUDA by taking a simple array addition program. The performance improvement
    of using PyCUDA can be quantified using CUDA events, which are explained in the
    next section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本节通过一个简单的数组加法程序介绍了 PyCUDA 的重要概念和函数。使用 PyCUDA 的性能提升可以通过下一节中解释的 CUDA 事件来量化。
- en: Measuring performance of PyCUDA programs using CUDA events
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CUDA 事件测量 PyCUDA 程序的性能
- en: So far, we have not determined the performance of the PyCUDA programs explicitly.
    In this section, we will see how to measure the performance of the programs using
    CUDA events. This is a very important concept in PyCUDA because it will allow
    you to choose the best performing algorithms for a particular application from
    many options.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有明确确定 PyCUDA 程序的性能。在本节中，我们将看到如何使用 CUDA 事件测量程序的性能。这在 PyCUDA 中是一个非常重要的概念，因为它将允许你从许多选项中选择特定应用的性能最佳算法。
- en: CUDA events
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA 事件
- en: We can use Python time measuring options for measuring the performance of CUDA
    programs, but it will not give accurate results. It will include the time overhead
    of thread latency in the OS and scheduling in the OS among many other factors.
    The time measured using the CPU will also depend on the availability of a high-precision
    CPU timer. Many times, the host is performing asynchronous computations while
    the GPU kernel is running, and hence CPU timers of Python may not give correct
    times for kernel executions. So, to measure the time for GPU kernel computations,
    PyCUDA provides an event API.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Python 时间测量选项来测量 CUDA 程序的性能，但它不会给出准确的结果。它将包括许多其他因素中的线程延迟在操作系统中的时间开销和调度。使用
    CPU 测量的时间也将取决于高精度 CPU 计时器的可用性。很多时候，当 GPU 内核运行时，主机正在执行异步计算，因此 Python 的 CPU 计时器可能不会给出内核执行的正确时间。因此，为了测量
    GPU 内核计算的时间，PyCUDA 提供了一个事件 API。
- en: 'A CUDA event is a GPU timestamp recorded at a specified point in a PyCUDA program.
    In this API, the GPU records the timestamp, which eliminates the issues that were
    present when using CPU timers for measuring performance. There are two steps to
    measure time using CUDA events: creating an event and recording an event. We can
    record two events, one at the start of our code and one at the end. Then we will
    try to calculate the difference in time between the two events that will give
    an overall performance for our code.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件是在 PyCUDA 程序中指定点记录的 GPU 时间戳。在这个 API 中，GPU 记录时间戳，消除了使用 CPU 计时器测量性能时存在的问题。使用
    CUDA 事件测量时间有两个步骤：创建事件和记录事件。我们可以记录两个事件，一个在代码的开始处，一个在结束处。然后我们将尝试计算两个事件之间的时间差，这将给出代码的整体性能。
- en: 'In PyCUDA code, the following lines can be included to measure performance
    using a CUDA event API:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyCUDA 代码中，可以包含以下行来使用 CUDA 事件 API 测量性能：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `record` method is used to measure a current timestamp. The timestamp is
    measured before and after the kernel code to measure time for the kernel execution.
    The difference between timestamps can be measured using the `time_till` method,
    as shown in the preceding code. It will give time in milliseconds, which is converted
    to seconds. In the next section, we will try to measure the performance of code
    using a CUDA event.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`record`方法来测量当前时间戳。在内核代码前后测量时间戳以测量内核执行时间。可以使用`time_till`方法测量时间戳之间的差异，如前述代码所示。它将以毫秒为单位给出时间，然后将其转换为秒。在下一节中，我们将尝试使用CUDA事件来测量代码的性能。
- en: Measuring performance of PyCUDA using large array addition
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大数组加法来测量PyCUDA的性能
- en: 'This section will demonstrate the method to use CUDA events to measure the
    performance of PyCUDA programs. The comparison of the performance of PyCUDA code
    with simple Python code is also described. The arrays with a million elements
    are taken so that performance can be accurately compared. The kernel code for
    large array addition is shown as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将演示如何使用CUDA事件来测量PyCUDA程序的性能。同时，还描述了PyCUDA代码与简单Python代码性能的比较。为了准确比较性能，选取了包含一百万个元素的数组。下面展示了用于大数组加法的内核代码：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As the number of elements is high, multiple blocks and threads are launched.
    So, both the thread ID and block ID are used to calculate the thread index. If
    the total number of threads launched is not equal to the number of elements, then
    multiple elements are added by the same thread. This is done by the `while` loop
    inside the kernel function. It will also ensure that the thread index does not
    go beyond the array elements. Apart from the input array and the output array,
    the size of an array is also taken as a parameter for the kernel function, as
    Python global variables are not accessible to kernel code in `SourceModule`. The
    Python code for adding large arrays is shown as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于元素数量较多，因此会启动多个块和线程。所以，既使用线程ID又使用块ID来计算线程索引。如果启动的总线程数不等于元素数，则同一线程会添加多个元素。这是通过内核函数内部的`while`循环实现的。同时，它也会确保线程索引不会超出数组元素。除了输入数组和输出数组外，数组的大小也被作为内核函数的参数，因为在`SourceModule`中内核代码无法访问Python的全局变量。下面展示了用于添加大数组的Python代码：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Two events, `start` and `stop` are created to measure timings for the GPU code.The
    `Event()` function from the driver class is used to define event objects. Then,
    the pointer reference to the kernel function is created using the `get_function`.
    Two arrays with a million elements each are initialized with random numbers using
    the `randn` function of the `numpy` library. It will generate floating point numbers
    so they are converted to the single precision number to speed up computation on
    the device.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了两个事件`start`和`stop`来测量GPU代码的执行时间。使用驱动类中的`Event()`函数来定义事件对象。然后，使用`get_function`创建内核函数的指针引用。使用`numpy`库的`randn`函数初始化两个包含一百万个元素的数组，并使用随机数。由于它们是浮点数，因此将它们转换为单精度数以加快设备上的计算速度。
- en: Each block supports 1,024 threads as we saw in the device property section.
    So based on that, the total number of blocks is calculated by dividing *N* by
    1,024\. It can be a float value so it is converted to next highest integer value
    using the `ceil` function of the `numpy` library. Then the kernel is launched
    with the calculated value number of blocks and 1,024 threads per block. The size
    of the array is passed with the `numpy.uint32` datatype.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块支持1,024个线程，正如我们在设备属性部分所看到的。因此，根据这一点，通过将*N*除以1,024来计算总块数。它可能是一个浮点值，因此使用`numpy`库的`ceil`函数将其转换为下一个最高整数值。然后，使用计算出的块数和每个块的1,024个线程来启动内核。使用`numpy.uint32`数据类型传递数组的大小。
- en: 'The time is recorded before and after calling the kernel function using the
    record function, and the time difference is calculated to measure the timing of
    the kernel function. The calculated time is printed on the console. To compare
    this performance with CPU timings, the following code is added to the program:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用记录函数在调用内核函数前后记录时间，并计算时间差以测量内核函数的执行时间。计算出的时间将打印在控制台上。为了将此性能与CPU时间进行比较，程序中添加了以下代码：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The time library from Python is used to measure CPU timings. The `for` loop
    is used to iterate through every element in an array. (Note: you can also use
    `h_result1 = h_a + h_b` as both arrays are numpy arrays.) Time is measured before
    and after the `for` loop using the `time.time()` function and the difference between
    this time is printed on the console. The output from the program is shown as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python的时间库来测量CPU时间。使用`for`循环遍历数组中的每个元素。（注意：由于两个数组都是numpy数组，你也可以使用`h_result1
    = h_a + h_b`。）使用`time.time()`函数在`for`循环前后测量时间，并将这两个时间之间的差值打印到控制台。程序的输出如下所示：
- en: '![](img/dbdc8dab-65bd-46de-bd99-aabacaede9b0.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dbdc8dab-65bd-46de-bd99-aabacaede9b0.png)'
- en: As can be seen from the output, GPU takes 9.4 ms to add a million elements,
    while the CPU takes 415.15 ms, so around a 50-times improvement can be accomplished
    by using a GPU.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，GPU添加一百万个元素需要9.4毫秒，而CPU需要415.15毫秒，因此使用GPU可以实现大约50倍的性能提升。
- en: To summarize, this section demonstrated the use of events to measure timings
    for GPU code. The performance of the GPU is compared with CPU performance to quantify
    the performance improvement while using the GPU.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本节展示了使用事件来测量GPU代码的计时。将GPU性能与CPU性能进行比较，以量化使用GPU时的性能提升。
- en: Complex programs in PyCUDA
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA中的复杂程序
- en: The PyCUDA syntax and terminology will be familiar by now. We will use this
    knowledge to develop advanced programs and learn some advanced concepts in PyCUDA.
    In this section, we will develop a program to square elements of an array using
    three different methods in PyCUDA. We will also learn the code for doing matrix
    multiplication in PyCUDA.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，PyCUDA的语法和术语应该已经熟悉了。我们将利用这些知识来开发高级程序，并学习一些PyCUDA的高级概念。在本节中，我们将使用PyCUDA开发一个程序，使用三种不同的方法对数组的元素进行平方。我们还将学习在PyCUDA中进行矩阵乘法的代码。
- en: Element-wise squaring of a matrix in PyCUDA
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA中矩阵的逐元素平方
- en: In this section, the program to perform element-wise squaring of numbers in
    a matrix is performed using three different methods. While doing this, the concepts
    of using multidimensional threads and blocks, the `inout` directive of the driver
    class, and the `gpuarray` class is explained in detail.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，使用三种不同的方法执行矩阵中数字的逐元素平方操作。在这个过程中，详细解释了使用多维线程和块的概念、驱动类中的`inout`指令以及`gpuarray`类。
- en: Simple kernel invocation with multidimensional threads
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多维线程的简单内核调用
- en: 'A simple kernel function using PyCUDA to square every element of a matrix is
    implemented in this section. The kernel function of squaring every element of
    a 5 x 5 matrix is shown as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节实现了使用PyCUDA对矩阵的每个元素进行平方的简单内核函数。以下是一个5 x 5矩阵中每个元素平方的内核函数示例：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The kernel function square takes only one device pointer that points to the
    matrix as input and replaces every element with the square of it. As multidimensional
    threads are launched, the thread index in both *x* and *y* directions are used
    to index the value from a matrix. You can assume that a 5 x 5 matrix is flattened
    to a 1 x 25 vector to understand the indexing mechanism. Please, note that the
    size of a matrix is hardcoded to `5` in this code, but it can also be user-defined
    like the size of an array in the last section. The Python code to use this kernel
    function is shown as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数`square`只接受一个设备指针作为输入，该指针指向矩阵，并将每个元素替换为其平方。随着多维线程的启动，*x*和*y*方向上的线程索引被用来索引矩阵中的值。你可以假设一个5
    x 5矩阵被展平为一个1 x 25的向量，以理解索引机制。请注意，在这个代码中，矩阵的大小是硬编码为`5`的，但它也可以像上一节中的数组大小一样由用户定义。使用此内核函数的Python代码如下所示：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Two events are created to measure timings of a kernel function. The matrix of
    5 x 5 is initialized with random numbers on the host. It is done by using a `randint`
    function of the `numpy.random` module. It requires three arguments. The first
    two arguments define the range of numbers used to generate random numbers. The
    first argument is the minimum value and the second argument is the maximum value
    used for generating numbers. The third argument is the size, which is specified
    as a tuple (5,5). This generated matrix is again converted to a single precision
    number for faster processing. The memory for the matrix is allocated on the device
    and the generated matrix of random numbers is copied to it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了两个事件来测量内核函数的计时。在主机上，一个 5x5 的矩阵通过 `numpy.random` 模块的 `randint` 函数初始化为随机数。这需要三个参数。前两个参数定义了用于生成随机数的数字范围。第一个参数是最小值，第二个参数是用于生成数字的最大值。第三个参数是大小，指定为元组
    (5,5)。这个生成的矩阵再次转换为单精度数以加快处理速度。矩阵的内存是在设备上分配的，生成的随机数矩阵被复制到其中。
- en: The pointer reference to the kernel function is created and the kernel is called
    by passing a device memory pointer as an argument. The kernel is called with multidimensional
    threads with the value of 5 in the *x* and *y* directions. So the total number
    of threads launched is 25, with each thread calculating a square of a single element
    of the matrix. The result calculated by the kernel is copied back to the host
    and displayed on the console. The time needed for the kernel is displayed on the
    console along with the input and output matrix. The output is displayed on the
    console.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了指向内核函数的指针引用，并通过传递设备内存指针作为参数调用内核。内核调用使用多维线程，*x* 和 *y* 方向上的值为 5。因此，总共启动了 25
    个线程，每个线程计算矩阵中单个元素的平方。内核计算的结果被复制回主机并在控制台上显示。内核所需的时间以及输入和输出矩阵都显示在控制台上。
- en: '![](img/f3635a78-4477-4e62-a0a8-a8497255a5c5.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f3635a78-4477-4e62-a0a8-a8497255a5c5.png)'
- en: It takes 149 ms to calculate the square of each element of a 5 x 5 matrix. The
    same calculation can be simplified by using the `inout` directive of the driver
    class. This is explained in the next section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一个 5x5 矩阵中每个元素的平方需要 149 毫秒。使用驱动类的 `inout` 指令可以简化相同的计算。这将在下一节中解释。
- en: Using inout with the kernel invocation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `inout` 与内核调用结合使用
- en: 'As can be seen from the kernel function of the program in the last section,
    the same array is used as both input and output. The driver module in PyCUDA provides
    an `inout` directive for this kind of cases. It removes the need for the separate
    allocation of memory for the array, uploading it to the device and downloading
    the result back to the host. All operations are performed simultaneously during
    a kernel call. This makes the code simpler and easier to read. The Python code
    for using the `inout` directive of driver class is shown as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一节程序的核心函数所示，相同的数组既用作输入也用作输出。PyCUDA 的驱动模块为这类情况提供了一个 `inout` 指令。它消除了为该数组单独分配内存、上传到设备以及将结果下载回主机的需求。所有操作都在内核调用期间同时进行。这使得代码更简单，更容易阅读。使用驱动类
    `inout` 指令的 Python 代码如下所示：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The CUDA events are initialized to measure the performance of the code using
    the `inout` directive. The kernel call is the same as in the last section so it
    is not repeated here. As can be seen, while calling the square kernel, a single
    variable is passed as an argument with the `drv.inout` directive. So, all device-related
    operations are performed in this single step. The kernel is called with multidimensional
    threads as is the case in the last section. The computed result and the time taken
    is printed on the console as the following shows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `inout` 指令初始化 CUDA 事件以测量代码的性能。内核调用与上一节相同，因此在此不再重复。可以看出，在调用平方内核时，通过 `drv.inout`
    指令传递了一个变量作为参数。因此，所有与设备相关的操作都在这一步中完成。内核调用使用多维线程，与上一节的情况相同。计算结果和耗时被打印到控制台，如下所示：
- en: '![](img/6e6b4841-76fc-4b09-aa48-80f126cdd70b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6e6b4841-76fc-4b09-aa48-80f126cdd70b.png)'
- en: The time taken is comparatively less than the original kernel. So, by using
    the `inout` directive the of driver class, the PyCUDA code can be made efficient
    and easy to read. PyCUDA also provides a `gpuarray` class for array-related operations.
    It can be also used for squaring operations, which is explained in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所需时间相对于原始内核来说比较少。因此，通过使用驱动类中的 `inout` 指令，PyCUDA 代码可以变得高效且易于阅读。PyCUDA 还提供了一个用于数组相关操作的
    `gpuarray` 类。它也可以用于平方操作，这将在下一节中解释。
- en: Using gpuarray class
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `gpuarray` 类
- en: 'Python provides a `numpy` library for numeric computations in Python. PyCUDA
    provides a `gpuarray` class, similar to `numpy`, that stores its data and performs
    its computations on the GPU device. The shape and datatype of the arrays work
    exactly as in `numpy`. The `gpuarray` class provides many arithmetic methods for
    computations. It removes the need to specify the kernel code in C or C++ using
    `SourceModule`. So, the PyCUDA code will contain only a Python code. The code
    of squaring every element of the matrix using the `gpuarray` class is shown as
    follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了一个 `numpy` 库，用于在 Python 中进行数值计算。PyCUDA 提供了一个与 `numpy` 类似的 `gpuarray`
    类，该类在 GPU 设备上存储其数据和执行其计算。数组的形状和数据类型与 `numpy` 中完全相同。`gpuarray` 类提供了许多用于计算的算术方法。它消除了使用
    `SourceModule` 在 C 或 C++ 中指定内核代码的需要。因此，PyCUDA 代码将只包含 Python 代码。使用 `gpuarray` 类对矩阵的每个元素进行平方的代码如下所示：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `gpuarray` class needs to be imported for using in the code. It is available
    in the `pycuda.gpuarray` module. The matrix is initialized with random integers
    from 1 to 5 for computation. This matrix is uploaded to the device memory by using
    the `to_gpu()` method of the `gpuarray` class. The matrix to be uploaded is provided
    as an argument to this method. The matrix is converted to a single precision number.
    All the operations on this uploaded matrix will be performed on the device. The
    square operation is performed in a similar way as we do in Python code but, as
    the variable is stored on the device using `gpuarray`, this operation will also
    be performed on the device. The result is downloaded back to the host by using
    the `get` method. This result along with the time needed to perform element-wise
    squaring using `gpuarray` is displayed on the console as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpuarray` 类需要在代码中使用，它位于 `pycuda.gpuarray` 模块中。矩阵使用从 1 到 5 的随机整数进行初始化，以便进行计算。这个矩阵通过使用
    `gpuarray` 类的 `to_gpu()` 方法上传到设备内存。要上传的矩阵作为参数传递给此方法。矩阵被转换为单精度数字。所有对这个上传矩阵的操作都将在该设备上执行。平方操作以与我们在
    Python 代码中执行的方式类似的方式进行，但由于变量是使用 `gpuarray` 存储在设备上的，因此此操作也将在该设备上执行。结果通过使用 `get`
    方法下载回主机。以下是在控制台上显示的结果，包括使用 `gpuarray` 进行逐元素平方所需的时间：'
- en: '![](img/5c1561d4-76d2-420f-9718-56e7c2a1578c.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5c1561d4-76d2-420f-9718-56e7c2a1578c.png)'
- en: It takes around 58 ms to compute the square. It completely removes the need
    to define kernel functions in C language, and its functionality is similar to
    the `numpy` library so Python programmers can easily work with it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平方需要大约 58 毫秒。它完全消除了在 C 语言中定义内核函数的需要，其功能与 `numpy` 库相似，因此 Python 程序员可以轻松地与之一起工作。
- en: To summarize, in this section we have developed an element-wise squaring program
    using PyCUDA in three different fashions. We have also seen the concepts of multidimensional
    threads, the `inout` directive and the `gpuarray` class in PyCUDA.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，我们使用 PyCUDA 以三种不同的方式开发了一个逐元素平方程序。我们还看到了 PyCUDA 中的多维线程、`inout` 指令和
    `gpuarray` 类的概念。
- en: Dot product using GPU array
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPU 数组进行点积
- en: 'The dot between two vectors is an important mathematical operation used in
    various applications. The `gpuarray` class used in the last section can be used
    to calculate the dot product between two vectors. The performance of the `gpuarray`
    method to calculate the dot product is compared with the `numpy` operation. The
    code used to calculate the dot product using `numpy` is shown as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量之间的点积是各种应用中重要的数学运算。上一节中使用的 `gpuarray` 类可以用来计算两个向量之间的点积。`gpuarray` 方法计算点积的性能与
    `numpy` 操作进行了比较。用于使用 `numpy` 计算点积的代码如下所示：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Two vectors with 100 elements each are initialized with random integers for
    calculating the dot product. The time module of Python is used to calculate the
    time needed for the computation of the dot product. The `*` operator is used to
    calculate the element-wise multiplication of two vectors and the result of this
    is summed up to calculate the overall dot product. Please, note that the `numpy.dot`
    method calculated is used in matrix multiplication, which can''t be used for the
    dot product. The calculated dot product and time are displayed on the console.
    The code to perform the same operation on a GPU using `gpuarray` is shown as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化了两个各有100个元素的向量，并使用随机整数来计算点积。Python的时间模块用于计算计算点积所需的时间。使用`*`运算符来计算两个向量的逐元素乘积，然后将这些结果相加以计算总的点积。请注意，这里使用的`numpy.dot`方法用于矩阵乘法，不能用于点积。计算出的点积和时间将在控制台上显示。以下是如何使用`gpuarray`在GPU上执行相同操作的代码：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `to_gpu` method is used to upload two vectors on a GPU for calculating
    the dot product. The `gpuarray` class provides a dot method, which can be used
    to calculate the dot product directly. It needs two GPU arrays as an argument.
    The calculated result is downloaded back to the host by using the `get()` method.
    The calculated result and time measured using CUDA events are displayed on the
    console. The result of the program is shown as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`to_gpu`方法将两个向量上传到GPU上以计算点积。`gpuarray`类提供了一个点积方法，可以直接用于计算点积。它需要一个GPU数组作为参数。使用`get()`方法将计算结果下载回主机。计算结果和通过CUDA事件测量的时间将在控制台上显示。程序的结果如下所示：
- en: '![](img/a712df91-3835-4c33-a470-33af90a81546.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a712df91-3835-4c33-a470-33af90a81546.png)'
- en: As can be seen from the output, that same result is obtained by calculating
    the dot product using `numpy` and `gpuarray`. The `numpy` library takes 37 ms
    to compute the dot product, while the GPU takes only 0.1 ms to do the same operation.
    This further exemplifies the benifit of using GPU and PyCUDA to do complex mathematical
    operations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，使用`numpy`和`gpuarray`计算点积得到相同的结果。`numpy`库计算点积需要37毫秒，而GPU只需0.1毫秒即可完成相同的操作。这进一步说明了使用GPU和PyCUDA进行复杂数学运算的优势。
- en: Matrix multiplication
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'An important mathematical operation used frequently is matrix multiplication.
    This section will demonstrate how it can be performed on a GPU using PyCUDA. It
    is a very complicated mathematical operation when the sizes of the matrix are
    very large. It should be kept in mind that for matrix multiplication, the number
    of columns in the first matrix should be equal to the number of rows in the second
    matrix. Matrix multiplication is not a cumulative operation. To avoid complexity,
    in this example, we are taking a square matrix of the same size. If you are familiar
    with the mathematics of matrix multiplication, then you may recall that a row
    in the first matrix will be multiplied with all the columns in the second matrix.
    This is repeated for all rows in the first matrix. The example of a 3 x 3 matrix
    multiplication is shown as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 经常使用的一个重要数学运算是矩阵乘法。本节将演示如何使用PyCUDA在GPU上执行矩阵乘法。当矩阵的大小非常大时，这是一个非常复杂的数学运算。应记住，对于矩阵乘法，第一个矩阵的列数应等于第二个矩阵的行数。矩阵乘法不是累积操作。为了避免复杂性，在这个例子中，我们使用相同大小的方阵。如果你熟悉矩阵乘法的数学，你可能还记得，第一个矩阵的每一行将与第二个矩阵的所有列相乘。这将对第一个矩阵的所有行重复进行。以下是一个3x3矩阵乘法的示例：
- en: '![](img/acf80d05-4b69-4582-8bc5-e5e48c2c7b97.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/acf80d05-4b69-4582-8bc5-e5e48c2c7b97.png)'
- en: 'Every element in the resultant matrix will be calculated by multiplying the
    corresponding row in the first matrix and column in the second matrix. This concept
    is used to develop a kernel function shown as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵中的每个元素将通过将第一个矩阵的对应行与第二个矩阵的对应列相乘来计算。这个概念被用来开发以下所示的内核函数：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The kernel function takes two input arrays and one output array as arguments.
    The size of a matrix is passed as a constant to the kernel function. This removes
    the need to pass the size of a vector as one of the parameters of the kernel function,
    as was explained earlier in the chapter. Both methods are equally correct, and
    it is up to the programmer which they find more convenient. Each thread computes
    one element of a resultant matrix. All elements of the row from the first matrix
    and columns from the second matrix are multiplied and summed up inside the `for`
    loop. The answer is copied to the location in the resultant matrix. The details
    of calculating the index inside the kernel function can be found in earlier chapters
    of the book. The Python code to use this kernel function is shown as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 内核函数接受两个输入数组和一个输出数组作为参数。矩阵的大小作为常量传递给内核函数。这样就消除了需要将向量的大小作为内核函数参数之一的需求，正如本章前面所解释的那样。两种方法都是正确的，取决于程序员认为哪种更方便。每个线程计算结果矩阵的一个元素。第一矩阵的行和第二矩阵的列的所有元素在`for`循环内相乘并求和。答案被复制到结果矩阵中的相应位置。内核函数内部计算索引的细节可以在本书的早期章节中找到。以下是如何使用此内核函数的Python代码：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Two matrices with the size of 3 x 3 are initialized with random integers from
    `1` to `5`. These matrices are uploaded to the device memory using the `to_gpu`
    method of the `gpuarray` class. The empty GPU array is created to store the result
    on the device. These three variables are passed as arguments to the kernel function.
    The kernel function is called with the matrix size as dimensions in *x* and *y*
    directions. The result is downloaded back to the host using the `get()` method.
    The two input matrices and the result calculated by the GPU are printed on the
    console. The matrix multiplication is also calculated on a CPU using the dot method
    of the `numpy` library. The result is compared with the GPU result for verifying
    the result of the kernel computation. The result of the program is displayed as
    follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 两个大小为3 x 3的矩阵被初始化为从`1`到`5`的随机整数。这些矩阵使用`gpuarray`类的`to_gpu`方法上传到设备内存。创建一个空的GPU数组以在设备上存储结果。这三个变量作为参数传递给内核函数。内核函数调用时，矩阵大小作为*x*和*y*方向的维度。结果使用`get()`方法下载回主机。两个输入矩阵和GPU计算的结果在控制台上打印。使用`numpy`库的dot方法在CPU上计算矩阵乘法。结果与内核计算的结果进行比较，以验证内核计算的结果。程序的结果如下所示：
- en: '![](img/ff3f4d41-f609-45da-8617-9491925e791b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ff3f4d41-f609-45da-8617-9491925e791b.png)'
- en: To summarize, We have developed a simple kernel function to perform matrix multiplication
    using PyCUDA. This kernel function can be further optimized by using shared memory,
    as explained earlier in the book.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们已经开发了一个简单的内核函数，使用PyCUDA执行矩阵乘法。这个内核函数可以通过使用共享内存进一步优化，正如本书前面所解释的那样。
- en: Advanced kernel functions in PyCUDA
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA中的高级内核函数
- en: So far, we have seen the use of the `SourceModule` class for defining kernel
    functions in C or C++. We have also used the `gpuarray` class for doing device
    computations without defining kernel functions explicitly. This section describes
    the advanced kernel definition features available in PyCUDA. These features are
    used to develop kernel functions for various parallel communication patterns like
    the map, reduce, and scan operations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了使用`SourceModule`类在C或C++中定义内核函数的使用。我们还使用了`gpuarray`类来进行设备计算，而不需要显式定义内核函数。本节描述了PyCUDA中可用的高级内核定义功能。这些功能用于开发各种并行通信模式的内核函数，如映射、归约和扫描操作。
- en: Element-wise kernel in PyCUDA
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA中的元素级内核
- en: 'This feature allows the programmer to define a kernel function that works on
    every element of an array. It allows the programmer to execute the kernel on complex
    expressions that are made of one or more operands into a single computational
    step. The kernel function for the element-wise addition of a large array can be
    defined in the following way:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特性允许程序员定义一个内核函数，该函数作用于数组的每个元素。它允许程序员将一个或多个操作数组成的复杂表达式执行为一个单一的计算步骤。以下是这样定义大型数组元素级加法内核函数的方式：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `PyCuda.elementwise.ElementwiseKernel` function is used to define the element-wise
    kernel function. It requires three arguments. The first argument is the list of
    parameters for the kernel function. The second argument defines the operation
    to be performed on each element, and the third argument specifies the name of
    the kernel function. The Python code to use this kernel function is shown as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`PyCuda.elementwise.ElementwiseKernel`函数来定义元素级核函数。它需要三个参数。第一个参数是核函数的参数列表。第二个参数定义了对每个元素要执行的操作，第三个参数指定了核函数的名称。以下是如何使用此核函数的Python代码示例：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The two arrays are initialized with random numbers using the `curand` function
    of the `pycuda.curandom` class. It is again a useful functionality, as it removes
    the need to initialize on the host and then upload to the device memory. An empty
    GPU array is created to store the result. The `add` kernel is called by passing
    these three variables as an argument. The time needed for the addition of a million
    elements is calculated using CUDA events and is displayed on the console.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pycuda.curandom`类中的`curand`函数用随机数初始化两个数组。这又是一个有用的功能，因为它消除了在主机上初始化然后上传到设备内存的需要。创建一个空的GPU数组来存储结果。通过将这些三个变量作为参数传递来调用`add`核函数。使用CUDA事件计算一百万个元素加法所需的时间，并在控制台上显示。
- en: 'The output of the program is shown as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的输出如下所示：
- en: '![](img/93b4cd61-1772-4525-8105-8c726b838bc4.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93b4cd61-1772-4525-8105-8c726b838bc4.png)'
- en: The element-wise kernel only needs 0.6 ms for the addition of a million elements
    in an array. This performance is better than the program seen earlier in this
    chapter. So element-wise, kernel definition is a very important concept to remember
    when element-wise operations are to be performed on a vector.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 元素级核函数只需要0.6毫秒即可完成数组中一百万个元素的加法操作。这种性能优于本章前面看到的程序。因此，当要对向量执行元素级操作时，元素级核定义是一个非常重要的概念需要记住。
- en: Reduction kernel
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归约核
- en: 'The reduction operation can be defined as reducing a collection of elements
    to a single value by using some expressions. It can be very useful in various
    parallel computation applications. The example of calculating the dot product
    between vectors is taken to demonstrate the concept of reduction in PyCUDA. The
    program for calculating the dot product using the feature of a reduction kernel
    in PyCUDA is shown as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 归约操作可以通过使用某些表达式将元素集合归约到单个值来定义。它在各种并行计算应用中非常有用。以计算向量点积的例子来展示PyCUDA中的归约概念。以下是如何使用PyCUDA中归约核功能计算点积的程序示例：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: PyCUDA provides the `pycuda.reduction.ReductionKernel` class to define reduction
    kernels. It requires many arguments. The first argument is the data type for the
    output. The second argument is neutral, which is mostly defined as `0`. The third
    argument is the expression used to reduce the collection of elements. The addition
    operation is defined in the preceding code. The fourth argument is defined as
    the expression used for mapping operations between operands before reduction.
    The element-wise multiplication is defined in the code. The final argument defines
    the argument for the kernel function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA提供了`pycuda.reduction.ReductionKernel`类来定义归约核。它需要许多参数。第一个参数是输出数据类型。第二个参数是中值，通常定义为`0`。第三个参数是用于归约元素集合的表达式。在前面代码中定义了加法操作。第四个参数定义为归约前操作数之间映射操作的表达式。在代码中定义了元素级乘法。最后一个参数定义了核函数的参数。
- en: 'The reduction kernel for computing the dot product requires element-wise multiplication
    between two vectors and then the addition of all elements. Two vectors are defined
    usingthe `arange` function. It works in a similar way as the `range` function
    in Python but `arange` saves the array on the device. The kernel function is called
    by passing these two vectors as an argument, and the result is fetched to the
    host. The time needed for computing is calculated using CUDA events and displayed
    on the console along with the result of the dot product, shown as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 计算点积的归约核函数需要两个向量之间的元素级乘法，然后对所有元素进行加法。使用`arange`函数定义了两个向量。它在Python中的`range`函数类似，但`arange`会将数组保存在设备上。通过将这些两个向量作为参数传递来调用核函数，并将结果检索到主机。使用CUDA事件计算所需的计算时间，并在控制台上与点积的结果一起显示，如下所示：
- en: '![](img/87bdb5b4-fcfa-4ac9-acbd-a4a99f4a819a.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87bdb5b4-fcfa-4ac9-acbd-a4a99f4a819a.png)'
- en: The reduction kernel takes around 2.5 s to compute the dot product, which is
    a relatively long time compared to the explicit kernel seen in the last section.
    Still, it is quite useful in parallel computing applications where reduction operation
    is required.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 减少内核计算点积大约需要2.5秒，与上一节中看到的显式内核相比，这是一个相对较长时间。然而，在需要减少操作的并行计算应用中，它非常有用。
- en: Scan kernel
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扫描内核
- en: 'The scan operation is again a very important parallel computing paradigm. The
    scan operator applies a specified function to the first item in the input. The
    result of that function is provided as the input along with the second item from
    the original input. All the intermediate results form the output sequence. This
    concept can be used for various applications. The example of a cumulative addition
    is taken as an example to demonstrate the concept of the scan kernel in PyCUDA.
    The cumulative addition is nothing but applying addition to every element of a
    vector sequentially. The example is shown as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描操作再次是一个非常重要的并行计算范式。扫描操作符将指定的函数应用于输入的第一个元素。该函数的结果作为输入提供，并带有原始输入的第二个元素。所有中间结果形成输出序列。这个概念可以用于各种应用。以累积加法为例，演示了PyCUDA中的扫描内核概念。累积加法不过是将加法应用于向量的每个元素，顺序进行。示例如下：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As can be seen, the output of the previous addition is added to the current
    element to calculate the output at the current position. This is called an **inclusive
    scan** operation. If the current element of the input is not involved, then it
    is known as an **exclusive scan**. The program to perform cumulative summation
    using an inclusive scan is shown as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，前一次加法的结果被添加到当前元素中，以计算当前位置的输出。这被称为**包含扫描**操作。如果输入的当前元素不参与，则称为**排除扫描**。使用包含扫描执行累积求和的程序如下所示：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'PyCUDA provides the `pycuda.scan.InclusiveScanKernel` class to define an inclusive
    scan kernel. It requires the data type of the output and the operation to be used
    for scanning as arguments. The addition operation is specified for a cumulative
    summation. The array with random integers is applied as an input to this kernel
    function. The kernel output will have the same size as the input. The input and
    output vector along with the time needed for calculating the cumulative sum is
    displayed on the console, as shown in the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA提供了`pycuda.scan.InclusiveScanKernel`类来定义一个包含扫描内核。它需要输出数据类型和用于扫描的操作作为参数。对于累积求和，指定了加法操作。随机整数的数组被应用于这个内核函数的输入。内核输出将与输入具有相同的大小。输入和输出向量以及计算累积和所需的时间将在控制台上显示，如下所示：
- en: '![](img/02280329-9c5d-498d-bc3c-83fdfd6646dd.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02280329-9c5d-498d-bc3c-83fdfd6646dd.png)'
- en: It takes around 2 ms to run a scan operation on 10 elements of an array. To
    summarize, in this section we saw various special methods for defining kernels
    for mapping, reduction, and scanning operations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个数组的10个元素上运行扫描操作大约需要2毫秒。总结来说，在本节中，我们看到了定义映射、减少和扫描操作内核的各种特殊方法。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter demonstrated the concepts of programming in PyCUDA. It started
    with the development of a simple `Hello, PyCUDA` program using PyCUDA. The concepts
    of kernel definition in C or C++ and calling it from Python code and the API for
    accessing GPU device properties from a PyCUDA program were discussed in detail.
    The execution mechanism for multiple threads and blocks in a PyCUDA program was
    explained with a simple program. The basic structure of a PyCUDA program was described
    with a simple example of an array addition. The simplification of PyCUDA code
    was described by using directives from a driver class. The use of CUDA events
    to measure the performance of the PyCUDA programs was explained in detail. The
    functionality of the `inout` directive of the driver class and the `gpuarray`
    class was explained using an element-wise squaring example. The `gpuarray` class
    was used to develop code for calculating the dot product using PyCUDA. The PyCUDA
    code for a complex mathematical operation of matrix multiplication was explained
    in detail. The last part of the chapter described various kernel-defining methods
    used for mapping, reduction, and scanning operations.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了 PyCUDA 编程的概念。它从使用 PyCUDA 开发一个简单的 `Hello, PyCUDA` 程序开始。详细讨论了在 C 或 C++ 中定义内核以及在
    Python 代码中调用它的概念，以及从 PyCUDA 程序中访问 GPU 设备属性的 API。通过一个简单的程序解释了 PyCUDA 程序中多线程和多块执行机制。使用一个数组加法的简单示例描述了
    PyCUDA 程序的基本结构。通过驱动类指令描述了 PyCUDA 代码的简化。详细解释了使用 CUDA 事件来衡量 PyCUDA 程序性能的方法。使用逐元素平方示例解释了驱动类中的
    `inout` 指令和 `gpuarray` 类的功能。使用 `gpuarray` 类开发了使用 PyCUDA 计算点积的代码。详细解释了 PyCUDA 中用于矩阵乘法等复杂数学运算的代码。本章的最后部分描述了用于映射、归约和扫描操作的多种内核定义方法。
- en: The next chapter will build on this knowledge and describe some advanced kernels
    available in PyCUDA along with the development of computer vision applications
    using PyCUDA.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将在此基础上构建知识，并描述 PyCUDA 中可用的某些高级内核以及使用 PyCUDA 开发计算机视觉应用程序。
- en: Questions
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which programming language is used to define the kernel function using the `SourceModule`
    class in PyCUDA? Which compiler will be used to compile this kernel function?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 PyCUDA 中使用 `SourceModule` 类定义内核函数时，使用哪种编程语言？将使用哪种编译器来编译这个内核函数？
- en: Write a kernel call function for the `myfirst_kernel` function used in this
    chapter, with the number of blocks equal to 1024 x 1024 and threads per block
    equal to 512 x 512.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为本章中使用的 `myfirst_kernel` 函数编写一个内核调用函数，块的数量等于 1024 x 1024，每个块中的线程数等于 512 x 512。
- en: 'State true or false: The block execution inside PyCUDA program is in sequential
    order.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断对错：PyCUDA 程序内部的块执行是按顺序进行的。
- en: What is the advantage of using the `In`, `Out` ,and `inout` driver class primitives
    in PyCUDA programs?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 PyCUDA 程序中使用 `In`、`Out` 和 `inout` 驱动类原语的优势是什么？
- en: Write a PyCUDA program to add two to every element of a vector with an arbitrary
    size using the `gpuarray` class.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个 PyCUDA 程序，使用 `gpuarray` 类将向量中每个元素的值增加 2，向量的大小是任意的。
- en: What is the advantage of using CUDA events to measure the time for a kernel
    execution?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 CUDA 事件来测量内核执行时间的优势是什么？
- en: 'State true or false: The `gpuarray` class is the GPU device version of the
    `numpy` library in Python.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断对错：`gpuarray` 类是 Python 中 `numpy` 库的 GPU 设备版本。
