- en: Working with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw the procedure to install PyCUDA for Windows and
    Linux operating systems. In this chapter, we will start by developing the first
    PyCUDA program that displays a string on the console. It is very important to
    know and access the device properties of the GPU on which PyCUDA is running; the
    method for doing this will be discussed in detail in this chapter. We will also
    look at the execution of threads and blocks for a kernel in PyCUDA. The important
    programming concepts for any CUDA programming, such as allocating and deallocating
    the memory on the device, transferring data from host to device and vice versa,
    and the kernel call will be discussed in detail, using an example of the vector
    addition program. The method to measure the performance of PyCUDA programs using
    CUDA events and to compare it with the CPU program will also be discussed. These
    programming concepts will be used to develop some complex PyCUDA programs, such
    as the squaring of elements in an array and matrix multiplication. The last part
    of the chapter describes some advanced methods to define kernel functions in PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing the first "Hello, PyCUDA!" program in PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing device properties from a PyCUDA program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread and block execution in PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic PyCUDA programming concepts using a vector addition program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the performance of PyCUDA programs using CUDA events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some complex programs in PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced kernel functions in PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a good understanding of the Python programming language.
    It also requires any computer or laptop with the Nvidia GPU onboard. All the code
    used in this chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2QPWojV](http://bit.ly/2QPWojV)'
  prefs: []
  type: TYPE_NORMAL
- en: Writing the first program in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the procedure for writing a simple "Hello, PyCUDA!" program
    using PyCUDA. It will demonstrate the workflow for writing any PyCUDA programs.
    As Python is an interpreted language, the code can also be run line by line from
    the Python terminal, or it can be saved with the `.py` extension and executed
    as a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program for displaying a simple string from the kernel using PyCUDA is
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step while developing PyCUDA code is to include all libraries needed
    for the code. The `import` directive is used to include a library, module, class,
    or function in a file. This is similar to including a directive in C or C++, and
    it can be done in three different ways, as shown in the following steps. The use
    of three imported modules is also shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pycuda.driver` as `drv`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This indicates that the driver submodule of the pymodule is imported and it
    is given a short notation `drv` , so wherever functions from the `pycuda.driver`
    module are to be used then they can be used as `drv.functionname`. This module
    contains memory management functions, device properties, data direction functions,
    and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Import `pycuda.autoinit`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This command indicates the `autoint` module from `pycuda` is imported. It is
    not given any shorthand notation. The `autoint` module is used for device initialization,
    context creation, and memory cleanup. This module is not mandatory, and all the
    above functions can also be done manually.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From `pycuda.compiler` import `SourceModule`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This command indicates that only the `SourceModule` class from the `pycuda.compiler`
    module is imported. This is important when you only want to use one class of a
    large module. The `SourceModule` class is used to define C-like kernel functions
    in PyCUDA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The C or C++ kernel code is fed as a constructor to the `Sourcemodule` class
    and the mod object is created. The kernel code is very simple as it is just printing
    a `Hello, PyCUDA!` string on the console. As the `printf` function is used inside
    kernel code, it is very important to include the `stdio.h` header file. The `myfirst_kernel`
    function is defined inside the kernel code using the `__global__` directive to
    indicate that the function will be executed on the GPU. The function does not
    take any arguments. It just prints a string on the console. This kernel function
    will be compiled by the `nvcc` compiler.
  prefs: []
  type: TYPE_NORMAL
- en: This function can be used inside Python code by creating a pointer to the function
    using the `get_function` method of the `mod` object. The name of the kernel function
    is given as arguments in quotes. The pointer variable can be given any name. This
    pointer variable is used to call the kernel in the last line of the code. The
    arguments to the kernel function can be specified here, but the `myfirst_kernel`
    function does not have any arguments, so no arguments are specified. The number
    of threads per block and blocks per grid to be launched for a kernel can also
    be provided as an argument by using optional block and grid arguments. The block
    argument is given a value of (1,1,1) which is a 1 x 3 Python tuple, which indicates
    a block size of 1 x 1 x 1\. So one thread will be launched that will print the
    string on the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the program is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5ea51cf-3057-4c87-8f71-b868bd39c37e.png)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, this section demonstrated the procedure to develop a simple PyCUDA
    program step by step.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel call
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The device code that is written using ANSI C keywords along with CUDA extension
    keywords is called a **kernel**. It is launched from a Python code by a method
    called **Kernel Call**. Basically, the meaning of a kernel call is that we are
    launching a device code from the host code. A kernel call typically generates
    a large number of blocks and threads to exploit data parallelism on a GPU. Kernel
    code is very similar to that of normal C functions; it is just that this code
    is executed by several threads in parallel. It has a very simple syntax in Python,
    which can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It starts with the pointer of the kernel function that we want to launch. You
    should make sure that this kernel pointer is created using the `get_function`
    method. Then, it can include parameters of the kernel function separated by a
    comma. The block parameter indicates the number of threads to be launched, and
    the grid parameter indicates the number of blocks in the grid. The block and grid
    parameters are specified using a 1 x 3 Python tuple, which indicates blocks and
    threads in three dimensions. The total number of threads started by a kernel launch
    will be the multiplication of these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing GPU device properties from PyCUDA program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyCUDA provides a simple API to find information such as, which CUDA-enabled
    GPU devices (if any) are present and which capabilities each device supports.
    It is important to find out the properties of a GPU device that is being used
    before writing PyCUDA programs so that the optimal resources of the device can
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program for displaying all properties of CUDA-enabled devices on a system
    by using PyCUDA is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First, it is important to get a count of how many CUDA-enabled devices are present
    on the system, as a system may contain more than one GPU-enabled device. This
    count can be determined by the `drv.Device.count()` function of a driver class
    in PyCUDA. All the devices present on a system are iterated to determine the properties
    of each device. A pointer object to each device is created using the `drv.Device`
    function. This pointer is used to determine all properties of a particular device.
  prefs: []
  type: TYPE_NORMAL
- en: The `name` function will give the name of a particular device and `total_memory`
    will give the size of the GPU global memory available on the device. The other
    properties are stored as a Python dictionary that can be fetched by the `get_attributes().items()`
    function. This is converted to a list of tuples by using list comprehension in
    Python. All the rows of this list contain the 2 x 1 tuple, which has the name
    of the property and its value.
  prefs: []
  type: TYPE_NORMAL
- en: 'This list is iterated using the `for` loop to display all the properties on
    the console. This program is executed on the laptop with a GeForce 940 GPU and
    CUDA 9\. The output of the program is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f52a087b-5ec6-4407-9326-48effac47ab0.png)'
  prefs: []
  type: TYPE_IMG
- en: The properties were discussed in detail in earlier chapters of the book, so
    we won't discuss them again; however, to summarize, this section demonstrated
    the method to access GPU device properties from a PyCUDA program.
  prefs: []
  type: TYPE_NORMAL
- en: Thread and block execution in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw in the *A k**ernel call* section that we can start multiple blocks and
    multiple threads in parallel. So, in which order do these blocks and threads start
    and finish their execution? It is important to know this if we want to use the
    output of one thread in other threads. To understand this, we have modified the
    kernel in the `hello,PyCUDA!` program, seen in the earlier section, by including
    a print statement in a kernel call, which prints the block number. The modified
    code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen from the code, we are launching a kernel with 10 blocks in parallel,
    with each block having a single thread. In the kernel code, we are printing the
    block ID of the kernel execution. We can think of that as 10 copies of the same
    `myfirstkernel` start execution in parallel. Each of these copies will have a
    unique block ID, which can be accessed by the `blockIdx.x` directive, and unique
    thread ID, which can be accessed by `threadIdx.x`. These IDs will tell us which
    block and thread are executing the kernel. When you run the program many times,
    you will find that each time, blocks execute in different orders. One sample output
    can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2aa20d82-b407-4168-8d84-f97a577bd75a.png)'
  prefs: []
  type: TYPE_IMG
- en: It can produce *n* factorial number of different outputs, where *n* indicates
    the number of blocks started in parallel. So, whenever you are writing the program
    in PyCUDA, you should be careful that blocks execute in random order.
  prefs: []
  type: TYPE_NORMAL
- en: Basic programming concepts in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start developing some useful stuff using PyCUDA in this section. The
    section will also demonstrate some useful functions and directives of PyCUDA,
    using a simple example of adding two numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Adding two numbers in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python provides a very fast library for numerical operations which is called
    **numpy (Numeric Python)**. It is developed in C or C++ and is very useful for
    array manipulations in Python. It is used frequently in PyCUDA programs as arguments
    to PyCUDA kernel functions are passed as numpy arrays. This section explains how
    to add two numbers using PyCUDA. The basic kernel code for adding two numbers
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SourceModule` class and driver class are imported as explained earlier.
    The `numpy` library is also imported as it will be required for passing arguments
    to the kernel code. The `add_num` kernel function is defined as a constructor
    to the `SourceModule` class. The function takes two device pointers as input and
    one device pointer that points to the answer of the addition as output. It is
    important to note that, though we are adding two numbers, the kernel function
    is defined so that it can work on two array additions as well. Two single numbers
    are nothing but two arrays with one element each. If there aren’t any errors,
    this code will be compiled and loaded onto the device. The code to call this kernel
    code from Python is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The pointer reference to the kernel function is created using `get_function`.
    Two random numbers are created using the `numpy.random.randn(1)` function, which
    is used to create a random number in the normal distribution. These numbers are
    converted to single precision floating point numbers, using the `astype(numpy.float32)`
    method. The numpy array to store the result on the host is initialized to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The memory on the device can be allocated using the `mem_alloc` function of
    a driver class in PyCUDA. The size of memory is passed as an argument to the function.
    The size for input is found using the `h_a.nbytes` function. PyCUDA provides a
    `memcpy` function in the driver class to copy data from host memory to the device
    memory and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `drv.memcpy_htod` function copies data from the host memory to the device
    memory. The pointer to the device memory is passed as the first argument and the
    host memory pointer is passed as the second argument. The `add_num` kernel is
    called by passing device pointers as arguments along with numbers that specify
    the number of blocks and threads to be launched. In the code given before, one
    block is launched with one thread. The result computed by the kernel is copied
    back to the host by using the `drv.memcpy_dtoh` function. The result is displayed
    on the console, which is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9455fd86-f37e-406b-a16f-e7e86be45476.png)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, this section demonstrated the structure of a PyCUDA program. It
    started with a kernel definition code. Then inputs are defined in Python. The
    memory is allocated on the device and inputs are transferred to the device memory.
    This is followed by a kernel call, which will compute the result. This result
    is transferred to the host for further processing. PyCUDA provides even simpler
    APIs to do this operation, which is explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying the addition program using driver class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyCUDA provides an even simpler API for kernel calling that does not require
    memory allocation and memory copying. It is done implicitly by the API. This can
    be accomplished by using the `In` and `Out` functions of the driver class in PyCUDA.
    The modified array addition code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Ten elements of an array are added instead of single elements in the preceding
    code. The kernel function is exactly the same as the code seen previously. Two
    arrays of ten random numbers are created on the host. Now instead of creating
    the memory of them and transferring that to the device, the kernel is called directly.
    The kernel call is modified by specifying the direction of data using `drv.Out`
    or `drv.In`. It simplifies the PyCUDA code and reduces the size of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel is called with one block and *N* threads per block. These *N* threads
    add *N* elements of the array in parallel, which accelerates the addition operation.
    The result of the kernel is automatically downloaded to the host memory by using
    the `drv.out` directive so this result is directly printed on the console using
    the `for` loop. The result for an addition of ten elements using PyCUDA is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82f501bd-24c0-4e17-84e7-bec4ddd25f74.png)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, this section described the important concepts and functions of
    PyCUDA by taking a simple array addition program. The performance improvement
    of using PyCUDA can be quantified using CUDA events, which are explained in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance of PyCUDA programs using CUDA events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have not determined the performance of the PyCUDA programs explicitly.
    In this section, we will see how to measure the performance of the programs using
    CUDA events. This is a very important concept in PyCUDA because it will allow
    you to choose the best performing algorithms for a particular application from
    many options.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use Python time measuring options for measuring the performance of CUDA
    programs, but it will not give accurate results. It will include the time overhead
    of thread latency in the OS and scheduling in the OS among many other factors.
    The time measured using the CPU will also depend on the availability of a high-precision
    CPU timer. Many times, the host is performing asynchronous computations while
    the GPU kernel is running, and hence CPU timers of Python may not give correct
    times for kernel executions. So, to measure the time for GPU kernel computations,
    PyCUDA provides an event API.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CUDA event is a GPU timestamp recorded at a specified point in a PyCUDA program.
    In this API, the GPU records the timestamp, which eliminates the issues that were
    present when using CPU timers for measuring performance. There are two steps to
    measure time using CUDA events: creating an event and recording an event. We can
    record two events, one at the start of our code and one at the end. Then we will
    try to calculate the difference in time between the two events that will give
    an overall performance for our code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyCUDA code, the following lines can be included to measure performance
    using a CUDA event API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `record` method is used to measure a current timestamp. The timestamp is
    measured before and after the kernel code to measure time for the kernel execution.
    The difference between timestamps can be measured using the `time_till` method,
    as shown in the preceding code. It will give time in milliseconds, which is converted
    to seconds. In the next section, we will try to measure the performance of code
    using a CUDA event.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance of PyCUDA using large array addition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will demonstrate the method to use CUDA events to measure the
    performance of PyCUDA programs. The comparison of the performance of PyCUDA code
    with simple Python code is also described. The arrays with a million elements
    are taken so that performance can be accurately compared. The kernel code for
    large array addition is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As the number of elements is high, multiple blocks and threads are launched.
    So, both the thread ID and block ID are used to calculate the thread index. If
    the total number of threads launched is not equal to the number of elements, then
    multiple elements are added by the same thread. This is done by the `while` loop
    inside the kernel function. It will also ensure that the thread index does not
    go beyond the array elements. Apart from the input array and the output array,
    the size of an array is also taken as a parameter for the kernel function, as
    Python global variables are not accessible to kernel code in `SourceModule`. The
    Python code for adding large arrays is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Two events, `start` and `stop` are created to measure timings for the GPU code.The
    `Event()` function from the driver class is used to define event objects. Then,
    the pointer reference to the kernel function is created using the `get_function`.
    Two arrays with a million elements each are initialized with random numbers using
    the `randn` function of the `numpy` library. It will generate floating point numbers
    so they are converted to the single precision number to speed up computation on
    the device.
  prefs: []
  type: TYPE_NORMAL
- en: Each block supports 1,024 threads as we saw in the device property section.
    So based on that, the total number of blocks is calculated by dividing *N* by
    1,024\. It can be a float value so it is converted to next highest integer value
    using the `ceil` function of the `numpy` library. Then the kernel is launched
    with the calculated value number of blocks and 1,024 threads per block. The size
    of the array is passed with the `numpy.uint32` datatype.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time is recorded before and after calling the kernel function using the
    record function, and the time difference is calculated to measure the timing of
    the kernel function. The calculated time is printed on the console. To compare
    this performance with CPU timings, the following code is added to the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The time library from Python is used to measure CPU timings. The `for` loop
    is used to iterate through every element in an array. (Note: you can also use
    `h_result1 = h_a + h_b` as both arrays are numpy arrays.) Time is measured before
    and after the `for` loop using the `time.time()` function and the difference between
    this time is printed on the console. The output from the program is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbdc8dab-65bd-46de-bd99-aabacaede9b0.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the output, GPU takes 9.4 ms to add a million elements,
    while the CPU takes 415.15 ms, so around a 50-times improvement can be accomplished
    by using a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, this section demonstrated the use of events to measure timings
    for GPU code. The performance of the GPU is compared with CPU performance to quantify
    the performance improvement while using the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Complex programs in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PyCUDA syntax and terminology will be familiar by now. We will use this
    knowledge to develop advanced programs and learn some advanced concepts in PyCUDA.
    In this section, we will develop a program to square elements of an array using
    three different methods in PyCUDA. We will also learn the code for doing matrix
    multiplication in PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Element-wise squaring of a matrix in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, the program to perform element-wise squaring of numbers in
    a matrix is performed using three different methods. While doing this, the concepts
    of using multidimensional threads and blocks, the `inout` directive of the driver
    class, and the `gpuarray` class is explained in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Simple kernel invocation with multidimensional threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A simple kernel function using PyCUDA to square every element of a matrix is
    implemented in this section. The kernel function of squaring every element of
    a 5 x 5 matrix is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel function square takes only one device pointer that points to the
    matrix as input and replaces every element with the square of it. As multidimensional
    threads are launched, the thread index in both *x* and *y* directions are used
    to index the value from a matrix. You can assume that a 5 x 5 matrix is flattened
    to a 1 x 25 vector to understand the indexing mechanism. Please, note that the
    size of a matrix is hardcoded to `5` in this code, but it can also be user-defined
    like the size of an array in the last section. The Python code to use this kernel
    function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Two events are created to measure timings of a kernel function. The matrix of
    5 x 5 is initialized with random numbers on the host. It is done by using a `randint`
    function of the `numpy.random` module. It requires three arguments. The first
    two arguments define the range of numbers used to generate random numbers. The
    first argument is the minimum value and the second argument is the maximum value
    used for generating numbers. The third argument is the size, which is specified
    as a tuple (5,5). This generated matrix is again converted to a single precision
    number for faster processing. The memory for the matrix is allocated on the device
    and the generated matrix of random numbers is copied to it.
  prefs: []
  type: TYPE_NORMAL
- en: The pointer reference to the kernel function is created and the kernel is called
    by passing a device memory pointer as an argument. The kernel is called with multidimensional
    threads with the value of 5 in the *x* and *y* directions. So the total number
    of threads launched is 25, with each thread calculating a square of a single element
    of the matrix. The result calculated by the kernel is copied back to the host
    and displayed on the console. The time needed for the kernel is displayed on the
    console along with the input and output matrix. The output is displayed on the
    console.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3635a78-4477-4e62-a0a8-a8497255a5c5.png)'
  prefs: []
  type: TYPE_IMG
- en: It takes 149 ms to calculate the square of each element of a 5 x 5 matrix. The
    same calculation can be simplified by using the `inout` directive of the driver
    class. This is explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using inout with the kernel invocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As can be seen from the kernel function of the program in the last section,
    the same array is used as both input and output. The driver module in PyCUDA provides
    an `inout` directive for this kind of cases. It removes the need for the separate
    allocation of memory for the array, uploading it to the device and downloading
    the result back to the host. All operations are performed simultaneously during
    a kernel call. This makes the code simpler and easier to read. The Python code
    for using the `inout` directive of driver class is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The CUDA events are initialized to measure the performance of the code using
    the `inout` directive. The kernel call is the same as in the last section so it
    is not repeated here. As can be seen, while calling the square kernel, a single
    variable is passed as an argument with the `drv.inout` directive. So, all device-related
    operations are performed in this single step. The kernel is called with multidimensional
    threads as is the case in the last section. The computed result and the time taken
    is printed on the console as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e6b4841-76fc-4b09-aa48-80f126cdd70b.png)'
  prefs: []
  type: TYPE_IMG
- en: The time taken is comparatively less than the original kernel. So, by using
    the `inout` directive the of driver class, the PyCUDA code can be made efficient
    and easy to read. PyCUDA also provides a `gpuarray` class for array-related operations.
    It can be also used for squaring operations, which is explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using gpuarray class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python provides a `numpy` library for numeric computations in Python. PyCUDA
    provides a `gpuarray` class, similar to `numpy`, that stores its data and performs
    its computations on the GPU device. The shape and datatype of the arrays work
    exactly as in `numpy`. The `gpuarray` class provides many arithmetic methods for
    computations. It removes the need to specify the kernel code in C or C++ using
    `SourceModule`. So, the PyCUDA code will contain only a Python code. The code
    of squaring every element of the matrix using the `gpuarray` class is shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gpuarray` class needs to be imported for using in the code. It is available
    in the `pycuda.gpuarray` module. The matrix is initialized with random integers
    from 1 to 5 for computation. This matrix is uploaded to the device memory by using
    the `to_gpu()` method of the `gpuarray` class. The matrix to be uploaded is provided
    as an argument to this method. The matrix is converted to a single precision number.
    All the operations on this uploaded matrix will be performed on the device. The
    square operation is performed in a similar way as we do in Python code but, as
    the variable is stored on the device using `gpuarray`, this operation will also
    be performed on the device. The result is downloaded back to the host by using
    the `get` method. This result along with the time needed to perform element-wise
    squaring using `gpuarray` is displayed on the console as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c1561d4-76d2-420f-9718-56e7c2a1578c.png)'
  prefs: []
  type: TYPE_IMG
- en: It takes around 58 ms to compute the square. It completely removes the need
    to define kernel functions in C language, and its functionality is similar to
    the `numpy` library so Python programmers can easily work with it.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in this section we have developed an element-wise squaring program
    using PyCUDA in three different fashions. We have also seen the concepts of multidimensional
    threads, the `inout` directive and the `gpuarray` class in PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Dot product using GPU array
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dot between two vectors is an important mathematical operation used in
    various applications. The `gpuarray` class used in the last section can be used
    to calculate the dot product between two vectors. The performance of the `gpuarray`
    method to calculate the dot product is compared with the `numpy` operation. The
    code used to calculate the dot product using `numpy` is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Two vectors with 100 elements each are initialized with random integers for
    calculating the dot product. The time module of Python is used to calculate the
    time needed for the computation of the dot product. The `*` operator is used to
    calculate the element-wise multiplication of two vectors and the result of this
    is summed up to calculate the overall dot product. Please, note that the `numpy.dot`
    method calculated is used in matrix multiplication, which can''t be used for the
    dot product. The calculated dot product and time are displayed on the console.
    The code to perform the same operation on a GPU using `gpuarray` is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `to_gpu` method is used to upload two vectors on a GPU for calculating
    the dot product. The `gpuarray` class provides a dot method, which can be used
    to calculate the dot product directly. It needs two GPU arrays as an argument.
    The calculated result is downloaded back to the host by using the `get()` method.
    The calculated result and time measured using CUDA events are displayed on the
    console. The result of the program is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a712df91-3835-4c33-a470-33af90a81546.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the output, that same result is obtained by calculating
    the dot product using `numpy` and `gpuarray`. The `numpy` library takes 37 ms
    to compute the dot product, while the GPU takes only 0.1 ms to do the same operation.
    This further exemplifies the benifit of using GPU and PyCUDA to do complex mathematical
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An important mathematical operation used frequently is matrix multiplication.
    This section will demonstrate how it can be performed on a GPU using PyCUDA. It
    is a very complicated mathematical operation when the sizes of the matrix are
    very large. It should be kept in mind that for matrix multiplication, the number
    of columns in the first matrix should be equal to the number of rows in the second
    matrix. Matrix multiplication is not a cumulative operation. To avoid complexity,
    in this example, we are taking a square matrix of the same size. If you are familiar
    with the mathematics of matrix multiplication, then you may recall that a row
    in the first matrix will be multiplied with all the columns in the second matrix.
    This is repeated for all rows in the first matrix. The example of a 3 x 3 matrix
    multiplication is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acf80d05-4b69-4582-8bc5-e5e48c2c7b97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every element in the resultant matrix will be calculated by multiplying the
    corresponding row in the first matrix and column in the second matrix. This concept
    is used to develop a kernel function shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel function takes two input arrays and one output array as arguments.
    The size of a matrix is passed as a constant to the kernel function. This removes
    the need to pass the size of a vector as one of the parameters of the kernel function,
    as was explained earlier in the chapter. Both methods are equally correct, and
    it is up to the programmer which they find more convenient. Each thread computes
    one element of a resultant matrix. All elements of the row from the first matrix
    and columns from the second matrix are multiplied and summed up inside the `for`
    loop. The answer is copied to the location in the resultant matrix. The details
    of calculating the index inside the kernel function can be found in earlier chapters
    of the book. The Python code to use this kernel function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Two matrices with the size of 3 x 3 are initialized with random integers from
    `1` to `5`. These matrices are uploaded to the device memory using the `to_gpu`
    method of the `gpuarray` class. The empty GPU array is created to store the result
    on the device. These three variables are passed as arguments to the kernel function.
    The kernel function is called with the matrix size as dimensions in *x* and *y*
    directions. The result is downloaded back to the host using the `get()` method.
    The two input matrices and the result calculated by the GPU are printed on the
    console. The matrix multiplication is also calculated on a CPU using the dot method
    of the `numpy` library. The result is compared with the GPU result for verifying
    the result of the kernel computation. The result of the program is displayed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff3f4d41-f609-45da-8617-9491925e791b.png)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, We have developed a simple kernel function to perform matrix multiplication
    using PyCUDA. This kernel function can be further optimized by using shared memory,
    as explained earlier in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced kernel functions in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen the use of the `SourceModule` class for defining kernel
    functions in C or C++. We have also used the `gpuarray` class for doing device
    computations without defining kernel functions explicitly. This section describes
    the advanced kernel definition features available in PyCUDA. These features are
    used to develop kernel functions for various parallel communication patterns like
    the map, reduce, and scan operations.
  prefs: []
  type: TYPE_NORMAL
- en: Element-wise kernel in PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This feature allows the programmer to define a kernel function that works on
    every element of an array. It allows the programmer to execute the kernel on complex
    expressions that are made of one or more operands into a single computational
    step. The kernel function for the element-wise addition of a large array can be
    defined in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `PyCuda.elementwise.ElementwiseKernel` function is used to define the element-wise
    kernel function. It requires three arguments. The first argument is the list of
    parameters for the kernel function. The second argument defines the operation
    to be performed on each element, and the third argument specifies the name of
    the kernel function. The Python code to use this kernel function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The two arrays are initialized with random numbers using the `curand` function
    of the `pycuda.curandom` class. It is again a useful functionality, as it removes
    the need to initialize on the host and then upload to the device memory. An empty
    GPU array is created to store the result. The `add` kernel is called by passing
    these three variables as an argument. The time needed for the addition of a million
    elements is calculated using CUDA events and is displayed on the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the program is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93b4cd61-1772-4525-8105-8c726b838bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: The element-wise kernel only needs 0.6 ms for the addition of a million elements
    in an array. This performance is better than the program seen earlier in this
    chapter. So element-wise, kernel definition is a very important concept to remember
    when element-wise operations are to be performed on a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Reduction kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reduction operation can be defined as reducing a collection of elements
    to a single value by using some expressions. It can be very useful in various
    parallel computation applications. The example of calculating the dot product
    between vectors is taken to demonstrate the concept of reduction in PyCUDA. The
    program for calculating the dot product using the feature of a reduction kernel
    in PyCUDA is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: PyCUDA provides the `pycuda.reduction.ReductionKernel` class to define reduction
    kernels. It requires many arguments. The first argument is the data type for the
    output. The second argument is neutral, which is mostly defined as `0`. The third
    argument is the expression used to reduce the collection of elements. The addition
    operation is defined in the preceding code. The fourth argument is defined as
    the expression used for mapping operations between operands before reduction.
    The element-wise multiplication is defined in the code. The final argument defines
    the argument for the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reduction kernel for computing the dot product requires element-wise multiplication
    between two vectors and then the addition of all elements. Two vectors are defined
    usingthe `arange` function. It works in a similar way as the `range` function
    in Python but `arange` saves the array on the device. The kernel function is called
    by passing these two vectors as an argument, and the result is fetched to the
    host. The time needed for computing is calculated using CUDA events and displayed
    on the console along with the result of the dot product, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87bdb5b4-fcfa-4ac9-acbd-a4a99f4a819a.png)'
  prefs: []
  type: TYPE_IMG
- en: The reduction kernel takes around 2.5 s to compute the dot product, which is
    a relatively long time compared to the explicit kernel seen in the last section.
    Still, it is quite useful in parallel computing applications where reduction operation
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: Scan kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scan operation is again a very important parallel computing paradigm. The
    scan operator applies a specified function to the first item in the input. The
    result of that function is provided as the input along with the second item from
    the original input. All the intermediate results form the output sequence. This
    concept can be used for various applications. The example of a cumulative addition
    is taken as an example to demonstrate the concept of the scan kernel in PyCUDA.
    The cumulative addition is nothing but applying addition to every element of a
    vector sequentially. The example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen, the output of the previous addition is added to the current
    element to calculate the output at the current position. This is called an **inclusive
    scan** operation. If the current element of the input is not involved, then it
    is known as an **exclusive scan**. The program to perform cumulative summation
    using an inclusive scan is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'PyCUDA provides the `pycuda.scan.InclusiveScanKernel` class to define an inclusive
    scan kernel. It requires the data type of the output and the operation to be used
    for scanning as arguments. The addition operation is specified for a cumulative
    summation. The array with random integers is applied as an input to this kernel
    function. The kernel output will have the same size as the input. The input and
    output vector along with the time needed for calculating the cumulative sum is
    displayed on the console, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02280329-9c5d-498d-bc3c-83fdfd6646dd.png)'
  prefs: []
  type: TYPE_IMG
- en: It takes around 2 ms to run a scan operation on 10 elements of an array. To
    summarize, in this section we saw various special methods for defining kernels
    for mapping, reduction, and scanning operations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated the concepts of programming in PyCUDA. It started
    with the development of a simple `Hello, PyCUDA` program using PyCUDA. The concepts
    of kernel definition in C or C++ and calling it from Python code and the API for
    accessing GPU device properties from a PyCUDA program were discussed in detail.
    The execution mechanism for multiple threads and blocks in a PyCUDA program was
    explained with a simple program. The basic structure of a PyCUDA program was described
    with a simple example of an array addition. The simplification of PyCUDA code
    was described by using directives from a driver class. The use of CUDA events
    to measure the performance of the PyCUDA programs was explained in detail. The
    functionality of the `inout` directive of the driver class and the `gpuarray`
    class was explained using an element-wise squaring example. The `gpuarray` class
    was used to develop code for calculating the dot product using PyCUDA. The PyCUDA
    code for a complex mathematical operation of matrix multiplication was explained
    in detail. The last part of the chapter described various kernel-defining methods
    used for mapping, reduction, and scanning operations.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will build on this knowledge and describe some advanced kernels
    available in PyCUDA along with the development of computer vision applications
    using PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which programming language is used to define the kernel function using the `SourceModule`
    class in PyCUDA? Which compiler will be used to compile this kernel function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a kernel call function for the `myfirst_kernel` function used in this
    chapter, with the number of blocks equal to 1024 x 1024 and threads per block
    equal to 512 x 512.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State true or false: The block execution inside PyCUDA program is in sequential
    order.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the advantage of using the `In`, `Out` ,and `inout` driver class primitives
    in PyCUDA programs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a PyCUDA program to add two to every element of a vector with an arbitrary
    size using the `gpuarray` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the advantage of using CUDA events to measure the time for a kernel
    execution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State true or false: The `gpuarray` class is the GPU device version of the
    `numpy` library in Python.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
