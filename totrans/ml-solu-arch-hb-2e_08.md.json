["```py\n!pip install transformers\n!pip install ipywidgets \n```", "```py\nimport logging\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AdamW, BertForSequenceClassification, BertTokenizer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom types import SimpleNamespace\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout)) \n```", "```py\nfilepath = './data/all-data.csv'\ndata = pd.read_csv(filepath, encoding=\"ISO-8859-1\",\nheader=None, usecols=[0, 1],\nnames=[\"sentiment\", \"article\"])\nord_enc = OrdinalEncoder()\ndata[\"sentiment\"] = ord_enc.fit_transform(data[[\"sentiment\"]])\ndata = data.astype({'sentiment':'int'})\ntrain, test = train_test_split(data)\ntrain.to_csv(\"./data/train.csv\", index=False)\ntest.to_csv(\"./data/test.csv\", index=False)\nMAX_LEN = data.article.str.len().max()  # this is the max length of the sentence \n```", "```py\ndef get_data_loader(batch_size, training_dir, filename):\n    logger.info(\"Get data loader\")\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n    dataset = pd.read_csv(os.path.join(training_dir, filename))\n    articles = dataset.article.values\n    sentiments = dataset.sentiment.values\n    input_ids = []\n    for sent in articles:\n        encoded_articles = tokenizer.encode(sent, add_special_tokens=True)\n        input_ids.append(encoded_articles)\n...\n       return tensor_dataloader \n```", "```py\ndef train(args):\n    use_cuda = args.num_gpus > 0\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    # set the seed for generating random numbers\n    torch.manual_seed(args.seed)\n    if use_cuda:\n        torch.cuda.manual_seed(args.seed)\n    train_loader = get_data_loader(args.batch_size, args.data_dir, args.train_file)\n    test_loader = get_data_loader(args.test_batch_size, args.data_dir, args.test_file)\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\",  \n        num_labels=args.num_labels,\n        output_attentions=False,  \n        output_hidden_states=False,  )\n... \nreturn model \n```", "```py\ndef test(model, test_loader, device):\n    def get_correct_count(preds, labels):\n        pred_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return np.sum(pred_flat == labels_flat), len(labels_flat)\n\n    model.eval()\n    _, eval_accuracy = 0, 0\n    total_correct = 0\n    total_count = 0\n...\n    logger.info(\"Test set: Accuracy: %f\\n\", total_correct/total_count) \n```", "```py\nargs = SimpleNamespace(num_labels=3, batch_size=16, test_batch_size=10, epochs=3, lr=2e-5, seed=1,log_interval =50, model_dir = \"model/\", data_dir=\"data/\", num_gpus=1, train_file = \"train.csv\", test_file=\"test.csv\")\nmodel = train(args) \n```", "```py\ndef input_fn(request_body, request_content_type):\n    if request_content_type == \"application/json\":\n        data = json.loads(request_body)\n        if isinstance(data, str):\n            data = [data]\n        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], str):\n            pass\nelse:\n            raise ValueError(\"Unsupported input type. Input type can be a string or a non-empty list. \\\n                             I got {}\".format(data))\n\n        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\n        input_ids = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n\n        # pad shorter sentence\n        padded =  torch.zeros(len(input_ids), MAX_LEN)\n        for i, p in enumerate(input_ids):\n            padded[i, :len(p)] = torch.tensor(p)\n\n        # create mask\n        mask = (padded != 0)\n\n        return padded.long(), mask.long()\n    raise ValueError(\"Unsupported content type: {}\".format(request_content_type)) \n```", "```py\ndef predict_fn(input_data, model):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    input_id, input_mask = input_data\n    input_id = input_id.to(device)\n    input_mask = input_mask.to(device)\n    with torch.no_grad():\n        y = model(input_id, attention_mask=input_mask)[0]\n    return y \n```", "```py\nimport json\nprint(\"sentiment label : \" + str(np.argmax(preds)))\narticle = \"Operating profit outpaced the industry average\"\nrequest_body = json.dumps(article)\nenc_data, mask = input_fn(request_body, 'application/json')\noutput = predict_fn((enc_data, mask), model)\npreds = output.detach().cpu().numpy()\nprint(\"sentiment label : \" + str(np.argmax(preds))) \n```", "```py\nimport argparse\nimport logging\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AdamW, BertForSequenceClassification, BertTokenizer\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n...\n    train(parser.parse_args()) \n```", "```py\ntransformers==2.3.0 \n```", "```py\nimport os\nimport numpy as np\nimport pandas as pd\nimport sagemaker\nsagemaker_session = sagemaker.Session()\nbucket = <bucket name>\nprefix = \"sagemaker/pytorch-bert-financetext\"\nrole = sagemaker.get_execution_role()\ninputs_train = sagemaker_session.upload_data(\"./data/train.csv\", bucket=bucket, key_prefix=prefix)\ninputs_test = sagemaker_session.upload_data(\"./data/test.csv\", bucket=bucket, key_prefix=prefix) \n```", "```py\nfrom sagemaker.pytorch import PyTorch\noutput_path = f\"s3://{bucket}/{prefix}\"\nestimator = PyTorch(\n    entry_point=\"train.py\",\n    source_dir=\"code\",\n    role=role,\n    framework_version=\"1.6\",\n    py_version=\"py3\",\n    instance_count=1,  \n    instance_type=\"ml.p3.2xlarge\",\n    output_path=output_path,\n    hyperparameters={\n        \"epochs\": 4,\n        \"lr\" : 5e-5,\n        \"num_labels\": 3,\n        \"train_file\": \"train.csv\",\n        \"test_file\" : \"test.csv\",\n        \"MAX_LEN\" : 315,\n        \"batch-size\" : 16,\n        \"test-batch-size\" : 10\n    }\n)\nestimator.fit({\"training\": inputs_train, \"testing\": inputs_test}) \n```", "```py\nimport logging\nimport os\nimport sys\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertForSequenceClassification, BertTokenizer\n... \ndef model_fn(model_dir):\n    ...\n    loaded_model = BertForSequenceClassification.from_pretrained(model_dir)\n    return loaded_model.to(device)\ndef input_fn(request_body, request_content_type):\n    ...\ndef predict_fn(input_data, model):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    ...\n    return y \n```", "```py\nfrom sagemaker.pytorch.model import PyTorchModel\nmodel_data = estimator.model_data\npytorch_model = PyTorchModel(model_data=model_data,\n                             role=role,\n                             framework_version=\"1.6\",\n                             source_dir=\"code\",\n                             py_version=\"py3\",\n                             entry_point=\"inference.py\")\npredictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\") \n```", "```py\npredictor.serializer = sagemaker.serializers.JSONSerializer()\npredictor.deserializer = sagemaker.deserializers.JSONDeserializer()\nresult = predictor.predict(\"The market is doing better than last year\")\nprint(\"predicted class: \", np.argmax(result, axis=1)) \n```", "```py\npredictor.delete_endpoint() \n```"]