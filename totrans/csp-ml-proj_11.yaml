- en: What's Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come a long way. From the basics and steps for building **machine learning**
    (**ML**) models to actually developing numerous ML models for various real-world
    projects, we have covered a lot so far. After a brief introductory chapter, where
    we learned the basics of ML and the essential steps that go into building ML models,
    we started building ML models. In [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering* and [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter
    Sentiment Analysis*, we discussed building classification models using text datasets.
    In [Chapter 4](part0045.html#1AT9A0-5ebdf09927b7492888e31e8436526470), *Foreign
    Exchange Rate Forecast* and [Chapter 5](part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470), *Fair
    Value of House and Property*, we used financial and real estate property data
    to build regression models. Then in [Chapter 6](part0073.html#25JP20-5ebdf09927b7492888e31e8436526470), *Customer
    Segmentation*, we covered how to use clustering algorithms to draw intuitive insights
    into customer behavior using the e-commerce dataset. In [Chapter 7](part0082.html#2E6E40-5ebdf09927b7492888e31e8436526470), *Music
    Genre Recommendation* and [Chapter 8](part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470),
    *Handwritten Digit Recognition*, we expanded our knowledge of building ML models
    to build music recommendation and image recognition models using music records
    and handwritten digit image data. In [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470),
    *Cyber Attack Detection* and [Chapter 10](part0132.html#3TSA80-5ebdf09927b7492888e31e8436526470), *Credit
    Card Fraud Detection* we built anomaly detection models for cyber attack detection
    and credit card fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to review the types of ML model we have built,
    the projects we have worked on so far, and code snippets for training various
    ML models using the Accod.NET framework. We will also discuss some of the challenges
    when using and applying ML in real-life projects and situations. Lastly, we are
    going to cover some of the other software packages that can be used for future
    ML projects, as well as other common technologies that are frequently used by
    data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A review of what we have learned so far
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-life challenges in building ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other common technologies used by data scientists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the first chapter onward, we have discussed and covered a large amount
    of material. From discussing the basics of ML to building classification, regression,
    and clustering models, it is worth reviewing what we have done so far before we
    end this book. Let's review some of the essential concepts and code that will
    be helpful for your future C# ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for building ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 1](part0020.html#J2B80-5ebdf09927b7492888e31e8436526470), *Basics
    of Machine Learning Modeling*, it can be challenging for aspiring data scientists
    and ML engineers to understand the flow and approaches to building real-world
    ML models that will be used in production systems. We have discussed the steps
    for building machine learning models in detail in [Chapter 1](part0020.html#J2B80-5ebdf09927b7492888e31e8436526470),
    *Basics of Machine Learning Modeling*, and we have followed those steps in each
    of the projects that we have worked on so far. The following diagram should be
    a good recap of the essential steps in building real-world ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you should already, we always start a ML project with the problem definition.
    In this step, we define the problems that we are going to solve with ML and why
    we need ML models to solve such problems. This is also the step where we brainstorm
    our ideas and the prerequisites, such as the types of data required, as well as
    the types of learning algorithms that we are going to experiment with. Lastly,
    this is where we need to clearly define the success criteria for the project.
    We can define some evaluation metrics not only for the prediction performance
    of ML models, but also the execution performance of your models, especially if
    the models need to be run in a real-time system, and output the prediction results
    within a given time window.
  prefs: []
  type: TYPE_NORMAL
- en: From the problem definition phase, we move on to the data collection step. For
    those projects that we have worked on in this book, we used publicly available
    data that was already compiled and labeled. However, in real-world situations,
    data might not be available to start with. In this case, we will have to come
    up with approaches to collect the data. For example, if we are planning to build
    ML models for user behavior predictions for users on our website or application,
    then we can collect user activities on the website or application. On the other
    hand, if we are building a credit model to score the credit worthiness of potential
    borrowers, most likely we will not be able to collect data ourselves. In this
    case, we will have to resort to third-party data vendors who sell credit-related
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have gathered all of our data, the next thing we will have to do is
    prepare and analyze the data. During the data preparation step, we will need to
    validate the dataset by looking at the formats of the data fields, the existence
    of duplicate records, or the number of missing values. With these criteria checked,
    we can then start analyzing the data to see if there is any noticeable pattern
    in the dataset. If you recall, we typically analyzed the target variable distribution
    first and then we started analyzing the distributions of the features for each
    of the target classes to identify any noticeable patterns that could separate
    the target classes from each other. During the data analysis step, we focused
    on gaining some insights into the patterns in the data, as well as the structure
    of the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: With insight and understanding of the data from the data analysis step, we can
    then start building features that will be used for our ML models. As Andrew Ng
    mentioned, applied ML is basically feature engineering. This is one of the most
    critical steps in building ML models and in determining the performance of our
    prediction models. If you recall, we discussed how to use one-hot encoding to
    transform text features into an encoded matrix of 1s and 0s for our text classification
    problems. We also discussed building time series features, such as moving averages
    and Bollinger Bands and using log transformations for highly skewed features,
    when we were building regression models. This feature engineering step is where
    we need to be creative.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have all the features ready, we can then move on to training and testing
    various learning algorithms. Depending on whether the target variable is continuous
    or categorical, we can decide whether to build a classification model or regression
    model. If you recall from previous projects, we trained and tested our models
    by using k-fold cross-validation or by splitting the dataset into two subsets
    and training with one group and testing with another hold-out group. Until we
    find the model that we are satisfied with, we will have to repeat the the previous
    steps. If we do not have enough data, we will have to go back to the data collection
    phase and try to collect more data for more accurate models. If we handled duplicate
    records or missing values poorly, we will have to go back to the data preparation
    step to clean up the data. If we can build more and better features, then repeating
    the feature engineering step can help by improving the performance of our ML models.
  prefs: []
  type: TYPE_NORMAL
- en: The last step in building ML models is to deploy them to production systems.
    All the models should have been fully tested and validated by this point. It will
    be beneficial to have some monitoring tools in place before the deployment, so
    that the performance of the models can be monitored.
  prefs: []
  type: TYPE_NORMAL
- en: We have followed these steps quite thoroughly throughout the chapters, so you
    will realize how comfortable and familiar with these steps you are when you start
    working on your future ML projects. However, there are a couple of essential steps
    that we could not fully cover in this book, such as the data collection and model
    deployment steps, so you should always keep in mind the importance and goals of
    those steps.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This first two ML models we built in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering* and [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter
    Sentiment Analysis*, were classification models. In [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering*, we built a classification model to classify emails into spam
    and ham (non-spam emails). In [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter
    Sentiment Analysis*, we built a classification model for Twitter sentiment analysis,
    where the model classified each tweet into one of the three emotions—positive,
    negative, and neutral. Classification problems are common among ML projects. Building
    a model to predict whether a customer will buy an item in an online store is a
    classification problem. Building a model to predict whether a borrower will pay
    back his/her loan is also a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: If there are only two classes in the target variable, typically a positive outcome
    and a negative outcome, then we call it a binary classification. A good example
    of a binary classification problem is the spam email filtering project that we
    did in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering*. If there are more than two classes in the target variable, then
    we call it a multi-class or multinomial classification. We had a case of having
    to classify a record into three different classes in the Twitter sentiment analysis
    project in [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter
    Sentiment Analysis*; this was a good example of a multinomial classification problem.
    We had two more classification projects in this book. If you recall, we had eight
    different genres or classes in our target variable for the Music Genre Recommendation
    project in [Chapter 7](part0082.html#2E6E40-5ebdf09927b7492888e31e8436526470), *Music
    Genre Recommendation*, and we had 10 different digits in our target variable for
    the handwritten digit recognition project in [Chapter 8](part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470),
    *Handwritten Digit Recognition*.
  prefs: []
  type: TYPE_NORMAL
- en: We experimented with numerous learning algorithms, such as logistic regression,
    Naive Bayes, **Support Vector Machine** (**SVM**), random forest, and neural network,
    for the aforementioned classification projects. To remind you how to train these
    learning algorithms in C#, we will reiterate how we initialized some of those
    learning algorithms in C# using the Accord.NET framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how we can train a binary logistic regression
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For multinomial classification problems, we trained a logistic regression classifier
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When building a Naive Bayes classifier, we used the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you recall, we used `NormalDistribution` when the features had continuous
    variables, as in the case of the Music Genre Recommendation project, where all
    the features were audio spectrum features and had continuous values. One the other
    hand, we used `BernoulliDistribution`, where the features can only take binary
    values (0 versus 1). In the case of the Twitter sentiment analysis project in
    [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter Sentiment
    Analysis*, all the features we had could only take 0s or 1s.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we could train a `RandomForestLearning` classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you might already be known, we could tune hyperparameters, such as the number
    of trees in the random forest (`NumberOfTrees`), the proportion of variables that
    can be used at maximum by each tree (`CoverageRatio`), and the proportion of samples
    used to train each of the trees (`SampleRatio`), to find better performing random
    forest models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the following code to train a SVM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you recall, we could use different kernels for SVMs. On top of the `Gaussian`
    kernel, we could use `Linear` and `Polynomial` kernels as well. Depending on the
    type of dataset you have, one kernel works better than the others and various
    kernels should be tried to find the best performing SVM model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we could train a neural network using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you might recall from [Chapter 8](part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470), *Handwritten
    Digit Recognition*, we trained a neural network model by running it through the
    dataset multiple times (epochs). After each iteration or epoch, we noticed the
    error rate decreased, as the neural network learned more and more from the dataset.
    We also noticed that in each epoch, the rate of improvements in the error rate
    was in diminishing return, so after enough epochs there would be no significant
    improvement in the performance of a neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: You can view the code samples at the following link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClassificationModelReview.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClassificationModelReview.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have also developed multiple regression ML models. In [Chapter 4](part0045.html#1AT9A0-5ebdf09927b7492888e31e8436526470),
    *Foreign Exchange Rate Forecast*, we worked on the Foreign Exchange Rate Forecast
    project, where we built models that could predict future exchange rates between
    Euros and US dollars. In [Chapter 5](part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470), *Fair
    Value of House and Property*, we trained different ML models that could predict
    house prices for the Fair Value of House and Property project. Regression problems
    are also common in real-world ML projects. Building a model that predicts the
    lifetime value of a customer is a regression problem. Building a model that predicts
    the maximum amount of money that a potential borrower can borrow without going
    bankrupt is another regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: We have explored numerous machine learning algorithms for regression projects
    in this book. We have experimented with linear regression and linear SVM models
    in [Chapter 4](part0045.html#1AT9A0-5ebdf09927b7492888e31e8436526470), *Foreign
    Exchange Rate Forecast* for the Foreign Exchange Rate Forecast project. We have
    also tried using different kernels, such as `Polynomial` and `Guassian` kernels,
    for SVM models in [Chapter 5](part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470), *Fair
    Value of House and Property* for the Fair Value of House and Property project.
    To remind you how to train these regression models in C#, we will reiterate how
    we could use C# and the Accord.NET framework to build these models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how we can train a linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When building a SVM with the linear kernel, we used the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you might recall, `Epsilon`, `Tolerance`, and `UseComplexityHeuristic` are
    hyperparameters that can be tuned further for better model performance. When building
    a SVM model, we recommend you try various combinations of the hyperparameters
    to find the best performing model for your business case.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want to use a polynomial kernel for a SVM, we can use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For a `Polynomial` kernel, you can tune the degree of a polynomial function.
    For example, for a second degree polynomial (quadratic) kernel, you can initialize
    the kernel with `new Polynomial(2)`. Similarly, for a fourth degree polynomial
    kernel, you can initialize the kernel with `new Polynomial(4)`. However, increasing
    the complexity of a kernel can result in overfitting, so you will need to take
    care when using a high-degree polynomial kernel for a SVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want to build a SVM with a Gaussian kernel, we can use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can find the code samples for the aforementioned regression models at the
    following link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/RegressionModelReview.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/RegressionModelReview.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed one unsupervised learning algorithm, k-means clustering, and how
    it can be used to draw insights from an unlabeled dataset. In [Chapter 6](part0073.html#25JP20-5ebdf09927b7492888e31e8436526470),
    *Customer Segmentation*, we used the k-means clustering algorithm on an e-commerce
    dataset and we learned about different customer behaviors from the dataset. We
    have covered how to use clustering algorithms to build different customer segments,
    based on their purchase history, but there are many other applications of clustering
    algorithms. For example, clustering algorithms can also be used in image analysis,
    for example in partitioning images into sub-sections, and in bioinformatics, such
    as discovering groups of closely related genes (gene clustering).
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the following code to build a k-means clustering algorithm using C#
    and the Accord.NET framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you might recall, we need to give the number of clusters we want to build
    to the `KMeans` class. One way to programmatically decide the best number of clusters
    that we discussed was to look at the Silhouette score, which measures how similar
    a data point is to its own cluster. Using this Silhouette score, you can iterate
    through different numbers for the number of clusters and then decide which one
    works the best for the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code samples for the k-means clustering algorithm at the following
    link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClusteringAlgorithmReview.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClusteringAlgorithmReview.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Real-life challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It would be great if we could just build ML models for all of our business
    problems. However, that is normally not the case. Often, there are more challenges
    in getting to the model development phase than in actually building working models.
    We will discuss the following frequently appearing data science challenges when
    we are working on ML projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Data issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructural issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability versus accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having the right data and enough data is the most important prerequisite for
    building a working ML model. However, often, this is the most difficult part in
    developing ML models for a few different reasons. We will discuss a few common
    challenges that many data scientists face in terms of issues related to data.
  prefs: []
  type: TYPE_NORMAL
- en: First, the data needed might simply not exist. For example, think of a recently
    formed online retail store wanting to apply ML to understand or predict their
    customers' spending patterns. Since they are a new business with a small customer
    base, with not much historical purchase data, they will not have enough data for
    data scientists to work with. In this case, all they can do is wait for a better
    time to embark on ML projects, even if they have data scientists on their team.
    Their data scientists will simply not be able to build anything meaningful with
    a limited amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the dataset exists, but it is not accessible. This kind of problem happens
    often in big corporations. Due to security issues, accessing the data might have
    been restricted to certain subgroups of an organization. In this case, data scientists
    might have to go through multiple levels of approval from different departments
    or business entities or they might have to build a separate data pipeline, through
    which they can ingest the data that they need. This kind of issue typically means
    it takes a long time before data scientists can start working on the ML project
    that they wanted to work on.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the data is segmented or too messy. Almost all of the time, the raw
    datasets that data scientists get include messy data and come from different data
    sources. There might be too many missing values or too many duplicate records
    in the data and data scientists will have to spend lots of time cleaning up the
    raw dataset. The data might be too unstructured. This typically happens when you
    work with text-heavy datasets. In this case, you might have to apply various text
    mining and **natural language processing** (**NLP**) techniques to clean up the
    data and make it usable for building ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a ML model on a large dataset requires a large amount of memory and
    CPU resources. As we get bigger and bigger data, it is inevitable that we run
    into infrastructural issues. If you do not have enough memory resources for training
    ML models, you might end up getting *Out of Memory* exceptions after many hours
    or days of training models. If you do not have enough processing power, then training
    a complex ML model can take weeks and even months. Getting the right amount of
    computational resources is a real challenge in building ML models. As the data
    that is being used for ML grows faster than ever, the amount of computational
    resources required also grows significantly year after year.
  prefs: []
  type: TYPE_NORMAL
- en: With the emerging popularity of cloud computing service providers, such as AWS,
    Google, and Microsoft Azure, it became easier to get the required computational
    resources. On any of those cloud computing platforms, you can easily request and
    use the amount of memory and CPUs that you need. However, as everything comes
    with a price, running ML jobs on those cloud platforms can cost lots of money.
    Depending on your budget, such costs can restrict how much computational resources
    you can use for your ML tasks, and it needs to be planned cleverly.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability versus accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last common real-life challenge in ML is the trade-off between the explainability
    and accuracy of ML models. More traditional and linear models, such as logistic
    regression and linear regression models, are easy to explain in terms of the prediction
    output. We can extract the intercept and the coefficients of those linear models
    and we can get the prediction output using simple arithmetic operations. However,
    more complex models, such as random forest and SVM, are more difficult to use
    in terms of explaining the prediction output. Unlike logistic regression or linear
    regression models, we cannot deduce the prediction output from simple arithmetic
    operations. Those complex models work more like a black box. We know the input
    and the output, but what goes in between is a black box to us.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of explainability issue among complex learning algorithms becomes
    a problem when users or auditors request explanations about the model behavior.
    If there is such a requirement for explainability, we will have to resort to more
    traditional linear models, even if more complex models perform better than those
    linear models.
  prefs: []
  type: TYPE_NORMAL
- en: Other common technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the field of ML and data science is evolving faster than ever, the number
    of new technologies being built is also growing at a fast pace. There are many
    resources and tools that help in building ML solutions and applications more easily
    and quickly. We are going to discuss a few technologies and tools that we recommend
    you get acquainted with for your future ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Other ML libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Accord.NET framework that we have used throughout this book is one of the
    most frequently used and well documented frameworks for ML. However, other libraries
    that are built for ML in C# are worth mentioning and taking a look at for your
    future ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Encog** is a ML framework that can be used in Java and C#. It is very similar
    to the Accord.NET framework that we have been using, in the sense that is has
    a wide range of numerous ML algorithms available within the framework. This framework
    is well documented and has lots of sample code that can be referenced for your
    future machine learning projects. More information and documentation about the **Encog**
    framework can be found at the following link: [https://www.heatonresearch.com/encog/](https://www.heatonresearch.com/encog/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weka** is another ML framework, but it is different from the Accord.NET framework
    in the sense that the **Weka** framework is specifically engineered for data mining.
    It is broadly used by many researchers and has good documentation and even a book
    that explains how to use **Weka** for data mining. Weka is written in Java, but
    it can also be used in C#. More information about the **Weka** framework can be
    found at the following link: [https://www.cs.waikato.ac.nz/~ml/index.html](https://www.cs.waikato.ac.nz/~ml/index.html).
    Also, information about how to use the **Weka** framework in C# can be found at
    the following link: [https://weka.wikispaces.com/Use%20WEKA%20with%20the%20Microsoft%20.NET%20Framework](https://weka.wikispaces.com/Use%20WEKA%20with%20the%20Microsoft%20.NET%20Framework).'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you can always search in NuGet, the package manager for .NET, for any
    other machine learning frameworks for C#. Any library or package that is available
    on NuGet can easily be downloaded and referenced in your development environment.
    It is a good practice to search the following link for any packages you might
    need or that might be helpful for your future machine learning projects: [https://www.nuget.org/](https://www.nuget.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization libraries and tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next set of tools and packages that we are going to discuss is about data
    visualizations. ML and data visualization are an inseparable combination for data
    science. For any ML models that you build, you should be able to present your
    findings, model performance, and model results to users or business partners.
    Furthermore, for continuous model performance monitoring purposes, data visualization
    techniques are often used to identify any issues with the models in production
    systems or any potential deterioration in the model performance. As a result,
    many data visualization libraries were built to make data visualization tasks
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: '**LiveCharts** is a .NET library for data visualization. We have used the Accord.NET
    framework''s charting libraries throughout this book, but for more complex plots,
    we recommend using **LiveCharts**. From basic charts, such as line and bar charts,
    to complex interactive charts, you can build various visualizations in C# relatively
    easily. The **LiveCharts** library has thorough documentation and lots of examples
    along with sample code. You can find more information about how to use **LiveCharts**
    for data visualizations at the following link: [https://lvcharts.net/](https://lvcharts.net/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from the C#.NET library for data visualization tasks, there are two more
    data visualization tools that are frequently used in the data science community:
    **D3.js** and **Tableau**. **D3.js** is a JavaScript library for building and
    presenting charts on web pages. Often, this JavaScript library is used to create
    a dashboard for various data science and data visualization tasks. **Tableau**
    is a business intelligence tool, with which you can drag and drop to create various
    visualizations. This tool is frequently used to create a dashboard not only by
    data scientists, but also by non-data professionals. For more information about
    the **D3.js** library, you can follow this link: [https://d3js.org/](https://d3js.org/).
    For more information about Tableau, you can follow this link: [https://www.tableau.com/](https://www.tableau.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Technologies for data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lastly, we are going to discuss some commonly used technologies and tools for
    processing data. Throughout this book, we have mostly used CSV files as input
    for our ML modeling projects. We have used the Deedle framework to load, manipulate,
    and aggregate the data. However, often, the type of input data for ML projects
    varies. For some projects, the data might be stored in SQL databases. For other
    projects, the data might be stored across distributed filesystems. Furthermore,
    the source of the input data can even be from real-time streaming services. We
    will briefly discuss a few commonly used technologies for such cases and where
    to look for more detailed information in order for you to do further research.
  prefs: []
  type: TYPE_NORMAL
- en: SQL databases, such as SQL Server or PostgreSQL, are the most commonly used
    technologies for data storage and data processing. Using the SQL language, data
    scientists can easily retrieve, manipulate, and aggregate data to process and
    prepare the data for their ML projects. As an aspiring data scientist, it will
    be beneficial for you to become more comfortable with using the SQL language for
    processing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Another technology that is often used within the data science community is **Spark**,
    which is a cluster-computing framework. With **Spark**, you can process a large
    amount of data at scale. Using clusters of machines and distributing heavy computations
    across those machines, **Spark** helps in building scalable big data solutions.
    This technology is widely used among numerous organizations and companies, such
    as Netflix, Yahoo, and eBay, which have lots of data to process every day.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, there are numerous stream-processing technologies for real-time ML applications.
    One of the most popular ones is **Kafka**. This technology is often used when
    building real-time applications or data pipelines that need to continuously stream
    the data. In the case of building real-time ML applications, using a stream-processing
    technology, such as **Kafka**, will be essential for the successful delivery of
    a real-time ML product.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed what we have discussed so far in this book. We
    briefly went over the essential steps in building ML models. Then, we summarized
    and compiled the code to build various ML models in C# using the Accord.NET framework
    for classification, regression, and clustering problems. We have also discussed
    the real-life challenges that we could not cover in this book, but that you will
    most likely face when you start working on your future ML projects. We discussed
    challenges in accessing and compiling the data to build ML models, infrastructural
    challenges that will occur for big data, and the trade-offs between the explainability
    and accuracy of the ML models. Lastly, we covered some commonly used technologies
    that we recommend you get acquainted with for your future ML projects. The code
    libraries and tools that were mentioned in this chapter are only a subset of the
    tools that are available, and the commonly used tools and technologies are going
    to evolve year on year. We recommend you consistently research upcoming technologies
    for ML and data science.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered various ML techniques, tools, and concepts throughout this book.
    As you have worked through this book from building basic classification and regression
    models to complex recommendation and image recognition systems, as well as anomaly
    detection models for real-world problems, I hope you have gained more confidence
    in building ML models for your future ML projects. I hope your journey throughout
    this book was worthwhile and meaningful, and that you have learned and gained
    many new and useful skills.
  prefs: []
  type: TYPE_NORMAL
