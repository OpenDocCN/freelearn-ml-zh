- en: Clustering News Articles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类新闻文章
- en: In most of the earlier chapters, we performed data mining knowing what we were
    looking for. Our use of *target classes* allowed us to learn how our features
    model those targets during the training phase, which lets the algorithm set internal
    parameters to maximize its learning. This type of learning, where we have targets
    to train against, is called **supervised learning**. In this chapter, we'll consider
    what we do without those targets. This is **unsupervised learning** and it's much
    more of an exploratory task. Rather than wanting to classify with our model, the
    goal in unsupervised learning is to explore the data to find insights.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数早期章节中，我们进行数据挖掘时已知我们正在寻找什么。我们使用*目标类别*使我们能够在训练阶段了解我们的特征如何模拟那些目标，这使得算法能够设置内部参数以最大化其学习。这种有目标进行训练的学习类型被称为**监督学习**。在本章中，我们将考虑在没有那些目标的情况下我们做什么。这是**无监督学习**，它更多的是一种探索性任务。在无监督学习中，我们的目标不是用我们的模型进行分类，而是探索数据以发现洞察。
- en: In this chapter, we will look at clustering news articles to find trends and
    patterns in the data. We'll look at how we can extract data from different websites
    using a link aggregation website to show a variety of news stories.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何通过链接聚合网站提取数据，以展示各种新闻故事，从而对新闻文章进行聚类以发现数据中的趋势和模式。
- en: 'The key concepts covered in this chapter include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的关键概念包括：
- en: Using the reddit API to collect interesting news stories
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用reddit API收集有趣的新闻故事
- en: Obtaining text from arbitrary websites
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任意网站获取文本
- en: Cluster analysis for unsupervised data mining
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督数据挖掘的聚类分析
- en: Extracting topics from documents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档中提取主题
- en: Online learning for updating a model without retraining it
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线学习以更新模型而无需重新训练
- en: Cluster ensembling to combine different models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类集成以结合不同的模型
- en: Trending topic discovery
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 趋势主题发现
- en: In this chapter, we will build a system that takes a live feed of news articles
    and groups them together such that the groups have similar topics. You could run
    the system multiple times over several weeks (or longer) to see how trends change
    over that time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个系统，该系统可以接收新闻文章的实时流并将其分组，使得组内的文章具有相似的主题。你可以运行这个系统几周（或更长的时间）多次，以查看趋势是如何随时间变化的。
- en: Our system will start with the popular link aggregation website ([https://www.reddit.com](https://www.reddit.com/)),
    which stores lists of links to other websites, as well as a comments section for
    discussion. Links on reddit are broken into several categories of links, called
    **subreddits**. There are subreddits devoted to particular TV shows, funny images,
    and many other things. What we are interested in are the subreddits for news.
    We will use the */r/worldnews* subreddit in this chapter, but the code should
    work with any other text-based subreddit.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统将从流行的链接聚合网站([https://www.reddit.com](https://www.reddit.com/))开始，该网站存储指向其他网站的链接列表，以及一个用于讨论的评论部分。reddit上的链接被分为几个链接类别，称为**subreddits**。有专门针对特定电视节目、搞笑图片和其他许多事物的subreddits。我们感兴趣的是新闻的subreddits。在本章中，我们将使用*/r/worldnews*
    subreddits，但代码应该适用于任何其他基于文本的subreddits。
- en: 'In this chapter, our goal is to download popular stories and then cluster them
    to see any major themes or concepts that occur. This will give us an insight into
    the popular focus, without having to manually analyze hundreds of individual stories.
    The general process is to:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是下载流行的故事，然后对它们进行聚类，以查看任何主要主题或概念的出现。这将使我们能够了解流行的焦点，而无需手动分析数百个单独的故事。一般过程如下：
- en: Collect links to recent popular news stories from reddit.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从reddit收集最近流行的新闻故事的链接。
- en: Download the web page from those links.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些链接下载网页。
- en: Extract just the news story from the downloaded website.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅从下载的网站中提取新闻故事。
- en: Perform cluster analysis to find clusters of stories.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行聚类分析以找到故事集群。
- en: Analyse those clusters to discover trends.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析那些集群以发现趋势。
- en: Using a web API to get data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Web API获取数据
- en: We have used web-based APIs to extract data in several of our previous chapters.
    For instance, in [Chapter 7](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml)*, Follow
    Recommendations Using Graph Mining*, we used Twitter's API to extract data. Collecting
    data is a critical part of the data mining pipeline, and web-based APIs are a
    fantastic way to collect data on a variety of topics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的几章中已经使用基于Web的API提取数据。例如，在[第7章](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml)*使用图挖掘遵循推荐*中，我们使用了Twitter的API来提取数据。收集数据是数据挖掘流程中的关键部分，基于Web的API是收集各种主题数据的一种极好的方式。
- en: 'There are three things you need to consider when using a web-based API for
    collecting data: authorization methods, rate limiting, and API endpoints.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用基于Web的API收集数据时，您需要考虑三个因素：授权方法、速率限制和API端点。
- en: '**Authorization methods** allow the data provider to know who is collecting
    the data, in order to ensure that they are being appropriately rate-limited and
    that data access can be tracked. For most websites, a personal account is often
    enough to start collecting data, but some websites will ask you to create a formal
    developer account to get this access.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**授权方法**允许数据提供者知道谁在收集数据，以确保他们被适当限制速率，并且数据访问可以被追踪。对于大多数网站，一个个人账户通常足以开始收集数据，但某些网站会要求您创建一个正式的开发者账户以获取这种访问权限。'
- en: '**Rate limiting** is applied to data collection, particularly free services.
    It is important to be aware of the rules when using APIs, as they can and do change
    from website to website. Twitter''s API limit is 180 requests per 15 minutes (depending
    on the particular API call). Reddit, as we will see later, allows 30 requests
    per minute. Other websites impose daily limits, while others limit on a per-second
    basis. Even within websites, there are drastic differences for different API calls.
    For example, Google Maps has smaller limits and different API limits per-resource,
    with different allowances for the number of requests per hour.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**速率限制**应用于数据收集，尤其是免费服务。在使用API时了解规则非常重要，因为它们可以从网站到网站而变化。Twitter的API限制为每15分钟180次请求（取决于特定的API调用）。Reddit，如我们稍后将要看到的，允许每分钟30次请求。其他网站可能实施每日限制，而有些网站则按每秒限制。即使在网站内部，不同的API调用之间也可能存在巨大的差异。例如，Google
    Maps有更小的限制和不同的API限制，每个资源都有不同的每小时请求次数限制。'
- en: If you find you are creating an app or running an experiment that needs more
    requests and faster responses, most API providers have commercial plans that allow
    for more calls. Contact the provider for more details.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现您正在创建一个需要更多请求和更快响应的应用或运行实验，大多数API提供商都有商业计划，允许更多的调用。请联系提供商获取更多详情。
- en: '**API Endpoints** are the actual URLs that you use to extract information.
    These vary from website to website. Most often, web-based APIs will follow a RESTful
    interface (short for **Representational State Transfer**). RESTful interfaces
    often use the same actions that HTTP does: `GET`, `POST`, and `DELETE` are the
    most common. For instance, to retrieve information on a resource, we might use
    the following (example only) API endpoint:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**API端点**是您用来提取信息的实际URL。这些URL因网站而异。通常，基于Web的API将遵循RESTful接口（即**表示状态传输**）。RESTful接口通常使用HTTP相同的操作：`GET`、`POST`和`DELETE`是最常见的。例如，为了检索资源的信息，我们可能会使用以下（仅作示例）API端点：'
- en: '[www.dataprovider.com/api/resource_type/resource_id/](http://www.dataprovider.com/api/resource_type/resource_id/)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.dataprovider.com/api/resource_type/resource_id/](http://www.dataprovider.com/api/resource_type/resource_id/)'
- en: To get the information, we just send an HTTP `GET` request to this URL. This
    will return information on the resource with the given type and ID. Most APIs
    follow this structure, although there are some differences in the implementation.
    Most websites with APIs will have them appropriately documented, giving you details
    of all the APIs that you can retrieve.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取信息，我们只需向该URL发送一个HTTP `GET`请求。这将返回有关给定类型和ID的资源的信息。大多数API遵循这种结构，尽管在实现上存在一些差异。大多数具有API的网站都会对其进行适当的文档说明，为您提供可以检索的所有API的详细信息。
- en: 'First, we set up the parameters to connect to the service. To do this, you
    will need a developer key for reddit. In order to get this key, log into the site
    at [https://www.reddit.com/login](https://www.reddit.com/login) and go to [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps).
    From here, click on are you a developer? create an app... and fill out the form,
    setting the type as script. You will get your client ID and a secret, which you
    can add to a new Jupyter Notebook:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置连接到服务的参数。为此，您需要 Reddit 的开发者密钥。为了获取这个密钥，请登录到 [https://www.reddit.com/login](https://www.reddit.com/login)
    网站，然后转到 [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)。从这里，点击“您是开发者吗？创建一个应用...”，填写表格，将类型设置为脚本。您将获得客户端
    ID 和一个密钥，您可以将它们添加到一个新的 Jupyter Notebook 中：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Reddit also asks you (when you use their API) to set the user agent to a unique
    string that includes your username. Create a user agent string that uniquely identifies
    your application. I used the name of the book, chapter 10, and a version number
    of 0.1 to create my user agent, but it can be any string you like. Note that not
    doing this may result in your connection being heavily rate-limited:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit 还要求您（当您使用他们的 API 时）设置一个包含您用户名的唯一字符串作为用户代理。创建一个唯一标识您的应用程序的用户代理字符串。我使用了书名、第
    10 章，以及版本号 0.1 来创建我的用户代理，但可以是任何您喜欢的字符串。请注意，如果不这样做，可能会导致您的连接被大量限制：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In addition, you will need to log in to reddit using your username and password.
    If you don't have one already, sign up for a new one (it is free and you don't
    need to verify with personal information either).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还需要使用您的用户名和密码登录 Reddit。如果您还没有，可以免费注册一个新的账户（您不需要用个人信息进行验证）。
- en: You will need your password to complete the next step, so be careful before
    sharing your code to others to remove it. If you don't put your password in, set
    it to none and you will be prompted to enter it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要密码来完成下一步，所以在将代码分享给他人之前，请小心移除它。如果您不输入密码，将其设置为 none，您将被提示输入它。
- en: 'Now let''s create the username and password:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建用户名和密码：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we are going to create a function to log with this information. The reddit
    login API will return a token that you can use for further connections, which
    will be the result of this function. The code obtains the necessary information
    to log in to reddit, set the user agent, and then obtain an access token that
    we can use with future requests:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个函数来记录这些信息。Reddit 登录 API 将返回一个令牌，您可以使用它进行进一步的连接，这将是这个函数的结果。以下代码获取登录
    Reddit 所需的必要信息，设置用户代理，然后获取我们可以用于未来请求的访问令牌：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can call now our function to get an access token:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以调用我们的函数来获取访问令牌：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This token object is just a dictionary, but it contains the `access_token`
    string that we will pass along with future requests. It also contains other information
    such as the scope of the token (which would be everything) and the time in which
    it expires, for example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个令牌对象只是一个字典，但它包含我们将与未来请求一起传递的 `access_token` 字符串。它还包含其他信息，例如令牌的作用域（这将是一切）和它过期的时间，例如：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you are creating a production-level app, make sure you check the expiry of
    the token and to refresh it if it runs out. You'll also know this has happened
    if your access token stops working when trying to make an API call.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在创建一个生产级别的应用程序，请确保检查令牌的有效期，并在其过期时刷新它。您也会在尝试进行 API 调用时，如果访问令牌停止工作，知道这种情况已经发生。
- en: Reddit as a data source
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Reddit 作为数据源
- en: Reddit is a link aggregation website used by millions worldwide, although the
    English versions are US-centric. Any user can contribute a link to a website they
    found interesting, along with a title for that link. Other users can then upvote
    it, indicating that they liked the link, or downvote it, indicating they didn't
    like the link. The highest voted links are moved to the top of the page, while
    the lower ones are not shown. Older links are removed from the front page over
    time, depending on how many upvotes it has. Users who have stories upvoted earn
    points called karma, providing an incentive to submit only good stories.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit 是一个全球数百万用户使用的链接聚合网站，尽管其英文版本以美国为中心。任何用户都可以提交他们发现的有趣网站的链接，并为该链接添加标题。其他用户可以对其进行点赞，表示喜欢该链接，或者点踩，表示不喜欢该链接。得票最高的链接会被移至页面顶部，而得票较低的链接则不会显示。随着时间的推移，根据得票数，较旧的链接会被从首页移除。那些获得点赞的用户会获得称为
    karma 的积分，这为提交优质故事提供了激励。
- en: Reddit also allows non-link content, called self-posts. These contain a title
    and some text that the submitter enters. These are used for asking questions and
    starting discussions. For this chapter, we will be considering only link-based
    posts, and not comment-based posts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit还允许非链接内容，称为self-posts。这些包含提交者输入的标题和一些文本。这些用于提问和开始讨论。在本章中，我们将只考虑基于链接的帖子，而不是基于评论的帖子。
- en: Posts are separated into different sections of the website called subreddits.
    A subreddit is a collection of posts that are related. When a user submits a link
    to reddit, they choose which subreddit it goes into. Subreddits have their own
    administrators, and have their own rules about what is valid content for that
    subreddit.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 帖子被分为网站的不同部分，称为subreddit。subreddit是一系列相关的帖子。当用户向reddit提交链接时，他们会选择它进入哪个subreddit。subreddit有自己的管理员，并有自己的关于该subreddit有效内容的规则。
- en: By default, posts are sorted by **Hot**, which is a function of the age of a
    post, the number of upvotes, the number of downvotes it has received and how liberal
    the content is. There is also **New**, which just gives you the most recently
    posted stories (and therefore contains lots of spam and bad posts), and **Top**,
    which is the highest voted stories for a given time period. In this chapter, we
    will be using Hot, which will give us recent, higher-quality stories (there really
    are a lot of poor-quality links in New).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，帖子按**热门**排序，这是一个帖子年龄、点赞数、踩数以及内容自由度的函数。还有**新**，它只提供最近发布的帖子（因此包含大量垃圾邮件和差评帖子），以及**Top**，它是在给定时间段内获得最高票数的帖子。在本章中，我们将使用热门，这将给我们带来最近、质量较高的故事（在新中确实有很多低质量的链接）。
- en: 'Using the token we previously created, we can now obtain sets of links from
    a subreddit. To do that, we will use the /r/<subredditname> API endpoint that,
    by default, returns the Hot stories. We will use the /r/worldnews subreddit:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前创建的令牌，我们现在可以从一个subreddit中获取链接集合。为此，我们将使用/r/<subredditname> API端点，默认情况下返回热门故事。我们将使用/r/worldnews
    subreddit：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The URL for the previous endpoint lets us create the full URL, which we can
    set using string formatting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个端点的URL让我们可以创建完整的URL，我们可以通过字符串格式化来设置它：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we need to set the headers. This is needed for two reasons: to allow
    us to use the authorization token we received earlier and to set the user agent
    to stop our requests from being heavily restricted. The code is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置头部信息。这是出于两个原因：允许我们使用我们之前收到的授权令牌，并将用户代理设置为防止我们的请求受到严格限制。代码如下：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, as before, we use the requests library to make the call, ensuring that
    we set the headers:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像之前一样，我们使用requests库来发起调用，确保我们设置了头部信息：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Calling `json()` on this will result in a Python dictionary containing the
    information returned by reddit. It will contain the top 25 results from the given
    subreddit. We can get the title by iterating over the stories in this response.
    The stories themselves are stored under the dictionary''s data key. The code is
    as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个端点上调用`json()`将返回一个包含reddit返回信息的Python字典。它将包含给定subreddit的前25个结果。我们可以通过遍历此响应中的故事来获取标题。故事本身存储在字典的data键下。代码如下：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Getting the data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: Our dataset is going to consist of posts from the Hot list of the /r/worldnews
    subreddit. We saw in the previous section how to connect to reddit and how to
    download links. To put it all together, we will create a function that will extract
    the titles, links, and score for each item in a given subreddit.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集将包括来自/r/worldnews subreddit热门列表的帖子。我们在前面的部分中看到了如何连接到reddit以及如何下载链接。为了将这些内容整合在一起，我们将创建一个函数，该函数将提取给定subreddit中每个项目的标题、链接和分数。
- en: We will iterate through the subreddit, getting a maximum of 100 stories at a
    time. We can also do pagination to get more results. We can read a large number
    of pages before reddit will stop us, but we will limit it to 5 pages.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历subreddit，每次获取最多100个故事。我们还可以进行分页以获取更多结果。在reddit阻止我们之前，我们可以读取大量页面，但我们将将其限制为5页。
- en: 'As our code will be making repeated calls to an API, it is important to remember
    to rate-limit our calls. To do so, we will need the sleep function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的代码将反复调用API，因此记住对我们的调用进行速率限制非常重要。为此，我们需要sleep函数：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our function will accept a subreddit name and an authorization token. We will
    also accept a number of pages to read, though we will set a default of 5:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将接受一个subreddit名称和一个授权令牌。我们还将接受要读取的页数，尽管我们将默认设置为5：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We saw in [Chapter 7](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml)*, Follow Recommendations
    Using Graph Mining*, how pagination works for the Twitter API. We get a cursor
    with our returned results, which we send with our request. Twitter will then use
    this cursor to get the next page of results. The reddit API does almost exactly
    the same thing, except it calls the parameter after. We don't need it for the
    first page, so we initially set it to None. We will set it to a meaningful value
    after our first page of results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第7章](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml)*，使用图挖掘遵循建议*中看到，分页是如何在Twitter
    API中工作的。我们通过返回的结果获得一个游标，我们将它与我们的请求一起发送。Twitter将使用这个游标来获取下一页的结果。Reddit API几乎做了完全相同的事情，只是它调用参数在后面。我们不需要它用于第一页，所以我们最初将其设置为None。在我们的第一页结果之后，我们将将其设置为有意义的值。
- en: 'Calling the stories function is a simple case of passing the authorization
    token and the subreddit name:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 调用故事功能是一个简单的案例，只需传递授权令牌和subreddit名称：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The returned results should contain the title, URL, and 500 stories, which
    we will now use to extract the actual text from the resulting websites. Here is
    a sample of the titles that I received by running the script:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果应包含标题、URL和500个故事，我们将使用这些结果提取实际网站上的文本。以下是我运行脚本时收到的标题样本：
- en: '*Russia considers banning sale of cigarettes to anyone born after 2015*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*俄罗斯考虑禁止向2015年后出生的人出售香烟*'
- en: '*Swiss Muslim girls must swim with boys*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*瑞士穆斯林女孩必须与男孩一起游泳*'
- en: '*Report: Russia spread fake news and disinformation in Sweden - Russia has
    coordinated a campaign over the past 2years to influence Sweden’s decision making
    by using disinformation, propaganda and false documents, according to a report
    by researchers at The Swedish Institute of International Affairs.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*报告：俄罗斯在瑞典散布虚假新闻和误导信息 - 根据瑞典国际事务研究所研究人员的一份报告，俄罗斯在过去两年中通过散布虚假信息、宣传和伪造文件协调了一场影响瑞典决策的运动*'
- en: '*100% of Dutch Trains Now Run on Wind Energy. The Netherlands met its renewable
    energy goals a year ahead of time.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*荷兰所有火车现在都使用风能。荷兰提前一年实现了可再生能源目标*'
- en: '*Legal challenge against UK’s sweeping surveillance laws quickly crowdfunded*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*对英国全面监控法律的合法挑战迅速获得众筹*'
- en: '*A 1,000-foot-thick ice block about the size of Delaware is snapping off of
    Antarctica*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*一块约等于特拉华州大小的、厚达1000英尺的冰块正在从南极洲断裂*'
- en: '*The U.S. dropped an average of 72 bombs every day — the equivalent of three
    an hour — in 2016, according to an analysis of American strikes around the world.
    U.S. Bombed Iraq, Syria, Pakistan, Afghanistan, Libya, Yemen, Somalia in 2016*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*根据对美国在全球范围内打击的分析，2016年美国平均每天投下72枚炸弹——相当于每小时三枚——2016年美国轰炸了伊拉克、叙利亚、巴基斯坦、阿富汗、利比亚、也门、索马里*'
- en: '*The German government is investigating a recent surge in fake news following
    claims that Russia is attempting to meddle in the country’s parliamentary elections
    later this year.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*德国政府正在调查最近虚假新闻激增的情况，据称俄罗斯试图干预该国今年晚些时候的议会选举*'
- en: '*Pesticides kill over 10 million bees in a matter of days in Brazil countryside*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*巴西乡村几天内农药杀死超过1000万只蜜蜂*'
- en: '*The families of American victims of Islamic State terrorist attacks in Europe
    have sued Twitter, charging that the social media giant allowed the terror group
    to proliferate online*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*欧洲伊斯兰国恐怖袭击的美国受害者家属起诉Twitter，指控这家社交媒体巨头允许恐怖组织在网上传播*'
- en: '*Gas taxes drop globally despite climate change; oil &amp; gas industry gets
    $500 billion in subsidies; last new US gas tax was in 1993*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管气候变化，全球汽油税下降；石油和天然气行业获得5000亿美元补贴；美国上一次新的汽油税是在1993年*'
- en: '*Czech government tells citizens to arm themselves and shoot Muslim terrorists
    in case of ''Super Holocaust''*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*在“超级大屠杀”的情况下，捷克政府告诉公民武装自己并射击穆斯林恐怖分子*'
- en: '*PLO threatens to revoke recognition of Israel if US embassy moves to Jerusalem*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果美国大使馆迁至耶路撒冷，巴勒斯坦解放组织威胁撤销对以色列的承认*'
- en: '*Two-thirds of all new HIV cases in Europe are being recorded in just one country
    – Russia: More than a million Russians now live with the virus and that number
    is expected to nearly double in the next decade*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*欧洲所有新发现的艾滋病病例中有三分之二仅记录在一个国家——俄罗斯：现在有超过一百万俄罗斯人感染了病毒，预计在下一个十年内这个数字将几乎翻倍*'
- en: '*Czech government tells its citizens how to fight terrorists: Shoot them yourselves
    | The interior ministry is pushing a constitutional change that would let citizens
    use guns against terrorists*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*捷克政府告诉其公民如何对抗恐怖分子：自己开枪射击 | 内政部正在推动一项宪法变革，这将允许公民使用枪支对抗恐怖分子*'
- en: '*Morocco Prohibits Sale of Burqa*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*摩洛哥禁止出售布卡*'
- en: '*Mass killer Breivik makes Nazi salute at rights appeal case*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*大规模杀手布雷维克在权利上诉案中行纳粹礼*'
- en: '*Soros Groups Risk Purge After Trump’s Win Emboldens Hungary*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*索罗斯集团在特朗普获胜后面临清洗风险，匈牙利变得更加大胆*'
- en: '*Nigeria purges 50,000 ‘ghost workers’ from State payroll in corruption sweep*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*尼日利亚在反腐行动中清除5万名“幽灵员工”*'
- en: '*Alcohol advertising is aggressive and linked to youth drinking, research finds
    | Society*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*研究发现，酒精广告具有侵略性，与青少年饮酒有关 | 社会*'
- en: '*UK Government quietly launched ‘assault on freedom’ while distracting people,
    say campaigners behind legal challenge - The Investigatory Powers Act became law
    at the end of last year, and gives spies the power to read through everyone’s
    entire internet history*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*英国政府在法律挑战者背后悄然发起“对自由的攻击”，同时分散人们的注意力 - 上一年年底，《调查权力法案》成为法律，赋予间谍阅读每个人整个互联网历史的能力*'
- en: '*Russia’s Reserve Fund down 70 percent in 2016*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*俄罗斯储备基金在2016年下跌70%*'
- en: '*Russian diplomat found dead in Athens*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*在雅典发现一名俄罗斯外交官死亡*'
- en: '*At least 21 people have been killed (most were civilians) and 45 wounded in
    twin bombings near the Afghan parliament in Kabul*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*在喀布尔阿富汗议会的附近发生双爆炸，造成至少21人死亡（其中大多数是平民）和45人受伤*'
- en: '*Pound’s Decline Deepens as Currency Reclaims Dubious Honor*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*英镑的下跌加深，货币重新获得可疑的荣誉*'
- en: World news isn't usually the most optimistic of places, but it does give insight
    into what is going on around the world, and trends on this subreddit are usually
    indicative of trends in the world.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 世界新闻通常不是最乐观的地方，但它确实能让我们了解世界上正在发生的事情，这个subreddit的趋势通常可以表明全球的趋势。
- en: Extracting text from arbitrary websites
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从任意网站提取文本
- en: The links that we get from reddit go to arbitrary websites run by many different
    organizations. To make it harder, those pages were designed to be read by a human,
    not a computer program. This can cause a problem when trying to get the actual
    content/story of those results, as modern websites have a lot going on in the
    background. JavaScript libraries are called, style sheets are applied, advertisements
    are loaded using AJAX, extra content is added to sidebars, and various other things
    are done to make the modern web page a complex document. These features make the
    modern Web what it is, but make it difficult to automatically get good information
    from!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Reddit获得的链接指向由许多不同组织运行的任意网站。为了使其更难，这些页面被设计成由人类阅读，而不是由计算机程序阅读。当试图获取这些结果的实际内容/故事时，这可能会引起问题，因为现代网站在后台有很多活动。JavaScript库被调用，样式表被应用，广告通过AJAX加载，侧边栏中添加了额外的内容，以及进行各种其他操作，使现代网页成为一个复杂的文档。这些功能使现代网络成为它现在这样，但使自动从其中获取良好信息变得困难！
- en: Finding the stories in arbitrary websites
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在任意网站上寻找故事
- en: 'To start with, we will download the full web page from each of these links
    and store them in our data folder, under a raw subfolder. We will process these
    to extract the useful information later on. This caching of results ensures that
    we don''t have to continuously download the websites while we are working. First,
    we set up the data folder path:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从每个链接下载完整的网页，并将它们存储在我们的数据文件夹中，在raw子文件夹下。我们将在稍后处理这些内容以提取有用的信息。这种结果的缓存确保我们在工作时不需要持续下载网站。首先，我们设置数据文件夹路径：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are going to use MD5 hashing to create unique filenames for our articles,
    by hashing the URL, and we will import `hashlib` to do that. A `hash` function
    is a function that converts some input (in our case a string containing the title)
    into a string that is seemingly random. The same input will always return the
    same output, but slightly different inputs will return drastically different outputs.
    It is also impossible to go from a hash value to the original value, making it
    a one-way function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MD5散列来为我们的文章创建唯一的文件名，通过散列URL，我们将导入`hashlib`来完成这个任务。散列函数是一个将某些输入（在我们的情况下是一个包含标题的字符串）转换为看似随机的字符串的函数。相同的输入将始终返回相同的输出，但略微不同的输入将返回截然不同的输出。从散列值到原始值也是不可能的，这使得它成为一个单向函数。
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For this chapter's experiments, we are going to simply skip any website downloads
    that fail. In order to make sure we don't lose too much information doing this,
    we maintain a simple counter of the number of errors that occur. We are going
    to suppress any error that occurs, which could result in a systematic problem
    prohibiting downloads. If this error counter is too high, we can look at what
    those errors were and try to fix them. For example, if the computer has no internet
    access, all 500 of the downloads will fail and you should probably fix that before
    continuing!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的实验，我们将简单地跳过任何失败的网站下载。为了确保我们不会因为这样做而丢失太多信息，我们维护一个简单的错误计数器来记录发生的错误数量。我们将抑制任何可能引起系统问题、阻止下载的错误。如果错误计数器过高，我们可以查看这些错误并尝试修复它们。例如，如果电脑没有互联网访问，所有500次下载都会失败，你应该在继续之前修复这个问题！
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we iterate through each of our stories, download the website, and save
    the results to a file:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历我们的每个故事，下载网站，并将结果保存到文件中：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If there is an error in obtaining the website, we simply skip this website and
    keep going. This code will work on a large number of websites and that is good
    enough for our application, as we are looking for general trends and not exactness.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在获取网站时出现错误，我们简单地跳过这个网站并继续。这段代码可以在大量网站上工作，这对我们的应用来说已经足够好了，因为我们寻找的是一般趋势而不是精确度。
- en: Note that sometimes you do care about getting 100 percent of responses, and
    you should adjust your code to accommodate more errors. Be warned though that
    there is a significant increase in effort required to create code the works reliably
    on data from the internet. The code to get those final 5 to 10 percent of websites
    will be significantly more complex.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有时你确实关心获取100%的响应，你应该调整你的代码以适应更多的错误。但是要警告，要创建在互联网数据上可靠工作的代码需要显著增加工作量。获取那些最终5%到10%的网站的代码将显著更复杂。
- en: In the preceding code, we simply catch any error that happens, record the error
    and move on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们简单地捕获发生的任何错误，记录错误并继续。
- en: If you find that too many errors occur, change the print(e) line to just type
    raise instead. This will cause the exception to be called, allowing you to debug
    the problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现错误太多，将print(e)行改为仅输入raise。这将导致异常被调用，允许你调试问题。
- en: After this has completed, we will have a bunch of websites in our `raw` subfolder.
    After taking a look at these pages (open the created files in a text editor),
    you can see that the content is there but there is HTML, JavaScript, CSS code,
    as well as other content. As we are only interested in the story itself, we now
    need a way to extract this information from these different websites.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们将在`raw`子文件夹中有一堆网站。在查看这些页面（在文本编辑器中打开创建的文件）后，你可以看到内容在那里，但还有HTML、JavaScript、CSS代码以及其他内容。因为我们只对故事本身感兴趣，所以我们现在需要一种方法从这些不同的网站上提取这些信息。
- en: Extracting the content
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取内容
- en: After we get the raw data, we need to find the story in each. There are several
    complex algorithms for doing this, as well as some simple ones. We will stick
    with a simple method here, keeping in mind that often enough, the simple algorithm
    is good enough. This is part of data mining—knowing when to use simple algorithms
    to get a job done, versus using more complicated algorithms to obtain that extra
    bit of performance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们获取原始数据后，我们需要在每个中找到故事。为此有几种复杂的算法，以及一些简单的算法。在这里我们将坚持使用简单的方法，考虑到通常情况下，简单的算法就足够了。这是数据挖掘的一部分——知道何时使用简单的算法来完成工作，以及何时使用更复杂的算法来获得额外的性能。
- en: 'First, we get a list of each of the filenames in our `raw` subfolder:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们获取`raw`子文件夹中每个文件名的列表：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we create an output folder for the text-only versions that we will extract:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个用于存放提取的纯文本版本的输出文件夹：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we develop the code that will extract the text from the files. We will
    use the lxml library to parse the HTML files, as it has a good HTML parser that
    deals with some badly formed expressions. The code is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写将提取文件文本的代码。我们将使用lxml库来解析HTML文件，因为它有一个处理一些格式不良表达式的良好HTML解析器。代码如下：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The actual code for extracting text is based on three steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实际提取文本的代码基于三个步骤：
- en: We iterate through each of the nodes in the HTML file and extract the text in
    it.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历HTML文件中的每个节点并提取其中的文本。
- en: We skip any node that is JavaScript, styling, or a comment, as this is unlikely
    to contain information of interest to us.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们跳过任何JavaScript、样式或注释的节点，因为这些不太可能包含对我们感兴趣的信息。
- en: We ensure that the content has at least 100 characters. This is a good baseline,
    but it could be improved upon for more accurate results.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们确保内容至少有100个字符。这是一个很好的基线，但可以通过更精确的结果来改进。
- en: 'As we said before, we aren''t interested in scripts, styles, or comments. So,
    we create a list to ignore nodes of those types. Any node that has a type in this
    list will not be considered as containing the story. The code is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，我们对脚本、样式或注释不感兴趣。因此，我们创建了一个列表来忽略这些类型的节点。任何类型在此列表中的节点都不会被视为包含故事。代码如下：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will now create a function that parses an HTML file into an lxml `etree`,
    and then we will create another function that parses this tree looking for text.
    This first function is pretty straightforward; simply open the file and create
    a tree using the lxml library''s parsing function for HTML files. The code is
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个函数，将HTML文件解析为lxml `etree`，然后我们将创建另一个函数来解析这个树，寻找文本。这个第一个函数相当直接；只需打开文件并使用lxml库的HTML文件解析函数创建一个树。代码如下：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the last line of that function, we call the `getroot()` function to get the
    root node of the tree, rather than the full `etree`. This allows us to write our
    text extraction function to accept any node, and therefore write a recursive function.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在该函数的最后一条语句中，我们调用`getroot()`函数来获取树的根节点，而不是完整的`etree`。这允许我们编写接受任何节点的文本提取函数，因此可以编写递归函数。
- en: This function will call itself on any child nodes to extract the text from them,
    and then return the concatenation of any child nodes text.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将在任何子节点上调用自身以提取文本，然后返回子节点文本的连接。
- en: If the node where this function is passed doesn't have any child nodes, we just
    return the text from it. If it doesn't have any text, we just return an empty
    string. Note that we also check here for our third condition—that the text is
    at least 100 characters long.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果传递给此函数的节点没有子节点，我们只需从它返回文本。如果没有文本，我们只返回一个空字符串。请注意，我们在这里也检查了我们的第三个条件——文本至少100个字符长。
- en: 'The code for checking that the text is at least 100 characters long is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 检查文本至少100个字符长的代码如下：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: At this point, we know that the node has child nodes, so we recursively call
    this function on each of those child nodes and then join the results when they
    return.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们知道该节点有子节点，因此我们将递归地对每个子节点调用此函数，然后在它们返回时将结果连接起来。
- en: The final condition inside the return line stops blank lines being returned
    (for example, when a node has no children and no text). We also use a generator,
    which makes the code more efficient by only grabbing text data when it is needed,
    namely the final return statement rather than creating a number of sub-lists.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 返回行内的最终条件阻止了空白行的返回（例如，当一个节点没有子节点且没有文本时）。我们还使用了一个生成器，这使得代码更高效，因为它只在需要时获取文本数据，即最终的返回语句，而不是创建多个子列表。
- en: 'We can now run this code on all of the raw HTML pages by iterating through
    them, calling the text extraction function on each, and saving the results to
    the text-only subfolder:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过遍历所有原始HTML页面来运行此代码，对每个页面调用文本提取函数，并将结果保存到文本仅子文件夹中：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can evaluate the results manually by opening each of the files in the text
    only subfolder and checking their content. If you find too many of the results
    have non-story content, try increasing the minimum-100-character limit. If you
    still can't get good results, or need better results for your application, try
    the methods listed in *Appendix A, Next Steps.*
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过打开文本仅子文件夹中的每个文件并检查其内容来手动评估结果。如果您发现结果中有太多非故事内容，尝试增加最小100字符的限制。如果您仍然无法获得良好的结果，或者需要为您的应用程序获得更好的结果，请尝试在*附录A，下一步*中列出的方法。
- en: Grouping news articles
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新闻文章分组
- en: The aim of this chapter is to discover trends in news articles by clustering,
    or grouping, them together. To do that, we will use the k-means algorithm, a classic
    machine learning algorithm originally developed in 1957.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是通过聚类或分组新闻文章来发现新闻文章的趋势。为此，我们将使用k-means算法，这是一种经典的机器学习算法，最初于1957年开发。
- en: '**Clustering** is an unsupervised learning technique and we often use clustering
    algorithms for exploring data. Our dataset contains approximately 500 stories
    and it would be quite arduous to examine each of those stories individually. Using
    clustering allows us to group similar stories together, and we can explore the
    themes in each cluster independently.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一种无监督学习技术，我们经常使用聚类算法来探索数据。我们的数据集包含大约500个故事，逐一检查这些故事会相当困难。使用聚类使我们能够将相似的故事分组在一起，我们可以独立地探索每个簇的主题。'
- en: We use clustering techniques when we don't have a clear set of target classes
    for our data. In that sense, clustering algorithms have little direction in their
    learning. They learn according to some function, regardless of the underlying
    meaning of the data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们没有明确的数据目标类别时，我们会使用聚类技术。从这个意义上说，聚类算法在他们的学习中几乎没有方向。它们根据某些函数学习，而不考虑数据的潜在含义。
- en: For this reason, it is critical to choose good features. In supervised learning,
    if you choose poor features, the learning algorithm can choose to not use those
    features. For instance, support vector machines will give little weight to features
    that aren't useful in classification. However, with clustering, all features are
    used in the final result—even if those features don't provide us with the answer
    we were looking for.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择好的特征至关重要。在监督学习中，如果你选择了不好的特征，学习算法可以选择不使用这些特征。例如，支持向量机会对在分类中无用的特征赋予很小的权重。然而，在聚类中，所有特征都会用于最终结果——即使这些特征没有为我们提供我们想要的答案。
- en: When performing cluster analysis on real-world data, it is always a good idea
    to have a sense of what sorts of features will work for your scenario. In this
    chapter, we will use the bag-of-words model. We are looking for topic-based groups,
    so we will use topic-based features to model the documents. We know those features
    work because of the work others have done in supervised versions of our problem.
    In contrast, if we were to perform an authorship-based clustering, we would use
    features such as those found in the [Chapter 9](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)*,
    Authorship Attribution*, experiment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在对现实世界数据进行聚类分析时，了解在您的场景中哪些类型的特征将起作用总是一个好主意。在本章中，我们将使用词袋模型。我们正在寻找基于主题的组，因此我们将使用基于主题的特征来建模文档。我们知道这些特征有效，因为其他人已经在我们的问题的监督版本中进行了工作。相比之下，如果我们进行基于作者身份的聚类，我们会使用诸如[第9章](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)*，作者身份归因*实验中找到的特征。
- en: The k-means algorithm
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means算法
- en: The k-means clustering algorithm finds centroids that best represent the data
    using an iterative process. The algorithm starts with a predefined set of centroids,
    which are normally data points taken from the training data. The **k** in k-means
    is the number of centroids to look for and how many clusters the algorithm will
    find. For instance, setting k to 3 will find three clusters in the dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类算法通过迭代过程找到最能代表数据的质心。算法从一个预定义的质心集合开始，这些质心通常是来自训练数据的数据点。k-means中的**k**是要寻找的质心数量以及算法将找到多少个簇。例如，将k设置为3将在数据集中找到三个簇。
- en: 'There are two phases to the k-means: **assignment** and **updating**. They
    are explained as below:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: k-means有两个阶段：**分配**和**更新**。它们如下所述：
- en: In the **assignment** step, we set a label to every sample in the dataset linking
    it to the nearest centroid. For each sample nearest to centroid 1, we assign the
    label 1\. For each sample nearest to centroid 2, we assign a label 2 and so on
    for each of the k centroids. These labels form the clusters, so we say that each
    data point with the label 1 is in cluster 1 (at this time only, as assignments
    can change as the algorithm runs).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**分配**步骤中，我们为数据集中的每个样本设置一个标签，将其与最近的质心关联。对于距离质心1最近的样本，我们分配标签1。对于距离质心2最近的样本，我们分配标签2，依此类推，为每个k个质心分配标签。这些标签形成了簇，因此我们说每个带有标签1的数据点都在簇1中（目前是这样，因为随着算法的运行，分配可能会改变）。
- en: In the **updating** step, we take each of the clusters and compute the centroid,
    which is the mean of all of the samples in that cluster.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**更新**步骤中，我们针对每个簇计算其质心，即该簇中所有样本的平均值。
- en: The algorithm then iterates between the assignment step and the updating step;
    each time the updating step occurs, each of the centroids moves a small amount.
    This causes the assignments to change slightly, causing the centroids to move
    a small amount in the next iteration. This repeats until some stopping criterion
    is reached.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在分配步骤和更新步骤之间迭代；每次更新步骤发生时，每个质心都会移动一小段距离。这导致分配略有变化，导致在下一个迭代中质心也会略有移动。这会一直重复，直到达到某个停止标准。
- en: It is common to stop after a certain number of iterations, or when the total
    movement of the centroids is very low. The algorithm can also complete in some
    scenarios, which means that the clusters are stable—the assignments do not change
    and neither do the centroids.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 常常在迭代一定次数后停止，或者当质心的总移动量非常低时停止。在某些情况下，算法也可以完成，这意味着簇是稳定的——分配不会改变，质心也不会改变。
- en: In the following figure, k-means was performed over a dataset created randomly,
    but with three clusters in the data. The stars represent the starting location
    of the centroids, which were chosen randomly by picking a random sample from the
    dataset. Over 5 iterations of the k-means algorithm, the centroids move to the
    locations represented by the triangles.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，对随机创建但包含三个数据集簇的数据集执行了k-means算法。星星代表质心的起始位置，这些位置是通过从数据集中随机选择一个样本来随机选择的。经过k-means算法的5次迭代，质心移动到由三角形表示的位置。
- en: '![](img/B06162_10_01.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_01.png)'
- en: The k-means algorithm is fascinating for its mathematical properties and historical
    significance. It is an algorithm that (roughly) only has a single parameter, and
    is quite effective and frequently used, even more than 50 years after its discovery.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法因其数学特性和历史意义而令人着迷。这是一个（大致上）只有一个参数的算法，非常有效且经常使用，即使在其发现50多年后也是如此。
- en: 'There is a k-means algorithm in scikit-learn, which we import from the `cluster` module
    in scikit-learn:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中有一个k-means算法，我们是从scikit-learn的`cluster`模块导入的：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We also import the `CountVectorizer` class''s close cousin, `TfidfVectorizer`.
    This vectorizer applies a weighting to each term''s counts, depending on how many
    documents it appears in, using the equation: tf / log(df), where tf is a term''s
    frequency (how many times it appears in the current document) and df is the term''s
    document frequency (how many documents in our corpus it appears in). Terms that
    appear in many documents are weighted lower (by dividing the value by the log
    of the number of documents it appears in). For many text mining applications,
    using this type of weighting scheme can improve performance quite reliably. The
    code is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还导入了`CountVectorizer`类的近亲`TfidfVectorizer`。这个向量器根据每个词的出现次数进行加权，具体取决于它在多少个文档中出现，使用以下公式：tf
    / log(df)，其中tf是一个词的频率（它在当前文档中出现的次数）和df是一个词的文档频率（在我们的语料库中它出现的文档数）。在许多文档中出现的词被赋予较低的权重（通过除以它出现的文档数的对数）。对于许多文本挖掘应用，使用这种类型的加权方案可以非常可靠地提高性能。代码如下：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then set up our pipeline for our analysis. This has two steps. The first
    is to apply our vectorizer, and the second is to apply our k-means algorithm.
    The code is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为我们的分析设置管道。这有两个步骤。第一步是应用我们的向量器，第二步是应用我们的k-means算法。代码如下：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `max_df` parameter is set to a low value of 0.4, which says ignore any word
    that occurs in more than 40 percent of documents. This parameter is invaluable
    for removing function words that give little topic-based meaning on their own.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_df`参数设置为低值0.4，这意味着忽略任何在超过40%的文档中出现的单词。这个参数对于移除那些本身对主题意义贡献很小的功能词来说是无价的。'
- en: Removing any word that occurs in more than 40 percent of documents will remove
    function words, making this type of preprocessing quite useless for the work we
    saw in [Chapter 9](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)*, Authorship Attribution*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 移除在超过40%的文档中出现的任何单词将移除功能词，这使得这种类型的预处理对于我们在[第9章](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)*，作者归属*中看到的工作来说非常无用。
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then fit and predict this pipeline. We have followed this process a number
    of times in this book so far for classification tasks, but there is a difference
    here—we do not give the target classes for our dataset to the fit function. This
    is what makes this an unsupervised learning task! The code is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们拟合并预测这个管道。到目前为止，这本书中我们已经多次遵循这个过程进行分类任务，但这里有一个区别——我们不将数据集的目标类别提供给拟合函数。这就是使其成为无监督学习任务的原因！代码如下：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The labels variable now contains the cluster numbers for each sample. Samples
    with the same label are said to belong to the same cluster. It should be noted
    that the cluster labels themselves are meaningless: clusters 1 and 2 are no more
    similar than clusters 1 and 3.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 标签变量现在包含每个样本的聚类编号。具有相同标签的样本被认为属于同一个聚类。需要注意的是，聚类标签本身没有意义：聚类1和2与聚类1和3的相似度没有区别。
- en: 'We can see how many samples were placed in each cluster using the `Counter`
    class:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Counter`类查看每个聚类中放置了多少个样本：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Many of the results (keeping in mind that your dataset will be quite different
    to mine) consist of a large cluster with the majority of instances, several medium
    clusters, and some clusters with only one or two instances. This imbalance is
    quite normal in many clustering applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 许多结果（记住你的数据集将与我的大不相同）包括一个包含大多数实例的大聚类，几个中等大小的聚类，以及一些只有一个或两个实例的聚类。这种不平衡在许多聚类应用中相当正常。
- en: Evaluating the results
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估结果
- en: Clustering is mainly an exploratory analysis, and therefore it is difficult
    to evaluate a clustering algorithm's results effectively. A straightforward way
    is to evaluate the algorithm based on the criteria the algorithm tries to learn
    from.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类主要是一种探索性分析，因此很难有效地评估聚类算法的结果。一种直接的方法是根据算法试图学习的标准来评估算法。
- en: If you have a test set, you can evaluate clustering against it. For more details,
    visit [http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个测试集，你可以用它来评估聚类效果。更多详情，请访问[http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
- en: 'In the case of the k-means algorithm, the criterion that it uses when developing
    the centroids is to minimize the distance from each sample to its nearest centroid.
    This is called the inertia of the algorithm and can be retrieved from any KMeans
    instance that has had fit called on it:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means算法的情况下，它开发质心时使用的标准是使每个样本到其最近质心的距离最小化。这被称为算法的惯性，可以从任何已经调用fit的KMeans实例中检索到：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The result on my dataset was 343.94\. Unfortunately, this value is quite meaningless
    by itself, but we can use it to determine how many clusters we should use. In
    the preceding example, we set `n_clusters` to 10, but is this the best value?
    The following code runs the k-means algorithm 10 times with each value of `n_clusters`
    from 2 to 20, taking some time to complete the large number of runs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的数据集上的结果是343.94。不幸的是，这个值本身相当没有意义，但我们可以用它来确定我们应该使用多少个聚类。在先前的例子中，我们将`n_clusters`设置为10，但这真的是最佳值吗？下面的代码运行k-means算法10次，每次使用从2到20的每个`n_clusters`值，这需要一些时间来完成大量运行。
- en: For each run, it records the inertia of the result.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次运行，它记录结果的惯性。
- en: You may notice the following code that we don't use a Pipeline; instead, we
    split out the steps. We only create the X matrix from our text documents once
    per value of `n_clusters` to (drastically) improve the speed of this code.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到以下代码我们没有使用Pipeline；相反，我们将步骤拆分出来。我们只为每个`n_clusters`值从我们的文本文档中创建一次X矩阵，以（显著地）提高代码的速度。
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The`inertia_scores` variable now contains a list of inertia scores for each
    n_clusters value between 2 and 20\. We can plot this to get a sense of how this
    value interacts with `n_clusters`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`inertia_scores`变量现在包含从2到20的每个`n_clusters`值的惯性分数列表。我们可以绘制这些值，以了解这个值如何与`n_clusters`相互作用：'
- en: '[PRE33]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](img/B06162_10_02.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_02.png)'
- en: Overall, the value of the inertia should decrease with reducing improvement
    as the number of clusters improves, which we can broadly see from these results.
    The increase between values of 6 to 7 is due only to the randomness in selecting
    the centroids, which directly affect how good the final results are. Despite this,
    there is a general trend (for my data; your results may vary) that about 6 clusters
    was the last time a major improvement in the inertia occurred.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，随着聚类数量的增加，惯性值应该随着改进的减少而降低，这一点我们可以从这些结果中大致看出。从6到7之间的值增加仅由于选择质心的随机性，这直接影响了最终结果的好坏。尽管如此，对于我的数据来说（你的结果可能会有所不同），大约6个聚类是惯性发生重大改进的最后一次。
- en: After this point, only slight improvements are made to the inertia, although
    it is hard to be specific about vague criteria such as this. Looking for this
    type of pattern is called the elbow rule, in that we are looking for an elbow-esque
    bend in the graph. Some datasets have more pronounced elbows, but this feature
    isn't guaranteed to even appear (some graphs may be smooth!).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点之后，对惯性只有轻微的改进，尽管很难具体说明这种模糊的标准。寻找这种类型的模式被称为肘部规则，因为我们正在寻找图表中的肘部弯曲。一些数据集有更明显的肘部，但这个特征并不保证出现（有些图表可能是平滑的！）
- en: 'Based on this analysis, we set `n_clusters` to be 6 and then rerun the algorithm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种分析，我们将 `n_clusters` 设置为 6，然后重新运行算法：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Extracting topic information from clusters
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从簇中提取主题信息
- en: Now we set our sights on the clusters in an attempt to discover the topics in
    each.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将注意力转向簇，试图发现每个簇中的主题。
- en: 'We first extract the term list from our feature extraction step:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从特征提取步骤中提取术语列表：
- en: '[PRE35]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We also set up another counter for counting the size of each of our classes:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了一个计数器来统计每个类的大小：
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Iterating over each cluster, we print the size of the cluster as before.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个簇，我们像以前一样打印簇的大小。
- en: It is important to keep in mind the sizes of the clusters when evaluating the
    results—some of the clusters will only have one sample, and are therefore not
    indicative of a general trend.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估结果时，重要的是要记住簇的大小——一些簇可能只有一个样本，因此不能表明一般趋势。
- en: Next (and still in the loop), we iterate over the most important terms for this
    cluster. To do this, we take the five largest values from the centroid, which
    we get by finding the features that have the highest values in the centroid itself.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来（仍然在循环中），我们遍历这个簇最重要的术语。为此，我们从质心中取出五个最大的值，这些值是通过找到质心中具有最高值的特征得到的。
- en: '[PRE37]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The results can be quite indicative of current trends. In my results (obtained
    January 2017), the clusters correspond to health matters, Middle East tensions,
    Korean tensions, and Russian affairs. These were the main topics frequenting news
    around this time—although this has hardly changed for a number of years!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以非常指示当前趋势。在我的结果（2017年1月获得）中，簇对应于健康问题、中东紧张局势、韩国紧张局势和俄罗斯事务。这些是当时新闻中频繁出现的主要话题——尽管这些年来几乎没有变化！
- en: 'You might notice some words that don''t provide much value come out on top,
    such as *you, her* and *mr.* These function words are great for authorship analysis
    - as we saw in [Chapter 9](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml), *Authorship
    Attribution*, but are not generally very good for topic analysis. Passing the
    list of function words into the `stop_words` parameter of the **TfidfVectorizer**
    in our pipeline above will ignore those words. Here is the updated code for building
    such a pipeline:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到一些没有太多价值的词出现在最上面，例如 *你, 她和*mr.* 这些功能词对于作者分析来说非常好——正如我们在[第9章](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)中看到的，*作者归属分析*，但它们通常并不适合主题分析。将功能词列表传递到我们上面管道中的
    `stop_words` 参数将会忽略这些词。以下是构建此类管道的更新代码：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Using clustering algorithms as transformers
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将聚类算法作为转换器使用
- en: As a side note, one interesting property about the k-means algorithm (and any
    clustering algorithm) is that you can use it for feature reduction. There are
    many methods to reduce the number of features (or create new features to embed
    the dataset on), such as **Principle Component Analysis**, **Latent Semantic Indexing**,
    and many others. One issue with many of these algorithms is that they often need
    lots of computing power.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 作为旁注，关于 k-means 算法（以及任何聚类算法）的一个有趣性质是，你可以用它来进行特征降维。有许多方法可以减少特征数量（或创建新特征以在数据集上嵌入），例如
    **主成分分析**、**潜在语义索引**和许多其他方法。这些算法的一个问题是它们通常需要大量的计算能力。
- en: In the preceding example, the terms list had more than 14,000 entries in it—it
    is quite a large dataset. Our k-means algorithm transformed these into just six
    clusters. We can then create a dataset with a much lower number of features by
    taking the distance to each centroid as a feature.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，术语列表中有超过 14,000 个条目——这是一个相当大的数据集。我们的 k-means 算法将其转换成仅仅六个簇。然后我们可以通过取每个质心的距离作为特征来创建一个具有更低特征数量的数据集。
- en: 'To do this, we call the transform function on a KMeans instance. Our pipeline
    is fit for this purpose, as it has a k-means instance at the end:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们在一个 KMeans 实例上调用 transform 函数。我们的管道适合这个目的，因为它在最后有一个 k-means 实例：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This calls the transform method on the final step of the pipeline, which is
    an instance of k-means. This results in a matrix that has six features and the
    number of samples is the same as the length of documents.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在管道的最后一步调用转换方法，这是一个k-means的实例。这导致一个具有六个特征和样本数量与文档长度相同的矩阵。
- en: You can then perform your own second-level clustering on the result, or use
    it for classification if you have the target values. A possible workflow for this
    would be to perform some feature selection using the supervised data, use clustering
    to reduce the number of features to a more manageable number, and then use the
    results in a classification algorithm such as SVMs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以对结果进行自己的二级聚类，或者如果你有目标值，可以使用它进行分类。这个工作流程的一个可能方法是使用监督数据执行一些特征选择，使用聚类将特征数量减少到更易于管理的数量，然后在一个分类算法（如SVMs）中使用这些结果。
- en: Clustering ensembles
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类集成
- en: In [Chapter 3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml)*, Predicting Sports
    Winners with Decision Trees*, we looked at a classification ensemble using the
    random forest algorithm, which is an ensemble of many low-quality, tree-based
    classifiers. Ensembling can also be performed using clustering algorithms. One
    of the key reasons for doing this is to smooth the results from many runs of an
    algorithm. As we saw before, the results from running k-means are varied, depending
    on the selection of the initial centroids. Variation can be reduced by running
    the algorithm many times and then combining the results.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml)*，使用决策树预测体育胜者*中，我们研究了使用随机森林算法的分类集成，它是由许多低质量的基于树的分类器组成的集成。集成也可以使用聚类算法来完成。这样做的一个关键原因是平滑算法多次运行的结果。正如我们之前看到的，k-means运行的结果因初始质心的选择而异。通过多次运行算法并合并结果可以减少这种变化。
- en: Ensembling also reduces the effects of choosing parameters on the final result.
    Most clustering algorithms are quite sensitive to the parameter values chosen
    for the algorithm. Choosing slightly different parameters results in different
    clusters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 集成也减少了选择参数对最终结果的影响。大多数聚类算法对算法选择的参数值非常敏感。选择略微不同的参数会导致不同的聚类。
- en: Evidence accumulation
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 证据累积
- en: As a basic ensemble, we can first cluster the data many times and record the
    labels from each run. We then record how many times each pair of samples was clustered
    together in a new matrix. This is the essence of the **Evidence Accumulation Clustering**
    (**EAC**) algorithm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基本集成，我们首先多次聚类数据并记录每次运行的标签。然后我们在一个新的矩阵中记录每一对样本被聚类的次数。这就是**证据累积聚类**（**EAC**）算法的本质。
- en: EAC has two major steps.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: EAC有两个主要步骤。
- en: The first step is to cluster the data many times using a lower-level clustering
    algorithm, such as k-means and record the frequency that samples were in the same
    cluster, in each iteration. This is stored in a **co-association matrix**.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是多次使用低级聚类算法（如k-means）对数据进行聚类，并记录样本在每个迭代中位于同一聚类的频率。这被存储在一个**共关联矩阵**中。
- en: The second step is to perform a cluster analysis on the resulting co-association
    matrix, which is performed using another type of clustering algorithm called hierarchical
    clustering. This has an interesting graph-theory-based property, as it is mathematically
    the same as finding a tree that links all the nodes together and removing weak
    links.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是对结果共关联矩阵进行聚类分析，这使用另一种称为层次聚类的聚类算法来完成。它具有一个有趣的基于图论的性质，因为它在数学上等同于找到一个将所有节点连接起来的树并移除弱连接。
- en: 'We can create a co-association matrix from an array of labels by iterating
    over each of the labels and recording where two samples have the same label. We
    use SciPy''s `csr_matrix`, which is a type of sparse matrix:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过遍历每个标签并记录两个样本具有相同标签的位置，从标签数组创建一个共关联矩阵。我们使用SciPy的`csr_matrix`，这是一种稀疏矩阵：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our function definition takes a set of labels and then record the rows and
    columns of each match. We do these in a list. Sparse matrices are commonly just
    sets of lists recording the positions of nonzero values, and `csr_matrix` is an
    example of this type of sparse matrix. For each pair of samples with the same
    label, we record the position of both samples in our list:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能定义接受一组标签，然后记录每个匹配的行和列。我们在一个列表中完成这些操作。稀疏矩阵通常只是记录非零值位置的列表集合，`csr_matrix`就是这种类型稀疏矩阵的一个例子。对于具有相同标签的每一对样本，我们在列表中记录这两个样本的位置：
- en: '[PRE41]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To get the co-association matrix from the labels, we simply call this function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要从标签中获得共关联矩阵，我们只需调用此函数：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: From here, we can add multiple instances of these matrices together. This allows
    us to combine the results from multiple runs of k-means. Printing out `C` (just
    enter C into a new cell of your Jupyter Notebook and run it) will tell you how
    many cells have nonzero values in them. In my case, about half of the cells had
    values in them, as my clustering result had a large cluster (the more even the
    clusters, the lower the number of nonzero values).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以将这些矩阵的多个实例相加。这允许我们将k-means多次运行的结果结合起来。打印出`C`（只需在Jupyter Notebook的新单元格中输入C并运行）将告诉你有多少单元格中有非零值。在我的情况下，大约一半的单元格中有值，因为我的聚类结果有一个大簇（簇越均匀，非零值的数量越低）。
- en: The next step involves the hierarchical clustering of the co-association matrix.
    We will do this by finding minimum spanning trees on this matrix and removing
    edges with a weight lower than a given threshold.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及共关联矩阵的层次聚类。我们将通过在这个矩阵上找到最小生成树并移除权重低于给定阈值的边来完成此操作。
- en: In graph theory, a spanning tree is a set of edges on a graph that connects
    all of the nodes together. The **Minimum Spanning Tree** (MST) is simply the spanning
    tree with the lowest total weight. For our application, the nodes in our graph
    are samples from our dataset, and the edge weights are the number of times those
    two samples were clustered together—that is, the value from our co-association
    matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在图论中，生成树是连接图中所有节点的边的集合。**最小生成树**（MST）只是具有最低总权重的生成树。在我们的应用中，图中的节点是我们数据集中的样本，边权重是这两个样本被聚在一起的次数——即我们的共关联矩阵中的值。
- en: In the following figure, a MST on a graph of six nodes is shown. Nodes on the
    graph can be connected to more than once in the MST, as long as all nodes are
    connected together.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，展示了六个节点的图上的MST。在MST中，图上的节点可以多次连接，只要所有节点都连接在一起即可。
- en: '![](img/B06162_10_03.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_03.png)'
- en: 'To compute the MST, we use SciPy''s `minimum_spanning_tree` function, which
    is found in the sparse package:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算MST，我们使用SciPy的`minimum_spanning_tree`函数，该函数位于sparse包中：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `mst` function can be called directly on the sparse matrix returned by
    our co-association function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`mst`函数可以直接在由我们的共关联函数返回的稀疏矩阵上调用：'
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'However, in our co-association matrix C, higher values are indicative of samples
    that are clustered together more often—a similarity value. In contrast, `minimum_spanning_tree`
    sees the input as a distance, with higher scores penalized. For this reason, we
    compute the minimum spanning tree on the negation of the co-association matrix
    instead:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的共关联矩阵C中，较高的值表示经常被聚在一起的样本——这是一个相似度值。相比之下，`minimum_spanning_tree`将输入视为距离，较高的分数会受到惩罚。因此，我们在共关联矩阵的取反上计算最小生成树：
- en: '[PRE45]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The result from the preceding function is a matrix the same size as the co-association
    matrix (the number of rows and columns is the same as the number of samples in
    our dataset), with only the edges in the MST kept and all others removed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个函数的结果是一个与共关联矩阵大小相同的矩阵（行数和列数与数据集中的样本数量相同），只保留了MST中的边，其他所有边都被移除。
- en: 'We then remove any node with a weight less than a predefined threshold. To
    do this, we iterate over the edges in the MST matrix, removing any that are less
    than a specific value. We can''t test this out with just a single iteration in
    a co-association matrix (the values will be either 1 or 0, so there isn''t much
    to work with). So, we will create extra labels first, create the co-association
    matrix, and then add the two matrices together. The code is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们移除任何权重低于预定义阈值的节点。为此，我们遍历MST矩阵中的边，移除任何小于特定值的边。我们无法仅通过在共关联矩阵中迭代一次来测试这一点（值将是1或0，因此没有太多可操作的空间）。因此，我们首先创建额外的标签，创建共关联矩阵，然后将两个矩阵相加。代码如下：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then compute the MST and remove any edge that didn''t occur in both of these
    labels:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算MST并移除在这两个标签中都没有出现的任何边：
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The threshold we wanted to cut off was any edge not in both clusterings—that
    is, with a value of 1\. However, as we negated the co-association matrix, we had
    to negate the threshold value too.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要截断的阈值是任何不在两个聚类中的边——即值为1的边。然而，由于我们取反了共关联矩阵，我们也必须取反阈值值。
- en: 'Lastly, we find all of the connected components, which is simply a way to find
    all of the samples that are still connected by edges after we removed the edges
    with low weights. The first returned value is the number of connected components
    (that is, the number of clusters) and the second is the labels for each sample.
    The code is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们找到所有的连通分量，这仅仅是一种找到在移除低权重边后仍然通过边连接的所有样本的方法。第一个返回值是连通分量的数量（即簇的数量），第二个是每个样本的标签。代码如下：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In my dataset, I obtained eight clusters, with the clusters being approximately
    the same as before. This is hardly a surprise, given we only used two iterations
    of k-means; using more iterations of k-means (as we do in the next section) will
    result in more variance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的数据集中，我获得了八个簇，簇与之前的大致相同。考虑到我们只使用了两次k-means迭代，这几乎不足为奇；使用更多的k-means迭代（如我们在下一节中做的那样）将导致更大的方差。
- en: How it works
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'In the k-means algorithm, each feature is used without any regard to its weight.
    In essence, all features are assumed to be on the same scale. We saw the problems
    with not scaling features in Chapter 2*, Classification with scikit-learn Estimators*.
    The result of this is that k-means is looking for circular clusters, visualized
    here:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means算法中，每个特征都是无差别地使用的。本质上，所有特征都被假定为处于相同的尺度。我们在第二章*使用scikit-learn估计器的分类*中看到了不缩放特征的问题。结果是k-means正在寻找圆形簇，如图所示：
- en: '![](img/B06162_10_04.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_04.png)'
- en: 'Oval shaped clusters can also be discovered by k-means. The separation usually
    isn''t quite so smooth, but can be made easier with feature scaling. An example
    of this shaped cluster is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: k-means也可以发现椭圆形簇。分离通常并不那么平滑，但可以通过特征缩放来简化。以下是一个这种形状簇的例子：
- en: '![](img/B06162_10_05.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_05.png)'
- en: As we can see in the preceding screenshot, not all clusters have this shape.
    The blue cluster is circular and is of the type that k-means is very good at picking
    up. The red cluster is an ellipse. The k-means algorithm can pick up clusters
    of this shape with some feature scaling.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个屏幕截图所示，并非所有簇都具有这种形状。蓝色簇是圆形的，是k-means非常擅长识别的类型。红色簇是椭圆形。k-means算法可以通过一些特征缩放识别这种形状的簇。
- en: 'The bellow third cluster isn''t even convex—it is an odd shape that k-means
    will have trouble discovering, but would still be considered a *cluster*, at least
    by most humans looking at the picture:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下面第三个簇甚至不是凸形的——它是一个k-means难以发现的奇形怪状，但至少在大多数观察图片的人类看来，它仍然被认为是*簇*：
- en: '![](img/B06162_10_06.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_06.png)'
- en: Cluster analysis is a hard task, with most of the difficulty simply in trying
    to define the problem. Many people intuitively understand what it means, but trying
    to define it in precise terms (necessary for machine learning) is very difficult.
    Even people often disagree on the term!
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是一个困难的任务，其中大部分困难仅仅在于试图定义问题。许多人直观地理解它的含义，但试图用精确的术语定义它（对于机器学习来说是必要的）是非常困难的。甚至人们经常对这个词有不同的看法！
- en: The EAC algorithm works by remapping the features onto a new space, in essence
    turning each run of the k-means algorithm into a transformer using the same principles
    we saw the previous section using k-means for feature reduction. In this case,
    though, we only use the actual label and not the distance to each centroid. This
    is the data that is recorded in the co-association matrix.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: EAC算法通过将特征重新映射到新的空间中工作，本质上是将k-means算法的每次运行转化为一个使用与上一节中用于特征减少的k-means相同原理的转换器。然而，在这种情况下，我们只使用实际的标签，而不是到每个质心的距离。这是记录在共关联矩阵中的数据。
- en: The result is that EAC now only cares about how close things are to each other,
    not how they are placed in the original feature space. There are still issues
    around unscaled features. Feature scaling is important and should be done anyway
    (we did it using tf**-**idf in this chapter, which results in feature values having
    the same scale).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，EAC现在只关心事物之间的接近程度，而不是它们在原始特征空间中的位置。仍然存在未缩放特征的问题。特征缩放很重要，无论如何都应该进行（我们在本章中使用了tf**-**idf进行缩放，这导致特征值具有相同的尺度）。
- en: We saw a similar type of transformation in [Chapter 9](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)*,
    Authorship Attribution*, through the use of kernels in SVMs. These transformations
    are very powerful and should be kept in mind for complex datasets. The algorithms
    for remapping data onto a new feature space does not need to be complex though,
    as you'll see in the EAC algorithm.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](3b134aaf-e967-486f-adc6-83f8ed1943c4.xhtml)*作者归属分析*中看到了类似类型的转换，通过在SVM中使用核。这些转换非常强大，应该记住用于复杂的数据集。然而，将数据重新映射到新特征空间上的算法不需要太复杂，正如你将在EAC算法中看到的那样。
- en: Implementation
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: Putting all this all together, we can now create a clustering algorithm fitting
    the scikit-learn interface that performs all of the steps in EAC. First, we create
    the basic structure of the class using scikit-learn's *ClusterMixin*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们现在可以创建一个符合scikit-learn接口的聚类算法，该算法执行EAC中的所有步骤。首先，我们使用scikit-learn的*ClusterMixin*创建类的基本结构。
- en: Our parameters are the number of k-means clusterings to perform in the first
    step (to create the co-association matrix), the threshold to cut off at, and the
    number of clusters to find in each k-means clustering. We set a range of n_clusters
    in order to get lots of variance in our k-means iterations. Generally, in ensemble
    terms, variance is a good thing; without it, the solution can be no better than
    the individual clusterings (that said, high variance is not an indicator that
    the ensemble will be better).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的参数包括在第一步中执行k-means聚类的数量（用于创建共关联矩阵）、截断的阈值以及每个k-means聚类中要找到的聚类数量。我们设置了一个n_clusters的范围，以便在我们的k-means迭代中获得大量的方差。一般来说，在集成术语中，方差是一件好事；没有它，解决方案可能不会比单个聚类更好（尽管如此，高方差并不是集成将更好的指标）。
- en: 'I''ll present the full class first, and then overview each of the functions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先展示完整的类，然后概述每个函数：
- en: '[PRE49]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The goal of the `fit` function is to perform the k-means clusters a number of
    times, combine the co-association matrices and then split it by finding the MST,
    as we saw earlier with the EAC example. We then perform our low-level clustering
    using k-means and sum the resulting co-association matrices from each iteration.
    We do this in a generator to save memory, creating only the co-association matrices
    when we need them. In each iteration of this generator, we create a new single
    k-means run with our dataset and then create the co-association matrix for it.
    We use `sum` to add these together.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`函数的目标是执行k-means聚类多次，合并共关联矩阵，然后通过找到MST来分割它，就像我们在之前的EAC示例中看到的那样。然后我们使用k-means执行我们的低级聚类，并将每个迭代的共关联矩阵结果相加。我们这样做是为了节省内存，只在需要时创建共关联矩阵。在这个生成器的每个迭代中，我们使用我们的数据集创建一个新的单次k-means运行，然后为其创建共关联矩阵。我们使用`sum`将这些值相加。'
- en: As before, we create the MST, remove any edges less than the given threshold
    (properly negating values as explained earlier), and find the connected components.
    As with any fit function in scikit-learn, we need to return self in order for
    the class to work in pipelines effectively.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们创建最小生成树（MST），移除任何小于给定阈值的边（如前所述，正确地取反值），并找到连通分量。与scikit-learn中的任何fit函数一样，我们需要返回self，以便类在管道中有效地工作。
- en: The `_single_clustering` function is designed to perform a single iteration
    of k-means on our data, and then return the predicted labels. To do this, we randomly
    choose a number of clusters to find using NumPy's `randint` function and our `n_clusters_range`
    parameter, which sets the range of possible values. We then cluster and predict
    the dataset using k-means. The return value here will be the labels coming from
    k-means.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`_single_clustering`函数被设计用来在我们的数据上执行一次k-means迭代，然后返回预测的标签。为此，我们使用NumPy的`randint`函数和我们的`n_clusters_range`参数随机选择要找到的聚类数量，该参数设置了可能值的范围。然后我们使用k-means聚类和预测数据集。这里的返回值将是来自k-means的标签。'
- en: Finally, the `fit_predict` function simply calls fit, and then returns the labels
    for the documents.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`fit_predict`函数简单地调用fit，然后返回文档的标签。
- en: 'We can now run this on our previous code by setting up a pipeline as before
    and using EAC where we previously used a KMeans instance as our final stage of
    the pipeline. The code is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过设置一个与之前相同的管道并使用EAC（而不是之前作为管道的最终阶段的KMeans实例）来运行此代码。代码如下：
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Online learning
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习
- en: In some cases, we don't have all of the data we need for training before we
    start our learning. Sometimes, we are waiting for new data to arrive, perhaps
    the data we have is too large to fit into memory, or we receive extra data after
    a prediction has been made. In cases like these, online learning is an option
    for training models over time.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '**Online learning** is the incremental updating of a model as new data arrives.
    Algorithms that support online learning can be trained on one or a few samples
    at a time, and updated as new samples arrive. In contrast, algorithms that are
    not **online** require access to all of the data at once. The standard k-means
    algorithm is like this, as are most of the algorithms we have seen so far in this
    book.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Online versions of algorithms have a means to partially update their model with
    only a few samples. Neural networks are a standard example of an algorithm that
    works in an online fashion. As a new sample is given to the neural network, the
    weights in the network are updated according to a learning rate, which is often
    a very small value such as 0.01\. This means that any single instance only makes
    a small (but hopefully improving) change to the model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can also be trained in batch mode, where a group of samples
    is given at once and the training is done in one step. Algorithms are generally
    faster in batch mode but use more memory.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: In this same vein, we can slightly update the k-means centroids after a single
    or small batch of samples. To do this, we apply a learning rate to the centroid
    movement in the updating step of the k-means algorithm. Assuming that samples
    are randomly chosen from the population, the centroids should tend to move towards
    the positions they would have in the standard, offline, and k-means algorithm.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Online learning is related to streaming-based learning; however, there are some
    important differences. Online learning is capable of reviewing older samples after
    they have been used in the model, while a streaming-based machine learning algorithm
    typically only gets one pass—that is, one opportunity to look at each sample.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn package contains the **MiniBatchKMeans** algorithm, which allows
    online learning. This class implements a partial_fit function, which takes a set
    of samples and updates the model. In contrast, calling fit() will remove any previous
    training and refit the model only on the new data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: MiniBatchKMeans follows the same clustering format as other algorithms in scikit-learn,
    so creating and using it is much the same as other algorithms.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works by taking a streaming average of all points that it has
    seen. To compute this, we only need to keep track of two values, which are the
    current sum of all seen points, and the number of points seen. We can then use
    this information, combined with a new set of points, to compute the new averages
    in the updating step.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can create a matrix X by extracting features from our dataset
    using `TfIDFVectorizer`, and then sample from this to incrementally update our
    model. The code is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We then import MiniBatchKMeans and create an instance of it:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we will randomly sample from our X matrix to simulate data coming in
    from an external source. Each time we get some data in, we update the model:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can then get the labels for the original dataset by asking the instance
    to predict:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: At this stage, though, we can't do this in a pipeline as `TfidfVectorizer` is
    not an online algorithm. To get over this, we use a `HashingVectorizer`. The `HashingVectorizer`
    class is a clever use of hashing algorithms to drastically reduce the memory of
    computing the bag-of-words model. Instead of recording the feature names, such
    as words found in documents, we record only hashes of those names. This allows
    us to know our features before we even look at the dataset, as it is the set of
    all possible hashes. This is a very large number, usually of the order of 2^(18).
    Using sparse matrices, we can quite easily store and compute even a matrix of
    this size, as a very large proportion of the matrix will have the value 0.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the `Pipeline` class doesn''t allow for its use in online learning.
    There are some nuances in different applications that mean there isn''t an obvious
    one-size-fits-all approach that could be implemented. Instead, we can create our
    own subclass of Pipeline, which allows us to use it for online learning. We first
    derive our class from Pipeline, as we only need to implement a single function:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The only function we need to implement is the `partial_fit` function, which
    is performed by first doing all transformation steps, and then calling partial
    fit on the final step (which should be the classifier or clustering algorithm).
    All other functions are the same as in the normal Pipeline, class, so we refer
    (through class inheritance) to those.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create a pipeline to use our `MiniBatchKMeans` in online learning,
    alongside our `HashingVectorizer`. Other than using our new classes `PartialFitPipeline`
    and `HashingVectorizer`, this is the same process as used in the rest of this
    chapter, except we only fit on a few documents at a time. The code is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: There are some downsides to this approach. For one, we can't easily find out
    which words are most important for each cluster. We can get around this by fitting
    another `CountVectorizer` and taking the hash of each word. We then look up values
    by hash rather than word. This is a bit cumbersome and defeats the memory gains
    from using HashingVectorizer. Further, we can't use the `max_df` parameter that
    we used earlier, as it requires us to know what the features mean and to count
    them over time.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: We also can't use tf-idf weighting when performing training online. It would
    be possible to approximate this and apply such weighting, but again this is a
    cumbersome approach. `HashingVectorizer` is still a very useful algorithm and
    a great use of hashing algorithms.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at clustering, which is an unsupervised learning
    approach. We use unsupervised learning to explore data, rather than for classification
    and prediction purposes. In the experiment here, we didn't have topics for the
    news items we found on reddit, so we were unable to perform classification. We
    used k-means clustering to group together these news stories to find common topics
    and trends in the data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: In pulling data from reddit, we had to extract data from arbitrary websites.
    This was performed by looking for large text segments, rather than a full-blown
    machine learning approach. There are some interesting approaches to machine learning
    for this task that may improve upon these results. In the Appendix of this book,
    I've listed, for each chapter, avenues for going beyond the scope of the chapter
    and improving upon the results. This includes references to other sources of information
    and more difficult applications of the approaches in each chapter.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at a straightforward ensemble algorithm, EAC. An ensemble is
    often a good way to deal with variance in the results, especially if you don't
    know how to choose good parameters (which is always difficult with clustering).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced online learning. This is a gateway to larger learning
    exercises, including big data, which will be discussed in the final two chapters
    of this book. These final experiments are quite large and require management of
    data as well as learning a model from them.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: As an extension on the work in this chapter, try implementing EAC to be an online
    learning algorithm. This is not a trivial task and will involve some thought on
    what should happen when the algorithm is updated. Another extension is to collect
    more data from more data sources (such as other subreddits or directly from news
    websites or blogs) and look for general trends.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll step away from unsupervised learning and go back
    to classification. We will look at deep learning, which is a classification method
    built on complex neural networks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
