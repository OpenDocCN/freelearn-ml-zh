["```py\nimport pandas as pdfrom aif360.sklearn.metrics import disparate_impact_ratio\ndef dir_grouping(data_df: pd.DataFrame,\n    sensitive_attr: str, priviledge_group,\n    dir_threshold = {'high': 1.2, 'low': 0.8}):\n        \"\"\"\n        Categorizing data as fair or unfair according to DIR\n        :param data_df: Dataframe of dataset\n        :param sensitive_attr: Sensitive attribute under investigation\n        :priviledge_group: The category in the sensitive attribute that needs to be considered as priviledged\n        :param dir_threshold:\n        \"\"\"\n    dir = disparate_impact_ratio(data_df,\n        prot_attr=sensitive_attr,\n        priv_group=priviledge_group, pos_label=True)\n    if dir < dir_threshold['high'] and dir > dir_threshold[\n        'low']:\n        assessment = \"unbiased data\"\n    else:\n        assessment = \"biased data\"\n    return assessment\n```", "```py\n# calculating DIR for a subset of adult income data in shap libraryimport shap\nX,y = shap.datasets.adult()\nX = X.set_index('Sex')\nX_subset = X.iloc[0:100,]\n```", "```py\nimport pytest# you can use ipytest if you are using Jupyter or Colab notebook\nimport ipytest\nipytest.autoconfig()\n```", "```py\n%%ipytest -qqdef test_dir_grouping():\n    bias_assessment = dir_grouping(data_df = X_subset,\n        sensitive_attr = 'Sex',priviledge_group = 1,\n        dir_threshold = {'high':1.2, 'low': 0.8})\n    assert bias_assessment == \"biased data\"\n```", "```py\npytest test_script.py\n```", "```py\n# \"testdir\" could be a directory containing test scriptspytest testdir/\n```", "```py\npytest\n```", "```py\npython -m pytest\n```", "```py\nipytest.run()\n```", "```py\n# Example of using Pytest fixtures available in https://docs.pytest.org/en/7.1.x/how-to/fixtures.html\nclass Fruit:\n    def __init__(self, name):\n        self.name = name\n        self.cubed = False\n    def cube(self):\n        self.cubed = True\nclass FruitSalad:\n    def __init__(self, *fruit_bowl):\n        self.fruit = fruit_bowl\n        self._cube_fruit()\n    def _cube_fruit(self):\n        for fruit in self.fruit:\n            fruit.cube()\n```", "```py\n# Arrange@pytest.fixture\ndef fruit_bowl():\n    return [Fruit(\"apple\"), Fruit(\"banana\")]\ndef test_fruit_salad(fruit_bowl):\n    # Act\n    fruit_salad = FruitSalad(*fruit_bowl)\n    # Assert\n    assert all(fruit.cubed for fruit in fruit_salad.fruit)\n```", "```py\nimport pandas as pdimport numpy as np\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier as RF\nfrom sklearn.datasets import load_breast_cancer\nimport mlflow\nimport mlflow.sklearn\nnp.random.seed(42)\n```", "```py\ndef eval_metrics(actual, pred, pred_proba):    rmse = np.sqrt(mean_squared_error(actual, pred))\n    roc_auc = roc_auc_score(actual, pred_proba)\n    return rmse, roc_auc\n```", "```py\nX, y = load_breast_cancer(return_X_y=True)# split the data into training and test sets. (0.7, 0.3) split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n    test_size = 0.3, random_state=42)\n```", "```py\nexperiment_name = \"mlflow-randomforest-cancer\"existing_exp = mlflow.get_experiment_by_name(\n    experiment_name)\nif not existing_exp:\n    experiment_id = mlflow.create_experiment(\n        experiment_name, artifact_location=\"...\")\nelse:\n    experiment_id = dict(existing_exp)['experiment_id']\nmlflow.set_experiment(experiment_name)\n```", "```py\nfor idx, n_estimators in enumerate([5, 10, 20]):    rf = RF(n_estimators = n_estimators, random_state = 42)\n    rf.fit(X_train, y_train)\n    pred_probs = rf.predict_proba(X_test)\n    pred_labels = rf.predict(X_test)\n    # calculating rmse and roc-auc for the randorm forest\n    # model predictions on the test set\n    rmse, roc_auc = eval_metrics(actual = y_test,\n        pred = pred_labels,pred_proba = [\n            iter[1]for iter in pred_probs])\n    # start mlflow\n    RUN_NAME = f\"run_{idx}\"\n    with mlflow.start_run(experiment_id=experiment_id,\n        run_name=RUN_NAME) as run:\n            # retrieve run id\n            RUN_ID = run.info.run_id\n        # track parameters\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        # track metrics\n        mlflow.log_metric(\"rmse\", rmse)\n        # track metrics\n        mlflow.log_metric(\"roc_auc\", roc_auc)\n        # track model\n        mlflow.sklearn.log_model(rf, \"model\")\n```", "```py\nfrom mlflow.tracking import MlflowClientesperiment_name = \"mlflow-randomforest-cancer\"\nclient = MlflowClient()\n# retrieve experiment information\nexperiment_id = client.get_experiment_by_name(\n    esperiment_name).experiment_id\n```", "```py\n# retrieve runs information (parameter: 'n_estimators',    metric: 'roc_auc')\nexperiment_info = mlflow.search_runs([experiment_id])\n# extracting run ids for the specified experiment\nruns_id = experiment_info.run_id.values\n# extracting parameters of different runs\nruns_param = [client.get_run(run_id).data.params[\n    \"n_estimators\"] for run_id in runs_id]\n# extracting roc-auc across different runs\nruns_metric = [client.get_run(run_id).data.metrics[\n    \"roc_auc\"] for run_id in runs_id]\n```", "```py\ndf = mlflow.search_runs([experiment_id],    order_by=[\"metrics.roc_auc\"])\nbest_run_id = df.loc[0,'run_id']\nbest_model_path = client.download_artifacts(best_run_id,\n    \"model\")\nbest_model = mlflow.sklearn.load_model(best_model_path)\nprint(\"Best model: {}\".format(best_model))\n```", "```py\nBest mode: RandomForestClassifier(n_estimators=5,    random_state=42)\n```", "```py\n# delete runs (make sure you are certain about deleting the runs)for run_id in runs_id:\n    client.delete_run(run_id)\n# delete experiment (make sure you are certain about deleting the experiment)\nclient.delete_experiment(experiment_id)\n```"]