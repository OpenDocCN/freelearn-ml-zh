- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with Data in AMLS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In **Machine Learning** (**ML**), regardless of the use case or the algorithm
    we use, an important component that will always be used is data. Without data,
    you cannot build machine learning models. The quality of the data is very critical
    for building performant models. Complex models such as deep neural networks require
    a lot more data than simpler models. Data in an ML workflow will often come from
    a variety of data sources and require different methods to be leveraged for data
    processing, cleansing, and feature selection. During this process of feature engineering,
    your Azure Machine Learning workspace will be leveraged to empower you to collaboratively
    work with your data. This will ensure secure connectivity to a variety of data
    sources, as well as enable you to register your datasets for use in training,
    testing, and validation.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of steps within this workflow, we may be required to take raw
    data, join with an additional dataset, cleanse data to remove duplicates, fill
    in missing values, and perform an initial analysis to identify outliers and skew
    in our data. This can be done even before an algorithm is selected to begin building
    a model to train leveraging our features and labels within our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning provides methods for connecting to a variety of data
    sources and registering datasets to be used to build a model, so you can use your
    data in a business context.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning datastore overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Blob Storage account datastore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Azure Machine Learning data assets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Azure Machine Learning data assets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read [*Chapter 1*](B18003_01.xhtml#_idTextAnchor020), *Introducing the Azure
    Machine Learning Service*, to get the environment workspace created to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the prerequisites for the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web browser, preferably Google Chrome or Microsoft Edge Chromium.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A supported storage service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to supported storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To access the Azure machine learning service workspace, please go to [https://ml.azure.com](https://ml.azure.com).
    Select the workspace from the drop-down list on the left in your web browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Machine Learning datastore overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within an Azure Machine Learning workspace, a storage service that is the source
    of data is registered as a datastore for reusability. A datastore securely holds
    connectivity information for accessing data within the key vault that was created
    with your Azure Machine Learning workspace. The credentials supplied to the datastore
    are used to access the data within a given data service. These datastores can
    be created via the Azure Machine Learning Studio through the Azure Machine Learning
    SDK for Python, or the Azure Machine Learning **command-line interface** (**CLI**).
    Datastores enable data scientists to connect to data by name rather than passing
    connection information within scripts. This allows the portability of code through
    different environments (in different environments, a datastore may point to different
    services) and prevents the leaking of sensitive credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supported datastores include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Blob Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure SQL Database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake Gen 1 (deprecated)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake Gen 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure file share
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Database for PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Database for MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks File System
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the supported datastores will have an authentication type associated
    with the data service. When creating a datastore, the authentication type will
    be selected and leveraged within the Azure Machine Learning workspace. Note that
    Azure Database for MySQL is currently only supported for the `DataTransferStep`
    pipeline and thus cannot be created with Azure Machine Learning Studio. The Databricks
    File System is only supported for the `DatabricksStep` pipeline, and thus cannot
    be created leveraging Azure Machine Learning Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides the authentication options for the Azure Storage
    types available for use with your Azure Machine Learning workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Storage Type** | **Authentication** **Options Available** |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Blob Container | SAS token, account key |'
  prefs: []
  type: TYPE_TB
- en: '| Azure SQL Database | Service principal, SQL authentication |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Data Lake Gen 1 | Service principal |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Data Lake Gen 2 | Service principal |'
  prefs: []
  type: TYPE_TB
- en: '| Azure File Share | SAS token, account key |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Database for PostgreSQL | SQL authentication |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Database for MySQL | SQL authentication |'
  prefs: []
  type: TYPE_TB
- en: '| Databricks File System | No authentication |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.1 – Supported authentication for Azure Storage types
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an understanding of the types of supported datastores, in
    the next section, you will learn how to connect your Azure Machine Learning workspace
    to datastores that you can use within your ML workflow. The most common and recommended
    datastore is an Azure blob container. In fact, one was created for you in [*Chapter
    1*](B18003_01.xhtml#_idTextAnchor020), *Introducing the Azure Machine Learning
    Service*, as part of the workspace deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing to the next section to create datastores using Azure Machine
    Learning Studio, the Python SDK, and the Azure Machine Learning CLI, we will briefly
    review the default datastore that was created for you.
  prefs: []
  type: TYPE_NORMAL
- en: Default datastore review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As noted in [*Chapter 1*](B18003_01.xhtml#_idTextAnchor020), *Introducing the
    Azure Machine Learning Service*, the left navigation includes a **Data** section,
    which you can use to access the datastores, as shown in *Figure 2**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on the hamburger icon on the top of the left navigation within your
    workspace will include words with the icons in your navigation bar.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the **Datastore** tab, you can see the storage accounts that
    have already been created for your workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Datastores](img/B18003_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Datastores
  prefs: []
  type: TYPE_NORMAL
- en: '**workspaceblobstore** is the default Azure Machine Learning workspace datastore
    and holds experiment logs as well as workspace artifacts. Data can be uploaded
    to this default datastore, which will be covered in an upcoming section. **workspacefilestore**
    is used to store your notebooks that are created within your Azure Machine Learning
    workspace.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to connect to a new datastore through Azure
    Machine Learning Studio, through the Azure Machine Learning Python SDK, as well
    as through the Azure Machine Learning CLI. This will enable you to use data where
    the data lives instead of bringing it into the default datastore associated with
    your Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a blob storage account datastore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the previous section, *Default datastore review*, we can create
    a datastore through Azure Machine Learning Studio, through the Azure Machine Learning
    Python SDK, and through the Azure Machine Learning CLI. In the next section, we
    will walk through creating a datastore for a blob storage account with each of
    these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a blob storage account datastore through Azure Machine Learning Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to create a blob storage account datastore, first you need to create
    a storage account that contains a blob. Follow these steps to create an Azure
    storage account and create blob storage within that storage account:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure portal at [https://ms.portal.azure.com/#home](https://ms.portal.azure.com/#home).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find **Storage accounts** under **Azure Services**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `amlv2sa`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the storage account is created, you can see it under **Storage accounts**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go ahead and click on the newly created storage account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then from the left side navigation click on `datacontainer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, go back to Azure Machine Learning Studio, click on the **Data** icon on
    the left navigation, go to the **Datastores** tab as shown in *Figure 2**.2*,
    and click on **+Create**. A new **Create datastore** pane will open, as shown
    in *Figure 2**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Create datastore](img/B18003_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Create datastore
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Create datastore** pane, configure the required settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Set `azureblobdatastore`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that this is an Azure blob storage account, leave `Azure` `Blob Storage`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your **Subscription ID** – note it should default to the Azure subscription
    of your workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the storage account that you just created (`amlv2sa`) by clicking on the
    dropdown under **Storage account**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the blob container that you just created (`datacontainer`) by clicking
    on the dropdown under **Blob container**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set **Save credentials with the datastore for data access** to **Yes**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set **Authentication type** to **Account key**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the **Account key** by entering the value found in the **Access Keys** section
    of your storage account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set **Use workspace managed identity for data preview and profiling in Azure
    Machine Learning studio** to **Yes**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will grant your Azure Machine Learning service workspace’s managed **identity
    Reader** and **Storage Blob Data** **Reader** access.
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can verify that a datastore called `azureblobdatastore` has been created
    for you by viewing the **Datastores** tab shown in *Figure 2**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to easily configure a datastore through the UI, we
    will continue with creating a datastore through the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a blob storage account datastore through the Python SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to use the Python SDK, you need to run Python scripts in a Jupyter
    notebook. To start a Jupyter notebook, please click on the **Compute** tab on
    the left navigation, as shown in *Figure 2**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Opening Jupyter Server from a compute instance](img/B18003_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Opening Jupyter Server from a compute instance
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, from an existing compute instance click on **Jupyter** to open Jupyter
    Server. Under **New**, click on **Python 3.10 – SDKV2** in order to create a new
    Jupyter notebook, as shown in *Figure 2**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Creating a new Jupyter notebook](img/B18003_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Creating a new Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the Azure Machine Learning Python SDK, an Azure blob container can be
    registered to your Azure Machine Learning workspace leveraging the following code
    in *Figure 2**.6*. Recall from when we created a new datastore through the UI
    that the value of the account key can be found in the **Access Keys** section
    of your storage account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Using the Python SDK to create a blob storage account datastore](img/B18003_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Using the Python SDK to create a blob storage account datastore
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that a new blob storage datastore called **blob_storage** has
    been created if you click on **Data** in the left navigation and then the **Datastores**
    option, as shown in *Figure 2**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – List of datastores created in the workspace](img/B18003_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – List of datastores created in the workspace
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s created a blob storage account datastore with the Azure Machine
    Learning CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a blob storage account datastore through the Azure Machine Learning
    CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming you have followed the instructions in [*Chapter 1*](B18003_01.xhtml#_idTextAnchor020),
    *Introducing the Azure Machine Learning Service*, to install the Azure CLI and
    the machine learning extension in your local environment, you can run the following
    command to create a blob storage account datastore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – CLI command to create a blob storage account datastore](img/B18003_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – CLI command to create a blob storage account datastore
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding command, `blobstore.yml` is a YAML file schema specifying
    the datastore type, name, description, storage account name, and credentials for
    the storage account, as shown in *Figure 2**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Blob datastore YAML schema file](img/B18003_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Blob datastore YAML schema file
  prefs: []
  type: TYPE_NORMAL
- en: You can verify that a new blob storage datastore called `blob_storage_cli` has
    been created if you click on the **Datastores** tab, as shown earlier in *Figure
    2**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have successfully created a blob storage datastore, within your
    Azure Machine Learning workspace, you will be able to use this datastore for multiple
    data assets. The connection information to this datastore is securely held within
    your Azure key vault, and you have a location to store data as it is generated.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Azure Machine Learning data assets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the previous datastore is created, the next step is to create a data asset.
    Please note that we will be using the terms “data asset” and “dataset” interchangeably
    throughout the chapter. A dataset is a logical connection to the datastore with
    versioning and schema management, such as choosing which columns of the data to
    use, the types of the columns in the dataset, and some statistics about the data.
    Data assets abstract the code from configuring data to be read. Also, data assets
    are very useful when we run multiple models as each model can be configured to
    read the dataset name instead of configuring or programming how to connect to
    the dataset and read it. This makes it easier to scale the model training.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, you will learn how to create datasets using the Azure
    Machine Learning Python SDK, CLI, and UI. Datasets allow us to create versions
    based on schema changes without changing the underlying datastore that holds the
    data. Specific versions can be used within code. We can also create profiles on
    the data for each dataset created and stored for further data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data asset using the UI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Azure Machine Learning Studio provides a great interface for creating a dataset
    through a guided UI. In order to create a dataset using Azure Machine Learning
    Studio, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the workspace UI, click **Data** and make sure that the **Data assets** option
    is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, click **+ Create** and fill out the **Create data asset** form, as shown
    in *Figure 2**.10*, and make sure to select **Tabular** in the **Type** field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Create data asset](img/B18003_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Create data asset
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen, select **From local files** and click **Next** to see the
    **Select a datastore** screen, as shown in *Figure 2**.11.* Go ahead and select
    the **blob_storage** datastore that you created in the last section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Select a datastore](img/B18003_02_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Select a datastore
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the datastore is selected, you are provided with the option to choose
    the path where the file is located, as shown in *Figure 2**.12*. For this example,
    we will use the `titanic.csv` file, which can be downloaded from our GitHub repository.
    Enter the path where you downloaded and saved the file and then click **Next**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Upload your data file](img/B18003_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Upload your data file
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the **Settings and preview** screen. On this screen, the file is automatically
    parsed, and options are displayed for the format detected. In our case, it’s a
    CSV file, and settings for CSV file format are shown. Check the preview section
    to validate whether the dataset shows the data in the right format like identifying
    columns, header, and values. If the format is not detected, then you can change
    the settings for **File Format**, **Delimiter**, **Encoding**, **Column headers**,
    and **Skip rows**, as shown in *Figure 2**.13*. If the dataset contains multi-line
    data, then check the option for **Dataset contains multi-line data**. Once the
    settings are configured properly for your dataset, click the **Next** button to
    go to the next section regarding schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Settings and preview screen](img/B18003_02_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Settings and preview screen
  prefs: []
  type: TYPE_NORMAL
- en: On this screen, the system will identify the schema of the data and display
    it for review, allowing changes as needed. Typically, a CSV or text file may require
    schema changes. As an example, a column may have an incorrect data type, so be
    sure to select the correct data type.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The first 200 rows are used to detect the column type. If the dataset has a
    column type mismatch, consider cleaning your dataset before registering using
    a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the available data type formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '**String**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integer**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boolean**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decimal (****dot ‘.’)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decimal (****comma ‘,’)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2**.14* shows the schema information for your dataset. Check the **Type**
    header dropdown to see the available data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Schema screen](img/B18003_02_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Schema screen
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down and review all the columns. For your registered data asset, you
    can choose to include or exclude a given column. The option to **Include** is
    available per column, as shown in *Figure 2**.14*. This screen also includes the
    option to **Search** a column. Once the screen is properly configured for your
    data asset, click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next is the **Review** screen. Confirm all the settings that were selected
    previously are correct and then click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Confirm review screen](img/B18003_02_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Confirm review screen
  prefs: []
  type: TYPE_NORMAL
- en: During the review, if any changes are required, click the **Back** button and
    change the settings.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data asset creation is complete, you will see the `titanicdataset`
    data asset page, which includes different options, such as **Explore**, **Consume**,
    and **Generate Profile**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Consume** option, as shown in *Figure 2**.16*, to review the
    code for retrieving your registered dataset by name and displaying a pandas dataframe
    from your dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Consuming the data asset using Python](img/B18003_02_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Consuming the data asset using Python
  prefs: []
  type: TYPE_NORMAL
- en: The code shown in *Figure 2**.16* can be copied and pasted directly into an
    Azure Machine Learning notebook and run on your Azure Machine Learning compute
    instance, as discussed later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the **Consume** option for the registered dataset, you can select the
    **Generate profile** option to begin a guided tour through profiling your dataset,
    as shown in *Figure 2**.17*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Generate profile screen](img/B18003_02_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Generate profile screen
  prefs: []
  type: TYPE_NORMAL
- en: If you want to create a new version of the dataset, click **New Version**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is also the **Explore** option to view a sample of the data set. Only
    the first 50 rows are shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Explore screen](img/B18003_02_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Explore screen
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we showed you how to create data assets using the UI. In the
    next section, we will show you how to create data assets using the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data asset using the Python SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you how to create a data asset using the Python
    SDK. As mentioned in the previous section, you can create data from datastores,
    local files, and public URLs. The Python script to create a data asset from a
    local file (for example, `titanic.csv`) is shown in *Figure 2**.19*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that in the following code snippet, `type = AssetTypes.mltable`
    abstracts the schema definition for the tabular data, making it easier to share
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Creating a data asset via the Python SDK](img/B18003_02_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Creating a data asset via the Python SDK
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `my_data` folder, there are two files:'
  prefs: []
  type: TYPE_NORMAL
- en: The actual data file, which in this case is `titanic.csv`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mltable` file, which is a YAML file specifying the data’s schema so that
    the `mltable` engine can use it in order to materialize the data into an in-memory
    object such as pandas or DASK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2**.20* shows the `mltable` YAML file for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – The mltable YAML file for creating an mltable data asset](img/B18003_02_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – The mltable YAML file for creating an mltable data asset
  prefs: []
  type: TYPE_NORMAL
- en: If you head back to the **Data** tab under **Data assets**, you will see that
    a new dataset called **titanic-mltable-sdk** has been created with its type set
    to **Table(mltable)** and its version to **1**.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we showed you how to create a data asset using the Python SDK.
    In the next section, you will learn how to consume a data asset.
  prefs: []
  type: TYPE_NORMAL
- en: Using Azure Machine Learning datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter, we have covered what an Azure Machine Learning datastore
    is and how to connect to a variety of supported data sources. We created connections
    to Azure Machine Learning datastores using Azure Machine Learning Studio, the
    Python SDK, and the Azure CLI. We have just covered Azure Machine Learning datasets,
    a valuable asset for your ML projects. We went through how to generate Azure Machine
    Learning datasets using Azure Machine Learning Studio and the Python SDK. Once
    an Azure Machine Learning dataset is created, it can be used throughout your Azure
    Machine Learning experiments, which are called **jobs**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.21* shows a code snippet for materializing the `mltable` artifact
    into a pandas dataframe. Please note that you need the `mltable` library installed
    in your environment (using the `pip install` `mltable` command).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Materializing the mltable artifact into a pandas dataframe](img/B18003_02_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – Materializing the mltable artifact into a pandas dataframe
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see how to use a data asset inside an ML job, which will be covered
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Read data in a job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Azure Machine Learning job consists of a Python script, which could be simple
    data processing or a complex code for model development, a Bash command to specify
    tasks to be performed, the inputs and outputs of the job, the Docker environment
    specifying the runtime libraries required to run the job and a compute where the
    Docker container would run on. The code that is executed inside a job will probably
    need to use a dataset. The primary way to pass data to an Azure Machine Learning
    job is by using datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk you through the steps needed to run a job that takes a dataset as
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an Azure Machine Learning environment, which is where your process to
    train a model or process your data takes place. *Figure 2**.22* shows the code
    snippet to create an environment called `env_docker_conda`, which will be used
    in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.22 – Creating the Azure Machine Learning environment](img/B18003_02_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 – Creating the Azure Machine Learning environment
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, `env-mltable.yml`, which is shown in *Figure 2**.22*,
    is a YAML file defining the Python libraries that need to be installed in the
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23 – Environment specification YAML file](img/B18003_02_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.23 – Environment specification YAML file
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a Python script to process your data and build a model. For this chapter,
    we will show you a simple Python script that takes an input dataset, converts
    it to a pandas dataframe, and then prints it. *Figure 2**.24* shows the script
    saved in a file called `read_data.py`, which will be used in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.24 – Python script processing the input dataset](img/B18003_02_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.24 – Python script processing the input dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an Azure Machine Learning compute cluster where the Azure Machine Learning
    containerized job will be submitted. *Figure 2**.25* shows the Python script to
    create a compute cluster called `cpu-cluster` by specifying its type and the minimum
    and maximum number of nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 2.25 – Create Azure Machine Learning compute cluster](img/B18003_02_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.25 – Create Azure Machine Learning compute cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you have all the necessary pieces to construct an Azure Machine Learning
    job and submit it for execution. *Figure 2**.26* shows the Python script for creating
    an Azure Machine Learning job called `job`. This job is essentially a Docker container
    containing your Python code in `read_data.py`, which is taking your previously
    created input dataset and is submitted to the compute cluster you created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.26 – Creating an Azure Machine Learning job](img/B18003_02_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.26 – Creating an Azure Machine Learning job
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the Jupyter notebook cell is a link to the job in Azure Machine
    Learning Studio, which displays the job’s overview, its status, the Python code,
    and the output of the job. If you navigate to this link and click on **Outputs
    + logs**, then **std_log.txt** under **user_logs**, you will see the output generated
    by the Python code, which is printing the input dataset to the standard log, as
    shown in *Figure 2**.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27 – Output of the Azure Machine Learning job after successful execution](img/B18003_02_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.27 – Output of the Azure Machine Learning job after successful execution
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize the chapter now.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have explored Azure Machine Learning datastores, which
    enable you to connect to datastore services. You have also learned about Azure
    Machine Learning datasets, empowering you to create a reference to a location
    within a datastore. These assets within Azure Machine Learning can be created
    through the UI for a low code experience, as well as through the Azure Machine
    Learning Python SDK or the Azure Machine Learning CLI. Once these references are
    created, datasets can be retrieved and used through the Azure Machine Learning
    Python SDK. Once the dataset has been retrieved, it can easily be converted into
    a pandas dataframe for use within your code. You have also seen how to use datasets
    within an Azure Machine Learning job by passing them as input to the job.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B18003_03.xhtml#_idTextAnchor053), *Training Machine Learning
    Models in AMLS*, you will explore training models; experiments will become a key
    asset in your toolbelt, enabling traceability as you build your model in AMLS.
  prefs: []
  type: TYPE_NORMAL
