<html><head></head><body>
		<div id="_idContainer143">
			<h1 id="_idParaDest-208"><em class="italic"><a id="_idTextAnchor215"/>Chapter 11</em>: Catastrophic Forgetting</h1>
			<p>In the previous two chapters, we started to look at a number of auxiliary tasks for online machine learning and working with streaming data. <a href="B18335_09_ePub.xhtml#_idTextAnchor184"><em class="italic">Chapter 9</em></a> covered drift detection and solutions and <a href="B18335_10_ePub.xhtml#_idTextAnchor201"><em class="italic">Chapter 10</em></a> covered feature transformation and scaling in a streaming context. The current chapter introduces a third and final topic to this list of auxiliary tasks, namely catastrophic forgetting.</p>
			<p>Catastrophic forgetting, also known as catastrophic interference, is the tendency of machine learning models to forget what they have learned upon new updates, wrongly de-learning correctly learned older tendencies as new tendencies are learned from new data.</p>
			<p>As you have seen a lot of examples of online models throughout this book, you will understand that continuous updating of the models creates a large risk of this learning going wrong. It has already been touched upon briefly, in the chapter on drift and drift detection, that model learning going wrong can also be seen as a real risk of performance degradation. </p>
			<p>Drift, however, tends to be used for pointing out drift in either the independent variables (data drift) or in the relations between independent variables and dependent variables (concept drift). As catastrophic forgetting is really a problem inside the coefficients of the model, we could not really consider catastrophic forgetting to be a part of drift.</p>
			<p>Machine learning models, especially online machine learning models, are often used in a relatively black-box manner, meaning that we look at their outcomes but do not necessarily spend much time looking at the inside mechanisms. This becomes a problem when detecting wrongly learned patterns. Machine learning explicability is therefore also related to the topic of catastrophic forgetting and will be covered as well.</p>
			<p>This chapter will cover the problem of machine learning models updating in the wrong manner, which we call catastrophic forgetting or catastrophic inference, with the following chapters being covered:</p>
			<ul>
				<li>Defining catastrophic forgetting</li>
				<li>Detection of catastrophic forgetting</li>
				<li>Model explicability versus catastrophic forgetting</li>
			</ul>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor216"/>Technical requirements</h1>
			<p>You can find all the code for this book on GitHub at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python">https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python</a>. If you are not yet familiar with Git and GitHub, the easiest way to download the notebooks and code samples is the following:</p>
			<ol>
				<li>Go to the link of the repository.</li>
				<li>Go to the green <strong class="bold">Code</strong> button.</li>
				<li>Select <strong class="bold">Download zip</strong>.</li>
			</ol>
			<p>When you download the ZIP file, you unzip it in your local environment, and you will be able to access the code through your preferred Python editor.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor217"/>Python environment</h2>
			<p>To follow along with this book, you can download the code in the repository and execute it using your preferred Python editor.</p>
			<p>If you are not yet familiar with Python environments, I would advise you to check out either Anaconda (<a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>), which comes with the Jupyter Notebook and JupyterLab, which are both great for executing notebooks. It also comes with Spyder and VS Code for editing scripts and programs.</p>
			<p>If you have difficulty installing Python or the associated programs on your machine, you can check out Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or Kaggle Notebooks (<a href="https://www.kaggle.com/code">https://www.kaggle.com/code</a>), which both allow you to run Python code in online notebooks for free, without any setup.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code in the book will generally use Colab and Kaggle Notebooks with Python version 3.7.13 and you can set up your own environment to mimic this. </p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor218"/>Introducing catastrophic forgetting</h1>
			<p>Catastrophic forgetting was initially defined as a problem that occurs on (deep) neural networks. Deep <a id="_idIndexMarker593"/>neural networks are a set of very complex machine learning models that, thanks to their extreme complexity, are able to learn very complex patterns. Of course, this is the case only when there is enough data.</p>
			<p>Neural networks have been studied for multiple decades. They used to be mathematically interesting but practically infeasible to execute due to the lack of computing power. The current-day progress in computing power has made it possible for neural networks to gain the popularity that they are currently observing.</p>
			<p>The complexity of neural networks also makes them sensitive to the problem of catastrophic forgetting. The way a neural network learns (from a high point of view) is by making many update passes to the coefficients and at every update, the model should fit a little bit better to the data. A schematic overview of a neural network's parameters can be seen here:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B18335_11_1.jpg" alt="Figure 11.1 – Schematic overview of the number of coefficients in a neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Schematic overview of the number of coefficients in a neural network</p>
			<p>In this schematic drawing, you see that even for a very small neural network there are many coefficients. The larger the number of nodes becomes, the larger the number of parameters to estimate. When comparing this to traditional statistical methods, you can see that the idea <a id="_idIndexMarker594"/>of making so many passes is relatively different and causes different problems than those that were common in traditional statistics. </p>
			<p>Catastrophic forgetting is one such problem. It was first observed in a study in 1989, in which an experiment was presented. This experiment trained neural networks on the task of doing additions (from 1 + 1 = 2 to 1 + 9 = 10). A sequential method was tested, in which the model first learned only the first task, and then a new task was added once the first one was mastered. </p>
			<p>The conclusion of this and other experiments was that adding new tasks after the first one has been learned will cause interference with the original learned model. They observed that the newer information has to be learned, the larger this disruption will be. Finally, they found out that the problem occurs in sequential learning only. If you learn all tasks at the same time, there is not really any re-learning happening so forgetting cannot really happen.</p>
			<p>For more detailed, scientific resources on catastrophic forgetting in the specific case of online learning using neural networks, I recommend checking out the two links here:</p>
			<ul>
				<li><a href="https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf</a></li>
				<li><a href="https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf">https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf</a></li>
			</ul>
			<p>Let's now see how <a id="_idIndexMarker595"/>catastrophic forgetting affects online models in general.</p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor219"/>Catastrophic forgetting in online models</h1>
			<p>Although catastrophic forgetting was initially identified as a problem for neural networks, you <a id="_idIndexMarker596"/>can imagine that online machine learning <a id="_idIndexMarker597"/>has the same problem of continuous re-learning. The problem of catastrophic forgetting, or catastrophic inference, is therefore also present and needs to be mastered.</p>
			<p>If models are updated at every new data point, it is expected that coefficients will change over time. Yet as modern-day machine learning algorithms are very complex and have huge numbers of coefficients or trees, it is a fairly difficult task to keep a close eye on them.</p>
			<p>In an ideal world, the most beneficial goal would probably be to try and avoid any wrong learning in your machine learning at all. One way to do this is to keep a close eye on model performance and keep tight versioning systems in place to make sure that even if your model is wrongly learning anything, it does not get deployed in a production system. We will go into this topic shortly.</p>
			<p>Another solution that is possible is to work with drift detection methods, as you saw in <a href="B18335_09_ePub.xhtml#_idTextAnchor184"><em class="italic">Chapter 9</em></a>. When you closely follow your model's performance and the distributions of your data, and other KPIs and descriptive statistics, you should be able to detect problems rather soon, which will allow you to intervene rapidly.</p>
			<p>As a third tool for managing catastrophic forgetting, you will see more tools for model explicability in this chapter. One of the problems of catastrophic forgetting is that the models are too much of a black box. Using tools from the domain of model explicability will help you to have a peek inside those black-box models. This will allow you to detect catastrophic forgetting and catastrophic inference based more on business logic rather than technical logic. The combination <a id="_idIndexMarker598"/>of business and <a id="_idIndexMarker599"/>technical logic together will be a strong combination to prepare against catastrophic forgetting.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor220"/>Detecting catastrophic forgetting</h1>
			<p>In this chapter, we are <a id="_idIndexMarker600"/>going to look at two different approaches that you could use to detect catastrophic forgetting. The first approach is to implement a system that can detect problems with a model just after it has learned something. To do this, we are going to implement a Python example in multiple steps:</p>
			<ol>
				<li value="1">Develop a model training loop with online learning.</li>
				<li>Add direct evaluation to this model.</li>
				<li>Add longer-term evaluation to this model.</li>
				<li>Add a system to avoid model updating in case of wrong learning.</li>
			</ol>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor221"/>Using Python to detect catastrophic forgetting</h2>
			<p>To work through <a id="_idIndexMarker601"/>this example, let's start by <a id="_idIndexMarker602"/>implementing an online regression model, just like you have already seen earlier on in this book: </p>
			<ol>
				<li value="1">To do this, we first need to generate some data. The code to generate the data for this example is shown here:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-1</p>
			<p class="source-code">import random</p>
			<p class="source-code">X = [</p>
			<p class="source-code">     1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, </p>
			<p class="source-code">     6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10</p>
			<p class="source-code">]</p>
			<p class="source-code">y = [</p>
			<p class="source-code">     x + random.random() for x in X[:15]] + </p>
			<p class="source-code">    [x * 2 + random.random() for x in X[15:]</p>
			<p class="source-code">]</p>
			<p>If you look at this code, you can see that there is a shift occurring in the pattern. In the first 15 observations, <strong class="source-inline">y</strong> is defined as <strong class="source-inline">x + random.randint()</strong>, meaning just <a id="_idIndexMarker603"/>the same value as <strong class="source-inline">x</strong> but <a id="_idIndexMarker604"/>with some random variation. After the 15th observation, this shift changes and becomes <strong class="source-inline">x * 2 + random.randint</strong>, meaning the double of <strong class="source-inline">x</strong> with some added random variation. This example will be perfect to see how a model needs to update with time.</p>
			<ol>
				<li value="2">Let's now make a quick plot of this data to have a better idea of what this shift actually looks like. This can be done with the code that is shown here:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-2</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.scatter(X, y)</p>
			<p>The resulting graph is shown here:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B18335_11_2.jpg" alt="Figure 11.2 – The scatter plot resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – The scatter plot resulting from the preceding code block </p>
			<p>The first linear trend clearly holds from x = 1 to x = 5, but a different, steeper function starts at x = 6 and goes on to x = 10.</p>
			<ol>
				<li value="3">We are going <a id="_idIndexMarker605"/>to use River in this example, so <a id="_idIndexMarker606"/>it will be necessary to get the data in the right format. You should by now have mastered the data formats for the River library, but you can refer to the following code if necessary:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-3</p>
			<p class="source-code">X_dict = [{'X': x} for x in X]</p>
			<p class="source-code">for X_i, y_i in zip(X_dict, y):</p>
			<p class="source-code">  print(X_i, y_i)</p>
			<p>The result of this code block should be something like the following:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B18335_11_3.jpg" alt="Figure 11.3 – The output resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – The output resulting from the preceding code block </p>
			<ol>
				<li value="4">Now, let's add a <strong class="source-inline">KNNRegressor</strong> function from the River library to this loop, and at each <a id="_idIndexMarker607"/>new data point, use <a id="_idIndexMarker608"/>the <strong class="source-inline">learn_one</strong> method to update the model. This is done using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-4</p>
			<p class="source-code">!pip install river</p>
			<p class="source-code">from river.neighbors import KNNRegressor</p>
			<p class="source-code">my_knn = KNNRegressor(window_size=3)</p>
			<p class="source-code">X_dict = [{'X': x} for x in X]</p>
			<p class="source-code">for X_i, y_i in zip(X_dict, y):</p>
			<p class="source-code">  my_knn.learn_one(X_i, y_i)</p>
			<ol>
				<li value="5">We can <a id="_idIndexMarker609"/>compute the final training error <a id="_idIndexMarker610"/>of this model to have a general idea of the amount of errors that we should expect. The following code does exactly that:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-4</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for X_i in X_dict:</p>
			<p class="source-code">  preds.append(my_knn.predict_one(X_i))</p>
			<p class="source-code">sum_absolute_error = 0</p>
			<p class="source-code">for pred, real in zip(preds, y):</p>
			<p class="source-code">  sum_absolute_error += abs(pred - real)</p>
			<p class="source-code">mean_absolute_error = sum_absolute_error / len(preds)</p>
			<p class="source-code">print(mean_absolute_error)</p>
			<p>In the current example, this computes a mean absolute error of 10.</p>
			<ol>
				<li value="6">Let's now have a more detailed look into the step-by-step learning quality of the model. We can <a id="_idIndexMarker611"/>do this by using continuous <a id="_idIndexMarker612"/>evaluation. This means that every time we learn, we will evaluate the model:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-5</p>
			<p class="source-code">my_knn = KNNRegressor(window_size=3)</p>
			<p class="source-code">X_dict = [{'X': x} for x in X]</p>
			<p class="source-code">step_by_step_error = []</p>
			<p class="source-code">for i in range(len(X_dict)):</p>
			<p class="source-code">  my_knn.learn_one(X_dict[i], y[i])</p>
			<p class="source-code">  abs_error = abs(my_knn.predict_one(X_dict[i]) - y[i])</p>
			<p class="source-code">  step_by_step_error.append(abs_error)</p>
			<ol>
				<li value="7">The following code will plot those errors over time to see how the model is learning:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-6</p>
			<p class="source-code">plt.plot(step_by_step_error)</p>
			<p>The following plot results from this code:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B18335_11_4.jpg" alt="Figure 11.4 – The plot resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The plot resulting from the preceding code block </p>
			<p>Interestingly, the model seems to obtain a perfect score every time that we see a new value for x, then <a id="_idIndexMarker613"/>the second time <a id="_idIndexMarker614"/>that the same x value occurs, we have a perfect score again, but the third time, we have a larger error!</p>
			<ol>
				<li value="8">It would be great to compare this with the final error, which was not computed step by step but just at once, using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-7</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for X_i in X_dict:</p>
			<p class="source-code">  preds.append(my_knn.predict_one(X_i))</p>
			<p class="source-code">all_errors = []</p>
			<p class="source-code">for pred, real in zip(preds, y):</p>
			<p class="source-code">  all_errors.append(abs(pred - real))</p>
			<p class="source-code">plt.plot(step_by_step_error)</p>
			<p class="source-code">plt.plot(all_errors)</p>
			<p class="source-code">plt.show()</p>
			<p>The output from <a id="_idIndexMarker615"/>this code block is shown <a id="_idIndexMarker616"/>hereafter:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B18335_11_5.jpg" alt="Figure 11.5 – The plot resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – The plot resulting from the preceding code block </p>
			<p>You can clearly observe that when evaluating the model step by step, the error on each data point does not seem too big. However, when evaluating all at the end, you see that the model has actually forgotten the first data points! This is a good example of how catastrophic forgetting can be observed in practice.</p>
			<ol>
				<li value="9">As a final step, let's add a small evaluation to the model loop to help you in realizing that the model has forgotten your first scores:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-8</p>
			<p class="source-code">my_knn = KNNRegressor(window_size=3)</p>
			<p class="source-code">X_dict = [{'X': x} for x in X]</p>
			<p class="source-code">step_by_step_error = []</p>
			<p class="source-code">for i in range(len(X_dict)):</p>
			<p class="source-code">  my_knn.learn_one(X_dict[i], y[i])</p>
			<p class="source-code">  abs_error = abs(my_knn.predict_one(X_dict[i]) - y[i])</p>
			<p class="source-code">  step_by_step_error.append(abs_error)</p>
			<p class="source-code">  all_errors_recomputed = []</p>
			<p class="source-code">  for j in range(i):</p>
			<p class="source-code">    orig_error = step_by_step_error[j]</p>
			<p class="source-code">    after_error = abs(my_knn.predict_one(X_dict[j]) - y[j])</p>
			<p class="source-code">    if after_error &gt; orig_error:</p>
			<p class="source-code">      print(f'At learning step {i}, data point {j} was partly forgotten')</p>
			<p>In this code block, a rule was made to detect forgetting as soon as the error was larger than the <a id="_idIndexMarker617"/>original error. Of course, this <a id="_idIndexMarker618"/>is a really severe detection mechanism, and you could imagine other approaches in the place of this one. For example, this could be a percentage change or an absolute number that must not be surpassed. This all depends on your business case.</p>
			<p>Now that you have <a id="_idIndexMarker619"/>seen an approach for detecting <a id="_idIndexMarker620"/>catastrophic forgetting using alarm mechanisms based on model performance, let's go on to the next part of this chapter, in which you'll see how to use model explicability to detect catastrophic forgetting.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor222"/>Model explicability versus catastrophic forgetting</h1>
			<p>Looking at model performance is generally a good way to keep track of your model and it will definitely <a id="_idIndexMarker621"/>help you to detect that <a id="_idIndexMarker622"/>something, somewhere in the model, has gone wrong. Generally, this will be enough of an alerting mechanism and will help you to manage your models in production.</p>
			<p>If you want to understand exactly what has gone wrong, however, you'll need to dig deeper into your model. Looking at performance only is more of a black-box approach, whereas we can also extract things such as trees, coefficients, variable importance, and the like to see what has actually changed inside the model.</p>
			<p>There is no one-size-fits-all method for deep diving into models. All model categories have their own specific method for fitting the data, and an inspection of their fit would therefore be strongly dependent on the model itself. In the remainder of this section, however, we will look at two very common structures in machine learning: linear models with coefficients and trees.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor223"/>Explaining models using linear coefficients</h2>
			<p>In this first <a id="_idIndexMarker623"/>example, we'll build a linear regression <a id="_idIndexMarker624"/>on some sample data and extract coefficients of the model to give an interpretation of them:</p>
			<ol>
				<li value="1">You can create the data for this example using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-9</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">ice_cream_sales = [10, 9, 8, 7, 6, 5, 4, 3, 2 , 1]</p>
			<p class="source-code">degrees_celsius = [30, 25, 20, 19, 18, 17, 15, 13, 10, 5]</p>
			<p class="source-code">price  = [2,2, 3, 3, 4, 4, 5, 5, 6, 6]</p>
			<p class="source-code">data = pd.DataFrame({</p>
			<p class="source-code">    'ice_cream_sales': ice_cream_sales,</p>
			<p class="source-code">    'degrees_celsius': degrees_celsius,</p>
			<p class="source-code">    'price': price</p>
			<p class="source-code">})</p>
			<p class="source-code">data</p>
			<p>The data is <a id="_idIndexMarker625"/>shown here in a dataframe <a id="_idIndexMarker626"/>format:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B18335_11_6.jpg" alt="Figure 11.6 – The plot resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – The plot resulting from the preceding code block </p>
			<ol>
				<li value="2">Let's create two scatter plots to have a better visual idea of how ice cream sales are related <a id="_idIndexMarker627"/>to temperature and <a id="_idIndexMarker628"/>price (in this fictitious example). The following code shows how to create the first scatter plot:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-10</p>
			<p class="source-code">plt.scatter(data['degrees_celsius'], data['ice_cream_sales'])</p>
			<p>This results in the following graph:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B18335_11_7.jpg" alt="Figure 11.7 – The plot resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – The plot resulting from the preceding code block </p>
			<ol>
				<li value="3">The second scatter plot can be created as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-11</p>
			<p class="source-code">plt.scatter(data['price'], data['ice_cream_sales'])</p>
			<p>This results in the following graph:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B18335_11_8.jpg" alt="Figure 11.8 – The plot resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – The plot resulting from the preceding code block </p>
			<p>You can clearly see that sales are higher when the temperature is higher, and sales are <a id="_idIndexMarker629"/>lower when the temperature <a id="_idIndexMarker630"/>is lower. Also, higher prices are correlated with lower sales, and lower prices are correlated with higher sales. </p>
			<ol>
				<li value="4">These are two logical and explainable factors in ice cream sales, but this is not yet a model. Let's use a <strong class="source-inline">LinearRegression</strong> function to model this straightforward linear relationship:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-12</p>
			<p class="source-code">from sklearn.linear_model import LinearRegression</p>
			<p class="source-code">my_lr = LinearRegression()</p>
			<p class="source-code">my_lr.fit(X = data[['degrees_celsius', 'price']], y = data['ice_cream_sales'])</p>
			<ol>
				<li value="5">We can evaluate the (in-sample) fit of this model as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-13</p>
			<p class="source-code">from sklearn.metrics import r2_score</p>
			<p class="source-code">r2_score(data['ice_cream_sales'], my_lr.predict(data[['degrees_celsius', 'price']]))</p>
			<p>This model yields a training R2 score of 0.98, meaning that the model fits really well to the training data.</p>
			<ol>
				<li value="6">We are now at the step where we need to go deeper into the model than just looking at <a id="_idIndexMarker631"/>performance. With the linear <a id="_idIndexMarker632"/>regression, we need to look at coefficients to be able to interpret what they have fitted. The coefficients are extracted in the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-14</p>
			<p class="source-code">pd.DataFrame({'variable': ['degrees_celsius', 'price'], 'coefficient': my_lr.coef_})</p>
			<p>This gives the following output:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B18335_11_9.jpg" alt="Figure 11.9 – The coefficients resulting from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – The coefficients resulting from the preceding code block </p>
			<p>You can interpret this as follows:</p>
			<ul>
				<li>Every additional degree Celsius will increase ice cream sales by 0.15, given a constant price.</li>
				<li>Every euro added to the price will decrease ice cream sales by 1.3, given a constant temperature.</li>
			</ul>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor224"/>Explaining models using dendrograms</h2>
			<p>While looking at coefficients is great for linear models, some models do not have any coefficients. Examples of <a id="_idIndexMarker633"/>this are basically any models <a id="_idIndexMarker634"/>that use trees. Trees have nodes and these nodes are split based on yes/no questions. Although you cannot extract coefficients from trees, the advantage is that you can simply print out the entire tree as a graph! We'll look at that in the next example:</p>
			<ol>
				<li value="1">To get started, we need to fit a <strong class="source-inline">DecisionTreeRegressor</strong> function on the same data as the one we used before, using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-15</p>
			<p class="source-code">from sklearn.tree import DecisionTreeRegressor</p>
			<p class="source-code">my_dt = DecisionTreeRegressor()</p>
			<p class="source-code">my_dt.fit(X = data[['degrees_celsius', 'price']], y = data['ice_cream_sales'])</p>
			<ol>
				<li value="2">To get a general idea whether the model fits, let's compute an R2 score on the training set, just like we did before:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-16</p>
			<p class="source-code">r2_score(data['ice_cream_sales'], my_dt.predict(data[['degrees_celsius', 'price']]))</p>
			<p>The result is 1.0, which means that the decision tree has obtained a perfect fit on the training data. Nothing guarantees that this will generalize out-of-sample, but that is not necessarily a problem for explaining the model.</p>
			<ol>
				<li value="3">To extract the tree as an image, you can simply use the code here:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-17</p>
			<p class="source-code">import sklearn</p>
			<p class="source-code">plt.figure(figsize=(15,15))</p>
			<p class="source-code">sklearn.tree.plot_tree(my_dt)</p>
			<p class="source-code">plt.show()</p>
			<p>This will <a id="_idIndexMarker635"/>print out the entire tree and <a id="_idIndexMarker636"/>give you perfect insight into how the predictions have been made:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B18335_11_10.jpg" alt="Figure 11.10 – The resulting dendrogram from the preceding code block &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10 – The resulting dendrogram from the preceding code block </p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor225"/>Explaining models using variable importance</h2>
			<p>As a third and <a id="_idIndexMarker637"/>final method for explaining models, you <a id="_idIndexMarker638"/>can look at variable importance. Again, this is something that will not work for all machine learning models. Yet, for rather complex models it is often too difficult to look at all dendrograms and variable importance estimates are a great replacement for this.</p>
			<p>Let's extract <a id="_idIndexMarker639"/>the variable importance from the <a id="_idIndexMarker640"/>decision tree model that was built previously. This can be done using the following code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 11-18</p>
			<pre class="source-code">pd.DataFrame({<strong class="bold">'variable'</strong>: [<strong class="bold">'degrees_celsius'</strong>, <strong class="bold">'price'</strong>], <strong class="bold">'importance'</strong>: my_dt.feature_importances_})</pre>
			<p>The resulting dataframe looks as follows:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B18335_11_11.jpg" alt="Figure 11.11 – The importance value&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11 – The importance value</p>
			<p>This tells us that the decision tree has used degrees Celsius more than it has used the price as a predictor variable.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor226"/>Summary</h1>
			<p>In this chapter, you have seen how catastrophic forgetting can cause bad performance in your model, especially when data arrives in a sequential manner. Especially when one trend is learned first and a second trend follows, the risk of forgetting the first trend is real and needs to be controlled.</p>
			<p>Although there is no one-stop solution for these issues, there are many things that can be done to avoid bad models from going into production systems. You have seen how to implement continuous evaluation metrics and you have seen how you would be able to detect that some trends have been forgotten.</p>
			<p>Performance-based metrics are great for detecting problems but are not able to tell you what exactly has gone wrong inside the model. You have seen three methods of model explanation that can help you deep-dive further into most models. By extracting from the model which trends or relationships the model has learned, you can identify whether this corresponds to an already known business logic or common sense.</p>
			<p>In the next and final chapter of this book, we will conclude the different topics that have been presented and consider a number of best practices to keep in mind while working on online models and streaming data.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor227"/>Further reading</h1>
			<ul>
				<li>KNNRegressor: <a href="https://riverml.xyz/latest/api/neighbors/KNNRegressor/">https://riverml.xyz/latest/api/neighbors/KNNRegressor/</a></li>
				<li>LinearRegression: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a></li>
				<li>DecisionTree: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</a></li>
				<li>Tree_plot: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html">https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html</a> </li>
			</ul>
		</div>
	</body></html>