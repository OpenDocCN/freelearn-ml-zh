- en: Activity Recognition with Mobile Phone Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the previous chapter focused on pattern recognition in images, this chapter
    is all about recognizing patterns in sensor data, which, in contrast to images,
    has temporal dependencies. We will discuss how to recognize granular daily activities
    such as walking, sitting, and running using mobile phone inertial sensors. The
    chapter also provides references to related research and emphasizes best practices
    in the activity recognition community.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter will include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing activity recognition, covering mobile phone sensors and the activity
    recognition pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting sensor data from mobile devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing activity classification and model evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an activity recognition model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing activity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Activity recognition is an underpinning step in behavior analysis, addressing
    healthy lifestyles, fitness tracking, remote assistance, security applications,
    elderly care, and so on. Activity recognition transforms low-level sensor data
    from sensors, such as an accelerometer, gyroscope, pressure sensor, and GPS location,
    to a higher-level description of behavior primitives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, these are basic activities, for example, walking, sitting, lying,
    jumping, and so on, as shown in the following diagram, or they could be more complex
    behaviors, such as going to work, preparing breakfast, and shopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4d199bf-5cb5-4eb0-a589-d09e8154e63f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will discuss how to add the activity recognition functionality
    into a mobile application. We will first look at what an activity recognition
    problem looks like, what kind of data we need to collect, what the main challenges
    are, and how to address them.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will follow an example to see how to actually implement activity recognition
    in an Android application, including data collection, data transformation, and
    building a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start!
  prefs: []
  type: TYPE_NORMAL
- en: Mobile phone sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first review what kind of mobile phone sensors there are and what they
    report. Most smart devices are now equipped with several built-in sensors that
    measure the motion, position, orientation, and conditions of the ambient environment.
    As sensors provide measurements with high precision, frequency, and accuracy,
    it is possible to reconstruct complex user motions, gestures, and movements. Sensors
    are often incorporated in various applications; for example, gyroscope readings
    are used to steer an object in a game, GPS data is used to locate the user, and
    accelerometer data is used to infer the activity that the user is performing,
    for example, cycling, running, or walking.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a couple of examples of what kinds of interactions
    the sensors are able to detect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6527115b-62c1-444e-a989-6ee81830bc25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Mobile phone sensors can be classified into the following three broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Motion sensors:** This sensor measures acceleration and rotational forces
    along the three perpendicular axes. Examples of sensors in this category include
    accelerometers, gravity sensors, and gyroscopes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environmental sensors:** This sensor measures a variety of environmental
    parameters, such as illumination, air temperature, pressure, and humidity. This
    category includes barometers, photometers, and thermometers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Position sensors:** This sensor measure the physical position of a device.
    This category includes orientation sensors and magnetometers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More detailed descriptions for different mobile platforms are available at
    the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Android sensors framework**: [http://developer.android.com/guide/topics/sensors/sensors_overview.html](http://developer.android.com/guide/topics/sensors/sensors_overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**iOS Core Motion framework**: [https://developer.apple.com/library/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/](https://developer.apple.com/library/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windows phone**: [https://msdn.microsoft.com/en-us/library/windows/apps/hh202968(v=vs.105).aspx](https://msdn.microsoft.com/en-us/library/windows/apps/hh202968(v=vs.105).aspx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will work only with Android's sensors framework.
  prefs: []
  type: TYPE_NORMAL
- en: Activity recognition pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifying multidimensional time series sensor data is inherently more complex
    than classifying traditional nominal data, as we saw in the previous chapters.
    First, each observation is temporally connected to the previous and following
    observations, making it very difficult to apply a straightforward classification
    of a single set of observations only. Second, the data obtained by sensors at
    different time points is stochastic, that is, unpredictable due to the influence
    of sensor noise, environmental disturbances, and many other factors. Moreover,
    an activity can consist of various sub-activities executed in a different manner
    and each person performs the activity a bit differently, which results in high
    intraclass differences. Finally, all these reasons make an activity recognition
    model imprecise, resulting in new data often being misclassified. One of the highly
    desirable properties of an activity recognition classifier is to ensure continuity
    and consistency in the recognized activity sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with these challenges, activity recognition is applied to a pipeline,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b904185-1b46-41ac-b1fa-73ebb843c034.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first step, we attenuate as much noise as we can, for example, by reducing
    the sensor sampling rate, removing outliers, applying high-or low-pass filters,
    and so on. In the next phase, we construct a feature vector. For instance, we
    convert sensor data from time domain to frequency domain by applying a **discrete
    Fourier transform** (**DFT**). DFT is a method that takes a list of samples as
    an input and returns a list of sinusoid coefficients ordered by their frequencies.
    They represent a combination of frequencies that are present in the original list
    of samples.
  prefs: []
  type: TYPE_NORMAL
- en: A gentle introduction to the Fourier transform was written by Pete Bevelacqua
    at [http://www.thefouriertransform.com/](http://www.thefouriertransform.com/).
    If you want to get a more technical and theoretical background on the Fourier
    transform, take a look at the eighth and ninth lectures in the class by Robert
    Gallager and Lizhong Zheng at this MIT open course: [http://theopenacademy.com/content/principles-digital-communication](http://theopenacademy.com/content/principles-digital-communication).
  prefs: []
  type: TYPE_NORMAL
- en: Next, based on the feature vector and set of training data, we can build an
    activity recognition model that assigns an atomic action to each observation.
    Therefore, for each new sensor reading, the model will output the most probable
    activity label. However, models make mistakes. Hence, the last phase smooths the
    transitions between activities by removing transitions that cannot occur in reality;
    for example, it is not physically feasible that the transition between the activities
    lying-standing-lying occur in less than half a second, hence such a transition
    between activities is smoothed as lying-lying-lying.
  prefs: []
  type: TYPE_NORMAL
- en: The activity recognition model is constructed with a supervised learning approach,
    which consists of training and classification steps. In the training step, a set
    of labeled data is provided to train the model. The second step is used to assign
    a label to the new unseen data by the trained model. The data in both phases must
    be preprocessed with the same set of tools, such as filtering and feature vector
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: The post processing phase, that is, spurious activity removal, can also be a
    model itself and hence also requires a learning step. In this case, the preprocessing
    step also includes activity recognition, which makes such arrangement of classifiers
    into a meta-learning problem. To avoid overfitting, it is important that the dataset
    used for training the post processing phase is not the same as the one used for
    training the activity recognition model.
  prefs: []
  type: TYPE_NORMAL
- en: The plan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The plan consists of a training phase and a deployment phase. The training
    phase boils down to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Android Studio and import `MyRunsDataCollector.zip`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the application on your Android phone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect your data, for example, standing, walking, and running, and transform
    the data to a feature vector consisting of FFTs. Don't panic; low-level signal
    processing functions such as FFTs will not be written from scratch as we will
    use existing code to do that. The data will be saved on your phone in a file called
    `features.arff`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and evaluate an activity recognition classifier using exported data and
    implement a filter for spurious activity transition removal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug the classifier back into the mobile application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don't have an Android phone, or if you want to skip all the steps related
    to the mobile application, just grab the collected dataset located in `data/features.arff`
    and jump directly to the *Building a classifier* section.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data from a mobile phone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the first three steps from the plan. If you want to directly
    work with the data, you can just skip this section and continue to the *Building
    a classifier* section. The application implements the essentials to collect sensor
    data for different activity classes, for example, standing, walking, running,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by preparing the Android development environment. If you have already
    installed it, jump to the *Loading the data collector* section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Android Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Android Studio is a development environment for the Android platform. We will
    quickly review the installation steps and basic configurations required to start
    the app on a mobile phone. For a more detailed introduction to Android development,
    I would recommend an introductory book, *Android 5 Programming by Example* by
    Kyle Mew, Packt Publishing.
  prefs: []
  type: TYPE_NORMAL
- en: Grab the latest Android Studio for developers at [https://developer.android.com/studio/](https://developer.android.com/studio/) and
    follow the installation instructions at [http://developer.android.com/sdk/installing/index.html?pkg=studio](http://developer.android.com/sdk/installing/index.html?pkg=studio).
    The installation will take around 10 minutes, occupying approximately 0.5 GB of
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the instructions and select your preferred options for installation,
    and finally click on Finish to start the installation, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b8f52fb-fe94-46e3-9211-02479536c078.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading the data collector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, grab the source code of `MyRunsDataCollector` from GitHub. Once Android
    Studio is installed, choose the Open an existing Android Studio project option, as
    shown in the following screenshot, and select the `MyRunsDataCollector` folder.
    This will import the project to Android Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01b5d01c-a503-4b36-a0d3-fba9979da67d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the project import is completed, you should be able to see the project
    file structure, as shown in the following screenshot. The collector consists of
    `CollectorActivity.java`, `Globals.java`, and `SensorsService.java`. The project
    also shows `FFT.java` implementing low-level signal processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4f1dc26-7697-465a-b268-a5dfcd0a3284.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The main `myrunscollector` package contains the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Globals.java`: This defines global constants, such as activity labels and
    IDs, and data filenames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CollectorActivity.java`: This implements user interface actions, that is,
    what happens when a specific button is pressed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SensorsService.java`: This implements a service that collects data, calculates
    the feature vector, as we will discuss in the following sections, and stores the
    data into a file on the phone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next question that we will address is how to design features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding an appropriate representation of a person's activities is probably the
    most challenging part of activity recognition. The behavior needs to be represented
    with simple and general features so that the model using these features will also
    be general and work well on behaviors different from those in the learning set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, it is not difficult to design features specific to the captured observations
    in a training set; such features would work well on them. However, as the training
    set captures only a part of the whole range of human behavior, overly specific
    features would likely fail on general behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33155cd1-f586-4674-a049-7ceb1c018cf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's see how this is implemented in `MyRunsDataCollector`. When the application
    is started, a method called `onSensorChanged()` gets a triple of accelerometer
    sensor readings (**x**, **y**, and **z**) with a specific timestamp and calculates
    the magnitude from the sensor readings. The methods buffers up to 64 consecutive
    magnitudes marked before computing the FFT coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the actual data collection.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now use the collector to collect training data for activity recognition.
    The collector supports three activities by default, standing, walking, and running,
    as shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: You can select an activity, that is, target class value, and start recording
    the data by clicking the START COLLECTING button. Make sure that each activity
    is recorded for at least three minutes; for example, if the Walking activity is
    selected, press START COLLECTING and walk around for at least three minutes. At
    the end of the activity, press Stop collecting. Repeat this for each of the activities.
  prefs: []
  type: TYPE_NORMAL
- en: You could also collect different scenarios involving these activities, for example,
    walking in the kitchen, walking outside, walking in a line, and so on. By doing
    so, you will have more data for each activity class and a better classifier. Makes
    sense, right? The more data, the less confused the classifier will be. If you
    only have a little data, overfitting will occur and the classifier will confuse
    classes—standing with walking, walking with running, and so on. However, the more
    data, the less they get confused. You might collect less than three minutes per
    class when you are debugging, but for your final polished product, the more data,
    the better it is. Multiple recording instances will simply be accumulated in the
    same file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note, the Delete Data button removes the data that is stored in a file on the
    phone. If you want to start over again, hit Delete Data before starting; otherwise,
    the new collected data will be appended at the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2686cb4-6318-41ff-a9fb-7477b78b3d24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The collector implements the diagram discussed in the previous sections: it
    collects accelerometer samples, computes the magnitudes, uses the `FFT.java` class
    to compute the coefficients, and produces the feature vectors. The data is then
    stored in a Weka-formatted `features.arff` file. The number of feature vectors
    will vary based on the amount of data you collect. The longer you collect the
    data, the more feature vectors are accumulated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you stop collecting the training data using the collector tool, we need
    to grab the data to carry on the workflow. We can use the file explorer in Android
    Device Monitor to upload the `features.arff` file from the phone and to store
    it on the computer. You can access your Android Device Monitor by clicking on
    the Android robot icon, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/740619dc-a878-409f-80b4-853062f94596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By selecting your device on the left, your phone storage content will be shown
    on the right-hand side. Navigate through `mnt/shell/emulated/Android/data/edu.dartmouth.cs.myrunscollector/files/features.arff`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff6dd849-fe2b-42f2-bb3b-a50b03814875.png)'
  prefs: []
  type: TYPE_IMG
- en: To upload this file to your computer, you need to select the file (it is highlighted)
    and click on Upload.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to build a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Building a classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once sensor samples are represented as feature vectors and have the class assigned,
    it is possible to apply standard techniques for supervised classification, including
    feature selection, feature discretization, model learning, k-fold cross-validation,
    and so on. The chapter will not delve into the details of the machine learning
    algorithms. Any algorithm that supports numerical features can be applied, including
    SVMs, random forest, AdaBoost, decision trees, neural networks, multilayer perceptrons,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, let''s start with a basic one: decision trees. Here, we will load
    the dataset, build the set class attribute, build a decision tree model, and output
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm first outputs the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree is quite simplistic and seemingly accurate, as majority class distributions
    in the terminal nodes are quite high. Let''s run a basic classifier evaluation
    to validate the results, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification accuracy scores very high, `92.62%`, which is an amazing
    result. One important reason why the result is so good lies in our evaluation
    design. What I mean here is the following: sequential instances are very similar
    to each other, so if we split them randomly during a 10-fold cross-validation,
    there is a high chance that we use almost identical instances for both training
    and testing; hence, straightforward k-fold cross-validation produces an optimistic
    estimate of model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: A better approach is to use folds that correspond to different sets of measurements
    or even different people. For example, we can use the application to collect learning
    data from five people. Then, it makes sense to run k-person cross-validation,
    where the model is trained on four people and tested on the fifth person. The
    procedure is repeated for each person and the results are averaged. This will
    give us a much more realistic estimate of the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Leaving evaluation comments aside, let's look at how to deal with classifier
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing spurious transitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the end of the activity recognition pipeline, we want to make sure that the
    classifications are not too volatile, that is, we don't want activities to change
    every millisecond. A basic approach is to design a filter that ignores quick changes
    in the activity sequence.
  prefs: []
  type: TYPE_NORMAL
- en: We build a filter that remembers the last window activities and returns the
    most frequent one. If there are multiple activities with the same score, it returns
    the most recent one.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new `SpuriousActivityRemoval` class, which will hold a list
    of activities and the `window` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the `Object filter(Object)` method, which will take an activity
    and return a filtered activity. The method first checks whether we have enough
    observations. If not, it simply stores the observation and returns the same value,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we already collected `window` observations, we simply return the most frequent
    observation, remove the oldest observation, and insert the new observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'What is missing here is a function that returns the most frequent element from
    a list of objects. We implement this with a hash map, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we iterate over all the elements in the list, insert each unique element
    into a hash map, or update its counter if it is already in the hash map. At the
    end of the loop, we store the most frequent element that we found so far, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The example outputs the following activities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The result is a continuous sequence of activities, that is, we do not have quick
    changes. This adds some delay, but unless this is absolutely critical for the
    application, it is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity recognition may be enhanced by appending *n* previous activities,
    as recognized by the classifier, to the feature vector. The danger of appending
    previous activities is that the machine learning algorithm may learn that the
    current activity is always the same as the previous one, as this will often be
    the case. The problem may be solved by having two classifiers, A and B: classifier
    B''s attribute vector contains *n* previous activities as recognized by classifier
    A. Classifier A''s attribute vector does not contain any previous activities.
    This way, even if B gives a lot of weight to the previous activities, the previous
    activities as recognized by A will change as A is not burdened with B''s inertia.'
  prefs: []
  type: TYPE_NORMAL
- en: All that remains to do is to embed the classifier and filter it into our mobile
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Plugging the classifier into a mobile app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to incorporate a classifier into a mobile application. The
    first one involves exporting a model in the Weka format, using the Weka library
    as a dependency in our mobile application, loading the model, and so on. The procedure
    is identical to the example we saw in [Chapter 3](e0c71e12-6bd7-4f63-b71d-78bb5a87b801.xhtml),
    *Basic Algorithms–Classification, Regression, and Clustering*. The second approach
    is more lightweight: we export the model as source code, for example, we create
    a class implementing the decision tree classifier. Then, we can simply copy and
    paste the source code into our mobile app, without even importing any Weka dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, some Weka models can be easily exported to source code by the
    `toSource(String)` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs an `ActivityRecognitionEngine` class that corresponds to our model.
    Now, let''s take a closer look at the output code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The outputted `ActivityRecognitionEngine` class implements the decision tree
    that we discussed earlier. The machine-generated function names, such as `N17a7cec20(Object
    [])`, correspond to decision tree nodes. The classifier can be called by the `classify(Object[])`
    method, where we should pass a feature vector obtained by the same procedure as
    we discussed in the previous sections. As usual, it returns a `double`, indicating
    a class label index.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to implement an activity recognition model
    for mobile applications. We looked into the completed process, including data
    collection, feature extraction, model building, evaluation, and model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will move on to another Java library targeted at text
    analysis: Mallet.'
  prefs: []
  type: TYPE_NORMAL
