["```py\npip install PyPDF4\n```", "```py\nimport PyPDF4file_name = \"PIIS0021925819524516.pdf\"\n```", "```py\nfile = open(file_name,'rb')pdf_reader = PyPDF4.PdfFileReader(file)\n```", "```py\nmetadata = pdf_reader.getDocumentInfo()print (f\"Title: {metadata.title}\")\nprint (f\"Author: {metadata.author}\")\nprint (f\"Subject: {metadata.subject}\")\n```", "```py\nTitle: PROTEIN MEASUREMENT WITH THE FOLIN PHENOL REAGENTAuthor: Oliver H. Lowry\nSubject: Journal of Biological Chemistry, 193 (1951) 265-275\\. doi:10.1016/S0021-9258(19)52451-6\n```", "```py\npages = pdf_reader.numPagesprint(f\"Pages: {pages}\")\n```", "```py\nPages: 11\n```", "```py\npage = 0while page < pages:\nÂ Â Â Â pdf_page = pdf_reader.getPage(page)\nÂ Â Â Â print(pdf_page.extractText())\nÂ Â Â Â page+=1\nÂ Â Â Â # write to a database here\n```", "```py\nimport PyPDF4from pathlib import Path\nfolder = \"./\"\n```", "```py\ndef print_metadata(pdf_reader):Â Â Â Â # print the meta data\nÂ Â Â Â metadata = pdf_reader.getDocumentInfo()\nÂ Â Â Â print (f\"Title: {metadata.title}\")\nÂ Â Â Â print (f\"Author: {metadata.author}\")\nÂ Â Â Â print (f\"Subject: {metadata.subject}\")\ndef save_content(pdf_reader):\nÂ Â Â Â # print number of pages in pdf file\nÂ Â Â Â pages = pdf_reader.numPages\nÂ Â Â Â print(f\"Pages: {pages}\")\nÂ Â Â Â # get content for each page\nÂ Â Â Â page = 0\nÂ Â Â Â while page < pages:\nÂ Â Â Â Â Â Â Â pdf_page = pdf_reader.getPage(page)\nÂ Â Â Â Â Â Â Â print(pdf_page.extractText())\nÂ Â Â Â Â Â Â Â page+=1\nÂ Â Â Â Â Â Â Â # write each page to a database here\n```", "```py\npathlist = Path(folder).rglob('*.pdf')for file_name in pathlist:\nÂ Â Â Â file_name = str(file_name)\nÂ Â Â Â pdf_file = open(file_name,'rb')\nÂ Â Â Â pdf_reader = PyPDF4.PdfFileReader(pdf_file)\nÂ Â Â Â print (f\"File name: {file_name}\")\nÂ Â Â Â print_metadata(pdf_reader)\nÂ Â Â Â save_content(pdf_reader)\nÂ Â Â Â pdf_file.close()\n```", "```py\npip install beautifulsoup4\n```", "```py\nimport bs4 as bsimport re\nimport time\nfrom urllib.request import urlopen\n```", "```py\nROOT_URL = \"https://en.wikipedia.org/wiki/Emotion\"\n```", "```py\np = re.compile(r'((<p[^>]*>(.(?!</p>))*.</p>\\s*){3,})',Â Â Â Â re.DOTALL)\n```", "```py\ndef get_url_content(url):Â Â Â Â with urlopen(url) as url:\nÂ Â Â Â Â Â Â Â raw_html = url.read().decode('utf-8')\nÂ Â Â Â Â Â Â Â for match in p.finditer(raw_html):\nÂ Â Â Â Â Â Â Â Â Â Â Â paragraph = match.group(1)\nÂ Â Â Â Â Â Â Â Â Â Â Â # clean up, extract HTML and save to database\n```", "```py\ndef get_url_content(url):Â Â Â Â with urlopen(url) as url:\nÂ Â Â Â Â Â Â Â raw_html = url.read().decode('utf-8')\nÂ Â Â Â Â Â Â Â # clean up, extract HTML and save to database\nÂ Â Â Â Â Â Â Â for match in p.finditer(raw_html):\nÂ Â Â Â Â Â Â Â Â Â Â Â paragraph = match.group(1)\nÂ Â Â Â Â Â Â Â Â Â Â Â soup = bs.BeautifulSoup(paragraph,'lxml')\nÂ Â Â Â Â Â Â Â Â Â Â Â for link in soup.findAll('a'):\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â new_url = (link.get('href'))\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # add a delay between each scrape\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â time.sleep(1)\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â get_url_content(new_url)\n```", "```py\nraw_html = get_url_content(ROOT_URL)\n```", "```py\npip install feedparser\n```", "```py\nimport feedparser\n```", "```py\nRSS_URL = \"http://feeds.bbci.co.uk/news/rss.xml\"\n```", "```py\ndef process_feed(rss_url):Â Â Â Â feed = feedparser.parse(rss_url)\nÂ Â Â Â # attributes of the feed\nÂ Â Â Â print (feed['feed']['title'])\nÂ Â Â Â print (feed['feed']['link'])\nÂ Â Â Â print (feed.feed.subtitle)\nÂ Â Â Â for post in feed.entries:\nÂ Â Â Â Â Â Â Â print (post.link)\nÂ Â Â Â Â Â Â Â print (post.title)\nÂ Â Â Â Â Â Â Â # save to database\nÂ Â Â Â Â Â Â Â print (post.summary)\n```", "```py\nprocess_feed(RSS_URL)\n```", "```py\nBBC News - Homehttps://www.bbc.co.uk/news/\nBBC News - Home\nhttps://www.bbc.co.uk/news/world-asia-63155169?at_medium=RSS&at_campaign=KARANGA\nThailand: Many children among dead in nursery attack\nAn ex-police officer killed at least 37 people at a childcare centre before killing himself and his family.\nhttps://www.bbc.co.uk/news/world-asia-63158837?at_medium=RSS&at_campaign=KARANGA\nThailand nursery attack: Witnesses describe shocking attack\nThere was terror and confusion as sleeping children were attacked by the former policeman.\nhttps://www.bbc.co.uk/news/science-environment-63163824?at_medium=RSS&at_campaign=KARANGA\nUK defies climate warnings with new oil and gas licences\nMore than 100 licences are expected to be granted for new fossil fuel exploration in the North Sea.\n```", "```py\npip install tweepy\n```", "```py\nimport tweepyimport time\nBEARER_TOKEN = \"YOUR_KEY_HERE\"\nACCESS_TOKEN = \"YOUR_KEY_HERE\"\nACCESS_TOKEN_SECRET = \"YOUR_KEY_HERE\"\nCONSUMER_KEY = \"YOUR_KEY_HERE\"\nCONSUMER_SECRET =\"YOUR_KEY_HERE\"\n```", "```py\nclient = tweepy.Client(BEARER_TOKEN, CONSUMER_KEY,Â Â Â Â CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\nauth = tweepy.OAuth1UserHandler(CONSUMER_KEY,\nÂ Â Â Â CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\napi = tweepy.API(auth)\nclass TwitterStream(tweepy.StreamingClient):\nÂ Â Â Â def on_tweet(self, tweet):\nÂ Â Â Â Â Â Â Â print(tweet.text)\nÂ Â Â Â Â Â Â Â time.sleep(0.2)\nÂ Â Â Â Â Â Â Â # save to database\nstream = TwitterStream(bearer_token=BEARER_TOKEN)\n```", "```py\nstream.add_rules(tweepy.StreamRule(\"#lfc\"))print(stream.get_rules())\nstream.filter()\nResponse(data=[StreamRule(value='#lfc', tag=None,\nÂ Â Â Â Â id='1579970831714844672')], includes={}, errors=[],\nÂ Â Â Â meta={'sent': '2022-10-12T23:02:31.158Z',\nÂ Â Â Â 'result_count': 1})\nRT @TTTLLLKK: Rangers Fans after losing 1 - 7 (SEVEN) to Liverpool. Sad Song ðŸŽ¶ #LFC #RFC https://t.co/CvTVEGRBU1\nToo bad Liverpool aren't in the Scottish league. Strong enough to definitely finish in the Top 4\\. #afc #lfc\nRT @LFCphoto: VAR GOAL CHECK\n#LFC #RANLIV #UCL #Elliott @MoSalah https://t.co/7A7MUzW0Pa\nNah we getting cooked on Sunday https://t.co/bUhQcFICUg\nRT @LFCphoto: #LFC #RANLIV #UCL https://t.co/6DrbZ2b9NT\n```", "```py\nimport pandas as pddata_file_path = \"EI-reg-En-anger-train.txt\"\ndf = pd.read_csv(data_file_path, sep='\\t')\n# drop rows where the emotion is not strong\ndf[df['Intensity Score'] <= 0.2]\n```", "```py\nAbusedAdmired\nAfraid\n.\n.\n.\nLonely\nLoved\nMad\n.\n.\n.\n```", "```py\nstemmer = PorterStemmer()def stem(sentence):\nÂ Â Â Â res = (\" \".join([stemmer.stem(i) for i in\nÂ Â Â Â Â Â Â Â sentence.split()]))\nÂ Â Â Â return res\n# create some new columns\nemotion_words['word_stemmed'] = emotion_words['word']\ndf['Tweet_stemmed'] = df['Tweet']\n# stem the tweets and the emotions words list\ndf['Tweet_stemmed'] = df['Tweet_stemmed'].apply(stem)\nemotion_words['word_stemmed'] = emotion_words[\nÂ Â Â Â 'word_stemmed'].apply(stem)\n# remove tweets that contain an emotion word\nres = []\ndropped = []\nfor _, t_row in df.iterrows():\nÂ Â Â Â tweet = t_row[\"Tweet_stemmed\"]\nÂ Â Â Â add = True\nÂ Â Â Â for _, e_row in emotion_words.iterrows():\nÂ Â Â Â Â Â Â Â emotion_word = e_row[\"word_stemmed\"]\nÂ Â Â Â Â Â Â Â if emotion_word in tweet:\nÂ Â Â Â Â Â Â Â Â Â Â Â add = False\nÂ Â Â Â Â Â Â Â Â Â Â Â break\nÂ Â Â Â if add:\nÂ Â Â Â Â Â Â Â res.append(t_row[\"Tweet\"])\nÂ Â Â Â else:\nÂ Â Â Â Â Â Â Â dropped.append(t_row[\"Tweet\"])\n```", "```py\n@Kristiann1125 lol wow i was gonna say really?! haha have you seen chris or nah? you dont even snap me anymore dude!And Republicans, you, namely Graham, Flake, Sasse and others are not safe from my wrath, hence that Hillary Hiney-Kissing ad I saw about you\n@leepg \\n\\nLike a rabid dog I pulled out the backs of my cupboards looking for a bakewell..Found a french fancie &amp; a mini batternburg #Winner!\n```", "```py\n@xandraaa5 @amayaallyn6 shut up hashtags are cool #offendedit makes me so fucking irate jesus. nobody is calling ppl who like hajime abusive stop with the strawmen lmao\nLol Adam the Bull with his fake outrage...\n```", "```py\nEver put your fist through your laptops screen? If so its time for a new one lmao #rage #anger #hp\n```", "```py\npip install translators\n```", "```py\nimport translators as tsphrase = 'ØªÙˆØªØ± ÙØ² Ù‚Ù„Ø¨ÙŠ'\nFROM_LANG = 'ar'\nTO_LANG = 'en'\ntext = ts.translate_text(phrase, translator='google',\nÂ Â Â Â from_language=FROM_LANG , to_language=TO_LANG)\nprint (res)\n```", "```py\nTwitter win my heart\n```", "```py\nts.baidutext = ts.translate_text(phrase, translator='bing',\nÂ Â Â Â from_language=FROM_LANG ,\nÂ Â Â Â to_language=TO_LANG)print (res)\n```", "```py\nTension broke my heart\n```", "```py\ndef makeSplits(dataset, classifier, f):Â Â Â Â scores = []\nÂ Â Â Â N = len(dataset)/f\nÂ Â Â Â # Randomize the dataset, but *make sure that you always shuffle\nÂ Â Â Â # it the same way*\nÂ Â Â Â random.seed(0)\nÂ Â Â Â random.shuffle(pruned)\nÂ Â Â Â for i in range(f):\nÂ Â Â Â Â Â Â Â # test is everything from i*N to (i+1)*N,\nÂ Â Â Â Â Â Â Â # train is everything else\nÂ Â Â Â Â Â Â Â test = dataset[i*N:(i+1)*N]\nÂ Â Â Â Â Â Â Â train, = dataset[:i*N]+dataset[(i+1)*N:]\nÂ Â Â Â Â Â Â Â clsf = classifier.train(training)\nÂ Â Â Â Â Â Â Â score = clsf.test(test)\nÂ Â Â Â Â Â Â Â scores.append(score)\nÂ Â Â Â return scores\n```", "```py\ndef accuracy(dataset):Â Â Â Â return sum(x.GS==x.predicted for x in\nÂ Â Â Â Â Â Â Â dataset)/len(dataset)\n```"]