- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Engineering on Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Applied machine learning is basically feature engineering.”
  prefs: []
  type: TYPE_NORMAL
- en: – Andrew Ng
  prefs: []
  type: TYPE_NORMAL
- en: 'As we progress from [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), where
    we harnessed the power of Databricks to explore and refine our datasets, we are
    now ready to delve into the components of Databricks that enable the next step:
    feature engineering. We will start by covering **Databricks Feature Engineering**
    (**DFE**) in Unity Catalog to show you how you can efficiently manage engineered
    features using **Unity Catalog** (**UC**). Understanding how to leverage DFE in
    UC is crucial for creating reusable and consistent features across training and
    inference. Next, you will learn how to leverage Sparka Structured Streaming for
    calculating features on a stream, which allows you to create stateful features
    needed for models to perform quick decision-making. Feature engineering is a broad
    topic. We will focus on how the DI Platform facilitates the development of certain
    feature categories, such as **point-in-time lookups** and **on-demand features**.
    You will also learn how to calculate features in real time during model inference,
    which is vital for scenarios requiring immediate data processing. The last product
    feature we will cover is the **Databricks online store**. You will understand
    how to make features available for real-time access and enhance the responsiveness
    of machine learning models in low-latency applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn about as part of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Feature Engineering in Unity Catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering on a stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing point-in-time lookups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing on-demand features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing features to the Databricks Online Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks Feature Engineering in Unity Catalog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on several types of features. Feature types
    can be roughly grouped based on when they are calculated relative to the time
    of model prediction. The three types we cover in this chapter are batch, streaming,
    and on-demand:'
  prefs: []
  type: TYPE_NORMAL
- en: '`holidays` field in our *Favorita Sales Forecasting* project, as we can compute
    it before we need it and don’t expect the values to change frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming features**: These are features processed in real or near-real time
    as the source data is ingested in the data pipeline, allowing for continuous and
    incremental feature creation. To demonstrate this, we will calculate a streaming
    feature for our *Streaming Transactions* project with Spark Structured Streaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-demand features**: Unlike batch or streaming features, on-demand features
    are only computed when needed, which is to say, at the time of inference. These
    features are crucial for scenarios where feature values are unknown beforehand
    and must be calculated in real time. We will delve into the mechanics of computing
    and storing these features, demonstrating their implementation and integration
    into predictive models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any Unity Catalog table with a defined primary key constraint can be a centralized
    repository for materialized, pre-computed (e.g., batch or streaming) features.
    These types of tables, whose explicit purpose is to centrally store features to
    be used throughout an organization’s analytics, data science, and machine learning
    projects, are commonly called **feature tables**. Feature tables allow data scientists
    to share their engineered features and find features that other team members have
    built.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are a wonderful way to ensure business logic is centrally stored, save
    team members from recreating already-created features, and prevent duplicate work.
    Plus, Unity Catalog manages all data’s discoverability, lineage, and governance,
    so your feature tables are easily managed just like any other table. In Databricks,
    any table with a non-nullable primary key can be a feature table. While there
    is no specific API call to list all tables eligible to be a feature table or to
    list those officially created as a feature table, we recommend using tags or naming
    conventions to identify feature tables. This book’s example projects will follow
    the convention of appending `_ft` at the end of feature table names. Feature Engineering
    in Unity Catalog provides the `FeatureEngineeringClient` Python client (commonly
    imported under the alias of `fe`) for creating and updating feature tables (*Figure
    5**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Example of creating a feature table with FeatureEngineeringClient](img/B16865_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Example of creating a feature table with FeatureEngineeringClient
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, the combination of Delta, UC, and the Feature Engineering client
    provides considerable benefits. Features can be shared and reused across teams,
    reducing the need to recreate them from scratch. Reusability can save time and
    resources, and it can also help to ensure that features are consistent across
    different teams. Centralized feature tables ensure the same code computes feature
    values for training and inference. This is especially important to avoid online/offline
    skew – that is, the discrepancy that can occur when the data transformations used
    during the model training phase (offline) are different from those used when the
    model is deployed and making predictions (online). *Figure 5**.2* shows a data
    science project workflow and highlights the offline and online transformations
    that must match.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – You can avoid accuracy issues by guaranteeing that the transformations
    performed on training data are the same as those performed on inference data](img/B16865_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – You can avoid accuracy issues by guaranteeing that the transformations
    performed on training data are the same as those performed on inference data
  prefs: []
  type: TYPE_NORMAL
- en: We will prevent online/offline skew in the *Streaming Transactions* project
    by wrapping the data transformation processes into a packaged model in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297). Consistency can improve the accuracy of
    models, as it ensures the same input generates the same output in production as
    in training.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For those who are familiar with the Databricks Feature Store, you may wonder
    why we do not utilize it in this chapter. Before Unity Catalog, the Databricks
    Feature Store provided incredible value with added lineage and metadata details.
    With the advancement of UC, instead of having a separate Feature Store product,
    every table utilizing UC automatically has these added benefits. The original
    Databricks Feature Store has been absorbed into UC.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Applying our learning* section ahead, we will save our features to a
    feature table using both SQL and `FeatureEngineeringClient`. Once stored, Unity
    Catalog makes it easy to track the data sources used to create your features and
    the downstream artifacts (such as models and notebooks) that use each feature.
    Even better, when you log a model using a training set of features from a feature
    table, feature metadata is packaged with the resulting model. Watch for this when
    we are working on the *Streaming* *Transactions* project.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering on a stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into feature engineering on a stream, we want to clarify the difference
    between **streaming pipelines** and **streaming data**. If you have not used Spark
    Structured Streaming before, it is a stream processing engine built on the Spark
    SQL engine. It makes it easy to write streaming calculations or transformations
    like you would write expressions for static data. Structured Streaming pipelines
    can process batch or streaming data. Streaming pipelines have elements such as
    checkpoints to automate the data flow. Streaming pipelines, however, are not necessarily
    always running; rather, they only run when the developer chooses it. In contrast,
    streaming data (also known as **real-time data**) refers to continuously generated
    data that can be processed in real time or batch. To simplify, think of streaming
    pipelines as a series of automated conveyor belts in a factory set up to process
    items (data) as they come. These conveyor belts can be turned on or off as needed.
    On the other hand, streaming data is like a never-ending flow of items piling
    up at the beginning of the line. Depending on the volume of the stream, immediate
    handling may be necessary. In the context of our *Streaming Transactions* example
    in the *Applying our learning* section, we are using these automated conveyor
    belts (streaming pipelines) to efficiently manage and transform this continuous
    flow of items (streaming data) into a useful form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Streaming data has several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Immediate insights**: Streaming data allows you to receive quick insights
    and make real-time decisions based on the data. Speed is essential for applications
    where timing is critical, such as financial trading or the real-time monitoring
    of industrial equipment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Up-to-date analysis**: Processing streaming data in real time allows for
    more current data analysis. Real-time analysis can help you find patterns and
    trends as they occur and take proactive steps to address potential data quality
    issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved efficiency**: Streaming data can help organizations become more
    efficient by enabling them to respond to events quickly and proactively. Short
    response times can improve customer satisfaction, reduce downtime, and increase
    productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data can provide significant benefits for organizations that need
    to process large volumes of data quickly and make real-time decisions based on
    that data. However, there are transformations that require a more complex type
    of streaming, specifically, **stateful streaming**. Stateful streaming refers
    to stream processing that consumes a continuous data stream and persists the state
    of past events. Persisting the state allows for the stream to “know” information
    from previous transactions. This is particularly helpful when calculating aggregates
    over a time window since the aggregate would need values from the previous window
    in the stream. To make this clearer, we provide an example of this in the *Streaming
    Transactions* project. There is also an informative video linked in the *Further
    reading* section that explains stateful streaming in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Streaming feature tables should be altered before you begin writing to them.
    Running an `ALTER TABLE` command causes the stream to quit. However, you can restart
    the stream if you must alter the table. Be prepared and plan ahead as much as
    possible!
  prefs: []
  type: TYPE_NORMAL
- en: The DFE client supports intelligent handling and lookups for time-based features,
    such as our timestamped streaming features. Let’s go over this time-saving product
    feature next.
  prefs: []
  type: TYPE_NORMAL
- en: Employing point-in-time lookups for time series feature tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series feature tables are any table in Unity Catalog with a `TIMESERIES`
    primary key. These tables are eligible for point-in-time lookups, which is a mechanism
    for looking up the correct feature values. Before `training_sets`, coming in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297), we often joined tables to connect training
    rows with their feature values. However, a fine-grained event timestamp is not
    ideal for joining. This led to rounding the timestamps to minutes, hours, or even
    days. Depending on the use case, this method may or may not work. For example,
    joining on `TransactionTimestamp` in *Figure 5**.3* is not realistic in a standard
    join so one might create `TransactionMinute` or `TransactionHour`.
  prefs: []
  type: TYPE_NORMAL
- en: '| **TransactionTimestamp** | **TransactionMinute** | **TransactionHour** |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-09-03 19:23:09.765676 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-09-03 19:23:09.765821 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-09-03 19:23:09.765899 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.3 – Example of timestamp rounding for easy joining
  prefs: []
  type: TYPE_NORMAL
- en: Employing point-in-time lookups fixes this problem by handling the time matching
    for you. For those familiar with *The Price Is Right*, it’s the closest feature
    value without going over. More specifically, it will match with the latest feature
    value as of the event timestamp without ever providing a feature value calculated
    after the event. You don’t worry about data leakage during training. Without a
    `Timeseries` primary key, the latest value for a feature is matched.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It’s recommended to apply Z-ordering on time series tables for better performance
    in point-in-time lookups. Z-ordering is covered in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325).
  prefs: []
  type: TYPE_NORMAL
- en: Looking up features is great and often the best choice for your use case. However,
    in some data science and machine learning tasks, we need feature values calculated
    quickly on data that we do not have ahead of time. For those projects, we need
    on-demand features.
  prefs: []
  type: TYPE_NORMAL
- en: Computing on-demand features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculating the number of transactions per customer in a brief time window works
    in a streaming fashion because we only need to use historical data. When we want
    to use a feature that requires data available only at inference time, we use on-demand
    features, with unknown values until inference time. In Databricks, you can create
    on-demand features with Python `training_set` configurations to create training
    datasets, as you will see in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the *Streaming Transactions* project again. We want to add a
    feature for the amount a product sold at, compared to its historical maximum price,
    and use this as part of the training data to predict the generated classification
    label. In this scenario, we don’t know the purchase price until the transaction
    has been received. We’ll cover how to build a Python UDF for calculating an on-demand
    feature for the *Streaming Transactions* project in the *Applying our* *learning*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we recommend checking out the *How Databricks AI improves model
    accuracy with real-time computations* and *Best Practices for Realtime Feature
    Computation on Databricks* articles for in-depth advice from the on-demand experts
    at Databricks; see the *Further reading* section for the links.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed saving features to feature tables in Unity Catalog, the standard
    “offline” pattern for applications that do not have low-latency requirements.
    If your business problem requires low latency or fast results, you’ll need your
    data in an online table or implement Databricks Feature Serving. Feature Serving
    can serve functions as well as precomputed features. For low-latency projects,
    we recommend Databricks Model Serving because it removes any need for Feature
    Serving. We don’t cover Feature Serving in this book, but if you are going to
    serve your models externally to Databricks, Feature Serving may be of interest
    to you.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learn about how to leverage the Databricks Online Store.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing features to the Databricks Online Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to use your features with real-time serving, you can publish the
    features to a low-latency database, also known as an online store. Publishing
    feature tables to a low-latency database allows for automatic feature lookup during
    model inference. There are a variety of options to choose from when choosing an
    online store. A typical data-serving solution requires expert engineers to select
    an appropriate database for online access, build data publishing pipelines, and
    orchestrate deployment. After deployment, someone has to monitor, manage, and
    optimize the pipelines feeding the online store. This is why we recommend Databricks’
    own fully managed serverless online store built right into the platform. It automatically
    syncs your Delta feature table with the online store, making it amazingly easy
    to use. Databricks Online Store is integrated with Databricks Model Serving, so
    it’s easy to set up your online store without ever leaving Databricks. To create
    an online store, go to the **Compute** tab, select **Online stores**, and then
    **Create store**. The next step is to enter your online store’s name and you can
    select a size for your store, based on how many lookups per second, as shown in
    *Figure 5**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Creating an online store from the Compute screen](img/B16865_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Creating an online store from the Compute screen
  prefs: []
  type: TYPE_NORMAL
- en: To sync data to your online store, go to the table with the data you want in
    the store, and from the hotdog menu to the left of the **Create** button, select
    **Sync to** **online store**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Syncing data from a table to your online store](img/B16865_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Syncing data from a table to your online store
  prefs: []
  type: TYPE_NORMAL
- en: You will need to specify the primary key for lookup, and a timestamp column
    if needed, then confirm to sync your data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Confirming the online store with its primary keys](img/B16865_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Confirming the online store with its primary keys
  prefs: []
  type: TYPE_NORMAL
- en: You can check the sync status on your online store on the **Details** tab.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The in-notebook UI for viewing the experiment runs](img/B16865_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The in-notebook UI for viewing the experiment runs
  prefs: []
  type: TYPE_NORMAL
- en: Syncing your tables is simple. No additional engineering is required! Once your
    feature tables are established and ready for Databricks Online Store, simply sync
    the table by providing the primary keys, a timestamp column (if appropriate),
    and how often to sync (*Figure 5**.8*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – UI for syncing a feature table to Databricks Online Store](img/B16865_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – UI for syncing a feature table to Databricks Online Store
  prefs: []
  type: TYPE_NORMAL
- en: Online stores are ideal for storing only a record’s most recent feature values
    when accessing the values at low latency speeds. Common use cases include models
    that require fast feature lookups and serving data to applications. You can use
    the Delta Lake **change data feed** (**CDF**) to make the most of Databricks Online
    Store, which tracks row-level changes in a Delta table. Using CDF, you can update
    feature tables with just the changed values instead of overwriting the entire
    table or keeping track of timestamps. As a result, there is less data you need
    to sync with Databricks Online Store learned about declaring feature tables by
    saving them with a primary key as a Delta table in Unity Catalog. Additionally,
    we can create an online feature store and sync our tables to make the features
    available for low-latency use cases or serving data to applications. Next, we
    must consider how to use features in our training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve gone through ways to save feature tables, build streaming features, implement
    point-in-time lookups, create on-demand features, and publish to the Databricks
    Online Store. It’s time to get ready to follow along in your own Databricks workspace
    as we work through the [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244) project
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned about the feature engineering components of the DI
    platform, let’s put these topics into action and build on the example project
    datasets with new features that will enhance the data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the technical requirements needed to complete the hands-on examples
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Streaming Transactions* project requires more compute power than is available
    in the single node cluster. We created a multi-node cluster to address this. See
    *Figure 5**.9* for the multi-node CPU configuration we used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Multi-node CPU cluster configuration (on AWS) used for this
    book](img/B16865_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Multi-node CPU cluster configuration (on AWS) used for this book
  prefs: []
  type: TYPE_NORMAL
- en: We will use managed volumes to store cleaned, featurized data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project – Streaming Transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’ve been following the code in the previous chapters, at this point, you
    have the streaming data you need. In this chapter, we will augment that data with
    some of the feature engineering techniques discussed earlier. First, we will create
    a streaming feature to count the number of transactions that have arrived in the
    last two minutes for each customer.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump in, let’s remember where we are and where we are going.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.10 – The project pipeline for the Streaming Transactions proj\uFEFF\
    ect](img/B16865_05_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – The project pipeline for the Streaming Transactions project
  prefs: []
  type: TYPE_NORMAL
- en: We are building a streaming pipeline to process the incoming transactions into
    a feature. We will use stateful streaming to count the number of transactions
    per customer over a two-minute timeframe. Stateful streaming is required because
    the calculations in the stream need to know how many transactions have already
    occurred and when a transaction falls outside the two-minute window. This information
    is known as the state of a customer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The streaming feature and the on-demand feature are created in two different
    notebooks. In the code repository, you have the following four notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH5-01-Generating Records`: This notebook is nearly identical to the data
    generator used in the previous chapters. The two key differences are that there
    is now always a product present in the records, and the total (meaning the number
    of time steps) has been increased to provide a stream for a longer period of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH5-02-Auto Loader`: The only change to the `Auto Loader` notebook is the
    location the data is being written to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH5-03-FE Using Spark Structured Streaming`: This notebook is explained in
    detail in the *Building a streaming feature with Spark Structured Streaming* subsection
    of this project. The code is written in Scala. Stateful streaming is now also
    available with PySpark. See the *Further reading* section at the end of this chapter
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH5-04-Building Maximum Price Feature Table`: This notebook calculates the
    maximum price for a product over a time window. The calculated price will be used
    at inference time by the Python UDF also created in this notebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two notebooks are nearly identical to their counterparts from the
    last chapter, so we won’t cover them again. We start from the `CH5-03-FE_Using_Spark_Structured_Streaming`
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Building a streaming feature with Spark Structured Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We calculate the number of transactions per customer in the last two minutes
    and call this new feature `transactionCount`, which is a case class in the code.
  prefs: []
  type: TYPE_NORMAL
- en: '| **CustomerID** | **transactionTimestamp** | **…** | **…** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2023-09-03 19:23:09.765676 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2023-09-03 19:23:09.765821 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2023-09-03 19:23:09.765899 |  |  |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.11 – The table contains example data corresponding to readStream –
    namely, inputDf
  prefs: []
  type: TYPE_NORMAL
- en: We need to aggregate the transactions from the incoming stream, the `InputRow`
    case class, by the `CustomerID` field. *Figure 5**.11* shows an example table
    of the incoming stream. Additionally, we must remove transactions once they have
    fallen outside the specified window; we will call these “expired transactions.”
    This means we must create and maintain a state for each customer, the `TransactionCountState`
    case class. Each customer state consists of `CustomerID`, `transactionCount`,
    and `transactionList` containing the times that the transactions occurred for
    each transaction accounted for in the transaction count. *Figure 5**.12* is a
    visual representation of the customer state. As a customer transaction arrives,
    it is added to a list of transactions with a timestamp as part of the customer
    state.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.12 – Each transaction updates the customer sta\uFEFFt\uFEFFe](img/B16865_05_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Each transaction updates the customer state
  prefs: []
  type: TYPE_NORMAL
- en: 'The state is created by applying the stateful streaming logic to the input
    data. The customer state is then used to write the feature table, `transaction_count_ft`,
    shown in *Figure 5**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CustomerID** | **transactionCount** | **eventTimestamp** | **isTimeout**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 | 2023-09-03T19:24:14.388 | `false` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4 | 2023-09-03T19:24:16.721 | `true` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 2023-09-03T19:24:16.720 | `true` |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.13 – This table is the result after applying the stateful streaming
    transformation
  prefs: []
  type: TYPE_NORMAL
- en: The feature table shown in *Figure 5**.13* includes the `CustomerID` reference,
    `transactionCount` for that customer, `eventTimestamp`, and a Boolean variable,
    `isTimeout`. The `eventTimestamp` is the time the feature record was written.
    We call it `eventTimestamp` because a new transaction or a timeout could have
    triggered the update to the customer state/event. To know which type of event
    it is, we include `isTimeout`. A timeout occurs when no new transactions for a
    customer have occurred but the value of `transactionCount` has changed – an indication
    that the count has decreased.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Logic flow for the stateful streaming transformation](img/B16865_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Logic flow for the stateful streaming transformation
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.14* visually represents the update logic applied to the customer
    state. The logical path can be broken into steps:'
  prefs: []
  type: TYPE_NORMAL
- en: For `n` new transactions coming in for customer `c`, `transactionCount` is incremented
    `n` times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, for each `transactionTimestamp`, `ti`, in `transactionList`, we compare
    the current time with `expirationTimestamp` (`ti+windowMinutes`) to determine
    whether the transaction accounted for in `transactionCount` has expired:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any transactions have expired, we decrement `transactionCount` by one for
    each and drop the corresponding `transactionTimestamp` from `transactionList`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If no transaction has expired or `transactionTimestamp` for the expired transactions
    has been dropped, then we add the new `transactionTimestamp` to `transactionList`
    and write out the customer record.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have outlined the goal of our transformation, let’s look at the
    code. To follow along in your own workspace, please refer to the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH5-01-Generating Records`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH5-02-Auto Loader`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH5-03-FE Using Spark` `Structured Streaming`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH5-04-Building Maximum Price` `Feature Table`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you begin executing *notebook 3*, please open *notebooks 1* and *2* and
    click **Run All** for both. These two notebooks relaunch the data streams that
    we need running in order to run *notebook 3*. However, you do not need all streams
    running in order to run *notebook 4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we are jumping into is in the `CH5-03-FE Using Spark Structured Streaming`
    notebook. We start with the basics – imports and delta configurations – as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Setting delta configurations for optimized writes and compaction](img/B16865_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Setting delta configurations for optimized writes and compaction
  prefs: []
  type: TYPE_NORMAL
- en: 'These configurations can also be set in the cluster configurations, but we
    call them out explicitly in *Figure 5**.15*. These settings will automatically
    compact sets of small files into larger files as it writes for optimal read performance.
    *Figure 5**.16* primarily shows the reset commands that enable starting fresh
    when needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Setting the widget value to True runs the commands in this
    cell, removing the output data, including the checkpoint, and dropping the table](img/B16865_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Setting the widget value to True runs the commands in this cell,
    removing the output data, including the checkpoint, and dropping the table
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the necessary variables in this next code snippet and create our
    table. Notice that variables passed from the setup file are in Python and SQL,
    meaning they are unavailable in Scala. Although all three languages can be used
    in a notebook, they do not share constants or variable values between them. As
    a result, we define the variables we need in Scala for access in the Scala notebook.
    We set the volume location for files, output paths, and the `inputTable` name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Setting variables, constants, and paths in Scala](img/B16865_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Setting variables, constants, and paths in Scala
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we enable the CDF on our streaming feature table, `transaction_count_ft`,
    in*Figure 5**.18*. We could publish this table to an online store if desired.
    Additionally, we set the table name we want to write all transactions, `transaction_count_history`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Creating a table and enabling CDF](img/B16865_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Creating a table and enabling CDF
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the `windowMinutes` constant is the number of minutes we want to aggregate
    transactions for each customer, whereas `maxWaitMinutes` is exactly what it sounds
    like. It is the minutes the stream waits for a transaction before writing out
    the state without new transactions. The value of `maxWaitMinutes` should always
    be less than the value of `windowMinutes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Setting windowMinutes and maxWaitMinutes for our stream](img/B16865_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Setting windowMinutes and maxWaitMinutes for our stream
  prefs: []
  type: TYPE_NORMAL
- en: 'We will initiate `FeatureEngineeringClient` and set feature table tags so we
    can easily see which project these tables are associated with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Setting feature table tags on our table](img/B16865_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Setting feature table tags on our table
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the case class structures. *Figure 5**.21* shows that our case
    classes define our data structures. This is because Scala is a typed language.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Defining class structures to support aggregations for each
    customer ID](img/B16865_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Defining class structures to support aggregations for each customer
    ID
  prefs: []
  type: TYPE_NORMAL
- en: 'In*Figure 5**.22*, we see the `addNewRecords` function. `latestTimestamp` is
    calculated by comparing the latest timestamp in `transactionCountState` with the
    latest timestamp from the new records. This is in case we’ve received data out
    of order. Finally, we create and return the new `TransactionCountState` object
    with the newly calculated `latestTimestamp` and combine the two record lists:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.22 – Defining the addNewRecords fun\uFEFFction](img/B16865_05_22.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – Defining the addNewRecords function
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function, *Figure 5**.23*, drops the records that are more than `windowMinutes`
    old from `TransactionCountState` by calculating the state expiration timestamp
    (the latest timestamp minus the transaction count minutes). Then, it loops through
    the list of current transactions and keeps any that occur before the expiration
    timestamp. This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Defining a function to drop stale records](img/B16865_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – Defining a function to drop stale records
  prefs: []
  type: TYPE_NORMAL
- en: The `updateState` function uses our helper functions to – you guessed it – update
    the customer state. This is the function called `flatMapGroupsWithState`. The
    `updateState` function receives the customer ID, values, and the current state.
    `CustomerID` is the key we are grouping on. `values` is an iterator of `InputRow`.
    In *Figure 5**.11*, we see that `InputRow` is a transaction record consisting
    of a `CustomerID` reference and the time the transaction occurred. The `updateState`
    function behaves in two ways. Suppose one or more `InputRow` records for a given
    `CustomerID` are received. In that case, it will add those records to the state,
    drop any records that are older than `windowMinutes` from the state, and calculate
    the transaction count. See *Figure 5**.18* for the notebook code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – If the state has not timed out, the updateState function receives
    and processes records](img/B16865_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – If the state has not timed out, the updateState function receives
    and processes records
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, as shown in *Figure 5**.25*, if no records are received
    for a given `CustomerID` within a minute since the last time this function was
    called, it will drop any records that are older than `windowMinutes` from the
    state and adjust the count:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – If the state has timed out, the updateState function receives
    no records and only updates the state after dropping expired records](img/B16865_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – If the state has timed out, the updateState function receives
    no records and only updates the state after dropping expired records
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `transactionCounts` list buffer created at the beginning of the
    `updateState` function is returned as an iterator. In this case, the output will
    contain one record: the transaction count record for the specific customer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, as we prepare to create a read stream, we define the input schema we
    need for our read stream; see *Figure 5**.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Read stream input schema](img/B16865_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – Read stream input schema
  prefs: []
  type: TYPE_NORMAL
- en: 'We now must create the read and write components of the stream. The read stream
    reads in the Delta table we created in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123).
    Recall we created the Bronze layer by streaming in JSON files and writing them
    to our `inputTable`. The `readStream` and `writeStream` code is long so we will
    break it up into smaller sections in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should recognize this from earlier chapters. The difference is we are using
    `selectExpr` to isolate `CustomerID` and `TransactionTimestamp`. Additionally,
    we specifically set the type of output dataframe to the case class that `flatMapGroupsWithState`
    is expecting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.27 – InputDF is a read stream reading the table we created in Chapter
    3](img/B16865_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – InputDF is a read stream reading the table we created in Chapter
    3
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the watermark and `flatMapGroupsWithState` function to `inputDf`.
    In Spark Streaming, a watermark is a time threshold determining the maximum allowable
    delay for late events. We’re allowing data to be 30 seconds late before it is
    dropped (*Watermarking in Spark Structured Streaming*, by Thomas Treml: [https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9](https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9)).
    The `flatMapGroupsWithState` function is an arbitrary stateful streaming aggregation
    operator. It applies our `updateState` function to each micro-batch of transactions
    while maintaining the state for each customer ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Applying the watermark and flatMapGroupsWithState function
    to inputDf](img/B16865_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – Applying the watermark and flatMapGroupsWithState function to
    inputDf
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the `updateCounts` function for `foreachBatch` to update the counts
    in the write stream. It performs upserts of the new transaction counts into the
    `transaction_count_ft` table; this is the CDC component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.29 – The updateCounts function upserts the new transaction counts
    into the transaction_count_ft table](img/B16865_05_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – The updateCounts function upserts the new transaction counts into
    the transaction_count_ft table
  prefs: []
  type: TYPE_NORMAL
- en: The last piece of the stream is the write, or, more clearly, the update. The
    write stream applies the `updateCounts` function. Delta tables do not support
    streaming updates directly, so we need to use a `foreachBatch` function. The `foreachBatch`
    function is like a streaming `for` loop applying the function to each micro-batch
    of data in the stream. This write stream is similar to `flatMapGroupsWithState`
    without grouping the data or maintaining the state. We are simply updating the
    CDC result table. Notice the checkpointing is handled for us, and our stream is
    triggered every 10 seconds, meaning every 10 seconds is a new micro-batch of the
    data. The query name is optional. It shows up in the SparkUI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.30 – The write stream applies the updateCounts function using foreachBatch](img/B16865_05_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – The write stream applies the updateCounts function using foreachBatch
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the writing out to the CDC table, we also want a historical
    record of the transaction values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.31 – The write stream records all of the transactionCounts values
    to the Delta table](img/B16865_05_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 – The write stream records all of the transactionCounts values to
    the Delta table
  prefs: []
  type: TYPE_NORMAL
- en: 'While the stream runs, we can observe the output table, `transaction_count_ft`.
    While your streams are running, refresh the table view so you can follow along
    with the output changes as the input data changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.32 – A snapshot of the transaction_count_ft table](img/B16865_05_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.32 – A snapshot of the transaction_count_ft table
  prefs: []
  type: TYPE_NORMAL
- en: Another thing you can observe while the stream runs is the stream statistics.
    This view is found by expanding the results section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.33 – Stream real-time statistics for writeStream](img/B16865_05_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.33 – Stream real-time statistics for writeStream
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the Catalog view to see that the two new tables have appeared. This
    wraps up our streaming feature. Next, we will build a Python UDF.
  prefs: []
  type: TYPE_NORMAL
- en: Building an on-demand feature with a Python UDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s also create a second feature table using on-demand feature engineering.
    Focus on the `CH5-04-Building_Maximum_Price_Feature_Table` notebook. We briefly
    introduced a scenario in an early section of this chapter requiring us to calculate
    a transaction’s difference from the maximum price on the fly. This could be useful
    for modeling. To get the difference as an on-demand feature, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the maximum price per product over a rolling window. We begin with
    creating `time_window`. We want the maximum price in the last three minutes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.34 – Creating a time window to calculate the rolling maximum prices
    by product](img/B16865_05_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.34 – Creating a time window to calculate the rolling maximum prices
    by product
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a window, we can calculate and save that maximum price to a
    DataFrame. Usually, the value of a maximum price doesn’t drastically change, so
    hourly is an appropriate timeframe. We add a new time column called `LookupTimestamp`
    for joining on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.35 – Creating a DataFrame feature table of product maximum prices](img/B16865_05_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.35 – Creating a DataFrame feature table of product maximum prices
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s create a new feature table from our DataFrame. We will assume that
    the maximum price for a specific product does not vary enough to calculate this
    value more than hourly, so we can set this table to update on a set schedule.
    In the GitHub code, we’ll instantiate `FeatureEngineeringClient`. In *Figure 5**.36*,
    we use it to write the new feature table as a Delta table in Unity Catalog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.36 – Writing our table to a Delta table in Unity Catalog](img/B16865_05_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.36 – Writing our table to a Delta table in Unity Catalog
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need a Python UDF to calculate the discount or the difference between
    the transaction amount and the product’s maximum price. We’ll name it `product_difference_ratio_on_demand_feature`.
    We can use the same notebook to build and save this simple function under the
    same catalog and schema as our tables in Unity Catalog.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.37 – Building an on-demand function to calculate product discounts](img/B16865_05_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.37 – Building an on-demand function to calculate product discounts
  prefs: []
  type: TYPE_NORMAL
- en: Once we run this code, we can navigate to Unity Catalog and see `product_difference_ratio_on_demand_feature`
    listed. It’s ready to use in a training set! We’ll refer to this function in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.38 – Viewing the on-demand feature we created in Unity Catalog](img/B16865_05_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.38 – Viewing the on-demand feature we created in Unity Catalog
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we have engineered features for our *Streaming Transactions*
    dataset. We enriched the original data with a streaming feature called `transactionCount`
    and an on-demand feature function that we will use to build the training dataset
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will aggregate store sales data and save it to a feature
    table.
  prefs: []
  type: TYPE_NORMAL
- en: Project – Favorita Store Sales – time series forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In[*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)*,* we used AutoML to explore
    the *Favorita Sales* dataset and create a baseline model predicting sales. To
    follow along in your own workspace, please refer to the following notebook: `CH5-01-Building
    –Favorita –``Feature Tables`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a Databricks feature table, we can use either Python (via `FeatureEngineeringClient`)
    or SQL. This chapter primarily uses Python, but we start by creating a `stores_ft`
    feature table using SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.39 – Creating the stores_ft feature table from the favorita_stores
    table using SQL](img/B16865_05_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.39 – Creating the stores_ft feature table from the favorita_stores
    table using SQL
  prefs: []
  type: TYPE_NORMAL
- en: Executing the code in *Figure 5**.39* creates a feature table called `stores_ft`
    that we will use as a central repository for important store details. `store_nbr`
    is set to `NOT NULL` because it is the primary key. The table does not contain
    a date column, so this feature table is not a time series feature table. If it
    did, we could include an additional `TIMESERIES` primary key. Note that when adding
    the primary key to the table, you can name the constraint for the primary key
    with a unique name, as shown in the documentation. We prefer to let the DFE client
    name the constraint automatically. You can use `DESCRIBE TABLE EXTENDED` to see
    the name of your primary key constraint.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining two feature tables are time series tables. After some transformations,
    we will create the feature tables using Python and the DFE client. Let’s focus
    on the holidays first.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.40 – The holiday events table](img/B16865_05_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.40 – The holiday events table
  prefs: []
  type: TYPE_NORMAL
- en: The features we create are their respective store’s local, regional, and national
    holidays. We can look at *Figure 5**.40* to recall what the holiday data looked
    like.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note about our data is that there are days when multiple holidays
    occur for the same store. We process the three types of holidays very similarly.
    The transformations for national holidays are slightly different, given that there
    is no need to match on locale. All holiday transformations are present in the
    GitHub code. However, we do not cover each in detail in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps take you through the transformations for the local holiday
    type we use for the holiday feature table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin the ETL for the local holidays by singling out the local locale and
    renaming the type to `holiday_type`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.41 – Isolating the local holidays](img/B16865_05_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.41 – Isolating the local holidays
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the bronze table created in *Step 1*, we construct a silver table consisting
    of the date, store number, and local holiday type. We must account for the issue
    of multiple local holidays happening on the same day for the same stores. We do
    so by grouping with a `MIN` function to select a holiday type. The local holiday
    type is changed to `Multiple` using a case statement and `num_holidays` accounts
    for these instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.42 – SQL checking for multiple holiday instances and identifying
    them with a new holiday type for the local holidays](img/B16865_05_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.42 – SQL checking for multiple holiday instances and identifying them
    with a new holiday type for the local holidays
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting silver table is shown in *Figure 5**.43*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.43 – The first five rows of the local_holidays_silver table](img/B16865_05_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.43 – The first five rows of the local_holidays_silver table
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing the same process in the previous steps for regional and national
    holidays, we combine the silver tables into a single DataFrame. We use full joins
    to include all holidays. To avoid `null` dates and store numbers, we chain two
    `ifnull()` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.44 – Combining all three silver tables into a single DataFrame](img/B16865_05_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.44 – Combining all three silver tables into a single DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have prepared our DataFrame with the features we want to save, we
    use the DFE `create_table` method to save the DataFrame as a feature table. We
    specify `primary_keys`. Be sure to note that we are *not* including a time series
    column. This is because we want the feature table lookup only to match exact dates.
    Holidays would not function well with the point-in-time logic. The primary keys
    will also be our lookup keys when we create the training set in the next chapter.
    A best practice is to include a thoughtful description as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.45 – Creating the store holidays feature table using the DFE client](img/B16865_05_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.45 – Creating the store holidays feature table using the DFE client
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in our feature table, *Figure 5**.46*, there are `null` values when
    there is not a holiday of a specific holiday of such type. When we create the
    training set, the feature lookup functionality will have `null` for the values
    of dates not in the table. We do not need to create `null` rows for each date
    that is not a holiday, thus saving us the preprocessing time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.46 – The store_holidays_ft feature table](img/B16865_05_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.46 – The store_holidays_ft feature table
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the holiday and store data, we were provided with oil prices
    data. We can use this data as a proxy for the economy. Let’s create one more feature
    table using the `oil_price_silver` table. Unlike `store_holidays_ft`, having the
    previous value of a stock price in the place of `null` would be helpful. This
    is a fitting example of when to use the point-in-time lookup functionality. To
    do so, a second primary key is required. Therefore, we include the date as the
    primary key but not as `timeseries_column`, as shown in *Figure 5**.47*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.47 – Creating a feature table from the oil prices](img/B16865_05_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.47 – Creating a feature table from the oil prices
  prefs: []
  type: TYPE_NORMAL
- en: We have multiple feature tables that we can use with the specific training data
    provided on Kaggle or any other related training data. For example, `oil_10d_lag_ft`
    can be used as a proxy for the economy for any dataset based in Ecuador.
  prefs: []
  type: TYPE_NORMAL
- en: For future modeling, saving the features in feature tables using the DFE client
    will be helpful. Doing so makes it seamless to look up the features with the model
    at inference time. In the next chapter, using the DFE client, we will combine
    our feature tables to create a training set for our model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we conclude [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), we have successfully
    navigated the multifaceted realm of feature engineering on Databricks. We have
    learned how to organize our features into feature tables, in both SQL and Python,
    by ensuring there is a non-nullable primary key. Unity Catalog provides lineage
    and discoverability, which makes features reusable. Continuing with the streaming
    project, we also highlighted creating a streaming feature using stateful streaming.
    We touched on the latest feature engineering products from Databricks, such as
    point-in-time lookups, on-demand feature functions, and publishing tables to the
    Databricks Online Store. These product features will reduce time to production
    and simplify production pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to tackle feature engineering for a variety of scenarios!
    Next up, we take what we’ve learned to build training sets and machine learning
    models in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  prefs: []
  type: TYPE_NORMAL
- en: What element of the Delta format helps with reproducibility?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the reasons why you would choose to publish a feature to an
    online store?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you create a training set using the Feature Engineering API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What distinguishes a feature table from any other table in Unity Catalog?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the *Streaming Transactions dataset* section of *Applying our learning*,
    we created a stream with a transformation. What could be the business drivers
    for creating this pipeline? We did the *how*; what is the *why*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: Delta’s ability to time travel helps with reproducibility. Delta has versioning
    that allows us a point-in-time lookup to see what data our model was trained on.
    For long-term versioning, deep clones or snapshots are appropriate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing data to an online store provides real-time feature lookup for real-time
    inference models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create `FeatureLookups` for each feature table you wish to include. Then, use
    `create_training_set`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A feature table has a unique primary key, which indicates the object or entity
    the features describe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The possibilities are vast. An example is behavior modeling or customer segmentation
    to support flagging fraud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we identified specific technologies, technical features, and
    options. Please take a look at these resources to get deeper into the areas that
    interest you most:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How Databricks AI improves model accuracy with real-time* *computations*:[https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations](https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use time series feature tables with point-in-time* *support*: [https://docs.databricks.com/en/machine-learning/feature-store/time-series.html](https://docs.databricks.com/en/machine-learning/feature-store/time-series.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Arbitrary Stateful Processing in Structured Streaming –* Databricks
    blog: [https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html](https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks* *Volumes*: [https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html](https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Experimenting with Databricks* *Volumes*: [https://medium.com/@tsiciliani/experimenting-with-databricks-volumes-5666cecb166](https://medium.com/@tsiciliani/experimenting-with-databricks-volumes-5666cecb166)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optimize stateful Structured Streaming* *queries*: [https://docs.databricks.com/en/structured-streaming/stateful-streaming.html](https://docs.databricks.com/en/structured-streaming/stateful-streaming.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks blog – *Introducing Delta Time Travel for Large Scale Data* *Lakes*:
    [https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html](https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YouTube video – *Arbitrary Stateful Aggregations in Structured Streaming in
    Apache Spark by Burak* *Yavuz*:[https://youtu.be/JAb4FIheP28?si=BjoeKkxP_OUxT7-K](https://youtu.be/JAb4FIheP28?si=BjoeKkxP_OUxT7-K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks documentation – *Compute features on demand using Python user-defined*
    *functions*: [https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html](https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Lake change data feed (**CDF)*: [https://docs.databricks.com/en/delta/delta-change-data-feed.html](https://docs.databricks.com/en/delta/delta-change-data-feed.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
