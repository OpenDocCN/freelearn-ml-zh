- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Feature Engineering on Databricks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks上的特征工程
- en: “Applied machine learning is basically feature engineering.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “应用机器学习基本上就是特征工程。”
- en: – Andrew Ng
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 安德鲁·吴
- en: 'As we progress from [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), where
    we harnessed the power of Databricks to explore and refine our datasets, we are
    now ready to delve into the components of Databricks that enable the next step:
    feature engineering. We will start by covering **Databricks Feature Engineering**
    (**DFE**) in Unity Catalog to show you how you can efficiently manage engineered
    features using **Unity Catalog** (**UC**). Understanding how to leverage DFE in
    UC is crucial for creating reusable and consistent features across training and
    inference. Next, you will learn how to leverage Sparka Structured Streaming for
    calculating features on a stream, which allows you to create stateful features
    needed for models to perform quick decision-making. Feature engineering is a broad
    topic. We will focus on how the DI Platform facilitates the development of certain
    feature categories, such as **point-in-time lookups** and **on-demand features**.
    You will also learn how to calculate features in real time during model inference,
    which is vital for scenarios requiring immediate data processing. The last product
    feature we will cover is the **Databricks online store**. You will understand
    how to make features available for real-time access and enhance the responsiveness
    of machine learning models in low-latency applications.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从[*第4章*](B16865_04.xhtml#_idTextAnchor180)（我们利用Databricks探索和精炼我们的数据集）的进展，我们现在准备深入研究Databricks的组件，这些组件使下一步：特征工程成为可能。我们将首先介绍Unity
    Catalog中的**Databricks特征工程**（**DFE**），向您展示您如何使用**Unity Catalog**（**UC**）高效地管理工程特征。了解如何在UC中利用DFE对于创建跨训练和推理的可重复和一致的特征至关重要。接下来，您将学习如何利用Spark
    Structured Streaming在流上计算特征，这允许您创建模型进行快速决策所需的状态化特征。特征工程是一个广泛的话题。我们将关注DI平台如何促进某些特征类别的开发，例如**时间点查找**和**按需功能**。您还将学习如何在模型推理期间实时计算特征，这对于需要即时数据处理的场景至关重要。我们将介绍的最后一个产品功能是**Databricks在线商店**。您将了解如何使特征可用于实时访问，并增强低延迟应用中机器学习模型的响应性。
- en: 'Here is what you will learn about as part of this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: Databricks Feature Engineering in Unity Catalog
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unity Catalog中的Databricks特征工程
- en: Feature engineering on a stream
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式特征工程
- en: Employing point-in-time lookups
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间点查找
- en: Computing on-demand features
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算按需特征
- en: Publishing features to the Databricks Online Store
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征发布到Databricks在线商店
- en: Applying our learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Databricks Feature Engineering in Unity Catalog
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity Catalog中的Databricks特征工程
- en: 'In this chapter, we will focus on several types of features. Feature types
    can be roughly grouped based on when they are calculated relative to the time
    of model prediction. The three types we cover in this chapter are batch, streaming,
    and on-demand:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注几种类型的特征。特征类型可以根据它们相对于模型预测时间的计算时间大致分组。本章中我们涵盖的三种类型是批量、流式和按需：
- en: '`holidays` field in our *Favorita Sales Forecasting* project, as we can compute
    it before we need it and don’t expect the values to change frequently.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的*Favorita销售预测*项目中，`holidays`字段，因为我们可以在需要之前计算它，并且不期望其值频繁变化。
- en: '**Streaming features**: These are features processed in real or near-real time
    as the source data is ingested in the data pipeline, allowing for continuous and
    incremental feature creation. To demonstrate this, we will calculate a streaming
    feature for our *Streaming Transactions* project with Spark Structured Streaming.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式功能**：这些功能在数据管道中实时或接近实时地处理，作为源数据被摄取，允许连续和增量地创建特征。为了演示这一点，我们将使用Spark Structured
    Streaming为我们的*流式事务*项目计算一个流式特征。'
- en: '**On-demand features**: Unlike batch or streaming features, on-demand features
    are only computed when needed, which is to say, at the time of inference. These
    features are crucial for scenarios where feature values are unknown beforehand
    and must be calculated in real time. We will delve into the mechanics of computing
    and storing these features, demonstrating their implementation and integration
    into predictive models.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按需功能**：与批量或流式功能不同，按需功能仅在需要时计算，也就是说，在推理时。这些功能对于事先未知且必须实时计算的特征值场景至关重要。我们将深入探讨计算和存储这些功能的机制，展示它们的实现和集成到预测模型中。'
- en: Any Unity Catalog table with a defined primary key constraint can be a centralized
    repository for materialized, pre-computed (e.g., batch or streaming) features.
    These types of tables, whose explicit purpose is to centrally store features to
    be used throughout an organization’s analytics, data science, and machine learning
    projects, are commonly called **feature tables**. Feature tables allow data scientists
    to share their engineered features and find features that other team members have
    built.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 任何具有定义好的主键约束的 Unity Catalog 表都可以是一个集中式仓库，用于存储和预计算（例如，批量或流式）的特征。这些类型的表，其明确目的是集中存储整个组织在分析、数据科学和机器学习项目中使用的特征，通常被称为**特征表**。特征表允许数据科学家共享他们构建的特征，并找到其他团队成员构建的特征。
- en: 'They are a wonderful way to ensure business logic is centrally stored, save
    team members from recreating already-created features, and prevent duplicate work.
    Plus, Unity Catalog manages all data’s discoverability, lineage, and governance,
    so your feature tables are easily managed just like any other table. In Databricks,
    any table with a non-nullable primary key can be a feature table. While there
    is no specific API call to list all tables eligible to be a feature table or to
    list those officially created as a feature table, we recommend using tags or naming
    conventions to identify feature tables. This book’s example projects will follow
    the convention of appending `_ft` at the end of feature table names. Feature Engineering
    in Unity Catalog provides the `FeatureEngineeringClient` Python client (commonly
    imported under the alias of `fe`) for creating and updating feature tables (*Figure
    5**.1*):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '它们是确保业务逻辑集中存储的绝佳方式，可以节省团队成员重新创建已创建的特征，并防止重复工作。此外，Unity Catalog 管理所有数据的可发现性、血缘和治理，因此您的特征表可以像任何其他表一样轻松管理。在
    Databricks 中，任何具有非空主键的表都可以是特征表。虽然没有特定的 API 调用来列出所有有资格成为特征表的表或列出官方创建的特征表，但我们建议使用标签或命名约定来识别特征表。本书的示例项目将遵循在特征表名称末尾附加
    `_ft` 的约定。Unity Catalog 中的特征工程提供了用于创建和更新特征表的 `FeatureEngineeringClient` Python
    客户端（通常以别名 `fe` 导入）(*图 5.1*):'
- en: '![Figure 5.1 – Example of creating a feature table with FeatureEngineeringClient](img/B16865_05_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 使用 FeatureEngineeringClient 创建特征表的示例](img/B16865_05_01.jpg)'
- en: Figure 5.1 – Example of creating a feature table with FeatureEngineeringClient
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 使用 FeatureEngineeringClient 创建特征表的示例
- en: As we mentioned, the combination of Delta, UC, and the Feature Engineering client
    provides considerable benefits. Features can be shared and reused across teams,
    reducing the need to recreate them from scratch. Reusability can save time and
    resources, and it can also help to ensure that features are consistent across
    different teams. Centralized feature tables ensure the same code computes feature
    values for training and inference. This is especially important to avoid online/offline
    skew – that is, the discrepancy that can occur when the data transformations used
    during the model training phase (offline) are different from those used when the
    model is deployed and making predictions (online). *Figure 5**.2* shows a data
    science project workflow and highlights the offline and online transformations
    that must match.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，Delta、UC 和特征工程客户端的组合提供了相当大的好处。特征可以在团队间共享和重用，减少从头开始重新创建它们的需求。可重用性可以节省时间和资源，还可以帮助确保特征在不同团队间的一致性。集中式特征表确保相同的代码在训练和推理时计算特征值。这对于避免在线/离线偏差尤为重要——也就是说，当模型训练阶段（离线）使用的数据转换与模型部署和进行预测时（在线）使用的数据转换不同时，可能会出现的差异。*图
    5.2* 展示了一个数据科学项目工作流程，并突出了必须匹配的离线和在线转换。
- en: '![Figure 5.2 – You can avoid accuracy issues by guaranteeing that the transformations
    performed on training data are the same as those performed on inference data](img/B16865_05_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 通过确保在训练数据和推理数据上执行的数据转换相同，你可以避免准确性问题](img/B16865_05_02.jpg)'
- en: Figure 5.2 – You can avoid accuracy issues by guaranteeing that the transformations
    performed on training data are the same as those performed on inference data
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 通过确保在训练数据和推理数据上执行的数据转换相同，你可以避免准确性问题
- en: We will prevent online/offline skew in the *Streaming Transactions* project
    by wrapping the data transformation processes into a packaged model in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297). Consistency can improve the accuracy of
    models, as it ensures the same input generates the same output in production as
    in training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将数据转换过程包装在 *第 6 章* [(*Chapter 6*](B16865_06.xhtml#_idTextAnchor297)) 中的打包模型中来防止
    *流式事务* 项目的在线/离线偏差。一致性可以提高模型的准确性，因为它确保相同的输入在生产环境中产生与训练时相同的输出。
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For those who are familiar with the Databricks Feature Store, you may wonder
    why we do not utilize it in this chapter. Before Unity Catalog, the Databricks
    Feature Store provided incredible value with added lineage and metadata details.
    With the advancement of UC, instead of having a separate Feature Store product,
    every table utilizing UC automatically has these added benefits. The original
    Databricks Feature Store has been absorbed into UC.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉 Databricks Feature Store 的人来说，你可能想知道为什么我们在这章中没有使用它。在 Unity Catalog 之前，Databricks
    Feature Store 通过添加血缘和元数据细节提供了巨大的价值。随着 UC 的发展，而不是有一个单独的特征存储产品，每个使用 UC 的表都自动具有这些附加的好处。原始的
    Databricks Feature Store 已经被吸收到 UC 中。
- en: In the *Applying our learning* section ahead, we will save our features to a
    feature table using both SQL and `FeatureEngineeringClient`. Once stored, Unity
    Catalog makes it easy to track the data sources used to create your features and
    the downstream artifacts (such as models and notebooks) that use each feature.
    Even better, when you log a model using a training set of features from a feature
    table, feature metadata is packaged with the resulting model. Watch for this when
    we are working on the *Streaming* *Transactions* project.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 *应用我们的学习* 部分中，我们将使用 SQL 和 `FeatureEngineeringClient` 将我们的特征保存到特征表中。一旦存储，Unity
    Catalog 就可以轻松跟踪创建特征所使用的数据源以及使用每个特征的下游工件（例如模型和笔记本）。更好的是，当你使用来自特征表的特征训练集记录模型时，特征元数据会与生成的模型一起打包。当我们在
    *流式* *事务* 项目上工作时，请注意这一点。
- en: Feature engineering on a stream
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流上的特征工程
- en: Before diving into feature engineering on a stream, we want to clarify the difference
    between **streaming pipelines** and **streaming data**. If you have not used Spark
    Structured Streaming before, it is a stream processing engine built on the Spark
    SQL engine. It makes it easy to write streaming calculations or transformations
    like you would write expressions for static data. Structured Streaming pipelines
    can process batch or streaming data. Streaming pipelines have elements such as
    checkpoints to automate the data flow. Streaming pipelines, however, are not necessarily
    always running; rather, they only run when the developer chooses it. In contrast,
    streaming data (also known as **real-time data**) refers to continuously generated
    data that can be processed in real time or batch. To simplify, think of streaming
    pipelines as a series of automated conveyor belts in a factory set up to process
    items (data) as they come. These conveyor belts can be turned on or off as needed.
    On the other hand, streaming data is like a never-ending flow of items piling
    up at the beginning of the line. Depending on the volume of the stream, immediate
    handling may be necessary. In the context of our *Streaming Transactions* example
    in the *Applying our learning* section, we are using these automated conveyor
    belts (streaming pipelines) to efficiently manage and transform this continuous
    flow of items (streaming data) into a useful form.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨流上的特征工程之前，我们想澄清 *流式管道* 和 *流数据* 之间的区别。如果你之前没有使用过 Spark Structured Streaming，它是一个建立在
    Spark SQL 引擎之上的流处理引擎。它使得编写类似于为静态数据编写表达式的流计算或转换变得容易。结构化流管道可以处理批量或流数据。流管道具有检查点等元素来自动化数据流。然而，流管道并不一定总是运行；相反，它们只有在开发者选择时才会运行。相比之下，流数据（也称为
    **实时数据**）是指持续生成并可实时或批量处理的数据。为了简化，可以将流管道想象成工厂中一系列自动化的传送带，用于处理（数据）物品。这些传送带可以根据需要打开或关闭。另一方面，流数据就像是一条不断堆积在生产线起点的物品的永不停止的流动。根据流量的多少，可能需要立即处理。在我们的
    *应用我们的学习* 部分中的 *流式事务* 示例中，我们使用这些自动化的传送带（流管道）来有效地管理和转换这种连续流动的物品（流数据）成有用的形式。
- en: 'Streaming data has several benefits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据有多个好处：
- en: '**Immediate insights**: Streaming data allows you to receive quick insights
    and make real-time decisions based on the data. Speed is essential for applications
    where timing is critical, such as financial trading or the real-time monitoring
    of industrial equipment.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**即时洞察**：流式数据允许您快速获得洞察力，并基于数据做出实时决策。速度对于时间至关重要的应用程序至关重要，例如金融交易或工业设备的实时监控。'
- en: '**Up-to-date analysis**: Processing streaming data in real time allows for
    more current data analysis. Real-time analysis can help you find patterns and
    trends as they occur and take proactive steps to address potential data quality
    issues.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最新分析**：实时处理流式数据允许进行更当前的数据分析。实时分析可以帮助您在事件发生时发现模式和趋势，并采取主动措施解决潜在的数据质量问题。'
- en: '**Improved efficiency**: Streaming data can help organizations become more
    efficient by enabling them to respond to events quickly and proactively. Short
    response times can improve customer satisfaction, reduce downtime, and increase
    productivity.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高效率**：流式数据可以帮助组织通过快速响应事件并主动采取行动来提高效率。短响应时间可以提高客户满意度，减少停机时间，并提高生产力。'
- en: Streaming data can provide significant benefits for organizations that need
    to process large volumes of data quickly and make real-time decisions based on
    that data. However, there are transformations that require a more complex type
    of streaming, specifically, **stateful streaming**. Stateful streaming refers
    to stream processing that consumes a continuous data stream and persists the state
    of past events. Persisting the state allows for the stream to “know” information
    from previous transactions. This is particularly helpful when calculating aggregates
    over a time window since the aggregate would need values from the previous window
    in the stream. To make this clearer, we provide an example of this in the *Streaming
    Transactions* project. There is also an informative video linked in the *Further
    reading* section that explains stateful streaming in detail.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 流式数据可以为那些需要快速处理大量数据并基于这些数据做出实时决策的组织提供显著的好处。然而，有一些转换需要更复杂的流式处理类型，特别是**有状态的流式处理**。有状态的流式处理是指消耗连续数据流并持久化过去事件状态的流处理。持久化状态使得流“知道”之前交易的信息。这在计算时间窗口内的聚合时特别有用，因为聚合需要从流中的上一个窗口中获取值。为了使这一点更清晰，我们在*流式事务*项目中提供了一个示例。在*进一步阅读*部分还有一个解释有状态流式处理的详细信息的视频链接。
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Streaming feature tables should be altered before you begin writing to them.
    Running an `ALTER TABLE` command causes the stream to quit. However, you can restart
    the stream if you must alter the table. Be prepared and plan ahead as much as
    possible!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始向流式特征表写入之前，应该修改这些表。运行`ALTER TABLE`命令会导致流停止。但是，如果您必须修改表，您可以重新启动流。尽可能做好准备和提前规划！
- en: The DFE client supports intelligent handling and lookups for time-based features,
    such as our timestamped streaming features. Let’s go over this time-saving product
    feature next.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: DFE客户端支持对基于时间的特征进行智能处理和查找，例如我们标记的时间流特征。接下来，让我们了解一下这个节省时间的产品特性。
- en: Employing point-in-time lookups for time series feature tables
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用时间序列特征表的时间点查找
- en: Time series feature tables are any table in Unity Catalog with a `TIMESERIES`
    primary key. These tables are eligible for point-in-time lookups, which is a mechanism
    for looking up the correct feature values. Before `training_sets`, coming in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297), we often joined tables to connect training
    rows with their feature values. However, a fine-grained event timestamp is not
    ideal for joining. This led to rounding the timestamps to minutes, hours, or even
    days. Depending on the use case, this method may or may not work. For example,
    joining on `TransactionTimestamp` in *Figure 5**.3* is not realistic in a standard
    join so one might create `TransactionMinute` or `TransactionHour`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列特征表是Unity Catalog中具有`TIMESERIES`主键的任何表。这些表有资格进行时间点查找，这是一种查找正确特征值的机制。在`training_sets`之前，在[*第6章*](B16865_06.xhtml#_idTextAnchor297)中，我们经常将表连接起来，将训练行与其特征值连接起来。然而，细粒度的事件时间戳不适合连接。这导致了将时间戳四舍五入到分钟、小时甚至天。根据用例，这种方法可能或可能不适用。例如，在*图5**.3中，基于`TransactionTimestamp`的连接在标准连接中是不现实的，因此可能会创建`TransactionMinute`或`TransactionHour`。
- en: '| **TransactionTimestamp** | **TransactionMinute** | **TransactionHour** |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **交易时间戳** | **交易分钟** | **交易小时** |'
- en: '| 2023-09-03 19:23:09.765676 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2023-09-03 19:23:09.765676 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
- en: '| 2023-09-03 19:23:09.765821 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2023-09-03 19:23:09.765821 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
- en: '| 2023-09-03 19:23:09.765899 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2023-09-03 19:23:09.765899 | 2023-09-03 19:23:00 | 2023-09-03 19:00:00 |'
- en: Figure 5.3 – Example of timestamp rounding for easy joining
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 时间戳舍入示例，便于连接
- en: Employing point-in-time lookups fixes this problem by handling the time matching
    for you. For those familiar with *The Price Is Right*, it’s the closest feature
    value without going over. More specifically, it will match with the latest feature
    value as of the event timestamp without ever providing a feature value calculated
    after the event. You don’t worry about data leakage during training. Without a
    `Timeseries` primary key, the latest value for a feature is matched.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用点时间查找，可以解决这个问题，因为它会为您处理时间匹配。对于那些熟悉*谁是价格先生*（The Price Is Right）的人来说，它是指不超过最近的特征值。更具体地说，它将与事件时间戳的最新特征值匹配，而不会提供事件之后计算的特征值。您无需担心训练过程中的数据泄露。如果没有`Timeseries`主键，特征的最新值将被匹配。
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It’s recommended to apply Z-ordering on time series tables for better performance
    in point-in-time lookups. Z-ordering is covered in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在时间序列表上应用Z排序以在点时间查找中提高性能。Z排序在[*第7章*](B16865_07.xhtml#_idTextAnchor325)中有介绍。
- en: Looking up features is great and often the best choice for your use case. However,
    in some data science and machine learning tasks, we need feature values calculated
    quickly on data that we do not have ahead of time. For those projects, we need
    on-demand features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 查找特征非常好，通常是您用例的最佳选择。然而，在某些数据科学和机器学习任务中，我们需要在事先没有数据的情况下快速计算特征值。对于这些项目，我们需要按需特征。
- en: Computing on-demand features
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按需计算特征
- en: Calculating the number of transactions per customer in a brief time window works
    in a streaming fashion because we only need to use historical data. When we want
    to use a feature that requires data available only at inference time, we use on-demand
    features, with unknown values until inference time. In Databricks, you can create
    on-demand features with Python `training_set` configurations to create training
    datasets, as you will see in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在短时间内计算每位客户的交易数量以流式处理方式工作，因为我们只需要使用历史数据。当我们想要使用仅在推理时间才可用的特征时，我们使用按需特征，直到推理时间之前其值未知。在Databricks中，您可以使用Python
    `training_set`配置创建按需特征，以创建训练数据集，正如您将在[*第6章*](B16865_06.xhtml#_idTextAnchor297)中看到的。
- en: Let’s consider the *Streaming Transactions* project again. We want to add a
    feature for the amount a product sold at, compared to its historical maximum price,
    and use this as part of the training data to predict the generated classification
    label. In this scenario, we don’t know the purchase price until the transaction
    has been received. We’ll cover how to build a Python UDF for calculating an on-demand
    feature for the *Streaming Transactions* project in the *Applying our* *learning*
    section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次考虑*流式事务*项目。我们希望添加一个功能，用于比较产品销售价格与其历史最高价格，并将其作为训练数据的一部分来预测生成的分类标签。在这种情况下，我们只有在收到交易后才知道购买价格。我们将在*应用我们的*
    *学习*部分中介绍如何为*流式事务*项目构建Python UDF以计算按需特征。
- en: Additionally, we recommend checking out the *How Databricks AI improves model
    accuracy with real-time computations* and *Best Practices for Realtime Feature
    Computation on Databricks* articles for in-depth advice from the on-demand experts
    at Databricks; see the *Further reading* section for the links.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们建议您查看*如何通过实时计算提高Databricks AI模型准确性*和*在Databricks上实时特征计算的最佳实践*这两篇文章，以获取Databricks按需专家的深入建议；请参阅*进一步阅读*部分获取链接。
- en: We have discussed saving features to feature tables in Unity Catalog, the standard
    “offline” pattern for applications that do not have low-latency requirements.
    If your business problem requires low latency or fast results, you’ll need your
    data in an online table or implement Databricks Feature Serving. Feature Serving
    can serve functions as well as precomputed features. For low-latency projects,
    we recommend Databricks Model Serving because it removes any need for Feature
    Serving. We don’t cover Feature Serving in this book, but if you are going to
    serve your models externally to Databricks, Feature Serving may be of interest
    to you.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了在 Unity Catalog 中保存功能到特征表，这是没有低延迟要求的应用的标准的“离线”模式。如果您的业务问题需要低延迟或快速结果，您需要在在线表中存储数据或实现
    Databricks Feature Serving。特征服务可以提供函数以及预计算的特性。对于低延迟项目，我们推荐使用 Databricks 模型服务，因为它消除了对特征服务的任何需求。本书中我们不涉及特征服务，但如果您打算将模型外部发布到
    Databricks，特征服务可能对您感兴趣。
- en: Next, we learn about how to leverage the Databricks Online Store.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何利用 Databricks 在线商店。
- en: Publishing features to the Databricks Online Store
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将功能发布到 Databricks 在线商店
- en: If you want to use your features with real-time serving, you can publish the
    features to a low-latency database, also known as an online store. Publishing
    feature tables to a low-latency database allows for automatic feature lookup during
    model inference. There are a variety of options to choose from when choosing an
    online store. A typical data-serving solution requires expert engineers to select
    an appropriate database for online access, build data publishing pipelines, and
    orchestrate deployment. After deployment, someone has to monitor, manage, and
    optimize the pipelines feeding the online store. This is why we recommend Databricks’
    own fully managed serverless online store built right into the platform. It automatically
    syncs your Delta feature table with the online store, making it amazingly easy
    to use. Databricks Online Store is integrated with Databricks Model Serving, so
    it’s easy to set up your online store without ever leaving Databricks. To create
    an online store, go to the **Compute** tab, select **Online stores**, and then
    **Create store**. The next step is to enter your online store’s name and you can
    select a size for your store, based on how many lookups per second, as shown in
    *Figure 5**.4*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用实时服务功能，可以将功能发布到低延迟数据库，也称为在线商店。将特征表发布到低延迟数据库允许在模型推理期间自动查找特征。选择在线商店时有很多选项可供选择。典型的数据服务解决方案需要专家工程师选择合适的数据库进行在线访问，构建数据发布管道，并协调部署。部署后，有人需要监控、管理和优化向在线商店提供数据的管道。这就是为什么我们推荐
    Databricks 自带的完全托管的无服务器在线商店，它直接集成到平台中。它自动将您的 Delta 特征表与在线商店同步，使得使用起来非常方便。Databricks
    在线商店与 Databricks 模型服务集成，因此您无需离开 Databricks 就可以轻松设置在线商店。要创建在线商店，请转到**计算**标签页，选择**在线商店**，然后选择**创建商店**。下一步是输入您的在线商店名称，您可以根据每秒查找次数选择商店的大小，如图
    *图 5**.4* 所示。
- en: '![Figure 5.4 – Creating an online store from the Compute screen](img/B16865_05_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 从计算屏幕创建在线商店](img/B16865_05_04.jpg)'
- en: Figure 5.4 – Creating an online store from the Compute screen
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 从计算屏幕创建在线商店
- en: To sync data to your online store, go to the table with the data you want in
    the store, and from the hotdog menu to the left of the **Create** button, select
    **Sync to** **online store**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据同步到您的在线商店，请转到商店中您想要的数据表格，然后从**创建**按钮左侧的热狗菜单中选择**同步到** **在线商店**。
- en: '![Figure 5.5 – Syncing data from a table to your online store](img/B16865_05_05.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 从表格同步数据到您的在线商店](img/B16865_05_05.jpg)'
- en: Figure 5.5 – Syncing data from a table to your online store
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 从表格同步数据到您的在线商店
- en: You will need to specify the primary key for lookup, and a timestamp column
    if needed, then confirm to sync your data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要指定用于查找的主键，如果需要，还需要指定时间戳列，然后确认同步您的数据。
- en: '![Figure 5.6 – Confirming the online store with its primary keys](img/B16865_05_06.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 使用其主键确认在线商店](img/B16865_05_06.jpg)'
- en: Figure 5.6 – Confirming the online store with its primary keys
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 使用其主键确认在线商店
- en: You can check the sync status on your online store on the **Details** tab.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在**详情**标签页上检查您在线商店的同步状态。
- en: '![Figure 5.7 – The in-notebook UI for viewing the experiment runs](img/B16865_05_07.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 查看实验运行的笔记本 UI](img/B16865_05_07.jpg)'
- en: Figure 5.7 – The in-notebook UI for viewing the experiment runs
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 查看实验运行的笔记本 UI
- en: Syncing your tables is simple. No additional engineering is required! Once your
    feature tables are established and ready for Databricks Online Store, simply sync
    the table by providing the primary keys, a timestamp column (if appropriate),
    and how often to sync (*Figure 5**.8*).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 同步您的表很简单。不需要额外的工程！一旦您的特征表建立并准备好用于 Databricks 在线商店，只需通过提供主键、时间戳列（如果适用）以及同步频率（*图
    5**.8*）来同步表。
- en: '![Figure 5.8 – UI for syncing a feature table to Databricks Online Store](img/B16865_05_08.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 同步特征表到 Databricks 在线商店的 UI](img/B16865_05_08.jpg)'
- en: Figure 5.8 – UI for syncing a feature table to Databricks Online Store
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 同步特征表到 Databricks 在线商店的 UI
- en: Online stores are ideal for storing only a record’s most recent feature values
    when accessing the values at low latency speeds. Common use cases include models
    that require fast feature lookups and serving data to applications. You can use
    the Delta Lake **change data feed** (**CDF**) to make the most of Databricks Online
    Store, which tracks row-level changes in a Delta table. Using CDF, you can update
    feature tables with just the changed values instead of overwriting the entire
    table or keeping track of timestamps. As a result, there is less data you need
    to sync with Databricks Online Store learned about declaring feature tables by
    saving them with a primary key as a Delta table in Unity Catalog. Additionally,
    we can create an online feature store and sync our tables to make the features
    available for low-latency use cases or serving data to applications. Next, we
    must consider how to use features in our training datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在线商店非常适合在低延迟速度下访问时仅存储记录的最新特征值。常见的用例包括需要快速特征查找和向应用程序提供数据的模型。您可以使用 Delta Lake
    **变更数据馈送**（**CDF**）充分利用 Databricks 在线商店，它跟踪 Delta 表中的行级更改。使用 CDF，您只需更新特征表中的更改值，而不是覆盖整个表或跟踪时间戳。因此，您需要与
    Databricks 在线商店同步的数据更少。此外，我们可以在 Unity Catalog 中通过将特征表保存为具有主键的 Delta 表来声明特征表。此外，我们可以创建在线特征商店并将我们的表同步，以便在低延迟用例或向应用程序提供数据时使用特征。接下来，我们必须考虑如何在我们的训练数据集中使用特征。
- en: We’ve gone through ways to save feature tables, build streaming features, implement
    point-in-time lookups, create on-demand features, and publish to the Databricks
    Online Store. It’s time to get ready to follow along in your own Databricks workspace
    as we work through the [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244) project
    code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了保存特征表、构建流式特征、实现点时间查找、创建按需特征以及发布到 Databricks 在线商店的方法。现在是时候准备好在自己的 Databricks
    工作区中跟随我们，在处理 [*第 5 章*](B16865_05.xhtml#_idTextAnchor244) 项目代码时进行操作了。
- en: Applying our learning
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Now that we have learned about the feature engineering components of the DI
    platform, let’s put these topics into action and build on the example project
    datasets with new features that will enhance the data science projects.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 DI 平台的特征工程组件，让我们将这些主题付诸实践，并在示例项目数据集上添加新特征，以增强数据科学项目。
- en: Technical requirements
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here are the technical requirements needed to complete the hands-on examples
    in this chapter:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是需要完成本章动手实践所需的技术要求：
- en: The *Streaming Transactions* project requires more compute power than is available
    in the single node cluster. We created a multi-node cluster to address this. See
    *Figure 5**.9* for the multi-node CPU configuration we used.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流式事务* 项目需要的计算能力超过了单节点集群所能提供的。我们创建了一个多节点集群来解决这个问题。参见 *图 5**.9* 中我们使用的多节点 CPU
    配置。'
- en: '![Figure 5.9 – Multi-node CPU cluster configuration (on AWS) used for this
    book](img/B16865_05_09.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 用于本书的多节点 CPU 集群配置（在 AWS 上）](img/B16865_05_09.jpg)'
- en: Figure 5.9 – Multi-node CPU cluster configuration (on AWS) used for this book
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 用于本书的多节点 CPU 集群配置（在 AWS 上）
- en: We will use managed volumes to store cleaned, featurized data.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用托管卷来存储清洗和特征化的数据。
- en: Project – Streaming Transactions
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 流式事务
- en: If you’ve been following the code in the previous chapters, at this point, you
    have the streaming data you need. In this chapter, we will augment that data with
    some of the feature engineering techniques discussed earlier. First, we will create
    a streaming feature to count the number of transactions that have arrived in the
    last two minutes for each customer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您一直在跟踪前几章中的代码，那么现在，您已经有了所需的数据流。在本章中，我们将使用之前讨论的一些特征工程技术来增强这些数据。首先，我们将创建一个流式特征，用于计算每个客户在过去两分钟内到达的交易数量。
- en: Before we jump in, let’s remember where we are and where we are going.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入之前，让我们回顾一下我们现在在哪里以及我们打算去哪里。
- en: "![Figure 5.10 – The project pipeline for the Streaming Transactions proj\uFEFF\
    ect](img/B16865_05_10.jpg)"
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – Streaming Transactions 项目的项目管道](img/B16865_05_10.jpg)'
- en: Figure 5.10 – The project pipeline for the Streaming Transactions project
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – Streaming Transactions 项目的项目管道
- en: We are building a streaming pipeline to process the incoming transactions into
    a feature. We will use stateful streaming to count the number of transactions
    per customer over a two-minute timeframe. Stateful streaming is required because
    the calculations in the stream need to know how many transactions have already
    occurred and when a transaction falls outside the two-minute window. This information
    is known as the state of a customer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建一个流式管道，将传入的交易处理成特征。我们将使用有状态的流式处理来计算在两分钟时间范围内每个客户的交易数量。由于流中的计算需要知道已经发生了多少交易以及何时交易超出了两分钟窗口，因此需要使用有状态的流式处理。这些信息被称为客户的“状态”。
- en: 'The streaming feature and the on-demand feature are created in two different
    notebooks. In the code repository, you have the following four notebooks:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 流式特征和按需特征是在两个不同的笔记本中创建的。在代码仓库中，你有以下四个笔记本：
- en: '`CH5-01-Generating Records`: This notebook is nearly identical to the data
    generator used in the previous chapters. The two key differences are that there
    is now always a product present in the records, and the total (meaning the number
    of time steps) has been increased to provide a stream for a longer period of time.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-01-生成记录`: 这个笔记本几乎与之前章节中使用的数据生成器相同。两个主要区别是现在记录中始终存在一个产品，并且总时间步数（即时间步的数量）已增加，以提供更长时间的流。'
- en: '`CH5-02-Auto Loader`: The only change to the `Auto Loader` notebook is the
    location the data is being written to.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-02-自动加载器`: 对 `自动加载器` 笔记本唯一的更改是数据写入的位置。'
- en: '`CH5-03-FE Using Spark Structured Streaming`: This notebook is explained in
    detail in the *Building a streaming feature with Spark Structured Streaming* subsection
    of this project. The code is written in Scala. Stateful streaming is now also
    available with PySpark. See the *Further reading* section at the end of this chapter
    for more details.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-03-FE 使用 Spark Structured Streaming`: 这个笔记本在本项目的 *使用 Spark Structured
    Streaming 构建流式功能* 子节中进行了详细解释。代码是用 Scala 编写的。现在 PySpark 也支持有状态的流式处理。有关更多详细信息，请参阅本章末尾的
    *进一步阅读* 部分。'
- en: '`CH5-04-Building Maximum Price Feature Table`: This notebook calculates the
    maximum price for a product over a time window. The calculated price will be used
    at inference time by the Python UDF also created in this notebook.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-04-构建最大价格特征表`: 这个笔记本计算产品在时间窗口内的最大价格。计算出的价格将在本笔记本中创建的 Python UDF 推理时使用。'
- en: The first two notebooks are nearly identical to their counterparts from the
    last chapter, so we won’t cover them again. We start from the `CH5-03-FE_Using_Spark_Structured_Streaming`
    notebook.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个笔记本几乎与上一章的对应笔记本相同，所以我们将不再介绍它们。我们从 `CH5-03-FE_Using_Spark_Structured_Streaming`
    笔记本开始。
- en: Building a streaming feature with Spark Structured Streaming
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Spark Structured Streaming 构建流式功能
- en: We calculate the number of transactions per customer in the last two minutes
    and call this new feature `transactionCount`, which is a case class in the code.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算过去两分钟内每个客户的交易数量，并将这个新特征称为 `transactionCount`，它在代码中是一个案例类。
- en: '| **CustomerID** | **transactionTimestamp** | **…** | **…** |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **客户ID** | **交易时间戳** | **…** | **…** |'
- en: '| 1 | 2023-09-03 19:23:09.765676 |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2023-09-03 19:23:09.765676 |  |  |'
- en: '| 4 | 2023-09-03 19:23:09.765821 |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2023-09-03 19:23:09.765821 |  |  |'
- en: '| 2 | 2023-09-03 19:23:09.765899 |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2023-09-03 19:23:09.765899 |  |  |'
- en: Figure 5.11 – The table contains example data corresponding to readStream –
    namely, inputDf
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 包含对应于 readStream 的示例数据的表
- en: We need to aggregate the transactions from the incoming stream, the `InputRow`
    case class, by the `CustomerID` field. *Figure 5**.11* shows an example table
    of the incoming stream. Additionally, we must remove transactions once they have
    fallen outside the specified window; we will call these “expired transactions.”
    This means we must create and maintain a state for each customer, the `TransactionCountState`
    case class. Each customer state consists of `CustomerID`, `transactionCount`,
    and `transactionList` containing the times that the transactions occurred for
    each transaction accounted for in the transaction count. *Figure 5**.12* is a
    visual representation of the customer state. As a customer transaction arrives,
    it is added to a list of transactions with a timestamp as part of the customer
    state.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要根据 `CustomerID` 字段聚合来自输入流的交易，`InputRow` 案例类。*图 5.11* 展示了输入流的示例表。此外，一旦交易超出指定窗口，我们必须删除这些交易；我们将这些称为“过期交易”。这意味着我们必须为每个客户创建并维护一个状态，即
    `TransactionCountState` 案例类。每个客户状态由 `CustomerID`、`transactionCount` 和包含每个交易发生时间的
    `transactionList` 组成。*图 5.12* 是客户状态的视觉表示。当客户交易到达时，它作为客户状态的一部分被添加到交易列表中。
- en: "![Figure 5.12 – Each transaction updates the customer sta\uFEFFt\uFEFFe](img/B16865_05_12.jpg)"
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 每笔交易更新客户状态](img/B16865_05_12.jpg)'
- en: Figure 5.12 – Each transaction updates the customer state
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 每笔交易更新客户状态
- en: 'The state is created by applying the stateful streaming logic to the input
    data. The customer state is then used to write the feature table, `transaction_count_ft`,
    shown in *Figure 5**.13*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 状态是通过将状态流式逻辑应用于输入数据来创建的。然后，使用客户状态来写入如图 5.13 所示的特征表 `transaction_count_ft`：
- en: '| **CustomerID** | **transactionCount** | **eventTimestamp** | **isTimeout**
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **CustomerID** | **transactionCount** | **eventTimestamp** | **isTimeout**
    |'
- en: '| 5 | 3 | 2023-09-03T19:24:14.388 | `false` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 3 | 2023-09-03T19:24:14.388 | `false` |'
- en: '| 2 | 4 | 2023-09-03T19:24:16.721 | `true` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4 | 2023-09-03T19:24:16.721 | `true` |'
- en: '| 3 | 0 | 2023-09-03T19:24:16.720 | `true` |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 2023-09-03T19:24:16.720 | `true` |'
- en: Figure 5.13 – This table is the result after applying the stateful streaming
    transformation
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 这是应用状态流式转换后的结果表
- en: The feature table shown in *Figure 5**.13* includes the `CustomerID` reference,
    `transactionCount` for that customer, `eventTimestamp`, and a Boolean variable,
    `isTimeout`. The `eventTimestamp` is the time the feature record was written.
    We call it `eventTimestamp` because a new transaction or a timeout could have
    triggered the update to the customer state/event. To know which type of event
    it is, we include `isTimeout`. A timeout occurs when no new transactions for a
    customer have occurred but the value of `transactionCount` has changed – an indication
    that the count has decreased.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.13* 中显示的特征表包括 `CustomerID` 引用、该客户的 `transactionCount`、`eventTimestamp`
    和一个布尔变量 `isTimeout`。`eventTimestamp` 是特征记录被写入的时间。我们称之为 `eventTimestamp`，因为新的交易或超时可能会触发对客户状态/事件的更新。为了知道它是哪种类型的事件，我们包括
    `isTimeout`。当没有新的交易发生，但 `transactionCount` 的值已更改时，会发生超时——这是计数已减少的指示。'
- en: '![Figure 5.14 – Logic flow for the stateful streaming transformation](img/B16865_05_14.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 状态流式转换的逻辑流程](img/B16865_05_14.jpg)'
- en: Figure 5.14 – Logic flow for the stateful streaming transformation
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 状态流式转换的逻辑流程
- en: '*Figure 5**.14* visually represents the update logic applied to the customer
    state. The logical path can be broken into steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.14* 直观地表示了应用于客户状态的更新逻辑。逻辑路径可以分解为以下步骤：'
- en: For `n` new transactions coming in for customer `c`, `transactionCount` is incremented
    `n` times.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于客户 `c` 的 `n` 个新交易，`transactionCount` 被增加 `n` 次。
- en: 'Then, for each `transactionTimestamp`, `ti`, in `transactionList`, we compare
    the current time with `expirationTimestamp` (`ti+windowMinutes`) to determine
    whether the transaction accounted for in `transactionCount` has expired:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对于 `transactionList` 中的每个 `transactionTimestamp`，`ti`，我们将当前时间与 `expirationTimestamp`
    (`ti+windowMinutes`) 进行比较，以确定 `transactionCount` 中记录的交易是否已过期：
- en: If any transactions have expired, we decrement `transactionCount` by one for
    each and drop the corresponding `transactionTimestamp` from `transactionList`.
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有任何交易已过期，我们将 `transactionCount` 减少一次，并从 `transactionList` 中删除相应的 `transactionTimestamp`。
- en: If no transaction has expired or `transactionTimestamp` for the expired transactions
    has been dropped, then we add the new `transactionTimestamp` to `transactionList`
    and write out the customer record.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有事务过期或已过期的交易中的`transactionTimestamp`已被删除，那么我们将新的`transactionTimestamp`添加到`transactionList`中，并写出客户记录。
- en: 'Now that we have outlined the goal of our transformation, let’s look at the
    code. To follow along in your own workspace, please refer to the following notebooks:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经概述了我们的转换目标，让我们看看代码。要在自己的工作空间中跟随，请参考以下笔记本：
- en: '`CH5-01-Generating Records`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-01-生成记录`'
- en: '`CH5-02-Auto Loader`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-02-自动加载器`'
- en: '`CH5-03-FE Using Spark` `Structured Streaming`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-03-FE 使用 Spark` `结构化流`'
- en: '`CH5-04-Building Maximum Price` `Feature Table`'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH5-04-构建最大价格` `特性表`'
- en: Before you begin executing *notebook 3*, please open *notebooks 1* and *2* and
    click **Run All** for both. These two notebooks relaunch the data streams that
    we need running in order to run *notebook 3*. However, you do not need all streams
    running in order to run *notebook 4*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始执行*笔记本3*之前，请打开*笔记本1*和*笔记本2*，并点击两个笔记本的**运行全部**。这两个笔记本重新启动我们需要运行的用于运行*笔记本3*的数据流。然而，您不需要所有流都在运行才能运行*笔记本4*。
- en: 'The code we are jumping into is in the `CH5-03-FE Using Spark Structured Streaming`
    notebook. We start with the basics – imports and delta configurations – as shown
    in the following screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要跳入的代码位于`CH5-03-FE 使用 Spark Structured Streaming`笔记本中。我们从头开始，包括导入和 delta
    配置，如下截图所示：
- en: '![Figure 5.15 – Setting delta configurations for optimized writes and compaction](img/B16865_05_15.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图5.15 – 为优化写入和压缩设置 delta 配置](img/B16865_05_15.jpg)'
- en: Figure 5.15 – Setting delta configurations for optimized writes and compaction
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 – 为优化写入和压缩设置 delta 配置
- en: 'These configurations can also be set in the cluster configurations, but we
    call them out explicitly in *Figure 5**.15*. These settings will automatically
    compact sets of small files into larger files as it writes for optimal read performance.
    *Figure 5**.16* primarily shows the reset commands that enable starting fresh
    when needed:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置也可以在集群配置中设置，但我们已在*图5.15*中明确指出。这些设置将自动将小文件集压缩成大文件，以便在写入时实现最佳读取性能。*图5.16*主要显示了重置命令，这些命令可以在需要时从头开始：
- en: '![Figure 5.16 – Setting the widget value to True runs the commands in this
    cell, removing the output data, including the checkpoint, and dropping the table](img/B16865_05_16.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图5.16 – 将小部件值设置为 True 运行此单元格中的命令，删除输出数据，包括检查点，并删除表](img/B16865_05_16.jpg)'
- en: Figure 5.16 – Setting the widget value to True runs the commands in this cell,
    removing the output data, including the checkpoint, and dropping the table
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 – 将小部件值设置为 True 运行此单元格中的命令，删除输出数据，包括检查点，并删除表
- en: 'We define the necessary variables in this next code snippet and create our
    table. Notice that variables passed from the setup file are in Python and SQL,
    meaning they are unavailable in Scala. Although all three languages can be used
    in a notebook, they do not share constants or variable values between them. As
    a result, we define the variables we need in Scala for access in the Scala notebook.
    We set the volume location for files, output paths, and the `inputTable` name:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码片段中，我们定义必要的变量并创建我们的表。请注意，从设置文件传递的变量在 Python 和 SQL 中，这意味着它们在 Scala 中不可用。尽管所有三种语言都可以在笔记本中使用，但它们之间不共享常量或变量值。因此，我们在
    Scala 中定义了我们需要的变量，以便在 Scala 笔记本中访问。我们设置了文件、输出路径和`inputTable`名称的卷位置：
- en: '![Figure 5.17 – Setting variables, constants, and paths in Scala](img/B16865_05_17.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图5.17 – 在 Scala 中设置变量、常量和路径](img/B16865_05_17.jpg)'
- en: Figure 5.17 – Setting variables, constants, and paths in Scala
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 – 在 Scala 中设置变量、常量和路径
- en: 'Notice that we enable the CDF on our streaming feature table, `transaction_count_ft`,
    in*Figure 5**.18*. We could publish this table to an online store if desired.
    Additionally, we set the table name we want to write all transactions, `transaction_count_history`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在*图5.18*中启用了我们的流特性表`transaction_count_ft`的CDF。如果需要，我们可以将此表发布到在线商店。此外，我们设置了要写入所有交易的表名`transaction_count_history`：
- en: '![Figure 5.18 – Creating a table and enabling CDF](img/B16865_05_18.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图5.18 – 创建表并启用 CDF](img/B16865_05_18.jpg)'
- en: Figure 5.18 – Creating a table and enabling CDF
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 – 创建表并启用 CDF
- en: 'Next, the `windowMinutes` constant is the number of minutes we want to aggregate
    transactions for each customer, whereas `maxWaitMinutes` is exactly what it sounds
    like. It is the minutes the stream waits for a transaction before writing out
    the state without new transactions. The value of `maxWaitMinutes` should always
    be less than the value of `windowMinutes`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`windowMinutes`常量是我们想要为每个客户聚合交易的分钟数，而`maxWaitMinutes`正如其名。这是流在写入状态前等待交易之前等待的分钟数。`maxWaitMinutes`的值应该始终小于`windowMinutes`的值：
- en: '![Figure 5.19 – Setting windowMinutes and maxWaitMinutes for our stream](img/B16865_05_19.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图5.19 – 为我们的流设置windowMinutes和maxWaitMinutes](img/B16865_05_19.jpg)'
- en: Figure 5.19 – Setting windowMinutes and maxWaitMinutes for our stream
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 – 为我们的流设置windowMinutes和maxWaitMinutes
- en: 'We will initiate `FeatureEngineeringClient` and set feature table tags so we
    can easily see which project these tables are associated with:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将启动`FeatureEngineeringClient`并设置特征表标签，这样我们就可以轻松地看到这些表与哪个项目相关联：
- en: '![Figure 5.20 – Setting feature table tags on our table](img/B16865_05_20.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图5.20 – 在我们的表上设置特征表标签](img/B16865_05_20.jpg)'
- en: Figure 5.20 – Setting feature table tags on our table
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 – 在我们的表上设置特征表标签
- en: Next, we define the case class structures. *Figure 5**.21* shows that our case
    classes define our data structures. This is because Scala is a typed language.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义案例类结构。*图5.21* 显示我们的案例类定义了我们的数据结构。这是因为Scala是一种静态类型语言。
- en: '![Figure 5.21 – Defining class structures to support aggregations for each
    customer ID](img/B16865_05_21.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图5.21 – 定义支持每个客户ID聚合的类结构](img/B16865_05_21.jpg)'
- en: Figure 5.21 – Defining class structures to support aggregations for each customer
    ID
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 – 定义支持每个客户ID聚合的类结构
- en: 'In*Figure 5**.22*, we see the `addNewRecords` function. `latestTimestamp` is
    calculated by comparing the latest timestamp in `transactionCountState` with the
    latest timestamp from the new records. This is in case we’ve received data out
    of order. Finally, we create and return the new `TransactionCountState` object
    with the newly calculated `latestTimestamp` and combine the two record lists:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.22*中，我们看到`addNewRecords`函数。`latestTimestamp`是通过比较`transactionCountState`中的最新时间戳与新记录中的最新时间戳来计算的。这是以防我们收到数据顺序出错。最后，我们创建并返回新的`TransactionCountState`对象，包含新计算的`latestTimestamp`，并将两个记录列表合并：
- en: "![Figure 5.22 – Defining the addNewRecords fun\uFEFFction](img/B16865_05_22.jpg)"
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图5.22 – 定义添加新记录的函数](img/B16865_05_22.jpg)'
- en: Figure 5.22 – Defining the addNewRecords function
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 – 定义添加新记录的函数
- en: 'The next function, *Figure 5**.23*, drops the records that are more than `windowMinutes`
    old from `TransactionCountState` by calculating the state expiration timestamp
    (the latest timestamp minus the transaction count minutes). Then, it loops through
    the list of current transactions and keeps any that occur before the expiration
    timestamp. This is shown in the following screenshot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数，*图5.23*，通过计算状态过期时间戳（最新时间戳减去交易计数分钟）从`TransactionCountState`中删除超过`windowMinutes`旧的记录。然后，它遍历当前交易的列表，并保留任何在过期时间戳之前发生的交易。这在上面的屏幕截图中有显示：
- en: '![Figure 5.23 – Defining a function to drop stale records](img/B16865_05_23.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图5.23 – 定义一个函数以删除过时的记录](img/B16865_05_23.jpg)'
- en: Figure 5.23 – Defining a function to drop stale records
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23 – 定义一个函数以删除过时的记录
- en: The `updateState` function uses our helper functions to – you guessed it – update
    the customer state. This is the function called `flatMapGroupsWithState`. The
    `updateState` function receives the customer ID, values, and the current state.
    `CustomerID` is the key we are grouping on. `values` is an iterator of `InputRow`.
    In *Figure 5**.11*, we see that `InputRow` is a transaction record consisting
    of a `CustomerID` reference and the time the transaction occurred. The `updateState`
    function behaves in two ways. Suppose one or more `InputRow` records for a given
    `CustomerID` are received. In that case, it will add those records to the state,
    drop any records that are older than `windowMinutes` from the state, and calculate
    the transaction count. See *Figure 5**.18* for the notebook code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`updateState`函数使用我们的辅助函数来更新客户状态。这是调用`flatMapGroupsWithState`的函数。`updateState`函数接收客户ID、值和当前状态。`CustomerID`是我们分组的键。`values`是`InputRow`的迭代器。在*图5.11*中，我们看到`InputRow`是一个包含`CustomerID`引用和交易发生时间的交易记录。`updateState`函数有两种行为。假设收到一个或多个针对给定`CustomerID`的`InputRow`记录。在这种情况下，它将那些记录添加到状态中，从状态中删除任何超过`windowMinutes`的记录，并计算交易计数。请参阅*图5.18*中的笔记本代码。'
- en: '![Figure 5.24 – If the state has not timed out, the updateState function receives
    and processes records](img/B16865_05_24.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.24 – 如果状态未超时，updateState 函数接收并处理记录](img/B16865_05_24.jpg)'
- en: Figure 5.24 – If the state has not timed out, the updateState function receives
    and processes records
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 – 如果状态未超时，updateState 函数接收并处理记录
- en: 'On the other hand, as shown in *Figure 5**.25*, if no records are received
    for a given `CustomerID` within a minute since the last time this function was
    called, it will drop any records that are older than `windowMinutes` from the
    state and adjust the count:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如**图 5**.25 所示，如果在调用此函数后的一个分钟内没有接收到针对特定 `CustomerID` 的记录，它将删除状态中任何早于 `windowMinutes`
    的记录，并调整计数：
- en: '![Figure 5.25 – If the state has timed out, the updateState function receives
    no records and only updates the state after dropping expired records](img/B16865_05_25.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.25 – 如果状态已超时，updateState 函数接收不到记录，并且仅在删除过期的记录后更新状态](img/B16865_05_25.jpg)'
- en: Figure 5.25 – If the state has timed out, the updateState function receives
    no records and only updates the state after dropping expired records
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25 – 如果状态未超时，updateState 函数接收并处理记录
- en: 'Note that the `transactionCounts` list buffer created at the beginning of the
    `updateState` function is returned as an iterator. In this case, the output will
    contain one record: the transaction count record for the specific customer.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 `updateState` 函数开始时创建的 `transactionCounts` 列表缓冲区作为迭代器返回。在这种情况下，输出将包含一条记录：特定客户的交易计数记录。
- en: 'Next, as we prepare to create a read stream, we define the input schema we
    need for our read stream; see *Figure 5**.26*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，当我们准备创建读取流时，我们定义我们需要的读取流输入模式；参见**图 5**.26：
- en: '![Figure 5.26 – Read stream input schema](img/B16865_05_26.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.26 – 读取流输入模式](img/B16865_05_26.jpg)'
- en: Figure 5.26 – Read stream input schema
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26 – 读取流输入模式
- en: 'We now must create the read and write components of the stream. The read stream
    reads in the Delta table we created in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123).
    Recall we created the Bronze layer by streaming in JSON files and writing them
    to our `inputTable`. The `readStream` and `writeStream` code is long so we will
    break it up into smaller sections in the following steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须创建流的读取和写入组件。读取流读取我们在 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123) 中创建的
    Delta 表。回想一下，我们通过流式传输 JSON 文件并将它们写入我们的 `inputTable` 来创建 Bronze 层。`readStream`
    和 `writeStream` 代码很长，所以我们将在以下步骤中将它们分成更小的部分：
- en: 'You should recognize this from earlier chapters. The difference is we are using
    `selectExpr` to isolate `CustomerID` and `TransactionTimestamp`. Additionally,
    we specifically set the type of output dataframe to the case class that `flatMapGroupsWithState`
    is expecting:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该从前面的章节中认出这一点。区别在于我们使用 `selectExpr` 来隔离 `CustomerID` 和 `TransactionTimestamp`。此外，我们特别设置了输出数据框的类型，使其符合
    `flatMapGroupsWithState` 所期望的案例类：
- en: '![Figure 5.27 – InputDF is a read stream reading the table we created in Chapter
    3](img/B16865_05_27.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.27 – InputDF 是一个读取我们在第 3 章创建的表的读取流](img/B16865_05_27.jpg)'
- en: Figure 5.27 – InputDF is a read stream reading the table we created in Chapter
    3
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27 – InputDF 是一个读取我们在第 3 章创建的表的读取流
- en: 'We apply the watermark and `flatMapGroupsWithState` function to `inputDf`.
    In Spark Streaming, a watermark is a time threshold determining the maximum allowable
    delay for late events. We’re allowing data to be 30 seconds late before it is
    dropped (*Watermarking in Spark Structured Streaming*, by Thomas Treml: [https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9](https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9)).
    The `flatMapGroupsWithState` function is an arbitrary stateful streaming aggregation
    operator. It applies our `updateState` function to each micro-batch of transactions
    while maintaining the state for each customer ID:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将水印和 `flatMapGroupsWithState` 函数应用于 `inputDf`。在 Spark Streaming 中，水印是一个时间阈值，用于确定允许的延迟事件的最大延迟。我们允许数据在丢弃之前晚到
    30 秒（参见 Thomas Treml 的文章《Spark Structured Streaming 中的水印》：[https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9](https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9)）。`flatMapGroupsWithState`
    函数是一个任意有状态的流聚合操作符。它将我们的 `updateState` 函数应用于每个事务的微批，同时维护每个客户 ID 的状态：
- en: '![Figure 5.28 – Applying the watermark and flatMapGroupsWithState function
    to inputDf](img/B16865_05_28.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.28 – 将水印和 flatMapGroupsWithState 函数应用于 inputDf](img/B16865_05_28.jpg)'
- en: Figure 5.28 – Applying the watermark and flatMapGroupsWithState function to
    inputDf
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.28 – 将水印和flatMapGroupsWithState函数应用于inputDf
- en: 'We define the `updateCounts` function for `foreachBatch` to update the counts
    in the write stream. It performs upserts of the new transaction counts into the
    `transaction_count_ft` table; this is the CDC component:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为`foreachBatch`函数定义了`updateCounts`函数，用于更新写入流中的计数。它将新事务计数上提到`transaction_count_ft`表中；这是CDC组件：
- en: '![Figure 5.29 – The updateCounts function upserts the new transaction counts
    into the transaction_count_ft table](img/B16865_05_29.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图5.29 – updateCounts函数将新事务计数上提到transaction_count_ft表中](img/B16865_05_29.jpg)'
- en: Figure 5.29 – The updateCounts function upserts the new transaction counts into
    the transaction_count_ft table
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.29 – updateCounts函数将新事务计数上提到transaction_count_ft表中
- en: The last piece of the stream is the write, or, more clearly, the update. The
    write stream applies the `updateCounts` function. Delta tables do not support
    streaming updates directly, so we need to use a `foreachBatch` function. The `foreachBatch`
    function is like a streaming `for` loop applying the function to each micro-batch
    of data in the stream. This write stream is similar to `flatMapGroupsWithState`
    without grouping the data or maintaining the state. We are simply updating the
    CDC result table. Notice the checkpointing is handled for us, and our stream is
    triggered every 10 seconds, meaning every 10 seconds is a new micro-batch of the
    data. The query name is optional. It shows up in the SparkUI.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流的最后一部分是写入，或者更确切地说，是更新。写入流应用`updateCounts`函数。Delta表不支持直接进行流更新，因此我们需要使用`foreachBatch`函数。`foreachBatch`函数类似于流式`for`循环，将函数应用于流中的每个微批数据。这个写入流类似于`flatMapGroupsWithState`，但没有对数据进行分组或维护状态。我们只是在更新CDC结果表。请注意，检查点是由我们处理的，并且我们的流每10秒触发一次，这意味着每10秒是数据的新微批。查询名称是可选的。它会在SparkUI中显示。
- en: '![Figure 5.30 – The write stream applies the updateCounts function using foreachBatch](img/B16865_05_30.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图5.30 – 写入流使用foreachBatch应用updateCounts函数](img/B16865_05_30.jpg)'
- en: Figure 5.30 – The write stream applies the updateCounts function using foreachBatch
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.30 – 写入流使用foreachBatch应用updateCounts函数
- en: 'In addition to the writing out to the CDC table, we also want a historical
    record of the transaction values:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了写入CDC表之外，我们还想保留事务值的历史记录：
- en: '![Figure 5.31 – The write stream records all of the transactionCounts values
    to the Delta table](img/B16865_05_31.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图5.31 – 写入流将所有transactionCounts值记录到Delta表中](img/B16865_05_31.jpg)'
- en: Figure 5.31 – The write stream records all of the transactionCounts values to
    the Delta table
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31 – 写入流将所有transactionCounts值记录到Delta表中
- en: 'While the stream runs, we can observe the output table, `transaction_count_ft`.
    While your streams are running, refresh the table view so you can follow along
    with the output changes as the input data changes:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流运行期间，我们可以观察输出表，`transaction_count_ft`。当你的流运行时，刷新表视图，以便你可以随着输入数据的变化跟踪输出变化：
- en: '![Figure 5.32 – A snapshot of the transaction_count_ft table](img/B16865_05_32.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图5.32 – transaction_count_ft表的快照](img/B16865_05_32.jpg)'
- en: Figure 5.32 – A snapshot of the transaction_count_ft table
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.32 – transaction_count_ft表的快照
- en: Another thing you can observe while the stream runs is the stream statistics.
    This view is found by expanding the results section.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流运行期间，你还可以观察流统计信息。这个视图可以通过展开结果部分找到。
- en: '![Figure 5.33 – Stream real-time statistics for writeStream](img/B16865_05_33.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图5.33 – writeStream的流实时统计](img/B16865_05_33.jpg)'
- en: Figure 5.33 – Stream real-time statistics for writeStream
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.33 – writeStream的流实时统计
- en: Navigate to the Catalog view to see that the two new tables have appeared. This
    wraps up our streaming feature. Next, we will build a Python UDF.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到目录视图，查看是否出现了两个新的表。这标志着我们的流功能完成。接下来，我们将构建一个Python UDF。
- en: Building an on-demand feature with a Python UDF
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python UDF构建按需功能
- en: 'Let’s also create a second feature table using on-demand feature engineering.
    Focus on the `CH5-04-Building_Maximum_Price_Feature_Table` notebook. We briefly
    introduced a scenario in an early section of this chapter requiring us to calculate
    a transaction’s difference from the maximum price on the fly. This could be useful
    for modeling. To get the difference as an on-demand feature, we do the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个使用按需特征工程的特征表。关注`CH5-04-Building_Maximum_Price_Feature_Table`笔记本。我们在本章早期部分简要介绍了一个场景，要求我们实时计算交易与最高价格之间的差异。这可能对建模很有用。为了将差异作为按需特征，我们执行以下操作：
- en: 'Calculate the maximum price per product over a rolling window. We begin with
    creating `time_window`. We want the maximum price in the last three minutes:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算滚动窗口中每个产品的最大价格。我们首先创建`time_window`。我们想要过去三分钟内的最大价格：
- en: '![Figure 5.34 – Creating a time window to calculate the rolling maximum prices
    by product](img/B16865_05_34.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图5.34 – 创建一个时间窗口来按产品计算滚动最大价格](img/B16865_05_34.jpg)'
- en: Figure 5.34 – Creating a time window to calculate the rolling maximum prices
    by product
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.34 – 创建一个时间窗口来按产品计算滚动最大价格](img/B16865_05_34.jpg)'
- en: Now that we have a window, we can calculate and save that maximum price to a
    DataFrame. Usually, the value of a maximum price doesn’t drastically change, so
    hourly is an appropriate timeframe. We add a new time column called `LookupTimestamp`
    for joining on.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了窗口，我们可以计算并将最大价格保存到 DataFrame 中。通常，最大价格的价值不会发生剧烈变化，所以每小时是一个合适的时间框架。我们添加一个名为`LookupTimestamp`的新时间列，用于连接。
- en: '![Figure 5.35 – Creating a DataFrame feature table of product maximum prices](img/B16865_05_35.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图5.35 – 创建产品最大价格的 DataFrame 特征表](img/B16865_05_35.jpg)'
- en: Figure 5.35 – Creating a DataFrame feature table of product maximum prices
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.35 – 创建产品最大价格的 DataFrame 特征表](img/B16865_05_35.jpg)'
- en: 'Next, let’s create a new feature table from our DataFrame. We will assume that
    the maximum price for a specific product does not vary enough to calculate this
    value more than hourly, so we can set this table to update on a set schedule.
    In the GitHub code, we’ll instantiate `FeatureEngineeringClient`. In *Figure 5**.36*,
    we use it to write the new feature table as a Delta table in Unity Catalog:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们从我们的 DataFrame 中创建一个新的特征表。我们将假设特定产品的最大价格变化不足以每小时计算此值，因此我们可以将此表设置为按固定时间表更新。在
    GitHub 代码中，我们将实例化`FeatureEngineeringClient`。在*图5*.36中，我们使用它将新的特征表作为 Delta 表写入
    Unity Catalog：
- en: '![Figure 5.36 – Writing our table to a Delta table in Unity Catalog](img/B16865_05_36.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图5.36 – 将我们的表写入 Unity Catalog 中的 Delta 表](img/B16865_05_36.jpg)'
- en: Figure 5.36 – Writing our table to a Delta table in Unity Catalog
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.36 – 将我们的表写入 Unity Catalog 中的 Delta 表
- en: Next, we need a Python UDF to calculate the discount or the difference between
    the transaction amount and the product’s maximum price. We’ll name it `product_difference_ratio_on_demand_feature`.
    We can use the same notebook to build and save this simple function under the
    same catalog and schema as our tables in Unity Catalog.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个 Python UDF 来计算折扣或交易金额与产品最大价格之间的差异。我们将将其命名为`product_difference_ratio_on_demand_feature`。我们可以使用相同的笔记本在
    Unity Catalog 中相同的目录和架构下构建和保存这个简单的函数。
- en: '![Figure 5.37 – Building an on-demand function to calculate product discounts](img/B16865_05_37.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图5.37 – 构建一个按需函数来计算产品折扣](img/B16865_05_37.jpg)'
- en: Figure 5.37 – Building an on-demand function to calculate product discounts
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.37 – 构建一个按需函数来计算产品折扣](img/B16865_05_37.jpg)'
- en: Once we run this code, we can navigate to Unity Catalog and see `product_difference_ratio_on_demand_feature`
    listed. It’s ready to use in a training set! We’ll refer to this function in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297).
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们运行此代码，我们就可以导航到 Unity Catalog 并看到`product_difference_ratio_on_demand_feature`被列出。它已准备好在训练集中使用！我们将在[*第6章*](B16865_06.xhtml#_idTextAnchor297)中引用此函数。
- en: '![Figure 5.38 – Viewing the on-demand feature we created in Unity Catalog](img/B16865_05_38.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图5.38 – 在 Unity Catalog 中查看我们创建的按需特征](img/B16865_05_38.jpg)'
- en: Figure 5.38 – Viewing the on-demand feature we created in Unity Catalog
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.38 – 查看 Unity Catalog 中我们创建的按需特征](img/B16865_05_38.jpg)'
- en: And with that, we have engineered features for our *Streaming Transactions*
    dataset. We enriched the original data with a streaming feature called `transactionCount`
    and an on-demand feature function that we will use to build the training dataset
    in the next chapter.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经为我们的*流式交易数据集*构建了特征。我们通过一个名为`transactionCount`的流式特征和一个按需特征函数丰富了原始数据，我们将使用这些特征在下一章构建训练数据集。
- en: In the next section, we will aggregate store sales data and save it to a feature
    table.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将汇总商店销售数据并将其保存到特征表中。
- en: Project – Favorita Store Sales – time series forecasting
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – Favorita 商店销售 – 时间序列预测
- en: 'In[*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)*,* we used AutoML to explore
    the *Favorita Sales* dataset and create a baseline model predicting sales. To
    follow along in your own workspace, please refer to the following notebook: `CH5-01-Building
    –Favorita –``Feature Tables`.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中，我们使用AutoML探索了*Favorita 销售数据集*并创建了一个预测销售的基线模型。要在您自己的工作空间中跟随，请参考以下笔记本：`CH5-01-Building
    –Favorita –``Feature Tables`。
- en: 'To create a Databricks feature table, we can use either Python (via `FeatureEngineeringClient`)
    or SQL. This chapter primarily uses Python, but we start by creating a `stores_ft`
    feature table using SQL:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 Databricks 特征表，我们可以使用 Python（通过 `FeatureEngineeringClient`）或 SQL。本章主要使用
    Python，但我们首先使用 SQL 创建一个 `stores_ft` 特征表：
- en: '![Figure 5.39 – Creating the stores_ft feature table from the favorita_stores
    table using SQL](img/B16865_05_39.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.39 – 使用 SQL 从 favorita_stores 表创建 stores_ft 特征表](img/B16865_05_39.jpg)'
- en: Figure 5.39 – Creating the stores_ft feature table from the favorita_stores
    table using SQL
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.39 – 使用 SQL 从 favorita_stores 表创建 stores_ft 特征表
- en: Executing the code in *Figure 5**.39* creates a feature table called `stores_ft`
    that we will use as a central repository for important store details. `store_nbr`
    is set to `NOT NULL` because it is the primary key. The table does not contain
    a date column, so this feature table is not a time series feature table. If it
    did, we could include an additional `TIMESERIES` primary key. Note that when adding
    the primary key to the table, you can name the constraint for the primary key
    with a unique name, as shown in the documentation. We prefer to let the DFE client
    name the constraint automatically. You can use `DESCRIBE TABLE EXTENDED` to see
    the name of your primary key constraint.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 *图 5.39* 中的代码将创建一个名为 `stores_ft` 的特征表，我们将将其用作重要店铺细节的中心存储库。`store_nbr` 被设置为
    `NOT NULL`，因为它是主键。该表不包含日期列，因此这个特征表不是一个时间序列特征表。如果它包含日期列，我们可以包括一个额外的 `TIMESERIES`
    主键。请注意，当将主键添加到表中时，您可以根据文档中的说明为约束命名一个唯一名称。我们更喜欢让 DFE 客户端自动命名约束。您可以使用 `DESCRIBE
    TABLE EXTENDED` 来查看您的主键约束名称。
- en: The remaining two feature tables are time series tables. After some transformations,
    we will create the feature tables using Python and the DFE client. Let’s focus
    on the holidays first.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的两个特征表是时间序列表。经过一些转换后，我们将使用 Python 和 DFE 客户端创建特征表。让我们首先关注节假日。
- en: '![Figure 5.40 – The holiday events table](img/B16865_05_40.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.40 – 节假日事件表](img/B16865_05_40.jpg)'
- en: Figure 5.40 – The holiday events table
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.40 – 节假日事件表
- en: The features we create are their respective store’s local, regional, and national
    holidays. We can look at *Figure 5**.40* to recall what the holiday data looked
    like.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的特征包括各自店铺的本地、区域和国家节假日。我们可以查看 *图 5.40* 来回忆节假日数据的样子。
- en: One thing to note about our data is that there are days when multiple holidays
    occur for the same store. We process the three types of holidays very similarly.
    The transformations for national holidays are slightly different, given that there
    is no need to match on locale. All holiday transformations are present in the
    GitHub code. However, we do not cover each in detail in the book.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们的数据，有一点需要注意，那就是有几天对于同一店铺会发生多个节假日。我们非常相似地处理三种类型的节假日。由于不需要在区域上进行匹配，国家节假日的转换略有不同。所有节假日转换都在
    GitHub 代码中。然而，在书中我们没有详细说明每个转换。
- en: 'The following steps take you through the transformations for the local holiday
    type we use for the holiday feature table:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将引导您完成用于节假日特征表的本地节假日类型的转换：
- en: 'We begin the ETL for the local holidays by singling out the local locale and
    renaming the type to `holiday_type`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过单独识别本地区域并将类型重命名为 `holiday_type` 来开始本地节假日的 ETL：
- en: '![Figure 5.41 – Isolating the local holidays](img/B16865_05_41.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.41 – 隔离本地节假日](img/B16865_05_41.jpg)'
- en: Figure 5.41 – Isolating the local holidays
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.41 – 隔离本地节假日
- en: 'Using the bronze table created in *Step 1*, we construct a silver table consisting
    of the date, store number, and local holiday type. We must account for the issue
    of multiple local holidays happening on the same day for the same stores. We do
    so by grouping with a `MIN` function to select a holiday type. The local holiday
    type is changed to `Multiple` using a case statement and `num_holidays` accounts
    for these instances:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在 *步骤 1* 中创建的青铜表，我们构建一个银色表，包含日期、店铺编号和本地节假日类型。我们必须考虑到同一店铺在同一天发生多个本地节假日的问题。我们通过使用
    `MIN` 函数进行分组来选择节假日类型。使用情况语句将本地节假日类型更改为 `Multiple`，而 `num_holidays` 负责这些实例：
- en: '![Figure 5.42 – SQL checking for multiple holiday instances and identifying
    them with a new holiday type for the local holidays](img/B16865_05_42.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.42 – SQL 检查多个节假日实例，并为本地节假日识别新的节假日类型](img/B16865_05_42.jpg)'
- en: Figure 5.42 – SQL checking for multiple holiday instances and identifying them
    with a new holiday type for the local holidays
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.42 – SQL 检查多个节假日实例，并为本地节假日识别新的节假日类型
- en: 'The resulting silver table is shown in *Figure 5**.43*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 结果银色表如图 *图 5.43* 所示：
- en: '![Figure 5.43 – The first five rows of the local_holidays_silver table](img/B16865_05_43.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.43 – local_holidays_silver 表的前五行](img/B16865_05_43.jpg)'
- en: Figure 5.43 – The first five rows of the local_holidays_silver table
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.43 – local_holidays_silver 表的前五行
- en: 'After completing the same process in the previous steps for regional and national
    holidays, we combine the silver tables into a single DataFrame. We use full joins
    to include all holidays. To avoid `null` dates and store numbers, we chain two
    `ifnull()` functions:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前一步骤中完成区域和国家假日的相同过程后，我们将银色表格合并成一个 DataFrame。我们使用完全连接来包含所有假日。为了避免 `null` 日期并存储数字，我们链式使用两个
    `ifnull()` 函数：
- en: '![Figure 5.44 – Combining all three silver tables into a single DataFrame](img/B16865_05_44.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.44 – 将三个银色表格合并成一个 DataFrame](img/B16865_05_44.jpg)'
- en: Figure 5.44 – Combining all three silver tables into a single DataFrame
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.44 – 将所有三个银色表格合并成一个 DataFrame
- en: Now that we have prepared our DataFrame with the features we want to save, we
    use the DFE `create_table` method to save the DataFrame as a feature table. We
    specify `primary_keys`. Be sure to note that we are *not* including a time series
    column. This is because we want the feature table lookup only to match exact dates.
    Holidays would not function well with the point-in-time logic. The primary keys
    will also be our lookup keys when we create the training set in the next chapter.
    A best practice is to include a thoughtful description as well.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了我们想要保存的特征 DataFrame，我们使用 DFE 的 `create_table` 方法将 DataFrame 保存为特征表。我们指定
    `primary_keys`。请注意，我们**不**包括时间序列列。这是因为我们希望特征表查找仅匹配确切的日期。假日与点时间逻辑结合使用效果不佳。在下一章创建训练集时，主键也将是我们的查找键。一个最佳实践是包括一个深思熟虑的描述。
- en: '![Figure 5.45 – Creating the store holidays feature table using the DFE client](img/B16865_05_45.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.45 – 使用 DFE 客户端创建商店假日特征表](img/B16865_05_45.jpg)'
- en: Figure 5.45 – Creating the store holidays feature table using the DFE client
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.45 – 使用 DFE 客户端创建商店假日特征表
- en: Notice that in our feature table, *Figure 5**.46*, there are `null` values when
    there is not a holiday of a specific holiday of such type. When we create the
    training set, the feature lookup functionality will have `null` for the values
    of dates not in the table. We do not need to create `null` rows for each date
    that is not a holiday, thus saving us the preprocessing time.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在我们的特征表 *图 5.46* 中，当没有特定类型的假日时，存在 `null` 值。当我们创建训练集时，特征查找功能将为不在表中的日期的值返回
    `null`。我们不需要为每个非假日日期创建 `null` 行，从而节省预处理时间。
- en: '![Figure 5.46 – The store_holidays_ft feature table](img/B16865_05_46.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.46 – store_holidays_ft 特征表](img/B16865_05_46.jpg)'
- en: Figure 5.46 – The store_holidays_ft feature table
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.46 – store_holidays_ft 特征表
- en: 'In addition to the holiday and store data, we were provided with oil prices
    data. We can use this data as a proxy for the economy. Let’s create one more feature
    table using the `oil_price_silver` table. Unlike `store_holidays_ft`, having the
    previous value of a stock price in the place of `null` would be helpful. This
    is a fitting example of when to use the point-in-time lookup functionality. To
    do so, a second primary key is required. Therefore, we include the date as the
    primary key but not as `timeseries_column`, as shown in *Figure 5**.47*:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了假日和商店数据外，我们还提供了石油价格数据。我们可以使用这些数据作为经济的代理。让我们使用 `oil_price_silver` 表创建一个额外的特征表。与
    `store_holidays_ft` 不同，在 `null` 的位置拥有股票价格的先前值将是有帮助的。这是一个使用点时间查找功能的合适示例。为此，需要一个第二个主键。因此，我们将日期作为主键，但不作为
    `timeseries_column`，如图 *图 5.47* 所示：
- en: '![Figure 5.47 – Creating a feature table from the oil prices](img/B16865_05_47.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.47 – 从石油价格创建特征表](img/B16865_05_47.jpg)'
- en: Figure 5.47 – Creating a feature table from the oil prices
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.47 – 从石油价格创建特征表
- en: We have multiple feature tables that we can use with the specific training data
    provided on Kaggle or any other related training data. For example, `oil_10d_lag_ft`
    can be used as a proxy for the economy for any dataset based in Ecuador.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多个特征表，我们可以使用 Kaggle 或任何其他相关训练数据提供的特定训练数据。例如，`oil_10d_lag_ft` 可以用作基于厄瓜多尔的任何数据集的经济代理。
- en: For future modeling, saving the features in feature tables using the DFE client
    will be helpful. Doing so makes it seamless to look up the features with the model
    at inference time. In the next chapter, using the DFE client, we will combine
    our feature tables to create a training set for our model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了未来的建模，使用 DFE 客户端将特征保存到特征表中将是有帮助的。这样做使得在推理时间查找特征与模型无缝对接。在下一章中，我们将使用 DFE 客户端将我们的特征表组合起来，为我们的模型创建训练集。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: As we conclude [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), we have successfully
    navigated the multifaceted realm of feature engineering on Databricks. We have
    learned how to organize our features into feature tables, in both SQL and Python,
    by ensuring there is a non-nullable primary key. Unity Catalog provides lineage
    and discoverability, which makes features reusable. Continuing with the streaming
    project, we also highlighted creating a streaming feature using stateful streaming.
    We touched on the latest feature engineering products from Databricks, such as
    point-in-time lookups, on-demand feature functions, and publishing tables to the
    Databricks Online Store. These product features will reduce time to production
    and simplify production pipelines.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们总结 [*第5章*](B16865_05.xhtml#_idTextAnchor244) 时，我们已经成功穿越了 Databricks 上特征工程的多元领域。我们学习了如何通过确保有一个非空主键，将我们的特征组织成特征表，无论是使用
    SQL 还是 Python。Unity Catalog 提供了血缘和可发现性，这使得特征可重用。在继续进行流式项目时，我们还强调了使用有状态流创建流式特征。我们提到了
    Databricks 最新的特征工程产品，如点时间查找、按需特征函数以及将表发布到 Databricks 在线商店。这些产品功能将缩短生产时间并简化生产流程。
- en: You are now ready to tackle feature engineering for a variety of scenarios!
    Next up, we take what we’ve learned to build training sets and machine learning
    models in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以应对各种场景的特征工程了！接下来，我们将所学知识用于在 [*第6章*](B16865_06.xhtml#_idTextAnchor297) 中构建训练集和机器学习模型。
- en: Questions
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题有助于巩固关键点并使内容与你的经验联系起来：
- en: What element of the Delta format helps with reproducibility?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Delta 格式的哪个元素有助于可重复性？
- en: What are some of the reasons why you would choose to publish a feature to an
    online store?
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会选择将特征发布到在线商店的一些原因是什么？
- en: How would you create a training set using the Feature Engineering API?
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何使用特征工程 API 创建训练集？
- en: What distinguishes a feature table from any other table in Unity Catalog?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 中，特征表与任何其他表有什么区别？
- en: In the *Streaming Transactions dataset* section of *Applying our learning*,
    we created a stream with a transformation. What could be the business drivers
    for creating this pipeline? We did the *how*; what is the *why*?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *应用我们的学习* 的 *流式事务数据集* 部分，我们创建了一个经过转换的流。创建此管道的业务驱动因素可能是什么？我们完成了 *如何做*；那么 *为什么*
    呢？
- en: Answers
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考了这些问题之后，比较你的答案和我们的答案：
- en: Delta’s ability to time travel helps with reproducibility. Delta has versioning
    that allows us a point-in-time lookup to see what data our model was trained on.
    For long-term versioning, deep clones or snapshots are appropriate.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Delta 的时间旅行能力有助于可重复性。Delta 具有版本控制，允许我们进行点时间查找，查看我们的模型训练了哪些数据。对于长期版本控制，深度克隆或快照是合适的。
- en: Writing data to an online store provides real-time feature lookup for real-time
    inference models.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据写入在线商店为实时推理模型提供实时特征查找。
- en: Create `FeatureLookups` for each feature table you wish to include. Then, use
    `create_training_set`.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为你希望包含的每个特征表创建 `FeatureLookups`。然后，使用 `create_training_set`。
- en: A feature table has a unique primary key, which indicates the object or entity
    the features describe.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征表有一个唯一的主键，它表示特征所描述的对象或实体。
- en: The possibilities are vast. An example is behavior modeling or customer segmentation
    to support flagging fraud.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能性是无限的。一个例子是行为建模或客户细分以支持欺诈标记。
- en: Further reading
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In this chapter, we identified specific technologies, technical features, and
    options. Please take a look at these resources to get deeper into the areas that
    interest you most:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们确定了特定的技术、技术特性和选项。请查看这些资源，深入了解你最感兴趣的领域：
- en: '*How Databricks AI improves model accuracy with real-time* *computations*:[https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations](https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks AI 如何通过实时* *计算* 提高模型精度：[https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations](https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations)'
- en: '*Use time series feature tables with point-in-time* *support*: [https://docs.databricks.com/en/machine-learning/feature-store/time-series.html](https://docs.databricks.com/en/machine-learning/feature-store/time-series.html)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 *带时间点支持的* 时间序列特征表：[https://docs.databricks.com/en/machine-learning/feature-store/time-series.html](https://docs.databricks.com/en/machine-learning/feature-store/time-series.html)
- en: '*Python Arbitrary Stateful Processing in Structured Streaming –* Databricks
    blog: [https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html](https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化流中的 Python 任意有状态处理 –* Databricks 博客：[https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html](https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html)'
- en: '*Databricks* *Volumes*: [https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html](https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks 卷*：[https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html](https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html)'
- en: '*Experimenting with Databricks* *Volumes*: [https://medium.com/@tsiciliani/experimenting-with-databricks-volumes-5666cecb166](https://medium.com/@tsiciliani/experimenting-with-databricks-volumes-5666cecb166)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 Databricks 卷中进行实验*：[https://medium.com/@tsiciliani/experimenting-with-databricks-volumes-5666cecb166](https://medium.com/@tsiciliani/experimenting-with-databricks-volumes-5666cecb166)'
- en: '*Optimize stateful Structured Streaming* *queries*: [https://docs.databricks.com/en/structured-streaming/stateful-streaming.html](https://docs.databricks.com/en/structured-streaming/stateful-streaming.html)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化有状态结构化流* *查询*：[https://docs.databricks.com/en/structured-streaming/stateful-streaming.html](https://docs.databricks.com/en/structured-streaming/stateful-streaming.html)'
- en: 'Databricks blog – *Introducing Delta Time Travel for Large Scale Data* *Lakes*:
    [https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html](https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 博客 – *介绍适用于大规模数据湖的 Delta 时间旅行*：[https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html](https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)
- en: YouTube video – *Arbitrary Stateful Aggregations in Structured Streaming in
    Apache Spark by Burak* *Yavuz*:[https://youtu.be/JAb4FIheP28?si=BjoeKkxP_OUxT7-K](https://youtu.be/JAb4FIheP28?si=BjoeKkxP_OUxT7-K)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YouTube 视频 – 由 Burak *Yavuz* 演讲的 *Apache Spark 中的结构化流任意有状态聚合*：[https://youtu.be/JAb4FIheP28?si=BjoeKkxP_OUxT7-K](https://youtu.be/JAb4FIheP28?si=BjoeKkxP_OUxT7-K)
- en: 'Databricks documentation – *Compute features on demand using Python user-defined*
    *functions*: [https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html](https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 文档 – 使用 Python 用户定义的 *函数* 在需要时 *计算功能*：[https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html](https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html)
- en: '*Delta Lake change data feed (**CDF)*: [https://docs.databricks.com/en/delta/delta-change-data-feed.html](https://docs.databricks.com/en/delta/delta-change-data-feed.html)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta Lake 变化数据馈送（**CDF**）*：[https://docs.databricks.com/en/delta/delta-change-data-feed.html](https://docs.databricks.com/en/delta/delta-change-data-feed.html)'
