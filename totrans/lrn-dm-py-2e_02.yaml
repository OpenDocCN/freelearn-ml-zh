- en: Classifying with scikit-learn Estimators
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn估计器进行分类
- en: The scikit-learn library is a collection of data mining algorithms, written
    in Python and using a. This library allows users to easily try different algorithms
    as well as utilize standard tools for doing effective testing and parameter searching.
    There are many algorithms and utilities in scikit-learn, including many of the
    commonly used algorithms in modern machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库是一组数据挖掘算法的集合，用Python编写并使用。这个库允许用户轻松尝试不同的算法，以及利用标准工具进行有效的测试和参数搜索。scikit-learn中包含许多算法和实用工具，包括现代机器学习中常用的许多算法。
- en: In this chapter, we focus on setting up a good framework for running data mining
    procedures. We will use this framework in later chapters, which focus on applications
    and techniques to use in those situations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于设置一个良好的框架来运行数据挖掘过程。我们将在后续章节中使用这个框架，这些章节将专注于应用和那些情况下使用的技术。
- en: 'The key concepts introduced in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的关键概念如下：
- en: '**Estimators:** This is to perform classification, clustering, and regression'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计器**: 这是为了执行分类、聚类和回归'
- en: '**Transformers**: This is to perform pre-processing and data alterations'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**: 这是为了执行预处理和数据修改'
- en: '**Pipelines**: This is to put together your workflow into a replicable format'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**: 这是为了将您的流程组合成一个可重复的格式'
- en: scikit-learn estimators
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn估计器
- en: '**Estimators** that allows for the standardized implementation and testing
    of algorithms a common, lightweight interface for classifiers to follow. By using
    this interface, we can apply these tools to arbitrary classifiers, without needing
    to worry about how the algorithms work.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**估计器**允许算法的标准化实现和测试，为分类器提供一个通用、轻量级的接口。通过使用此接口，我们可以将这些工具应用于任意分类器，而无需担心算法的工作方式。'
- en: 'Estimators must have the following two important functions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器必须具有以下两个重要功能：
- en: '`fit()`: This function performs the training of the algorithm - setting the
    values of internal parameters. The `fit()` takes two inputs, the training sample
    dataset and the corresponding classes for those samples.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit()`: 此函数执行算法的训练 - 设置内部参数的值。`fit()`函数接受两个输入，即训练样本数据集和对应于这些样本的类别。'
- en: '`predict()`: This the class of the testing samples that we provide as the only
    input. This function returns a `NumPy` array with the predictions of each input
    testing sample.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict()`: 这是测试样本的类别，我们将其作为唯一输入提供。此函数返回一个包含每个输入测试样本预测的`NumPy`数组。'
- en: Most scikit-learn estimators use `NumPy` arrays or a related format for input
    and output. However this is by convention and not required to use the interface.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数scikit-learn估计器使用`NumPy`数组或相关格式作为输入和输出。然而，这仅是一种惯例，并非必须使用该接口。
- en: There are many estimators implemented in scikit-learn and more in other open
    source projects that use the same interface. These (SVM), random forests. We will
    use many
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中实现了许多估计器，在其他使用相同接口的开源项目中还有更多。我们将使用许多（SVM）、随机森林。我们将使用许多
- en: of these algorithms in later chapters. In this chapter, we will use the nearest
    neighbor
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中介绍这些算法。在本章中，我们将使用最近邻算法。
- en: algorithm.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 算法。
- en: 'For this chapter, you will need to install a new library called `matplotlib`.
    The easiest way to install it is to use `pip3`, as you did in [Chapter 1](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml),
    *Getting Started with Data Mining*, to install scikit-learn:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要安装一个名为`matplotlib`的新库。最简单的方法是使用`pip3`，就像在[第1章](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml)“数据挖掘入门”中安装scikit-learn一样：
- en: '`**$pip3 install matplotlib**`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`**pip3 install matplotlib**`'
- en: 'If you have `matplotlib`, seek the official installation instructions at:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有`matplotlib`，请查找官方安装说明：
- en: '[http://matplotlib.org/users/installing.html](http://matplotlib.org/users/installing.html%22http://matplotlib.org/users/installing.html)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[matplotlib安装指南](http://matplotlib.org/users/installing.html)'
- en: Nearest neighbors
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近邻
- en: The **Nearest neighbors** algorithm is our new sample. We take the most similar
    samples
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**最近邻**算法是我们新的样本。我们选取最相似的样本'
- en: and predict the same class that most of these nearby samples have. This vote
    is often simply a simple count,although more complicated methods do exist such
    as weighted voting.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 并预测这些附近样本中大多数样本的相同类别。这种投票通常只是一个简单的计数，尽管也存在更复杂的方法，如加权投票。
- en: 'As an example in the below diagram, we wish to predict the class of the triangle,
    based on which class it is more like (represented here by having similar objects
    closer together). We seek the three nearest neighbors, which are the two diamonds
    and one square within the drawn circle. There are more diamonds than circles,
    and the predicted class for the triangle is, therefore, a diamond:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图为例，我们希望根据三角形更接近哪个类别（在此处表示为相似对象更靠近）来预测三角形的类别。我们寻找最近的三个邻居，即画圈内的两个钻石和一个正方形。钻石比圆多，因此预测的三角形类别是钻石：
- en: '![](img/image_02_001-1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_001-1.jpg)'
- en: Nearest neighbors are used for nearly any dataset - however, it can be computationally
    expensive to compute the distance between all pairs of samples. For example, if
    there are ten samples in the dataset, there are 45 unique distances to compute.
    However, if there are 1000 samples, there are nearly 500,000! Various methods
    exist for improving this speed, such as the use of tree structures for distance
    computation. Some of these algorithms can be quite complex, but thankfully a version
    is implemented in scikit-learn already, enabling us to classify on larger datasets.
    As these tree structures are the default in scikit-learn, we do not need to configure
    anything to use it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻算法几乎适用于任何数据集——然而，计算所有样本对之间的距离可能计算成本很高。例如，如果数据集中有十个样本，则需要计算45个独特的距离。然而，如果有1000个样本，则几乎有500,000个！存在各种方法来提高这种速度，例如使用树结构进行距离计算。其中一些算法可能相当复杂，但幸运的是，scikit-learn已经实现了这些算法的版本，使我们能够在更大的数据集上进行分类。由于这些树结构是scikit-learn的默认设置，我们不需要进行任何配置即可使用它。
- en: Nearest neighbors can do poorly in **categorical-based datasets**, with categorical
    features, and another algorithm should be used for these instead. Nearest Neighbor's
    issue is due to the difficulty in comparing differences in categorical values,
    something better left to an algorithm that gives weight to each feature's importance.
    Comparing categorical features can be done with some distance metrics or pre-processing
    steps such as one hot encoding that we use in later chapters. Choosing the correct
    algorithm for the task is one of the difficult issues in data mining, often it
    can be easiest to test a set of algorithms and see which performs best on your
    task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于类别的数据集、具有类别特征的情况下，最近邻算法表现不佳，应使用其他算法代替。最近邻算法的问题在于比较类别值差异的困难，这最好留给一个考虑每个特征重要性的算法。可以使用一些距离度量或预处理步骤（如我们在后续章节中使用的独热编码）来比较类别特征。选择正确的算法是数据挖掘中的难题之一，通常，测试一组算法并查看哪个在你的任务上表现最好是最简单的方法。
- en: Distance metrics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离度量
- en: A key underlying concept in data mining is that of **distance**. If we have
    two samples, we need to answer questions such as *are these two samples more similar
    than the other two? *Answering questions like these is important to the outcome
    of the data mining exercise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘中的一个基本概念是**距离**。如果我们有两个样本，我们需要回答诸如*这两个样本是否比另外两个样本更相似？*等问题。回答这些问题对于数据挖掘的结果非常重要。
- en: The most common use is **Euclidean** distance, which is the *real-world* distance
    between two objects. If you were to plot the points on a graph and measure the
    distance with a ruler, the result would be the Euclidean distance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的距离是**欧几里得**距离，它是两个对象之间的*实际世界*距离。如果你要在图上绘制点并使用尺子测量距离，结果将是欧几里得距离。
- en: A little more formally, the Euclidean distances between points a and b is the
    square root of the sum of the squared distances for each feature.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式一点，点a和点b之间的欧几里得距离是每个特征平方距离之和的平方根。
- en: Euclidean distance is intuitive but provides poor accuracy if some features
    have larger values than a value of 0, known as a sparse matrix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离直观易懂，但如果某些特征值大于0（称为稀疏矩阵），则准确性较差。
- en: There are other distance metrics in use; two commonly employed ones are the
    Manhattan and Cosine distance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用中的其他距离度量还有曼哈顿距离和余弦距离。
- en: The **Manhattan** distance is the sum of the absolute differences in each feature
    (with no use of square distances).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**曼哈顿**距离是每个特征绝对差异之和（不使用平方距离）。'
- en: Intuitively we can imagine Manhattan distance of as the number of moves a Rook
    piece
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们可以将曼哈顿距离想象成车象棋子移动的步数。
- en: (also called a Castle) in if it were limited to moving one square at a time.
    While the Manhattan distance does suffer if some features have larger values than
    others, the effect is not as dramatic as in the case of Euclidean points if it
    were limited to moving one square at a time. While the Manhattan distance does
    suffer if some features have larger values than others, the effect is not as dramatic
    as in the case of Euclidean.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （也称为城堡）在如果它被限制为每次移动一个方格的情况下。虽然当一些特征值大于其他特征时，曼哈顿距离确实会受到影响，但如果它被限制为每次移动一个方格，其影响不如欧几里得点那样显著。当一些特征值大于其他特征时，曼哈顿距离确实会受到影响，但其影响不如欧几里得距离那样剧烈。
- en: The **Cosine** distance is better suited to cases where some features are larger
    than others and when there are lots of zeros in the dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**余弦**距离更适合某些特征值大于其他特征，并且数据集中有很多零的情况。'
- en: 'Intuitively, we draw a line from the origin to each of the samples and measure
    the angle between those lines. We can observe the differences between the algorithms
    in the following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们从原点到每个样本画一条线，并测量这些线之间的角度。我们可以在以下图中观察到算法之间的差异：
- en: '![](img/image_02_002-1.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_002-1.jpg)'
- en: In this example, each of the gray circles is exactly the same distance from
    the white circle. In (a), the distances are Euclidean, and therefore, similar
    distances fit around a circle. This distance can be measured using a ruler. In
    (b), the distances are Manhattan, also called City Block. We compute the distance
    by moving across rows and columns, like how a Rook (Castle) in Chess moves. Finally,
    in (c), we have the Cosine distance that is measured by computing the angle between
    the lines drawn from the sample to the vector and ignore the actual length of
    the line.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，每个灰色圆圈与白色圆圈的距离完全相同。在（a）中，距离是欧几里得距离，因此，相似的距离适合围绕一个圆。这个距离可以用尺子来测量。在（b）中，距离是曼哈顿距离，也称为城市街区距离。我们通过跨越行和列来计算距离，就像国际象棋中的车（城堡）移动一样。最后，在（c）中，我们有余弦距离，它是通过计算从样本到向量的线之间的角度来测量的，并忽略线的实际长度。
- en: The distance metric chosen can have a large impact on the final performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所选的距离度量可以极大地影响最终性能。
- en: For example, if you have many features, the Euclidean distance between random
    samples converges (due to the famous *curse of dimensionality*). Euclidean distances
    in high dimension have a hard time comparing samples, as the distances are always
    nearly the same!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有很多特征，随机样本之间的欧几里得距离会收敛（由于著名的*维度诅咒*）。在高维空间中，欧几里得距离很难比较样本，因为距离总是几乎相同！
- en: Manhattan distance can be more stable in these circumstances, but if some features
    have very large values, this can *overrule* lots similarity in other features.
    For example, if feature A has values between 1 and 2, and another feature B has
    values between 1000 and 2000, in such a case feature A is unlikely to have any
    impact on the result. This problem can be addressed through normalization, which
    makes Manhattan (and Euclidean) distance more reliable with different features,
    which we will see later in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，曼哈顿距离可能更稳定，但如果某些特征值非常大，这可能会*掩盖*其他特征中的许多相似性。例如，如果特征A的值在1到2之间，而另一个特征B的值在1000到2000之间，在这种情况下，特征A不太可能对结果有任何影响。这个问题可以通过归一化来解决，这使得曼哈顿（和欧几里得）距离在不同特征上更加可靠，我们将在本章后面看到。
- en: Finally, Cosine distance is a good metric for comparing items with many features,
    but it discards some information about the length of the vector, which is useful
    in some applications. We would often use Cosine distance in text mining due to
    the large number of features inherent in text mining (see [Chapter 6](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml),
    *Social Media Insight Using Naive Bayes*).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，余弦距离是比较具有许多特征的项目的良好度量，但它丢弃了关于向量长度的某些信息，这在某些应用中是有用的。我们通常会在文本挖掘中使用余弦距离，因为文本挖掘固有的特征数量很大（见[第6章](ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml)，*使用朴素贝叶斯进行社交媒体洞察*)。
- en: Ultimately, either a theoretical approach is needed to determine which distance
    method is needed, or an empirical evaluation is needed to see which performed
    more effectively. I prefer the empirical approach, but either approach can yield
    good results.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，需要一种理论方法来确定哪种距离方法需要，或者需要一种经验评估来查看哪种方法更有效。我更喜欢经验方法，但任何一种方法都可以产生良好的结果。
- en: For this chapter, we will stay with Euclidean distance, using other metrics
    in later chapters. If you'd like to experiment, then try setting the metric to
    Manhattan and see how that affects the results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用欧几里得距离，在后面的章节中使用其他度量标准。如果您想进行实验，请尝试将度量标准设置为曼哈顿距离，看看这对结果有何影响。
- en: Loading the dataset
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: The dataset `Ionosphere`, which high-frequency antennas. The aim of the antennas
    is to determine whether there is a structure in the ionosphere and a region in
    the upper atmosphere. We consider readings with a structure to be good, while
    those that do not have structure are deemed bad. The aim of this application is
    to build a data mining classifier that can determine whether an image is good
    or bad.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 `Ionosphere` 与高频天线相关。天线的目的是确定电离层中是否存在结构以及上层大气中的区域。我们将具有结构的读取视为良好，而没有结构的读取则被视为不良。本应用的目的是构建一个数据挖掘分类器，以确定图像是良好还是不良。
- en: '![](img/image_02_003.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_003.png)'
- en: '(Image Credit: https://www.flickr.com/photos/geckzilla/16149273389/)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：https://www.flickr.com/photos/geckzilla/16149273389/）
- en: You can download this dataset  for different data mining applications. Go to
    [http://archive.ics.uci.edu/ml/datasets/Ionosphere](http://archive.ics.uci.edu/ml/datasets/Ionosphere) 
    and click on Data Folder. Download the `ionosphere.data` and `ionosphere.names`
    files to a folder on your computer. For this example, I'll assume that you have
    put the dataset in a directory called `Data` in your `home` folder. You can place
    the data in another folder, just be sure to update your data folder (here, and
    in all other chapters).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以下载此数据集用于不同的数据挖掘应用。访问 [http://archive.ics.uci.edu/ml/datasets/Ionosphere](http://archive.ics.uci.edu/ml/datasets/Ionosphere)
    并点击数据文件夹。将 `ionosphere.data` 和 `ionosphere.names` 文件下载到您的计算机上的一个文件夹中。对于本例，我将假设您已将数据集放在主文件夹中名为
    `Data` 的目录下。您可以将数据放在另一个文件夹中，只需确保更新您的数据文件夹（此处以及所有其他章节）。
- en: 'The location of your home folder depends on your operating system. For Windows,
    it is usually at `C:Documents` and `Settingsusername`. For Mac or Linux machines,
    it is usually at `/home/username`. You can get your home folder by running this
    python code inside a Jupyter Notebook:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您的主文件夹位置取决于您的操作系统。对于 Windows，它通常位于 `C:Documents` 和 `Settingsusername`。对于 Mac
    或 Linux 计算机，它通常位于 `/home/username`。您可以通过在 Jupyter Notebook 中运行以下 Python 代码来获取您的家文件夹：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For each row in the dataset, there are 35 values. The first 34 are measurements
    taken from the 17 antennas (two values for each antenna). The last is either 'g'
    or 'b'; that stands for good and bad, respectively.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的每一行，都有 35 个值。前 34 个是从 17 个天线（每个天线两个值）测量的。最后一个值是 'g' 或 'b'；分别代表良好和不良。
- en: Start the Jupyter Notebook server and create a new notebook called Ionosphere
    Nearest Neighbors. To start with, we load up the `NumPy` and `csv` libraries that
    we will need for our code, and set the data's filename that we will need for our
    code.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Jupyter Notebook 服务器并创建一个名为 Ionosphere Nearest Neighbors 的新笔记本。首先，我们加载所需的
    `NumPy` 和 `csv` 库，并设置我们代码中需要的数据文件名。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then create the `X` and `y` `NumPy` arrays to store the dataset in. The
    sizes of these arrays are known from the dataset. Don''t worry if you don''t know
    the size of future datasets - we will use other methods to load the dataset in
    future chapters and you won''t need to know this size beforehand:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建 `X` 和 `y` 的 `NumPy` 数组来存储数据集。这些数组的大小来自数据集。如果您不知道未来数据集的大小，请不要担心——我们将在未来的章节中使用其他方法来加载数据集，您不需要事先知道这个大小：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The dataset is in a **Comma-Separated Values** (**CSV**) format, which is a
    commonly used format for datasets. We are going to use the `csv` module to load
    this file. Import it and set up a `csv` reader object, then loop through the file,
    setting the appropriate row in `X` and class value in `y` for every line in our
    dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是 **逗号分隔值**（**CSV**）格式，这是数据集常用的格式。我们将使用 `csv` 模块来加载此文件。导入它并设置一个 `csv` 读取对象，然后遍历文件，为数据集中的每一行设置
    `X` 中的适当行和 `y` 中的类别值：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now have a dataset of samples and features in `X` as well as the corresponding
    classes in `y`, as we did in the classification example in [Chapter 1](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml),
    Getting Started with Data Mining.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个包含在 `X` 中的样本和特征数据集以及相应的 `y` 类别，正如我们在第 1 章[入门数据挖掘](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml)中的分类示例中所做的那样。
- en: To begin with, try applying the OneR algorithm from [Chapter 1](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml),
    *Getting Started with Data Mining* to this dataset. It won't work very well, as
    the information in this dataset is spread out within the correlations of certain
    features. OneR is only interested in the values of a single feature and cannot
    pick up information in more complex datasets very well. Other algorithms, including
    Nearest Neighbor, merge information from multiple features, making them applicable
    in more scenarios. The downside is that they are often more computationally expensive
    to compute.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尝试将[第1章](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml)中介绍的OneR算法应用于这个数据集。它不会很好用，因为这个数据集中的信息分布在某些特征的关联中。OneR只对单个特征的值感兴趣，并且不能很好地捕捉更复杂数据集中的信息。其他算法，包括最近邻算法，合并多个特征的信息，使它们适用于更多场景。缺点是它们通常计算起来更昂贵。
- en: Moving towards a standard workflow
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向标准工作流程迈进
- en: Estimators scikit-learn have two and `predict()`. We train the algorithm using
    the
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的估计器有两个：`fit()`和`predict()`。我们使用`fit()`方法来训练算法
- en: '`predict()` method on our testing set. We evaluate it using the `predict()`
    method on our testing set.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试集上使用`predict()`方法。我们使用测试集上的`predict()`方法来评估它。
- en: 'First, we need to create these training and testing sets. As before, import
    and run the `train_test_split` function:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要创建这些训练集和测试集。像以前一样，导入并运行`train_test_split`函数：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we import the `nearest neighbor` class and create an instance for it.
    We leave the parameters as defaults for now and will test other values later in
    this chapter. By default, the algorithm will choose the five nearest neighbors
    to predict the class of a testing sample:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们导入`nearest neighbor`类并为其创建一个实例。现在我们将参数保留为默认值，将在本章后面测试其他值。默认情况下，算法将选择预测测试样本类别的五个最近邻：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After creating our `estimator`, we must then fit it on our training dataset.
    For the `nearest neighbor` class, this training step simply records our dataset,
    allowing us to find the nearest neighbor for a new data point, by comparing that
    point to the training dataset:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建我们的`estimator`之后，我们必须将其拟合到我们的训练数据集上。对于`nearest neighbor`类，这个训练步骤只是记录我们的数据集，使我们能够通过将新数据点与训练数据集进行比较来找到最近邻：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then train the algorithm with our test set and evaluate with our testing
    set:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后使用测试集训练算法，并使用测试集进行评估：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This model scores 86.4 percent accuracy, which is impressive for a default algorithm
    and just a few lines of code! Most scikit-learn default parameters are chosen
    deliberately to work well with a range of datasets. However, you should always
    aim to choose parameters based on knowledge of the application experiment. We
    will use strategies for doing this **parameter search** in later chapters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型准确率达到86.4%，对于一个默认算法来说非常出色，而且只需几行代码！大多数scikit-learn默认参数都是经过精心选择的，以便与各种数据集良好地工作。然而，你应该始终根据对应用实验的了解来选择参数。我们将在后面的章节中使用策略来进行这种**参数搜索**。
- en: Running the algorithm
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行算法
- en: The previous results are quite good, based on our testing set of data, based
    on the testing set. However, what happens if we get lucky and choose an easy testing
    set? Alternatively, what if it was particularly troublesome? We can discard a
    good model due to poor results resulting from such an *unlucky* split of our data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的测试数据集，之前的结果相当不错。然而，如果我们运气好，选择了一个容易的测试集会怎样？或者，如果它特别麻烦呢？我们可能会因为数据分割的不幸而丢弃一个好的模型。
- en: 'The **cross-fold validation** framework is a way to address the problem of
    choosing a single testing set and is a standard *best-practice *methodology in
    data mining. The process works by doing many experiments with different training
    and testing splits, but using each sample in a testing set only once. The procedure
    is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**框架是一种解决选择单个测试集问题的方法，并且在数据挖掘中是一种标准的*最佳实践*方法论。这个过程通过进行许多实验，使用不同的训练和测试分割，但每个测试集中的每个样本只使用一次。程序如下：'
- en: Split the entire dataset into several sections called folds.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将整个数据集分成几个称为折的部分。
- en: 'For each fold in the data, execute the following steps:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据集中的每个折，执行以下步骤：
- en: Set that fold aside as the current testing set
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该折放在一边作为当前测试集
- en: Train the algorithm on the remaining folds
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在剩余的折上训练算法
- en: Evaluate on the current testing set
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在当前测试集上评估
- en: Report on all the evaluation scores, including the average score.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告所有评估分数，包括平均分数。
- en: In this process, each sample is used in the testing set only once, reducing
    (but not eliminating) the likelihood of choosing lucky testing sets.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，每个样本只用于测试集一次，这减少了（但并未消除）选择幸运测试集的可能性。
- en: Throughout this book, the code examples build upon each other within a chapter.
    Each chapter's code should be entered into the same Jupyter Notebook unless otherwise
    specified in-text.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，代码示例在每一章内相互构建。除非文本中另有说明，否则每个章节的代码应输入到同一个 Jupyter Notebook 中。
- en: 'The scikit-learn library contains a few cross-fold validation methods. A `helper`
    function is given that performs the preceding procedure. We can import it now
    in our Jupyter Notebook:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 库包含几种交叉验证方法。提供了一个执行先前过程的 `helper` 函数。我们现在可以在我们的 Jupyter Notebook
    中导入它：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By `cross_val_score` uses a specific methodology called **Stratified K-Fold**
    to create folds that have approximately the same proportion of classes in each
    fold, again reducing the likelihood of choosing poor folds. Stratified K-Fold
    is a great default -we won't mess with it right now.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `cross_val_score` 使用一种称为 **Stratified K-Fold** 的特定方法来创建每个折叠中类比例大致相同的折叠，再次减少选择较差折叠的可能性。Stratified
    K-Fold 是一个很好的默认选项——我们现在不会去修改它。
- en: 'Next, we use this new function to evaluate our model using cross-fold validation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用这个新函数通过交叉验证评估我们的模型：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our new code returns a slightly more modest result of 82.3 percent, but it is
    still quite good considering we have not yet tried setting better parameters.
    In the next section, we will see how we would go about changing the parameters
    to achieve a better outcome.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新代码返回了一个稍微谦虚的结果，82.3%，但考虑到我们还没有尝试设置更好的参数，这仍然相当不错。在下一节中，我们将看到如何改变参数以实现更好的结果。
- en: It is quite natural for variation in results when performing data mining, and
    attempting to repeat experiments. This is due to variations in how the folds are
    created and randomness inherent in some classification algorithms. We can deliberately
    choose to replicate an experiment exactly by setting the random state (which we
    will do in later chapters). In practice, it's a good idea to rerun experiments
    multiple times to get a sense of the average result and the spread of the results
    (the mean and standard deviation) across all experiments.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据挖掘和尝试重复实验时，结果的变化是很自然的。这是由于折叠创建方式的不同以及某些分类算法中固有的随机性。我们可以故意选择通过设置随机状态（我们将在后面的章节中这样做）来精确复制一个实验。在实践中，多次重新运行实验以获得平均结果和所有实验结果（平均值和标准差）的分布（范围）是一个好主意。
- en: Setting parameters
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置参数
- en: Almost all parameters that the user can set, letting algorithms focus more on
    the specific dataset, rather than only being applicable across a small and specific
    range of problems. Setting these parameters can be quite difficult, as choosing
    good parameter values is often highly reliant on features of the dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有用户可以设置的参数，让算法更多地关注特定的数据集，而不是只适用于一小部分特定的问题。设置这些参数可能相当困难，因为选择好的参数值通常高度依赖于数据集的特征。
- en: 'The nearest neighbor algorithm has several parameters, but the most important
    one is that of the number of nearest neighbors to use when predicting the class
    of an unseen attribution. In `-learn`, this parameter is called `n_neighbors`.
    In the following figure, we show that when this number is too low, a randomly
    labeled sample can cause an error. In contrast, when it is too high, the actual
    nearest neighbors have a lower effect on the result:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻算法有几个参数，但最重要的一个是在预测未见属性类别时使用的最近邻数量。在 `-learn` 中，这个参数称为 `n_neighbors`。在下面的图中，我们展示了当这个数字太低时，随机标记的样本可能会引起错误。相反，当它太高时，实际最近邻对结果的影响会降低：
- en: '![](img/image_02_004-1.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_004-1.jpg)'
- en: In figure (a), on the left-hand side, we would usually expect to classify the
    test sample (the triangle) as a circle. However, if `n_neighbors` is 1, the single
    red diamond in this area (likely a noisy sample) causes the sample to be predicted
    as a diamond. In figure (b), on the right-hand side, we would usually expect to
    classify the test sample as a diamond. However, if `n_neighbors` is 7, the three
    nearest neighbors (which are all diamonds) are overridden by a large number of
    circle samples. Nearest neighbors a difficult problem to solve, as the parameter
    can make a huge difference. Luckily, most of the time the specific parameter value
    does not greatly affect the end result, and the standard values (usually 5 or
    10) are often *near enough*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图(a)的左侧，我们通常会期望将测试样本（三角形）分类为圆形。然而，如果`n_neighbors`为1，这个区域中唯一的红色菱形（可能是噪声样本）会导致样本被预测为菱形。在图(b)的右侧，我们通常会期望将测试样本分类为菱形。但是，如果`n_neighbors`为7，三个最近的邻居（它们都是菱形）被大量圆形样本所覆盖。最近邻是一个难以解决的问题，因为参数可以产生巨大的差异。幸运的是，大多数时候，具体的参数值不会对最终结果产生很大影响，标准值（通常是5或10）通常*足够接近*。
- en: With that in mind, we can test out a range of values, and investigate the impact
    that this parameter has on performance. If we want to test a number of values
    for the `n_neighbors` parameter, for example, each of the values from 1 to 20,
    we can rerun the experiment many times by setting `n_neighbors` and observing
    the result. The code below does this, storing the values in the `avg_scores` and
    `all_scores` variables.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们可以测试一系列的值，并调查这个参数对性能的影响。如果我们想测试`n_neighbors`参数的多个值，例如，从1到20的每个值，我们可以通过设置`n_neighbors`并观察结果来多次重新运行实验。下面的代码就是这样做的，将值存储在`avg_scores`和`all_scores`变量中。
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can then plot the relationship between the value of `n_neighbors` and the
    accuracy. First, we tell the Jupyter Notebook that we want to show plots `inline`
    in the notebook itself:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制`n_neighbors`值与准确率之间的关系图。首先，我们告诉Jupyter Notebook我们希望在笔记本本身中显示`inline`图：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then import `pyplot` from the `matplotlib` library and plot the parameter
    values alongside average scores:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从`matplotlib`库中导入`pyplot`并绘制参数值和平均分数：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/image_02_005.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_005.png)'
- en: While there is a lot of variance, the plot shows a decreasing trend as the number
    of neighbors increases. With regard to the variance, you can expect large amounts
    of variance whenever you do evaluations of this nature. To compensate, update
    the code to run 100 tests, per value of `n_neighbors`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有很大的变异性，但随着邻居数量的增加，图表显示了一个下降趋势。关于变异性，你可以预期在进行此类评估时会有大量的变异性。为了补偿，更新代码以运行100次测试，每次测试`n_neighbors`的每个值。
- en: Preprocessing
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: 'When taking measurements of real-world objects, we can often get features in
    different ranges. For instance, if we measure the qualities of an animal, we might
    have several features, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在对现实世界对象进行测量时，我们通常可以得到不同范围的特征。例如，如果我们测量动物的特性，我们可能会有几个特征，如下所示：
- en: '**Number of legs**: This is between the range of 0-8 for most animals, while
    some have more! more! more!'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**腿的数量**：对于大多数动物来说，这个范围在0-8之间，而有些动物更多！更多！更多！'
- en: '**Weight**: This is between the ranges of only a few micrograms, all the way
    to a blue whale with a weight of 190,000 kilograms!'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重量**：这个范围只在几毫克到一只重达190,000千克的蓝鲸之间！'
- en: '**Number of hearts**: This can be between zero to five, in the case of the
    earthworm.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心脏的数量**：对于蚯蚓来说，这个范围在零到五之间。'
- en: For a mathematical-based algorithm to compare each of these features, the differences
    in the scale, range, and units can be difficult to interpret. If we used the above
    features in many algorithms, the weight would probably be the most influential
    feature due to only the larger numbers and not anything to do with the actual
    effectiveness of the feature.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于数学的算法来比较这些特征，尺度、范围和单位之间的差异可能难以解释。如果我们使用上述特征在许多算法中，权重可能是最有影响力的特征，因为只有较大的数字，而与特征的真正有效性无关。
- en: One of the possible strategies *normalizes* the features so that they all have
    the same range, or the values are turned into categories like *small*, *medium*
    and *large*. Suddenly, the large differences in the types of features have less
    of an impact on the algorithm and can lead to large increases in the accuracy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的策略*标准化*特征，使它们都具有相同的范围，或者将值转换为如*小*、*中*和*大*这样的类别。突然之间，特征类型之间的巨大差异对算法的影响减小，可以导致准确率的大幅提高。
- en: Pre-processing can also be used to choose only the more effective features,
    create new features, and so on. Pre-processing in scikit-learn is done through
    `Transformer` objects, which take a dataset in one form and return an altered
    dataset after some transformation of the data. These don't have to be numerical,
    as Transformers are also used to extract features-however, in this section, we
    will stick with pre-processing.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理还可以用来选择更有效的特征，创建新特征等。scikit-learn中的预处理是通过`Transformer`对象完成的，这些对象接受一种形式的数据集，并在数据的一些转换后返回修改后的数据集。这些不必是数值型的，因为转换器也用于提取特征。然而，在本节中，我们将坚持使用预处理。
- en: We can show an example of the problem by *breaking* the `Ionosphere` dataset.
    While this is only an example, many real-world datasets have problems of this
    form.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过*破坏*`Ionosphere`数据集来展示这个问题的一个例子。虽然这只是一个例子，但许多现实世界的数据集都存在这种形式的问题。
- en: 'First, we create a copy of the array so that we do not alter the original dataset:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建数组的副本，以确保我们不改变原始数据集：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we *break* the dataset by dividing every second feature by `10`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过将每个第二个特征除以`10`来*破坏*数据集：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In theory, this should not have a great effect on the result. After all, the
    values of these features are still relatively the same. The major issue is that
    the scale has changed and the odd features are now *larger* than the even features.
    We can see the effect of this by computing the accuracy:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这不应该对结果有太大影响。毕竟，这些特征的值仍然相对相同。主要问题是尺度发生了变化，奇数特征现在比偶数特征*更大*。我们可以通过计算准确率来看到这种影响：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This testing methodology gives a score of 82.3 percent for the original dataset,
    which drops down to 71.5 percent on the broken dataset. We can fix this by scaling
    all the features to the range `0` to `1`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这种测试方法给原始数据集评分为82.3%，在破坏的数据集上降至71.5%。我们可以通过将所有特征缩放到`0`到`1`的范围来解决这个问题。
- en: Standard pre-processing
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准预处理
- en: 'The pre-processing we will perform for this experiment is called feature-based
    normalization, which we perform using scikit-learn''s `MinMaxScaler` class. Continuing
    with the Jupyter Notebook from the rest of this chapter, first, we import this
    class:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这次实验执行的预处理称为基于特征的归一化，我们使用scikit-learn的`MinMaxScaler`类来完成。继续使用本章其余部分的Jupyter
    Notebook，首先，我们导入这个类：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This class takes each feature and scales it to the range `0` to `1`. This pre-processor
    replaces the minimum value with `0`, the maximum with `1`, and the other values
    somewhere in between based on a linear mapping.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类将每个特征缩放到`0`到`1`的范围。这个预处理程序将最小值替换为`0`，最大值替换为`1`，其他值根据线性映射位于两者之间。
- en: 'To apply our pre-processor, we run the `transform` function on it. Transformers
    often need to be trained first, in the same way that the classifiers do. We can
    combine these steps by running the `fit_transform` function instead:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用我们的预处理程序，我们在其上运行`transform`函数。转换器通常需要先进行训练，就像分类器一样。我们可以通过运行`fit_transform`函数来合并这些步骤：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, `X_transformed` will have the same shape as `*X*`. However, each column
    will have a maximum of `1` and a minimum of `0`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`X_transformed`将与`*X*`具有相同的形状。然而，每一列的最大值将是`1`，最小值是`0`。
- en: 'There are various other forms of normalizing in this way, which is effective
    for other applications and feature types:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式进行归一化有各种其他形式，这对于其他应用和特征类型是有效的：
- en: Ensure the sum of the values for each sample equals to 1, using `sklearn.preprocessing.Normalizer`
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sklearn.preprocessing.Normalizer`确保每个样本的值之和等于1
- en: Force each feature to have a zero mean and a variance of 1, using `sklearn.preprocessing.StandardScaler`,
    which is a commonly used starting point for normalization
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sklearn.preprocessing.StandardScaler`将每个特征强制转换为具有零均值和方差为1，这是归一化的常用起点
- en: Turn numerical features into binary features, where any value above a threshold
    is 1 and any below is 0, using `sklearn.preprocessing.Binarizer`
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sklearn.preprocessing.Binarizer`将数值特征转换为二元特征，其中高于阈值的值为1，低于阈值的值为0
- en: We will use combinations of these pre-processors in later chapters, along with
    other types of `Transformers` object.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续章节中使用这些预处理器的组合，以及其他类型的`Transformers`对象。
- en: Pre-processing is a critical step in the data mining pipeline and one that can
    mean the difference between a bad and great result.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理是数据挖掘流程中的关键步骤，它可能意味着结果的好坏之分。
- en: Putting it all together
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: 'We can now create a workflow by combining the code from the previous sections,
    using the broken dataset previously calculated:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过结合前几节中的代码，使用之前计算出的损坏数据集来创建一个工作流程：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We now recover our original score of 82.3 percent accuracy. The `MinMaxScaler`
    resulted in features of the same scale, meaning that no features overpowered others
    by simply being bigger values. While the Nearest Neighbor algorithm can be confused
    with larger features, some algorithms handle scale differences better. In contrast,
    some are much worse!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们恢复了原来的准确率82.3%，`MinMaxScaler`导致特征具有相同的尺度，这意味着没有特征仅仅因为值更大而压倒其他特征。虽然最近邻算法可能会被较大的特征所迷惑，但某些算法更好地处理尺度差异。相比之下，有些算法则差得多！
- en: Pipelines
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: As experiments grow, so does the complexity of the operations. We may split
    up our dataset, binarize features, perform feature-based scaling, perform sample-based
    scaling, and many more operations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 随着实验的增多，操作的复杂性也在增加。我们可能需要分割我们的数据集，二值化特征，执行基于特征的缩放，执行基于样本的缩放，以及许多其他操作。
- en: Keeping track of these operations can get quite confusing and can result in
    being unable to replicate the result. Problems include forgetting a step, incorrectly
    applying a transformation, or adding a transformation that wasn't needed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪这些操作可能会变得相当混乱，并可能导致无法复制结果。问题包括忘记一个步骤，错误地应用转换，或者添加不必要的转换。
- en: Another issue is the order of the code. In the previous section, we created
    our `X_transformed` dataset and then created a new estimator for the cross validation.If
    we had multiple steps, we would need to track these changes to the dataset in
    code.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是对代码的顺序。在前一节中，我们创建了我们的`X_transformed`数据集，然后为交叉验证创建了一个新的估计器。如果我们有多个步骤，我们就需要在代码中跟踪这些对数据集的更改。
- en: 'Pipelines are a construct that addresses these problems (and others, which
    we will see in the next chapter). Pipelines store the steps in your data mining
    workflow. They can take your raw data in, perform all the necessary transformations,
    and then create a prediction. This allows us to use pipelines in functions such
    as `cross_val_score`, where they expect an estimator. First, import the `Pipeline`
    object:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 管道（Pipelines）是一种解决这些问题的结构（以及我们将在下一章中看到的其他问题）。管道存储数据挖掘工作流程中的步骤。它们可以接收原始数据，执行所有必要的转换，然后创建预测。这使得我们可以在`cross_val_score`等函数中使用管道，这些函数期望一个估计器。首先，导入`Pipeline`对象：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Pipelines take a list of steps as input, representing the chain of the data
    mining application. The last step needs to be an Estimator, while all previous
    steps are Transformers. The input dataset is altered by each Transformer, with
    the output of one step being the input of the next step. Finally, we classify
    the samples by the last step''s estimator. In our pipeline, we have two steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 管道接受一个步骤列表作为输入，表示数据挖掘应用的链。最后一个步骤需要是一个估计器，而所有之前的步骤都是转换器。输入数据集被每个转换器所改变，一个步骤的输出成为下一个步骤的输入。最后，我们通过最后一个步骤的估计器对样本进行分类。在我们的管道中，我们有两个步骤：
- en: Use `MinMaxScaler` to scale the feature values from 0 to 1
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MinMaxScaler`将特征值缩放到0到1
- en: Use `KNeighborsClassifier` as the classification algorithms
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KNeighborsClassifier`作为分类算法
- en: 'We then represent each step using a tuple `(''name'',` `step)`. We can then
    create our pipeline:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后使用元组`('name', `step)`来表示每个步骤。然后我们可以创建我们的管道：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The key here is the list of tuples. The first tuple is our scaling step and
    the second tuple is the predicting step. We give each step a name: the first we
    call `scale` and the second we call `predict`, but you can choose your own names.
    The second part of the tuple is the actual `Transformer` or `estimator` object.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于元组的列表。第一个元组是我们的缩放步骤，第二个元组是预测步骤。我们给每个步骤起一个名字：第一个我们称之为`scale`，第二个我们称之为`predict`，但你可以选择自己的名字。元组的第二部分是实际的`Transformer`或`estimator`对象。
- en: 'Running this pipeline is now very easy, using the cross-validation code from
    before:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行这个管道非常简单，使用之前交叉验证的代码：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This gives us the same score as before (82.3 percent), which is expected, as
    we are running exactly the same steps, just with an improved interface.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了与之前相同的分数（82.3%），这是预期的，因为我们正在运行完全相同的步骤，只是界面有所改进。
- en: In later chapters, we will use more advanced testing methods and setting up
    pipelines is a great way to ensure that the code complexity does not grow unmanageably.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将使用更高级的测试方法，设置管道是确保代码复杂度不会无序增长的好方法。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we used several of scikit-learn's methods for building a standard
    workflow to run and evaluate data mining models. We introduced the Nearest Neighbors
    algorithm, which is implemented in scikit-learn as an estimator. Using this class
    is quite easy; first, we call the `fit` function on our training data, and second,
    we use the `predict` function to predict the class of testing samples.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了scikit-learn的几种方法来构建一个标准的流程来运行和评估数据挖掘模型。我们介绍了最近邻算法，该算法在scikit-learn中作为估计器实现。使用这个类相当简单；首先，我们在训练数据上调用`fit`函数，然后使用`predict`函数来预测测试样本的类别。
- en: We then looked at pre-processing by fixing poor feature scaling. This was done
    using a `Transformer` object and the `MinMaxScaler` class. These functions also
    have a `fit` method and then a transform, which takes data of one form as an input
    and returns a transformed dataset as an output.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过修复不良的特征缩放来查看预处理。这是通过`Transformer`对象和`MinMaxScaler`类来完成的。这些函数也有一个`fit`方法，然后是转换，它接受一种形式的数据作为输入，并返回一个转换后的数据集作为输出。
- en: To investigate these transformations further, try swapping out the `MinMaxScaler`
    with some of the other mentioned transformers. Which is the most effective and
    why would this be the case?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步研究这些转换，尝试用其他提到的转换器替换`MinMaxScaler`。哪个最有效，为什么会是这样？
- en: Other transformers also exist in scikit-learn, which we will use later in this
    book, such as PCA. Try some of these out as well, referencing scikit-learn's excellent
    documentation at [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中还存在其他转换器，我们将在本书的后续部分使用，例如PCA。也尝试一些这些转换器，参考scikit-learn的优秀文档[https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)
- en: In the next chapter, we will use these concepts in a larger example, predicting
    the outcome of sports matches using real-world data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用这些概念在一个更大的例子中，使用现实世界的数据预测体育比赛的结果。
