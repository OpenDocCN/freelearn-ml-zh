["```py\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport lightgbm as lgb\nfrom path import Path\nfrom sklearn.model_selection import StratifiedKFold\nclass Config:\n    input_path = Path('../input/porto-seguro-safe-driver-prediction')\n    optuna_lgb = False\n    n_estimators = 1500\n    early_stopping_round = 150\n    cv_folds = 5\n    random_state = 0\n    params = {'objective': 'binary',\n              'boosting_type': 'gbdt',\n              'learning_rate': 0.01,\n              'max_bin': 25,\n              'num_leaves': 31,\n              'min_child_samples': 1500,\n              'colsample_bytree': 0.7,\n              'subsample_freq': 1,\n              'subsample': 0.7,\n              'reg_alpha': 1.0,\n              'reg_lambda': 1.0,\n              'verbosity': 0,\n              'random_state': 0}\n\nconfig = Config()\n```", "```py\ntrain = pd.read_csv(config.input_path / 'train.csv', index_col='id')\ntest = pd.read_csv(config.input_path / 'test.csv', index_col='id')\nsubmission = pd.read_csv(config.input_path / 'sample_submission.csv', index_col='id')\ncalc_features = [feat for feat in train.columns if \"_calc\" in feat]\ncat_features = [feat for feat in train.columns if \"_cat\" in feat]\n```", "```py\ntarget = train[\"target\"]\ntrain = train.drop(\"target\", axis=\"columns\")\n```", "```py\ntrain = train.drop(calc_features, axis=\"columns\")\ntest = test.drop(calc_features, axis=\"columns\")\n```", "```py\ntrain = pd.get_dummies(train, columns=cat_features)\ntest = pd.get_dummies(test, columns=cat_features)\nassert((train.columns==test.columns).all())\n```", "```py\nfrom numba import jit\n@jit\ndef eval_gini(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_pred)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\ndef gini_lgb(y_true, y_pred):\n    eval_name = 'normalized_gini_coef'\n    eval_result = eval_gini(y_true, y_pred)\n    is_higher_better = True\n    return eval_name, eval_result, is_higher_better\n```", "```py\nif config.optuna_lgb:\n\n    def objective(trial):\n        params = {\n    'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n    'num_leaves': trial.suggest_int(\"num_leaves\", 3, 255),\n    'min_child_samples': trial.suggest_int(\"min_child_samples\", \n                                           3, 3000),\n    'colsample_bytree': trial.suggest_float(\"colsample_bytree\", \n                                            0.1, 1.0),\n    'subsample_freq': trial.suggest_int(\"subsample_freq\", 0, 10),\n    'subsample': trial.suggest_float(\"subsample\", 0.1, 1.0),\n    'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-9, 10.0),\n    'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-9, 10.0),\n        }\n\n        score = list()\n        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, \n                              random_state=config.random_state)\n        for train_idx, valid_idx in skf.split(train, target):\n            X_train = train.iloc[train_idx]\n            y_train = target.iloc[train_idx]\n            X_valid = train.iloc[valid_idx] \n            y_valid = target.iloc[valid_idx]\n            model = lgb.LGBMClassifier(**params,\n                                    n_estimators=1500,\n                                    early_stopping_round=150,\n                                    force_row_wise=True)\n            callbacks=[lgb.early_stopping(stopping_rounds=150, \n                                          verbose=False)]\n            model.fit(X_train, y_train, \n                      eval_set=[(X_valid, y_valid)],  \n                      eval_metric=gini_lgb, callbacks=callbacks)\n\n            score.append(\n                model.best_score_['valid_0']['normalized_gini_coef'])\n        return np.mean(score)\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=300)\n    print(\"Best Gini Normalized Score\", study.best_value)\n    print(\"Best parameters\", study.best_params)\n\n    params = {'objective': 'binary',\n              'boosting_type': 'gbdt',\n              'verbosity': 0,\n              'random_state': 0}\n\n    params.update(study.best_params)\n\nelse:\n    params = config.params\n```", "```py\npreds = np.zeros(len(test))\noof = np.zeros(len(train))\nmetric_evaluations = list()\nskf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\nfor idx, (train_idx, valid_idx) in enumerate(skf.split(train, \n                                                       target)):\n    print(f\"CV fold {idx}\")\n    X_train, y_train = train.iloc[train_idx], target.iloc[train_idx]\n    X_valid, y_valid = train.iloc[valid_idx], target.iloc[valid_idx]\n\n    model = lgb.LGBMClassifier(**params,\n                               n_estimators=config.n_estimators,\n                    early_stopping_round=config.early_stopping_round,\n                               force_row_wise=True)\n\n    callbacks=[lgb.early_stopping(stopping_rounds=150), \n               lgb.log_evaluation(period=100, show_stdv=False)]\n\n    model.fit(X_train, y_train, \n              eval_set=[(X_valid, y_valid)], \n              eval_metric=gini_lgb, callbacks=callbacks)\n    metric_evaluations.append(\n                model.best_score_['valid_0']['normalized_gini_coef'])\n    preds += (model.predict_proba(test,  \n              num_iteration=model.best_iteration_)[:,1] \n              / skf.n_splits)\n    oof[valid_idx] = model.predict_proba(X_valid,  \n                    num_iteration=model.best_iteration_)[:,1]\n```", "```py\nprint(f\"LightGBM CV normalized Gini coefficient:  \n        {np.mean(metric_evaluations):0.3f}  \n        ({np.std(metric_evaluations):0.3f})\")\n```", "```py\nLightGBM CV Gini Normalized Score: 0.289 (0.015)\n```", "```py\nsubmission['target'] = preds\nsubmission.to_csv('lgb_submission.csv')\noofs = pd.DataFrame({'id':train_index, 'target':oof})\noofs.to_csv('dnn_oof.csv', index=False)\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom path import Path\nimport gc\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.special import erfinv\nimport tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation, LeakyReLU\nget_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})\n```", "```py\ndef gpu_cleanup(objects):\n    if objects:\n        del(objects)\n    K.clear_session()\n    gc.collect()\n```", "```py\nclass Config:\n    input_path = Path('../input/porto-seguro-safe-driver-prediction')\n    dae_batch_size = 128\n    dae_num_epoch = 50\n    dae_architecture = [1500, 1500, 1500]\n    reuse_autoencoder = False\n    batch_size = 128\n    num_epoch = 150\n    units = [64, 32]\n    input_dropout=0.06\n    dropout=0.08\n    regL2=0.09\n    activation='selu'\n\n    cv_folds = 5\n    nas = False\n    random_state = 0\n\nconfig = Config()\n```", "```py\ntrain = pd.read_csv(config.input_path / 'train.csv', index_col='id')\ntest = pd.read_csv(config.input_path / 'test.csv', index_col='id')\nsubmission = pd.read_csv(config.input_path / 'sample_submission.csv', index_col='id')\ncalc_features = [feat for feat in train.columns if \"_calc\" in feat]\ncat_features = [feat for feat in train.columns if \"_cat\" in feat]\ntarget = train[\"target\"]\ntrain = train.drop(\"target\", axis=\"columns\")\ntrain = train.drop(calc_features, axis=\"columns\")\ntest = test.drop(calc_features, axis=\"columns\")\ntrain = pd.get_dummies(train, columns=cat_features)\ntest = pd.get_dummies(test, columns=cat_features)\nassert((train.columns==test.columns).all())\n```", "```py\nprint(\"Applying GaussRank to columns: \", end='')\nto_normalize = list()\nfor k, col in enumerate(train.columns):\n    if '_bin' not in col and '_cat' not in col and '_missing' not in col:\n        to_normalize.append(col)\nprint(to_normalize)\ndef to_gauss(x): return np.sqrt(2) * erfinv(x) \ndef normalize(data, norm_cols):\n    n = data.shape[0]\n    for col in norm_cols:\n        sorted_idx = data[col].sort_values().index.tolist()\n        uniform = np.linspace(start=-0.99, stop=0.99, num=n)\n        normal = to_gauss(uniform)\n        normalized_col = pd.Series(index=sorted_idx, data=normal)\n        data[col] = normalized_col\n    return data\ntrain = normalize(train, to_normalize)\ntest = normalize(test, to_normalize)\n```", "```py\nApplying GaussRank to columns: ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15']\n```", "```py\nfeatures = train.columns\ntrain_index = train.index\ntest_index = test.index\ntrain = train.values.astype(np.float32)\ntest = test.values.astype(np.float32)\n```", "```py\ndef plot_keras_history(history, measures):\n    rows = len(measures) // 2 + len(measures) % 2\n    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, \n                        hspace=0.4, wspace=0.2)\n    try:\n        panels = [item for sublist in panels for item in sublist]\n    except:\n        pass\n    for k, measure in enumerate(measures):\n        panel = panels[k]\n        panel.set_title(measure + ' history')\n        panel.plot(history.epoch, history.history[measure],  \n                   label=\"Train \"+measure)\n        try:\n            panel.plot(history.epoch,  \n                       history.history[\"val_\"+measure], \n                       label=\"Validation \"+measure)\n        except:\n            pass\n        panel.set(xlabel='epochs', ylabel=measure)\n        panel.legend()\n\n    plt.show(fig)\nfrom numba import jit\n@jit\ndef eval_gini(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_pred)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n```", "```py\ndef batch_generator(x, batch_size, shuffle=True, random_state=None):\n    batch_index = 0\n    n = x.shape[0]\n    while True:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                np.random.seed(seed=random_state)\n                index_array = np.random.permutation(n)\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n        batch = x[index_array[current_index: current_index + current_batch_size]]\n        yield batch\n```", "```py\ndef mixup_generator(X, batch_size, swaprate=0.15, shuffle=True, random_state=None):\n    if random_state is None:\n        random_state = np.randint(0, 999)\n    num_features = X.shape[1]\n    num_swaps = int(num_features * swaprate)    \n    generator_a = batch_generator(X, batch_size, shuffle, \n                                  random_state)\n    generator_b = batch_generator(X, batch_size, shuffle, \n                                  random_state + 1)\n    while True:\n        batch = next(generator_a)\n        mixed_batch = batch.copy()\n        effective_batch_size = batch.shape[0]\n        alternative_batch = next(generator_b)\n        assert((batch != alternative_batch).any())\n        for i in range(effective_batch_size):\n            swap_idx = np.random.choice(num_features, num_swaps, \n                                        replace=False)\n            mixed_batch[i, swap_idx] = alternative_batch[i, swap_idx]\n        yield (mixed_batch, batch)\n```", "```py\ndef get_DAE(X, architecture=[1500, 1500, 1500]):\n    features = X.shape[1]\n    inputs = Input((features,))\n    for i, nodes in enumerate(architecture):\n        layer = Dense(nodes, activation='relu', \n                      use_bias=False, name=f\"code_{i+1}\")\n        if i==0:\n            x = layer(inputs)\n        else:\n            x = layer(x)\n        x = BatchNormalization()(x)\n    outputs = Dense(features, activation='linear')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='mse', \n                  metrics=['mse', 'mae'])\n    return model\n```", "```py\ndef extract_dae_features(autoencoder, X, layers=[3]):\n    data = []\n    for layer in layers:\n        if layer==0:\n            data.append(X)\n        else:\n            get_layer_output = Model([autoencoder.layers[0].input], \n                                  [autoencoder.layers[layer].output])\n            layer_output = get_layer_output.predict(X, \n                                                    batch_size=128)\n            data.append(layer_output)\n    data = np.hstack(data)\n    return data\n```", "```py\ndef autoencoder_fitting(X_train, X_valid, filename='dae',  \n                        random_state=None, suppress_output=False):\n    if suppress_output:\n        verbose = 0\n    else:\n        verbose = 2\n        print(\"Fitting a denoising autoencoder\")\n    tf.random.set_seed(seed=random_state)\n    generator = mixup_generator(X_train, \n                                batch_size=config.dae_batch_size, \n                                swaprate=0.15, \n                                random_state=config.random_state)\n\n    dae = get_DAE(X_train, architecture=config.dae_architecture)\n    steps_per_epoch = np.ceil(X_train.shape[0] / \n                              config.dae_batch_size)\n    early_stopping = EarlyStopping(monitor='val_mse', \n                                mode='min', \n                                patience=5, \n                                restore_best_weights=True,\n                                verbose=0)\n    history = dae.fit(generator,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=config.dae_num_epoch,\n                    validation_data=(X_valid, X_valid),\n                    callbacks=[early_stopping],\n                    verbose=verbose)\n    if not suppress_output: plot_keras_history(history, \n                                           measures=['mse', 'mae'])\n    dae.save(filename)\n    return dae\n```", "```py\ndef dense_blocks(x, units, activation, regL2, dropout):\n    kernel_initializer = keras.initializers.RandomNormal(mean=0.0, \n                                stddev=0.1, seed=config.random_state)\n    for k, layer_units in enumerate(units):\n        if regL2 > 0:\n            x = Dense(layer_units, activation=activation, \n                      kernel_initializer=kernel_initializer, \n                      kernel_regularizer=l2(regL2))(x)\n        else:\n            x = Dense(layer_units, \n                      kernel_initializer=kernel_initializer, \n                      activation=activation)(x)\n        if dropout > 0:\n            x = Dropout(dropout)(x)\n    return x\n```", "```py\ndef dnn_model(dae, units=[4500, 1000, 1000], \n            input_dropout=0.1, dropout=0.5,\n            regL2=0.05,\n            activation='relu'):\n\n    inputs = dae.get_layer(\"code_2\").output\n    if input_dropout > 0:\n        x = Dropout(input_dropout)(inputs)\n    else:\n        x = tf.keras.layers.Layer()(inputs)\n    x = dense_blocks(x, units, activation, regL2, dropout)\n    outputs = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=dae.input, outputs=outputs)\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n                loss=keras.losses.binary_crossentropy,\n                metrics=[AUC(name='auc')])\n    return model\n```", "```py\ndef model_fitting(X_train, y_train, X_valid, y_valid, autoencoder, \n                 filename, random_state=None, suppress_output=False):\n        if suppress_output:\n            verbose = 0\n        else:\n            verbose = 2\n            print(\"Fitting model\")\n        early_stopping = EarlyStopping(monitor='val_auc', \n                                    mode='max', \n                                    patience=10, \n                                    restore_best_weights=True,\n                                    verbose=0)\n        rlrop = ReduceLROnPlateau(monitor='val_auc', \n                                mode='max',\n                                patience=2,\n                                factor=0.75,\n                                verbose=0)\n\n        tf.random.set_seed(seed=random_state)\n        model = dnn_model(autoencoder,\n                    units=config.units,\n                    input_dropout=config.input_dropout,\n                    dropout=config.dropout,\n                    regL2=config.regL2,\n                    activation=config.activation)\n\n        history = model.fit(X_train, y_train, \n                            epochs=config.num_epoch, \n                            batch_size=config.batch_size, \n                            validation_data=(X_valid, y_valid),\n                            callbacks=[early_stopping, rlrop],\n                            shuffle=True,\n                            verbose=verbose)\n        model.save(filename)\n\n        if not suppress_output:  \n            plot_keras_history(history, measures=['loss', 'auc'])\n        return model, history\n```", "```py\nif config.nas is True:\n    def evaluate():\n        metric_evaluations = list()\n        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n        for k, (train_idx, valid_idx) in enumerate(skf.split(train, target)):\n\n            X_train, y_train = train[train_idx, :], target[train_idx]\n            X_valid, y_valid = train[valid_idx, :], target[valid_idx]\n            if config.reuse_autoencoder:\n                autoencoder = load_model(f\"./dae_fold_{k}\")\n            else:\n                autoencoder = autoencoder_fitting(X_train, X_valid,\n                                                filename=f'./dae_fold_{k}', \n                                                random_state=config.random_state,\n                                                suppress_output=True)\n\n            model, _ = model_fitting(X_train, y_train, X_valid, y_valid,\n                                        autoencoder=autoencoder,\n                                        filename=f\"dnn_model_fold_{k}\", \n                                        random_state=config.random_state,\n                                        suppress_output=True)\n\n            val_preds = model.predict(X_valid, batch_size=128, verbose=0)\n            best_score = eval_gini(y_true=y_valid, y_pred=np.ravel(val_preds))\n            metric_evaluations.append(best_score)\n\n            gpu_cleanup([autoencoder, model])\n\n        return np.mean(metric_evaluations)\n    def objective(trial):\n        params = {\n                'first_layer': trial.suggest_categorical(\"first_layer\", [8, 16, 32, 64, 128, 256, 512]),\n                'second_layer': trial.suggest_categorical(\"second_layer\", [0, 8, 16, 32, 64, 128, 256]),\n                'third_layer': trial.suggest_categorical(\"third_layer\", [0, 8, 16, 32, 64, 128, 256]),\n                'input_dropout': trial.suggest_float(\"input_dropout\", 0.0, 0.5),\n                'dropout': trial.suggest_float(\"dropout\", 0.0, 0.5),\n                'regL2': trial.suggest_uniform(\"regL2\", 0.0, 0.1),\n                'activation': trial.suggest_categorical(\"activation\", ['relu', 'leaky-relu', 'selu'])\n        }\n        config.units = [nodes for nodes in [params['first_layer'], params['second_layer'], params['third_layer']] if nodes > 0]\n        config.input_dropout = params['input_dropout']\n        config.dropout = params['dropout']\n        config.regL2 = params['regL2']\n        config.activation = params['activation']\n\n        return evaluate()\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=60)\n    print(\"Best Gini Normalized Score\", study.best_value)\n    print(\"Best parameters\", study.best_params)\n    config.units = [nodes for nodes in [study.best_params['first_layer'], study.best_params['second_layer'], study.best_params['third_layer']] if nodes > 0]\n    config.input_dropout = study.best_params['input_dropout']\n    config.dropout = study.best_params['dropout']\n    config.regL2 = study.best_params['regL2']\n    config.activation = study.best_params['activation']\n```", "```py\npreds = np.zeros(len(test))\noof = np.zeros(len(train))\nmetric_evaluations = list()\nskf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\nfor k, (train_idx, valid_idx) in enumerate(skf.split(train, target)):\n    print(f\"CV fold {k}\")\n\n    X_train, y_train = train[train_idx, :], target[train_idx]\n    X_valid, y_valid = train[valid_idx, :], target[valid_idx]\n    if config.reuse_autoencoder:\n        print(\"restoring previously trained dae\")\n        autoencoder = load_model(f\"./dae_fold_{k}\")\n    else:\n        autoencoder = autoencoder_fitting(X_train, X_valid,\n                                        filename=f'./dae_fold_{k}', \n                                        random_state=config.random_state)\n\n    model, history = model_fitting(X_train, y_train, X_valid, y_valid,\n                                autoencoder=autoencoder,\n                                filename=f\"dnn_model_fold_{k}\", \n                                random_state=config.random_state)\n\n    val_preds = model.predict(X_valid, batch_size=128)\n    best_score = eval_gini(y_true=y_valid, \n                           y_pred=np.ravel(val_preds))\n    best_epoch = np.argmax(history.history['val_auc']) + 1\n    print(f\"[best epoch is {best_epoch}]\\tvalidation_0-gini_dnn: {best_score:0.5f}\\n\")\n\n    metric_evaluations.append(best_score)\n    preds += (model.predict(test, batch_size=128).ravel() / \n              skf.n_splits)\n    oof[valid_idx] = model.predict(X_valid, batch_size=128).ravel()\n    gpu_cleanup([autoencoder, model])\n```", "```py\nprint(f\"DNN CV normalized Gini coefficient: {np.mean(metric_evaluations):0.3f} ({np.std(metric_evaluations):0.3f})\")\n```", "```py\nDNN CV Gini Normalized Score: 0.276 (0.015)\n```", "```py\nsubmission['target'] = preds\nsubmission.to_csv('dnn_submission.csv')\noofs = pd.DataFrame({'id':train_index, 'target':oof})\noofs.to_csv('dnn_oof.csv', index=False)\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom numba import jit\n@jit\ndef eval_gini(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_pred)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\nlgb_oof = pd.read_csv(\"../input/workbook-lgb/lgb_oof.csv\")\ndnn_oof = pd.read_csv(\"../input/workbook-dae/dnn_oof.csv\")\ntarget = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/train.csv\", usecols=['id','target']) \n```", "```py\nlgb_oof_ranks = (lgb_oof.target.rank() / len(lgb_oof))\ndnn_oof_ranks = (dnn_oof.target.rank() / len(dnn_oof))\n```", "```py\nbaseline = eval_gini(y_true=target.target, y_pred=lgb_oof_ranks)\nprint(f\"starting from a oof lgb baseline {baseline:0.5f}\\n\")\nbest_alpha = 1.0\nfor alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n    ensemble = alpha * lgb_oof_ranks + (1.0 - alpha) * dnn_oof_ranks\n    score = eval_gini(y_true=target.target, y_pred=ensemble)\n    print(f\"lgd={alpha:0.1f} dnn={(1.0 - alpha):0.1f} -> {score:0.5f}\")\n\n    if score > baseline:\n        baseline = score\n        best_alpha = alpha\n\nprint(f\"\\nBest alpha is {best_alpha:0.1f}\")\n```", "```py\nstarting from a oof lgb baseline 0.28850\nlgd=0.1 dnn=0.9 -> 0.27352\nlgd=0.2 dnn=0.8 -> 0.27744\nlgd=0.3 dnn=0.7 -> 0.28084\nlgd=0.4 dnn=0.6 -> 0.28368\nlgd=0.5 dnn=0.5 -> 0.28595\nlgd=0.6 dnn=0.4 -> 0.28763\nlgd=0.7 dnn=0.3 -> 0.28873\nlgd=0.8 dnn=0.2 -> 0.28923\nlgd=0.9 dnn=0.1 -> 0.28916\nBest alpha is 0.8\n```", "```py\nlgb_submission = pd.read_csv(\"../input/workbook-lgb/lgb_submission.csv\")\ndnn_submission = pd.read_csv(\"../input/workbook-dae/dnn_submission.csv\")\nsubmission = pd.read_csv(\n\"../input/porto-seguro-safe-driver-prediction/sample_submission.csv\")\n```", "```py\nlgb_ranks = (lgb_submission.target.rank() / len(lgb_submission))\ndnn_ranks = (dnn_submission.target.rank() / len(dnn_submission))\nsubmission.target = lgb_ranks * 0.5 + dnn_ranks * 0.5\nsubmission.to_csv(\"equal_blend_rank.csv\", index=False)\n```", "```py\nlgb_ranks = (lgb_submission.target.rank() / len(lgb_submission))\ndnn_ranks = (dnn_submission.target.rank() / len(dnn_submission))\nsubmission.target = lgb_ranks * best_alpha +  dnn_ranks * (1.0 - best_alpha)\nsubmission.to_csv(\"blend_rank.csv\", index=False)\n```"]