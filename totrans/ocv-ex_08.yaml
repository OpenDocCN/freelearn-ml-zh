- en: Chapter 8. Video Surveillance, Background Modeling, and Morphological Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to detect a moving object in a video that
    is taken from a static camera. This is used extensively in video surveillance
    systems. We will discuss the different characteristics that can be used to build
    this system. We will learn about background modeling and see how we can use it
    to build a model of the background in a live video. Once we do this, we will combine
    all the blocks to detect the objects of interest in the video.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is naive background subtraction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is frame differencing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a background model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to identify a new object in a static video?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is morphological image processing and how is it related to background modeling?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to achieve different effects using morphological operators?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Background subtraction is very useful in video surveillance. Basically, the
    background subtraction technique performs really well in cases where we need to
    detect moving objects in a static scene. Now, how is this useful for video surveillance?
    The process of video surveillance involves dealing with a constant data flow.
    The data stream keeps coming in at all times, and we need to analyze it to identify
    any suspicious activities. Let's consider the example of a hotel lobby. All the
    walls and furniture have a fixed location. Now, if we build a background model,
    we can use it to identify suspicious activities in the lobby. We can take advantage
    of the fact that the background scene remains static (which happens to be true
    in this case). This helps us avoid any unnecessary computation overheads.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, this algorithm works by detecting the background and
    assigning each pixel of an image to two classes: either the background (assuming
    that it''s static and stable) or the foreground. It then subtracts the background
    from the current frame to obtain the foreground. By the static assumption, foreground
    objects will naturally correspond to objects or people moving in front of the
    background.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to detect moving objects, we first need to build a model of the background.
    This is not the same as direct frame differencing because we are actually modeling
    the background and using this model to detect moving objects. When we say that
    we are *modeling the background*, we are basically building a mathematical formulation
    that can be used to represent the background. So, this performs in a much better
    way than the simple frame differencing technique. This technique tries to detect
    static parts of the scene and then updates the background model. This background
    model is then used to detect background pixels. So, it's an adaptive technique
    that can adjust according to the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Naive background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start the background subtraction discussion from the beginning. What
    does a background subtraction process look like? Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive background subtraction](img/B04283_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image represents the background scene. Now, let''s introduce
    a new object into this scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive background subtraction](img/B04283_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding image, there is a new object in the scene. So, if
    we compute the difference between this image and our background model, you should
    be able to identify the location of the TV remote:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive background subtraction](img/B04283_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The overall process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive background subtraction](img/B04283_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Does it work well?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There's a reason why we call it the **naive** approach. It works under ideal
    conditions, and as we know, nothing is ideal in the real world. It does a reasonably
    good job of computing the shape of the given object, but it does so under some
    constraints. One of the main requirements of this approach is that the color and
    intensity of the object should be sufficiently different from that of the background.
    Some of the factors that affect these kinds of algorithms are image noise, lighting
    conditions, autofocus in cameras, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a new object enters our scene and stays there, it will be difficult to
    detect new objects that are in front of it. This is because we don''t update our
    background model, and the new object is now part of our background. Consider the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Does it work well?](img/B04283_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s say a new object enters our scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Does it work well?](img/B04283_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We identify this to be a new object, which is fine. Let''s say another object
    comes into the scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Does it work well?](img/B04283_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It will be difficult to identify the location of these two different objects
    because their locations overlap. Here''s what we get after subtracting the background
    and applying the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Does it work well?](img/B04283_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this approach, we assume that the background is static. If some parts of
    our background start moving, then those parts will start getting detected as new
    objects. So, even if the movements are minor, say a waving flag, it will cause
    problems in our detection algorithm. This approach is also sensitive to changes
    in illumination, and it cannot handle any camera movement. Needless to say, it's
    a delicate approach! We need something that can handle all these things in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: Frame differencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that we cannot keep a static background image that can be used to detect
    objects. So, one of the ways to fix this would be to use frame differencing. It
    is one of the simplest techniques that we can use to see what parts of the video
    are moving. When we consider a live video stream, the difference between successive
    frames gives a lot of information. The concept is fairly straightforward. We just
    take the difference between successive frames and display the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'If I move my laptop rapidly, we can see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frame differencing](img/B04283_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of the laptop, let''s move the object and see what happens. If I rapidly
    shake my head, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frame differencing](img/B04283_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the preceding images, only the moving parts of the video
    get highlighted. This gives us a good starting point to see the areas that are
    moving in the video. Let''s take a look at the function to compute the frame difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Frame differencing is fairly straightforward. You compute the absolute difference
    between the current frame and previous frame and between the current frame and
    next frame. We then take these frame differences and apply a bitwise AND operator.
    This will highlight the moving parts in the image. If you just compute the difference
    between the current frame and previous frame, it tends to be noisy. Hence, we
    need to use the bitwise AND operator between successive frame differences to get
    some stability when we see the moving objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the function that can extract and return a frame from
    the webcam:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, it''s pretty straightforward. We just need to resize the frame
    and convert it to grayscale. Now that we have the helper functions ready, let''s
    take a look at the `main` function and see how it all comes together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How well does it work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we can see, frame differencing addresses a couple of important problems
    that we faced earlier. It can quickly adapt to lighting changes or camera movements.
    If an object comes in the frame and stays there, it will not be detected in the
    future frames. One of the main concerns of this approach is about detecting uniformly
    colored objects. It can only detect the edges of a uniformly colored object. This
    is because a large portion of this object will result in very low pixel differences,
    as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How well does it work?](img/B04283_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say this object moved slightly. If we compare this with the previous
    frame, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How well does it work?](img/B04283_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we have very few pixels that are labeled on that object. Another concern
    is that it is difficult to detect whether an object is moving toward the camera
    or away from it.
  prefs: []
  type: TYPE_NORMAL
- en: The Mixture of Gaussians approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we talk about **Mixture of Gaussians** (**MOG**), let''s see what a
    *mixture model* is. A mixture model is just a statistical model that can be used
    to represent the presence of subpopulations within our data. We don''t really
    care about what category each data point belongs to. All we need to do is identify
    whether the data has multiple groups inside it. Now, if we represent each subpopulation
    using the Gaussian function, then it''s called Mixture of Gaussians. Let''s consider
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mixture of Gaussians approach](img/B04283_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, as we gather more frames in this scene, every part of the image will gradually
    become part of the background model. This is what we discussed earlier as well.
    If a scene is static, the model adapts itself to make sure that the background
    model is updated. The foreground mask, which is supposed to represent the foreground
    object, looks like a black image at this point because every pixel is part of
    the background model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV has multiple algorithms implemented for the Mixture of Gaussians approach.
    One of them is called **MOG** and the other is called **MOG2**. To get a detailed
    explanation, you can refer to [http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0](http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0).
    You will also be able check out the original research papers that were used to
    implement these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce a new object into this scene and see what the foreground mask
    looks like using the MOG approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mixture of Gaussians approach](img/B04283_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s wait for some time and introduce a new object into the scene. Let''s
    take a look at what the new foreground mask looks like using the MOG2 approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mixture of Gaussians approach](img/B04283_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the preceding images, the new objects are being identified
    correctly. Let''s take a look at the interesting part of the code (you can get
    the complete code in the `.cpp` files):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What happened in the code?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s quickly go through the code and see what''s happening there. We use
    the Mixture of Gaussians model to create a background subtractor object. This
    object represents the model that will be updated as and when we encounter new
    frames from the webcam. As we can see in the code, we initialize two background
    subtraction models: `BackgroundSubtractorMOG` and `BackgroundSubtractorMOG2`.
    They represent two different algorithms that are used for background subtraction.
    The first one refers to the paper by *P. KadewTraKuPong* and *R. Bowden titled*,
    *An improved adaptive background mixture model for real-time tracking with shadow
    detection*. You can check it out at [http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf](http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf).
    The second one refers to the paper by *Z.Zivkovic* titled, *Improved adaptive
    Gausian Mixture Model for background subtraction*. You can check it out at [http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf](http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf).
    We start an infinite `while` loop and continuously read the input frames from
    the webcam. With each frame, we update the background model, as shown in the following
    lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The background model gets updated in these steps. Now, if a new object enters
    the scene and stays there, it will become part of the background model. This helps
    us overcome one of the biggest shortcomings of the naïve background subtraction
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, background subtraction methods are affected by many factors.
    Their accuracy depends on how we capture the data and how it's processed. One
    of the biggest factors that tend to affect these algorithms is the noise level.
    When we say *noise*, we are talking about things, such as graininess in an image,
    isolated black/white pixels, and so on. These issues tend to affect the quality
    of our algorithms. This is where morphological image processing comes into picture.
    Morphological image processing is used extensively in a lot of real-time systems
    to ensure the quality of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological image processing refers to processing the shapes of features in
    the image. For example, you can make a shape thicker or thinner. Morphological
    operators rely on how the pixels are ordered in an image, but on their values.
    This is the reason why they are really well suited to manipulate shapes in binary
    images. Morphological image processing can be applied to grayscale images as well,
    but the pixel values will not matter much.
  prefs: []
  type: TYPE_NORMAL
- en: What's the underlying principle?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Morphological operators use a structuring element to modify an image. What is
    a structuring element? A structuring element is basically a small shape that can
    be used to inspect a small region in the image. It is positioned at all the pixel
    locations in the image so that it can inspect that neighborhood. We basically
    take a small window and overlay it on top of a pixel. Depending on the response,
    we take an appropriate action at that pixel location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What''s the underlying principle?](img/B04283_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will apply a bunch of morphological operations to this image to see how the
    shape changes.
  prefs: []
  type: TYPE_NORMAL
- en: Slimming the shapes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can achieve this effect using an operation called **erosion**. This is an
    operation that makes a shape thinner by peeling the boundary layers of all the
    shapes in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Slimming the shapes](img/B04283_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the function that performs morphological erosion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can check out the complete code in the `.cpp` files to understand how to
    use this function. Basically, we build a structuring element using an built-in
    OpenCV function. This object is used as a probe to modify each pixel based on
    certain conditions. These *conditions* refer to what's happening around that particular
    pixel in the image. For example, is it surrounded by white pixels? Or is it surrounded
    by black pixels? Once we have an answer, we can take an appropriate action.
  prefs: []
  type: TYPE_NORMAL
- en: Thickening the shapes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use an operation called **dilation** to achieve thickening. This is an operation
    that makes a shape thicker by adding boundary layers to all the shapes in the
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thickening the shapes](img/B04283_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Other morphological operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are some other morphological operators that are interesting. Let's first
    take a look at the output image. We can take a look at the code at the end of
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological opening
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is an operation that *opens* a shape. This operator is frequently used
    for noise removal in an image. We can achieve morphological opening by applying
    erosion followed by dilation to an image. The morphological opening process basically
    removes small objects from the foreground in the image by placing them in the
    background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Morphological opening](img/B04283_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the function to the perform morphological opening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we can see here, we apply erosion and dilation to the image to perform the
    morphological opening.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological closing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an operation that *closes* a shape by filling the gaps. This operation
    is also used for noise removal. We achieve morphological closing by applying dilation
    followed by erosion to an image. This operation removes tiny holes in the foreground
    by changing small objects in the background into the foreground.
  prefs: []
  type: TYPE_NORMAL
- en: '![Morphological closing](img/B04283_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s quickly take a look at the function to perform the morphological closing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Drawing the boundary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We achieve this using the morphological gradient. This is an operation that
    draws the boundary around a shape by taking the difference between dilation and
    erosion of an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Drawing the boundary](img/B04283_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the function to perform the morphological gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: White Top-Hat transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While Top-Hat transform, also simply called Top-Hat transform, extracts finer
    details from the images. We can apply white top-hat transform by computing the
    difference between the input image and its morphological opening. This gives us
    the objects in the image that are smaller than the structuring elements and are
    brighter than the surroundings. So, depending on the size of the structuring element,
    we can extract various objects in the given image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![White Top-Hat transform](img/B04283_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look carefully at the output image, you can see those black rectangles.
    This means that the structuring element was able to fit in there, and so those
    regions are blackened out. Here is the function to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Black Top-Hat transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Black Top-Hat transform, also simply called Black Hat transform, extracts finer
    details from the image as well. We can apply black top-hat transform by computing
    the difference between the morphological closing of an image and the image itself.
    This gives us the objects in the image that are smaller than the structuring element
    and are darker than the surroundings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Black Top-Hat transform](img/B04283_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the function to perform the black hat transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the algorithms that are used for background
    modeling and morphological image processing. We discussed naïve background subtraction
    and its limitations. We learned how to get motion information using frame differencing
    and how it can be constrain us when we want to track different types of objects.
    We also discussed Mixture of Gaussians, along with its formulation and implementation
    details. We then discussed morphological image processing. We learned how it can
    be used for various purposes and different operations were demonstrated to show
    the use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to track an object and the various
    techniques that can be used to do it.
  prefs: []
  type: TYPE_NORMAL
