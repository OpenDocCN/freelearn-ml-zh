<html><head></head><body>
<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques"><div class="book" id="BE6O2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01" class="calibre1"/>Chapter 1. Introduction to Ensemble Techniques</h1></div></div></div><p class="calibre7">Ensemble techniques are model output aggregating techniques that have evolved over the past decade and a half in the area of statistical and machine learning. This forms the central theme of this book. Any user of statistical models and machine learning tools will be familiar with the problem of building a model and the vital decision of choosing among potential candidate models. A model's accuracy is certainly not the only relevant criterion; we are also concerned with its complexity, as well as whether or not the overall model makes practical sense.</p><p class="calibre7">Common modeling problems include the <a id="id0" class="calibre1"/>decision to choose a model, and various methodologies exist to aid this task. In statistics, we resort to measures such as <span class="strong"><strong class="calibre8">Akaike Information Criteria</strong></span> (<span class="strong"><strong class="calibre8">AIC</strong></span>) and <span class="strong"><strong class="calibre8">Bayesian Information Criteria</strong></span> (<span class="strong"><strong class="calibre8">BIC</strong></span>), and on other fronts, the p-value associated with the variable in the fitted model helps with the decision. This is<a id="id1" class="calibre1"/> a process generally known as <span class="strong"><strong class="calibre8">model selection</strong></span>. Ridge penalty, Lasso, and other statistics also help with this task. For<a id="id2" class="calibre1"/> machine learning models such as neural networks, decision trees, and so on, a k-fold cross-validation is useful when the model is built using a part of the data referred to as training data, and then accuracy is looked for in the untrained area or validation data. If the model is sensitive to its complexity, the exercise could be futile.</p><p class="calibre7">The process of obtaining the <span class="strong"><em class="calibre9">best</em></span> model means that we create a host of other models, which are themselves nearly as efficient as the best model. Moreover, the best model accurately covers the majority of samples, and other models might accurately assess the variable space region where it is inaccurate. Consequently, we can see that the final shortlisted model has few advantages over the runner up. The next models in line are not so poor as to merit outright rejection. This makes it necessary to find a way of taking most of the results already obtained from the models and combining them in a meaningful way. The search for a method for putting together various models is the main objective of ensemble learning. Alternatively, one can say that ensemble learning transforms competing models into collaborating models. In fact, ensemble techniques are not the end of the modeling exercise, as they will also be extended to the unsupervised learning problems. We will demonstrate an example that justifies the need for this.</p><p class="calibre7">The implementation of ensemble methods would have been impossible without the invention of modern computational power. Statistical methods foresaw techniques that required immense computations. Methods such as permutation tests and jackknife are evidence of the effectiveness of computational power. We will undertake an exercise to learn these later in the chapter, and we will revisit them later on in the book.</p><p class="calibre7">From a machine learning perspective, <span class="strong"><em class="calibre9">supervised</em></span> and <span class="strong"><em class="calibre9">unsupervised</em></span> are the two main types of learning technique. <span class="strong"><strong class="calibre8">Supervised learning</strong></span> is the arm of machine learning, the process in which a certain variable is known, and the purpose is to understand this variable through various other variables. Here, we have a target variable. Since learning takes place with respect to the output <a id="id3" class="calibre1"/>variable, supervised learning is sometimes referred to as learning with a teacher. All target variables are not alike, and they often fall under one of the following four types. If the goal is to classify observations into one of <span class="strong"><em class="calibre9">k</em></span> types of class (for example, Yes/No, Satisfied/Dissatisfied), then we have a classification problem. Such a variable is referred to as a <span class="strong"><em class="calibre9">categorical variable</em></span> in statistics. It is possible that the variable of interest might be a continuous variable, which is numeric from a software perspective. This may include car mileage per liter, a person's income, or a person's age. For such scenarios, the purpose of the machine learning problem is to learn the variables in<a id="id4" class="calibre1"/> terms of other associated variables, and then predict it for unknown cases in which only the values of associated variables are available. We will broadly refer to this class of problem as a <span class="strong"><strong class="calibre8">regression problem</strong></span>.</p><p class="calibre7">In clinical trials, the time to event is often of interest. When an illness is diagnosed, we would ask whether the proposed drug is an improvement on the existing one. While the variable in question here is the length of time between diagnosis and death, clinical trial data poses several other problems. The analysis cannot wait until all the patients have died, and/or some of the patients may have moved away from the study, making it no longer possible to know their status. Consequently, we have censored data. As part of the study observations, complete information is not available. Survival analysis largely deals with such problems, and we will undertake the problem of creating ensemble models here.</p><p class="calibre7">With classification, regression, and survival data, it may be assumed that that the instances/observations are independent of each other. This is a very reasonable assumption in that there is a valid reason to believe that patients will respond to a drug independently of other patients, a customer will churn or pay the loan independently of other customers, and so forth. In yet another important class of problems, this assumption is not met, and we are left with observations depending on each other via time series data. An example of time series data is the closure stock exchange points of a company. Clearly, the performance of a company's stock can't be independent each day, and thus we need to factor in dependency.</p><p class="calibre7">In many practical problems, the goal is to understand patterns or find groups of observations, and we don't have a specific variable of interest with regard to which algorithm needs to be trained. Finding groups or clusters is referred to as unsupervised learning or learning without a teacher. Two main practical problems that arise in finding clusters is that (i) it is generally not known in advance how many clusters are in the population, and (ii) different choices of initial cluster centers lead to different solutions. Thus, we need a solution that is free from, or at least indifferent to, initialization and takes the positives of each useful solution into consideration. This will lead us toward unsupervised ensemble techniques.</p><p class="calibre7">The search for the best models, supervised or unsupervised, is often hindered by the presence of outliers. The presence of a single outlier is known to heavily influence the overall fit of linear models, and it is also known to significantly impact even nonlinear models. Outlier detection is a challenge in itself, and a huge body of statistical methods help in identifying outliers. A host of machine learning methods also help in identifying outliers. Of course, ensembles will help here, and we will develop R programs that will help solve the problem of identifying outliers. This method will be referred to as outlier ensembles.</p><p class="calibre7">At the outset, it is important that the reader becomes familiar with the datasets used in this book. All major datasets will be introduced in the first section. We begin the chapter with a brief introduction to the core statistical/machine learning models and put them into action immediately afterward. It will quickly become apparent that there is not a single class of model that would perform better than any other model. If any such solution existed, we wouldn't need the ensemble technique.</p><p class="calibre7">In this chapter, we will cover:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre8">Datasets</strong></span>: The core datasets that will be used throughout the book</li><li class="listitem"><span class="strong"><strong class="calibre8">Statistical/machine learning models</strong></span>: Important classification models will be explained here</li><li class="listitem"><span class="strong"><strong class="calibre8">The right model dilemma</strong></span>: The absence of a <span class="strong"><em class="calibre9">dominating</em></span> model</li><li class="listitem"><span class="strong"><strong class="calibre8">An ensemble purview</strong></span>: The need for ensembles</li><li class="listitem"><span class="strong"><strong class="calibre8">Complementary statistical tests</strong></span>: Important statistical tests that will be useful for model comparisons will be discussed here</li></ul></div><p class="calibre7">The following R packages will be required for this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">ACSWR</code></li><li class="listitem"><code class="literal">caret</code></li><li class="listitem"><code class="literal">e1071</code></li><li class="listitem"><code class="literal">factoextra</code></li><li class="listitem"><code class="literal">mlbench</code></li><li class="listitem"><code class="literal">NeuralNetTools</code></li><li class="listitem"><code class="literal">perm</code></li><li class="listitem"><code class="literal">pROC</code></li><li class="listitem"><code class="literal">RSADBE</code></li><li class="listitem"><code class="literal">Rpart</code></li><li class="listitem"><code class="literal">survival</code></li><li class="listitem"><code class="literal">nnet</code></li></ul></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch01lvl1sec10" class="calibre1"/>Datasets</h1></div></div></div><p class="calibre7">Data is undoubtedly the most important component of machine learning. If there was no data, we wouldn't have a common purpose. In most cases, the purpose for which the data is collected defines the<a id="id5" class="calibre1"/> problem itself. As we know that the variable might be of several types, the way it is stored and organized is also very important.</p><p class="calibre7">Lee and Elder (1997) considered a series of datasets and introduced the need for ensemble models. We will begin by looking at the details of the datasets considered in their paper, and we will then refer to other important datasets later on in the book.</p></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Hypothyroid"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec05" class="calibre1"/>Hypothyroid</h2></div></div></div><p class="calibre7">The hypothyroid dataset <code class="literal">Hypothyroid.csv</code> is available in<a id="id6" class="calibre1"/> the book's code bundle<a id="id7" class="calibre1"/> packet, located at <code class="literal">/…/Chapter01/Data</code>. While we have 26 variables in the dataset, we will only be using seven of these variables. Here, the number of <a id="id8" class="calibre1"/>observations is <span class="strong"><em class="calibre9">n </em></span>= 3163. The dataset is downloaded from <a class="calibre1" href="http://archive.ics.uci.edu/ml/datasets/thyroid+disease">http://archive.ics.uci.edu/ml/datasets/thyroid+disease</a> and the filename is <code class="literal">hypothyroid.data</code> (<a class="calibre1" href="http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data">http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data</a>). After some tweaks to the order of relabeling certain values, the CSV file is made available in the book's code bundle. The purpose of the study is to classify a patient with a thyroid problem based on the information provided by other variables. There are multiple variants of the dataset and the reader can delve into details at the following web page: <a class="calibre1" href="http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO">http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO</a>. Here, the column representing the variable of interest is named <code class="literal">Hypothyroid</code>, which shows that we have 151 patients with thyroid problems. The remaining 3012 tested negative for it. Clearly, this dataset is an example of <span class="strong"><em class="calibre9">unbalanced data</em></span>, which means that one of the two cases is outnumbered by a huge number; for each thyroid case, we have about 20 negative cases. Such problems need to be handled differently, and we need to get into the subtleties of the algorithms to build meaningful models. The additional variables or covariates that we will use while building the predictive models include <code class="literal">Age</code>, <code class="literal">Gender</code>, <code class="literal">TSH</code>, <code class="literal">T3</code>, <code class="literal">TT4</code>, <code class="literal">T4U</code>, and <code class="literal">FTI</code>. The data is first imported into an R session and is subset according to the variables of interest as follows:</p><div class="informalexample"><pre class="programlisting">&gt; HT &lt;- read.csv("../Data/Hypothyroid.csv",header = TRUE,stringsAsFactors = F)
&gt; HT$Hypothyroid &lt;- as.factor(HT$Hypothyroid)
&gt; HT2 &lt;- HT[,c("Hypothyroid","Age","Gender","TSH","T3","TT4","T4U","FTI")]</pre></div><p class="calibre7">The first line of code imports the data from the <code class="literal">Hypothyroid.csv</code> file using the <code class="literal">read.csv</code> function. The dataset now has a lot of missing data in the variables, as seen here:</p><div class="informalexample"><pre class="programlisting">&gt; sapply(HT2,function(x) sum(is.na(x)))
Hypothyroid         Age      Gender         TSH          T3         TT4 
          0         446          73         468         695         249 
        T4U         FTI 
        248         247 </pre></div><p class="calibre7">Consequently, we remove all the rows that have a missing value, and then split the data into training and testing datasets. We will also create a formula for the classification problem:</p><div class="informalexample"><pre class="programlisting">&gt; HT2 &lt;- na.omit(HT2)
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(HT2),replace=TRUE, prob=c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; HT2_Train &lt;- HT2[Train_Test=="Train",]
&gt; HT2_TestX &lt;- within(HT2[Train_Test=="Test",],rm(Hypothyroid))
&gt; HT2_TestY &lt;- HT2[Train_Test=="Test",c("Hypothyroid")]
&gt; HT2_Formula &lt;- as.formula("Hypothyroid~.")</pre></div><p class="calibre7">The <code class="literal">set.seed</code> function ensures that the results are reproducible each time we run the program. After removing<a id="id9" class="calibre1"/> the missing observations with the <code class="literal">na.omit</code> function, we split the hypothyroid data into training and testing parts. The former is used to build the model and the latter is used to validate it, using data that has not been used to build the model. Quinlan – the inventor of the popular tree algorithm C4.5 – used this dataset extensively.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Waveform"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec06" class="calibre1"/>Waveform</h2></div></div></div><p class="calibre7">This dataset is an example of a<a id="id10" class="calibre1"/> simulation study. Here, we have twenty-one variables as input or independent variables, and a class variable referred to as <code class="literal">classes</code>. The data is generated <a id="id11" class="calibre1"/>using the <code class="literal">mlbench.waveform</code> function from the <code class="literal">mlbench</code> R package. For more details, refer to the following link: <a class="calibre1" href="ftp://ftp.ics.uci.edu/pub/machine-learning-databases">ftp://ftp.ics.uci.edu/pub/machine-learning-databases</a>. We will simulate 5,000 observations for this dataset. As mentioned earlier, the <code class="literal">set.seed</code> function guarantees reproducibility. Since we are solving binary classification problems, we will reduce the three classes generated by the waveform function to two, and then partition the data into training and testing parts for model building and testing purposes:</p><div class="informalexample"><pre class="programlisting">&gt; library(mlbench)
&gt; set.seed(123)
&gt; Waveform &lt;- mlbench.waveform(5000)
&gt; table(Waveform$classes)
   1    2    3 
1687 1718 1595 
&gt; Waveform$classes &lt;- ifelse(Waveform$classes!=3,1,2)
&gt; Waveform_DF &lt;- data.frame(cbind(Waveform$x,Waveform$classes)) # Data Frame
&gt; names(Waveform_DF) &lt;- c(paste0("X",".",1:21),"Classes")
&gt; Waveform_DF$Classes &lt;- as.factor(Waveform_DF$Classes)
&gt; table(Waveform_DF$Classes)
   1    2 
3405 1595 </pre></div><p class="calibre7">The R function <code class="literal">mlbench.waveform</code> creates a new object of the <code class="literal">mlbench</code> class. Since it consists of two sub-parts in <code class="literal">x</code> and classes, we will convert it into <code class="literal">data.frame</code> following some further manipulations. The <code class="literal">cbind</code> function binds the two objects <code class="literal">x</code> (a matrix) and classes (a numeric vector) into a single matrix. The <code class="literal">data.frame</code> function converts the matrix object into a data frame, which is the class desired for the rest of the program.</p><p class="calibre7">After partitioning the data, we will create the required <code class="literal">formula</code> for the waveform dataset:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(Waveform_DF),replace = TRUE,
+ prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; Waveform_DF_Train &lt;- Waveform_DF[Train_Test=="Train",]
&gt; Waveform_DF_TestX &lt;- within(Waveform_DF[Train_Test=="Test",],rm(Classes))
&gt; Waveform_DF_TestY &lt;- Waveform_DF[Train_Test=="Test","Classes"]
&gt; Waveform_DF_Formula &lt;- as.formula("Classes~.")</pre></div></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="German Credit"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch01lvl2sec07" class="calibre1"/>German Credit</h2></div></div></div><p class="calibre7">Loans are not always repaid in full, and there are defaulters. In this case, it becomes important for the bank to identify<a id="id12" class="calibre1"/> potential defaulters based on the<a id="id13" class="calibre1"/> available information. Here, we adapt the <code class="literal">GC</code> dataset from the <code class="literal">RSADBE</code> package to properly reflect the labels of the factor variable. The transformed dataset is available as <code class="literal">GC2.RData</code> in the data folder. The <code class="literal">GC</code> dataset itself is <a id="id14" class="calibre1"/>mainly an adaptation of the version available at <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a>. Here, we have 1,000 observations, and 20 covariate/independent variables such as the status of existing checking account, duration, and so forth. The final status of whether the loan was completely paid or not is available in the <code class="literal">good_bad</code> column. We will partition the data into training and testing parts, and create the formula too:</p><div class="informalexample"><pre class="programlisting">&gt; library(RSADBE)
&gt; load("../Data/GC2.RData")
&gt; table(GC2$good_bad)
 bad good 
 300  700 
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(GC2),replace = TRUE,prob=c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; GC2_Train &lt;- GC2[Train_Test=="Train",]
&gt; GC2_TestX &lt;- within(GC2[Train_Test=="Test",],rm(good_bad))
&gt; GC2_TestY &lt;- GC2[Train_Test=="Test","good_bad"]
&gt; GC2_Formula &lt;- as.formula("good_bad~.")</pre></div></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Iris"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch01lvl2sec08" class="calibre1"/>Iris</h2></div></div></div><p class="calibre7">Iris is probably the <a id="id15" class="calibre1"/>most famous classification dataset. The great statistician Sir R. A. Fisher popularized the dataset, which he used for classifying the three types of <code class="literal">iris</code> plants based on length and width measurements of their petals and sepals. Fisher used this <a id="id16" class="calibre1"/>dataset to pioneer the invention of the statistical classifier linear discriminant analysis. Since there are three species of <code class="literal">iris</code>, we converted this into a binary classification problem, separated the dataset, and created a formula as seen here:</p><div class="informalexample"><pre class="programlisting">&gt; data("iris")
&gt; ir2 &lt;- iris
&gt; ir2$Species &lt;- ifelse(ir2$Species=="setosa","S","NS")
&gt; ir2$Species &lt;- as.factor(ir2$Species)
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(ir2),replace = TRUE,prob=c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; ir2_Train &lt;- ir2[Train_Test=="Train",]
&gt; ir2_TestX &lt;- within(ir2[Train_Test=="Test",],rm(Species))
&gt; ir2_TestY &lt;- ir2[Train_Test=="Test","Species"]
&gt; ir2_Formula &lt;- as.formula("Species~.")</pre></div></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Pima Indians Diabetes"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch01lvl2sec09" class="calibre1"/>Pima Indians Diabetes</h2></div></div></div><p class="calibre7">Diabetes is a health hazard, which is mostly incurable, and patients who are diagnosed with it have to adjust their<a id="id17" class="calibre1"/> lifestyles in order to cater to this condition. Based on variables such as <code class="literal">pregnant</code>, <code class="literal">glucose</code>, <code class="literal">pressure</code>, <code class="literal">triceps</code>, <code class="literal">insulin</code>, <code class="literal">mass</code>, <code class="literal">pedigree</code>, and <code class="literal">age</code>, the <a id="id18" class="calibre1"/>problem here is to classify the person as diabetic or not. Here, we have 768 observations. This dataset is drawn from the <code class="literal">mlbench</code> package:</p><div class="informalexample"><pre class="programlisting">&gt; data("PimaIndiansDiabetes")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(PimaIndiansDiabetes),replace = TRUE,
+ prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; PimaIndiansDiabetes_Train &lt;- PimaIndiansDiabetes[Train_Test=="Train",]
&gt; PimaIndiansDiabetes_TestX &lt;- within(PimaIndiansDiabetes[Train_Test=="Test",],
+                                     rm(diabetes))
&gt; PimaIndiansDiabetes_TestY &lt;- PimaIndiansDiabetes[Train_Test=="Test","diabetes"]
&gt; PID_Formula &lt;- as.formula("diabetes~.")</pre></div><p class="calibre7">The five datasets described up to this point are classification problems. We look at one example each for regression, time series, survival, clustering, and outlier detection problems.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="US Crime"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_7"><a id="ch01lvl2sec10" class="calibre1"/>US Crime</h2></div></div></div><p class="calibre7">A study of the crime rate per million of the population among the 47 different states of the US is <a id="id19" class="calibre1"/>undertaken here, and an attempt is made to find its dependency on 13 variables. These include age<a id="id20" class="calibre1"/> distribution, indicator of southern states, average number of schooling years, and so on. As with the earlier datasets, we will also partition this one into the following chunks of R program:</p><div class="informalexample"><pre class="programlisting">&gt; library(ACSWR)
Warning message:
package 'ACSWR' was built under R version 3.4.1 
&gt; data(usc)
&gt; str(usc)
'data.frame':	47 obs. of  14 variables:
 $ R  : num  79.1 163.5 57.8 196.9 123.4 ...
 $ Age: int  151 143 142 136 141 121 127 131 157 140 ...
 $ S  : int  1 0 1 0 0 0 1 1 1 0 ...
 $ Ed : int  91 113 89 121 121 110 111 109 90 118 ...
 $ Ex0: int  58 103 45 149 109 118 82 115 65 71 ...
 $ Ex1: int  56 95 44 141 101 115 79 109 62 68 ...
 $ LF : int  510 583 533 577 591 547 519 542 553 632 ...
 $ M  : int  950 1012 969 994 985 964 982 969 955 1029 ...
 $ N  : int  33 13 18 157 18 25 4 50 39 7 ...
 $ NW : int  301 102 219 80 30 44 139 179 286 15 ...
 $ U1 : int  108 96 94 102 91 84 97 79 81 100 ...
 $ U2 : int  41 36 33 39 20 29 38 35 28 24 ...
 $ W  : int  394 557 318 673 578 689 620 472 421 526 ...
 $ X  : int  261 194 250 167 174 126 168 206 239 174 ...
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(usc),replace = TRUE,prob=c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; usc_Train &lt;- usc[Train_Test=="Train",]
&gt; usc_TestX &lt;- within(usc[Train_Test=="Test",],rm(R))
&gt; usc_TestY &lt;- usc[Train_Test=="Test","R"]
&gt; usc_Formula &lt;- as.formula("R~.")</pre></div><p class="calibre7">In each example discussed in this<a id="id21" class="calibre1"/> section thus far, we had a reason to believe that the observations are independent of each other. This assumption simply means that the regressands and regressors of one observation have no relationship with other observations' regressands and regressors. This is a simple and reasonable assumption. We have another class of observations/datasets where such assumptions are not practical. For example, the maximum temperature of a day is not completely independent of the previous day's temperature. If that were to be the case, we could have a scorchingly hot day, followed by winter, followed by another hot day, which in turn is followed by a very heavy rainy day. However, weather does not happen in this way as on successive days, the weather is dependent on previous days. In the next example, we consider the number of overseas visitors to New Zealand.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Overseas visitors"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_8"><a id="ch01lvl2sec11" class="calibre1"/>Overseas visitors</h2></div></div></div><p class="calibre7">The New Zealand overseas <a id="id22" class="calibre1"/>dataset is dealt with in detail in Chapter 10 of Tattar, et al. (2017). Here, the number of overseas visitors is captured on a monthly basis from January 1977 to December <a id="id23" class="calibre1"/>1995. We have visitors' data available for over 228 months. The <code class="literal">osvisit.dat</code> file is available at multiple web links, including <a class="calibre1" href="https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat">https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat</a> and <a class="calibre1" href="https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat">https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat</a>. It is also available in the book's code bundle. We will<a id="id24" class="calibre1"/> import the data in R, convert it into a time series object, and visualize it:</p><div class="informalexample"><pre class="programlisting">&gt; osvisit &lt;- read.csv("../Data/osvisit.dat", header= FALSE)
&gt; osv &lt;- ts(osvisit$V1, start = 1977, frequency = 12)
&gt; class(osv)
[1] "ts"
&gt; plot.ts(osv)</pre></div><div class="mediaobject"><img src="../images/00002.jpeg" alt="Overseas visitors" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: New Zealand overseas visitors</p></div></div><p class="calibre11"> </p><p class="calibre7">Here, the dataset is not partitioned! Time series data can't be arbitrarily partitioned into training and testing parts. The reason is quite simple: if we have five observations in a time sequential order <span class="strong"><em class="calibre9">y1</em></span>, <span class="strong"><em class="calibre9">y2</em></span>, <span class="strong"><em class="calibre9">y3</em></span>, <span class="strong"><em class="calibre9">y4</em></span>, <span class="strong"><em class="calibre9">y5</em></span>, and we believe that the order of impact is <span class="strong"><em class="calibre9">y1→y2→y3→y4→y5</em></span>, an arbitrary partition of <span class="strong"><em class="calibre9">y1</em></span>, <span class="strong"><em class="calibre9">y2</em></span>, <span class="strong"><em class="calibre9">y5</em></span>, will have different behavior. It won't have the same information as three consecutive observations. Consequently, the time series partitioning has to preserve the dependency structure; we keep the most recent part of the time as the test data. For the five observations example, we choose a sample of <span class="strong"><em class="calibre9">y1</em></span>, <span class="strong"><em class="calibre9">y2</em></span>, <span class="strong"><em class="calibre9">y3</em></span>, as the test data. The partitioning is simple, and we will cover this in <a class="calibre1" title="Chapter 11. Ensembling Time Series Models" href="part0076_split_000.html#28FAO1-2006c10fab20488594398dc4871637ee">Chapter 11</a>, <span class="strong"><em class="calibre9">Ensembling Time Series Models</em></span>.</p><p class="calibre7">Live testing experiments rarely yield complete observations. In reliability analysis, as well as survival analysis/clinical trials, the units/patients are observed up to a predefined time and a note is made regarding whether a specific event occurs, which is usually failure or death. A considerable fraction of<a id="id25" class="calibre1"/> observations would not have failed by the pre-decided time, and the analysis cannot wait for all units to fail. A reason to curtail the study might be that the time by which all units would have failed would be very large, and it would be expensive to continue the study until such a time. Consequently, we are left with incomplete observations; we only know that the lifetime of the units lasts for at least the predefined time before the study was called off, and the event of interest may occur sometime in the future. Consequently, some observations are censored and the data is referred to as censored data. Special statistical methods are required for the analysis of such datasets. We will give an example of these types of datasets next, and analyze them later, in <a class="calibre1" title="Chapter 10. Ensembling Survival Models" href="part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee">Chapter 10</a>, <span class="strong"><em class="calibre9">Ensembling Survival Models</em></span>.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Primary Biliary Cirrhosis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_9"><a id="ch01lvl2sec12" class="calibre1"/>Primary Biliary Cirrhosis</h2></div></div></div><p class="calibre7">The <code class="literal">pbc</code> dataset from the survival package<a id="id26" class="calibre1"/> is a benchmark dataset in the domain of clinical trials. Mayo Clinic collected the data, which is concerned with the primary biliary cirrhosis (PBC) of the liver. The study was conducted between 1974 and 1984. More details can be found by running <code class="literal">pbc</code>, followed by <code class="literal">library(survival)</code> on the R terminal. Here, the main time to the <a id="id27" class="calibre1"/>event of interest is the number of days between registration and either death, transplantation, or study analysis in July 1986, and this is captured in the time variable. Similarly to a survival study, the events might be censored and the indicator is in the column status. The time to event needs to be understood, factoring in variables such as <code class="literal">trt</code>, <code class="literal">age</code>, <code class="literal">sex</code>, <code class="literal">ascites</code>, <code class="literal">hepato</code>, <code class="literal">spiders</code>, <code class="literal">edema</code>, <code class="literal">bili</code>, <code class="literal">chol</code>, <code class="literal">albumin</code>, <code class="literal">copper</code>, <code class="literal">alk.phos</code>, <code class="literal">ast</code>, <code class="literal">trig</code>, <code class="literal">platelet</code>, <code class="literal">protime</code>, and <code class="literal">stage</code>.</p><p class="calibre7">The eight datasets discussed up until this point have a target variable, or a regressand/dependent variable, and are examples of the supervised learning problem. On the other hand, there are practical cases in which we simply attempt to understand the data and find useful patterns and groups/clusters in it. Of course, it is important to note that the purpose of clustering is to find an identical group and give it a sensible label. For instance, if we are trying to group cars based on their characteristics such as length, width, horsepower, engine cubic capacity, and so on, we may find groups that might be labeled as hatch, sedan, and saloon classes, while another clustering solutions might result in labels of basic, premium, and sports variant groups. The two main problems posed in clustering are the choice of the number of groups and the formation of robust clusters. We consider a simple dataset from the <code class="literal">factoextra</code> R package.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Multishapes"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_10"><a id="ch01lvl2sec13" class="calibre1"/>Multishapes</h2></div></div></div><p class="calibre7">The <code class="literal">multishapes</code> dataset<a id="id28" class="calibre1"/> from the <code class="literal">factoextra</code> package consists of three variables: <code class="literal">x</code>, <code class="literal">y</code>, and <code class="literal">shape</code>. It consists of different<a id="id29" class="calibre1"/> shapes, with each shape forming a cluster. Here, we have two concurrent circle shapes, two parallel rectangles/beds, and one cluster of points at the bottom-right. Outliers are also added across scatterplots. Some brief <a id="id30" class="calibre1"/>R code gives a useful display:</p><div class="informalexample"><pre class="programlisting">&gt; library(factoextra)
&gt; data("multishapes")
&gt; names(multishapes)
[1] "x"     "y"     "shape"
&gt; table(multishapes$shape)
  1   2   3   4   5   6 
400 400 100 100  50  50 
&gt; plot(multishapes[,1],multishapes[,2],col=multishapes[,3])</pre></div><div class="mediaobject"><img src="../images/00003.jpeg" alt="Multishapes" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Finding shapes or groups</p></div></div><p class="calibre11"> </p><p class="calibre7">This dataset includes a column named shape, as it is a hypothetical dataset. In true clustering problems, we will have neither a cluster group indicator nor the visualization luxury of only two variables. Later in this book, we will see how ensemble clustering techniques help overcome the problems of deciding the number of clusters and the consistency of cluster membership.</p><p class="calibre7">Although it doesn't happen that often, frustrations can <a id="id31" class="calibre1"/>arise when fine-tuning different parameters, fitting different models, and other tricks all fail to find a useful working model. The culprit of this is often the outlier. A single outlier is known to wreak havoc on an otherwise potentially useful model, and their detection is of paramount importance. Hitherto this, the parametric and nonparametric outlier detections would be a matter of deep expertise. In complex scenarios, the identification would be an insurmountable task. A consensus on an observation being an outlier can be achieved using the ensemble outlier framework. To consider this, the board stiffness dataset will be considered. We will see how an outlier is pinned down in the conclusion of this book.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Introduction to Ensemble Techniques">
<div class="book" title="Datasets">
<div class="book" title="Board Stiffness"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_11"><a id="ch01lvl2sec14" class="calibre1"/>Board Stiffness</h2></div></div></div><p class="calibre7">The board stiffness dataset is <a id="id32" class="calibre1"/>available in the <code class="literal">ACSWR</code> package through the stiff <code class="literal">data.frame</code> stiff. The dataset consists of four measures of stiffness for 30 boards. The first measure of stiffness is obtained by sending a shock wave down the board, the second measure is <a id="id33" class="calibre1"/>obtained by vibrating the board, and the remaining two are obtained from static tests. A quick method of identifying the outliers in a multivariate dataset is by using the Mahalanobis distance function. The further the distance an observation is from the center, the more likely it is that the observation will be an outlier:</p><div class="informalexample"><pre class="programlisting">&gt; data(stiff)
&gt; sort(mahalanobis(stiff,colMeans(stiff),cov(stiff)),decreasing = TRUE)
 [1] 16.8474070168 12.2647549939  9.8980384087  7.6166439053
 [5]  6.2837628235  5.4770195915  5.2076098038  5.0557446013
 [9]  4.9883497928  4.5767867224  3.9900602512  3.5018290410
[13]  3.3979804418  2.9951752177  2.6959023813  2.5838186338
[17]  2.5385575365  2.3816049840  2.2191408683  1.9307771418
[21]  1.4876569689  1.4649908273  1.3980776252  1.3632123553
[25]  1.0792484215  0.7962095966  0.7665399704  0.6000128595
[29]  0.4635158597  0.1295713581</pre></div></div></div></div>

<div class="book" title="Statistical/machine learning models"><div class="book" id="CCNA2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec11" class="calibre1"/>Statistical/machine learning models</h1></div></div></div><p class="calibre7">The previous <a id="id34" class="calibre1"/>section introduced a host of problems through real datasets, and we will now discuss some standard model variants that are useful for dealing with such problems. First, we set up the required mathematical framework.</p><p class="calibre7">Suppose that we have <span class="strong"><em class="calibre9">n </em></span>independent pairs of observations, <span class="strong"><img src="../images/00004.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>, where <span class="strong"><img src="../images/00005.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> denotes the random variable of interest, also known as the <span class="strong"><em class="calibre9">dependent variable</em></span>, regress and, endogenous variable, and so on. <span class="strong"><img src="../images/00006.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> is the associated vector of explanatory variables, or independent/exogenous variables. The explanatory vector will consist of <span class="strong"><em class="calibre9">k </em></span>elements, that is, <span class="strong"><img src="../images/00007.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>. The data realized is of the form <span class="strong"><img src="../images/00008.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>, where <span class="strong"><img src="../images/00009.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> is the realized value (data) of random variable <span class="strong"><img src="../images/00010.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>. A convention will be adapted throughout the book that <span class="strong"><img src="../images/00011.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>, and this will take care of the intercept term. We assume that the observations are from the true distribution <span class="strong"><em class="calibre9">F</em></span>, which is not completely known. The general regression model, including the classification model as well as the regression model, is specified by:</p><div class="mediaobject"><img src="../images/00012.jpeg" alt="Statistical/machine learning models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Here, the function <span class="strong"><em class="calibre9">f</em></span> is an unknown function and <span class="strong"><img src="../images/00013.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> is the regression parameter, which captures the influence of <span class="strong"><img src="../images/00014.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> on <span class="strong"><img src="../images/00015.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>. The error <span class="strong"><img src="../images/00016.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> is the associated unobservable error term. Diverse methods can be applied to model the relationship between the Ys and the <code class="literal">xes</code>. The statistical regression model focused on the complete specification of the error distribution <span class="strong"><img src="../images/00017.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>, and in general the functional form would be linear as in <span class="strong"><img src="../images/00018.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span>. The function <span class="strong"><img src="../images/00019.jpeg" alt="Statistical/machine learning models" class="calibre15"/></span> is the link function in the class of generalized linear models. Nonparametric and semiparametric regression models are more flexible, as we don't place a restriction on the error's probability distribution. Flexibility would come with a price though, and here we need a much higher number of observations to make a valid inference, although that number is unspecified and is often subjective.</p><p class="calibre7">The machine learning<a id="id35" class="calibre1"/> paradigm includes some <span class="strong"><em class="calibre9">black box</em></span> methods, and we have a healthy overlap between this paradigm and non- and semi-parametric models. The reader is also cautioned that black box does not mean unscientific in any sense. The methods have a firm mathematical foundation and are reproducible every time. Next, we quickly review some of the most important statistical and machine learning models, and illustrate them through the datasets discussed earlier.</p></div>

<div class="book" title="Statistical/machine learning models">
<div class="book" title="Logistic regression model"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec15" class="calibre1"/>Logistic regression model</h2></div></div></div><p class="calibre7">The logistic regression model is a <a id="id36" class="calibre1"/>binary classification model, and it is a member of the exponential family which belongs to the class of <a id="id37" class="calibre1"/>generalized linear models. Now, let <span class="strong"><img src="../images/00020.jpeg" alt="Logistic regression model" class="calibre15"/></span>denote the binary label:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Logistic regression model" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Using the information contained in the explanatory vector <span class="strong"><img src="../images/00022.jpeg" alt="Logistic regression model" class="calibre15"/></span> we are trying to build a model that will help in this task. The logistic regression model is the following:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Logistic regression model" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Here, <span class="strong"><img src="../images/00024.jpeg" alt="Logistic regression model" class="calibre15"/></span> is the vector of regression coefficients. Note that the logit function <span class="strong"><img src="../images/00025.jpeg" alt="Logistic regression model" class="calibre15"/></span> is linear in the regression coefficients and hence the name for the model is a logistic regression model. A logistic regression model can be equivalently written as follows:</p><div class="mediaobject"><img src="../images/00026.jpeg" alt="Logistic regression model" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Here, <span class="strong"><img src="../images/00027.jpeg" alt="Logistic regression model" class="calibre15"/></span> is the binary <a id="id38" class="calibre1"/>error term that follows a Bernoulli distribution. For more information, refer to Chapter 17 of Tattar, et al. (2016). The estimation of the parameters of the logistic regression requires the <span class="strong"><strong class="calibre8">iterative reweighted least squares</strong></span> (<span class="strong"><strong class="calibre8">IRLS</strong></span>) algorithm, and we would use the <code class="literal">glm</code> R function to get this task done. We will use the Hypothyroid dataset in<a id="id39" class="calibre1"/> this section. In the previous section, the training and test datasets and formulas were already created, and we will carry on from that point.</p><div class="book" title="Logistic regression for hypothyroid classification"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch01lvl3sec01" class="calibre1"/>Logistic regression for hypothyroid classification</h3></div></div></div><p class="calibre7">For the <code class="literal">hypothyroid</code> dataset, we <a id="id40" class="calibre1"/>had <code class="literal">HT2_Train</code> as the training dataset. The test dataset is split as the covariate matrix in <code class="literal">HT2_TestX</code> and the outputs of the test dataset in <code class="literal">HT2_TestY</code>, while the formula for the logistic regression model is available in <code class="literal">HT2_Formula</code>. First, the logistic regression model is fitted to the training dataset using the <code class="literal">glm</code> function and the fitted model is christened <code class="literal">LR_fit</code>, and then we inspect it for model fit summaries using <code class="literal">summary(LR_fit)</code>. The fitted model is then applied to the covariate data in the test part using the <code class="literal">predict</code> function to create <code class="literal">LR_Predict</code>. The predicted probabilities are then labeled in <code class="literal">LR_Predict_Bin</code>, and these labels are compared with the actual <code class="literal">testY_numeric</code> and overall accuracy is obtained:</p><div class="informalexample"><pre class="programlisting">&gt; ntr &lt;- nrow(HT2_Train) # Training size
&gt; nte &lt;- nrow(HT2_TestX) # Test size
&gt; p &lt;- ncol(HT2_TestX)
&gt; testY_numeric &lt;- as.numeric(HT2_TestY)
&gt; LR_fit &lt;- glm(HT2_Formula,data=HT2_Train,family = binomial())
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
&gt; summary(LR_fit)
Call:
glm(formula = HT2_Formula, family = binomial(), data = HT2_Train)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.6390   0.0076   0.0409   0.1068   3.5127  
Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -8.302025   2.365804  -3.509 0.000449 ***
Age         -0.024422   0.012145  -2.011 0.044334 *  
GenderMALE  -0.195656   0.464353  -0.421 0.673498    
TSH         -0.008457   0.007530  -1.123 0.261384    
T3           0.480986   0.347525   1.384 0.166348    
TT4         -0.089122   0.028401  -3.138 0.001701 ** 
T4U          3.932253   1.801588   2.183 0.029061 *  
FTI          0.197196   0.035123   5.614 1.97e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 609.00  on 1363  degrees of freedom
Residual deviance: 181.42  on 1356  degrees of freedom
AIC: 197.42
Number of Fisher Scoring iterations: 9
&gt; LR_Predict &lt;- predict(LR_fit,newdata=HT2_TestX,type="response")
&gt; LR_Predict_Bin &lt;- ifelse(LR_Predict&gt;0.5,2,1)
&gt; LR_Accuracy &lt;- sum(LR_Predict_Bin==testY_numeric)/nte
&gt; LR_Accuracy
[1] 0.9732704</pre></div><p class="calibre7">It can be seen from the <a id="id41" class="calibre1"/>summary of the fitted GLM (the output following the line <code class="literal">summary(LR_fit)</code>) that we are having four significant variables <code class="literal">in Age</code>, <code class="literal">TT4</code>, <code class="literal">T4U</code>, and <code class="literal">FTI</code>. Using the <code class="literal">predict</code> function, we apply the fitted model on unknown test cases in <code class="literal">HT2_TestX</code>, compare it with the actuals, and find the accuracy to be 97.33%. Consequently, logistic regression is easily deployed in the R software.</p></div></div></div>

<div class="book" title="Statistical/machine learning models">
<div class="book" title="Neural networks"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec16" class="calibre1"/>Neural networks</h2></div></div></div><p class="calibre7">Logistic regression might appear restricted as it allows only a linear impact of the covariates through<a id="id42" class="calibre1"/> the link function. The linearity <a id="id43" class="calibre1"/>assumption might not hold, and in most practical cases, we don't have enough information to specify the functional form of the nonlinear relationship. Thus, all we know is that there is most likely an unknown nonlinear relationship. Neural networks are the nonlinear generalization of logistic regression, and this involves two important components: hidden neurons and learning rate. We will revise the structure of neural networks first.</p><p class="calibre7">In a neural network, the input variables are considered the first layer of neurons and the output the final and concluding layer of neurons. The structure of a neural network model can be visualized using the R package <code class="literal">NeuralNetTools</code>. Suppose that we have three input variables and two hidden layers, and each contains two hidden neurons. Here, we have a neural network with four layers. The next code segment gives a visualization of a neural network's structure with three input variables, two hidden neurons in two hidden layers, and one output variable:</p><div class="informalexample"><pre class="programlisting">&gt; library(NeuralNetTools) 
&gt; plotnet(rep(0,17),struct=c(3,2,2,1))
&gt; title("A Neural Network with Two Hidden Layers")</pre></div><p class="calibre7">We find the R package <code class="literal">NeuralNetTools</code> very useful in visualizing the structure of a neural network. Neural <a id="id44" class="calibre1"/>networks built using the core R package <code class="literal">nnet</code> can also be visualized using the <code class="literal">NeuralNetTools::plotnet</code> function. The <code class="literal">plotnet</code> function sets up a neural network whose structure consists of three neurons in the first layer, two neurons in each of the second and third layers, and one in the final output layer, through the <code class="literal">struct</code> option. The weights along the arcs are set at zero in <code class="literal">rep(0,17)</code>:</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Neural networks" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Structure of a neural network</p></div></div><p class="calibre11"> </p><p class="calibre7">In the previous diagram, we have four layers of the neural network. The first layer consists of <span class="strong"><strong class="calibre8">B1</strong></span> (the bias), <span class="strong"><strong class="calibre8">I1 (X1)</strong></span>, <span class="strong"><strong class="calibre8">I2 (X2)</strong></span>, and <span class="strong"><strong class="calibre8">I3 (X3)</strong></span>. The second layer consists of three neurons in <span class="strong"><strong class="calibre8">B2</strong></span> (the bias of the first hidden layer), <span class="strong"><strong class="calibre8">H1</strong></span>, and <span class="strong"><strong class="calibre8">H2</strong></span>. Note that the bias <span class="strong"><strong class="calibre8">B2</strong></span> does not receive any input from the first hidden layer. Next, each neuron receives an overall input from each of the neurons of the previous layer, which are <span class="strong"><strong class="calibre8">B1</strong></span>, <span class="strong"><strong class="calibre8">X1</strong></span>, <span class="strong"><strong class="calibre8">X2</strong></span>, and <span class="strong"><strong class="calibre8">X3</strong></span> here. However, <span class="strong"><strong class="calibre8">H1</strong></span> and <span class="strong"><strong class="calibre8">H2</strong></span> of the first hidden layer will receive different aggregated input from <span class="strong"><strong class="calibre8">B1</strong></span>, <span class="strong"><strong class="calibre8">X1</strong></span>, <span class="strong"><strong class="calibre8">X2</strong></span>, and <span class="strong"><strong class="calibre8">X3</strong></span>. Appropriate weights are in action on each of the arcs of the network and it is the weights that form the parameters of the neural networks; that is, the arrival of <span class="strong"><strong class="calibre8">H1</strong></span> (of the first layer) would be like </p><div class="mediaobject"><img src="../images/00029.jpeg" alt="Neural networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7"> and the effective arrival is through a <span class="strong"><em class="calibre9">transfer function</em></span>. A transfer function might be an identity function, sigmoidal function, and so on. Similarly, the arrival at the second neuron of the first layer is </p><div class="mediaobject"><img src="../images/00030.jpeg" alt="Neural networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">. By extension, <span class="strong"><strong class="calibre8">B2</strong></span>, <span class="strong"><strong class="calibre8">H1</strong></span>, and <span class="strong"><strong class="calibre8">H2</strong></span> (of the first layer) will be the input for the second hidden layer, and <span class="strong"><strong class="calibre8">B3</strong></span>, <span class="strong"><strong class="calibre8">H1</strong></span>, and <span class="strong"><strong class="calibre8">H2</strong></span> will be the input for the final output. At each stage of the<a id="id45" class="calibre1"/> neural network, we have weights. The weights need to be determined in such a manner that the difference between predicted output <span class="strong"><strong class="calibre8">O1</strong></span> and the true <span class="strong"><strong class="calibre8">Y1</strong></span> is as small as possible. Note that the logistic regression is a particular case of the neural network as can be seen by directly removing all hidden layers and input layer leads in the output one. The neural network will be fitted for the hypothyroid problem.</p><div class="book" title="Neural network for hypothyroid classification"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch01lvl3sec02" class="calibre1"/>Neural network for hypothyroid classification</h3></div></div></div><p class="calibre7">We use the <code class="literal">nnet</code> function<a id="id46" class="calibre1"/> from the package of the same name to set up the neural network for the hypothyroid classification problem. The formula, training, and test datasets continue as before. The accuracy calculation follows along similar lines to the segment in logistic regression. The fitted neural network is visualized using the <code class="literal">plotnet</code> graphical function from the <code class="literal">NeuralNetTools</code> package:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(12345)
&gt; NN_fit &lt;- nnet(HT2_Formula,data = HT2_Train,size=p,trace=FALSE)
&gt; NN_Predict &lt;- predict(NN_fit,newdata=HT2_TestX,type="class")
&gt; NN_Accuracy &lt;- sum(NN_Predict==HT2_TestY)/nte
&gt; NN_Accuracy
[1] 0.9827044025
&gt; plotnet(NN_fit)
&gt; title("Neural Network for Hypothyroid Classification")</pre></div><p class="calibre7">Here, the accuracy is 98.27%, which is an improvement on the logistic regression model. The visual display of the fitted model is given in the following diagram. We have fixed the seed for the random initialization of the neural network parameters at <code class="literal">12345</code>, using <code class="literal">set.seed(12345)</code>, so that the results are reproducible at the reader's end. This is an interesting case for ensemble modeling. Different initial seeds – which the reader can toy around with – will lead to different accuracies. Sometimes, you will get an accuracy lower than any of the models considered in this section, and at other times you will get the highest accuracy. The choice of seed as arbitrary leads to the important question of which solution is useful. Since the seeds are arbitrary, the question of a good seed or a bad seed does not arise. In this case, if a <a id="id47" class="calibre1"/>model is giving you a higher accuracy, it does not necessarily mean anything:</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="Neural network for hypothyroid classification" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Neural network for the hypothyroid classification</p></div></div><p class="calibre11"> </p></div></div></div>

<div class="book" title="Statistical/machine learning models">
<div class="book" title="Naïve Bayes classifier"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec17" class="calibre1"/>Naïve Bayes classifier</h2></div></div></div><p class="calibre7">The naïve Bayes classifier is a<a id="id48" class="calibre1"/> simplistic implementation based on the Bayes formula. It is based on simple empirical and conditional <a id="id49" class="calibre1"/>probabilities, as evidenced in the actual data. Beyond the simplest assumption of observation independence, we don't have any restrictions in using this model.</p><div class="book" title="Naïve Bayes for hypothyroid classification"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch01lvl3sec03" class="calibre1"/>Naïve Bayes for hypothyroid classification </h3></div></div></div><p class="calibre7">A naïve Bayes classifier is fit<a id="id50" class="calibre1"/> using the <code class="literal">naiveBayes</code> function from the <code class="literal">e1071</code> R package. The prediction and accuracy assessment is carried out using two functions, <code class="literal">predict</code> and <code class="literal">sum</code>:</p><div class="informalexample"><pre class="programlisting">&gt; NB_fit &lt;- naiveBayes(HT2_Formula,data=HT2_Train)
&gt; NB_predict &lt;- predict(NB_fit,newdata=HT2_TestX)
Warning message:
In data.matrix(newdata) : NAs introduced by coercion
&gt; NB_Accuracy &lt;- sum(NB_predict==HT2_TestY)/nte
&gt; NB_Accuracy
[1] 0.9732704403</pre></div><p class="calibre7">The accuracy of the naïve Bayes classifier is 97.33%, which is the same as the logistic regression model and less than the one provided by the neural network. We remark here that it is only a coincidence that the accuracy of this method and logistic regression is the same.</p></div></div></div>

<div class="book" title="Statistical/machine learning models">
<div class="book" title="Decision tree"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch01lvl2sec18" class="calibre1"/>Decision tree</h2></div></div></div><p class="calibre7">Breiman and Quinlan mainly developed decision trees, which have evolved a lot since the 1980s. If the dependent variable is continuous, the decision tree will be a regression tree and if it is categorical <a id="id51" class="calibre1"/>variable, it will be a <a id="id52" class="calibre1"/>classification tree. Of course, we can have a survival tree as well. Decision trees will be the main model that will be the beneficiary of the ensemble technique, as will be seen throughout the book.</p><p class="calibre7">Consider the regression tree given in the following diagram. We can see that there are three input variables, which are <span class="strong"><img src="../images/00032.jpeg" alt="Decision tree" class="calibre15"/></span>, and the output variable is <span class="strong"><em class="calibre9">Y</em></span>. Strictly speaking, a decision tree will not display all the variables used to build the tree. In this tree structure, a decision tree is conventionally displayed upside down. We have four terminal nodes. If the condition <span class="strong"><img src="../images/00033.jpeg" alt="Decision tree" class="calibre15"/></span> is satisfied, we move to the right side of the tree and conclude that the average <span class="strong"><em class="calibre9">Y</em></span> value is 40. If the condition is not satisfied, we move to the left, and check whether <span class="strong"><img src="../images/00034.jpeg" alt="Decision tree" class="calibre15"/></span>. If this condition is not satisfied, we move to the left side of the tree and conclude that the average <span class="strong"><em class="calibre9">Y</em></span> value is 100. Upon the satisfactory meeting of this condition, we move to the right side and then if the categorical variable <span class="strong"><img src="../images/00035.jpeg" alt="Decision tree" class="calibre15"/></span>, the average <span class="strong"><em class="calibre9">Y</em></span> value would be 250, or 10 otherwise. This decision tree can be captured in the form of an equation too, as follows:</p><div class="mediaobject"><img src="../images/00036.jpeg" alt="Decision tree" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00037.jpeg" alt="Decision tree" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00038.jpeg" alt="Decision tree" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Regression tree</p></div></div><p class="calibre11"> </p><p class="calibre7">The statistician Terry Therneau <a id="id53" class="calibre1"/>developed the <code class="literal">rpart</code> R package.</p><div class="book" title="Decision tree for hypothyroid classification"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch01lvl3sec04" class="calibre1"/>Decision tree for hypothyroid classification</h3></div></div></div><p class="calibre7">Using the <code class="literal">rpart</code> function <a id="id54" class="calibre1"/>from the <code class="literal">rpart</code> package, we build a classification tree for the same formula as the earlier partitioned data. The constructed tree can be visualized using the plot function, and the variable name is embossed on the tree with the text function. The equation of the fitted classification tree (see Figure <span class="strong"><em class="calibre9">Classification Tree for Hypothyroid</em></span>) is the following:</p><div class="mediaobject"><img src="../images/00039.jpeg" alt="Decision tree for hypothyroid classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Prediction and accuracy is carried out in a similar way as mentioned earlier:</p><div class="informalexample"><pre class="programlisting">&gt; CT_fit &lt;- rpart(HT2_Formula,data=HT2_Train)
&gt; plot(CT_fit,uniform=TRUE)
&gt; text(CT_fit)
&gt; CT_predict &lt;- predict(CT_fit,newdata=HT2_TestX,type="class")
&gt; CT_Accuracy &lt;- sum(CT_predict==HT2_TestY)/nte
&gt; CT_Accuracy
[1] 0.9874213836</pre></div><div class="mediaobject"><img src="../images/00040.jpeg" alt="Decision tree for hypothyroid classification" class="calibre10"/><div class="caption"><p class="calibre14">Figure 6: Classification tree for Hypothyroid</p></div></div><p class="calibre11"> </p><p class="calibre7">Consequently, the <a id="id55" class="calibre1"/>classification tree gives an accuracy of 98.74%, which is the best of the four models considered thus far. Next, we will consider the final model, support vector machines.</p></div></div></div>

<div class="book" title="Statistical/machine learning models">
<div class="book" title="Support vector machines"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch01lvl2sec19" class="calibre1"/>Support vector machines</h2></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre8">Support vector machines</strong></span>, abbreviated popularly as <span class="strong"><strong class="calibre8">SVM</strong></span>, are an important class of machine learning<a id="id56" class="calibre1"/> techniques. Theoretically, SVM can take <a id="id57" class="calibre1"/>an infinite number of features/covariates and build the appropriate classification or regression SVMs.</p><div class="book" title="SVM for hypothyroid classification"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch01lvl3sec05" class="calibre1"/>SVM for hypothyroid classification </h3></div></div></div><p class="calibre7">The <code class="literal">svm</code> function from the <code class="literal">e1071</code> package will be useful for building an <code class="literal">SVM</code> classifier on the Hypothyroid dataset. Following<a id="id58" class="calibre1"/> the usual practice, we have the following output in the R session:</p><div class="informalexample"><pre class="programlisting">&gt; SVM_fit &lt;- svm(HT2_Formula,data=HT2_Train)
&gt; SVM_predict &lt;- predict(SVM_fit,newdata=HT2_TestX,type="class")
&gt; SVM_Accuracy &lt;- sum(SVM_predict==HT2_TestY)/nte
&gt; SVM_Accuracy
[1] 0.9842767296</pre></div><p class="calibre7">The SVM technique gives us an accuracy of 98.43%, which is the second best of the models set up thus far.</p><p class="calibre7">In the next section, we will run each of the five classification models for the Waveform, German Credit, Iris, and Pima Indians Diabetes problem datasets.</p></div></div></div>
<div class="book" title="The right model dilemma!" id="DB7S1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec12" class="calibre1"/>The right model dilemma!</h1></div></div></div><p class="calibre7">In the previous section, we ran five classification models for the <code class="literal">Hypothyroid</code> dataset. Here, the task is to repeat the exercise for four other datasets. It would be a very laborious task to change the code in<a id="id59" class="calibre1"/> the appropriate places and repeat the exercise four times over. Thus, to circumvent this problem, we will create a new function referred to as <code class="literal">Multiple_Model_Fit</code>. This function will take four arguments: <code class="literal">formula</code>, <code class="literal">train</code>, <code class="literal">testX</code>, and <code class="literal">testY</code>. The four arguments have already been set up for each of the five datasets. The function is then set up in a way that generalizes the steps of the previous section for each of the five models.</p><p class="calibre7">The function proceeds to create a matrix whose first column consists of the model name, while the second column consists of the accuracy. This matrix is returned as the output of this function:</p><div class="informalexample"><pre class="programlisting">&gt; Multiple_Model_Fit &lt;- function(formula,train,testX,testY){
+   ntr &lt;- nrow(train) # Training size
+   nte &lt;- nrow(testX) # Test size
+   p &lt;- ncol(testX)
+   testY_numeric &lt;- as.numeric(testY)
+   
+   # Neural Network
+   set.seed(12345)
+   NN_fit &lt;- nnet(formula,data = train,size=p,trace=FALSE)
+   NN_Predict &lt;- predict(NN_fit,newdata=testX,type="class")
+   NN_Accuracy &lt;- sum(NN_Predict==testY)/nte
+   
+   # Logistic Regressiona
+   LR_fit &lt;- glm(formula,data=train,family = binomial())
+   LR_Predict &lt;- predict(LR_fit,newdata=testX,type="response")
+   LR_Predict_Bin &lt;- ifelse(LR_Predict&gt;0.5,2,1)
+   LR_Accuracy &lt;- sum(LR_Predict_Bin==testY_numeric)/nte
+   
+   # Naive Bayes
+   NB_fit &lt;- naiveBayes(formula,data=train)
+   NB_predict &lt;- predict(NB_fit,newdata=testX)
+   NB_Accuracy &lt;- sum(NB_predict==testY)/nte
+   
+   # Decision Tree
+   CT_fit &lt;- rpart(formula,data=train)
+   CT_predict &lt;- predict(CT_fit,newdata=testX,type="class")
+   CT_Accuracy &lt;- sum(CT_predict==testY)/nte
+   
+   # Support Vector Machine
+   svm_fit &lt;- svm(formula,data=train)
+   svm_predict &lt;- predict(svm_fit,newdata=testX,type="class")
+   svm_Accuracy &lt;- sum(svm_predict==testY)/nte
+   
+   Accu_Mat &lt;- matrix(nrow=5,ncol=2)
+   Accu_Mat[,1] &lt;- c("Neural Network","Logistic Regression","Naive Bayes",
+                 "Decision Tree","Support Vector Machine")
+   Accu_Mat[,2] &lt;- round(c(NN_Accuracy,LR_Accuracy,NB_Accuracy,
+                     CT_Accuracy,svm_Accuracy),4)
+   return(Accu_Mat)
+   
+ }</pre></div><p class="calibre7">
<code class="literal">Multiple_Model_Fit</code> is now applied to the <code class="literal">Hypothyroid</code> dataset, and the results can be seen to be in agreement with the<a id="id60" class="calibre1"/> previous section:</p><div class="informalexample"><pre class="programlisting">&gt; Multiple_Model_Fit(formula=HT2_Formula,train=HT2_Train,
+                    testX=HT2_TestX,
+                    testY=HT2_TestY)
     [,1]                     [,2]    
[1,] "Neural Network"         "0.989" 
[2,] "Logistic Regression"    "0.9733"
[3,] "Naive Bayes"            "0.9733"
[4,] "Decision Tree"          "0.9874"
[5,] "Support Vector Machine" "0.9843"</pre></div><p class="calibre7">The <code class="literal">Multiple_Model_Fit</code> function is then applied to the other four classification datasets:</p><div class="informalexample"><pre class="programlisting">&gt; Multiple_Model_Fit(formula=Waveform_DF_Formula,train=Waveform_DF_Train,
+                    testX=Waveform_DF_TestX,
+                    testY=Waveform_DF_TestY)
     [,1]                     [,2]    
[1,] "Neural Network"         "0.884" 
[2,] "Logistic Regression"    "0.8873"
[3,] "Naive Bayes"            "0.8601"
[4,] "Decision Tree"          "0.8435"
[5,] "Support Vector Machine" "0.9171"
&gt; Multiple_Model_Fit(formula=GC2_Formula,train=GC2_Train,
+                    testX=GC2_TestX,
+                    testY =GC2_TestY )
     [,1]                     [,2]    
[1,] "Neural Network"         "0.7252"
[2,] "Logistic Regression"    "0.7572"
[3,] "Naive Bayes"            "0.8083"
[4,] "Decision Tree"          "0.7061"
[5,] "Support Vector Machine" "0.754" 
&gt; Multiple_Model_Fit(formula=ir2_Formula,train=ir2_Train,
+                    testX=ir2_TestX,
+                    testY=ir2_TestY)
     [,1]                     [,2]
[1,] "Neural Network"         "1" 
[2,] "Logistic Regression"    "1" 
[3,] "Naive Bayes"            "1" 
[4,] "Decision Tree"          "1" 
[5,] "Support Vector Machine" "1"  
&gt; Multiple_Model_Fit(formula=PID_Formula,train=PimaIndiansDiabetes_Train,
+                    testX=PimaIndiansDiabetes_TestX,
+                    testY=PimaIndiansDiabetes_TestY)
     [,1]                     [,2]    
[1,] "Neural Network"         "0.6732"
[2,] "Logistic Regression"    "0.751" 
[3,] "Naive Bayes"            "0.7821"
[4,] "Decision Tree"          "0.7588"
[5,] "Support Vector Machine" "0.7665"</pre></div><p class="calibre7">The results for each of the datasets are summarized in the following table:</p><div class="mediaobject"><img src="../images/00041.jpeg" alt="The right model dilemma!" class="calibre10"/><div class="caption"><p class="calibre14">Table 1: Accuracy of five models for five datasets</p></div></div><p class="calibre11"> </p><p class="calibre7">The <code class="literal">iris</code> dataset is a straightforward and simplistic problem, and therefore each of the five models gives us 100% accuracy on the test data. This dataset will not be pursued any further.</p><p class="calibre7">For each dataset, we highlight the highest accuracy cell in grey, and highlight the next highest in yellow.</p><p class="calibre7">Here is the modeling dilemma. The<a id="id61" class="calibre1"/> naïve Bayes method turns out the best for the <code class="literal">German</code> and <code class="literal">Pima Indian Diabetes</code> datasets. The decision tree gives the highest accuracy for the <code class="literal">Hypothyroid</code> dataset, while SVM gives the best results for <code class="literal">Waveform</code>. The runner-up place is secured twice by logistic regression and twice by SVM. However, we also know that, depending <a id="id62" class="calibre1"/>on the initial seeds and maybe the number of hidden neurons, the neural networks are also expected to perform the best for some datasets. We then also have to consider whether the results will turn out differently for different partitions.</p><p class="calibre7">It is in such practical scenarios we would prefer to have a single approach that ensures reasonable properties. With the <code class="literal">Hypothyroid</code> dataset, the accuracy for each of the models is 97% or higher, and one might not go wrong with any of the models. However, in the <code class="literal">German</code> and <code class="literal">Pima Indian Diabetes</code> problems, the maximum accuracy is 80% and 78%, respectively. It would then be better if we can make good use of all the models and build a single unified one with increased accuracy.</p></div>
<div class="book" title="An ensemble purview"><div class="book" id="E9OE2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec13" class="calibre1"/>An ensemble purview</h1></div></div></div><p class="calibre7">The <code class="literal">caret</code> R package is core to <a id="id63" class="calibre1"/>ensemble machine learning methods. It provides a large framework and we can also put different statistical and machine learning models together to create an ensemble. For the recent version of the package on the author's laptop, the <code class="literal">caret</code> package provides access to the following models:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; names(getModelInfo())
  [1] "ada"                 "AdaBag"              "AdaBoost.M1" 
  [4] "adaboost"            "amdai"               "ANFIS" 
  [7] "avNNet"              "awnb"                "awtan"        
     
[229] "vbmpRadial"          "vglmAdjCat"          "vglmContRatio 
[232] "vglmCumulative"      "widekernelpls"       "WM" 
[235] "wsrf"                "xgbLinear"           "xgbTree" 
[238] "xyf"               </pre></div><p class="calibre7">Depending on your requirements, you can choose any combination of these 238 models. The authors of the package keep on updating this list. It is to be noted that not all models will be available in the <code class="literal">caret</code> package, and that it is a platform that facilitates the ensembling of these methods. Consequently, if you choose a model such as <code class="literal">ANFIS</code>, and the R package <code class="literal">frbs</code> contains<a id="id64" class="calibre1"/> this function, which is not available on your machine, then caret will display a message on the terminal as indicated in the following snippet:</p><div class="mediaobject"><img src="../images/00042.jpeg" alt="An ensemble purview" class="calibre10"/><div class="caption"><p class="calibre14">Figure 7: Caret providing a message to install the required R package</p></div></div><p class="calibre11"> </p><p class="calibre7">You need to key in the number <code class="literal">1</code> and continue. The package will be installed and loaded, and the program will continue. It is good to know the host of options for ensemble methods. A brief method for stack ensembling analytical models is provided here, and the details will unfold later in the book.</p><p class="calibre7">For the <code class="literal">Hypothyroid</code> dataset, we had a high accuracy of an average of 98% between the five models. The <code class="literal">Waveform</code> dataset saw an average accuracy of approximately 88%, while the average for <code class="literal">German</code> Credit data is 75%. We will try to increase the accuracy for this dataset. The accuracy improvement will be attempted using three models: naïve Bayes, logistic regression, and classification tree. First, we need to partition the data into three parts: train, test, and stack:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/GC2.RData")
&gt; set.seed(12345)
&gt; Train_Test_Stack &lt;- sample(c("Train","Test","Stack"),nrow(GC2),replace = TRUE,prob = c(0.5,0.25,0.25))
&gt; GC2_Train &lt;- GC2[Train_Test_Stack=="Train",]
&gt; GC2_Test &lt;- GC2[Train_Test_Stack=="Test",]
&gt; GC2_Stack &lt;- GC2[Train_Test_Stack=="Stack",]The dependent and independent variables will be marked next in character vectors for programming convenient. 

&gt; # set label name and Exhogenous
&gt; Endogenous &lt;- 'good_bad'
&gt; Exhogenous &lt;- names(GC2_Train)[names(GC2_Train) != Endogenous]</pre></div><p class="calibre7">The model will be built on the training data first and accuracy will be assessed using the metric of Area Under Curve, the curve being the ROC. The control parameters will be set up first and the three models, naïve Bayes, classification tree, and logistic regression, will be created using the training dataset:</p><div class="informalexample"><pre class="programlisting">&gt; # Creating a caret control object for the number of 
&gt; # cross-validations to be performed
&gt; myControl &lt;- trainControl(method='cv', number=3, returnResamp='none')
&gt; # train all the ensemble models with GC2_Train
&gt; model_NB &lt;- train(GC2_Train[,Exhogenous], GC2_Train[,Endogenous], 
+                    method='naive_bayes', trControl=myControl)
&gt; model_rpart &lt;- train(GC2_Train[,Exhogenous], GC2_Train[,Endogenous], 
+                      method='rpart', trControl=myControl)
&gt; model_glm &lt;- train(GC2_Train[,Exhogenous], GC2_Train[,Endogenous], 
+                        method='glm', trControl=myControl)</pre></div><p class="calibre7">Predictions for the test and stack blocks <a id="id65" class="calibre1"/>are carried out next. We store the predicted probabilities along the test and stack data frames:</p><div class="informalexample"><pre class="programlisting">&gt; # get predictions for each ensemble model for two last datasets
&gt; # and add them back to themselves
&gt; GC2_Test$NB_PROB &lt;- predict(object=model_NB, GC2_Test[,Exhogenous],
+                              type="prob")[,1]
&gt; GC2_Test$rf_PROB &lt;- predict(object=model_rpart, GC2_Test[,Exhogenous],
+                             type="prob")[,1]
&gt; GC2_Test$glm_PROB &lt;- predict(object=model_glm, GC2_Test[,Exhogenous],
+                                  type="prob")[,1]
&gt; GC2_Stack$NB_PROB &lt;- predict(object=model_NB, GC2_Stack[,Exhogenous],
+                               type="prob")[,1]
&gt; GC2_Stack$rf_PROB &lt;- predict(object=model_rpart, GC2_Stack[,Exhogenous],
+                              type="prob")[,1]
&gt; GC2_Stack$glm_PROB &lt;- predict(object=model_glm, GC2_Stack[,Exhogenous],
+                                   type="prob")[,1]</pre></div><p class="calibre7">The ROC is an important<a id="id66" class="calibre1"/> measure for model assessments. The higher the area under the ROC, the better the model would be. Note that these measures, or any other measure, will not be the same as the models fitted earlier since the data has changed:</p><div class="informalexample"><pre class="programlisting">&gt; # see how each individual model performed on its own
&gt; AUC_NB &lt;- roc(GC2_Test[,Endogenous], GC2_Test$NB_PROB )
&gt; AUC_NB$auc
Area under the curve: 0.7543
&gt; AUC_rf &lt;- roc(GC2_Test[,Endogenous], GC2_Test$rf_PROB )
&gt; AUC_rf$auc
Area under the curve: 0.6777
&gt; AUC_glm &lt;- roc(GC2_Test[,Endogenous], GC2_Test$glm_PROB )
&gt; AUC_glm$auc
Area under the curve: 0.7446</pre></div><p class="calibre7">For the <code class="literal">test</code> dataset, we can see that the area under curve for the naïve Bayes, classification tree, and logistic regression are respectively <code class="literal">0.7543</code>, <code class="literal">0.6777</code>, and <code class="literal">0.7446</code>. If we put the predicted values together in some format, and that leads to an increase in the accuracy, the purpose of the ensemble technique has been accomplished. As such, we consider the new predicted probabilities under the three models and append them to the stacked data frame. These three columns will now be treated as new input vectors. We then build a naïve Bayes model, an arbitrary choice, and you can try any other model (not necessarily restricted to one of these) for the stacked data frame. The AUC can then be predicted and calculated:</p><div class="informalexample"><pre class="programlisting">&gt; # Stacking it together
&gt; Exhogenous2 &lt;- names(GC2_Stack)[names(GC2_Stack) != Endogenous]
&gt; Stack_Model &lt;- train(GC2_Stack[,Exhogenous2], GC2_Stack[,Endogenous], 
+                      method='naive_bayes', trControl=myControl)
&gt; Stack_Prediction &lt;- predict(object=Stack_Model,GC2_Test[,Exhogenous2],type="prob")[,1]
&gt; Stack_AUC &lt;- roc(GC2_Test[,Endogenous],Stack_Prediction)
&gt; Stack_AUC$auc
Area under the curve: 0.7631</pre></div><p class="calibre7">The AUC for the stacked data observations is higher than any of the earlier models, which is an improvement.</p><p class="calibre7">A host of questions should arise for the critical thinker. Why should this technique work? Will it lead to improvisations under all possible cases? If yes, will simply adding new model predictions lead to further improvements? If no, how does one pick the base models so that we can be reasonably assured of improvisations? What are the restrictions on the choice of models? We will provide solutions to most of these questions throughout this book. In the next section, we will quickly look at some useful statistical tests that will aid the assessment of model performance.</p></div>

<div class="book" title="Complementary statistical tests"><div class="book" id="F8902-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec14" class="calibre1"/>Complementary statistical tests</h1></div></div></div><p class="calibre7">Here, a model is selected over another plausible one. The accuracy of one model seems higher than the other. The <span class="strong"><strong class="calibre8">area under curve</strong></span> (<span class="strong"><strong class="calibre8">AUC</strong></span>) of the ROC of a model is greater than that of another. However, it is not appropriate to base the conclusion on pure numbers only. It is important to<a id="id67" class="calibre1"/> conclude whether the numbers hold significance from the point of view of statistical inference. In the analytical world, it is pivotal that we <a id="id68" class="calibre1"/>make use of statistical tests whenever they are available to validate claims/hypotheses. A reason for using statistical tests is that probability can be highly counterintuitive, and what appears on the surface might not be the case upon closer inspection, after incorporating the chance variation. For instance, if a fair coin is tossed 100 times, it is imprudent to think that the number of heads must be exactly 50. Hence, if a fair coin shows up 45 heads, we need to incorporate the chance variation that the number of heads can be less than 50 too. Caution must be exerted all the while when we are dealing with uncertain data. A few examples are in order here. Two variables might appear to be independent of each other, and the correlation might also be nearly equal to zero. However, applying the correlation test might result in the conclusion that the correlation is not significantly zero. Since we will be sampling and resampling a lot in this text, we will look at related tests.</p></div>

<div class="book" title="Complementary statistical tests">
<div class="book" title="Permutation test"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec20" class="calibre1"/>Permutation test</h2></div></div></div><p class="calibre7">Suppose that we have two processes, A and B, and the variances of these two processes are known to be equal, though <a id="id69" class="calibre1"/>unknown. Three independent observations from process A result in yields of 18, 20, and 22, while three independent observations from process B gives yields of 24, 26, and 28. Under the assumption that the yield <a id="id70" class="calibre1"/>follows a normal distribution, we would like to test whether the means of processes A and B are the same. This is a suitable case for applying the t-test, since the number of observations is smaller. An application of the <code class="literal">t.test</code> function shows that the two means are different to each other, and this intuitively appears to be the case.</p><p class="calibre7">Now, the assumption under the null hypothesis is that the means are equal, and that the variance is unknown and assumed to be equal under the two processes. Consequently, we have a genuine reason to believe that the observations from process A might well have occurred in process B too, and vice versa. We can therefore swap one observation in process B with process A, and recompute the t-test. The process can be repeated for all possible permutations of the two samples. In general, if we have m samples from population 1 and n samples from population 2, we can have </p><div class="mediaobject"><img src="../images/00043.jpeg" alt="Permutation test" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7"> different samples and as many tests. An overall test can be based on such<a id="id71" class="calibre1"/> permutation samples and such tests are called <span class="strong"><strong class="calibre8">permutation tests</strong></span>.</p><p class="calibre7">For process A and B <a id="id72" class="calibre1"/>observations, we will first apply the t-test and then the permutation test. The <code class="literal">t.test</code> is available in the core <code class="literal">stats</code> package and the permutation t-test is taken from the <code class="literal">perm</code> package:</p><div class="informalexample"><pre class="programlisting">&gt; library(perm)
&gt; x &lt;- c(18,20,22); y &lt;- c(24,26,28)
&gt; t.test(x,y,var.equal = TRUE)
Two Sample t-test
data:  x and y
t = -3.6742346, df = 4, p-value = 0.02131164
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -10.533915871  -1.466084129
sample estimates:
mean of x mean of y 
       20        26 </pre></div><p class="calibre7">The smaller p-value suggests that the means of processes A and B are not equal. Consequently, we now apply the permutation test <code class="literal">permTS</code> from the <code class="literal">perm</code> package:</p><div class="informalexample"><pre class="programlisting">&gt; permTS(x,y)
Exact Permutation Test (network algorithm)
data:  x and y
p-value = 0.1
alternative hypothesis: true mean x - mean y is not equal to 0
sample estimates:
mean x - mean y 
             -6 </pre></div><p class="calibre7">The p-value is now at 0.1, which means that the permutation test concludes that the means of the processes are equal. Does this mean that the permutation test will always lead to this conclusion, contradicting the t-test? The answer is given in the next code segment:</p><div class="informalexample"><pre class="programlisting">&gt; x2 &lt;- c(16,18,20,22); y2 &lt;- c(24,26,28,30)
&gt; t.test(x2,y2,var.equal = TRUE)
Two Sample t-test
data:  x2 and y2
t = -4.3817805, df = 6, p-value = 0.004659215
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -12.46742939  -3.53257061
sample estimates:
mean of x mean of y 
       19        27 
&gt; permTS(x2,y2)
Exact Permutation Test (network algorithm)
data:  x2 and y2
p-value = 0.02857143
alternative hypothesis: true mean x2 - mean y2 is not equal to 0
sample estimates:
mean x2 - mean y2 
               -8 </pre></div></div></div>

<div class="book" title="Complementary statistical tests">
<div class="book" title="Chi-square and McNemar test"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec21" class="calibre1"/>Chi-square and McNemar test</h2></div></div></div><p class="calibre7">We had five models for the<a id="id73" class="calibre1"/> hypothyroid test. We then calculated the accuracy and were satisfied with the numbers. Let's first look at the number of <a id="id74" class="calibre1"/>errors that the fitted model makes. We have 636 observations in the test partition and 42 of them test positive for the<a id="id75" class="calibre1"/> hypothyroid problem. Note that if we mark all the patients as<a id="id76" class="calibre1"/> negative, we would be getting an accuracy of <span class="strong"><em class="calibre9">1-42/636 = 0.934</em></span>, or about 93.4%. Using the table function, we pit the actuals against the predicted values and see how often the fitted model goes wrong. We remark here that identifying the hypothyroid cases as the same and the negative cases as negative is the correct prediction, while marking the hypothyroid case as negative and vice versa leads to errors. For each model, we look at the misclassification errors:</p><div class="informalexample"><pre class="programlisting">&gt; table(LR_Predict_Bin,testY_numeric)
              testY_numeric
LR_Predict_Bin   1   2
             1  32   7
             2  10 587
&gt; table(NN_Predict,HT2_TestY)
             HT2_TestY
NN_Predict    hypothyroid negative
  hypothyroid          41       22
  negative              1      572
&gt; table(NB_predict,HT2_TestY)
             HT2_TestY
NB_predict    hypothyroid negative
  hypothyroid          33        8
  negative              9      586
&gt; table(CT_predict,HT2_TestY)
             HT2_TestY
CT_predict    hypothyroid negative
  hypothyroid          38        4
  negative              4      590
&gt; table(SVM_predict,HT2_TestY)
             HT2_TestY
SVM_predict   hypothyroid negative
  hypothyroid          34        2
  negative              8      592</pre></div><p class="calibre7">From the misclassification table, we can see that the neural network identifies 41 out of the 42 cases of hypothyroid <a id="id77" class="calibre1"/>correctly, but it identifies way more cases of hypothyroid incorrectly too. The question that arises is whether the correct predictions of the fitted models only occur by chance, or <a id="id78" class="calibre1"/>whether they depend on truth and can be explained. To test this, in the hypotheses framework we would like to test whether the actuals and predicted values of the actuals are independent of or dependent on each other. Technically, the null hypothesis is that the prediction is independent of the actual, and if a model explains<a id="id79" class="calibre1"/> the truth, the null hypothesis must be rejected. We should conclude that the fitted model predictions depend on the truth. We<a id="id80" class="calibre1"/> deploy two solutions here, the chi-square test and the McNemar test:</p><div class="informalexample"><pre class="programlisting">&gt; chisq.test(table(LR_Predict_Bin,testY_numeric))
Pearson's Chi-squared test with Yates' continuity correction
data:  table(LR_Predict_Bin, testY_numeric)
X-squared = 370.53501, df = 1, p-value &lt; 0.00000000000000022204
&gt; chisq.test(table(NN_Predict,HT2_TestY))
Pearson's Chi-squared test with Yates' continuity correction
data:  table(NN_Predict, HT2_TestY)
X-squared = 377.22569, df = 1, p-value &lt; 0.00000000000000022204
&gt; chisq.test(table(NB_predict,HT2_TestY))
Pearson's Chi-squared test with Yates' continuity correction
data:  table(NB_predict, HT2_TestY)
X-squared = 375.18659, df = 1, p-value &lt; 0.00000000000000022204
&gt; chisq.test(table(CT_predict,HT2_TestY))
Pearson's Chi-squared test with Yates' continuity correction
data:  table(CT_predict, HT2_TestY)
X-squared = 498.44791, df = 1, p-value &lt; 0.00000000000000022204
&gt; chisq.test(table(SVM_predict,HT2_TestY))
Pearson's Chi-squared test with Yates' continuity correction
data:  table(SVM_predict, HT2_TestY)
X-squared = 462.41803, df = 1, p-value &lt; 0.00000000000000022204
&gt; mcnemar.test(table(LR_Predict_Bin,testY_numeric))
McNemar's Chi-squared test with continuity correction
data:  table(LR_Predict_Bin, testY_numeric)
McNemar's chi-squared = 0.23529412, df = 1, p-value = 0.6276258
&gt; mcnemar.test(table(NN_Predict,HT2_TestY))
McNemar's Chi-squared test with continuity correction
data:  table(NN_Predict, HT2_TestY)
McNemar's chi-squared = 17.391304, df = 1, p-value = 0.00003042146
&gt; mcnemar.test(table(NB_predict,HT2_TestY))
McNemar's Chi-squared test with continuity correction
data:  table(NB_predict, HT2_TestY)
McNemar's chi-squared = 0, df = 1, p-value = 1
&gt; mcnemar.test(table(CT_predict,HT2_TestY))
McNemar's Chi-squared test
data:  table(CT_predict, HT2_TestY)
McNemar's chi-squared = 0, df = 1, p-value = 1
&gt; mcnemar.test(table(SVM_predict,HT2_TestY))
McNemar's Chi-squared test with continuity correction
data:  table(SVM_predict, HT2_TestY)
McNemar's chi-squared = 2.5, df = 1, p-value = 0.1138463</pre></div><p class="calibre7">The answer provided by<a id="id81" class="calibre1"/> the chi-square tests clearly shows that the<a id="id82" class="calibre1"/> predictions of each fitted model is not<a id="id83" class="calibre1"/> down to chance. It also shows that the prediction of hypothyroid cases, as <a id="id84" class="calibre1"/>well as the negative cases, is expected of the fitted models. The interpretation of and conclusions from the McNemar's test is left to the reader. The final important measure in classification problems is the ROC curve, which is considered next.</p></div></div>

<div class="book" title="Complementary statistical tests">
<div class="book" title="ROC test"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec22" class="calibre1"/>ROC test</h2></div></div></div><p class="calibre7">The ROC curve is an<a id="id85" class="calibre1"/> important improvement on the false <a id="id86" class="calibre1"/>positive and true negative measures of model performance. For a detailed explanation, refer to Chapter 9 of Tattar et al. (2017). The ROC curve basically plots the true positive rate against the false positive rate, and we measure the AUC for the fitted model.</p><p class="calibre7">The main goal that the ROC test attempts to achieve is the following. Suppose that Model 1 gives an AUC of 0.89 and Model 2 gives 0.91. Using the simple AUC criteria, we outright conclude that Model 2 is better than Model 1. However, an important question that arises is whether 0.91 is significantly higher than 0.89. The <code class="literal">roc.test</code>, from the <code class="literal">pROC</code> R package, provides the answer here. For the neural network and classification tree, the following R segment gives the required answer:</p><div class="informalexample"><pre class="programlisting">&gt; library(pROC)
&gt; HT_NN_Prob &lt;- predict(NN_fit,newdata=HT2_TestX,type="raw")
&gt; HT_NN_roc &lt;- roc(HT2_TestY,c(HT_NN_Prob))
&gt; HT_NN_roc$auc
Area under the curve: 0.9723826
&gt; HT_CT_Prob &lt;- predict(CT_fit,newdata=HT2_TestX,type="prob")[,2]
&gt; HT_CT_roc &lt;- roc(HT2_TestY,HT_CT_Prob)
&gt; HT_CT_roc$auc
Area under the curve: 0.9598765
&gt; roc.test(HT_NN_roc,HT_CT_roc)
	DeLong's test for two correlated ROC curves
data:  HT_NN_roc and HT_CT_roc
Z = 0.72452214, p-value = 0.4687452
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
 AUC of roc1  AUC of roc2 
0.9723825557 0.9598765432 </pre></div><p class="calibre7">Since the p-value is very large, we <a id="id87" class="calibre1"/>conclude that the AUC for the two models is not significantly different.</p><p class="calibre7">Statistical tests are vital and<a id="id88" class="calibre1"/> we recommend that they be used whenever suitable. The concepts highlighted in this chapter will be drawn on in more detail in the rest of the book.</p></div></div>
<div class="book" title="Summary" id="G6PI1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec15" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">The chapter began with an introduction to some of the most important datasets that will be used in the rest of the book. The datasets covered a range of analytical problems including classification, regression, time series, survival, clustering, and a dataset in which identifying an outlier is important. Important families of classification models were then introduced in the statistical/machine learning models section. Following the introduction of a variety of models, we immediately saw the shortcoming, in that we don't have a model for all seasons. Model performance varies from dataset to dataset. Depending on the initialization, the performance of certain models (such as neural networks) is affected. Consequently, there is a need to find a way to ensure that the models can be improved upon in most scenarios.</p><p class="calibre7">This paves the way for the ensemble method, which forms the title of this book. We will elaborate on this method in the rest of the book. This chapter closed with quick statistical tests that will help in carrying out model comparisons. Resampling forms the core of ensemble methods, and we will look at the important jackknife and bootstrap methods in the next chapter.</p></div></body></html>