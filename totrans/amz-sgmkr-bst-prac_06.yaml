- en: 'Chapter 5: Centralized Feature Repository with Amazon SageMaker Feature Store'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's begin with the basic questions – what is a feature store and why is it
    necessary? A feature store is a repository that persists engineered features.
    A lot of time goes into feature engineering, sometimes involving multi-step data
    processing pipelines executed over hours of compute time. ML models depend on
    these engineered features that often come from a variety of data sources. A feature
    store accelerates this process by reducing repetitive data processing that is
    required to convert raw data into features. A feature store not only allows you
    to share engineered features during model-building activities, but also allows
    consistency in using engineered features for inference.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon SageMaker Feature Store** is a managed repository with capabilities
    to store, update, retrieve, and share features. SageMaker Feature Store provides
    the ability to reuse the engineered features in two different scenarios. First,
    the features can be shared between the training and inference phases of a single
    ML project resulting in consistent model inputs and reduced training-serving skew.
    Second, features from SageMaker'
  prefs: []
  type: TYPE_NORMAL
- en: Feature Store can also be shared across multiple ML projects, leading to improved
    data scientist productivity.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to use Amazon SageMaker Feature
    Store capabilities and apply best practices to implement solutions to address
    the challenges of reducing data processing time and architecting features for
    near real-time ML inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of Amazon SageMaker Feature Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating reusable features to reduce feature inconsistencies and inference latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing solutions for near real-time ML predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039), *Data Science Environments*,
    which provides a walk-through of the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter05](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter05).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Feature Store essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn the basic terminology and capabilities of Amazon
    SageMaker Feature Store. Amazon SageMaker Feature Store provides a centralized
    repository with capabilities to store, update, retrieve, and share features. Scalable
    storage and near real-time feature retrieval are at the heart of Amazon SageMaker
    Feature Store. Utilizing Amazon SageMaker Feature Store involves three high-level
    steps, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – High-level steps with Amazon SageMaker Feature Store'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – High-level steps with Amazon SageMaker Feature Store
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what is involved in each of these steps in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Creating feature groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Amazon SageMaker Feature Store, features are stored in a collection called
    a `RecordIdentifier` value. Every record belonging to a feature group will use
    the same feature as `RecordIdentifier`. For example, the record identifier for
    the feature store created for the weather data could be `parameter_id` or `location_id`.
    Think of `RecordIdentifier` as a primary key for the feature group. Using this
    primary key, you can query feature groups for the fast lookup of features. It's
    also important to note that each record of a feature group must, at a minimum,
    contain a `RecordIdentifier` and an event time feature. The event time feature
    is identified by `EventTimeFeatureName` when a feature group is set up. When a
    feature record is ingested into a feature group, SageMaker adds three features
    – `is_deleted`, `api_invocation` time, and `write_time` – for each feature record.
    `is_deleted` is used to manage the deletion of records, `api_invocation_time`
    is the time when the API call is invoked to write a record to a feature store,
    and `write_time` is the time when the feature record is persisted to the offline
    store.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.2* shows a high-level view of how a feature store is structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Amazon SageMaker feature store structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Amazon SageMaker feature store structure
  prefs: []
  type: TYPE_NORMAL
- en: While each feature group is managed and scaled independently, you can search
    and discover features from multiple feature groups as long as the appropriate
    access is in place.
  prefs: []
  type: TYPE_NORMAL
- en: When you create a feature store group with SageMaker, you can choose to enable
    an offline store, online store, or both. When both online and offline stores are
    enabled, the service replicates the online store contents into the offline store
    maintained in Amazon S3\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code blocks show the process of creating a feature store:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First define the feature group name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create the feature definitions that capture the feature name and the
    type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the record identifier feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, create the feature group using the `create()` API, which, by default,
    creates a feature group with an offline store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To enable an online store in addition to an offline store, use `enable_online_store`,
    as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create a feature group with only an online store enabled, set `s3_uri` to
    `False`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that you can also create a feature group using **SageMaker Studio**. Once
    feature groups are created either using the APIs or SageMaker Studio, you can
    view them along with their status in SageMaker Studio. *Figure 5.3* shows a list
    of feature groups in SageMaker Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Feature groups list in SageMaker Studio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Feature groups list in SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up the feature group creation discussion, the following table summarizes
    the differences between the online and offline feature stores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Comparison of online and offline feature stores'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Comparison of online and offline feature stores
  prefs: []
  type: TYPE_NORMAL
- en: Now that you can create feature groups in the feature store, let's take a look
    at how to populate them.
  prefs: []
  type: TYPE_NORMAL
- en: Populating feature groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After creating the feature groups, you will populate them with features. You
    can ingest features into a feature group using either **batch ingestion** or **streaming
    ingestion**, as shown in *Figure 5.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Ingesting features into feature groups'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Ingesting features into feature groups
  prefs: []
  type: TYPE_NORMAL
- en: To ingest features into the feature store, you create a feature pipeline that
    can populate the feature store. A `PutRecord` API call is the core SageMaker API
    for ingesting features. This is used for both online and offline feature stores
    as well as ingesting through batch or streaming methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows the usage of the `PutRecord` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use a wrapper API, `fg.ingest`, which takes in a pandas `dataframe`
    as input and allows you to configure multiple workers and processes to ingest
    features in parallel. The following code block shows how to use the `ingest()`
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For batch ingestion, you can author features (for example, using `PutRecord`
    API call. When ingesting records to the online feature store, you maintain only
    the latest feature values for a given record identifier. Historical values are
    only maintained in the replicated offline store if the feature group is configured
    for both online and offline stores. *Figure 5.6* outlines the methods to ingest
    features as they relate to the online and offline feature stores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Ingesting feature store records'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Ingesting feature store records
  prefs: []
  type: TYPE_NORMAL
- en: 'With the ingestion APIs in hand, let''s take a look at a generic batch ingestion
    architecture. *Figure 5.7* shows the architecture for batch ingestion with **Amazon
    SageMaker Processing**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Batch ingestion with SageMaker Processing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Batch ingestion with SageMaker Processing
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the high-level steps involved in the batch ingestion architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Bulk raw data is available in an S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Amazon SageMaker Processing job takes raw data as input and applies feature
    engineering techniques to the data. The processing job can be configured to run
    on a distributed cluster of instances to process data at scale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The processing job also ingests the engineered features ingested into the online
    store of the feature group, using the `PutRecord` API. Features are then automatically
    replicated to the offline store of the feature group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Features from the offline store can then be used for training other models and
    by other data science teams to address a wide variety of other use cases. Features
    from the online store can be used for feature lookup during real-time predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that if the feature store used in this architecture is offline only, the
    processing job can directly write into the offline store using the `PutRecord`
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at a possible streaming ingestion architecture in
    *Figure 5.8*. This should look very similar to batch ingestions, except instead
    of using a processing job, you use a single compute instance or an **AWS Lambda
    function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Streaming ingestion with AWS Lambda'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – Streaming ingestion with AWS Lambda
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the high-level steps involved in the streaming ingestion architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Raw data lands in an S3 bucket, which triggers an AWS Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Lambda function processes data and inserts features into the online store
    of the feature group, using the `PutRecord` API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Features are then automatically replicated to the offline store of the feature
    group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Features from the offline store can then be used for training other models and
    by other data science teams to address a wide variety of other use cases. Features
    from the online store can be used for feature lookup during real-time predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In addition to using the ingestion APIs to populate the offline store, you
    can populate the underlying S3 bucket directly. If you don''t have a need for
    real-time inference and have huge volumes of historical feature data (terabytes
    or even hundreds of gigabytes) that you want to migrate to an offline feature
    store to be used for training models, you can directly upload them to the underlying
    S3 bucket. To do this effectively, it is important to understand the S3 folder
    structure of the offline bucket. Feature groups in the offline store are organized
    in the structure `s3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Also note that, when you use ingestion APIs, the features `isdeleted`, `api_invocation_time`,
    and `write-time` are included automatically in the feature record, but when you
    write directly to the offline store, you are responsible for including them.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving features from feature groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once feature groups are populated, to retrieve features from the feature store,
    there are two APIs available – `get_record` and `batch_get_record`. The following
    code block shows retrieving a single record from a feature group using the `get_record`
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the following code shows retrieving multiple records from one or
    more feature groups using the `batch_get_record` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The response from the code block should look similar to the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_record` and `batch_get_record` APIs should be used with online stores.
    Additionally, since the underlying storage for an offline store is an S3 bucket,
    you can query the offline store directly using Athena or other ways of accessing
    S3\. The following code shows a sample Athena query that retrieves all feature
    records directly from the S3 bucket supporting the offline store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For the dataset used in this book, we will use two feature groups – location
    and weather data. The location feature group will have `location_id` as the record
    identifier and capture features related to the location such as the city name. The
    weather data feature group will also have `location_id` as the record identifier
    and capture weather quality measurements such as pm25. This allows us to use the
    feature groups across multiple ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: For example, features from both location and weather data feature groups are
    used for a regression model to predict future weather measurements for a given
    location. On the other hand, features from the weather data feature group can
    also be used for a clustering model to find stations with similar measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The example notebook provides a walk-through of the key Amazon SageMaker Feature
    Store APIs for creating a feature group, ingesting features into feature groups,
    and retrieving features from a feature group. To see all the feature store capabilities
    in action, we recommend that you execute the sample notebook in the data science
    environment you set up in [*Chapter 2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039),
    *Data Science Environments*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/feature_store_apis.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/feature_store_apis.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned the capabilities of SageMaker Feature Store, in the
    next two sections, you will learn how to use these capabilities to solve feature
    design challenges that data scientists and organizations face.
  prefs: []
  type: TYPE_NORMAL
- en: Creating reusable features to reduce feature inconsistencies and inference latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the challenges data scientists face is the long data processing time
    – hours and sometimes days – necessary for preparing features to be used for ML
    training. Additionally, the data processing steps applied in feature engineering
    need to be applied to the inference requests during prediction time, which increases
    the inference latency. Each data science team will need to spend this data processing
    time even when they use the same raw data for different models. In this section,
    we will discuss best practices to address these challenges by using Amazon SageMaker
    Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: For use cases that require low latency features for inference, an online feature
    store should be configured, and it's generally recommended to enable both the
    online and offline feature store. A feature store enabled with both online and
    offline stores allows you to reuse the same feature values for the training and
    inference phases. This configuration reduces the inconsistencies between the two
    phases and minimizes training and inference skew. In this mode, to populate the
    store, ingest features into the online store either using batch or streaming.
  prefs: []
  type: TYPE_NORMAL
- en: As you ingest features into an online store, SageMaker automatically replicates
    feature values to an offline store, continuously appending the latest values.
    It's important to note that for the online feature store, only the most current
    feature record is maintained and the `PutRecord` API is always processed as `insert`/`upsert`.
    This is key because if you need to update a feature record, the process to do
    so is to re-insert or overlay the existing record. This is to allow the retrieval
    of features with the minimum possible latency for inference use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Although the online feature store maintains only the latest record, the offline
    store will provide a full history of feature values over time. Records will stay
    in the offline store until they are explicitly removed. As a result, you should
    establish a process to prune unnecessary records in the offline feature store
    using the standard mechanisms provided for S3 archival.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The example notebook from the GitHub repository shows the end-to-end flow of
    creating a feature store, ingesting features, retrieving features, and further
    using the features for training the model, deploying the model, and using the
    features from the feature store during inference: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH04/feature_store_train_deploy_models.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH04/feature_store_train_deploy_models.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Another best practice is to set up standards for versioning features. As features
    evolve, it is important to keep track of feature versions. Consider versioning
    at two levels – versions of the feature group itself and versions of features
    within a feature group. You need to create a new version of the feature group
    for when the schema of the features change, such as when feature definitions need
    to be added or deleted.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this book's publication, feature groups are immutable. To add
    or remove features, you will need to create a new feature group. To address the
    requirement of multiple versions of a feature group with different numbers of
    features, establish and stick to naming conventions. For example, you could create
    a `weather-conditions-v1` feature group initially. When that feature group needs
    to be updated, you can create a new `weather-conditions-v2` feature group. You
    can also consider adding descriptive labels on data readiness or usage, such as
    `weather-conditions-latest-v2` or `weather-conditions-stable-v2`. You also can
    tag feature groups to provide metadata. Additionally, you should also establish
    standards for how many concurrent versions to support and when to deprecate old
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the versioning of the individual features, the offline store keeps a history
    of all values of the features in a feature group. Each feature record is required
    to have an `eventTime`, which supports the ability to access feature versions
    by date. To retrieve previous version values of features from the offline store,
    use an Athena query with a specific timestamp, as shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that you can further fine-tune the Athena query to include `write-time`
    and `api_call_time` to extract very specific versions of the features. Please
    see the references section for a link to a detailed blog on point-in-time queries
    with SageMaker Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, when a record is deleted from the online store, the corresponding
    record in the offline store is only logically deleted, which is typically referred
    to as a tombstone. When you query the offline store, you may see a tombstone in
    the results. Use the `is_deleted` feature of the record to filter these records
    from the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have the feature groups created and populated, how do teams in
    your organization discover and reuse the features? All authorized users of the
    Amazon SageMaker Feature Store can view and browse through a list of feature groups
    in a feature store in a SageMaker Studio environment. You can also search for
    specific feature groups by name, description, record identifier, creation date,
    and tags, as shown in *Figure 5.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Search and discover feature groups'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Search and discover feature groups
  prefs: []
  type: TYPE_NORMAL
- en: 'You can go a step further, view feature definitions of the feature group, and
    search for specific features as shown in *Figure 5.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Search and discover features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – Search and discover features
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about designing an ML system that provides
    near real-time predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Designing solutions for near real-time ML predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes machine learning applications demand high-throughput updates to features
    and near real-time access to the updated features. Timely access to fast-changing
    features is critical for the accuracy of predictions made by these applications.
    As an example, consider a machine learning application in a call center that predicts
    how to route the incoming customer calls to available agents. This application
    needs to have knowledge of the customer's latest web session clicks to make accurate
    routing decisions. If you capture a customer's web-click behavior as features,
    the features need to be updated instantly and the application needs access to
    the updated features in near-real time. Similarly, for weather prediction problems,
    you may want to capture the weather measurement features frequently for accurate
    weather predictions and need the ability to look up features in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some best practices in designing a reliable solution that meets
    the requirement of high-throughput writes and low-latency reads. At a high level,
    this solution will couple streaming ingestion into a feature group with streaming
    predictions. We will discuss the best practices to apply to ingestion into and
    serving from a feature store.
  prefs: []
  type: TYPE_NORMAL
- en: For ingesting features, the decision to choose between batch and streaming ingestion
    should be based on how often feature values in the feature store need to be updated
    for use by downstream training or inference. While simple machine models may need
    features from a single feature group, if you are working with data from multiple
    sources, you will find yourself using features from multiple feature groups. Some
    of these features need to be updated on a periodic basis (hourly, daily, weekly)
    and others must be streamed in near-real time.
  prefs: []
  type: TYPE_NORMAL
- en: Feature update frequency and inference access patterns should also be used as
    a consideration for creating different feature groups and isolating features.
    By isolating features that need to be inserted on different schedules, the ingestion
    throughput for streaming features can be improved independently. However, retrieving
    values from multiple feature groups increases the number of API calls and can
    increase overall retrieval times.
  prefs: []
  type: TYPE_NORMAL
- en: Your solution needs to balance feature isolation and retrieval performance.
    If your models require features from a large number of different feature groups
    at inference, design the solution to utilize larger feature groups or to retrieve
    from the feature store in parallel to meet the near real-time SLAs for predictions.
    For example, if your model requires features from three feature groups for inference,
    you can issue three API calls to get the feature record data in parallel before
    merging that data for model inference. This can be done through a typical inference
    workflow executing through an AWS service such as **AWS Step Functions**. Optionally,
    if that same set of features are always used together for inference, you may want
    to consider combining those into a single feature group.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.11* shows the end-to-end architecture for streaming ingestion and
    streaming inferences to support high-throughput writes and low-latency reads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – End-to-end architecture for real-time feature ingestion and
    retrieval'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_05_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – End-to-end architecture for real-time feature ingestion and retrieval
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the high-level steps involved in this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the ingestion side:'
  prefs: []
  type: TYPE_NORMAL
- en: The client application collects and processes the live data. For streaming applications,
    one option is to use **Kinesis Data Streams**. To ingest features, the client
    application calls an ingestion API hosted by an API Gateway.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An API Gateway invokes the lambda function that uses the `put_record` API to
    push features into the online feature store. As necessary, the lambda function
    can also perform additional processing on the raw data before pushing features
    to the feature store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the prediction side:'
  prefs: []
  type: TYPE_NORMAL
- en: A model-consuming client application calls a prediction API hosted by an API
    Gateway. An API Gateway invokes a lambda function that looks up the features related
    to inference requests from the online feature store and creates an enhanced request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The enhanced request is sent to the SageMaker deployed endpoint. The prediction
    from the endpoint traverses back to the client application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using these techniques and best practices, you can design real-time ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you reviewed the basic capabilities of Amazon SageMaker Feature
    Store along with the APIs to use. By combining different capabilities, you learned
    how to reuse engineered features across training and inference phases of a single
    machine learning project and across multiple ML projects. Finally, you combined
    streaming ingestion and serving to design near real-time inference solutions.
    In the next chapter, you will use these engineered features to train and tune
    machine learning models at scale.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional reading material, please review these references:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using streaming ingestion with Amazon SageMaker Feature Store to make ML-backed
    decisions in near-real time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/using-streaming-ingestion-with-amazon-sagemaker-feature-store-to-make-ml-backed-decisions-in-near-real-time/](https://aws.amazon.com/blogs/machine-learning/using-streaming-ingestion-with-amazon-sagemaker-feature-store-to-make-ml-backed-decisions-in-near-real-time/
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Enable feature reuse across accounts and teams using Amazon SageMaker Feature
    Store:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/](https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build accurate ML training datasets using point-in-time queries with Amazon
    SageMaker Feature Store and Apache Spark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/build-accurate-ml-training-datasets-using-point-in-time-queries-with-amazon-sagemaker-feature-store-and-apache-spark/](https://aws.amazon.com/blogs/machine-learning/build-accurate-ml-training-datasets-using-point-in-time-queries-with-amazon-sagemaker-feature-store-and-apache-spark/
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Ingesting historical feature data into Amazon SageMaker Feature Store:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/ingesting-historical-feature-data-into-sagemaker-feature-store-5618e41a11e6](https://towardsdatascience.com/ingesting-historical-feature-data-into-sagemaker-feature-store-5618e41a11e6)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
