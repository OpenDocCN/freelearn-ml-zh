- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Determining Position Using Monte Carlo Localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have several interesting sensors on our robot. However, we have yet to
    combine them to understand the position of our robot. The **Monte Carlo simulation**
    is a method that uses multiple sensors and a model of a robot’s world to estimate
    its location and heading in that world.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how to make a test arena for a robot, followed by how to model
    this arena in code, and how to send this data over Bluetooth to view on a computer.
    You will practice statistical methods for the robot to start guessing its location.
    You will see how to enrich encoder data and move the guesses, and then integrate
    this with distance sensor data to refine the guesses, using a method that is effective
    in the face of noisy sensor data and can cope with minor inaccuracies. This will
    come together in a Monte Carlo guess and check loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training area for our robot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling a space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using sensors to track a relative pose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The robot and code from [*Chapter 12*](B18001_12.xhtml#_idTextAnchor252), *Detecting
    Orientation with an IMU on Raspberry* *Pi Pico*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PC or laptop with Bluetooth LE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7 with the Python `matplotlib`, `bleak`, and `NumPy` libraries installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 x 10-mm A1 sheet foam boards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duct or gaffer tape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tape measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metal ruler, set square, and pencil
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sharp craft knife
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A floor space of 1.5 sq meters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code for this chapter at [https://github.com/PacktPublishing/Robotics-at-Home-with-Raspberry-Pi-Pico/tree/main/ch-13](https://github.com/PacktPublishing/Robotics-at-Home-with-Raspberry-Pi-Pico/tree/main/ch-13).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training area for our robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be estimating a robot’s location in a space. The robot needs a known
    space to work in, so we will build a simple world for it to operate in. This training
    area, or arena, is loosely based on those used in Pi Wars (see [https://piwars.org/2022-competition/general-rules/](https://piwars.org/2022-competition/general-rules/)
    under *Arena construction rules*), a British robotics competition, where this
    algorithm could be used for a robot to compete autonomously.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at the arena.
  prefs: []
  type: TYPE_NORMAL
- en: What we will make
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram shows the arena we will make:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – A robot test arena ](img/Figure_13.01_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – A robot test arena
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.1* shows a top-view drawing of an arena, complete with dimensions.
    The arena is mostly square to keep it simple to make and model. To help the Monte
    Carlo simulation work, there must be a cutout on one side to prevent rotational
    symmetry – that is, you can’t rotate the arena and have it appear identical from
    multiple angles.'
  prefs: []
  type: TYPE_NORMAL
- en: The arena should be large enough for the robot to move freely inside of it,
    without being excessively large, making 1,500 mm a good compromise. The arena
    walls should be tall enough that the robot’s distance sensors cannot miss them.
    A reasonable wall height would be 200 mm. We will work with *mm* throughout this
    chapter to keep things consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Arena size versus robot speed
  prefs: []
  type: TYPE_NORMAL
- en: Beware that you may want a larger arena for a faster robot, and that a smaller
    arena will give the robot less time to detect its features.
  prefs: []
  type: TYPE_NORMAL
- en: The arena floor surface is important; if the robot’s wheels are slipping, then
    the calculations will suffer in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can see how we’ll build this.
  prefs: []
  type: TYPE_NORMAL
- en: How we will make the arena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use foam board to build the arena, as it is lightweight and easy to cut;
    A1 boards are readily available, and panels can be cut from these.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how we can make the arena:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Making an arena ](img/Figure_13.02_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Making an arena
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.2* shows a 3D view of the arena. The letters indicate different
    parts. The Dumont Cybernetics team inspired this style. We can make the arena
    in sections, slotting together corner joints, as shown between panels *D* and
    *B*, or use tape (such as duct tape) to make hinging sections, such as those between
    panels *A* and *B*. This arena is 1,500 mm, so it can be disassembled and folded
    small when not in use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the parts we will need to make this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – A drawing of the parts to make the arena ](img/Figure_13.03_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – A drawing of the parts to make the arena
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.3* shows the parts to cut to make the arena. Each part has a letter,
    the number of pieces you’ll need to make, and the measurements to cut the part.
    The slot profiles are all the same as panel **A**, along with the wall heights.'
  prefs: []
  type: TYPE_NORMAL
- en: Four A panels can be cut from a board with some material left over. Let’s see
    how to cut them.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for cutting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can cut the foam board in a similar way to the plastics you cut in [*Chapter
    4*](B18001_04.xhtml#_idTextAnchor080), *Building a Robot around Pico*. Use the
    tape measure, large ruler, and set square to mark where you will cut in pencil.
    Ensure the surface you are using to cut is at a comfortable height so that the
    long, repeated cutting does not make your back sore.
  prefs: []
  type: TYPE_NORMAL
- en: Then, following a straight metal edge, draw a sharp knife along the cut multiple
    times. For the first cut, aim only to score the top plastic layer, and then keep
    making cuts until you are through. Take care to cut the same area – this is a
    matter of letting later cuts follow the earlier cuts by holding the blade lightly
    over them.
  prefs: []
  type: TYPE_NORMAL
- en: I suggest cutting wall height strips first, before marking slots and wall lengths
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: These sheets often come sandwiched with throw-away foam padding; this will help
    as a cutting surface so that you do not damage a table or floor underneath.
  prefs: []
  type: TYPE_NORMAL
- en: If there is tearing, either you are applying too much pressure or need to change
    your blade for a sharp, fresh one.
  prefs: []
  type: TYPE_NORMAL
- en: Take care cutting the slots. The wall heights do not need to be super precise;
    within a few mm is good enough. The real world is often not as precise and clear
    as a simulation, and this algorithm will be able to cope with this.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have cut the parts, assemble the corners of the arena, and then make
    tape hinges on the inside joins (not the slots). When you disassemble the first
    time, fold the parts along these hinges, and then put tape on the outside of this
    joint. With gaffer tape or duct tape, this should be sturdy enough.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a real space, we’ll need to model this so that the robot can
    use it.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of a Monte Carlo system is to model or simulate a space and a robot’s
    location. In this section, we will learn how code for the robot will represent
    this space. We will also look at how a computer can be used to visualize our robot’s
    guesses. Monte Carlo-based behavior code checks sensor readings frequently against
    the model of the space, so we should represent the space on the robot to optimize
    this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The role of the computer and the robot in this are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Visualizing with the computer ](img/Figure_13.04_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Visualizing with the computer
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.4* shows an overview of this system’s display and control architecture.
    The behavior code runs on the robot. The computer displays the state of the robot
    code, along with start and stop controls. The arena and state of the system all
    belong to the robot.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to represent the arena on the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Representing the arena and robot position as numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a model like this, the boundaries of the arena are important. We can start
    by taking 2D *X* and *Y* coordinates of the corners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following representation of the arena:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – The arena and poses as coordinates ](img/Figure_13.05_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – The arena and poses as coordinates
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.5* shows a simplified version of the arena. Coordinates describe
    each corner as numbers. We can directly use those in the code. These corners can
    be joined as line segments. A line segment is a set of coordinates for the start
    and end of the line segment. All the coordinates in the code will be in mm.'
  prefs: []
  type: TYPE_NORMAL
- en: Our robot will have a **pose** somewhere within the arena. A pose describes
    the robot’s location in space – in this case, anywhere in the 2D space of the
    arena and facing any of 360 degrees. Imagine this like a map pin, with an arrow
    point showing the heading.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.5* also shows two robot poses, *A* and *B*. Each has an *X* and
    *Y* coordinate in mm within the arena, and each has a heading theta (θ) in degrees.
    These three numbers will represent every robot pose in this 2D space. At the start
    of a simulation, the robot could be at any position and facing any heading within
    the arena in degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: Our arena representation has *0, 0* for the bottom left. Heading *0* faces right,
    with positive theta angles going anticlockwise. For example, pose *A* has a heading
    of around 30 degrees anticlockwise from the right, and pose *B* has a heading
    of 300 degrees from the right.
  prefs: []
  type: TYPE_NORMAL
- en: In this system, we will have many pose estimates, which behave like particles.
    The Monte Carlo simulation here is also known as a **particle filter**, due to
    how poses are manipulated and then filtered away based on sensor data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s develop code for the arena boundary line segments.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the representation into code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll represent the arena as code and render it on the computer. Then, we’ll
    move the arena representation over to the robot, with the computer fetching data
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the `arena.py` file, we can add the arena points to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `boundary_lines` variable represents a list of line segments, each of which
    is an array of start and end coordinates, read as `[(start_x, start_y), (end_x,
    end_y)]`. We also store the arena `width` and `height` values here. If your arena
    is a different size, please update these values.
  prefs: []
  type: TYPE_NORMAL
- en: We can display this using `matplotlib`, a mathematical plotting library for
    Python. To do this, first install Python 3.7 (or later) on your computer, and
    in a terminal, use the `python3 -mpip install matplotlib numpy` command to get
    the libraries. For Linux, you may need additional `python3-tk` packages in your
    package manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `display_arena.py` file to draw the arena. This file starts by importing
    `matplotlib`. The convention is to import `pyplot`, a data-plotting module, as
    `plt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will loop over the lines in the arena. The `plot` method takes *X* coordinates
    for a line, followed by *Y* coordinates for it, and allows us to specify a line
    color. Run this with `python3 display_arena.py`, which will draw the arena for
    us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – A matplotlib drawing of the arena ](img/Figure_13.06_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – A matplotlib drawing of the arena
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows the arena drawn by the computer from our code. It
    has grid coordinates along the left and the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: We can now look at moving this model data to the robot where it will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Serving the arena from the robot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The robot should be the source of truth for arena data, so let’s put the arena
    model there. Make a `robot` folder on your computer and move `arena.py` into it.
    We will be copying the contents of this `robot` folder to Raspberry Pi Pico. From
    the previous chapters, copy `robot.py`, `pid_controller.py`, and `pio_encoder.py`
    into the `robot` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then add a little code to serve up our arena boundary lines from the
    robot. In `robot/code.py`, start with imports and helpers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: On the robot, we can handle commands as we have been since [*Chapter 10*](B18001_10.xhtml#_idTextAnchor210),
    *Using the PID Algorithm to Follow Walls*; however, we will use **JavaScript Object
    Notation** (**JSON**), a convenient method to represent more complex information.
  prefs: []
  type: TYPE_NORMAL
- en: Any data we send is converted into JSON, and then a `"\n"` newline is added
    to show that it’s a complete message. We then encode this. The data we receive
    is unpacked with `json.loads`, which will result in data structures of dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then add a handler to this that will send back the arena when requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will loop and wait for the arena command. It prints out any JSON it receives
    for troubleshooting. It will use the JSON to send back the arena data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The robot can be interacted with via the Bluefruit app in UART mode by sending
    `{"command": "arena"}`. The robot will send the boundary lines back as lists.
    However, ideally, we want the computer to display this from the robot with `matplotlib`.
    We’ll need to connect the computer to the robot first.'
  prefs: []
  type: TYPE_NORMAL
- en: The Bleak library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `python3 -mpip install bleak` to install this. Bleak is documented at [https://bleak.readthedocs.io/en/latest/](https://bleak.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: We will also need information about the Adafruit Bluefruit system. Bluetooth
    LE has device IDs, and IDs for services on Bluetooth. See [https://learn.adafruit.com/introducing-adafruit-ble-bluetooth-low-energy-friend/uart-service](https://learn.adafruit.com/introducing-adafruit-ble-bluetooth-low-energy-friend/uart-service)
    for details. We will be using these in the following piece of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with an example to list the devices, to check whether we can find
    the robot’s Bluetooth UART. Create the `find_devices.py` file and add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code starts by importing the `asyncio` and `bleak` libraries. The run function
    needs to be asynchronous so that it can await the `bleak` scanner.
  prefs: []
  type: TYPE_NORMAL
- en: We define the ID and name of the Adafruit Bluefruit based on the Adafruit documentation,
    and then ask the `bleak` library to discover available devices with the Adafruit
    UART service. After waiting for the result, the next few lines print these out.
    The function then filters for the device with the matching name, checks that it
    found it, and prints it successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run this with `python3 find_devices.py`. If the robot is off, you will see
    a `Could not find robot` error. However, running with the robot turned on should
    show the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From time to time, `bleak` will have trouble finding the robot and display the
    preceding error. You will need to rerun the example to find the robot. We can
    now put this code into a library that we can use in the remaining experiments
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Bluetooth LE wrapper library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll call the library `robot_ble_connection.py`. We’ll start with imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll put our connection handling into a class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: See [https://learn.adafruit.com/introducing-adafruit-ble-bluetooth-low-energy-friend/gatt-service-details](https://learn.adafruit.com/introducing-adafruit-ble-bluetooth-low-energy-friend/gatt-service-details)
    for an explanation of these variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we create the object to handle the connection, we will have two functions
    that the client code can provide, one for a connection being complete and one
    for data being received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`receive_handler` is a function that can be called with a Python `bytes` object
    holding the received data. We’ll adapt our receive handler into one that the `bleak`
    library can use to receive data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we add a connect method. This starts the same as the `find_devices` example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we then need to connect to this device and handle the received data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We create a `BleakClient` object and then wait for a connection to the robot.
    After connection, it will create a background task to notify the handler when
    data arrives. This `start_notify` method uses `rx_gatt` to receive UART data from
    this Adafruit Bluefruit device.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to be able to close the connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the final part of this code can send data to the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will wait for data to be sent and use the right UUID for transmitting to
    the UART.
  prefs: []
  type: TYPE_NORMAL
- en: This `robot_ble_connection` library is now ready to be used in code.
  prefs: []
  type: TYPE_NORMAL
- en: Showing the robot’s data on the computer screen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use `matplotlib` to display the data from the robot, connecting to the
    robot with the preceding code, and asking it for the arena. This demonstration
    will tie `matplotlib` together with a Bluetooth connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll put this in a new file named `display_from_robot.py`, starting with the
    imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll put our display system in a class called `RobotDisplay`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first part sets up the BLE connection and prepares it with a `handle_data`
    method (this is the BLE data handler, which we’ll implement shortly).
  prefs: []
  type: TYPE_NORMAL
- en: When data arrives via BLE to the computer, a whole message can be split across
    a few calls to the `handle_data` method. We are working in lines of text, so we
    will use `self.buffer` to store any partial line until we get a line ending, signaling
    a line is complete. We also have a place to store the arena from the robot, and
    a flag to detect when the app is closed. The display system is prepared with `plt.subplots`,
    which gets a figure and axes – we’ll use these in a `draw` method to draw the
    display.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a handler for the app being closed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This handler will just set the `closed` flag to `True`, which we can check for
    later. `matplotlib` will automatically create an app window for us to display
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will build the BLE data handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This collects decoded incoming data into the `self.buffer` variable. While that
    buffer has line endings, `"\n"`, it splits a single line off and decodes it as
    JSON.
  prefs: []
  type: TYPE_NORMAL
- en: We then check whether this JSON has arena data in it. If so, we store it in
    the `arena` data member.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we put the arena line drawing into a method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This function clears the previous display using the `self.axes.clear()` function
    and then redraws the arena lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The app `main` method starts the connection and asks the robot for the arena:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This function enables interactive mode in `matplotlib` with `plt.ion()` – this
    means we get to handle when the screen is redrawn, which suits our data model.
  prefs: []
  type: TYPE_NORMAL
- en: We then call and wait for the BLE `connect` function. Once a connection has
    been made, we wrap the rest in a `try`/`finally` block that will ensure the BLE
    connection is closed if this code is stopped or breaks. We then send a request
    to the robot, asking for the arena.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code sets up the `close` handler so we can detect whether the window is
    closed, and immediately gets into a main `while` loop based on the closed flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The main loop uses `plt.draw()` to update the display and then waits 0.05 seconds,
    giving `matplotlib` time to handle interactive events. It also has a 0.01-second
    asynchronous sleep to give the BLE tasks time to run. These sleeps and pauses
    must be called frequently. At the end, `finally` ensures we close the BLE connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then need to create an instance of the class and start the `main` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the display code is complete. Send the `robot` folder to Raspberry
    Pi Pico, and with battery power turned on, start the display code with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the BLE connecting messages and then the following output on
    the computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After around 30 seconds, you should see the computer display the arena. This
    will look identical to *Figure 13**.6*, but the data is now coming from the robot.
  prefs: []
  type: TYPE_NORMAL
- en: We have the computer connecting to the robot and retrieving arena information
    from it. The robot has modeled the space in simple terms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll look more at robot poses, displaying them on our
    computer, and updating them from encoder sensors.
  prefs: []
  type: TYPE_NORMAL
- en: Using sensors to track relative pose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore what a pose is, how to create, send, and display
    poses, and how to move the poses relative to the movement of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up poses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll make some random poses in `robot/code.py` using `ulab` providing this
    functionality in CircuitPython. This library also gives us handy ways of storing
    and dealing with arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `ulab` library, and `random` to generate random poses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After the `read_json` function, we’ll add a `Simulation` class to hold the
    poses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will create a small population of 20 random poses. The `poses` variable is
    a `NumPy` array of `population_size` items, with each item an *X*, *Y* heading
    pose. NumPy allows us to specify a datatype; we use the `float` type so that we
    can work in fractional values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a function (before `Simulation`) to send *X* and *Y* pose coordinates to
    the computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `[:, :2]` notation lets us extract the first two entries of each pose in
    the `poses` array, the *X* and *Y* coordinates. We convert this to `int16` to
    reduce how much data is being sent – the UART is easily overwhelmed by pose data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command handler can now send poses after the arena for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, `command_handler` has the simulation passed into it and sends the poses
    back after the arena. Before we start the handler, we create `simulation` from
    its class.
  prefs: []
  type: TYPE_NORMAL
- en: This code is now ready for the computer to display these poses.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying poses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now enhance our `matplotlib` file, `display_from_robot.py`, with the
    poses. First, we will add `numpy` to the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When we set up the display in the `__init__` method, we add an empty `poses`
    member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to extend `handle_data` to load poses into an `int16` NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add extend the `draw` method to display poses, checking whether any
    are loaded and, if so, putting them into a scatter plot, slicing into the *X*
    and *Y* components to fit `matplotlib` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Send the `robot` folder over to Pico, and then run `display_from_robot.py`
    on the computer, and after the BLE startup, you should see something like the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Poses drawn in the arena ](img/Figure_13.07_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Poses drawn in the arena
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.7* shows the arena with 20 poses drawn as dots. Each dot is a
    potential guess of where the robot might be. Some are in the cutout area and will
    later be eliminated.'
  prefs: []
  type: TYPE_NORMAL
- en: These poses will need to move when our robot moves, so let’s make our robot
    move.
  prefs: []
  type: TYPE_NORMAL
- en: Moving with collision avoidance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The robot will be moving while we perform the simulation, and it would be good
    to avoid collisions while the robot moves. We’ll do this as an asynchronous routine
    so that other parts of the code can run at the same time. The following architecture
    diagram shows how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – A simulation with a collision avoidance architecture ](img/Figure_13.08_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – A simulation with a collision avoidance architecture
  prefs: []
  type: TYPE_NORMAL
- en: The `command handler` system accepts Bluetooth command requests. The command
    handler starts the `main` loop in `simulation`. The simulation will start both
    a `simulation` `main` method also sends poses via Bluetooth.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with `DistanceSensorTracker`, a class to keep tabs on the distance
    sensors and their last readings. Place this in `robot/code.py` under the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We are being explicit about sensor mode here, adjusting it to the size of the
    arena. We also put in starting values until a reading is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sensor tracker loop fetches readings when ready and resets the sensor interrupts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We are multiplying the sensor readings by 10 to convert them to mm, and then
    storing them. The remaining code can just use these stored readings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll build the `CollisionAvoid` class to turn the robot away from a
    wall that it detects with the sensors. Add this class after the `DistanceSensorTracker`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This has an initial robot speed, along with a reference to the distance sensor
    tracker. This then has a `main` collision-avoiding loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This loop starts the right motor moving, and if a collision looks likely, it
    will set the left motor going backward and wait before driving forward. The `asyncio.sleep`
    delays mean that other tasks can continue on the robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `Simulation` class, add the sensors and `collision_avoider` to the
    `__init__` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add a simulation `main` method below the `__init__` simulation. This
    starts tasks for the other components and then loops over, sending the poses back
    to the computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: There’s also error handling to stop the robot if anything goes wrong here –
    we cancel the collision avoider task (which would set the robot’s speed) and stop
    the motors. The sleep here allows the other tasks to run and avoids overwhelming
    the BLE UART.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extend the `command_handler` method to start the simulation’s `main` task.
    We’ll do so based on a **Start** button in the display UI. First, we’ll store
    the task state at the top of the handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll handle a `start` command in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The start button will run the simulation `main` task if it’s not yet been run.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the start button on the computer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to add the corresponding button to the computer display code. Open
    `display_from_robot.py`. In the imports, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `RobotDisplay` class, we can add a helper to send a JSON command, much
    as we did on the robot. Add this to the robot display class above its `main` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This must be asynchronous to use `await` on the BLE `send_uart_data` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Above `main`, add a start button handler to call when the button is pressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This will start sending the data but not wait for it – so the `matplotlib` event
    loop doesn’t get stuck waiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can replace the JSON sending in the `main` method with the `send_command`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add the button. Add the highlighted code into the `main` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The code uses the `send_command` wrapper to request the arena on startup. We
    then add `start_button`, using `plt.axes` to position it.
  prefs: []
  type: TYPE_NORMAL
- en: We connect a button `on_clicked` handler to the `start` method to enable the
    button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Send the `robot` folder to Raspberry Pi Pico, and on the computer, run `display_from_robot.py`.
    I recommend propping the robot up for troubleshooting while connected, and then
    test it in the arena. The display will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – The arena display with a button ](img/Figure_13.09_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – The arena display with a button
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.9* shows the **Start** button on the display. Press **Start**
    to get the robot running – poses will appear, and the robot should be avoiding
    walls in the arena. If not, the next section will help.'
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These suggestions will help if you are having trouble here:'
  prefs: []
  type: TYPE_NORMAL
- en: If the distance sensors are showing errors, please go back to [*Chapter 8*](B18001_08.xhtml#_idTextAnchor166),
    *Sensing Distances to Detect Objects with Pico*, and check the wiring, using the
    tests there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot is turning too far and getting trapped in the corners, lower the
    sleep after `robot.set_left(-self.speed)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot is going very quickly, either reduce `self.speed` or replace the
    motors with a greater gear ratio – ideally, 298:1, as recommended in [*Chapter
    11*](B18001_11.xhtml#_idTextAnchor233) in the *Slowing the Robot* *Down* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the robot is now avoiding walls, the poses are not changing when the robot
    moves. To remedy this, we can add a motion model in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Moving poses with the encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want the poses to move with our robot’s motion, updating both their position
    and their heading. The wheel encoders provide data about each wheel’s motion,
    and we can convert this into rotations and translations of the pose. First, we
    need to store more data about the shape of the chassis in `robot/robot.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We ensure our tick conversion is in mm. We then add the wheelbase – this is
    a measurement between the central contact point of each wheel. Use a value measured
    from your own robot. We can use the wheelbase to calculate the robot’s movement
    from the encoders, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Calculating motion from encoders ](img/Figure_13.10_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Calculating motion from encoders
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.10 (a)* shows the robot moving along an arc. Each wheel encoder
    will sense travel along an arc on an inner radius or an outer radius. Our robot
    uses a differential drive, so we can assume all motion takes place around the
    axis between the two wheels. The center of the robot, our pose, travels along
    the turn radius. We can use these with the wheel distance – the distance between
    the two wheels to calculate the arc.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.10 (b)* relates an arc length to the arc angle and radius. Each
    wheel will have traveled an arc, and the encoder will have measured the arc length.
    This arc length is the radius multiplied by the angle (in radians). We will use
    this to calculate the arc. From the motion of the two wheels, measured by the
    encoders (arc lengths) and the wheel distance, we can get the radius and angle
    change (`d_theta`).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.10 (c)* represents a robot motion. Although the robot has moved
    in an arc, for the simulation, we will simplify this arc motion into three components
    – *rotation 1* aligns the robot for a straight-line *translation*, and then *rotation
    2* turns the robot to face the heading expected at the end of the curve.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following into the `Simulation` class in `robot/code.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This function will take the changes (or deltas) in encoders and convert them
    to obtain representations of *rotation 1*, *translation*, and *rotation 2*. The
    encoder changes are turned into measurements in mm. We then check for the straight-line
    case, and if it is there, return a translation component only. This prevents `0`
    causing the next part to crash.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining cases now need us to calculate an arc:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The first line uses the wheelbase and the two encoder movements to calculate
    an arc radius. The difference between the motion of the two wheels is then used
    to calculate `d_theta`, how much the robot’s heading changed throughout this arc.
    The `d` prefix represents a delta.
  prefs: []
  type: TYPE_NORMAL
- en: The arc length is then `d_theta` multiplied by the radius. Because this will
    be called fairly frequently, we are going to assume that the arc length is close
    enough to the translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, the rotation components can be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: If we assume the arc to be regular, then each rotation component is half of
    the full arc rotation. We also convert this into degrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then write a method to move poses this way. Add this to `Simulation`
    by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the method and add the first rotation into the array of pose rotations
    (the third element):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then move the translation term in the new pose direction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `rot1_radians` variable will hold a `NumPy` array. This comes from the second
    element of the poses array, converted into radians. The ability of `NumPy` (or
    `ulab`) to operate on whole arrays is handy here. We will use it again to calculate
    the *X* and *Y* motions. `trans * np.cos` applies the cosine function to every
    element in `rot1_radians` and multiplies each one by the translation term.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then need to add the `rot2` term:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, we constrain the angles between 0 and 360 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to tie these together with getting the encoder deltas. First,
    we extend the `Simulation.__init__` method to get the initial encoder readings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use this encoder data in a motion model, moving all our poses with
    the robot’s motion. In the `Simulation` class, we will then add a `motion_model`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It first gets the latest encoder readings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the deltas and feed these into `convert_odometry_to_motion`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We must update the last encoder readings so that we''ll get deltas next time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we apply these odometry values to our poses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now need to call this motion model. In `Simulation.main`, add the highlighted
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This will apply the motion model at every cycle before sending the poses. Since
    the motion model requires time to run, the sleep is reduced to compensate, keeping
    the UART data rate similar. Copy the contents of the `robot` folder to Raspberry
    Pi Pico, and launch the `display_from_robot` app.
  prefs: []
  type: TYPE_NORMAL
- en: When you press **Start**, you should now see the poses moving as the robot moves
    around the arena. All the poses should follow the same path, but each from a different
    starting point and orientation.
  prefs: []
  type: TYPE_NORMAL
- en: These poses are moving, but the real world is messy, so let’s add randomness
    to this.
  prefs: []
  type: TYPE_NORMAL
- en: Pose movement probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Robot movement is not always certain; although we have encoders, wheels can
    slip, wheel sizes can have minor variations, and our calculations might not be
    100% accurate. Imagine that the preceding poses are in a cluster or the cloud,
    and then our robot drives in a particular direction. The following diagram should
    demonstrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Movement probability distributions ](img/Figure_13.11_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Movement probability distributions
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.11 (a)* shows a section of the arena, with arena coordinates shown
    on the axes. The cluster around the point (250,300) is the initial robot pose
    guesses. The thick line and angle arc show a robot movement at 300 mm, bearing
    30 degrees. However, due to the uncertainties of the motion, the cluster gets
    spread out. The arc shape is due to uncertainty in the angle of the motion, and
    the width of the arc represents the uncertainty of the forward motion of the robot.
    This banana shape represents where a robot could end up. The image here has been
    exaggerated, as the spread on the robot should be far less than this.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.11 (b)* shows how we can model this uncertainty centered around
    a **mean** (average or most likely value) of 0, with a variation on either side.
    A **probability distribution** maps how likely a value is to come up in a random
    selection. The height of each point signifies how likely a particular value is
    to come up. If we use a uniform distribution, all possibilities between -1.0 and
    +1.0 are equal, giving us a rectangle shown for *n=1*. However, we want this distribution
    centered around the mean. If we sum two samples from the uniform distribution
    from -1.0 to 1.0 and divide by 2, we get the *n=2* graph. This is approximately
    a triangle. Here, *n* represents the number of uniform random sample picks we
    add together. We could refine this to the *n=4* curve, using a sum of four uniform
    samples and dividing by 4; however, the trade-off between ideal curves and the
    time cost for each uniform distribution sample makes the triangle at *n=2* good
    enough for our purposes to center the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the *n=2* distribution in our model. In `robot/code.py`, add the
    following piece of code before `class Simulation`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This code will add the two samples, scaling the uniform distributions to match
    how much the model varies, divide by 2, and then add the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other factor we will need to account for is that the larger the movement
    we make, the larger the random error factor will be. A large forward motion will
    influence the rotation, and large rotations will affect the forward motion (translation).
    It is conventional to refer to the factors for these influences as `alpha`. Let’s
    add these values to our `Simulation.__init__` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We have four terms here. They should be values between 0 and 1 and kept low.
    The value `0.05` will represent a 5% error. Tune them to reflect the error seen
    in your robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use this to apply randomness to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following method in `Simulation`, after `move_poses`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the scaling factors from the `alpha` terms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The rotation scaling factors are based on the absolute value of each element;
    they must not be negative. The rotation scales have both a rotation factor and
    a lower translation factor. The translation scale has a translation factor (usually
    larger) and a factor based on both rotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use this to generate noise around the motion for every pose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This uses the scaled sample function we created, with our scale factor. It uses
    the calculated rotation or translation as a mean. We run this through loops for
    each pose dimension, so for a population of 200, we will get 200 random samples,
    centered around the calculated measurement, with variation scaled to the calculated
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we return these models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have a model that generates noise in our motion, meaning that it will
    compensate for the inaccuracies in the measurement by modeling the uncertainty
    in that measurement. Add the highlighted code for this to the `motion_model` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The changes swap the `rot1` variable out for `rot1_model` and do a similar swap
    for the other pose elements. As `rot1_model` has the same number of elements as
    our poses, passing this into `move_poses` will add each sample element-wise to
    the respective pose element. This method takes advantage of how `NumPy` manipulates
    lists.
  prefs: []
  type: TYPE_NORMAL
- en: Copy the `robot` folder to the robot and run the `display_from_robot.py` app
    on your computer. The motion will be a little randomized. Now, let’s check that
    your robot code is working and behaving as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If this example does not work, try the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Run with the robot propped up and connected to the computer so that the Mu editor
    serial can see its state. This will show you whether there are any errors in the
    code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the movements are too far or too little, adjust the measurements in `robot/robot.py`
    to match your robot, as they may vary from example values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you see sensor or I2C issues, backtrack to check the wiring and previous
    sensor demonstration examples. Also, ensure that you have fresh batteries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now have our poses motion model based on the encoders. We can now bring the
    distance sensors and arena data into play with the Monte Carlo simulation.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our robot’s poses are going outside of the arena, and the distance sensor readings
    should show which guesses (poses) are more likely than others. The **Monte Carlo**
    simulation can improve these guesses, based on the sensor-reading likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: The simulation moves the poses and then observes the state of the sensors to
    create weights based on their likelihood, a process known as the **observation
    model**.
  prefs: []
  type: TYPE_NORMAL
- en: The simulation **resamples** the guesses by picking them, so those with higher
    weights are more likely. The result is a new generation of guesses. This movement
    of particles followed by filtering is why this is also known as a **particle filter**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by giving our poses weights, based on being inside or outside the
    arena, and then we’ll look at how to resample from this.
  prefs: []
  type: TYPE_NORMAL
- en: Generating pose weights from a position
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial weight generation can be based on a simple question – is the robot
    inside the arena or not? If not, then we can reduce the pose probability. Note
    that we don’t eliminate these, as the robot could have been placed outside the
    arena map or been tested on your desk. We will just give them a lower probability
    than those that are inside the arena.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `robot/arena.py` file, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add a value to indicate a very low probability – close to but not zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a function to check whether the arena contains a point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, check whether the point’s outside the arena rectangle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we check whether it’s in the cutout section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Otherwise, this point is in the arena:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then add an `observation_model` method to our `robot/code.py` `Simulation`
    class to generate the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up the weights to `ones`, with a weight per pose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then loop over the poses, lowering the weights of those outside the
    arena:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then return the weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this point, the weights aren’t being used. We will need to resample them
    for them to act on the poses.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling the poses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we step through the Monte Carlo simulation on the robot, we would like a
    subsequent generation of particles to favor more likely poses. We are going to
    use a technique illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Low variance resampling ](img/Figure_13.12_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Low variance resampling
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.12* starts with **Weighted Sample Space**, a number line between
    0 and the sum of all weights. Below this is a bar representing 10 samples (named
    *a* to *j*) in a sample space. The weights of these samples are represented by
    their widths. The shading highlights the different samples.'
  prefs: []
  type: TYPE_NORMAL
- en: In the diagram, we generate a new space with five samples *(n=5*). This number
    could be the same as the original space (for generating a new generation), may
    have a smaller number for sending via BLE, and may have a larger number for interpolating.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling the original set starts by dividing the total sum by the number of
    new samples, which will give a sample interval size of *sum/n*, shown as **Sample
    intervals**. We then generate a single uniform random number between 0 and *sum/n*,
    which will shift the intervals.
  prefs: []
  type: TYPE_NORMAL
- en: We can then look at the weighted sample space and pick out the sample that matches
    the start of each interval – this is the **weight index**. This will produce **New
    sample space**. Note that sample *c*, which has the highest weight, gets sampled
    more times. With larger sample populations, the resampled space will more accurately
    resemble the original.
  prefs: []
  type: TYPE_NORMAL
- en: The new samples do not have weights and are all considered equally weighted,
    but some samples appear multiple times to represent their previous weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique of using random shifted intervals is known as the **low variance
    resampling** method. We will now see how to perform this through code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `robot/code.py`, inside the `Simulation` class, add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `weights` variable refers to the list of weights, and `sample_count` refers
    to the number of samples to get. This method will sample new poses from the poses
    array. We will set up a `samples` variable to hold the new samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set up the interval size based on `sample_count`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use that to set the interval `shift` value – the start position.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to store the cumulative weights while we loop through the original
    samples (poses). We will also store an index in the source sample set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code will loop until we have the expected number of samples. For each sample,
    there is a `weight_index` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now start adding up weights from the source samples in `cumulative_weights`,
    until they meet the weight index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We keep track of the source sample index that met this weight requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this `source_index` to add a sample to our set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will drop out of the `while` loop and be the end of the `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we return the new set of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also increase our population, while sending only a subset. In `Simulation.__init__`,
    change the population size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: We are limiting the population size here due to Pico memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then apply our observation model in the `main` loop (in `Simulation.main`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: In our loop, we use the observation model to get weights for the poses. We use
    the `resample` method to get 20 poses to send. We then use `resample` again to
    get a new population of poses. The cycle of updating our poses, observing their
    state, weighting the poses, and then resampling them is known as a **recursive**
    **Bayes filter**.
  prefs: []
  type: TYPE_NORMAL
- en: If you send the `robot` folder to Raspberry Pi Pico and launch the app, you
    should start seeing the number of samples outside the arena being reduced. They
    will jump around, since we are sampling 20 from a larger set of 200.
  prefs: []
  type: TYPE_NORMAL
- en: The system reduces the sample space to those that are more likely. However,
    we can include distance sensors to improve this process.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating distance sensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our robot has two distance sensors. Our code will check distances the robot
    senses against model data. If each pose is an imaginary map pin, then the distance
    to the nearest obstacle would be a string stretched out from this pin, or a sensor
    beam with a sensed endpoint – the **beam endpoint model**. With 200 poses, this
    could be slow. Let’s see a faster method to model them.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling distance sensors in our space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way we could do this is to take an estimate of the robot’s position, and
    then perform the math needed to get the distance to the nearest obstacle. However,
    we can make a lookup table to simplify this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Distance representation ](img/Figure_13.13_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Distance representation
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.13 (a)* shows a **likelihood field** based on distance. The white
    dot is the endpoint at the distance reading from the distance sensor. The bright
    middle line of this grid represents an arena wall. Each grid square has a value
    between 0 and 1 (shown as brightness), representing how likely a sensed distance
    projected here is to have detected the wall. Instead of asking the question, *Is
    this distance measurement a match?*, we can ask, *How likely is this distance
    sensor a match?* We can calculate this grid once only when the system starts up,
    so other code can make fast lookups into the grid to check for sensor readings.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.13 (b)* shows how the likelihood changes, with a distance from
    0 to 1,500 in a decaying function. The *Y* axis shows the likelihood of it being
    a hit. The dashed vertical line is a value, currently 250, at the inflection point,
    at which the curve changes direction. A smaller inflection point makes a tighter
    curve; a larger value makes a wider curve.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import `numpy` at the top of `robot/arena.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'We will convert these values to a grid of 50 mm2 cells. As some poses will
    have distance endpoints outside the boundary, we’ll give the grid an overscan.
    Extend the `robot/arena.py` library with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll start with how we get the distance to a grid square. In `robot/arena.py`,
    after defining `boundary_lines`, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: The code unpacks the line segment into `x1` and `y1`, and `x2` and `y2`. It
    then checks whether the line is horizontal (the same *Y*), and whether the point
    being checked is above or below it; this allows a shortcut by subtracting the
    *Y* values. The code can repeat this for vertical lines.
  prefs: []
  type: TYPE_NORMAL
- en: The code then uses Pythagoras’ theorem, where the resulting distance will be
    the hypotenuse.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will convert distances into likelihoods with a decaying function, which
    will return a lower value as we get further from zero. The following function
    will, for a specific point, find the nearest segment distance and then apply a
    decaying function to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'We can make a function to generate this grid. It starts by making a 2D array
    of float fields filled with zeros to hold the grid values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: The width of the grid in cells is the width of the arena divided by the cell
    sizes. We then add in the overscan for either side of the arena. The height uses
    a similar calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then loop over the grid rows and columns to fill in the cell data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'This loop gets the arena *X* and *Y* coordinates for each cell and uses the
    coordinates to fetch the likelihood at that position, storing it in the grid.
    We call this function and store the result in the `distance_grid` variable so
    that this calculation will run when the `arena.py` file is imported. The calculated
    distance grid looks like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.14 – The distance-based likelihood field ](img/Figure_13.14_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.14 – The distance-based likelihood field
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.14* shows the distance-based likelihood field.'
  prefs: []
  type: TYPE_NORMAL
- en: The overscan extends to -500 mm and 2,000 mm, and the boundary lines are drawn
    in black. Each cell’s value is based on its bottom-left coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: We can now use this likelihood field in the observation model.
  prefs: []
  type: TYPE_NORMAL
- en: Generating weights from the distance sensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For each pose, we will need to project the sensed distances from the sensor
    positions. See the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.15 – Distance sensor geometry ](img/Figure_13.15_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Distance sensor geometry
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.15 (1)* shows poses *A*, *B*, and *C* as dots, with an arrow showing
    their heading. There are dark bars representing the location of the two distance
    sensors relative to the pose – they stick out to the side and forward. From this
    location, we will have sensor readings, represented by the arrows pointing from
    the front of the bars.'
  prefs: []
  type: TYPE_NORMAL
- en: By projecting the sensor distances against the likelihood field, we can see
    that pose B is a more likely match than pose C, with A as the least likely pose
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.15 (2)* shows how we can project the sensors. *Ds* is how far
    the distance sensor goes out to the side (`dist_side_mm` in the code). *Df* is
    how far forward the sensors are from the robot wheels (`dist_forward_mm`). *Dr*
    is the distance sensed, from *Df*. We will have to add this to every pose. Pre-calculating
    a triangle from the sensed distance makes this a little easier. *θ* is the robot’s
    heading. Using the *SOHCAHTOA* mnemonic, we can get *θr*, the angle from the robots
    heading to the right sensor, and using Pythagoras’ theorem, we can get *Hr*, the
    hypotenuse. The adjacent side will be *Df* and *Dr*, and the opposite *Ds*. We
    can then add *Hr* at the *θr* angle to each pose to get the right sensor beam
    endpoint for every pose. The same can be applied to the left sensor reading. While
    complicated, this is faster than calculating the endpoint projecting out to the
    side and forward for each pose.'
  prefs: []
  type: TYPE_NORMAL
- en: Measure the position of the distance sensors on your robot relative to the middle
    of the wheels (or use the CAD drawings). The active part of each sensor to measure
    to is the shiny part at the top middle of each sensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll add these distance sensor positions to our `robot/robot.py` measurement
    after the `wheelbase_mm` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to add a function to `robot/arena.py` to look up a position in the
    distance grid:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function converts an `(x,y)` pose argument to grid coordinates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Out-of-bounds requests should return an incredibly low probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then return the result stored at the grid location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `robot/code.py` file, we can add a method to the `Simulation` class
    to perform the preceding triangle calculations, to get the sensor endpoints for
    each pose:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method will take the sensor reading, and inform us whether it’s on the
    right side:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then calculate the adjacent and angle of our triangle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note here that we calculate the negative of the angle if it is on the right
    side.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then get the hypotenuse – this will be the distance from between the wheels
    to the sensor beam endpoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we use `numpy` to help us calculate the angle relative to each pose, converting
    the pose angle to radians as we go:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then build a list of endpoints by projecting from the coordinate of
    each pose, with the hypotenuse at the calculated angle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We finally return these calculated lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now create an `observe_the_distance_sensors` method inside the `Simulation`
    class and apply those to the existing set of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by accepting an existing list of weights as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then call `get_sensor_endpoints` for each side:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have a list of distance sensor projections for every pose. We can look
    up the distance likelihood grid at each of those points and add them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then multiply this by the existing weight (inside or outside the arena):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we leave this loop and return the modified weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then need to incorporate this code into `observation_model`. Make the highlighted
    change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'If you send this code to the robot, it will weigh and resample from two distance
    sensors. The robot poses will start to form in blobs, located around likely positions
    for the sensors. The blobs will form, scatter, and reform as the robot filters
    and moves them. The following diagram shows what you will see on the display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.16 – The localization blob ](img/Figure_13.16_B18001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.16 – The localization blob
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 13**.16*, the poses have grouped together in a blob, roughly representing
    the robot’s position, which will move with the robot as it drives around the arena.
  prefs: []
  type: TYPE_NORMAL
- en: You may see one blob, or a few, and they may drive exactly with the robot or
    seem a little off. This is where you will need to tune the model to better suit
    the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning and improving the Monte Carlo model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tuning the following factors can improve this model:'
  prefs: []
  type: TYPE_NORMAL
- en: The `ch-13/4.3-monte-carlo_perf` folder in the GitHub repository contains an
    instrumented version of the code in this chapter for troubleshooting. You will
    need to tune `robot.py` and `arena.py` for your own setup, but this code reports
    issues and tracebacks back to the computer for diagnosis, weight output from the
    observation model rendered on the display, and, if you are connected via USB,
    also sends performance data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurements in `robot/robot.py` – the accuracy of measurements such as wheel
    diameters, wheelbase, gear ratio, and encoders will guide the odometry model.
    If the movement of the blob doesn’t match the speed and turning, these are the
    likely suspects. The model assumes wheels to be identical in size, which may be
    false if it’s consistently pulling to one side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, if the distance sensor position measurements in `robot/robot.py`
    are incorrect or the sensors need calibration, the position will be consistently
    off. The sensors can be disrupted by strong sunlight and flickering room lighting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the arena, the `get_distance_likelihood` factor, 100, adjusts the decay for
    the likelihood field around each boundary. Lowering this will tighten the fields.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In `code.py`, the number of poses is a major factor. Increasing this will make
    for a better model, but beware of memory constraints on Raspberry Pi Pico.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `code.py` alpha factors encode your certainty in the motion model; make
    these lower if you trust the motion model more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model also assumes the arena construction to be fairly accurate. Errors
    in these assumptions can stack up to make it harder for the localization algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need to spend time with these factors to make a model that is more
    likely to find its location, or is quicker in doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize what we have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by building a test arena for our robot using foam
    board construction, then modeled this in code, and displayed it along with a distance
    sensor likelihood field. We put this on the robot, sent it over BLE, and then
    added poses.
  prefs: []
  type: TYPE_NORMAL
- en: We modeled how poses move using sensors, adding uncertainty to the model. We
    then added a model of distance sensor observations, generating weights that we
    used in a resampling algorithm to generate new poses.
  prefs: []
  type: TYPE_NORMAL
- en: We finished with a look at tuning factors to improve the performance of this
    system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will summarize your Raspberry Pi Pico robotics learning
    journey so far and discuss how you can continue your journey by improving this
    robot or building more robots.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following exercises will deepen your understanding of the topics discussed
    in this chapter and make the robot code better:'
  prefs: []
  type: TYPE_NORMAL
- en: The IMU could be added by storing a previous state and calculating the delta.
    You could mix this into the `rot1`/`rot2` values by taking the average of encoder
    calculations versus the IMU angles, or consider whether one sensor is more trusted
    than the others. You will need to calibrate the IMU before it can be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The robot’s pose guesses get stuck in **local maxima** – good but wrong guesses
    that are likely based on sensor positions. Consider throwing in 10 fresh guesses
    at every population to nudge the code to try other options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are using only two observations per pose – having more distance sensors could
    improve this but will make the model slower.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could you add a target zone to the arena? Consider how PIDs could be used to
    steer the robot toward this. Perhaps feed the PID with the mean pose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can improve the visualization by sending more pose data, including orientation.
    You may need to consider the `msgpack` library or switching to Wi-Fi or BLE over
    SPI, as the amount of data can easily overwhelm the BLE UART connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These aids for further study will let you read on and dive deeper into the
    Monte Carlo algorithm and its quirks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Probabilistic Robotics* by Sebastian Thrun, Wolfram Burgard, and Dieter Fox,
    published by MIT Press, covers the Monte Carlo particle filter, along with the
    Kalman filter and other probability-based models in far more depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I strongly recommend the *Khan Academy* material on modeling data distributions
    for learning and practicing data distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A playlist of 21 videos from Bonn University and Cyrill Stachniss at [https://www.youtube.com/playlist?list=PLgnQpQtFTOGQEn33QDVGJpiZLi-SlL7vA](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQEn33QDVGJpiZLi-SlL7vA)
    covers the topics used here in detail. I recommend them if you want to dive far
    deeper into this topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
