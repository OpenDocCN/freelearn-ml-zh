["```py\nimport tensorflow as tf\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\nimport numpy as np\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n2024-01-29 17:34:32.614910: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n```", "```py\n!git clone git@github.com:alexeygrigorev/clothing-dataset-small.git\n```", "```py\npath = './clothing-dataset-small/train/pants/12bfe0f0-accc-4539-ab51-53f63534938e.jpg'\nload_img(path)\n```", "```py\nimg = load_img(path=path, target_size=(299,299))\nimg_input = np.array(img)\nimg_input.shape\n(299, 299, 3)\n```", "```py\nimg\n```", "```py\nmodel = Xception(weights='imagenet', input_shape=(299,299,3))\n```", "```py\nimg_preprocessed = preprocess_input(np.array([img_input]))\npred = model.predict(img_preprocessed)\npred.shape\n1/1 [==============================] - 3s 3s/step\n(1, 1000)\n```", "```py\ndecode_predictions(pred)\n[[('n03594734', 'jean', 0.651147),\n  ('n04371430', 'swimming_trunks', 0.22369406),\n  ('n03710637', 'maillot', 0.004711655),\n  ('n04525038', 'velvet', 0.0038891942),\n  ('n03595614', 'jersey', 0.003085624)]]\n```", "```py\ntrain_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n```", "```py\ntrain_ds = train_gen.flow_from_directory(directory='./clothing-dataset-small/train/', target_size=(150,150), batch_size=32, seed=42)\nvalidation_ds = train_gen.flow_from_directory(directory=\"./clothing-dataset-small/validation/\", target_size=(150,150), batch_size=32, shuffle=False)\nFound 3081 images belonging to 10 classes.\nFound 341 images belonging to 10 classes.\n```", "```py\ntrain_ds.class_indices\n{'dress': 0,\n 'hat': 1,\n 'longsleeve': 2,\n 'outwear': 3,\n 'pants': 4,\n 'shirt': 5,\n 'shoes': 6,\n 'shorts': 7,\n 'skirt': 8,\n 't-shirt': 9}\n```", "```py\nbase_cnn_model = Xception(weights='imagenet', include_top=False, input_shape=(150,150,3))\nbase_cnn_model.trainable = False\n```", "```py\ninputs = keras.Input(shape=(150,150,3))\nbase = base_cnn_model(inputs)\nvectors = keras.layers.GlobalAveragePooling2D()(base)\ninner = keras.layers.Dense(100, activation='relu')(vectors)\ndrop = keras.layers.Dropout(rate=0.2)(inner)\noutputs = keras.layers.Dense(10, activation='softmax')(drop)\nmodel = keras.Model(inputs, outputs)\nlearning_rate = 0.005\noptimizer = keras.optimizers.Adam(learning_rate=learning_rate)\nloss = keras.losses.CategoricalCrossentropy()\n```", "```py\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n```", "```py\nmodel.fit(train_ds, validation_data=validation_ds, epochs=10)\nEpoch 1/10\n97/97 [==============================] - 18s 156ms/step - loss: 1.1373 - accuracy: 0.6228 - val_loss: 0.7507 - val_accuracy: 0.7830\n...\nEpoch 10/10\n97/97 [==============================] - 12s 118ms/step - loss: 0.1951 - accuracy: 0.9289 - val_loss: 0.7641 - val_accuracy: 0.7918\n```", "```py\ntrain_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                               rotation_range=10,\n                               shear_range=10,\n                               width_shift_range=0.2,\n                               height_shift_range=0.2,\n                               zoom_range=0.1,\n                               vertical_flip=True)\ntrain_ds = train_gen.flow_from_directory(directory='./clothing-dataset-small/train/', target_size=(150,150), batch_size=32, seed=42)\nval_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\nvalidation_ds = val_gen.flow_from_directory(directory=\"./clothing-dataset-small/validation/\", target_size=(150,150), batch_size=32, shuffle=False)\nFound 3081 images belonging to 10 classes.\nFound 341 images belonging to 10 classes.\n```", "```py\ndef make_fashion_classification_model(learning_rate: float=0.1):\n    \"\"\"Function to create a dense custom model\"\"\"\n    #### Base model ####\n    inputs = keras.Input(shape=(150,150,3))\n    base = base_cnn_model(inputs)\n    vectors = keras.layers.GlobalAveragePooling2D()(base)\n    #### Dense model ####\n    outputs = keras.layers.Dense(10, activation='softmax')(vectors)\n    model = keras.Model(inputs, outputs)\n    #### Optimizing the model ####\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    loss = keras.losses.CategoricalCrossentropy()\n    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n    return model\n```", "```py\nmodel = make_fashion_classification_model(learning_rate=0.005)\nmodel_run = model.fit(train_ds, validation_data=validation_ds, epochs=50)\nEpoch 1/50\n97/97 [==============================] - 28s 266ms/step - loss: 1.5281 - accuracy: 0.4920 - val_loss: 0.9664 - val_accuracy: 0.6686\n...\nEpoch 32/50\n97/97 [==============================] - 25s 253ms/step - loss: 0.7445 - accuracy: 0.7491 - val_loss: 0.6161 - val_accuracy: 0.8123\n```", "```py\ndef make_fashion_classification_model(learning_rate: float=0.001, inner_layer: int=50, drop_rate: float=0.2):\n    \"\"\"Function to create a dense custom model with learning rate, inner layer and dropout rate\"\"\"\n    #### Base model ####\n    inputs = keras.Input(shape=(150,150,3))\n    base = base_cnn_model(inputs)\n    vectors = keras.layers.GlobalAveragePooling2D()(base)\n    #### Dense model layers ####\n    inner = keras.layers.Dense(inner_layer, activation='relu')(vectors)\n    drop = keras.layers.Dropout(rate=drop_rate)(inner)\n    outputs = keras.layers.Dense(10, activation='softmax')(drop)\n    model = keras.Model(inputs, outputs)\n    #### Optimizing the model ####\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    loss = keras.losses.CategoricalCrossentropy()\n    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n    return model\n```", "```py\ncheckpoint_model = keras.callbacks.ModelCheckpoint(\n    filepath=\"xception_v1_{epoch:02d}_{val_accuracy:.4f}.h5\",\n    monitor=\"val_accuracy\",\n    save_best_only=True,\n    initial_value_threshold=0.8,\n    mode=\"max\")\nmodel = make_fashion_classification_model()\nmodel.fit(train_ds, validation_data=validation_ds, epochs=50, callbacks=[checkpoint_model])\nEpoch 1/50\n97/97 [==============================] - 28s 266ms/step - loss: 1.4822 - accuracy: 0.5138 - val_loss: 0.8920 - val_accuracy: 0.7155\n...\nEpoch 37/50\n97/97 [==============================] - 24s 250ms/step - loss: 0.5789 - accuracy: 0.7916 - val_loss: 0.5809 - val_accuracy: 0.8123\n```", "```py\nmodel = keras.models.load_model('xception_v1_37_0.8123.h5')\ntest_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_ds = test_gen.flow_from_directory(directory=\"./clothing-dataset-small/test/\", target_size=(150,150), batch_size=32, shuffle=False, seed=42)\naccuracy = model.evaluate(test_ds)[1]\nprint(f\"accuracy on train data was 79.16, where as validation accuracy was 81.23, but test accuracy is {accuracy*100:.2f}\")\nFound 372 images belonging to 10 classes.\n12/12 [==============================] - 2s 94ms/step - loss: 0.6317 - accuracy: 0.7715\naccuracy on train data was 79.16, whereas validation accuracy was 81.23, but test accuracy is 77.15\n```", "```py\nlabels = [i for i in train_ds.class_indices.keys()]\ndef preprocess_image(path: str, target_size: tuple):\n    \"\"\"Function to preprocess image\"\"\"\n    img = load_img(path=path, target_size=target_size)\n    img_input = np.array([np.array(img)])\n    preprocessed_image = preprocess_input(img_input)\n    return preprocessed_image\ndef decode_predictions(pred):\n    \"\"\"Function to decode prediction\"\"\"\n    result = {c: format(float(p), '.8f') for c, p in zip(labels, pred)}\n    final_prediction = sorted(result.items(), key=lambda x:x[1],  reverse=True)[0]\n    return final_prediction\n```", "```py\npath = './clothing-dataset-small/train/pants/188eaa2d-1a69-49b1-a9fb-7b3789ac93b4.jpg'\nload_img(path)\n```", "```py\npreprocessed_image = preprocess_image(path=path, target_size=(150,150))\npreds = model.predict(preprocessed_image)\nresults = decode_predictions(preds[0])\nresults\n1/1 [==============================] - 0s 26ms/step\n('pants', '0.99980551')\n```", "```py\npip install transformers sentencepiece\npip install mosestokenizer sacremoses\n```", "```py\nfrom transformers import MarianMTModel, MarianTokenizer\nsrc_text = ['The man glanced suspiciously at the door', 'Peter thought he looked very cool', 'Most individuals are rather nice']\n```", "```py\ntranslator = 'Helsinki-NLP/opus-mt-en-es'\ntokenizer = MarianTokenizer.from_pretrained(translator)\nmodel = MarianMTModel.from_pretrained(translator)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\ntrans_out = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n```", "```py\nback_translator = 'Helsinki-NLP/opus-mt-es-en'\ntokenizer = MarianTokenizer.from_pretrained(back_translator)\nmodel_es = MarianMTModel.from_pretrained(back_translator)\nback_translated = model_es.generate(**tokenizer(trans_out, return_tensors=\"pt\", padding=True))\n[tokenizer.decode(t, skip_special_tokens=True) for t in back_translated]\n```", "```py\npip install sdv\npip install pandas\n```", "```py\nnames = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n                 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n                 'capital-gain', 'capital-loss', 'hours-per-week',\n                 'native-country', 'income']\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\ndf = pd.read_csv(url, header=None, names=names, na_values=['?', ' ?'])\n```", "```py\nfrom sdv.metadata import SingleTableMetadata\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(df)\nfrom sdv.single_table import GaussianCopulaSynthesizer\ngc_model = GaussianCopulaSynthesizer(metadata)\ngc_model.fit(df)\n```", "```py\ngc_synthetic = gc_model.sample(num_rows=df.shape[0] )\n```", "```py\nfrom sdmetrics.single_column import CategoryCoverage\nCategoryCoverage.compute(\n    real_data=df['workclass'],\n    synthetic_data=synthetic['workclass']\n)\n```", "```py\nfrom sdmetrics.single_column import MissingValueSimilarity\nMissingValueSimilarity.compute(\n    real_data=df['marital-status'],\n    synthetic_data=synthetic['marital-status']\n)\n```", "```py\nfrom sdv.evaluation.single_table import run_diagnostic\ndiagnostic = run_diagnostic(\n    real_data=df,\n    synthetic_data=gc_synthetic,\n    metadata=metadata\n)\n```", "```py\nOverall Score: 100.0%\nProperties:\n- Data Validity: 100.0%\n- Data Structure: 100.0%\n```", "```py\nfrom sdv.evaluation.single_table import evaluate_quality\nquality_report = evaluate_quality(\n    real_data=df,\n    synthetic_data=gc_synthetic,\n    metadata=metadata\n)\n```", "```py\nOverall Score: 84.6%\nProperties:\n- Column Shapes: 87.57%\n- Column Pair Trends: 81.63%\n```", "```py\nfig = quality_report.get_visualization(property_name='Column Pair Trends')\nfig.show()\n```", "```py\nfrom sdv.evaluation.single_table import get_column_plot\nfig = get_column_plot(\n    real_data=df,\n    synthetic_data=gc_synthetic,\n    metadata=metadata,\n    column_name='capital-gain'\n)\nfig.show()\n```", "```py\ngc_model.get_learned_distributions()\n```", "```py\ngc_model2 = GaussianCopulaSynthesizer(\n    metadata,\n    numerical_distributions={\n        'capital-gain': 'gamma',\n    })\ngc_model2.fit(df)\n```", "```py\nfrom sdv.single_table import CopulaGANSynthesizer\ncg_model = CopulaGANSynthesizer(metadata)\ncg_model.fit(df)\n```", "```py\nquality_report = evaluate_quality(\n    real_data=df,\n    synthetic_data=cg_synthetic,\n    metadata=metadata\n)\nOverall Score: 87.39%\nProperties:\n- Column Shapes: 91.75%\n- Column Pair Trends: 83.04%\n```", "```py\nnew_df = df.head(1000)\nmodel = GaussianCopulaSynthesizer(metadata)\nmodel.fit(new_df)\nsynthetic = model.sample(num_rows=df.shape[0] )\n```", "```py\npip install gower\nimport gower\ngowerMatrix=gower.gower_matrix(new_df, synthetic)\nprint(gowerMatrix)\n```", "```py\ngower.gower_topn(df.iloc[:,], synthetic.iloc[:,], n = 10)\n```", "```py\n{'index': array([26320, 29200, 18735, 24149, 18316, 22925,  4836, 15360, 42, 3523]),'values': array([0.02205753, 0.02578343, 0.03649067, 0.0374441 , 0.03785798, 0.04503146, 0.06345809, 0.08126822, 0.08237292, 0.08368524], dtype=float32)}\n```", "```py\nimport imblearn\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom collections import Counter\nprint(imblearn.__version__)\n0.10.1\n```", "```py\nresults = []\nover_sampling = [0.65,0.7, 0.75, 0.8, 'auto']\nn_neighbours = [1,3,5,7,9,10]\nfor os in over_sampling:\n    for k in n_neighbours:\n        oversample = ADASYN(random_state=1, sampling_strategy=os, n_neighbors=k)\n        counter = Counter(y_train)\n        print(f\"data size before applying smote technique is {counter}\")\n        X_train_synthetic, y_train_synthetic = oversample.fit_resample(X_train_transformed, y_train)\n        counter = Counter(y_train_synthetic)\n        print(f\"data size after applying smote technique is {counter}\")\n        model, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(\n        X_train=X_train_synthetic,\n        y_train=y_train_synthetic,\n        X_test=X_test_transformed,\n        y_test=y_test,\n        clf=d_clf,\n        params=d_param_grid)\n        results.append((os, k, train_roc, test_roc, train_acc, test_acc))\nsynthetic_df = pd.DataFrame(columns=['os_strategy', \"n_neighbours\", \"train_roc\", \"test_roc\", \"train_acc\", \"test_acc\"], data=results)\n```", "```py\ncounter = Counter(y_train)\nprint(f\"data size before applying smote technique {tech_name} is {counter}\")\n# transform the dataset\noversample = ADASYN(random_state=1, n_neighbors=7, sampling_strategy=0.75)\nX_train_synthetic, y_train_synthetic = oversample.fit_resample(X_train_transformed, y_train)\ncounter = Counter(y_train_synthetic)\nprint(f\"data size after applying smote technique {tech_name} is {counter}\")\nmodel, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(\nX_train=X_train_synthetic,\ny_train=y_train_synthetic,\nX_test=X_test_transformed,\ny_test=y_test,\nclf=d_clf,\nparams=d_param_grid)\ndata size before applying smote technique adasyn is Counter({1: 379, 0: 173})\ndata size after applying smote technique adasyn is Counter({1: 379, 0: 321})\nDecision tree optimised\nGetting the best params which are {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 30, 'random_state': 1}\nTraining roc is 0.8816528164788466, and testing roc is 0.8629130966952264\n             training accuracy is 0.8371428571428572, testing_acc as 0.8387096774193549\n```"]