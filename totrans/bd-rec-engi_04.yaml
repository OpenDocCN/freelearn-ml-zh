- en: Chapter 4. Data Mining Techniques Used in Recommendation Engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data mining techniques lie at the heart of recommendation engines. These data
    mining techniques help us extract patterns, group users, calculate similarities,
    predict preferences, handle sparse input data, evaluate recommendation models,
    and so on. In the previous chapter, we have learned about recommendation engines
    in detail. Though we did not get into the implementation of recommendation engines,
    we learned the theory behind the different types of recommendations engines, such
    as neighborhood-based, personalized, contextual recommenders, hybrids, and so
    on. In this chapter, we shall look into the popular data mining techniques currently
    used in building recommendation engines. The reason for dedicating a separate
    chapter to this is that we will come across many techniques while implementing
    recommendation engines in the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is broadly divided into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Neighbourhood-based techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard similarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson correlation coefficient
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematical modelling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix factorization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternating Least Squares
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector space models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency-inverse document frequency
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root-mean-square error
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each section is explained with the basic technique and its implementation in
    R.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start refreshing the basics that are mostly commonly used in recommendation
    engines.
  prefs: []
  type: TYPE_NORMAL
- en: Neighbourhood-based techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As introduced in the previous chapters, the neighbourhood methods are very
    simple techniques, which are used right from the beginning of building recommendation
    engines. These are the oldest yet most widely used approaches, even today. The
    popularity of these widely used approaches is because of their accuracy in generating
    recommendations. We know that almost every recommender system works on the concept
    of similarity between items or users. These neighbourhood methods consider the
    available information between two users or items as two vectors, and simple mathematic
    calculation is applied between these two vectors to see how close they are. In
    this section, we will discuss the following neighbourhood techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard Similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson correlation coefficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Euclidean distance similarity is one of the most common similarity measures
    used to calculate the distance between two points or two vectors. It is the path
    distance between two points or vectors in vector space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we see the path distance between the two vectors,
    a and b, as Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Euclidean distance](img/image00256.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Euclidean distance is based on the Pythagoras's theorem to calculate the distance
    between two points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Euclidean distance** between two points or objects (point *x* and point
    *y*) in a dataset is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Euclidean distance](img/image00257.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* and *y* are two consecutive data points, and n is the number of attributes
    for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How is Euclidean distance applied in recommendation engines?
  prefs: []
  type: TYPE_NORMAL
- en: Consider a rating matrix containing user IDs as rows, item IDs as columns, and
    the preference values as cell values. The Euclidean distance between two rows
    gives us the user similarity, and the Euclidean distance between two columns gives
    us the item similarity. This measure is used when the data is comprised of continuous
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R script for calculating the Euclidean distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Euclidean distance](img/image00258.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Cosine similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cosine similarity** is a measure of similarity between two vectors of an
    inner product space that measures the cosine of the angle between them; it''s
    given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00259.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let *a* be a vector (*a1*, *a2*, *a3*, *a4*) and *b* be another vector (*b1*,
    *b2*, *b3*, *b4*). The dot product between these vectors *a* and *b* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a.b = a1b1 + a2b2 + a3b3 + a4b4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resultant will be a single value, a scalar constant. What does it mean
    to take the dot product between two vectors? To answer this question, let''s define
    the geometric definition of dot product between two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00260.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'On rearranging the preceding equation, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00261.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the earlier equation, *cosθ* is the angle between two vectors, and *acosθ*
    is the projection of vector A onto vector B.
  prefs: []
  type: TYPE_NORMAL
- en: 'The visual vector space representation of the dot product between two vectors
    can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00262.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'When cos angle between the two vectors is 90 degrees, the *cos 90* will become
    zero, and the whole dot product will be zero, that is, they will be orthogonal
    to each other. The logical conclusion we can infer is that they are very far from
    each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00263.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When we reduce the cosine angle between the two vectors, their orientation will
    look very similar to each other's.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the angle between the two vectors is zero, *cos 0* will be 1, and both
    the vectors will lie on each other, as shown in the following image. Thus, we
    can say that the two vectors are similar to each other in terms of their orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00264.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So on summarizing, we can conclude that when we compute the cosine angle between
    two vectors, the resultant scalar value will indicate how close the two vectors
    are with each other in terms of orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine similarity](img/image00265.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s revisit our original question: what does the dot product mean? When
    we take the dot product between two vectors, the resultant scalar value represents
    the cosine angle between them. If the scalar is zero, the two vectors are orthogonal
    and unrelated. If the scalar is 1, the two vectors are similar.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how is this applied in recommendation engines?
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, consider a rating matrix containing user IDs as rows and
    item IDs as columns. We can assume each row as user vectors and each column as
    item vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The cosine angle between row vectors will give the user similarity, and cosine
    angle between column vectors give the item similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R script for calculating the cosine distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, *x* is matrix containing all the variables in a dataset; the cosine function
    is available in the `lsa` package. The `lsa` is a text mining package available
    in r used for discovering latent features or topics within the text. This package
    provides `cosine()` method to calculate cosine angle between two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Jaccard similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jaccard simlarity is another type of similarity measure used in recommendation
    engines. The **Jaccard similarity** coefficient is calculated as the ratio of
    the intersection of features to the union of features between two users or items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically speaking, if *A* and *B* are two vectors, the Jaccard similarity
    is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jaccard similarity](img/image00266.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Jaccard similarity coefficient metric is a statistic used to find the similarity
    and diversity in sample sets. Since users and items can be represented as vectors
    or sets, we can easily apply the Jaccard coefficient to recommender systems in
    order to find similarity between users or items.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R script for calculating the Jaccard similarity is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `clusteval` package in r is a popular package for evaluating clustering
    techniques. `Cluster_similarity()` method provides a very good implementation
    for calculating Jaccard similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way of finding the aforementioned similarity is to find the correlation
    between two vectors. Instead of using the distance measures as a way of finding
    similarity among vectors, we use the correlation between vectors in this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pearson correlation coefficient can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pearson correlation coefficient](img/image00267.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *r* is the correlation coefficient, *n* is the total number of data points,
    *x[i]* is the *i^(th)* vector point of the x vector, *y[i]* is the *i^(th)* vector
    point of the y vector*, x-bar* is the mean of vector x*, y-bar* is the mean of
    vector y*, s[x]* is the standard deviation of vector x*, and s[y]* is the standard
    deviation of vector y.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of computing the correlation coefficient between two variables is
    by dividing the covariance of the two variables by the product of their standard
    deviations,
  prefs: []
  type: TYPE_NORMAL
- en: 'given by ![Pearson correlation coefficient](img/image00268.jpeg)(rho):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pearson correlation coefficient](img/image00269.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand this with an example, as shown in the following image. We
    plot the values of two vectors a, b; it is natural to assume that if all the points
    of the vectors vary together, there exists a positive relation among them. This
    tendency to vary together, or covariance, in simple terms, can be called correlation.
    Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pearson correlation coefficient](img/image00270.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now examine the following image. We can observe that the vectors are
    not varying together, and the corresponding points are scattered randomly. So
    the tendency to vary together, or covariance, is less or in other less correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pearson correlation coefficient](img/image00271.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the similarity calculation aspect, we can conclude that the more the correlation
    between two vectors, the more similar they are.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how is the Pearson correlation coefficient applied in recommendation engines?
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, consider a rating matrix containing user IDs as rows and
    item IDs as columns. We can assume each row as user vectors and each column as
    item vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation coefficient between the row vectors will give the user similarity,
    and the correlation coefficient between the column vectors will give the item
    similarity using the following equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R script is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, `mtcars` is the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematic model techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematical models such as matrix factorization and SVD have proved to be very
    accurate when it comes to building recommendation engines over the similarity
    calculation measures. Another advantage is their ability to scale down easily
    also allowed to design the systems easily. In this chapter, we will learn about
    the mathematical models as explained next.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix factorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A matrix can be decomposed into two low rank matrices, which when multiplied
    back will result in a single matrix approximately equal to the original matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that *R*, a rating matrix of size *U X M* can be decomposed into two
    low rank matrices, *P* and *Q*, of size *U X K* and *M X K* respectively, where
    *K* is called the rank of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the original matrix of size *4 X 4* is decomposed
    into two matrices, *P (4 X 2)* and *Q (4 X 2)*; multiplying back *P* and *Q* will
    bring me the original matrix of size *4 X 4* and values approximately equal to
    those of the original matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix factorization](img/image00272.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the major advantages of the matrix factorization method is that we can
    compute the empty cells in the original matrix, *R*, using the dot product between
    the low-rank matrices *P*, *Q*. This is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix factorization](img/image00273.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When we apply the previous equation, we can reproduce the original matrix, *R*,
    with all the empty cells filled.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make the predicted values as close as to the original matrix as
    possible, we have to minimize the difference between the original values and the
    predicted values, which is also also known as error. The error between the original
    value and predicted value can be given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix factorization](img/image00274.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In order to minimize the aforementioned error term and reproduce the original
    matrix as closely as possible, we have to use gradient descend technique-an algorithm
    to find out optimal parameters of an objective function and minimize the function
    in an iterative manner, introduce Regularization term to the equation.
  prefs: []
  type: TYPE_NORMAL
- en: How is matrix factorization applied to recommendation engines?
  prefs: []
  type: TYPE_NORMAL
- en: This is core question, which we are more interested in rather than the mathematics
    involved in matrix factorization. We will see how we can apply matrix factorization
    techniques in building recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the core tasks in building recommendation engines: finding similar users
    or items, then predicting the non-rated preferences, and finally recommending
    new items to active users. In short, we are predicting the non-rated item preferences.
    Recall this is what matrix factorization does: predicting the empty cells in the
    original rating matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how do we justify our approach of applying matrix decomposition to low-rank
    matrices to recommendation engines? To answer this question, we will discuss how
    users rate movies. People rate movies because of the story or actors or the genre
    of the movie, that is, users rate items because of the features of the items.
    When given a rating matrix containing user IDs, item IDs, and rating values, we
    can make an assumption that users will have some inherent preferences toward rating
    items, and the items will also have inherent features that help users rate them.
    These features of users and items are called **latent features**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the earlier assumption, we apply the matrix factorization technique
    to the rating matrix the two low-rank matrices, which are assumed to be the user
    latent feature matrix and item latent feature matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix factorization](img/image00275.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering these assumptions, researchers started applying matrix factorization
    techniques in building recommender systems. The advantage of the matrix factorization
    methods is that since it is a machine-learning model, the feature weightages are
    learned overtime, improving the model accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code explains the implementation of Matrix Factorization using
    `nmf` package in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Alternating least squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall the error minimization equation in the previous section. Upon introducing
    a regularization term to avoid over fitting, the final error term would look like
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternating least squares](img/image00276.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to optimize the preceding equation, there are two popular techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent** (**SGD**): A mini-batch optimizing technique,
    similar to gradient descend, for finding optimal parameters in large-scale data
    or sparse data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternating least squares**(**ALS**): The main advantage of ALS method over
    SGD is that it can be easily parallelized on distributed platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will look into the ALS method.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding equation involves two unknowns, which we need to solve. Since
    two unknowns are involved, the aforementioned equation is a non-convex problem.
    If we fix one of the unknown term constants, this optimization problem will become
    quadratic and can be solved optimally.
  prefs: []
  type: TYPE_NORMAL
- en: Alternating least squares is an iterative method, which involves computing one
    feature vector term using the least squares function by fixing the other feature
    vector term constant until we solve the preceding equation optimally.
  prefs: []
  type: TYPE_NORMAL
- en: In order to compute the user feature vector, we fix the item feature vector
    as a constant and solve for least squares. Similarly, while computing the item
    feature vector, we fix the user feature vector as a constant and solve for least
    squares.
  prefs: []
  type: TYPE_NORMAL
- en: Following this approach, we are able to convert a non-convex problem into a
    quadratic one, which can be solved optimally.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the open source distributed platforms such as Mahout and Spark use the
    ALS method to implement scalable recommender systems for their ability to be parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Singular value decomposition** (**SVD**) is another very popular matrix factorization
    method. In simple terms, an SVD method decomposes a real matrix A of size m x
    n into three matrices, U,![Singular value decomposition](img/image00277.jpeg),V,
    which satisfy the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/image00278.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous equation, *r* is called the rank of the matrix, *A*. *U*, *V*
    are orthogonal matrices, and ![Singular value decomposition](img/image00277.jpeg) is
    a diagonal matrix having all singular values of the matrix *A*. The values of
    the *U* and *V* are real if *A* is a real matrix. The values of matrix ![Singular
    value decomposition](img/image00277.jpeg) are positive and real and are available
    in a decreasing order.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVD can also be used as a dimensionality reduction technique, following two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a rank *k* that is less than *r.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recompute or subsize the *U*, ![Singular value decomposition](img/image00277.jpeg) ,*V*
    matrices to *(m x k)*, *(k x k)*, *(k x n)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/image00279.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The matrices obtained by applying SVD are very much applicable to recommender
    systems as they provide the best low-rank approximations of the original matrix.
    How do we apply the SVD approach to recommendation? Let's take a rating matrix
    R of size m x n containing many empty cells. Similar to matrix factorization,
    our objective is to compute an approximate rating matrix as close as possible
    o the original matrix with the missing values being predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Applying SVD on R will produce three matrices, *U*, ![Singular value decomposition](img/image00277.jpeg),
    *V*, of sizes, let's say, *m x r*, *r x r*, *r x n*. Here, *U* represents the
    user latent feature vector representations, *V* represents the item latent feature
    vector representations, and ![Singular value decomposition](img/image00277.jpeg) represents
    the independent feature representation, r, of user and items. By setting the value
    of the independent feature representation to a value *k* less than *r*, we are
    choosing the k optimal latent features, thereby reducing the size of the matrix.
    The k-value can be chosen using the cross-validation approach as the value of
    *k* defines the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simpler method for choosing the value *k* as ![Singular value decomposition](img/image00277.jpeg) is
    to take a diagonal matrix that contains singular values in a descending order,
    choose the values in the diagonal that have higher values, and eliminate very
    less diagonal values.
  prefs: []
  type: TYPE_NORMAL
- en: After choosing the k-value, we now resize or choose the first k-column in each
    of the matrices *U*,![Singular value decomposition](img/image00277.jpeg),*V*.
    This step will render matrices *U*,![Singular value decomposition](img/image00277.jpeg),*V*
    of size *m x k*, *k x k*, and *k x n* respectively, please refer to the below
    image. After we resize the matrices, we move ahead to the final step.
  prefs: []
  type: TYPE_NORMAL
- en: In the final step, we will compute the dot products of the following series
    of matrices in order to calculate the approximate rating matrix ![Singular value
    decomposition](img/image00280.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition](img/image00281.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Below code snippet shows SVD implementations in R, the following code creates
    a sample matrix and then SVD is applied, using `svd()` available in base package
    in r, on the sample data to create 3 matrices, dot product between three matrices
    will get back our approximate original matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please refer to [Chapter 7](part0051.xhtml#aid-1GKCM2 "Chapter 7. Building Real-Time
    Recommendation Engines with Spark"), *Building Real-Time Recommendation Engines
    with Spark* for ALS implementation in Spark-python.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about the most important or the most frequently
    used machine learning techniques, which are widely used in building recommendation
    engines.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear regression** may be treated as a simple, popular, and the foremost
    approach for solving prediction problems. We employ linear regression where our
    objective is to predict the future outcomes, given input features and the output
    label is a continuous variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In linear regression, given historical input and output data, the model will
    try to find out the relation between independent feature variables and the dependent
    output variable given by the following equation and diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image00282.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* represents the output continuous dependent variable, *x* represents
    independent feature variables, *β0* and *β1* are the unknowns or feature weights,
    *e* represents the error.
  prefs: []
  type: TYPE_NORMAL
- en: Using the **ordinary least squares (OLS)** approach, we will estimate the unknowns
    in the preceding equation. We shall not go deep into the linear regression approach,
    but here we will discuss how we can use linear regression in recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the core tasks in recommendation engines is to make predictions for
    non-rated items for users. For example, in case of item-based recommendation engines,
    the prediction for item *i* by user *u* is done by computing the sum of ratings
    given by user *u* to items similar to item *i*. Then each rating is weighted by
    its similarity value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image00283.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of using this weighted average approach to make the predictions, we
    can use the linear regression approach to calculate the preference values for
    user *u* for item *i*. While using regression approach, instead of using the original
    rating values of similar items, we use their approximate rating values based on
    the linear regression model. For example, to predict the rating for item *i* by
    user *u*, we can use the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image00284.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Linear regression using R is given by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `lm()` function, available in `stats` package I R, usually used to fit linear
    regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification models fall into the category of the supervised form of machine
    learning. These models are usually employed in prediction problems, where the
    response is binary or multiclass labels. In this chapter, we will discuss many
    types of classification models, such as logistic regression, KNN classification,
    SVM, decision trees, random forests, bagging, and boosting. Classification models
    play a very crucial role in recommender systems. Though classification models
    don't play a great role in neighbourhood methods, they play a very important role
    in building personalized recommendations, contextual aware systems, and hybrid
    recommenders. Also, we can apply classification models to the feedback information
    about the recommendations, which can further be used for calculating the weightages
    for user features.
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Logistic regression is the most common among classification models. **Logistic
    regression** is also known as **linear classification** as it is very similar
    to linear regression, except that in regression, the output label is continuous,
    whereas in linear classification, the output label is class variable. In regression,
    the model is a least squares function, whereas in logistic regression, the prediction
    model is a logit function given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear classification](img/image00285.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *e* is the natural logarithm, *x* is the input variable,
    *β[0]* is the intercept, and *β[1]* is the weight of variable x.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may interpret the preceding equation as the conditional probability of response
    variable against the linear combination of input variables. The logit function
    allows to take any continuous variable and gives response in the range of *(0,1)*,
    as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear classification](img/image00286.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic regression using R is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `glm()` function in R is used to fit generalized linear models, popularly
    employed for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: KNN classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The k nearest neighbors classification is popularly known as the KNN classification.
    This is one of the most popular classification techniques. The basic concept in
    KNN classification is that the algorithm considers k nearest items surrounding
    a particular data point and tries to classify this data point into one of the
    output labels based on its k-nearest data points. Unlike other classification
    techniques such as logistic regression, SVM, or any other classification algorithms,
    KNN classification is a non-parametric model, which doesn''t involves any parameter
    estimation. The k in KNN is the number of nearest neighbors to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![KNN classification](img/image00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider 10 data points. We need to classify a test data point, highlighted
    in the preceding diagram, into one of two classes, blue or orange. In this example,
    we classify the test data point using the KNN classification. Let''s say k is
    4; it means that by considering four data points surrounding the active data point,
    we need to classify it by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we need to calculate the distance of each point from the test
    data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the top four closest data points to the test data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the voting mechanism, assign the majority class label count to the test
    data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KNN classification works well in case of highly non-linear problems. Though
    this method works well in most cases, this method being a non-parametric approach
    cannot find the feature importance or weightages.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the KNN classification, there is a regression version of KNN, which
    can be used to predict the continuous output labels.
  prefs: []
  type: TYPE_NORMAL
- en: Both the KNN classification and egression methods find their wide applicability
    in collaborative filtering recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet shows KNN classification using R, in the below code
    snippet we are using `knn3()` available in `caret` package for fitting KNN classification
    and `sample_n()` available in `dplyr` package to select random rows from a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Support vector machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Support vector machines** algorithms are a form of supervised learning algorithms
    employed for solving classification problems. SVM is generally treated as one
    of the best algorithms for dealing with classification problems. Given a set of
    training examples, where each of the data points falls into one of two categories,
    an SVM training algorithm builds a model that assigns new data points into one
    category or the other. This model is a representation of the examples as points
    in space, mapped so that the examples of the separate categories are divided by
    a margin that is as wide as possible, as shown in the following figure. New examples
    are then mapped into that same space and predicted to belong to a category based
    on which side of the gap they fall on. In this section, we will go through an
    overview and implementation of SVMs without going into mathematical details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When SVM is applied to a p-dimensional dataset, the data is mapped to a p-1
    dimensional hyper plane, and the algorithm finds a clear boundary with sufficient
    margin between classes. Unlike other classification algorithms that also create
    a separating boundary for classifying data points, SVM tries to choose a boundary
    that has maximum margin for separating the classes, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machines](img/image00288.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Consider a two-dimensional dataset having two classes, as shown in the previous
    figure. Now, when the SVM algorithm is applied, firstly it checks whether a one-dimensional
    hyper plane exists to map all the data points. If the hyper plane exists, the
    linear classifier creates a decision boundary with a margin to separate the classes.
    In the preceding figure, the thick redline is the decision boundary, and the thinner
    blue and red lines are the margins of each class from the boundary. When new test
    data is used to predict the class, the new data falls into one of the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few key points to be noted:'
  prefs: []
  type: TYPE_NORMAL
- en: Though an infinite number of hyper planes can be created, SVM chooses only one
    hyper plane that has maximum margin, that is, the separating hyper plane that
    is farthest from the training observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This classifier is only dependent on the data points that lie on the margins
    of the hyper plane, that is, on thin margins in the figure but not on other observations
    in the dataset. These points are called support vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision boundary is affected only by the support vectors but not by other
    observations located away from the boundaries, that is, if we change the data
    points other than the support vectors, there will not be any effect on the decision
    boundary, but if the support vectors are changed, the decision boundary changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large margin on the training data will also have a large margin on the test
    data so as to classify the test data correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines also perform well with non-linear datasets. In this
    case, we use radial kernel functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See below for R implementation of SVM on the `iris` dataset. We use the `e1071`
    package to run SVM. In R, the `SVM()` function contains the implementation of
    Support Vector Machines present in the `e1071` package.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cross-validation method is used to evaluate the accuracy of predictive models
    before testing future unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the SVM method is called with the `tune()` method, which performs
    cross validation and runs the model on different values of the cost parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tune$best.model`tells us that the model works best with cost parameter
    as `10` and the total number of support vectors as `25`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Decision trees` is a simple, fast, and tree-based supervised learning algorithm
    for solving classification problems. Though not very accurate when compared to
    other logistic regression methods, this algorithm comes in handy while dealing
    with recommender systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the decision trees with an example. Imagine a situation where
    you have to predict the class of flower based on its features such as petal length,
    petal width, sepal length and sepal width. We apply the decision trees methodology
    to solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: We consider the entire data at the start of the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we choose a proper question/variable to divide the data into two parts.
    In our case, we choose to divide the data based on petal length > 2.45 and <=
    2.45\. This separates flower class `setosa` from the rest of the classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We further divide the data having petal length > 2.45, based on same variable
    with petal length < 4.5 and >= 4.5 as shown in the following diagram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This splitting of the data will be further divided by narrowing down the data
    space until we reach a point where all the bottom points represent the response
    variables or where further logical split cannot be done on the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following decision tree diagram, we have one root node, four internal
    nodes where data split occurred, five terminal nodes where data split cannot be
    done further, and they are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Petal Length < 2.5 as root node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length < 2.5, petal length < 4.85, sepal length < 5.15, and petal width
    < 1.75 are called internal nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final nodes having the class of the flowers are called terminal nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lines connecting the nodes are called the branches of the tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While predicting responses on new data using the aforementioned built model,
    each of the new data points is taken through each of the nodes, a question is
    asked, and a logical path is taken to reach its logical class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/image00289.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Take a look at the decision tree implementation in R on the `iris` dataset using
    tree package available from CRAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary of the mode given next tells us that the misclassification rate
    is 0.0381, indicating that the model is very accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Decision trees](img/image00290.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Below code shows plotting the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code displays the decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/image00291.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the prediction values made using `pred()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/image00292.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Ensemble methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In data mining, we use ensemble methods, which refer to using multiple learning
    algorithms to obtain better predictive results than applying any single learning
    algorithm on any statistical problem. This section deals with an overview of popular
    ensemble methods such as bagging, boosting, and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Random forests refer to improvised supervised algorithm than bootstrap aggregation
    or bagging method though built on a similar approach. Unlike selecting all the
    variables in all the B samples generated using the bootstrap technique in bagging,
    we select only a few predictor variables randomly from total variables for each
    of the B samples, and then these samples are trained with the models. Predictions
    are made by averaging the result of each model. The number of predictors in each
    sample is decided using the formula ![Random forests](img/image00293.jpeg), where
    *p* is the total variable count in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach removes the condition of dependency of strong predictor in the
    dataset as we intentionally select fewer variables than all the variables for
    every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: This approach also decorrelates variables resulting in less variability in the
    model, hence more reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following R implementation of random forests on the iris
    dataset using the `randomForest` package available from CRAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image will display the model details for random forest built
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/image00294.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Random forests](img/image00295.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Bagging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Bagging** is also known as **bootstrap aggregating**. It is designed to improve
    the stability and accuracy of machine learning algorithms. It helps in avoiding
    overfitting and reduces variance. This is mostly used with decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging involves randomly generating bootstrap samples, random sample with replacement,
    from the dataset and training the models individually. Predictions are then made
    by aggregating or averaging all the response variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a dataset (*Xi*, *Yi*) where *i*=1 ...n, contains n data
    points. The following are the steps to perform bagging on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Now randomly select B samples with replacement from the original dataset using
    the bootstrap technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, train the B samples with regression/classification models independently,
    and then predictions are made on the test set by averaging the responses from
    all the B models generated in case of regression or selecting the most-occurring
    class among B samples in case of classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unlike in bagging where multiple copies of bootstrap samples are created and
    a new model is fitted for each copy of dataset and all the individual models are
    combined to create a single predictive model, in boosting, each new model is built
    using information from previously built models. Boosting can be understood as
    an iterative method involving two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A new model is built on the residuals of the previous models instead of response
    variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now the residuals are calculated from this model and updated to the residuals
    used in previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding two steps are repeated multiple iterations allowing each new model
    to learn from its previous mistakes, thereby improving the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows us gradient boosting using R, `gbm()` package
    in r generally used to perform various regression tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Boosting](img/image00296.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows them model summary visually, showing the relative
    importance of each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Boosting](img/image00297.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding summary states the relative importance of the variables of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Boosting](img/image00298.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Pick the response with the highest probability from the resulting `pred` matrix,
    by doing `apply(pred, 1, which.max)` on the vector output from prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Boosting](img/image00299.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, the output value for the `predict()` function
    is used in the `apply()` function to pick the response with highest probability
    among each row in the pred matrix, and the resultant output from the `apply()`
    function is the prediction for the response variable.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cluster analysis** is the process of grouping objects together in a way that
    objects in one group are more similar than objects in other groups.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, identifying and grouping clients with similar booking activities
    on travel portal, as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, each group is called a **cluster**, and each member
    (data point) of the cluster behaves similar to its group members:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering techniques](img/image00300.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Cluster analysis is an unsupervised learning method. In supervised methods
    such as regression analysis, we have input variables and response variables; we
    fit a statistical model to the input variables to predict the response variable,
    whereas in unsupervised learning methods, we do not have any response variable
    to predict; we only have input variables. Instead of fitting a model to the input
    variables to predict the response variable, we just try to find patterns within
    the dataset. There are three popular clustering algorithms: hierarchical cluster
    analysis, k-means cluster analysis, and two-step cluster analysis. In this section,
    we will learn about k-means clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K-means is an unsupervised, iterative algorithm where k is the number of clusters
    to formed from the data. Clustering is achieved in two steps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The cluster assignment step**: In this step, we randomly choose two cluster
    points (red dot & green dot) and assign each data point to one of the two cluster
    points, whichever is closer to it (Take a look at the top part of the following
    figure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The move centroid step:** In this step, we take the average of the points
    of all the examples in each group and move the centroid to the new position, that
    is, the mean position calculated (Take a look at the bottom part of the following
    image).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding steps are repeated until all the data points are grouped into
    two groups and the mean of the data points at the end of the move centroid step
    doesn''t change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/image00301.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The previous figure shows how a clustering algorithm works on data to form clusters.
    Take a look at the following R implementation of k-means clustering on the iris
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means clustering using R is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Clustplot()` method available in `cluster` package is used to plot the
    clusters formed for the IRIS dataset and is shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/image00302.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The previous figure shows the formation of clusters on the iris data, and the
    clusters account for 95% of the data. In the previous example, the number of clusters
    k value is selected using the elbow method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet explains implementation of k-means clustering, which
    is shown in the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![K-means clustering](img/image00303.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the previous figure, we can observe that the direction of the cost function
    is changed at cluster number 5, hence we choose 5 as our number of clusters k.
    Since the number of optimal clusters is found at elbow of the graph, we call it
    the **elbow method**.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most commonly faced problems while building recommender systems is
    high dimensional and sparse data. Many times, we face a situation where we have
    a large set of features and less number of data points. In such situations, when
    we fit a model to the dataset, the predictive power of the model would be lower.
    This scenario is often termed as the curse of dimensionality. In general, adding
    more data points or decreasing the feature space, also known as dimensionality
    reduction, often reduces the effects of curse of dimensionality. In this section,
    we will discuss principal component analysis, a popular dimensionality reduction
    technique to reduce the effects of the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is a classical statistical technique
    for dimensionality reduction. PCA algorithm transforms the data with high dimensional
    space to a space with fewer dimensions. The algorithm linearly transforms m-dimensional
    input space to n-dimensional (n<m) output space, with the objective of minimizing
    the amount of information/variance lost by discarding (m-n) dimensions. PCA allows
    us to discard the variables/features that have less variance.'
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, PCA uses orthogonal projection of highly correlated variables
    to a set of values of linearly uncorrelated variables called principal components.
    The number of principal components is less than or equal to the number of original
    variables. This linear transformation is defined in such a way that the first
    principal component has the largest possible variance, that is, it accounts for
    as much of the variability in the data as possible by considering highly correlated
    features, and each succeeding component, in turn, has the highest variance by
    using the features that are less correlated with the first principal component,
    which is orthogonal to the preceding component.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this in simple terms. Assume that we have a three-dimensional
    data space with two features more correlated with each other than with the third.
    We now want to reduce the data to a two-dimensional space using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component is created in such a way that it explains maximum
    variance using the two correlated variables along the data. In the following figure,
    the first principal component (the bigger line) is along the data explaining most
    variance. To choose the second principal component, we need to choose another
    line that has the highest variance, is uncorrelated, and is orthogonal to the
    first principal component, The implementation and technical details of PCA is
    out of scope of this book, so we will discuss how it used in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains the spatial representation of principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/image00304.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We illustrate PCA using the `USArrests` dataset. The `USArrests` dataset contains
    crime related statistics, such as `Assault`, `Murder`, `Rape`, `UrbanPop` per
    100,000 residents in 50 states in the U.S..
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA implementation in R is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the `apply()` function to the `USArrests` dataset row-wise to calculate
    the variance to see how each variable is varying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We observe that `Assault` has the most variance. It is important to note at
    this point that scaling the features is a very important step while applying PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply PCA after scaling the feature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s understand the components of PCA output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Pca$rotation` contains the principal component loadings matrix, which explains
    the proportion of each variable along each principal component.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's learn how to interpret the results of PCA using a biplot graph. Biplot
    is used to show the proportions of each variable along the two principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code changes the directions of the biplot; if we don''t include
    the following two lines, the plot will be a mirror image of the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows a plot showing principal components for the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/image00305.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous figure, known as biplot, we can see the two principal components
    (PC1, PC2) of the `USArrests` dataset. The red arrows represent the loading vectors,
    which shows how the feature space varies along the principal component vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plot, we observe that the first principal component vector, PC1, more
    or less places equal weight to three features: rape, assault, and murder. This
    means that these three features are more correlated with each other than with
    the UrbanPop feature. The second principal component, PC2, places more weight
    on UrbanPop than and is less correlated with the remaining three features.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector space models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vector space models** are algebraic models most commonly used in text analysis
    applications for representing text documents using the words as vectors. This
    is widely used in information retrieval applications. In text analysis, let''s
    say we want to find the similarity between two sentences. How do we go about this?
    We know that to compute similarity measure metric, the data should be all numeric.
    When it comes to a sentence we have words rather than numerals. Vectors space
    models allow us to represent the words present in the sentences in numeric form
    so that we can apply any of the similarity calculation metrics, such as cosine
    similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This representation of sentences of words in numeric form can be done in two
    popular ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency inverse document frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand the previously mentioned approaches with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1**: THE CAT CHASES RAT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2**: THE DOG CHASES CAT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3**: THE MAN WALKS ON MAT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given three sentences, our objective is to find the similarity between the sentences.
    It is clear that we cannot directly apply similarity metrics such as cosine directly.
    So now let's learn how to represent them in numeric format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a general notation, each sentence in vector space model is known as a **document**.
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Term frequency** simply means the frequency of the word in a document. To
    find the frequency, we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to find all the unique keywords present in all the documents,
    represented as V:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: V = {THE, CAT, CHASES, RAT, DOG, MAN, WALKS, ON, MAT}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to create vectors of documents, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D1 = {THE, CAT, CHASES, RAT}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D2 = {THE, DOG, CHASES, CAT}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D3 = {THE, MAN, WALKS, ON, MAT}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we have to count the term frequency of all the terms in each
    document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D1 = {(THE,1),(CAT,1),(CHASES,1),(RAT,1)}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D2 = {(THE,1),(DOG,1),(CHASES,1),(CAT,1)}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D3 = {(THE,1),(MAN,1),(WALKS,1),(ON,1),(MAT,1)}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now we will create a term-document matrix with document IDs as rows, unique
    terms as columns, and term frequency as cell values, as follows:![Term frequency](img/image00306.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a while and understand what is happening here: we have put 1 in places
    where the word occurs in the sentences and 0 where the word does not.'
  prefs: []
  type: TYPE_NORMAL
- en: Now observe that we have represented our documents in the form of a numerical
    matrix using the term frequency in each document.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in this term-matrix document, also known as TDM, we can directly apply
    similarity metrics such as cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Term frequency](img/image00307.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The previous figure visually represents the similarity between the documents
    after calculating the cosine angle. From the figure, we can infer that the angle
    between D1 and D2 is less and that between D1 and D3 is more, indicating that
    D1 is more similar to D2 than D3.
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency inverse document frequency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The earlier approach is also known as the bag of words approach where we just
    have to find the frequency of each term in each document and numerically represent
    it in a TDM. But inherently, there is a flaw in this approach. This approach gives
    more weightage or importance to the terms that occur more frequently and less
    importance to the rarely occurring terms. It is important to understand that if
    a term occurs more frequently in most document collections, that term will not
    contribute as a differentiator in identifying a document. Similarly, a term which
    occurs more frequently in a document and less frequently in the whole of the document
    collection will contribute as a differentiator in identifying a particular document.
    This scaling down of weightage to more frequently occurring terms in the document
    collection and scaling up the weightage to more frequently occurring terms in
    document but less frequently in all of the document collection can be achieved
    using **term frequency inverse document frequency** (**tf-idf**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The *tf-idf* can be computed as a product of term frequency of document and
    inverse document frequency of the term:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tf-idf = tf X idf*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *idf* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*idf = log(D/(1+ n(d,t)))*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *D* is the total number of document collections, and *n(d,t)* is the number
    of times a term *t* occurs in all the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute TDM in terms of *tf-idf* for the previous set of documents (*D1*,
    *D2*, *D3*):'
  prefs: []
  type: TYPE_NORMAL
- en: Take the TDM and calculate the term frequency of each term in each of the documents.
    This is the same as what we have done in the TF section:![Term frequency inverse
    document frequency](img/image00308.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, we need to calculate the **document frequency** (**DF**), that
    is, how many times a term occurred in all the document collection. For example,
    let's calculate the DF for the term THE. The term is present in all the three
    documents, hence its DF will be 3\. Similarly, for the term CAT the DF is 2:![Term
    frequency inverse document frequency](img/image00309.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, we shall calculate the inverse of document frequency (IDF) using
    the aforementioned formula for IDF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So for the term THE, the idf will be given by: idf(THE) = log(3/(1+3)) = -0.12494'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Term frequency inverse document frequency](img/image00310.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Compute tf-idf as the product of tf and idf for each of the terms in the whole
    document collection, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, for the term RAT in D1, the tf-idf will be computed as *1 X 0.4777121
    = 0.4777121*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Term frequency inverse document frequency](img/image00311.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now that we have calculated the *tf-idf*, we can compare the previous TDM based
    on *tf-df* with the TDM with *tf*. The main comparisons we can draw are in the
    differences in weightages for each of the terms in TDM. More frequent words across
    the document collection have got less weightage compared to the rarely occurred
    words in the document.
  prefs: []
  type: TYPE_NORMAL
- en: Now, on top of this TDM based on *tf-idf* representation, we can directly apply
    the similarity metric measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we learned about the vector space models and *tf*, *tf-idf*
    concepts, which are widely used in text analysis. Now the real question is: how
    do we apply these techniques in recommendation engines?'
  prefs: []
  type: TYPE_NORMAL
- en: Many a time, while building content-based recommendation engines, we will be
    getting the user preference data in text format or the features of the items as
    text. In such cases we might able to apply the aforementioned techniques to represent
    the text data as numerical vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Also many times we need to find the feature importance or feature weightage
    to the item features while building personalized content-based recommendation
    engines. In such cases, vector space models concepts are very useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to calculate `tfidf` in R. In the code,
    we use `TermDocumentMatrix()` and `weightTFidf()` to calculate the term document
    matrix and `tfidf` respectively available in the `tm` package in R. The `inspect()`
    method is used to attain the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot just shows a very small portion of the large document
    term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Term frequency inverse document frequency](img/image00312.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we saw various data mining techniques used in recommender
    systems. In this section, we will learn how to evaluate models built using data
    mining techniques. The ultimate goal for any data analytics model is to perform
    well on future data. This objective can be achieved only if we build a model,
    which is efficient and robust during the development stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'While evaluating any model, the most important things we need to consider are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the model is overfitting or underfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How well the model fits the future data or the test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting, also known as bias, is a scenario in which the model doesn't even
    perform well on training data; this means that we are fitting a less robust model
    to the data; for example, letting the data be distributed non-linearly and fitting
    it with a linear model. From the following image, we see that data is non-linearly
    distributed. Assume that we have fitted a linear model (orange line). In this
    case, during the model-building stage itself, the prediction power will be low.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting is a scenario in which the model performs well on training data
    but does really bad on test data. This scenario arises when the model memorizes
    the data pattern rather than learning from the data; for example, letting the
    data be distributed non-linearly and fitting a complex model (green line). In
    this case, we observe that the model is fitted very close to the data distribution,
    taking care of every up and down. In this case, the model is most likely to fail
    on previously unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation techniques](img/image00313.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows simple, complex, and appropriately fitted models
    training data. The green fit represents overfitting, the orange line represents
    underfitting, the black and blue lines represent the appropriate model, which
    is a trade-off between the underfit and the overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Any fitted model is evaluated to avoid the aforementioned scenarios using cross-validation,
    regularization, pruning, model comparisons, ROC curves, confusion matrix, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is very popular technique for model evaluation for almost all models. In
    this technique, we divide the original data into multiple folds/sets (say 5) of
    training dataset and test dataset. At each fold iteration the model is built using
    the training dataset and evaluated using the test dataset. This process is repeated
    for all the folds. The test errors are calculated for very iteration. The average
    test error is calculated to generalize the model accuracy at the end of all the
    iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation implementation is explained in [Chapter 5](part0037.xhtml#aid-1394Q1
    "Chapter 5. Building Collaborative Filtering Recommendation Engines"), *Building
    Collaborative Filtering Recommendation Engines*.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this technique, the data variables are penalized to reduce the complexity
    of the model with the objective of minimizing the cost function. There are the
    two most popular regularization techniques: Ridge Regression and Lasso Regression.
    In both the techniques, we try to reduce the variable coefficients to zero so
    less number of variables will fit the data optimally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The popular evaluation metrics for recommendation engines are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Root-mean-square error (RMSE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean absolute error (MAE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root-mean-square error (RMSE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Root-mean-square error** is one of the most popular, frequently used, simple
    measures to find the accuracy of a model. In a general sense, it is the difference
    between the actual and predicted values. By definition, it is the squared root
    of mean square error, as given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Root-mean-square error (RMSE)](img/image00314.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *X[act]* refers to the observed values, and *X[pred]* refers to the predicted
    values.
  prefs: []
  type: TYPE_NORMAL
- en: How is RMSE applicable to recommendation engines?
  prefs: []
  type: TYPE_NORMAL
- en: One of the core tasks in recommendation engines is to predict the preference
    values for non-rated items for particular users. We use many approaches discussed
    in previous sections to predict these non-rated preference values. Consider the
    following rating matrix used for building a recommendation model. Assume that
    our recommendation engine model has predicted all the empty cells in the following
    figure let they be represented as r hat. Also assume that we know the actual values
    of these predicted empty cells; let they be represented as r.
  prefs: []
  type: TYPE_NORMAL
- en: '![Root-mean-square error (RMSE)](img/image00315.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now use the values of *r hat* and *r* in the preceding equation to calculate
    the model accuracy of the predictive power of the recommendation engine.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error (MAE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another popular evaluation technique for the data-mining model is **mean absolute
    error** (**MAE**). This evaluation metric is very similar to RMSE and is given
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean absolute error (MAE)](img/image00316.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is a very simple measure computed as mean error between predicted and actual
    values. MAE is applied in recommendation engines as a way to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After we have deployed a recommendation engine in production, we will be interested
    only if the suggested recommendations are accepted by the users. How do we measure
    the effectiveness of the recommendation engine in terms of whether the model is
    generating valid recommendations? To measure the effectiveness, we can borrow
    the precision-recall evaluation technique, a popular technique in evaluating a
    classification model. The preceding discussion about whether a served recommendation
    is useful or not for a user can be treated as the binary class label of a classification
    model, and then we can calculate the precision-recall.
  prefs: []
  type: TYPE_NORMAL
- en: To understand precision-recall, we should understand a few more metrics that
    go together with precision-recall, such as true positive, true negative, false
    positive, and true negative.
  prefs: []
  type: TYPE_NORMAL
- en: To build what is popularly known as a confusion matrix, as shown next, let's
    take an example of online news recommending site, which contains 50 web pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have generated 35 recommendations to user A. Of these, A has
    clicked on 25 suggested web pages, and 10 web pages are non-clicked. Now with
    this information, we create a table with the number of clicks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the top-left column, enter the count of suggested links that A has clicked
    on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the top-right column, enter the count of suggested links that A has not clicked
    on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the bottom-left column, enter the count of links that A has clicked on but
    have not been suggested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the bottom-right column, enter the count of links that A has not clicked
    on and have not been suggested:![Precision and recall](img/image00317.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top-left count is called **true positive** (**tf**), which indicates the
    count of all the responses where the actual response is positive and the model
    predicted as positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top-right count is called **false positive** (**fp**), which indicates the
    count of all the responses where the actual response is negative but the model
    predicted as positive, in other words, a **FALSE ALARM**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-left count is called **false negative** (**fn**), which indicates
    the count of all the responses where the actual response is positive but the model
    predicted as negative; in general, we call it **A MISS**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-right count is called **true negative** (**tn**), which indicates
    the count of all the responses where the actual response is negative and the model
    predicted negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision and recall](img/image00318.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the preceding information, we shall compute precision-recall metrics,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision and recall](img/image00319.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Precision** is calculated by true-positive divided by the sum of true-positive
    and false-positive. Precision indicates what per cent of the total recommendations
    are useful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** is calculated by true-positive divided by the sum of true positive
    and false negative. Recall indicates of the total recommendations what percentage
    of recommendations is useful.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall are both required while evaluating a recommendation model.
    Sometimes we would be interested in generating good recommendations with high
    precision, and other times we would be interested in generating recommendations
    with high recall. But the problem with these two is that if we focus on improving
    one metric, the other metric suffers. We need to choose an optimal trade-off between
    precision and recall based on our requirements. The implementations for precision-recall
    is covered in [Chapter 5](part0037.xhtml#aid-1394Q1 "Chapter 5. Building Collaborative
    Filtering Recommendation Engines"), *Building Collaborative Filtering Recommendation
    Engines*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw various data-mining steps that are popularly used in
    building recommendation engines. We started by learning similarity calculations,
    such as Euclidean distance measures, followed by mathematical models, such as
    matrix factorization techniques. Then we covered supervised and unsupervised machine
    learning techniques, such as regression, classification, clustering techniques,
    and dimensionality reduction techniques. In the last sections of the chapter,
    we covered how information retrieval methods from natural language processing,
    such as vector space models, can be used in recommendation engines. We concluded
    the chapter by covering popular evaluating metrics. Till now we have covered theoretical
    background required for building recommendation engines. In the next chapter,
    we will learn building collaborative filtering recommendation engines in R and
    Python.
  prefs: []
  type: TYPE_NORMAL
