- en: Chapter 4. Data Mining Techniques Used in Recommendation Engines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：推荐引擎中使用的数据挖掘技术
- en: Data mining techniques lie at the heart of recommendation engines. These data
    mining techniques help us extract patterns, group users, calculate similarities,
    predict preferences, handle sparse input data, evaluate recommendation models,
    and so on. In the previous chapter, we have learned about recommendation engines
    in detail. Though we did not get into the implementation of recommendation engines,
    we learned the theory behind the different types of recommendations engines, such
    as neighborhood-based, personalized, contextual recommenders, hybrids, and so
    on. In this chapter, we shall look into the popular data mining techniques currently
    used in building recommendation engines. The reason for dedicating a separate
    chapter to this is that we will come across many techniques while implementing
    recommendation engines in the subsequent chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘技术位于推荐引擎的核心。这些数据挖掘技术帮助我们提取模式、分组用户、计算相似度、预测偏好、处理稀疏输入数据、评估推荐模型等。在前一章中，我们详细学习了推荐引擎。尽管我们没有深入研究推荐引擎的实现，但我们学习了不同类型推荐引擎背后的理论，例如基于邻域、个性化、上下文推荐者、混合推荐者等。在本章中，我们将探讨目前用于构建推荐引擎的流行数据挖掘技术。我们之所以将这一内容单独成章，是因为在后续章节实现推荐引擎时，我们将遇到许多技术。
- en: 'This chapter is broadly divided into the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章大致分为以下几部分：
- en: Neighbourhood-based techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于邻域的技术
- en: Euclidean distance
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Cosine similarity
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Jaccard similarity
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard 相似度
- en: Pearson correlation coefficient
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: Mathematical modelling techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学建模技术
- en: Matrix factorization
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: Alternating Least Squares
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交替最小二乘法
- en: Singular value decomposition
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Machine learning techniques
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习技术
- en: Linear regression
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Classification models
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型
- en: Clustering techniques
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类技术
- en: K-means clustering
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: Dimensionality reduction
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低
- en: Principal component analysis
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Vector space models
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量空间模型
- en: Term frequency
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语频率
- en: Term frequency-inverse document frequency
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语频率-逆文档频率
- en: Evaluation techniques
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估技术
- en: Root-mean-square error
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方根误差
- en: Mean absolute error
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均绝对误差
- en: Precision and recall
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确率和召回率
- en: Each section is explained with the basic technique and its implementation in
    R.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分都通过基本技术和其在 R 中的实现进行了说明。
- en: Let's start refreshing the basics that are mostly commonly used in recommendation
    engines.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从推荐引擎中最常用的基础知识开始复习。
- en: Neighbourhood-based techniques
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于邻域的技术
- en: 'As introduced in the previous chapters, the neighbourhood methods are very
    simple techniques, which are used right from the beginning of building recommendation
    engines. These are the oldest yet most widely used approaches, even today. The
    popularity of these widely used approaches is because of their accuracy in generating
    recommendations. We know that almost every recommender system works on the concept
    of similarity between items or users. These neighbourhood methods consider the
    available information between two users or items as two vectors, and simple mathematic
    calculation is applied between these two vectors to see how close they are. In
    this section, we will discuss the following neighbourhood techniques:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，邻域方法是简单的技术，从构建推荐引擎的开始就被使用。这些是最古老但也是最广泛使用的方法，即使在今天也是如此。这些广泛使用的方法之所以受欢迎，是因为它们在生成推荐方面的准确性。我们知道几乎每个推荐系统都是基于物品或用户之间相似性的概念。这些邻域方法将两个用户或物品之间的可用信息视为两个向量，并在这些向量之间应用简单的数学计算来查看它们有多接近。在本节中，我们将讨论以下邻域技术：
- en: Euclidean distance
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Cosine similarity
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Jaccard Similarity
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard 相似度
- en: Pearson correlation coefficient
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: Euclidean distance
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Euclidean distance similarity is one of the most common similarity measures
    used to calculate the distance between two points or two vectors. It is the path
    distance between two points or vectors in vector space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离相似度是用于计算两点或两个向量之间距离的最常见相似度度量之一。它是向量空间中两点或向量之间的路径距离。
- en: 'In the following diagram, we see the path distance between the two vectors,
    a and b, as Euclidean distance:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们看到向量 a 和 b 之间的路径距离是欧几里得距离：
- en: '![Euclidean distance](img/image00256.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![欧几里得距离](img/image00256.jpeg)'
- en: Euclidean distance is based on the Pythagoras's theorem to calculate the distance
    between two points.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是基于毕达哥拉斯定理来计算两点之间的距离。
- en: 'The **Euclidean distance** between two points or objects (point *x* and point
    *y*) in a dataset is defined by the following equation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，两点或物体（点 *x* 和点 *y*）之间的**欧几里得距离**由以下方程定义：
- en: '![Euclidean distance](img/image00257.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![欧几里得距离](img/image00257.jpeg)'
- en: Here, *x* and *y* are two consecutive data points, and n is the number of attributes
    for the dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 和 *y* 是两个连续的数据点，n 是数据集的属性数量。
- en: How is Euclidean distance applied in recommendation engines?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离在推荐引擎中是如何应用的？
- en: Consider a rating matrix containing user IDs as rows, item IDs as columns, and
    the preference values as cell values. The Euclidean distance between two rows
    gives us the user similarity, and the Euclidean distance between two columns gives
    us the item similarity. This measure is used when the data is comprised of continuous
    values.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含用户 ID 作为行，项目 ID 作为列，偏好值作为单元格值的评分矩阵。两个行之间的欧几里得距离给出了用户相似度，两个列之间的欧几里得距离给出了项目相似度。当数据由连续值组成时，使用此度量。
- en: 'The R script for calculating the Euclidean distance is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算欧几里得距离的 R 脚本如下：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Euclidean distance](img/image00258.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![欧几里得距离](img/image00258.jpeg)'
- en: Cosine similarity
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: '**Cosine similarity** is a measure of similarity between two vectors of an
    inner product space that measures the cosine of the angle between them; it''s
    given by the following equation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**余弦相似度**是内积空间中两个向量之间相似度的度量，它衡量的是它们之间角度的余弦值；它由以下方程给出：'
- en: '![Cosine similarity](img/image00259.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00259.jpeg)'
- en: 'Let *a* be a vector (*a1*, *a2*, *a3*, *a4*) and *b* be another vector (*b1*,
    *b2*, *b3*, *b4*). The dot product between these vectors *a* and *b* is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *a* 为一个向量 (*a1*, *a2*, *a3*, *a4*)，*b* 为另一个向量 (*b1*, *b2*, *b3*, *b4*)。这两个向量
    *a* 和 *b* 的点积如下：
- en: '*a.b = a1b1 + a2b2 + a3b3 + a4b4*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*a.b = a1b1 + a2b2 + a3b3 + a4b4*'
- en: 'The resultant will be a single value, a scalar constant. What does it mean
    to take the dot product between two vectors? To answer this question, let''s define
    the geometric definition of dot product between two vectors:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个单一值，一个标量常数。两个向量之间的点积意味着什么？为了回答这个问题，让我们定义两个向量之间点积的几何定义：
- en: '![Cosine similarity](img/image00260.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00260.jpeg)'
- en: 'On rearranging the preceding equation, we get the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列前面的方程，我们得到以下方程：
- en: '![Cosine similarity](img/image00261.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00261.jpeg)'
- en: In the earlier equation, *cosθ* is the angle between two vectors, and *acosθ*
    is the projection of vector A onto vector B.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*cosθ* 是两个向量之间的角度，*acosθ* 是向量 A 在向量 B 上的投影。
- en: 'The visual vector space representation of the dot product between two vectors
    can be shown as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量点积的视觉向量空间表示如下：
- en: '![Cosine similarity](img/image00262.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00262.jpeg)'
- en: 'When cos angle between the two vectors is 90 degrees, the *cos 90* will become
    zero, and the whole dot product will be zero, that is, they will be orthogonal
    to each other. The logical conclusion we can infer is that they are very far from
    each other:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个向量之间的余弦角度为 90 度时，*cos 90* 将变为零，整个点积也将为零，即它们将相互垂直。我们可以推断出的逻辑结论是它们彼此非常遥远：
- en: '![Cosine similarity](img/image00263.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00263.jpeg)'
- en: When we reduce the cosine angle between the two vectors, their orientation will
    look very similar to each other's.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们减小两个向量之间的余弦角度时，它们的方向看起来将非常相似。
- en: 'When the angle between the two vectors is zero, *cos 0* will be 1, and both
    the vectors will lie on each other, as shown in the following image. Thus, we
    can say that the two vectors are similar to each other in terms of their orientation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个向量之间的角度为零时，*cos 0* 将为 1，两个向量将重合，如下面的图像所示。因此，我们可以说这两个向量在方向上相似：
- en: '![Cosine similarity](img/image00264.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00264.jpeg)'
- en: 'So on summarizing, we can conclude that when we compute the cosine angle between
    two vectors, the resultant scalar value will indicate how close the two vectors
    are with each other in terms of orientation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以总结一下，我们可以得出结论，当我们计算两个向量之间的余弦角度时，得到的标量值将指示两个向量在方向上的接近程度：
- en: '![Cosine similarity](img/image00265.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度](img/image00265.jpeg)'
- en: 'Now let''s revisit our original question: what does the dot product mean? When
    we take the dot product between two vectors, the resultant scalar value represents
    the cosine angle between them. If the scalar is zero, the two vectors are orthogonal
    and unrelated. If the scalar is 1, the two vectors are similar.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新审视我们的原始问题：点积意味着什么？当我们对两个向量取点积时，得到的标量值代表它们之间的余弦角。如果标量为零，则两个向量是正交的且无关。如果标量为1，则两个向量是相似的。
- en: Now, how is this applied in recommendation engines?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是如何在推荐引擎中应用的？
- en: As mentioned earlier, consider a rating matrix containing user IDs as rows and
    item IDs as columns. We can assume each row as user vectors and each column as
    item vectors.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，考虑一个包含用户ID作为行和物品ID作为列的评分矩阵。我们可以假设每一行是用户向量，每一列是物品向量。
- en: The cosine angle between row vectors will give the user similarity, and cosine
    angle between column vectors give the item similarity.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 行向量之间的余弦角将给出用户相似度，列向量之间的余弦角给出物品相似度。
- en: 'The R script for calculating the cosine distance is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 计算余弦距离的R脚本如下：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, *x* is matrix containing all the variables in a dataset; the cosine function
    is available in the `lsa` package. The `lsa` is a text mining package available
    in r used for discovering latent features or topics within the text. This package
    provides `cosine()` method to calculate cosine angle between two vectors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*是包含数据集中所有变量的矩阵；余弦函数在`lsa`包中可用。`lsa`是R中用于在文本中查找潜在特征或主题的文本挖掘包。此包提供`cosine()`方法来计算两个向量之间的余弦角。
- en: Jaccard similarity
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jaccard相似度
- en: Jaccard simlarity is another type of similarity measure used in recommendation
    engines. The **Jaccard similarity** coefficient is calculated as the ratio of
    the intersection of features to the union of features between two users or items.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard相似度是推荐引擎中使用的另一种相似度度量。**Jaccard相似度**系数是两个用户或物品之间特征交集与特征并集的比值。
- en: 'Mathematically speaking, if *A* and *B* are two vectors, the Jaccard similarity
    is given by the following equation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果*A*和*B*是两个向量，Jaccard相似度由以下方程给出：
- en: '![Jaccard similarity](img/image00266.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![Jaccard相似度](img/image00266.jpeg)'
- en: The Jaccard similarity coefficient metric is a statistic used to find the similarity
    and diversity in sample sets. Since users and items can be represented as vectors
    or sets, we can easily apply the Jaccard coefficient to recommender systems in
    order to find similarity between users or items.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard相似系数指标是一种用于在样本集中寻找相似性和多样性的统计量。由于用户和物品可以被表示为向量或集合，我们可以轻松地将Jaccard系数应用于推荐系统，以找到用户或物品之间的相似性。
- en: 'The R script for calculating the Jaccard similarity is as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Jaccard相似度的R脚本如下：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `clusteval` package in r is a popular package for evaluating clustering
    techniques. `Cluster_similarity()` method provides a very good implementation
    for calculating Jaccard similarity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: R中的`clusteval`包是一个用于评估聚类技术的流行包。`Cluster_similarity()`方法提供了计算Jaccard相似度的良好实现。
- en: Pearson correlation coefficient
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: Another way of finding the aforementioned similarity is to find the correlation
    between two vectors. Instead of using the distance measures as a way of finding
    similarity among vectors, we use the correlation between vectors in this approach.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 找到上述相似性的另一种方法是找到两个向量之间的相关性。在这种方法中，我们不是使用距离度量作为在向量中寻找相似性的方式，而是使用向量之间的相关性。
- en: 'The Pearson correlation coefficient can be computed as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数可以按以下方式计算：
- en: '![Pearson correlation coefficient](img/image00267.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![皮尔逊相关系数](img/image00267.jpeg)'
- en: Here, *r* is the correlation coefficient, *n* is the total number of data points,
    *x[i]* is the *i^(th)* vector point of the x vector, *y[i]* is the *i^(th)* vector
    point of the y vector*, x-bar* is the mean of vector x*, y-bar* is the mean of
    vector y*, s[x]* is the standard deviation of vector x*, and s[y]* is the standard
    deviation of vector y.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*r*是相关系数，*n*是数据点的总数，*x[i]*是x向量的第*i*个向量点，*y[i]*是y向量的第*i*个向量点，*x-bar*是向量x的均值，*y-bar*是向量y的均值，*s[x]*是向量x的标准差，*s[y]*是向量y的标准差。
- en: Another way of computing the correlation coefficient between two variables is
    by dividing the covariance of the two variables by the product of their standard
    deviations,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 计算两个变量之间相关系数的另一种方法是，通过将两个变量的协方差除以它们标准差的乘积，
- en: 'given by ![Pearson correlation coefficient](img/image00268.jpeg)(rho):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由 ![皮尔逊相关系数](img/image00268.jpeg)(rho) 给出：
- en: '![Pearson correlation coefficient](img/image00269.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![皮尔逊相关系数](img/image00269.jpeg)'
- en: 'Let''s understand this with an example, as shown in the following image. We
    plot the values of two vectors a, b; it is natural to assume that if all the points
    of the vectors vary together, there exists a positive relation among them. This
    tendency to vary together, or covariance, in simple terms, can be called correlation.
    Take a look at the following diagram:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下图像中的示例来理解这一点。我们绘制了两个向量 a, b 的值；自然地假设如果向量的所有点都一起变化，它们之间存在正相关关系。这种一起变化的趋势，或者说协方差，在简单术语中可以称为相关性。看看以下图表：
- en: '![Pearson correlation coefficient](img/image00270.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![皮尔逊相关系数](img/image00270.jpeg)'
- en: 'Let''s now examine the following image. We can observe that the vectors are
    not varying together, and the corresponding points are scattered randomly. So
    the tendency to vary together, or covariance, is less or in other less correlation:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查以下图像。我们可以观察到向量不是一起变化的，相应的点随机分布。因此，一起变化的趋势，或者说协方差，较小，或者说是较低的相关性：
- en: '![Pearson correlation coefficient](img/image00271.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![皮尔逊相关系数](img/image00271.jpeg)'
- en: From the similarity calculation aspect, we can conclude that the more the correlation
    between two vectors, the more similar they are.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从相似度计算的角度来看，我们可以得出结论：两个向量之间的相关性越大，它们就越相似。
- en: Now, how is the Pearson correlation coefficient applied in recommendation engines?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，皮尔逊相关系数在推荐引擎中是如何应用的？
- en: As mentioned earlier, consider a rating matrix containing user IDs as rows and
    item IDs as columns. We can assume each row as user vectors and each column as
    item vectors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，考虑一个包含用户ID作为行和项目ID作为列的评分矩阵。我们可以假设每一行是用户向量，每一列是项目向量。
- en: The correlation coefficient between the row vectors will give the user similarity,
    and the correlation coefficient between the column vectors will give the item
    similarity using the following equation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 行向量之间的相关系数将给出用户相似度，列向量之间的相关系数将给出项目相似度，使用以下方程：
- en: 'The R script is given by the following equation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: R 脚本由以下方程给出：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, `mtcars` is the dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`mtcars` 是数据集。
- en: Mathematic model techniques
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学模型技术
- en: Mathematical models such as matrix factorization and SVD have proved to be very
    accurate when it comes to building recommendation engines over the similarity
    calculation measures. Another advantage is their ability to scale down easily
    also allowed to design the systems easily. In this chapter, we will learn about
    the mathematical models as explained next.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如矩阵分解和奇异值分解等数学模型在构建基于相似度计算的推荐引擎时已被证明非常准确。另一个优点是它们可以轻松地缩小规模，也允许轻松设计系统。在本章中，我们将学习如后所述的数学模型。
- en: Matrix factorization
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: A matrix can be decomposed into two low rank matrices, which when multiplied
    back will result in a single matrix approximately equal to the original matrix.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩阵可以被分解成两个低秩矩阵，当它们相乘时，将得到一个与原始矩阵近似相等的单个矩阵。
- en: Let's say that *R*, a rating matrix of size *U X M* can be decomposed into two
    low rank matrices, *P* and *Q*, of size *U X K* and *M X K* respectively, where
    *K* is called the rank of the matrix.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *R*，一个大小为 *U X M* 的评分矩阵可以被分解成两个低秩矩阵，*P* 和 *Q*，分别大小为 *U X K* 和 *M X K*，其中
    *K* 被称为矩阵的秩。
- en: 'In the following example, the original matrix of size *4 X 4* is decomposed
    into two matrices, *P (4 X 2)* and *Q (4 X 2)*; multiplying back *P* and *Q* will
    bring me the original matrix of size *4 X 4* and values approximately equal to
    those of the original matrix:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，原始矩阵的大小为 *4 X 4* 被分解成两个矩阵，*P (4 X 2)* 和 *Q (4 X 2)*；将 *P* 和 *Q* 相乘将带给我原始矩阵的大小为
    *4 X 4* 和与原始矩阵近似相等的值：
- en: '![Matrix factorization](img/image00272.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵分解](img/image00272.jpeg)'
- en: 'One of the major advantages of the matrix factorization method is that we can
    compute the empty cells in the original matrix, *R*, using the dot product between
    the low-rank matrices *P*, *Q*. This is given by the following equation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解方法的一个主要优点是我们可以使用低秩矩阵 *P* 和 *Q* 之间的点积来计算原始矩阵 *R* 中的空单元格。这由以下方程给出：
- en: '![Matrix factorization](img/image00273.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵分解](img/image00273.jpeg)'
- en: When we apply the previous equation, we can reproduce the original matrix, *R*,
    with all the empty cells filled.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用前面的方程时，我们可以重新生成原始矩阵 *R*，所有空单元格都被填充。
- en: 'In order to make the predicted values as close as to the original matrix as
    possible, we have to minimize the difference between the original values and the
    predicted values, which is also also known as error. The error between the original
    value and predicted value can be given by the following equation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使预测值尽可能接近原始矩阵，我们必须最小化原始值和预测值之间的差异，这也就是误差。原始值和预测值之间的误差可以用以下方程表示：
- en: '![Matrix factorization](img/image00274.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵分解](img/image00274.jpeg)'
- en: In order to minimize the aforementioned error term and reproduce the original
    matrix as closely as possible, we have to use gradient descend technique-an algorithm
    to find out optimal parameters of an objective function and minimize the function
    in an iterative manner, introduce Regularization term to the equation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化上述误差项并尽可能准确地重现原始矩阵，我们必须使用梯度下降技术——一种寻找目标函数最优参数并迭代最小化函数的算法，并在方程中引入正则化项。
- en: How is matrix factorization applied to recommendation engines?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解是如何应用于推荐引擎的？
- en: This is core question, which we are more interested in rather than the mathematics
    involved in matrix factorization. We will see how we can apply matrix factorization
    techniques in building recommendation engines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个核心问题，我们对此比矩阵分解中的数学更感兴趣。我们将看到如何将矩阵分解技术应用于构建推荐引擎。
- en: 'Recall the core tasks in building recommendation engines: finding similar users
    or items, then predicting the non-rated preferences, and finally recommending
    new items to active users. In short, we are predicting the non-rated item preferences.
    Recall this is what matrix factorization does: predicting the empty cells in the
    original rating matrix.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾构建推荐引擎的核心任务：找到相似的用户或项目，然后预测未评分的偏好，最后向活跃用户推荐新项目。简而言之，我们正在预测未评分的项目偏好。回顾一下，这正是矩阵分解所做的：预测原始评分矩阵中的空单元格。
- en: Now, how do we justify our approach of applying matrix decomposition to low-rank
    matrices to recommendation engines? To answer this question, we will discuss how
    users rate movies. People rate movies because of the story or actors or the genre
    of the movie, that is, users rate items because of the features of the items.
    When given a rating matrix containing user IDs, item IDs, and rating values, we
    can make an assumption that users will have some inherent preferences toward rating
    items, and the items will also have inherent features that help users rate them.
    These features of users and items are called **latent features**.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何证明将矩阵分解应用于低秩矩阵在推荐引擎中的方法？为了回答这个问题，我们将讨论用户如何评分。人们评分电影是因为故事、演员或电影的类型，也就是说，用户评分项目是因为项目的特征。当给定一个包含用户ID、项目ID和评分值的评分矩阵时，我们可以假设用户在评分项目时会有一些固有的偏好，项目也会有一些固有的特征，这些特征有助于用户评分。这些用户和项目的特征被称为**潜在特征**。
- en: 'Considering the earlier assumption, we apply the matrix factorization technique
    to the rating matrix the two low-rank matrices, which are assumed to be the user
    latent feature matrix and item latent feature matrix:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到之前的假设，我们将矩阵分解技术应用于评分矩阵，得到两个低秩矩阵，这些矩阵被假定为用户潜在特征矩阵和项目潜在特征矩阵：
- en: '![Matrix factorization](img/image00275.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵分解](img/image00275.jpeg)'
- en: 'Considering these assumptions, researchers started applying matrix factorization
    techniques in building recommender systems. The advantage of the matrix factorization
    methods is that since it is a machine-learning model, the feature weightages are
    learned overtime, improving the model accuracy:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些假设，研究人员开始将矩阵分解技术应用于构建推荐系统。矩阵分解方法的优势在于，由于它是一个机器学习模型，特征权重会随着时间的推移而学习，从而提高模型精度：
- en: 'The following code explains the implementation of Matrix Factorization using
    `nmf` package in R:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码解释了使用R中的`nmf`包实现矩阵分解的实现：
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alternating least squares
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交替最小二乘法
- en: 'Recall the error minimization equation in the previous section. Upon introducing
    a regularization term to avoid over fitting, the final error term would look like
    the following equation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾上一节中的误差最小化方程。在引入正则化项以避免过拟合后，最终的误差项将类似于以下方程：
- en: '![Alternating least squares](img/image00276.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![交替最小二乘法](img/image00276.jpeg)'
- en: 'In order to optimize the preceding equation, there are two popular techniques:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化前面的方程，有两种流行的技术：
- en: '**Stochastic gradient descent** (**SGD**): A mini-batch optimizing technique,
    similar to gradient descend, for finding optimal parameters in large-scale data
    or sparse data.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）：一种小批量优化技术，类似于梯度下降，用于在大规模数据或稀疏数据中寻找最优参数。'
- en: '**Alternating least squares**(**ALS**): The main advantage of ALS method over
    SGD is that it can be easily parallelized on distributed platforms.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘**（**ALS**）：与 SGD 相比，ALS 方法的主要优势是它可以在分布式平台上轻松并行化。'
- en: In this section, we will look into the ALS method.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 ALS 方法。
- en: The preceding equation involves two unknowns, which we need to solve. Since
    two unknowns are involved, the aforementioned equation is a non-convex problem.
    If we fix one of the unknown term constants, this optimization problem will become
    quadratic and can be solved optimally.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程涉及两个未知数，我们需要求解。由于涉及两个未知数，上述方程是一个非凸问题。如果我们固定其中一个未知项的常数，这个优化问题将变为二次的，并且可以最优地解决。
- en: Alternating least squares is an iterative method, which involves computing one
    feature vector term using the least squares function by fixing the other feature
    vector term constant until we solve the preceding equation optimally.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 交替最小二乘是一种迭代方法，它涉及通过固定另一个特征向量项为常数，使用最小二乘函数计算一个特征向量项，直到我们最优地解决前面的方程。
- en: In order to compute the user feature vector, we fix the item feature vector
    as a constant and solve for least squares. Similarly, while computing the item
    feature vector, we fix the user feature vector as a constant and solve for least
    squares.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算用户特征向量，我们将物品特征向量固定为一个常数并求解最小二乘。同样，在计算物品特征向量时，我们将用户特征向量固定为一个常数并求解最小二乘。
- en: Following this approach, we are able to convert a non-convex problem into a
    quadratic one, which can be solved optimally.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们可以将非凸问题转化为二次问题，从而可以最优地解决。
- en: Most of the open source distributed platforms such as Mahout and Spark use the
    ALS method to implement scalable recommender systems for their ability to be parallelized.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数开源分布式平台，如 Mahout 和 Spark，都使用 ALS 方法来实现可扩展的推荐系统，因为它们可以并行化。
- en: Singular value decomposition
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: '**Singular value decomposition** (**SVD**) is another very popular matrix factorization
    method. In simple terms, an SVD method decomposes a real matrix A of size m x
    n into three matrices, U,![Singular value decomposition](img/image00277.jpeg),V,
    which satisfy the following equation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）是另一种非常流行的矩阵分解方法。简单来说，SVD 方法将一个大小为 m x n 的实矩阵 A 分解为三个矩阵 U，![奇异值分解](img/image00277.jpeg)，V，它们满足以下方程：'
- en: '![Singular value decomposition](img/image00278.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/image00278.jpeg)'
- en: In the previous equation, *r* is called the rank of the matrix, *A*. *U*, *V*
    are orthogonal matrices, and ![Singular value decomposition](img/image00277.jpeg) is
    a diagonal matrix having all singular values of the matrix *A*. The values of
    the *U* and *V* are real if *A* is a real matrix. The values of matrix ![Singular
    value decomposition](img/image00277.jpeg) are positive and real and are available
    in a decreasing order.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*r* 被称为矩阵 *A* 的秩，*U*，*V* 是正交矩阵，而![奇异值分解](img/image00277.jpeg)是一个对角矩阵，包含矩阵
    *A* 的所有奇异值。如果 *A* 是实矩阵，则 *U* 和 *V* 的值是实数。矩阵![奇异值分解](img/image00277.jpeg)的值是正实数，并且按递减顺序排列。
- en: 'SVD can also be used as a dimensionality reduction technique, following two
    steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 也可以用作降维技术，遵循以下两个步骤：
- en: Choose a rank *k* that is less than *r.*
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个小于 *r* 的秩 *k*。
- en: Recompute or subsize the *U*, ![Singular value decomposition](img/image00277.jpeg) ,*V*
    matrices to *(m x k)*, *(k x k)*, *(k x n)*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新计算或缩小 *U*，![奇异值分解](img/image00277.jpeg)，*V* 矩阵到 *(m x k)*，*(k x k)*，*(k x
    n)*。
- en: '![Singular value decomposition](img/image00279.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/image00279.jpeg)'
- en: The matrices obtained by applying SVD are very much applicable to recommender
    systems as they provide the best low-rank approximations of the original matrix.
    How do we apply the SVD approach to recommendation? Let's take a rating matrix
    R of size m x n containing many empty cells. Similar to matrix factorization,
    our objective is to compute an approximate rating matrix as close as possible
    o the original matrix with the missing values being predicted.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 SVD 得到的矩阵非常适合推荐系统，因为它们提供了原始矩阵的最佳低秩近似。我们如何将 SVD 方法应用于推荐？让我们以一个大小为 m x n 的评分矩阵
    R 为例，其中包含许多空单元格。类似于矩阵分解，我们的目标是计算一个尽可能接近原始矩阵的近似评分矩阵，其中缺失的值被预测。
- en: Applying SVD on R will produce three matrices, *U*, ![Singular value decomposition](img/image00277.jpeg),
    *V*, of sizes, let's say, *m x r*, *r x r*, *r x n*. Here, *U* represents the
    user latent feature vector representations, *V* represents the item latent feature
    vector representations, and ![Singular value decomposition](img/image00277.jpeg) represents
    the independent feature representation, r, of user and items. By setting the value
    of the independent feature representation to a value *k* less than *r*, we are
    choosing the k optimal latent features, thereby reducing the size of the matrix.
    The k-value can be chosen using the cross-validation approach as the value of
    *k* defines the performance of the model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在R上应用SVD将产生三个矩阵，*U*、![奇异值分解](img/image00277.jpeg)、*V*，大小分别为，比如说，*m x r*、*r x
    r*、*r x n*。在这里，*U*代表用户潜在特征向量表示，*V*代表项目潜在特征向量表示，![奇异值分解](img/image00277.jpeg)代表用户和项目的独立特征表示，r。通过将独立特征表示的值设置为小于*r*的*k*，我们选择了k个最优的潜在特征，从而减少了矩阵的大小。k值可以通过交叉验证方法选择，因为*k*的值定义了模型的性能。
- en: Note
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A simpler method for choosing the value *k* as ![Singular value decomposition](img/image00277.jpeg) is
    to take a diagonal matrix that contains singular values in a descending order,
    choose the values in the diagonal that have higher values, and eliminate very
    less diagonal values.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 选择值*k*的一个更简单的方法是![奇异值分解](img/image00277.jpeg)，即取一个包含奇异值的对角矩阵，选择对角线上具有更高值的值，并消除非常小的对角线值。
- en: After choosing the k-value, we now resize or choose the first k-column in each
    of the matrices *U*,![Singular value decomposition](img/image00277.jpeg),*V*.
    This step will render matrices *U*,![Singular value decomposition](img/image00277.jpeg),*V*
    of size *m x k*, *k x k*, and *k x n* respectively, please refer to the below
    image. After we resize the matrices, we move ahead to the final step.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择k值之后，我们现在调整或选择矩阵*U*、![奇异值分解](img/image00277.jpeg)、*V*中的第一k列的大小。这一步将使矩阵*U*、![奇异值分解](img/image00277.jpeg)、*V*分别变为*m
    x k*、*k x k*和*k x n*的大小，请参考下面的图片。调整矩阵大小后，我们继续进行最后一步。
- en: In the final step, we will compute the dot products of the following series
    of matrices in order to calculate the approximate rating matrix ![Singular value
    decomposition](img/image00280.jpeg)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们将计算以下一系列矩阵的点积，以计算近似评分矩阵 ![奇异值分解](img/image00280.jpeg)
- en: '![Singular value decomposition](img/image00281.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](img/image00281.jpeg)'
- en: Below code snippet shows SVD implementations in R, the following code creates
    a sample matrix and then SVD is applied, using `svd()` available in base package
    in r, on the sample data to create 3 matrices, dot product between three matrices
    will get back our approximate original matrix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了R中SVD的实现，下面的代码创建了一个样本矩阵，然后使用r中基础包中的`svd()`函数对样本数据进行SVD，创建3个矩阵，三个矩阵的点积将得到我们的近似原始矩阵。
- en: '[PRE5]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please refer to [Chapter 7](part0051.xhtml#aid-1GKCM2 "Chapter 7. Building Real-Time
    Recommendation Engines with Spark"), *Building Real-Time Recommendation Engines
    with Spark* for ALS implementation in Spark-python.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第7章](part0051.xhtml#aid-1GKCM2 "第7章。使用Spark构建实时推荐引擎")，*使用Spark构建实时推荐引擎*，了解Spark-python中ALS的实现。
- en: Machine learning techniques
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习技术
- en: In this section, we will learn about the most important or the most frequently
    used machine learning techniques, which are widely used in building recommendation
    engines.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习最重要的或最常用的机器学习技术，这些技术在构建推荐引擎中得到了广泛应用。
- en: Linear regression
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: '**Linear regression** may be treated as a simple, popular, and the foremost
    approach for solving prediction problems. We employ linear regression where our
    objective is to predict the future outcomes, given input features and the output
    label is a continuous variable.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**可能被视为解决预测问题的简单、流行和最基本的方法。我们使用线性回归，我们的目标是预测给定输入特征的未来结果，输出标签是一个连续变量。'
- en: 'In linear regression, given historical input and output data, the model will
    try to find out the relation between independent feature variables and the dependent
    output variable given by the following equation and diagram:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，给定历史输入和输出数据，模型将试图找出独立特征变量和由以下方程和图表给出的依赖输出变量之间的关系：
- en: '![Linear regression](img/image00282.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image00282.jpeg)'
- en: Here, *y* represents the output continuous dependent variable, *x* represents
    independent feature variables, *β0* and *β1* are the unknowns or feature weights,
    *e* represents the error.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 代表输出连续的依赖变量，*x* 代表独立特征变量，*β0* 和 *β1* 是未知数或特征权重，*e* 代表误差。
- en: Using the **ordinary least squares (OLS)** approach, we will estimate the unknowns
    in the preceding equation. We shall not go deep into the linear regression approach,
    but here we will discuss how we can use linear regression in recommendation engines.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**普通最小二乘法 (OLS**)，我们将估计前面方程中的未知数。我们不会深入探讨线性回归方法，但在这里我们将讨论如何在推荐引擎中使用线性回归。
- en: 'One of the core tasks in recommendation engines is to make predictions for
    non-rated items for users. For example, in case of item-based recommendation engines,
    the prediction for item *i* by user *u* is done by computing the sum of ratings
    given by user *u* to items similar to item *i*. Then each rating is weighted by
    its similarity value:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎中的核心任务之一是为用户预测未评分的项目。例如，在基于项目的推荐引擎中，用户 *u* 对项目 *i* 的预测是通过计算用户 *u* 对与项目 *i*
    类似的项目的评分总和来完成的。然后，每个评分都通过其相似性值进行加权：
- en: '![Linear regression](img/image00283.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image00283.jpeg)'
- en: 'Instead of using this weighted average approach to make the predictions, we
    can use the linear regression approach to calculate the preference values for
    user *u* for item *i*. While using regression approach, instead of using the original
    rating values of similar items, we use their approximate rating values based on
    the linear regression model. For example, to predict the rating for item *i* by
    user *u*, we can use the following equation:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不用这种加权平均方法来做出预测，而是可以使用线性回归方法来计算用户 *u* 对项目 *i* 的偏好值。在使用回归方法时，我们不是使用相似项目的原始评分值，而是使用基于线性回归模型的近似评分值。例如，为了预测用户
    *u* 对项目 *i* 的评分，我们可以使用以下方程：
- en: '![Linear regression](img/image00284.jpeg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image00284.jpeg)'
- en: 'Linear regression using R is given by the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 R 进行线性回归的代码如下：
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `lm()` function, available in `stats` package I R, usually used to fit linear
    regression models.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: R 中的 `stats` 包提供的 `lm()` 函数通常用于拟合线性回归模型。
- en: Classification models
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类模型
- en: Classification models fall into the category of the supervised form of machine
    learning. These models are usually employed in prediction problems, where the
    response is binary or multiclass labels. In this chapter, we will discuss many
    types of classification models, such as logistic regression, KNN classification,
    SVM, decision trees, random forests, bagging, and boosting. Classification models
    play a very crucial role in recommender systems. Though classification models
    don't play a great role in neighbourhood methods, they play a very important role
    in building personalized recommendations, contextual aware systems, and hybrid
    recommenders. Also, we can apply classification models to the feedback information
    about the recommendations, which can further be used for calculating the weightages
    for user features.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型属于监督学习形式的机器学习类别。这些模型通常用于预测问题，其中响应是二元或多类标签。在本章中，我们将讨论许多类型的分类模型，例如逻辑回归、KNN
    分类、SVM、决策树、随机森林、bagging 和 boosting。分类模型在推荐系统中起着非常关键的作用。尽管分类模型在邻域方法中不起很大作用，但它们在构建个性化推荐、上下文感知系统和混合推荐器中起着非常重要的作用。此外，我们可以将分类模型应用于关于推荐的反馈信息，这可以进一步用于计算用户特征的权重。
- en: Linear classification
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性分类
- en: 'Logistic regression is the most common among classification models. **Logistic
    regression** is also known as **linear classification** as it is very similar
    to linear regression, except that in regression, the output label is continuous,
    whereas in linear classification, the output label is class variable. In regression,
    the model is a least squares function, whereas in logistic regression, the prediction
    model is a logit function given by the following equation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是分类模型中最常见的。**逻辑回归**也被称为**线性分类**，因为它与线性回归非常相似，只是在回归中，输出标签是连续的，而在线性分类中，输出标签是类别变量。在回归中，模型是最小二乘函数，而在逻辑回归中，预测模型是以下方程给出的
    logit 函数：
- en: '![Linear classification](img/image00285.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![线性分类](img/image00285.jpeg)'
- en: In the preceding equation, *e* is the natural logarithm, *x* is the input variable,
    *β[0]* is the intercept, and *β[1]* is the weight of variable x.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*e* 是自然对数，*x* 是输入变量，*β[0]* 是截距，*β[1]* 是变量 *x* 的权重。
- en: 'We may interpret the preceding equation as the conditional probability of response
    variable against the linear combination of input variables. The logit function
    allows to take any continuous variable and gives response in the range of *(0,1)*,
    as illustrated in the following diagram:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的方程解释为响应变量相对于输入变量线性组合的条件概率。logit函数允许将任何连续变量转换为范围在*(0,1)*之间的响应，如下面的图所示：
- en: '![Linear classification](img/image00286.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![线性分类](img/image00286.jpeg)'
- en: 'The logistic regression using R is given as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用R实现的逻辑回归：
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `glm()` function in R is used to fit generalized linear models, popularly
    employed for classification problems.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: R中的`glm()`函数用于拟合广义线性模型，通常用于分类问题。
- en: KNN classification
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KNN分类
- en: 'The k nearest neighbors classification is popularly known as the KNN classification.
    This is one of the most popular classification techniques. The basic concept in
    KNN classification is that the algorithm considers k nearest items surrounding
    a particular data point and tries to classify this data point into one of the
    output labels based on its k-nearest data points. Unlike other classification
    techniques such as logistic regression, SVM, or any other classification algorithms,
    KNN classification is a non-parametric model, which doesn''t involves any parameter
    estimation. The k in KNN is the number of nearest neighbors to be considered:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: k近邻分类通常被称为KNN分类。这是最受欢迎的分类技术之一。KNN分类的基本概念是算法考虑围绕特定数据点的k个最近邻项，并尝试根据其k个最近邻数据点将该数据点分类到输出标签之一。与逻辑回归、SVM或其他任何分类算法等其他分类技术不同，KNN分类是一个非参数模型，不涉及任何参数估计。KNN中的k是考虑的最近邻的数量：
- en: '![KNN classification](img/image00287.jpeg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![KNN分类](img/image00287.jpeg)'
- en: 'Consider 10 data points. We need to classify a test data point, highlighted
    in the preceding diagram, into one of two classes, blue or orange. In this example,
    we classify the test data point using the KNN classification. Let''s say k is
    4; it means that by considering four data points surrounding the active data point,
    we need to classify it by performing the following steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑10个数据点。我们需要将一个测试数据点（如前图所示）分类到两个类别之一，蓝色或橙色。在这个例子中，我们使用KNN分类来分类测试数据点。假设k是4；这意味着通过考虑围绕活动数据点的四个数据点，我们需要通过以下步骤来分类它：
- en: As a first step, we need to calculate the distance of each point from the test
    data point.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为第一步，我们需要计算每个点与测试数据点的距离。
- en: Identify the top four closest data points to the test data point.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别与测试数据点最近的四个数据点。
- en: Using the voting mechanism, assign the majority class label count to the test
    data point.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用投票机制，将多数类标签计数分配给测试数据点。
- en: The KNN classification works well in case of highly non-linear problems. Though
    this method works well in most cases, this method being a non-parametric approach
    cannot find the feature importance or weightages.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: KNN分类在高度非线性问题中表现良好。尽管这种方法在大多数情况下都表现良好，但作为一种非参数方法，这种方法不能找到特征重要性或权重。
- en: Similar to the KNN classification, there is a regression version of KNN, which
    can be used to predict the continuous output labels.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与KNN分类类似，还有一个回归版本的KNN，可以用来预测连续输出标签。
- en: Both the KNN classification and egression methods find their wide applicability
    in collaborative filtering recommender systems.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: KNN分类和回归方法在协同过滤推荐系统中都有广泛的应用。
- en: The following code snippet shows KNN classification using R, in the below code
    snippet we are using `knn3()` available in `caret` package for fitting KNN classification
    and `sample_n()` available in `dplyr` package to select random rows from a dataframe.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了使用R的KNN分类，在下面的代码片段中，我们使用`caret`包中的`knn3()`来拟合KNN分类，以及使用`dplyr`包中的`sample_n()`来从数据框中选择随机行。
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Support vector machines
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '**Support vector machines** algorithms are a form of supervised learning algorithms
    employed for solving classification problems. SVM is generally treated as one
    of the best algorithms for dealing with classification problems. Given a set of
    training examples, where each of the data points falls into one of two categories,
    an SVM training algorithm builds a model that assigns new data points into one
    category or the other. This model is a representation of the examples as points
    in space, mapped so that the examples of the separate categories are divided by
    a margin that is as wide as possible, as shown in the following figure. New examples
    are then mapped into that same space and predicted to belong to a category based
    on which side of the gap they fall on. In this section, we will go through an
    overview and implementation of SVMs without going into mathematical details.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**算法是一种用于解决分类问题的监督学习算法。SVM通常被视为处理分类问题的最佳算法之一。给定一组训练示例，其中每个数据点属于两个类别之一，SVM训练算法构建一个模型，将新数据点分配到其中一个类别。该模型是将示例表示为空间中的点，映射得使不同类别的示例通过尽可能宽的间隔分开，如图所示。然后，将新示例映射到相同的空间，并根据它们落在间隔的哪一侧预测它们属于哪个类别。在本节中，我们将概述SVM的实现，而不涉及数学细节。'
- en: 'When SVM is applied to a p-dimensional dataset, the data is mapped to a p-1
    dimensional hyper plane, and the algorithm finds a clear boundary with sufficient
    margin between classes. Unlike other classification algorithms that also create
    a separating boundary for classifying data points, SVM tries to choose a boundary
    that has maximum margin for separating the classes, as shown in the following
    figure:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当SVM应用于p维数据集时，数据被映射到p-1维超平面，算法在类别之间找到具有足够间隔的清晰边界。与其他也创建用于分类数据点的分离边界的分类算法不同，SVM试图选择一个具有最大间隔的边界来分离类别，如图所示：
- en: '![Support vector machines](img/image00288.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/image00288.jpeg)'
- en: Consider a two-dimensional dataset having two classes, as shown in the previous
    figure. Now, when the SVM algorithm is applied, firstly it checks whether a one-dimensional
    hyper plane exists to map all the data points. If the hyper plane exists, the
    linear classifier creates a decision boundary with a margin to separate the classes.
    In the preceding figure, the thick redline is the decision boundary, and the thinner
    blue and red lines are the margins of each class from the boundary. When new test
    data is used to predict the class, the new data falls into one of the two classes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有两个类别的二维数据集，如图所示。现在，当应用SVM算法时，首先检查是否存在一个一维超平面来映射所有数据点。如果超平面存在，线性分类器创建一个具有间隔的决定边界来分离类别。在前面的图中，粗红色线是决定边界，较细的蓝色和红色线是每个类别与边界之间的间隔。当使用新的测试数据预测类别时，新数据将落入两个类别之一。
- en: 'The following are a few key points to be noted:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些需要注意的关键点：
- en: Though an infinite number of hyper planes can be created, SVM chooses only one
    hyper plane that has maximum margin, that is, the separating hyper plane that
    is farthest from the training observations.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然可以创建无限多个超平面，但支持向量机（SVM）只选择具有最大间隔的一个超平面，即离训练观察结果最远的分离超平面。
- en: This classifier is only dependent on the data points that lie on the margins
    of the hyper plane, that is, on thin margins in the figure but not on other observations
    in the dataset. These points are called support vectors.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个分类器仅依赖于位于超平面边缘的数据点，即在图中的细边缘，而不是数据集中的其他观察结果。这些点被称为支持向量。
- en: The decision boundary is affected only by the support vectors but not by other
    observations located away from the boundaries, that is, if we change the data
    points other than the support vectors, there will not be any effect on the decision
    boundary, but if the support vectors are changed, the decision boundary changes.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策边界仅受支持向量的影响，而不受远离边界的其他观察结果的影响，也就是说，如果我们改变除了支持向量之外的数据点，决策边界不会有任何影响，但如果支持向量改变，决策边界会发生变化。
- en: A large margin on the training data will also have a large margin on the test
    data so as to classify the test data correctly.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据上的大间隔也会在测试数据上产生大间隔，以便正确分类测试数据。
- en: Support vector machines also perform well with non-linear datasets. In this
    case, we use radial kernel functions.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机在非线性数据集上也表现良好。在这种情况下，我们使用径向核函数。
- en: See below for R implementation of SVM on the `iris` dataset. We use the `e1071`
    package to run SVM. In R, the `SVM()` function contains the implementation of
    Support Vector Machines present in the `e1071` package.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cross-validation method is used to evaluate the accuracy of predictive models
    before testing future unseen data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the SVM method is called with the `tune()` method, which performs
    cross validation and runs the model on different values of the cost parameters:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `tune$best.model`tells us that the model works best with cost parameter
    as `10` and the total number of support vectors as `25`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Decision trees
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Decision trees` is a simple, fast, and tree-based supervised learning algorithm
    for solving classification problems. Though not very accurate when compared to
    other logistic regression methods, this algorithm comes in handy while dealing
    with recommender systems.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the decision trees with an example. Imagine a situation where
    you have to predict the class of flower based on its features such as petal length,
    petal width, sepal length and sepal width. We apply the decision trees methodology
    to solve this problem:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: We consider the entire data at the start of the algorithm.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we choose a proper question/variable to divide the data into two parts.
    In our case, we choose to divide the data based on petal length > 2.45 and <=
    2.45\. This separates flower class `setosa` from the rest of the classes.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We further divide the data having petal length > 2.45, based on same variable
    with petal length < 4.5 and >= 4.5 as shown in the following diagram.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This splitting of the data will be further divided by narrowing down the data
    space until we reach a point where all the bottom points represent the response
    variables or where further logical split cannot be done on the data.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following decision tree diagram, we have one root node, four internal
    nodes where data split occurred, five terminal nodes where data split cannot be
    done further, and they are defined as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Petal Length < 2.5 as root node
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length < 2.5, petal length < 4.85, sepal length < 5.15, and petal width
    < 1.75 are called internal nodes
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final nodes having the class of the flowers are called terminal nodes
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lines connecting the nodes are called the branches of the tree
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While predicting responses on new data using the aforementioned built model,
    each of the new data points is taken through each of the nodes, a question is
    asked, and a logical path is taken to reach its logical class:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/image00289.jpeg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: Take a look at the decision tree implementation in R on the `iris` dataset using
    tree package available from CRAN.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary of the mode given next tells us that the misclassification rate
    is 0.0381, indicating that the model is very accurate:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Decision trees](img/image00290.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Below code shows plotting the decision tree:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following code displays the decision tree model:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/image00291.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following image displays the prediction values made using `pred()` method:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了使用`pred()`方法做出的预测值：
- en: '![Decision trees](img/image00292.jpeg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/image00292.jpeg)'
- en: Ensemble methods
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成方法
- en: In data mining, we use ensemble methods, which refer to using multiple learning
    algorithms to obtain better predictive results than applying any single learning
    algorithm on any statistical problem. This section deals with an overview of popular
    ensemble methods such as bagging, boosting, and random forests.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据挖掘中，我们使用集成方法，这指的是使用多个学习算法来获得比在任何一个统计问题中应用任何单个学习算法更好的预测结果。本节讨论了流行的集成方法概述，如Bagging、Boosting和随机森林。
- en: Random forests
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random forests refer to improvised supervised algorithm than bootstrap aggregation
    or bagging method though built on a similar approach. Unlike selecting all the
    variables in all the B samples generated using the bootstrap technique in bagging,
    we select only a few predictor variables randomly from total variables for each
    of the B samples, and then these samples are trained with the models. Predictions
    are made by averaging the result of each model. The number of predictors in each
    sample is decided using the formula ![Random forests](img/image00293.jpeg), where
    *p* is the total variable count in the original dataset.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林指的是一种改进的监督算法，虽然基于类似的方法，但不同于自助聚合或Bagging方法。与Bagging中从使用自助技术生成的所有B个样本中选择所有变量不同，我们只为每个B个样本从总变量中随机选择几个预测变量，然后使用这些样本进行训练。预测是通过平均每个模型的预测结果来进行的。每个样本中的预测变量数量是通过公式![随机森林](img/image00293.jpeg)决定的，其中*p*是原始数据集中的总变量数。
- en: Note
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This approach removes the condition of dependency of strong predictor in the
    dataset as we intentionally select fewer variables than all the variables for
    every iteration.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法消除了数据集中强预测变量的依赖性条件，因为我们故意选择比每个迭代的所有变量更少的变量。
- en: This approach also decorrelates variables resulting in less variability in the
    model, hence more reliability.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此方法还可以消除变量之间的相关性，从而在模型中减少变异性，因此更加可靠。
- en: 'Take a look at the following R implementation of random forests on the iris
    dataset using the `randomForest` package available from CRAN:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下使用CRAN上可用的`randomForest`包在鸢尾花数据集上实现的随机森林的R实现：
- en: '[PRE14]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following image will display the model details for random forest built
    above:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像将显示上述构建的随机森林模型的详细信息：
- en: '![Random forests](img/image00294.jpeg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/image00294.jpeg)'
- en: '[PRE15]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Random forests](img/image00295.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/image00295.jpeg)'
- en: Bagging
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Bagging
- en: '**Bagging** is also known as **bootstrap aggregating**. It is designed to improve
    the stability and accuracy of machine learning algorithms. It helps in avoiding
    overfitting and reduces variance. This is mostly used with decision trees.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging**也称为**自助聚合**。它旨在提高机器学习算法的稳定性和准确性。它有助于避免过拟合并减少方差。这通常与决策树一起使用。'
- en: Bagging involves randomly generating bootstrap samples, random sample with replacement,
    from the dataset and training the models individually. Predictions are then made
    by aggregating or averaging all the response variables.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging涉及从数据集中随机生成自助样本，随机替换样本，并单独训练模型。然后通过汇总或平均所有响应变量来做出预测。
- en: 'For example, consider a dataset (*Xi*, *Yi*) where *i*=1 ...n, contains n data
    points. The following are the steps to perform bagging on this dataset:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个数据集(*Xi*, *Yi*)，其中*i*=1 ...n，包含n个数据点。以下是对此数据集进行Bagging的步骤：
- en: Now randomly select B samples with replacement from the original dataset using
    the bootstrap technique.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在从原始数据集中使用自助法随机选择B个样本进行替换。
- en: Next, train the B samples with regression/classification models independently,
    and then predictions are made on the test set by averaging the responses from
    all the B models generated in case of regression or selecting the most-occurring
    class among B samples in case of classification.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，独立地对B个样本进行回归/分类模型的训练，并在测试集上通过平均所有生成的B个模型的响应来做出预测（在回归的情况下）或在分类的情况下从B个样本中选择最常出现的类别。
- en: Boosting
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Boosting
- en: 'Unlike in bagging where multiple copies of bootstrap samples are created and
    a new model is fitted for each copy of dataset and all the individual models are
    combined to create a single predictive model, in boosting, each new model is built
    using information from previously built models. Boosting can be understood as
    an iterative method involving two steps:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: A new model is built on the residuals of the previous models instead of response
    variable.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now the residuals are calculated from this model and updated to the residuals
    used in previous step.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding two steps are repeated multiple iterations allowing each new model
    to learn from its previous mistakes, thereby improving the model accuracy.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows us gradient boosting using R, `gbm()` package
    in r generally used to perform various regression tasks:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Boosting](img/image00296.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows them model summary visually, showing the relative
    importance of each feature:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Boosting](img/image00297.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: The preceding summary states the relative importance of the variables of the
    model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Boosting](img/image00298.jpeg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Pick the response with the highest probability from the resulting `pred` matrix,
    by doing `apply(pred, 1, which.max)` on the vector output from prediction.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Boosting](img/image00299.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, the output value for the `predict()` function
    is used in the `apply()` function to pick the response with highest probability
    among each row in the pred matrix, and the resultant output from the `apply()`
    function is the prediction for the response variable.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Clustering techniques
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cluster analysis** is the process of grouping objects together in a way that
    objects in one group are more similar than objects in other groups.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: For example, identifying and grouping clients with similar booking activities
    on travel portal, as shown in the following figure.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, each group is called a **cluster**, and each member
    (data point) of the cluster behaves similar to its group members:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering techniques](img/image00300.jpeg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: 'Cluster analysis is an unsupervised learning method. In supervised methods
    such as regression analysis, we have input variables and response variables; we
    fit a statistical model to the input variables to predict the response variable,
    whereas in unsupervised learning methods, we do not have any response variable
    to predict; we only have input variables. Instead of fitting a model to the input
    variables to predict the response variable, we just try to find patterns within
    the dataset. There are three popular clustering algorithms: hierarchical cluster
    analysis, k-means cluster analysis, and two-step cluster analysis. In this section,
    we will learn about k-means clustering.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K-means is an unsupervised, iterative algorithm where k is the number of clusters
    to formed from the data. Clustering is achieved in two steps, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '**The cluster assignment step**: In this step, we randomly choose two cluster
    points (red dot & green dot) and assign each data point to one of the two cluster
    points, whichever is closer to it (Take a look at the top part of the following
    figure).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The move centroid step:** In this step, we take the average of the points
    of all the examples in each group and move the centroid to the new position, that
    is, the mean position calculated (Take a look at the bottom part of the following
    image).'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding steps are repeated until all the data points are grouped into
    two groups and the mean of the data points at the end of the move centroid step
    doesn''t change:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/image00301.jpeg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: The previous figure shows how a clustering algorithm works on data to form clusters.
    Take a look at the following R implementation of k-means clustering on the iris
    dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means clustering using R is as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `Clustplot()` method available in `cluster` package is used to plot the
    clusters formed for the IRIS dataset and is shown in the following image:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/image00302.jpeg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: The previous figure shows the formation of clusters on the iris data, and the
    clusters account for 95% of the data. In the previous example, the number of clusters
    k value is selected using the elbow method.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet explains implementation of k-means clustering, which
    is shown in the next screenshot:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![K-means clustering](img/image00303.jpeg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: From the previous figure, we can observe that the direction of the cost function
    is changed at cluster number 5, hence we choose 5 as our number of clusters k.
    Since the number of optimal clusters is found at elbow of the graph, we call it
    the **elbow method**.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most commonly faced problems while building recommender systems is
    high dimensional and sparse data. Many times, we face a situation where we have
    a large set of features and less number of data points. In such situations, when
    we fit a model to the dataset, the predictive power of the model would be lower.
    This scenario is often termed as the curse of dimensionality. In general, adding
    more data points or decreasing the feature space, also known as dimensionality
    reduction, often reduces the effects of curse of dimensionality. In this section,
    we will discuss principal component analysis, a popular dimensionality reduction
    technique to reduce the effects of the curse of dimensionality.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is a classical statistical technique
    for dimensionality reduction. PCA algorithm transforms the data with high dimensional
    space to a space with fewer dimensions. The algorithm linearly transforms m-dimensional
    input space to n-dimensional (n<m) output space, with the objective of minimizing
    the amount of information/variance lost by discarding (m-n) dimensions. PCA allows
    us to discard the variables/features that have less variance.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, PCA uses orthogonal projection of highly correlated variables
    to a set of values of linearly uncorrelated variables called principal components.
    The number of principal components is less than or equal to the number of original
    variables. This linear transformation is defined in such a way that the first
    principal component has the largest possible variance, that is, it accounts for
    as much of the variability in the data as possible by considering highly correlated
    features, and each succeeding component, in turn, has the highest variance by
    using the features that are less correlated with the first principal component,
    which is orthogonal to the preceding component.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this in simple terms. Assume that we have a three-dimensional
    data space with two features more correlated with each other than with the third.
    We now want to reduce the data to a two-dimensional space using PCA.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component is created in such a way that it explains maximum
    variance using the two correlated variables along the data. In the following figure,
    the first principal component (the bigger line) is along the data explaining most
    variance. To choose the second principal component, we need to choose another
    line that has the highest variance, is uncorrelated, and is orthogonal to the
    first principal component, The implementation and technical details of PCA is
    out of scope of this book, so we will discuss how it used in R.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains the spatial representation of principal components:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/image00304.jpeg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: We illustrate PCA using the `USArrests` dataset. The `USArrests` dataset contains
    crime related statistics, such as `Assault`, `Murder`, `Rape`, `UrbanPop` per
    100,000 residents in 50 states in the U.S..
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA implementation in R is as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s use the `apply()` function to the `USArrests` dataset row-wise to calculate
    the variance to see how each variable is varying:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We observe that `Assault` has the most variance. It is important to note at
    this point that scaling the features is a very important step while applying PCA.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply PCA after scaling the feature, as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now let''s understand the components of PCA output:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Pca$rotation` contains the principal component loadings matrix, which explains
    the proportion of each variable along each principal component.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Now let's learn how to interpret the results of PCA using a biplot graph. Biplot
    is used to show the proportions of each variable along the two principal components.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code changes the directions of the biplot; if we don''t include
    the following two lines, the plot will be a mirror image of the following one:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following image shows a plot showing principal components for the dataset:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/image00305.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: In the previous figure, known as biplot, we can see the two principal components
    (PC1, PC2) of the `USArrests` dataset. The red arrows represent the loading vectors,
    which shows how the feature space varies along the principal component vectors.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plot, we observe that the first principal component vector, PC1, more
    or less places equal weight to three features: rape, assault, and murder. This
    means that these three features are more correlated with each other than with
    the UrbanPop feature. The second principal component, PC2, places more weight
    on UrbanPop than and is less correlated with the remaining three features.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Vector space models
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vector space models** are algebraic models most commonly used in text analysis
    applications for representing text documents using the words as vectors. This
    is widely used in information retrieval applications. In text analysis, let''s
    say we want to find the similarity between two sentences. How do we go about this?
    We know that to compute similarity measure metric, the data should be all numeric.
    When it comes to a sentence we have words rather than numerals. Vectors space
    models allow us to represent the words present in the sentences in numeric form
    so that we can apply any of the similarity calculation metrics, such as cosine
    similarity.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'This representation of sentences of words in numeric form can be done in two
    popular ways:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency inverse document frequency
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand the previously mentioned approaches with an example:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1**: THE CAT CHASES RAT.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2**: THE DOG CHASES CAT.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3**: THE MAN WALKS ON MAT.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given three sentences, our objective is to find the similarity between the sentences.
    It is clear that we cannot directly apply similarity metrics such as cosine directly.
    So now let's learn how to represent them in numeric format.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a general notation, each sentence in vector space model is known as a **document**.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Term frequency** simply means the frequency of the word in a document. To
    find the frequency, we need to perform the following steps:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to find all the unique keywords present in all the documents,
    represented as V:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: V = {THE, CAT, CHASES, RAT, DOG, MAN, WALKS, ON, MAT}
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to create vectors of documents, shown as follows:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D1 = {THE, CAT, CHASES, RAT}
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D2 = {THE, DOG, CHASES, CAT}
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D3 = {THE, MAN, WALKS, ON, MAT}
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we have to count the term frequency of all the terms in each
    document:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D1 = {(THE,1),(CAT,1),(CHASES,1),(RAT,1)}
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D2 = {(THE,1),(DOG,1),(CHASES,1),(CAT,1)}
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D3 = {(THE,1),(MAN,1),(WALKS,1),(ON,1),(MAT,1)}
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now we will create a term-document matrix with document IDs as rows, unique
    terms as columns, and term frequency as cell values, as follows:![Term frequency](img/image00306.jpeg)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a while and understand what is happening here: we have put 1 in places
    where the word occurs in the sentences and 0 where the word does not.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Now observe that we have represented our documents in the form of a numerical
    matrix using the term frequency in each document.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Now, in this term-matrix document, also known as TDM, we can directly apply
    similarity metrics such as cosine similarity.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Term frequency](img/image00307.jpeg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: The previous figure visually represents the similarity between the documents
    after calculating the cosine angle. From the figure, we can infer that the angle
    between D1 and D2 is less and that between D1 and D3 is more, indicating that
    D1 is more similar to D2 than D3.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency inverse document frequency
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The earlier approach is also known as the bag of words approach where we just
    have to find the frequency of each term in each document and numerically represent
    it in a TDM. But inherently, there is a flaw in this approach. This approach gives
    more weightage or importance to the terms that occur more frequently and less
    importance to the rarely occurring terms. It is important to understand that if
    a term occurs more frequently in most document collections, that term will not
    contribute as a differentiator in identifying a document. Similarly, a term which
    occurs more frequently in a document and less frequently in the whole of the document
    collection will contribute as a differentiator in identifying a particular document.
    This scaling down of weightage to more frequently occurring terms in the document
    collection and scaling up the weightage to more frequently occurring terms in
    document but less frequently in all of the document collection can be achieved
    using **term frequency inverse document frequency** (**tf-idf**).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'The *tf-idf* can be computed as a product of term frequency of document and
    inverse document frequency of the term:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '*tf-idf = tf X idf*'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *idf* is defined as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '*idf = log(D/(1+ n(d,t)))*'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Here, *D* is the total number of document collections, and *n(d,t)* is the number
    of times a term *t* occurs in all the documents.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute TDM in terms of *tf-idf* for the previous set of documents (*D1*,
    *D2*, *D3*):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Take the TDM and calculate the term frequency of each term in each of the documents.
    This is the same as what we have done in the TF section:![Term frequency inverse
    document frequency](img/image00308.jpeg)
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, we need to calculate the **document frequency** (**DF**), that
    is, how many times a term occurred in all the document collection. For example,
    let's calculate the DF for the term THE. The term is present in all the three
    documents, hence its DF will be 3\. Similarly, for the term CAT the DF is 2:![Term
    frequency inverse document frequency](img/image00309.jpeg)
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, we shall calculate the inverse of document frequency (IDF) using
    the aforementioned formula for IDF.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So for the term THE, the idf will be given by: idf(THE) = log(3/(1+3)) = -0.12494'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Term frequency inverse document frequency](img/image00310.jpeg)'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Compute tf-idf as the product of tf and idf for each of the terms in the whole
    document collection, as follows:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, for the term RAT in D1, the tf-idf will be computed as *1 X 0.4777121
    = 0.4777121*
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Term frequency inverse document frequency](img/image00311.jpeg)'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now that we have calculated the *tf-idf*, we can compare the previous TDM based
    on *tf-df* with the TDM with *tf*. The main comparisons we can draw are in the
    differences in weightages for each of the terms in TDM. More frequent words across
    the document collection have got less weightage compared to the rarely occurred
    words in the document.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Now, on top of this TDM based on *tf-idf* representation, we can directly apply
    the similarity metric measurements.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we learned about the vector space models and *tf*, *tf-idf*
    concepts, which are widely used in text analysis. Now the real question is: how
    do we apply these techniques in recommendation engines?'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Many a time, while building content-based recommendation engines, we will be
    getting the user preference data in text format or the features of the items as
    text. In such cases we might able to apply the aforementioned techniques to represent
    the text data as numerical vectors.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Also many times we need to find the feature importance or feature weightage
    to the item features while building personalized content-based recommendation
    engines. In such cases, vector space models concepts are very useful.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to calculate `tfidf` in R. In the code,
    we use `TermDocumentMatrix()` and `weightTFidf()` to calculate the term document
    matrix and `tfidf` respectively available in the `tm` package in R. The `inspect()`
    method is used to attain the results:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following screenshot just shows a very small portion of the large document
    term:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![Term frequency inverse document frequency](img/image00312.jpeg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: Evaluation techniques
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we saw various data mining techniques used in recommender
    systems. In this section, we will learn how to evaluate models built using data
    mining techniques. The ultimate goal for any data analytics model is to perform
    well on future data. This objective can be achieved only if we build a model,
    which is efficient and robust during the development stage.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'While evaluating any model, the most important things we need to consider are
    as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Whether the model is overfitting or underfitting
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How well the model fits the future data or the test data
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting, also known as bias, is a scenario in which the model doesn't even
    perform well on training data; this means that we are fitting a less robust model
    to the data; for example, letting the data be distributed non-linearly and fitting
    it with a linear model. From the following image, we see that data is non-linearly
    distributed. Assume that we have fitted a linear model (orange line). In this
    case, during the model-building stage itself, the prediction power will be low.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting is a scenario in which the model performs well on training data
    but does really bad on test data. This scenario arises when the model memorizes
    the data pattern rather than learning from the data; for example, letting the
    data be distributed non-linearly and fitting a complex model (green line). In
    this case, we observe that the model is fitted very close to the data distribution,
    taking care of every up and down. In this case, the model is most likely to fail
    on previously unseen data:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation techniques](img/image00313.jpeg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows simple, complex, and appropriately fitted models
    training data. The green fit represents overfitting, the orange line represents
    underfitting, the black and blue lines represent the appropriate model, which
    is a trade-off between the underfit and the overfit.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Any fitted model is evaluated to avoid the aforementioned scenarios using cross-validation,
    regularization, pruning, model comparisons, ROC curves, confusion matrix, and
    so on.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is very popular technique for model evaluation for almost all models. In
    this technique, we divide the original data into multiple folds/sets (say 5) of
    training dataset and test dataset. At each fold iteration the model is built using
    the training dataset and evaluated using the test dataset. This process is repeated
    for all the folds. The test errors are calculated for very iteration. The average
    test error is calculated to generalize the model accuracy at the end of all the
    iterations.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation implementation is explained in [Chapter 5](part0037.xhtml#aid-1394Q1
    "Chapter 5. Building Collaborative Filtering Recommendation Engines"), *Building
    Collaborative Filtering Recommendation Engines*.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this technique, the data variables are penalized to reduce the complexity
    of the model with the objective of minimizing the cost function. There are the
    two most popular regularization techniques: Ridge Regression and Lasso Regression.
    In both the techniques, we try to reduce the variable coefficients to zero so
    less number of variables will fit the data optimally.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'The popular evaluation metrics for recommendation engines are as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Root-mean-square error (RMSE)
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean absolute error (MAE)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision and recall
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root-mean-square error (RMSE)
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Root-mean-square error** is one of the most popular, frequently used, simple
    measures to find the accuracy of a model. In a general sense, it is the difference
    between the actual and predicted values. By definition, it is the squared root
    of mean square error, as given by the following equation:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![Root-mean-square error (RMSE)](img/image00314.jpeg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: Here, *X[act]* refers to the observed values, and *X[pred]* refers to the predicted
    values.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: How is RMSE applicable to recommendation engines?
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: One of the core tasks in recommendation engines is to predict the preference
    values for non-rated items for particular users. We use many approaches discussed
    in previous sections to predict these non-rated preference values. Consider the
    following rating matrix used for building a recommendation model. Assume that
    our recommendation engine model has predicted all the empty cells in the following
    figure let they be represented as r hat. Also assume that we know the actual values
    of these predicted empty cells; let they be represented as r.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '![Root-mean-square error (RMSE)](img/image00315.jpeg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: Now use the values of *r hat* and *r* in the preceding equation to calculate
    the model accuracy of the predictive power of the recommendation engine.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error (MAE)
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another popular evaluation technique for the data-mining model is **mean absolute
    error** (**MAE**). This evaluation metric is very similar to RMSE and is given
    by the following equation:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean absolute error (MAE)](img/image00316.jpeg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: This is a very simple measure computed as mean error between predicted and actual
    values. MAE is applied in recommendation engines as a way to evaluate the model.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After we have deployed a recommendation engine in production, we will be interested
    only if the suggested recommendations are accepted by the users. How do we measure
    the effectiveness of the recommendation engine in terms of whether the model is
    generating valid recommendations? To measure the effectiveness, we can borrow
    the precision-recall evaluation technique, a popular technique in evaluating a
    classification model. The preceding discussion about whether a served recommendation
    is useful or not for a user can be treated as the binary class label of a classification
    model, and then we can calculate the precision-recall.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: To understand precision-recall, we should understand a few more metrics that
    go together with precision-recall, such as true positive, true negative, false
    positive, and true negative.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: To build what is popularly known as a confusion matrix, as shown next, let's
    take an example of online news recommending site, which contains 50 web pages.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have generated 35 recommendations to user A. Of these, A has
    clicked on 25 suggested web pages, and 10 web pages are non-clicked. Now with
    this information, we create a table with the number of clicks, as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: In the top-left column, enter the count of suggested links that A has clicked
    on
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the top-right column, enter the count of suggested links that A has not clicked
    on
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the bottom-left column, enter the count of links that A has clicked on but
    have not been suggested
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the bottom-right column, enter the count of links that A has not clicked
    on and have not been suggested:![Precision and recall](img/image00317.jpeg)
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top-left count is called **true positive** (**tf**), which indicates the
    count of all the responses where the actual response is positive and the model
    predicted as positive
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top-right count is called **false positive** (**fp**), which indicates the
    count of all the responses where the actual response is negative but the model
    predicted as positive, in other words, a **FALSE ALARM**
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-left count is called **false negative** (**fn**), which indicates
    the count of all the responses where the actual response is positive but the model
    predicted as negative; in general, we call it **A MISS**
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-right count is called **true negative** (**tn**), which indicates
    the count of all the responses where the actual response is negative and the model
    predicted negative.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following table:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision and recall](img/image00318.jpeg)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: 'Using the preceding information, we shall compute precision-recall metrics,
    as follows:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision and recall](img/image00319.jpeg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: '**Precision** is calculated by true-positive divided by the sum of true-positive
    and false-positive. Precision indicates what per cent of the total recommendations
    are useful.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** is calculated by true-positive divided by the sum of true positive
    and false negative. Recall indicates of the total recommendations what percentage
    of recommendations is useful.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall are both required while evaluating a recommendation model.
    Sometimes we would be interested in generating good recommendations with high
    precision, and other times we would be interested in generating recommendations
    with high recall. But the problem with these two is that if we focus on improving
    one metric, the other metric suffers. We need to choose an optimal trade-off between
    precision and recall based on our requirements. The implementations for precision-recall
    is covered in [Chapter 5](part0037.xhtml#aid-1394Q1 "Chapter 5. Building Collaborative
    Filtering Recommendation Engines"), *Building Collaborative Filtering Recommendation
    Engines*.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw various data-mining steps that are popularly used in
    building recommendation engines. We started by learning similarity calculations,
    such as Euclidean distance measures, followed by mathematical models, such as
    matrix factorization techniques. Then we covered supervised and unsupervised machine
    learning techniques, such as regression, classification, clustering techniques,
    and dimensionality reduction techniques. In the last sections of the chapter,
    we covered how information retrieval methods from natural language processing,
    such as vector space models, can be used in recommendation engines. We concluded
    the chapter by covering popular evaluating metrics. Till now we have covered theoretical
    background required for building recommendation engines. In the next chapter,
    we will learn building collaborative filtering recommendation engines in R and
    Python.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
