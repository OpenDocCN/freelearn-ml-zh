- en: Chapter 5. Transforming Images with Morphological Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Eroding and dilating images using morphological filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opening and closing images using morphological filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying morphological operators on gray-level images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting images using watersheds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting distinctive regions using MSER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mathematical morphology** is a theory that was developed in the 1960s for
    the analysis and processing of discrete images. It defines a series of operators
    that transform an image by probing it with a predefined shape element. The way
    this shape element intersects the neighborhood of a pixel determines the result
    of the operation. This chapter presents the most important morphological operators.
    It also explores the problems of image segmentation and feature detection using
    algorithms based on morphological operators.'
  prefs: []
  type: TYPE_NORMAL
- en: Eroding and dilating images using morphological filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Erosion and dilation are the most fundamental morphological operators. Therefore,
    we will present them in this first recipe. The fundamental component in mathematical
    morphology is the **structuring element**. A structuring element can be simply
    defined as a configuration of pixels (the square shape in the following figure)
    on which an origin is defined (also called an **anchor point**). Applying a morphological
    filter consists of probing each pixel of the image using this structuring element.
    When the origin of the structuring element is aligned with a given pixel, its
    intersection with the image defines a set of pixels on which a particular morphological
    operation is applied (the nine shaded pixels in the following figure). In principle,
    the structuring element can be of any shape, but most often, a simple shape such
    as a square, circle, or diamond with the origin at the center is used. Custom
    structuring elements can be useful to emphasize or eliminate regions of particular
    shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Eroding and dilating images using morphological filters](img/image_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As morphological filters often work on binary images, we will use the binary
    image that was created through thresholding in the first recipe of the previous
    chapter. However, since the convention is to have the foreground objects represented
    by high (white) pixel values and the background objects by low (black) pixel values
    in morphology, we have negated the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In morphological terms, the following image is said to be the **complement**
    of the image that was created in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/image_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Erosion and dilation are implemented in OpenCV as simple functions, which are
    `cv::erode` and `cv::dilate`. Their usage is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The two images produced by these function calls are seen in the following images.
    The first one shows erosion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second image shows the dilation result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for all morphological filters, the two filters of this recipe operate on
    sets of pixels defined by a structuring element. Recall that when applied to a
    given pixel, the anchor point of the structuring element is aligned with this
    pixel location, and all the pixels that intersect the structuring element are
    included in the current set. **Erosion** replaces the current pixel with the minimum
    pixel value found in the defined pixel set. **Dilation** is the complementary
    operator, and it replaces the current pixel with the maximum pixel value found
    in the defined pixel set. Since the input binary image contains only black (value
    `0`) and white (value `255`) pixels, each pixel is replaced by either a white
    or black pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good way to picturize the effect of these two operators is to think in terms
    of background (black) and foreground (white) objects. With erosion, if the structuring
    element when placed at a given pixel location touches the background (that is,
    one of the pixels in the intersecting set is black), then this pixel will be sent
    to the background. In the case of dilation, if the structuring element on a background
    pixel touches a foreground object, then this pixel will be assigned a white value.
    This explains why the size of the objects has been reduced (the shape has been
    eroded) in the eroded image while it has been expanded in the dilated image. Note
    how some of the small objects (which can be considered as "noisy" background pixels)
    have also been completely eliminated in the eroded image. Similarly, the dilated
    objects are now larger, and some of the "holes" inside them have been filled.
    By default, OpenCV uses a `3x3` square structuring element. This default structuring
    element is obtained when an empty matrix (that is, `cv::Mat()`) is specified as
    the third argument in the function call, as it was done in the preceding example.
    You can also specify a structuring element of the size (and shape) you want by
    providing a matrix in which the nonzero element defines the structuring element.
    For example, to apply a `7x7` structuring element, you would proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The effect is much more destructive in this case, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way to obtain a similar result is to repetitively apply the same structuring
    element on an image. The two functions have an optional parameter to specify the
    number of repetitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `cv::Point(-1,-1)` argument means that the origin is at the center of the
    matrix (default); it can be defined anywhere on the structuring element. The image
    that is obtained will be identical to the image we obtained with the `7x7` structuring
    element. Indeed, eroding an image twice is similar to eroding an image with a
    structuring element dilated with itself. This also applies to dilation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, since the notion of background/foreground is arbitrary, we can make
    the following observation (which is a fundamental property of the erosion/dilation
    operators). Eroding the foreground objects with a structuring element can be seen
    as a dilation of the background part of the image. In other words, we can make
    the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The erosion of an image is equivalent to the complement of the dilation of the
    complement image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dilation of an image is equivalent to the complement of the erosion of the
    complement image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that even though we applied our morphological filters on binary images
    here, these filters can be applied on gray-level or even color images with the
    same definitions. The third recipe of this chapter will present few morphological
    operators and their effect on gray-level images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that the OpenCV morphological functions support in-place processing.
    This means that you can use the input image as the destination image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV will create the required temporary image for you for this to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Opening and closing images using morphological filters* recipe applies
    the erosion and dilation filters in cascade to produce new operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Applying morphological operators on gray-level images* recipe introduces
    other morphological operators that can usefully be applied to gray-level images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opening and closing images using morphological filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous recipe introduced you to the two fundamental morphological operators:
    dilation and erosion. From these, other operators can be defined. The next two
    recipes will present some of them. The opening and closing operators are presented
    in this recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to apply higher-level morphological filters, you need to use the `cv::morphologyEx`
    function with the appropriate function code. For example, the following call will
    apply the closing operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we used a `5x5` structuring element to make the effect of the filter
    more apparent. If we use the binary image of the preceding recipe as input, we
    will obtain the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, applying the morphological opening operator will result in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is obtained from the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The opening and closing filters are simply defined in terms of the basic erosion
    and dilation operations. **Closing** is defined as the erosion of the dilation
    of an image. **Opening** is defined as the dilation of the erosion of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, one can compute the closing of an image using the following calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The opening filter can be obtained by interchanging these two function calls.
  prefs: []
  type: TYPE_NORMAL
- en: While examining the result of the closing filter, it can be seen that the small
    holes of the white foreground objects have been filled. The filter also connects
    several adjacent objects together. Basically, any holes or gaps that are too small
    to completely contain the structuring element will be eliminated by the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Reciprocally, the opening filter eliminated several small objects from the scene.
    All the objects that were too small to contain the structuring element have been
    removed.
  prefs: []
  type: TYPE_NORMAL
- en: These filters are often used in object detection. The closing filter connects
    the objects erroneously fragmented into smaller pieces together, while the opening
    filter removes the small blobs introduced by the image noise. Therefore, it is
    advantageous to use them in a sequence. You can then apply the opening filter
    before the closing filter if you wish to prioritize noise filtering, but this
    could be at the price of eliminating parts of fragmented objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is the result of applying the opening filter before the
    closing filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that applying the same opening (and similarly the closing) operator on
    an image several times has no effect. Indeed, as the holes have been filled by
    the first opening filter, an additional application of the same filter will not
    produce any other changes to the image. In mathematical terms, these operators
    are said to be **idempotent**.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The opening and closing operators are often used to clean up an image before
    extracting its connected components as explained in the *Extracting connected
    components* recipe of [Chapter 7](ch07.html "Chapter 7. Extracting Lines, Contours,
    and Components") , *Extracting Lines, Contours, and Components*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying morphological operators on gray-level images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More advanced morphological operators can be composited by combining the different
    basic morphological filters introduced in this chapter. This recipe will present
    two morphological operators that, when applied to gray-level images, can lead
    to the detection of interesting image features.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One interesting morphological operator is the morphological gradient that allows
    extracting the edges of an image. This one can be accessed through the `cv::morphologyEx`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result shows the extracted contours of the image''s elements
    (the resulting image has been inverted for better viewing):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another useful morphological operator is the top-hat transform. This operator
    can be used to extract local small foreground objects in an image. The effect
    of this operator can be demonstrated by applying it on the book image of the last
    recipe of the previous chapter. This image shows an unevenly illuminated page
    of a book. A black top-hat transform will extract the characters of this page
    (considered here as the foreground objects). This operator is also called by using
    the `cv::morphologyEx` function with the appropriate flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As it can be seen in the following image, this operator successfully extracted
    most of the characters of the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good way to understand the effect of morphological operators on a gray-level
    image is to consider an image as a topological relief in which the gray levels
    correspond to elevation (or altitude). Under this perspective, the bright regions
    correspond to mountains, while the dark areas correspond to the valleys of the
    terrain. Also, since edges correspond to a rapid transition between the dark and
    bright pixels, these can be pictured as abrupt cliffs. If an erosion operator
    is applied on such a terrain, the net result will be to replace each pixel by
    the lowest value in a certain neighborhood, thus reducing its height. As a result,
    cliffs will be eroded as the valleys expand. Dilation has the exact opposite effect;
    that is, cliffs will gain terrain over the valleys. However, in both cases, the
    plateau (that is, the area of constant intensity) will remain relatively unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: These observations lead to a simple way to detect the edges (or cliffs) of an
    image. This can be done by computing the difference between the dilated and eroded
    images. Since these two transformed images differ mostly at the edge locations,
    the image edges will be emphasized by the subtraction. This is exactly what the
    `cv::morphologyEx` function does when the `cv::MORPH_GRADIENT` argument is inputted.
    Obviously, the larger the structuring element is, the thicker the detected edges
    will be. This edge detection operator is called the **Beucher ** **gradient**
    (the next chapter will discuss the concept of an image gradient in more detail).
    Note that similar results can also be obtained by simply subtracting the original
    image from the dilated one or the eroded image from the original. The resulting
    edges would simply be thinner.
  prefs: []
  type: TYPE_NORMAL
- en: The top-hat operator is also based on image difference. This time, the operator
    uses opening and closing. When a gray-level image is morphologically opened, its
    local peaks are eliminated; this is due to the erosion operator that is applied
    first. The rest of the image is preserved. Consequently, the difference between
    the original image and the opened one is the set of local peaks. These local peaks
    are the foreground objects we want to extract. In the book example of this recipe,
    the objective was to extract the characters of the page. Since the foreground
    objects are, in this case, black over a white background, we used the complementary
    operator, called the black top-hat, which consists of subtracting the original
    image from its closing. We used a `7x7` structuring element in order to have the
    closing operation big enough to remove the characters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Applying directional filters to detect edges* recipe in [Chapter 6](ch06.html
    "Chapter 6. Filtering the Images"), *Filtering the Images*, describes the other
    filters that perform edge detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article, *The Morphological gradients, J.-F. Rivest, P. Soille, and S. Beucher,
    ISET's symposium on electronic imaging science and technology, SPIE*, Feb. 1992,
    discusses the concept of morphological gradients in more detail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *Morphological operator for corner detection*, *R. Laganière*, *Pattern
    Recognition*, volume 31, issue 11, 1998, presents an operator for the detection
    of corners using morphological filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting images using watersheds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The watershed transformation is a popular image processing algorithm that is
    used to quickly segment an image into homogenous regions. It relies on the idea
    that when the image is seen as a topological relief, the homogeneous regions correspond
    to relatively flat basins delimited by steep edges. With the watershed algorithm,
    segmentation is achieved by flooding this relief by gradually increasing the level
    of water in this one. As a result of its simplicity, the original version of this
    algorithm tends to over-segment the image, which produces multiple small regions.
    This is why OpenCV proposes a variant of this algorithm that uses a set of predefined
    markers to guide the definition of the image segments.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The watershed segmentation is obtained through the use of the `cv::watershed`
    function. The input for this function is a 32-bit signed integer marker image
    in which each nonzero pixel represents a label. The idea is to mark some pixels
    of the image that are known to belong to a given region. From this initial labeling,
    the watershed algorithm will determine the regions to which the other pixels belong.
    In this recipe, we will first create the marker image as a gray-level image and
    then convert it into an image of integers. We have conveniently encapsulated this
    step into a `WatershedSegmenter` class containing a method to specify the marker
    image and a method to compute the watershed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The way these markers are obtained depends on the application. For example,
    some preprocessing steps might have resulted in the identification of some pixels
    that belong to an object of interest. The watershed would then be used to delimitate
    the complete object from that initial detection. In this recipe, we will simply
    use the binary image used throughout this chapter in order to identify the animals
    of the corresponding original image (this is the image shown at the beginning
    of [Chapter 4](ch04.html "Chapter 4. Counting the Pixels with Histograms") , *Counting
    the Pixels with Histograms*). Therefore, from our binary image, we need to identify
    pixels that belong to the foreground (the animals) and pixels that belong to the
    background (mainly the grass). Here, we will mark the foreground pixels with the
    label `255` and the background pixels with the label `128` (this choice is totally
    arbitrary; any label number other than `255` will work). The other pixels, that
    is, the ones for which the labeling is unknown, are assigned the value `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of now, the binary image includes white pixels that belong to the various
    parts of the image. We will then severely erode this image in order to retain
    only the pixels that certainly belong to the foreground objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that a few pixels that belong to the background forest are still present.
    Let''s keep them. Therefore, they will be considered to correspond to an object
    of interest. Similarly, we can select a few pixels of the background by a large
    dilation of the original binary image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting black pixels correspond to background pixels. This is why the
    thresholding operation assigns the value `128` to these pixels immediately after
    the dilation. The following image is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These images are combined to form the marker image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we used the overloaded `operator+` here in order to combine the images.
    The following image will be used as the input to the watershed algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this input image, the white areas belong, for sure, to the foreground objects,
    the gray areas are a part of the background, and the black areas have an unknown
    label. The role of the watershed segmentation is therefore to assign a label (background/foreground)
    to the black marked pixels by establishing the exact border delimitating the foreground
    objects from the background. This segmentation is then obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The marker image is then updated such that each zero pixel is assigned one
    of the input labels, while the pixels that belong to the found boundaries have
    a value `-1`. The resulting image of the labels is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the boundary image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we did in the preceding recipes, we will use the topological map analogy
    in the description of the watershed algorithm. In order to create watershed segmentation,
    the idea is to progressively flood the image starting at level 0\. As the level
    of water progressively increases (to levels 1, 2, 3, and so on), catchment basins
    are formed. The size of these basins also gradually increases and, consequently,
    the water of two different basins will eventually merge. When this happens, a
    watershed is created in order to keep the two basins separated. Once the level
    of water has reached its maximum level, the sets of these created basins and watersheds
    form the watershed segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the flooding process initially creates many small individual basins.
    When all of these are merged, many watershed lines are created, which results
    in an over-segmented image. To overcome this problem, a modification to this algorithm
    has been proposed in which the flooding process starts from a predefined set of
    marked pixels. The basins created from these markers are labeled in accordance
    with the values assigned to the initial marks. When two basins having the same
    label merge, no watershed is created, thus preventing over-segmentation. This
    is what happens when the `cv::watershed` function is called. The input marker
    image is updated to produce the final watershed segmentation. Users can input
    a marker image with any number of labels and pixels of unknown labeling left to
    value `0`. The marker image is chosen to be an image of a 32-bit signed integer
    in order to be able to define more than `255` labels. It also allows the special
    value, `-1`, to be assigned to the pixels associated with a watershed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate the display of the result, we have introduced two special methods.
    The first method returns an image of the labels (with watersheds at value `0`).
    This is easily done through thresholding, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the second method returns an image in which the watershed lines
    are assigned the value `0`, and the rest of the image is at `255`. This time,
    the `cv::convertTo` method is used to achieve this result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The linear transformation that is applied before the conversion allows the `-1`
    pixels to be converted into `0` (since `-1*255+255=0`).
  prefs: []
  type: TYPE_NORMAL
- en: Pixels with a value greater than `255` are assigned the value `255`. This is
    due to the saturation operation that is applied when signed integers are converted
    into unsigned characters.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Obviously, the marker image can be obtained in many different ways. For example,
    users can be interactively asked to mark the objects of an image by painting some
    areas on the objects and the background of a scene. Alternatively, in an attempt
    to identify an object located at the center of an image, one can also simply input
    an image with the central area marked with a certain label and the border of the
    image (where the background is assumed to be present) marked with another label.
    This marker image can be created by drawing thick rectangles on a marker image
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If we superimpose this marker image on a test image, we will obtain the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/image_05_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the resulting watershed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/image_05_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article, *The viscous watershed transform*, *C. Vachier* and *F. Meyer*,
    *Journal of Mathematical Imaging and Vision*, volume 22, issue 2-3, May 2005,
    gives more information on the watershed transform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting distinctive regions using MSER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, you learned how an image can be segmented into regions
    by gradually flooding it and creating watersheds. The **Maximally Stable External
    Regions** (**MSER**) algorithm uses the same immersion analogy in order to extract
    meaningful regions in an image. These regions will also be created by flooding
    the image level by level, but this time, we will be interested in the basins that
    remain relatively stable for a period of time during the immersion process. It
    will be observed that these regions correspond to some distinctive parts of the
    scene objects pictured in the image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic class to compute the MSER of an image is `cv::MSER`. This class is
    an abstract interface that inherits from the `cv::Feature2D` class; in fact, all
    feature detectors in OpenCV inherit from this super-class. An instance of the
    `cv::MSER` class can be created by using the `create` method. Here, we initialize
    it by specifying a minimum and maximum size for the detected regions in order
    to limit the number of detected features as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the MSER can be obtained by a call to the `detectRegions` method, specifying
    the input image and the appropriate output data structures, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The detection results are provided in the form of a vector of regions represented
    by the pixel points that compose each of them and by a vector of bounding boxes
    enclosing the regions. In order to visualize the results, we create a blank image
    on which we will display the detected regions in different colors (which are randomly
    chosen). This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the MSER form a hierarchy of regions. Therefore, to make all of these
    visible, we have chosen not to overwrite the larger regions when they include
    smaller ones. We can detect MSERs on the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The resulting image will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_05_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Not all regions are visible in this image. Nevertheless, it can be observed
    how this operator has been able to extract some meaningful regions (for example,
    the building's windows) from this image.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MSER uses the same mechanism as the watershed algorithm; that is, it proceeds
    by gradually flooding the image from level `0` to level `255`. Note that in image
    processing, the set of pixels above a certain threshold is often call a **level
    set**. As the level of water increases, you can observe that the sharply delimitated
    darker areas form basins that have a relatively stable shape for a period of time
    (recall that under the immersion analogy, the water levels correspond to the intensity
    levels). These stable basins are the MSER. These are detected by considering the
    connected regions (the basins) at each level and measuring their stability. This
    is done by comparing the current area of a region with the area it previously
    had when the level was down by a value of delta. When this relative variation
    reaches a local minimum, the region is identified as a MSER. The delta value that
    is used to measure the relative stability is the first parameter in the constructor
    of the `cv::MSER` class; its default value is `5`. In addition, to be considered,
    the size of a region must be within a certain predefined range. The acceptable
    minimum and maximum region sizes are the next two parameters of the constructor.
    We must also ensure that the MSER is stable (the fourth parameter), that is, the
    relative variation of its shape is small enough. Stable regions can be included
    in the larger regions (called parent regions).
  prefs: []
  type: TYPE_NORMAL
- en: To be valid, a parent MSER must be sufficiently different from its child; this
    is the diversity criterion, and it is specified by the fifth parameter of the
    `cv::MSER` constructor. In the example used in the previous section, the default
    values for these last two parameters were used. (The default values are `0.25`
    for the maximum allowable variation of a MSER and `0.2` for the minimum diversity
    of a parent MSER). As you see, the detection of MSERs requires the specification
    of several parameters which can make it difficult to work well in various contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first output of the MSER detector is a vector of point sets; each of these
    point sets constitutes a region. Since we are generally more interested in a region
    as a whole rather than its individual pixel locations, it is common to represent
    a MSER by a simple geometrical shape that enclosed the detected region. The second
    output of the detection is therefore a list of bounding boxes. We can therefore
    show the result of the detection by drawing all these rectangular bounding boxes.
    However, this may represent a large number of rectangles to be drawn which would
    make the results difficult to visualize (remember that we also have regions inside
    regions which makes the representation even more cluttered). In the case of our
    example, let''s assume we are mainly interested in detecting the building''s windows.
    We will therefore extract all regions that have an upright rectangular shape.
    This could be done by comparing the area of each bounding box with the area of
    the corresponding detected region. If both have the same value (here, we check
    if the ratio of these two areas is greater than `0.6`), then we accept this MSER.
    The following code implements this test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The extracted MSERs are then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_05_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Other criteria and representation can also be adopted depending on the application.
    The following code tests if the detected region is not too elongated (based on
    the aspect ratio of its rotated bounding rectangle) and then displays them using
    properly oriented bounding ellipses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_05_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note how the child and parent MSER are often represented by very similar ellipses.
    In some cases, it would then be interesting to apply a minimum variation criterion
    on these ellipses in order to eliminate these repeated representations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Computing components' shape descriptors* recipe in [Chapter 7](ch07.html
    "Chapter 7. Extracting Lines, Contours, and Components") , *Extracting Lines,
    Contours, and Components*, will show you how to compute other properties of connected
    point sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting
    Interest Points*, will explain how to use MSER as an interest point detector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
