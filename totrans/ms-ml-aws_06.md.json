["```py\nwget -O /tmp/adform.click.2017.01.json.gz https://dataverse.harvard.edu/api/access/datafile/:persistentId/?persistentId=doi:10.7910/DVN/TADBY7/JCI3VG\n```", "```py\ngunzip /tmp/adform.click.2017.01.json.gz\n\naws s3 cp /tmp/adform.click.2017.01.json s3://mastering-ml-aws/chapter4/training-data/adform.click.2017.01.json\n```", "```py\nctr_df = spark.read.json(s3_train_path)\nctr_df.show(5)\n```", "```py\ndf = ctr_df.selectExpr(\"coalesce(c0[0],0) as f0\",\n                       \"coalesce(c1[0],0) as f1\",\n                       \"coalesce(c2[0],0) as f2\",\n                       \"coalesce(c3[0],0) as f3\",\n                       \"coalesce(c4[0],0) as f4\",\n                       \"l as click\")\n```", "```py\ndf = df.repartition(100).cache()\n```", "```py\nfrom pyspark.ml.feature import StringIndexer\n\nstring_indexer = StringIndexer(inputCol=\"f0\", outputCol=\"f0_index\")\nstring_indexer_model = string_indexer.fit(df)\nctr_df_indexed = string_indexer_model.transform(df).select('f0','f0_index')\nctr_df_indexed.show(5)\n```", "```py\nfrom pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder(inputCol=\"f0_index\", outputCol=\"f0_encoded\")\nencoder.transform(ctr_df_indexed).distinct().show(5)\n```", "```py\ndef categorical_one_hot_encoding_stages(columns):\n    indexers = [StringIndexer(inputCol=column, \n                              outputCol=column + \"_index\", \n                              handleInvalid='keep') \n                for column in columns]\n    encoders = [OneHotEncoder(inputCol=column + \"_index\", \n                              outputCol=column + \"_encoded\") \n                for column in columns]\n    return indexers + encoders\n```", "```py\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import ChiSqSelector\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\ncategorical_columns = ['f0','f1','f2','f3','f4']\nencoded_columns = [column + '_encoded' for column in categorical_columns] \n\ncategorical_stages = categorical_one_hot_encoding_stages(categorical_columns) vector_assembler = VectorAssembler(inputCols=encoded_columns,\n                                   outputCol=\"features\")\nselector = ChiSqSelector(numTopFeatures=100, featuresCol=\"features\",\n                         outputCol=\"selected_features\", labelCol=\"click\")\ndecision_tree = DecisionTreeClassifier(labelCol=\"click\",                                       \n                                       featuresCol=\"selected_features\")\n\npipeline = Pipeline(stages=categorical_stages + [vector_assembler, selector, \n                                                 decision_tree])\n```", "```py\ntrain_df, test_df = df.randomSplit([0.8, 0.2], seed=17)\npipeline_model = pipeline.fit(train_df)\n```", "```py\nprint(pipeline_model.stages[-1].toDebugString)\n\nDecisionTreeClassificationModel (uid=DecisionTreeClassifier_3cc3252e8007) of depth 5 with 11 nodes\n  If (feature 3 in {1.0})\n   Predict: 1.0\n  Else (feature 3 not in {1.0})\n   If (feature 21 in {1.0})\n    Predict: 1.0\n   Else (feature 21 not in {1.0})\n    If (feature 91 in {1.0})\n     Predict: 1.0\n    Else (feature 91 not in {1.0})\n     If (feature 27 in {1.0})\n      Predict: 1.0\n     Else (feature 27 not in {1.0})\n      If (feature 29 in {1.0})\n       Predict: 1.0\n      Else (feature 29 not in {1.0})\n       Predict: 0.0 \n```", "```py\n If (feature 1 in {3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,17.0,27.0})\n       Predict: 0.0\n Else (feature 1 not in {3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,17.0,27.0})\n       Predict: 1.0\n```", "```py\ntest_transformed = pipeline_model.transform(test_df)\n```", "```py\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", \n                                          labelCol=\"click\")\nevaluator.evaluate(test_transformed, \n                   {evaluator.metricName: \"areaUnderROC\"})\n```", "```py\nfrom pyspark.ml.classification import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(labelCol=\"click\",                                                                                        \n                                       featuresCol=\"features\")\n\npipeline_rf = Pipeline(stages=categorical_stages + \\\n                              [vector_assembler, random_forest])\n```", "```py\nrf_pipeline_model = pipeline_rf.fit(train_df)\n\nevaluator.evaluate(rf_pipeline_model.transform(test_df), \n                   {evaluator.metricName: \"areaUnderROC\"})\n\n>> 0.62\n```", "```py\ntest_transformed = model.transform(test_df)\ntrain_transformed = model.transform(train_df)\n```", "```py\ndef deconstruct_vector(row):\n    arr = row['selected_features'].toArray()\n    return tuple([row['click']] + arr.tolist())\n\ndf_for_csv = train_transformed.select(\"click\", \"selected_features\") \\\n                .rdd.map(deconstruct_vector).toDF() \n\ndf_for_csv.write.csv('s3://mastering-ml-aws/chapter4/train-trans-vec-csv-1/', \n                     header=False)\n```", "```py\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport boto3\n\nsess = sagemaker.Session()\nrole = get_execution_role()\ncontainer = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', \n                                                            'xgboost', \n                                                            'latest')\n\ns3_validation_data = \\\n    's3://mastering-ml-aws/chapter4/test-trans-vec-csv-1/'\ns3_train_data = \\\n    's3://mastering-ml-aws/chapter4/train-trans-vec-csv-1/'\ns3_test_data = \\\n    's3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label/'\ns3_output_location = \\\n    's3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/'\n```", "```py\nsagemaker_model = sagemaker.estimator.Estimator(container,\n    role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.4xlarge',\n    train_volume_size=30,\n    train_max_run=360000,\n    input_mode='File',\n    output_path=s3_output_location,\n    sagemaker_session=sess)\n```", "```py\nsagemaker_model.set_hyperparameters(objective='binary:logistic',\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.7,\n    silent=0,\n    num_round=50)\n```", "```py\ntrain_data = sagemaker.session.s3_input(s3_train_data, \n    distribution='FullyReplicated',\n    content_type='text/csv', \n    s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.session.s3_input(s3_validation_data, \n    distribution='FullyReplicated',\n    content_type='text/csv', \n    s3_data_type='S3Prefix')\n\ndata_channels = {'train': train_data, \n                 'validation': validation_data}\n\nsagemaker_model.fit(inputs=data_channels, \n                    logs=True)\n```", "```py\nINFO:sagemaker:Creating training-job with name: xgboost-2019-04-27-20-39-02-968\n2019-04-27 20:39:03 Starting - Starting the training job...\n2019-04-27 20:39:05 Starting - Launching requested ML instances......\n...\ntrain-error:0.169668#011validation-error:0.169047\n2019-04-27 20:49:02 Uploading - Uploading generated training model\n2019-04-27 20:49:02 Completed - Training job completed\nBillable seconds: 480\n```", "```py\ntransformer = sagemaker_model.transformer(instance_count=1, \n                                          instance_type='ml.m4.2xlarge',\n                                          output_path=s3_output_location)\ntransformer.transform(s3_test_data, \n                      content_type='text/csv', \n                      split_type='Line')\ntransformer.wait()\n```", "```py\naws s3 ls s3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/ | head\n```", "```py\n2019-04-28 01:29:58 361031 part-00000-19e45462-84f7-46ac-87bf-d53059e0c60c-c000.csv.out\n2019-04-28 01:29:58 361045 part-00001-19e45462-84f7-46ac-87bf-d53059e0c60c-c000.csv.out\n```", "```py\nimport pandas as pd\n\nscores_df = pd.read_csv(output_path + \\\n   'part-00000-19e45462-84f7-46ac-87bf-d53059e0c60c-c000.csv.out',\n    header=None, \n    names=['score'])\n```", "```py\naws s3 ls --recursive s3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/ | grep model\n```", "```py\nchapter4/sagemaker/output/xgboost/xgboost-2019-04-27-20-39-02-968/output/model.tar.gz\n```", "```py\naws s3 cp s3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/xgboost-2019-04-27-20-39-02-968/output/model.tar.gz /tmp/model.tar.gz\ntar xvf model.tar.gz\n```", "```py\nxgboost-model\n```", "```py\nimport xgboost\nimport pickle as pkl\n\nmodel_local = pkl.load(open('xgboost-model', 'rb'))\n```", "```py\ncolumn_names = ['click'] + ['f' + str(i) for i in range(0, 100)]\nvalidation_df = pd.read_csv(s3_validation_data + \\\n                            'part-00000-25f35551-ffff-41d8-82a9-75f429553035-c000.csv',\n                            header=None, \n                            names=column_names)\n```", "```py\nimport xgboost\nmatrix = xgboost.DMatrix(validation_df[column_names[1:]])\nvalidation_df['score'] = model_local.predict(matrix)\n```", "```py\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(validation_df['click'], validation_df['score'])\n```"]