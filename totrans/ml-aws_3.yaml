- en: '3'
  prefs: []
  type: TYPE_NORMAL
- en: Perform Topic Modeling and Theme Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract and analyze common themes through topic modeling with Amazon Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the basics of topic modeling analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform topic modeling on a set of documents and analyze the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter describes Topic Modeling on common themes using Amazon Comprehend
    analyzing the result for document set.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first part of this chapter, you will learn how to analyze Topic modeling
    output from Amazon Comprehend. Specifically, you will learn the fundamentals of
    the algorithm used for Topic modeling, Latent Dirichlet Allocation (LDA). Learning
    LDA will allow you to apply Topic modeling to a multitude of unique business use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: You will then perform Topic modeling on two documents with a known Topic structure.
    The first is the story **Romeo and Juliet** and the second is **War of the Worlds**.
    Lastly, you will analyze topics from 1,000 text documents containing negative
    movie reviews with Amazon Comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and Analyzing Common Themes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also utilize Amazon Comprehend to analyze a corpus of archives to locate
    the normal topics contained inside the corpus. Amazon Comprehend inspects reports
    in the corpus and, afterward, restores the most noticeable themes and the reports
    that are related to every subject. Subject displaying is an offbeat procedure:
    you present an arrangement of records for preparation and later get the outcomes
    when handling is finished. Amazon Comprehend performs point displaying on huge
    report sets. For the best results, you ought to incorporate around 1,000 records
    when you present a subject demonstrating work.'
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modeling with Latent Dirichlet Allocation (LDA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The subjects or **common themes** of a set of documents can be determined with
    Amazon Comprehend. For example, you have a movie review website with two message
    boards, and you want to determine which message board is discussing two newly
    released movies (one about sport and the other about a political Topic). You can
    provide the message board text data to Amazon Comprehend to discover the most
    prominent topics discussed on each message board.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning algorithm that Amazon Comprehend uses to perform Topic
    Modeling is called latent Dirichlet allocation (LDA). LDA is a learning-based
    model that's used to determine the most important topics in a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: The way that LDA works is it considers every document to be a combination of
    topics, and each word in the document is associated to one of these topics.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the first paragraph of a document consists of words like **eat**,
    **chicken**, **restaurant**, and **cook** then you conclude that the Topic can
    be generalized to **Food**. And if the second paragraph of a document contains
    words like **ticket**, **train**, **kilometer**, and **vacation** then you can
    conclude that the Topic is **Travel**.
  prefs: []
  type: TYPE_NORMAL
- en: Basic LDA example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Topic modeling can seem complex, and understanding the fundamental steps of
    how LDA determines Topics is essential to performing Topic modeling on more complex
    business use cases. Thus, let's deconstruct LDA with the following simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have one document with five sentences. Your goal is to determine the two
    most common topics present in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: I like to eat bread and bananas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I ate a bread and banana smoothie for breakfast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Puppies and kittens are cute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My brother adopted a puppy yesterday.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at this cute opossum munching a piece of broccoli.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LDA discovers the topics that these sentences contain. For example, given the
    above sentences and asked for two topics, LDA might produce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentences 1 and 2: 100% Topic A'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentences 3 and 4: 100% Topic B'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence 5: 60% Topic A, 40% Topic B'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic A**: 30% bread, 15% banana, 10% breakfast, 10% munching, (Thus, you
    can assume Topic A is about food)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic B**: 20% Puppies, 20% kittens, 20% cute, 15% opossum, (This, you can
    assume Topic B is about cute animals).'
  prefs: []
  type: TYPE_NORMAL
- en: Why Use LDA?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LDA is useful when you have an arrangement of records that you need to find
    designs inside, without thinking about the reports themselves. LDA can be utilized
    to create subjects to comprehend an archives, general Topic. This is, usually
    utilized in suggestion frameworks, report arrangement, and record synopsis. In
    conclusion, LDA is helpful in preparing prescient models with subjects and events.
  prefs: []
  type: TYPE_NORMAL
- en: LDA has many use cases. For example, you have 30,000 user emails and want to
    determine the most common topics to provide group-specific recommended content
    based on the most prevalent topics. Manually reading, or even outsourcing the
    manual reading of, 30,000 emails, would take an excessive investment in terms
    of time and money, and the accuracy would be difficult to confirm. However, Amazon
    Comprehend can seamlessly provide the most common topics present in 30,000 emails
    in a few steps with incredible accuracy. First, convert the emails to text files,
    upload them to an S3 bucket, then imitate a Topic modeling job with Amazon Comprehend.
    The output is two `CSV` with the corresponding Topics and terms.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Comprehend–Topic Modeling Guidelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most accurate results are given if you provide Comprehend with the largest
    possible corpus. More specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: You should use no less than 1,000 records in every subject demonstrating work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each report ought to be something like three sentences in length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a record comprises of for the most part numeric information, you should expel
    it from the corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Currently, Topic Modeling is limited to two document languages: **English**
    and **Spanish**.'
  prefs: []
  type: TYPE_NORMAL
- en: A Topic modeling job allows two format types for input data (see the following
    table 1). This allows users to process both collections of large documents (for
    example, newspaper articles or scientific journals), and short documents (for
    example, tweets or social media posts).
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Format Options:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure3.1:  AWS Comprehend - Topic modeling input format options.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure3.1: AWS Comprehend– Topic modeling input format options'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Output Format Options:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: AWS Comprehend - topic modeling output files description.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: AWS Comprehend– Topic modeling output files description'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After Amazon Comprehend processes your document collection, the modeling outputs
    two CSV files: Topic-terms.csv (see Figure 1) and `doc-topics.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `topic-terms.csv` file provides a list of topics in the document collection
    with the terms, respective Topic and weight. For example, if you gave Amazon Comprehend
    two hypothetical documents, **learning to garden** and **investment strategies**,
    it might return the following to describe the two topics in the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Sample topic modeling output (topic-terms.csv) for two documents
    input.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.3: Sample Topic modeling output (`topic-terms.csv`) for two document''s
    input'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `doc-topics.csv` file provides a list of the documents provided for the
    Topic modeling job with the document names, and the respective topics and their
    proportions in each document. Given two hypothetical documents, `learning_to_garden.txt`
    and `investment_strategies.txt,` you can expect the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Sample topic modeling output (doc-topics.csv) for two documents
    input'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: Sample Topic modeling output (`doc-topics.csv`) for two document''s
    input'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 13: Topic Modeling of a Known Topic Structure'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use Amazon Comprehend to perform Topic modeling on
    two documents with known topics (**Romeo and Juliet** and **War of the Worlds**).
    We are using two known topics to better understand LDA. Before proceeding to the
    exercise, just look at an overview of the data pipeline architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Data pipeline architecture overview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: Data pipeline architecture overview'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following are the steps to complete the Topic modelling of a known Topic
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: We need an input and output S3 bucket. Let's create both. Navigate to [https://s3.console.aws.amazon.com/s3/](https://s3.console.aws.amazon.com/s3/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, click on the **Create** **bucket** button to create a bucket:![Figure
    3.6: Create bucket'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.6: Creating a bucket'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For Bucket name, enter a unique name that describes the function. Here, the
    name `aws-ml-input-for-topic-modeling` is used. Click on the **Create** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Clicking Create versus Next uses all default settings for: properties and permissions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7: Create bucket name input'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.7: Creating bucket name input'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, click on the **Create** button to create a folder:![Figure 3.8: Create
    a folder in S3 for topic modeling input'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.8: Creating a folder in S3 for Topic modeling input'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, type in `known_structure`, as the folder name, and then click on the `'known_structure`'
    folder name
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After clicking on the `known_structure` folder:![Figure 3.9: Input bucket screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.9: Input bucket screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, click on the **Upload** button:![Figure 3.10: Upload screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: Upload screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, you will prompted by the following for adding files. Click on **Add files**
    or drag the files onto the screen:![Figure 3.11 Add files screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 Add files screen
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Navigate to download and upload the following two text files from the machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the Romeo and Juliet text file from /lesson3/topic_a/romeo_and_juliet.txt
    [https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/romeo_and_juliet.txt](https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/romeo_and_juliet.txt)
    /lesson3/topic_a/the_war_of_the_worlds.txt [https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/the_war_of_the_worlds.txt](https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/the_war_of_the_worlds.txt)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the files have been uploaded, click on the `known_structure` text files
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the Amazon S3 homescreen:![Figure 3.13: Click Amazon S3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.13: Amazon S3'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, create an output S3 bucket. Use the same S3 bucket creation process.
    Click on the **Create** **bucket** button:![Figure 3.14: Click Create Bucket.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.14: Creating a bucket'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, name the bucket, and then click on the **Create** button:![Figure 3.15:
    Create bucket output for topic modeling'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.15: Create bucket output for Topic modeling'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Navigate to Amazon Comprehend: [https://console.aws.amazon.com/comprehend/](https://console.aws.amazon.com/comprehend/).
    If you are presented with the following screen, click **Try Amazon Comprehend**:![Figure
    3.16: Amazon Comprehend home screen.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.16: Amazon Comprehend home screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, Click on the **Organization** in the left-hand side toolbar:![Figure 3.17:
    Amazon Comprehend Organization screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.17: Amazon Comprehend Organization screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, click on the `known_structure_topic_modeling_job` in the **Name** field:![Figure
    3.18: Name of the topic modeling job input'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_019.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.18: Name of the Topic modeling job'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, scroll down to **Choose input data** and then click on **Search**:![Figure
    3.19: Click Search to locate the topic modeling input data source'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_020.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.19: Clicking Search to locate the Topic modeling input data source'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Navigate to the `known_structure` folder and then click on **Select**:![Figure
    3.20: Click Select for the S3 folder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_021.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.20: Clicking on Select for the S3 folder'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, from the drop-down menu, select **One document per file**:![Figure 3.21:
    Select One document per file'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.21: Selecting One document per file'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, enter **two** for the **Number of Topics** you need to have:![Figure 3.22:
    Enter 2 for the number of topics to perform topic modeling'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_023.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.22: Entering 2 for the number of topics to perform Topic modeling'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, click on **Search** to search the bucket that was created previously:![Figure
    3.23: Click search for the topic modeling S3 output location'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_024.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.23: Clicking on search for the Topic modeling S3 output location'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once you find the bucket you created, click on the bucket you created to output
    Topic modeling:![Figure 3.24: Select the output S3 bucket'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_025.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.24: Selecting the output S3 bucket'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, select the appropriate bucket and then click on **Select**:![Figure 3.25:
    Confirm by clicking Select'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_026.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.25: Confirming by clicking Select'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Scroll down to choose an **IAM** role, and click the circle next to create
    an **IAM** role:![Figure 3.26: Select ‘Create an IAM role’'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_027.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.26: Selecting Create an IAM role'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, select the **Input** and **Output S3 buckets** from the **Permissions
    to access**:![Figure 3.27: Provide permission to Input and Output S3 buckets'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_028.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.27: Providing permission to Input and Output S3 buckets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Enter `myTopicModelingRole` in the Name suffix field and then click on the
    **Create job** button:![Figure 3.28: Click the Create job button'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_029.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.28: Clicking the Create job button'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Creating the job may take a few minutes, but when completed, you will be redirected
    to the Comprehend home screen:![Figure 3.29: Comprehend home screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_030.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.29: Comprehend home screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While the job is being processed, the status displayed will be **In Progress**:![Figure
    3.30: ‘In progress’ status displayed'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_031.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.30: In progress status displayed'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When the status updates to **Completed**, click on the Topic Modeling job name:![Figure3.31:
    Completed status displayed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_032.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure3.31: Completed status displayed'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, scroll down to the **Output** section:![Figure 3.32: Topic modeling output
    display home screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_033.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.32: Topic modeling output display home screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Click on the hyperlink under **Data location**:![Figure 3.33: Topic modeling
    data output hyperlinked location'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_034.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.33: Topic modeling data output hyperlinked location'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Click on the link of the output folder:![Figure 34: Topic modeling output folder'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_035.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 34: Topic modeling output folder'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Click on the output folder. Then, click on `output.tar.gz` and download the
    file:![Figure 3.35: Clicking on Download'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_036.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.35: Clicking on Download'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Click on `output.tar.gz` and select **Show in folder**. Click on **OK** to
    extract the files on your desktop:![Figure 3.36: Click OK'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_037.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.36: Clicking on OK'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Navigate to your desktop. Two files will be extracted: `doc-topics.csv` and
    `topics-terms.csv`. There will be two files to examine: `topic-terms.xlsx` and
    `doc-topics.xlsx`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure3.37: Topic modeling output CSV files'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure3.37: Topic modeling output CSV files'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Your Topic-terms.csv and doc-topics.csv results should be the same as the following
    results. If your results are NOT the same, use the output files for the remainder
    of the chapter, which are located at Lesson3\topic_a\doc-topics.csv [https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/doc-topics.csv](https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/doc-topics.csv)
    and lesson3\topic_a\topic-terms.csv [https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/topic-terms.csv](https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/topic-terms.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.38: topic-terms.csv result'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_039.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.38: `topic-terms.csv` result'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.39: doc-topics.csv results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_040.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.39: doc-topics.csv results'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 14: Performing Known Structure Analysis'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will programmatically upload the CSV (`doc-topics.csv`
    and `Topic-terms.csv`) to S3, merge the CSV on the Topic column, and print the
    output to the console. The following are the steps for performing Known Structure
    Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this step, you may either follow along with the exercise and type in the
    code or obtain it from the source code folder, `local_csv_to_s3_for_analysis.py`,
    and paste it into the editor. The source code is available on GitHub in the following
    repository: [https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/local_csv_to_s3_for_analysis.py](https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/local_csv_to_s3_for_analysis.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import `boto3` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will import `pandas` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will create the S3 client object using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a variable with a unique bucket name. Here, the selected
    bucket name is `known-tm-analysis`, but you will need to create a unique name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a new bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list of the CSV filenames to import:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, iterate on each file to upload to S3 using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, check if the filename is `doc-topics.csv` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, get the `doc-topics.csv` file object and assign it to the `obj` variable
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, read the `csv` obj and assign it to the `doc_topics` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, merge the files on the Topic column to obtain the most common terms per
    document using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, navigate to the location of the `CSV` files in the Command Prompt, and
    execute the code with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The console output is a merged `dataframe` that provides the `docnames` with
    their respective terms and the term''s weights (see the following):![Figure 3.40:
    known_strucutre topic modeling merged results'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_041.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.40: known_strucutre Topic modeling merged results'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To verify the CSV''s, navigate to S3, (reload the page if the new bucket does
    not appear), and the new bucket has been created in S3\. Click on the bucket to
    verify a successful import:![Figure 3.41: known-tm-analysis S3 bucket'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_042.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.41: known-tm-analysis S3 bucket'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There will be two CSV files in the bucket: `doc-topics.csv` and `topic-terms.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.42: Topic modeling results uploaded to S3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image_Lesson3_043.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.42: Topic modeling results uploaded to S3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 4: Perform Topic Modeling on a Set of Documents with Unknown Topics'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will perform Topic modeling on a set of documents with
    unknown topics. Topic modeling, we will consider an example. Suppose your employer
    wants you to build a data pipeline to analyze negative movie reviews that are
    in individual text files with a unique ID filename. Thus, you need to perform
    Topic modeling to determine which files represent the respective topics. Overall,
    negative reviews provide more monetary benefit or loss to the company, thus, they
    are prioritizing negative reviews versus positive reviews. The company's end goal
    is to incorporate the data into a feedback chatbot application. To ensure that
    this happens correctly, you need have a file that contains negative comments.
    The expected outcome for this activity will be the Topic modeling results from
    the negative movie review files.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performing Topic Modeling**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the following link to obtain the text data file that contains negative
    review comments: [https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/activity/localfoldernegative_movie_review_files/cv000_29416.txt](https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/activity/localfoldernegative_movie_review_files/cv000_29416.txt).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bucket for the Topic modelling with a unique name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the Topic modelling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the OS and Boto3\. Mention your unique bucket name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gather all of the working directories of the local path and make them into text files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a list for all of the text files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate the files and upload them to S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a job on Organization using Amazon Comprehend.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As per the requirements, choose the input data. It may be **My document** or
    **Example document**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the file from the data source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the input format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the number of topics to perform the modeling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose an IAM role and create a job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the output file and extract the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generated output will include the two `.CSV` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Analysis of Unknown Topics**'
  prefs: []
  type: TYPE_NORMAL
- en: Import `Boto3` and `pandas`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the S3 client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new bucket with a unique name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a list of CSV filenames to import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the filename and assign it to the **obj** variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the **obj** variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the files on the Topic column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the merged files to the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: To refer to the detailed steps, go to the *Appendix A* at the end of this book
    on Page no.203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we learned about analyzing Topic modeling results from AWS
    Comprehend. You are now able to incorporate S3 to store data and can use it to
    perform analysis. In addition, we learned how to analyze documents that we know
    the topics of before performing Topic modeling, and those documents where the
    Topic is known. The latter requires additional analysis to determine the relevant
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into the concept of chatbots and their understanding
    by using Natural Processing Language.
  prefs: []
  type: TYPE_NORMAL
