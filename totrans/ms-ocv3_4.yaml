- en: Non-Rigid Face Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-rigid face tracking, which is the estimation of a quasi-dense set of facial
    features in each frame of a video stream, is a difficult problem for which modern
    approaches borrow ideas from a number of related fields, including Computer Vision,
    computational geometry, machine learning, and image processing. Non-rigidity here
    refers to the fact that relative distances between facial features vary between
    facial expression and across the population, and is distinct from face detection
    and tracking, which aims only to find the location of the face in each frame,
    rather than the configuration of facial features. Non-rigid face tracking is a
    popular research topic that has been pursued for over two decades, but it is only
    recently that various approaches have become robust enough, and processors fast
    enough, which makes the building of commercial applications possible.
  prefs: []
  type: TYPE_NORMAL
- en: Although commercial-grade face tracking can be highly sophisticated and pose
    a challenge even for experienced Computer Vision scientists, in this chapter we
    will see that a face tracker that performs reasonably well under constrained settings
    can be devised using modest mathematical tools and OpenCV's substantial functionality
    in linear algebra, image processing, and visualization. This is particularly the
    case when the person to be tracked is known ahead of time, and training data in
    the form of images and landmark annotations are available. The techniques described
    henceforth will act as a useful starting point and a guide for further pursuits
    towards a more elaborate face-tracking system.
  prefs: []
  type: TYPE_NORMAL
- en: 'An outline of this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview**: This section covers a brief history of face tracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilities**: This section outlines the common structures and conventions
    used in this chapter. It includes object-oriented design, data storage and representation,
    and a tool for data collection and annotation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geometrical constraints**: This section describes how facial geometry and
    its variations are learned from the training data and utilized during tracking
    to constrain the solution. This includes modeling the face as a linear shape model
    and how global transformations can be integrated into its representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial feature detectors**: This section describes how to learn the appearance
    of facial features in order to detect them in an image where the face is to be
    tracked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face detection and initialization**: This section describes how to use face
    detection to initialize the tracking process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face tracking**: This section combines all components described previously
    into a tracking system through the process of image alignment. Discussion on the
    settings in which the system can be expected to work best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following block diagram illustrates the relationships between the various
    components of the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that all methods employed in this chapter follow a data-driven paradigm
    whereby all models used are learned from data rather than designed by hand in
    a rule-based setting. As such, each component of the system will involve two components:
    training and testing. Training builds the models from data and testing employs
    these models on new unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-rigid face tracking was first popularized in the early to mid-1990s with
    the advent of **Active Shape Models** (**ASM**) by Cootes and Taylor. Since then,
    a tremendous amount of research has been dedicated to solving the difficult problem
    of generic face tracking with many improvements over the original method that
    ASM proposed. The first milestone was the extension of ASM to **Active Appearance
    Models** (**AAM**) in 2001, also by Cootes and Taylor. This approach was later
    formalized though the principled treatment of image warps by Baker and colleges
    in the mid-2000s. Another strand of work along these lines was the **3D morphable
    model** (**3DMM**) by Blanz and Vetter, which like AAM, not only modeled image
    textures as opposed to profiles along object boundaries as in ASM, but took it
    one step further by representing the models with a highly dense 3D data learned
    from laser scans of faces. From the mid- to late 2000s, the focus of research
    on face tracking shifted away from how the face was parameterized to how the objective
    of the tracking algorithm was posed and optimized. Various techniques from the
    machine-learning community were applied with various degrees of success. Since
    the turn of the century, the focus has shifted once again, this time towards joint
    parameter and objective design strategies that guarantee global solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the continued intense research into face tracking, there have been relatively
    few commercial applications that use it. There has also been a lag in uptake by
    hobbyists and enthusiasts, despite there being a number of freely available source
    code packages for a number of common approaches. Nonetheless, in the past 2 years
    there has been a renewed interest in the public domain for the potential use of
    face tracking and commercial-grade products are beginning to emerge.
  prefs: []
  type: TYPE_NORMAL
- en: Utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the intricacies of face tracking, a number of book-keeping
    tasks and conventions common to all face-tracking methods must first be introduced.
    The rest of this section will deal with these issues. An interested reader may
    want to skip this section at the first reading and go straight to the section
    on geometrical constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Object-oriented design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with face detection and recognition, programmatically, face tracking consists
    of two components: data and algorithms. The algorithms typically perform some
    kind of operation on the incoming (that is, online) data by referencing prestored
    (that is, offline) data as a guide. As such, an object-oriented design that couples
    algorithms with the data they rely on is a convenient design choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In OpenCV v2.x, a convenient XML/YAML file storage class was introduced that
    greatly simplifies the task of organizing offline data for use in the algorithms.
    To leverage this feature, all classes described in this chapter will implement
    read-and write-serialization functions. An example of this is shown as follows
    for an imaginary class `foo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, `Mat` is OpenCV's matrix class and `type_b` is a (imaginary) user-defined
    class that also has the serialization functionality defined. The I/O functions
    `read` and `write` implement the serialization. The `FileStorage` class supports
    two types of data structures that can be serialized. For simplicity, in this chapter
    all classes will only utilize mappings, where each stored variable creates a `FileNode`
    object of type `FileNode::MAP`. This requires a unique key to be assigned to each
    element. Although the choice for this key is arbitrary, we will use the variable
    name as the label for consistency reasons. As illustrated in the preceding code
    snippet, the `read` and `write` functions take on a particularly simple form,
    whereby the streaming operators (`<<` and `>>`) are used to insert and extract
    data to the `FileStorage` object. Most OpenCV classes have implementations of
    the `read` and `write` functions, allowing the storage of the data that they contain
    to be done with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to defining the serialization functions, one must also define two
    additional functions for the serialization in the `FileStorage` class to work,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As the functionality of these two functions remains the same for all classes
    we describe in this section, they are templated and defined in the `ft.hpp` header
    file found in the source code pertaining to this chapter. Finally, to easily save
    and load user-defined classes that utilize the serialization functionality, templated
    functions for these are also implemented in the header file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the label associated with the object is always the same (that is,
    `ft object`). With these functions defined, saving and loading object data is
    a painless process. This is shown with the help of the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `.xml` extension results in an XML-formatted data file. For any
    other extension, it defaults to the (more human-readable) YAML format.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection - image and video annotation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern face-tracking techniques are almost entirely data driven, that is, the
    algorithms used to detect the locations of facial features in the image rely on
    models of the appearance of the facial features and the geometrical dependencies
    between their relative locations from a set of examples. The larger the set of
    examples, the more robust the algorithms behave, as they become more aware of
    the gamut of variability that faces can exhibit. Thus, the first step in building
    a face-tracking algorithm is to create an image/video annotation tool, where the
    user can specify the locations of the desired facial features in each example
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Training data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data for training face tracking algorithms generally consists of four components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Images**: This component is a collection of images (still images or video
    frames) that contain an entire face. For best results, this collection should
    be specialized to the types of conditions (that is, identity, lighting, distance
    from camera, capturing device, among others) in which the tracker is later deployed.
    It is also crucial that the faces in the collection exhibit the range of head
    poses and facial expressions that the intended application expects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annotations**: This component has ordered hand-labeled locations in each
    image that correspond to every facial feature to be tracked. More facial features
    often lead to a more robust tracker as the tracking algorithm can use their measurements
    to reinforce each other. The computational cost of common tracking algorithms
    typically scales linearly with the number of facial features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Symmetry indices**: This component has an index for each facial feature point
    that defines its bilaterally symmetrical feature. This can be used to mirror the
    training images, effectively doubling the training set size and symmetrizing the
    data along the *y* axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connectivity indices**: This component has a set of index pairs of the annotations
    that define the semantic interpretation of the facial features. These connections
    are useful for visualizing the tracking results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A visualization of these four components is shown in the following image, where
    from left to right we have the raw image, facial feature annotations, color-coded
    bilateral symmetry points, mirrored image, and annotations and facial feature
    connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To conveniently manage such data, a class that implements storage and access
    functionality is a useful component. The `CvMLData` class in the `ml` module of
    OpenCV has the functionality for handling general data often used in machine-learning
    problems. However, it lacks the functionality required from the face-tracking
    data. As such, in this chapter, we will use the `ft_data` class, declared in the
    `ft_data.hpp` header file, which is designed specifically with the peculiarity
    of face-tracking data in mind. All data elements are defined as public members
    of the class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `Vec2i` and `Point2f` types are OpenCV classes for vectors of two integers
    and 2D floating-point coordinates respectively. The `symmetry` vector has as many
    components as there are feature points on the face (as defined by the user). Each
    of the `connections` define a zero-based index pair of connected facial features.
    As the training set can potentially be very large, rather than storing the images
    directly, the class stores the filenames of each image in the `imnames` member
    variable (note that this requires the images to be located in the same relative
    path for the filenames to remain valid). Finally, for each training image, a collection
    of facial feature locations are stored as vectors of floating-point coordinates
    in the `points` member variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ft_data` class implements a number of convenience methods for accessing
    the data. To access an image in the dataset, the `get_image` function loads the
    image at the specified index, `idx`, and optionally mirrors it around the y axis
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The (`0`,`1`) flag passed to OpenCV's `imread` function specifies whether the
    image is loaded as a three-channel color image or as a single-channel grayscale
    image. The flag passed to OpenCV's `flip` function specifies the mirroring around
    the *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access a point set corresponding to an image at a particular index, the
    `get_points` function returns a vector of floating-point coordinates with the
    option of mirroring their indices as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that when the mirroring flag is specified, this function calls the `get_image`
    function. This is required to determine the width of the image in order to correctly
    mirror the facial feature coordinates. A more efficient method could be devised
    by simply passing the image width as a variable. Finally, the utility of the `symmetry`
    member variable is illustrated in this function. The mirrored feature location
    of a particular index is simply the feature location at the index specified in
    the `symmetry` variable with its *x* coordinate flipped and biased.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the `get_image` and `get_points` functions return empty structures if
    the specified index is outside the one that exists for the dataset. It is also
    possible that not all images in the collection are annotated. Face-tracking algorithms
    can be designed to handle missing data; however, these implementations are often
    quite involved and are outside the scope of this chapter. The `ft_data` class
    implements a function for removing samples from its collection that do not have
    corresponding annotations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The sample instance that has the most number of annotations is assumed to be
    the canonical sample. All data instances that have a point set with less than
    that number of points are removed from the collection using the vector's `erase`
    function. Also notice that points with (*x, y*) coordinates less than 1 are considered
    missing in their corresponding image (possibly due to occlusion, poor visibility,
    or ambiguity).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ft_data` class implements the serialization functions `read` and `write`,
    and can thus be stored and loaded easily. For example, saving a dataset can be
    done as simply as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For visualizing the dataset, `ft_data` implements a number of drawing functions.
    Their use is illustrated in the `visualize_annotations.cpp` file. This simple
    program loads annotation data stored in the file specified in the command-line,
    removes the incomplete samples, and displays the training images with their corresponding
    annotations, symmetry, and connections superimposed. A few notable features of
    OpenCV's `highgui` module are demonstrated here. Although quite rudimentary and
    not well suited for complex user interfaces, the functionality in OpenCV's `highgui`
    module is extremely useful for loading and visualizing data and algorithmic outputs
    in Computer Vision applications. This is perhaps one of OpenCV's distinguishing
    qualities compared to other Computer Vision libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To aid in generating annotations for use with the code in this chapter, a rudimentary
    annotation tool can be found in the `annotate.cpp` file. The tool takes as input
    a video stream, either from a file or from the camera. The procedure for using
    the tool is listed in the following four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Capture images**: In this first step, the image stream is displayed on the
    screen and the user chooses the images to annotate by pressing the `S` key. The
    best set of features to annotate are those that maximally span the range of facial
    behaviors that the face-tracking system will be required to track.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Annotate first image**: In this second step, the user is presented with the
    first image selected in the previous stage. The user then proceeds to click on
    the image at the locations pertaining to the facial features that require tracking.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Annotate connectivity**: In this third step, to better visualize a shape,
    the connectivity structure of points needs to be defined. Here, the user is presented
    with the same image as in the previous stage, where the task now is to click a
    set of point pairs, one after the other, to build the connectivity structure for
    the face model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Annotate symmetry**: In this step, still with the same image, the user selects
    pairs of points that exhibit bilateral symmetry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Annotate remaining images**: In this final step, the procedure here is similar
    to that of *step 2*, except that the user can browse through the set of images
    and annotate them asynchronously.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An interested reader may want to improve on this tool by improving its usability
    or may even integrate an incremental learning procedure, whereby a tracking model
    is updated after each additional image is annotated and is subsequently used to
    initialize the points to reduce the burden of annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Although some publicly available datasets are available for use with the code
    developed in this chapter (see for example, the description in the following section),
    the annotation tool can be used to build person-specific face-tracking models,
    which often perform far better than their generic, person-independent, counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-annotated data (the MUCT dataset)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the hindering factors of developing face-tracking systems is the tedious
    and error-prone process of manually annotating a large collection of images, each
    with a large number of points. To ease this process for the purpose of following
    the work in this chapter, the publicly available MUCT dataset can be downloaded
    from [h t t p ://w w w /m i l b o . o r g /m u c t](http://www/milbo.org/muct)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 3,755 face images annotated with 76 point landmarks.
    The subjects in the dataset vary in age and ethnicity and are captured under a
    number of different lighting conditions and head poses.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the MUCT dataset with the code in this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download the image set**: In this step, all the images in the dataset can
    be obtained by downloading the files `muct-a-jpg-v1.tar.gz` to `muct-e-jpg-v1.tar.gz`
    and uncompressing them. This will generate a new folder in which all the images
    will be stored.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Download the annotations**: In this step, download the file containing the
    annotations `muct-landmarks-v1.tar.gz`. Save and uncompress this file in the same
    folder as the one in which the images were downloaded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define connections and symmetry using the annotation tool**: In this step,
    from the command-line, issue the command `./annotate -m $mdir -d $odir`, where
    `$mdir` denotes the folder where the MUCT dataset was saved and `$odir` denotes
    the folder to which the `annotations.yaml` file, containing the data stored as
    an `ft_data` object, will be written.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usage of the MUCT dataset is encouraged to get a quick introduction to the functionality
    of the face-tracking code described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Geometrical constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In face tracking, geometry refers to the spatial configuration of a predefined
    set of points that correspond to physically consistent locations on the human
    face (such as eye corners, nose tips, and eyebrow edges). A particular choice
    of these points is application dependent, with some applications requiring a dense
    set of over 100 points and others requiring only a sparser selection. However,
    the robustness of face-tracking algorithms generally improves with an increased
    number of points, as their separate measurements can reinforce each other through
    their relative spatial dependencies. For example, the location of an eye corner
    is a good indication of where to expect the nose to be located. However, there
    are limits to improvements in robustness gained by increasing the number of points,
    where performance typically plateaus after around 100 points. Furthermore, increasing
    the point set used to describe a face carries with it a linear increase in computational
    complexity. Thus, applications with strict constraints on computational load may
    fare better with fewer points.
  prefs: []
  type: TYPE_NORMAL
- en: It is also the case that faster tracking often leads to more accurate tracking
    in the online setting. This is because, when frames are dropped, the perceived
    motion between frames increases, and the optimization algorithm used to find the
    configuration of the face in each frame has to search a larger space of possible
    configurations of feature points; a process that often fails when displacement
    between frames becomes too large. In summary, although there are general guidelines
    on how to best design the selection of facial feature points, to get an optimal
    performance, this selection should be specialized to the application's domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Facial geometry is often parameterized as a composition of two elements: a
    **global transformation** (rigid) and a **local deformation** (non-rigid). The
    global transformation accounts for the overall placement of the face in the image,
    which is often allowed to vary without constraint (that is, the face can appear
    anywhere in the image). This includes the (*x, y*) location of the face in the
    image, the in-plane head rotation, and the size of the face in the image. Local
    deformations, on the other hand, account for differences between facial shapes
    across identities and between expressions. In contrast to the global transformation,
    these local deformations are often far more constrained largely due to the highly
    structured configuration of facial features. Global transformations are generic
    functions of 2D coordinates, applicable to any type of object, whereas local deformations
    are object specific and must be learned from a training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will describe the construction of a geometrical model of
    a facial structure, hereby referred to as the shape model. Depending on the application,
    it can capture expression variations of a single individual, differences between
    facial shapes across a population, or a combination of both. This model is implemented
    in the `shape_model` class which can be found in the `shape_model.hpp` and `shape_model.cpp`
    files. The following code snippet is a part of the header of the `shape_model`
    class that highlights its primary functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The model that represents variations in face shapes is encoded in the subspace
    matrix `V` and variance vector `e`. The parameter vector `p` stores the encoding
    of a shape with respect to the model. The connectivity matrix `C` is also stored
    in this class as it pertains only to visualizing instances of the face's shape.
    The three functions of primary interest in this class are `calc_params`, `calc_shape`,
    and `train`. The `calc_params` function projects a set of points onto the space
    of plausible face shapes. It optionally provides separate confidence weights for
    each of the points to be projected. The `calc_shape` function generates a set
    of points by decoding the parameter vector `p` using the face model (encoded by
    `V` and `e`). The `train` function learns the encoding model from a dataset of
    face shapes, each of which consists of the same number of points. The parameters
    `frac` and `kmax` are parameters of the training procedure that can be specialized
    for the data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: The functionality of this class will be elaborated in the sections that follow,
    where we begin by describing **Procrustes analysis**, a method for rigidly registering
    a point set, followed by the linear model used to represent local deformations.
    The programs in the `train_shape_model.cpp` and `visualize_shape_model.cpp` files
    train and visualize the shape model respectively. Their usage will be outlined
    at the end of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Procrustes analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a deformation model of face shapes, we must first process
    the raw annotated data to remove components pertaining to global rigid motion.
    When modeling geometry in 2D, a rigid motion is often represented as a similarity
    transform; this includes the scale, in-plane rotation, and translation. The following
    image illustrates the set of permissible motion types under a similarity transform.
    The process of removing global rigid motion from a collection of points is called
    **Procrustes analysis**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Mathematically, the objective of Procrustes analysis is to simultaneously find
    a canonical shape and similarity, and transform each data instance that brings
    them into alignment with the canonical shape. Here, alignment is measured as the
    least-squares distance between each transformed shape with the canonical shape.
    An iterative procedure for fulfilling this objective is implemented in the `shape_model`
    class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm begins by subtracting the center of mass of each shape''s instance
    followed by an iterative procedure that alternates between computing the canonical
    shape, as the normalized average of all shapes, and rotating and scaling each
    shape to best match the canonical shape. The normalization step of the estimated
    canonical shape is necessary to fix the scale of the problem and prevent it from
    shrinking all the shapes to zero. The choice of this anchor scale is arbitrary;
    here, we have chosen to enforce the length of the canonical shape vector `C` to
    1.0, as is the default behavior of OpenCV''s `normalize` function. Computing the
    in-plane rotation and scaling that best aligns each shape''s instance to the current
    estimate of the canonical shape is effected through the `rot_scale_align` function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This function minimizes the following least squares difference between the
    rotated and canonical shapes. Mathematically this can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_04-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here the solution to the least-squares problem takes on the closed-form solution
    shown in the following image on the right-hand side of the equation. Note that
    rather than solving for the scaling and in-plane rotation, which are nonlinearly
    related in the scaled 2D rotation matrix, we solve for the variables (`a`, `b`).
    These variables are related to the scale and rotation matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A visualization of the effects of Procrustes analysis on raw annotated shape
    data is illustrated in the following image. Each facial feature is displayed with
    a unique color. After translation normalization, the structure of the face becomes
    apparent, where the locations of facial features cluster around their average
    locations. After the iterative scale and rotation normalization procedure, the
    feature clustering becomes more compact and their distribution becomes more representative
    of the variation induced by facial deformation. This last point is important as
    it is these deformations that we will attempt to model in the following section.
    Thus, the role of Procrustes analysis can be thought of as a preprocessing operation
    on the raw data that will allow better local deformation models of the face to
    be learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear shape models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of facial-deformation modeling is to find a compact parametric representation
    of how the face's shape varies across identities and between expressions. There
    are many ways of achieving this goal with various levels of complexity. The simplest
    of these is to use a linear representation of facial geometry. Despite its simplicity,
    it has been shown to accurately capture the space of facial deformations, particularly
    when the faces in the dataset are largely in a frontal pose. It also has the advantage
    that inferring the parameters of its representation is an extremely simple and
    cheap operation, in contrast to its nonlinear counterparts. This plays an important
    role when deploying it to constrain the search procedure during tracking.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of linearly modeling facial shapes is illustrated in the following
    image. Here, a face shape, which consists of *N* facial features, is modeled as
    a single point in a 2*N*-dimensional space. The aim of linear modeling is to find
    a low-dimensional hyperplane embedded within this 2*N*-dimensional space in which
    all the face shape points lie (that is, the green points in the image). As this
    hyperplane spans only a subset of the entire 2*N*-dimensional space, it is often
    referred to as the subspace. The lower the dimensionality of the subspace, the
    more compact the representation of the face is and the stronger the constraint
    that it places on the tracking procedure becomes. This often leads to more robust
    tracking. However, care should be taken in selecting the subspace's dimension
    so that it has enough capacity to span the space of all faces, but not so much
    that non-face shapes lie within its span (that is, the red points in the image).
    It should be noted that when modeling data from a single person, the subspace
    that captures the face's variability is often far more compact than the one that
    models multiple identities. This is one of the reasons why person-specific trackers
    perform much better than generic ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The procedure for finding the best low-dimensional subspace that spans a dataset
    is called **Principal Component Analysis** (**PCA**). OpenCV implements a class
    for computing PCA; however, it requires the number of preserved subspace dimensions
    to be prespecified. As this is often difficult to determine a priori, a common
    heuristic is to choose it based on the fraction of the total amount of variation
    it accounts for. In the `shape_model::train` function, PCA is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, each column of the `dY` variable denotes the mean-subtracted Procrustes-aligned
    shape. Thus, **Singular Value Decomposition** (**SVD**) is effectively applied
    to the covariance matrix of the shape data (that is, `dY.t()*dY`). The `w` member
    of OpenCV''s `SVD` class stores the variance in the major directions of variability
    of the data, ordered from largest to smallest. A common approach to choose the
    dimensionality of the subspace is to choose the smallest set of directions that
    preserve a fraction `frac` of the total energy of the data, which is represented
    by the entries of `svd.w`. As these entries are ordered from largest to smallest,
    it suffices to enumerate the subspace selection by greedily evaluating the energy
    in the top `k` directions of variability. The directions themselves are stored
    in the `u` member of the `SVD` class. The `svd.w` and `svd.u` components are generally
    referred to as the eigen spectrum and eigen vectors respectively. A visualization
    of these two components is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the eigen spectrum decreases rapidly, which suggests that most of
    the variation contained in the data can be modeled with a low-dimensional subspace.
  prefs: []
  type: TYPE_NORMAL
- en: A combined local-global representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A shape in the image frame is generated by the composition of a local deformation
    and a global transformation. Mathematically, this parameterization can be problematic,
    as the composition of these transformations results in a nonlinear function that
    does not admit a closed-form solution. A common way to circumvent this problem
    is to model the global transformation as a linear subspace and append it to the
    deformation subspace. For a fixed shape, a similarity transform can be modeled
    with a subspace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the `shape_model` class, this subspace is generated using the `calc_rigid_basis`
    function. The shape from which the subspace is generated (that is, the `x` and
    `y` components in the preceding equation) is the mean shape over the Procustes-aligned
    shape (that is, the canonical shape). In addition to constructing the subspace
    in the aforementioned form, each column of the matrix is normalized to unit length.
    In the `shape_model::train` function, the variable `dY` described in the previous
    section is computed by projecting out the components of the data that pertain
    to rigid motion, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that this projection is implemented as a simple matrix multiplication.
    This is possible because the columns of the rigid subspace have been length normalized.
    This does not change the space spanned by the model, and means only that `R.t()*R`
    equals the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the directions of variability stemming from rigid transformations have been
    removed from the data before learning the deformation model, the resulting deformation
    subspace will be orthogonal to the rigid transformation subspace. Thus, concatenating
    the two subspaces results in a combined local-global linear representation of
    facial shapes that is also orthonormal. Concatenation here can be performed by
    assigning the two subspace matrices to submatrices of the combined subspace matrix
    through the ROI extraction mechanism implemented in OpenCV''s `Mat` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The orthonormality of the resulting model means that the parameters describing
    a shape can be computed easily, as is done in the `shape_model::calc_params` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here `s` is a vectorized face shape and `p` stores the coordinates in the face
    subspace that represents it.
  prefs: []
  type: TYPE_NORMAL
- en: A final point to note about linearly modeling facial shapes is how to constrain
    the subspace coordinates such that shapes generated using it remain valid. In
    the following image, instances of face shapes that lie within the subspace are
    shown for an increasing value of the coordinates in one of the directions of variability
    in increments of four standard deviations. Notice that for small values, the resulting
    shape remains face-like, but deteriorates as the values become too large.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A simple way to prevent such deformation is to clamp the subspace coordinate
    values to lie within a permissible region as determined from the dataset. A common
    choice for this is a box constraint within &pm;3 standard deviations of the data,
    which accounts for 99.7 percent of variation in the data. These clamping values
    are computed in the `shape_model::train` function after the subspace is found,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the variance is computed over the subspace coordinate `Q` after
    normalizing with respect to the coordinate of the first dimension (that is, scale).
    This prevents data samples that have relatively large scale from dominating the
    estimate. Also, notice that a negative value is assigned to the variance of the
    coordinates of the rigid subspace (that is, the first four columns of `V`). The
    clamping function `shape_model::clamp` checks to see if the variance of a particular
    direction is negative and only applies clamping if it is not, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this is that the training data is often captured under contrived
    settings where the face is upright and centered in the image at a particular scale.
    Clamping the rigid components of the shape model to adhere to the configurations
    in the training set would then be too restrictive. Finally, as the variance of
    each deformable coordinate is computed in the scale-normalized frame, the same
    scaling must be applied to the coordinates during clamping.
  prefs: []
  type: TYPE_NORMAL
- en: Training and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An example program for training a shape model from the annotation data can
    be found in `train_shape_model.cpp`. With the command-line argument `argv[1]`
    containing the path to the annotation data, training begins by loading the data
    into memory and removing incomplete samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The annotations for each example, and optionally their mirrored counterparts,
    are then stored in a vector before passing them to the training function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape model is then trained by a single function call to `shape_model::train` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `frac` (that is, the fraction of variation to retain) and `kmax` (that
    is, the maximum number of eigen vectors to retain) can be optionally set through
    command-line options, although the default settings of 0.95 and 20, respectively,
    tend to work well in most cases. Finally, with the command-line argument `argv[2]`
    containing the path to save the trained shape model to, saving can be performed
    by a single function call as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The simplicity of this step results from defining the `read` and `write` serialization
    functions for the `shape_model` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the trained shape model, the `visualize_shape_model.cpp` program
    animates the learned non-rigid deformations of each direction in turn. It begins
    by loading the shape model into memory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The rigid parameters that place the model at the center of the display window
    are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `calc_scale` function finds the scaling coefficient that would generate
    face shapes with a width of 200 pixels. The translation components are computed
    by finding the coefficients that generate a translation of 150 pixels (that is,
    the model is mean-centered and the display window is 300x300 pixels in size).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the first column of `shape_model::V` corresponds to scale and the
    third and fourth columns to `x` and `y` translations respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'A trajectory of parameter values is then generated, which begins at zero, moves
    to the positive extreme, moves to the negative extreme, and then back to zero,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, each phase of the animation is composed of 50 increments. This trajectory
  prefs: []
  type: TYPE_NORMAL
- en: is then used to animate the face model and render the results in a display window
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that the rigid coefficients (that is, those corresponding to the first
    four columns of `shape_model::V`) are always set to the values computed previously,
    to place the face at the center of the display window.
  prefs: []
  type: TYPE_NORMAL
- en: Facial feature detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Detecting facial features in images bares a strong resemblance to general object
    detection. OpenCV has a set of sophisticated functions for building general object
    detectors, the most well-known of which is the cascade of Haar-based feature detectors
    used in their implementation of the well-known **Viola-Jones** **face detector**.
    There are, however, a few distinguishing factors that make facial feature detection
    unique. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision versus robustness**: In generic object detection, the aim is to
    find the coarse position of the object in the image; facial feature detectors
    are required to give highly precise estimates of the location of the feature.
    An error of a few pixels is considered inconsequential in object detection but
    it can mean the difference between a smile and a frown in facial expression estimation
    through feature detections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ambiguity from limited spatial support**: It is common to assume that the
    object of interest in generic object detection exhibits sufficient image structure
    such that it can be reliably discriminated from image regions that do not contain
    the object. This is often not the case for facial features, which typically have
    limited spatial support. This is because image regions that do not contain the
    object can often exhibit a very similar structure to facial features. For example,
    a feature on the periphery of the face, seen from a small bounding box centered
    at the feature, can be easily confused with any other image patch that contains
    a strong edge through its center.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational complexity**: Generic object detection aims to find all instances
    of the object in an image. Face tracking, on the other hand, requires the locations
    of all facial features, which often ranges from around 20 to 100 features. Thus,
    the ability to evaluate each feature detector efficiently is paramount in building
    a face tracker that can run in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to these differences, the facial feature detectors used in face tracking
    are often specifically designed with that purpose in mind. There are, of course,
    many instances of generic object-detection techniques being applied to facial
    feature detectors in face tracking. However, there does not appear to be a consensus
    in the community about which representation is best suited for the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will build facial feature detectors using a representation
    that is perhaps the simplest model one would consider: a linear image patch. Despite
    its simplicity, with due care in designing its learning procedure, we will see
    that this representation can in fact give reasonable estimates of facial feature
    locations for use in a face-tracking algorithm. Furthermore, their simplicity
    enables an extremely rapid evaluation that makes real-time face tracking possible.
    Due to their representation as an image patch, the facial feature detectors are
    hereby referred to as patch models. This model is implemented in the `patch_model`
    class that can be found in the'
  prefs: []
  type: TYPE_NORMAL
- en: '`patch_model.hpp` and `patch_model.cpp` files. The following code snippet is'
  prefs: []
  type: TYPE_NORMAL
- en: 'of the header of the `patch_model` class that highlights its primary functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The patch model used to detect a facial feature is stored in the matrix `P`.
    The two functions of primary interest in this class are `calc_response` and `train`.
    The `calc_response` function evaluates the patch model's response at every integer
    displacement over the search region `im`. The `train` function learns the patch
    model `P` of size `psize` that, on an average, yields response maps over the training
    set that is as close as possible to the ideal response map. The parameters `var`,
    `lambda`, `mu_init`, and `nsamples` are parameters of the training procedure that
    can be tuned to optimize performance for the data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: The functionality of this class will be elaborated in this section. We begin
    by discussing the correlation patch and its training procedure, which will be
    used to learn the patch model. Next, the `patch_models` class, which is a collection
    of the patch models for each facial feature and has functionality that accounts
    for global transformations will be described. The programs in `train_patch_model.cpp`
    and `visualize_patch_model.cpp` train and visualize the patch models, respectively,
    and their usage will be outlined at the end of this section on facial feature
    detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation-based patch models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In learning detectors, there are two primary competing paradigms: generative
    and discriminative. Generative methods learn an underlying representation of image
    patches that can best generate the object appearance in all its manifestations.
    Discriminative methods, on the other hand, learn a representation that best discriminates
    instances of the object from other objects that the model will likely encounter
    when deployed. Generative methods have the advantage that the resulting model
    encodes properties specific to the object, allowing novel instances of the object
    to be visually inspected. A popular approach that falls within the paradigm of
    generative methods is the famous `Eigenfaces` method. Discriminative methods have
    the advantage that the full capacity of the model is geared directly towards the
    problem at hand; discriminating instances of the object from all others. Perhaps
    the most well-known of all discriminative methods is the support vector machine.
    Although both paradigms can work well in many situations, we will see that when
    modeling facial features as an image patch, the discriminative paradigm is far
    superior.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `Eigenfaces` and support vector machine methods were originally
    developed for classification rather than detection or image alignment. However,
    their underlying mathematical concepts have been shown to be applicable to the
    face-tracking domain.
  prefs: []
  type: TYPE_NORMAL
- en: Learning discriminative patch models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given an annotated dataset, the feature detectors can be learned independently
    from each other. The learning objective of a discriminative patch model is to
    construct an image patch that, when cross-correlated with an image region containing
    the facial feature, yields a strong response at the fease. Mathematically, this
    can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_11-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, **P** denotes the patch model, **I** denotes the i^(th) training image,
    **I**(*a:b, c:d*) denotes the rectangular region whose top-left and bottom-right
    corners are located at *(a, c)* and *(b, d)*, respectively. The period symbol
    denotes the inner product operation and **R** denotes the ideal response map.
    The solution to this equation is a patch model that generates response maps that
    are, on average, closest to the ideal response map as measured using the least-squares
    criterion. An obvious choice for the ideal response map, **R**, is a matrix with
    zeros everywhere except at the center (assuming the training image patches are
    centered at the facial feature of interest). In practice, since the images are
    hand-labeled, there will always be an annotation error. To account for this, it
    is common to describe R as a decaying function of distance from the center. A
    good choice is the 2D-Gaussian distribution, which is equivalent to assuming the
    annotation error is Gaussian distributed. A visualization of this setup is shown
    in the following figure for the left outer eye corner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: The learning objective as written previously is in a form commonly referred
    to as linear least squares. As such, it affords a closed-form solution. However,
    the degrees of freedom of this problem; that is, the number of ways the variables
    can vary to solve the problem, is equal to the number of pixels in the patch.
    Thus, the computational cost and memory requirements of solving for the optimal
    patch model can be prohibitive, even for a moderately sized patch; for example,
    a 40x40 patch model has 1,600 degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: 'An efficient alternative to solving the learning problem as a linear system
    of equations is a method called stochastic gradient descent. By visualizing the
    learning objective as an error terrain over the degrees of freedom of the patch
    model, stochastic gradient descent iteratively makes an approximate estimate of
    the gradient direction of the terrain and takes a small step in the opposite direction.
    For our problem, the approximation to gradient can be computed by considering
    only the gradient of the learning objective for a single, randomly chosen image
    from the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the `patch_model` class, this learning process is implemented in the `train`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The first highlighted code snippet in the preceding code is where the ideal
    response map is computed. Since the images are centered on the facial feature
    of interest, the response map is the same for all samples. In the second highlighted
    code snippet, the decay rate, `step`, of the step sizes is determined such that
    after `nsamples` iterations, the step size would have decayed to a value close
    to zero. The third highlighted code snippet is where the stochastic gradient direction
    is computed and used to update the patch model. There are two things to note here.
    First, the images used in training are passed to the `patch_model::convert_image`
    function, which converts the image to a single-channel image (if it is a color
    image) and applies the natural logarithm to the image pixel intensities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: A bias value of 1 is added to each pixel before applying the logarithm since
    the logarithm of zero is undefined. The reason for performing this pre-processing
    on the training images is because log-scale images are more robust against differences
    in contrast and changes in illumination conditions. The following figure shows
    images of two faces with different degrees of contrast in the facial region. The
    difference between the images is much less pronounced in the log-scale images
    than it is in the raw images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The second point to note about the update equation is the subtraction of `lambda*P`
    from the update direction. This effectively regularizes the solution from growing
    too large; a procedure that is often applied in machine-learning algorithms to
    promote generalization to unseen data. The scaling factor `lambda` is user defined
    and is usually problem dependent. However, a small value typically works well
    for learning patch models for facial feature detection.
  prefs: []
  type: TYPE_NORMAL
- en: Generative versus discriminative patch models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the ease of which discriminative patch models can be learned as described
    previously, it is worth considering whether generative patch models and their
    corresponding training regimes are simple enough to achieve similar results. The
    generative counterpart of the correlation patch model is the average patch. The
    learning objective for this model is to construct a single image patch that is
    as close as possible to all examples of the facial feature as measured via the
    least-squares criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: The solution to this problem is exactly the average of all the feature-centered
    training image patches. Thus, in a way, the solution afforded by this objective
    is far simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, a comparison is shown for the response maps obtained
    by cross-correlating the average and correlation patch models with an example
    image. The respective average and correlation patch models are also shown, where
    the range of pixel values is normalized for visualization purposes. Although the
    two patch model types exhibit some similarities, the response maps they generate
    differ substantially. While the correlation patch model generates response maps
    that are highly peaked around the feature location, the response map generated
    by the average patch model is overly smooth and does not strongly distinguish
    the feature location from those close by. Inspecting the patch models'' appearance,
    the correlation patch model is mostly gray, which corresponds to zero in the un-normalized
    pixel range, with strong positive and negative values strategically placed around
    prominent areas of the facial feature. Thus, it preserves only those components
    of the training patches, useful for discriminating it from misaligned configuration,
    which leads to highly peaked responses. In contrast, the average patch model encodes
    no knowledge of misaligned data. As a result, it is not well suited to the task
    of facial feature localization, where the task is to discriminate an aligned image
    patch from locally shifted versions of itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Accounting for global geometric transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have assumed that the training images are centered at the facial
    feature and are normalized with respect to global scale and rotation. In practice,
    the face can appear at any scale and rotation within the image during tracking.
    Thus, a mechanism must be devised to account for this discrepancy between the
    training and testing conditions. One approach is to synthetically perturb the
    training images in scale and rotation within the ranges one expects to encounter
    during deployment. However, the simplistic form of the detector as a correlation
    patch model often lacks the capacity to generate useful response maps for that
    kind of data. On the other hand, the correlation patch model does exhibit a degree
    of robustness against small perturbations in scale and rotation. Since motion
    between consecutive frames in a video sequence is relatively small, one can leverage
    the estimated global transformation of the face in the previous frame to normalize
    the current image with respect to scale and rotation. All that is needed to enable
    this procedure is to select a reference frame in which the correlation patch models
    are learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `patch_models` class stores the correlation patch models for each facial
    feature as well as the reference frame in which they are trained. It is the `patch_models`
    class, rather than the `patch_model` class, that the face tracker code interfaces
    with directly, to obtain the feature detections. The following code snippet of
    the declaration of this class highlights its primary functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reference` shape is stored as an interleaved set of (*x, y*) coordinates
    that are used to normalize the scale and rotation of the training images, and
    later, during deployment, that of the test images. In the `patch_models::train`
    function, this is done by first computing the similarity transform between the
    `reference` shape and the annotated shape for a given image using the `patch_models::calc_simil`
    function, which solves a similar problem to that in the `shape_model::procrustes`
    function, albeit for a single pair of shapes. Since the rotation and scale is
    common across all facial features, the image normalization procedure only requires
    adjusting this similarity transform to account for the centers of each feature
    in the image and the center of the normalized image patch. In `patch_models::train`,
    this is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `wsize` is the total size of the normalized training image, which is
    the sum of the patch size and the search region size. As just mentioned, the top-left
    (2x2) block of the similarity transform from the reference shape to the annotated
    shape `pt`, which corresponds to the scale and rotation component of the transformation,
    is preserved in the affine transform passed to OpenCV''s `warpAffine` function.
    The last column of the affine transform `A` is an adjustment that will render
    the i^(th) facial feature location centered in the normalized image after warping
    (that is, the normalizing translation). Finally, the `cv::warpAffine` function
    has the default setting of warping from the image to the reference frame. Since
    the similarity transform was computed for transforming the `reference` shape to
    the image-space annotations, the `pt`, the `WARP_INVERSE_MAP` flag needs to be
    set to ensure the function applies the warp in the desired direction. Exactly
    the same procedure is performed in the `patch_models::calc_peaks` function, with
    the additional step that the computed similarity transform between the reference
    and the current shape in the image-frame is re-used to un-normalize the detected
    facial features, placing them appropriately in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the first highlighted code snippet in the preceding code, both the forward
    and inverse similarity transforms are computed. The reason why the inverse transform
    is required here is so that the peaks of the response map for each feature can
    be adjusted according to the normalized locations of the current shape estimate.
    This must be performed before reapplying the similarity transform to place the
    new estimates of the facial feature locations back into the image frame using
    the
  prefs: []
  type: TYPE_NORMAL
- en: '`patch_models::apply_simil` function.'
  prefs: []
  type: TYPE_NORMAL
- en: Training and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An example program for training the patch models from the annotation data can
    be found in `train_patch_model.cpp`. With the command-line argument `argv[1]`
    containing the path to the annotation data, training begins by loading the data
    into memory and removing incomplete samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The simplest choice for the reference shape in the `patch_models` class is
    the average shape of the training set, scaled to a desired size. Assuming that
    a shape model has previously been trained for this dataset, the reference shape
    is computed by first loading the shape model stored in `argv[2]` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is followed by the computation of the scaled-centered average shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `calc_scale` function computes the scaling factor to transform the average
    shape (that is, the first column of `shape_model::V`) to one with a width of `width`.
    Once the reference shape `r` is defined, training the set of patch models can
    be done with a single function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The optimal choices for the parameters `width`, `psize`, and `ssize` are application
    dependent; however, the default values of 100, 11, and 11, respectively, give
    reasonable results in general.
  prefs: []
  type: TYPE_NORMAL
- en: Although the training process is quite simple, it can still take some time to
    complete. Depending on the number of facial features, the size of the patches,
    and the number of stochastic samples in the optimization algorithm, the training
    process can take anywhere from between a few minutes to over an hour. However,
    since the training of each patch can be performed independently of all others,
    this process can be sped
  prefs: []
  type: TYPE_NORMAL
- en: up substantially by parallelizing the training process across multiple processorcores
    or machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once training has been completed, the program in `visualize_patch_model.cpp`
    can be used to visualize the resulting patch models. As with the `visualize_shape_model.cpp`
    program, the aim here is to visually inspect the results to verify if anything
    went wrong during the training process. The program generates a composite image
    of all the patch models, `patch_model::P`, each centered at their respective feature
    location in the reference shape, `patch_models::reference`, and displaying a bounding
    rectangle around the patch whose index is currently active. The `cv::waitKey`
    function is used to get user input for selecting the activee patch index and terminating
    the program. The following image shows three examples of composite patch images
    learned for patch models with varying spatial support. Despite using the same
    training data, modifying the spatial support of the patch model appears to change
    the structure of the patch models substantially. Visually inspecting the results
    in this way can lend intuition into how to modify the parameters of the training
    process, or even the training process itself, in order to optimize results for
    a particular application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Face detection and initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The method for face tracking described thus far has assumed that the facial
    features in the image are located within a reasonable proximity to the current
    estimate. Although this assumption is reasonable during tracking, where face motion
    between frames is often quite small, we are still faced with the dilemma of how
    to initialize the model in the first frame of the sequence. An obvious choice
    for this is to use OpenCV's in-built cascade detector to find the face. However,
    the placement of the model within the detected bounding box will depend on the
    selection made for the facial features to track. In keeping with the data-driven
    paradigm we have followed so far in this chapter, a simple solution is to learn
    the geometrical relationship between the face detection's bounding box and the
    facial features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `face_detector` class implements exactly this solution. A snippet of its
    declaration that highlights its functionality is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The class has four public member variables: the path to an object of type `cv::CascadeClassifier`
    called `detector_fname`, a set of offsets from a detection bounding box to the
    location and scale of the face in the image `detector_offset`, a reference shape
    to place in the bounding box `reference`, and a face detector `detector`. The
    primary function of use to a face-tracking system is `face_detector::detect`,
    which takes an image as the input, along with standard options for the `cv::CascadeClassifier`
    class, and returns a rough estimate of the facial feature locations in the image.
    Its implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The face is detected in the image in the usual way, except that the `CV_HAAR_FIND_BIGGEST_OBJECT`
    flag is set so as to enable tracking the most prominent face in the image. The
    highlighted code is where the reference shape is placed in the image in accordance
    with the detected face''s bounding box. The `detector_offset` member variable
    consists of three components: an (x, y) offset of the center of the face from
    the center of the detection''s bounding box, and the scaling factor that resizes
    the reference shape to best fit the face in the image. All three components are
    a linear function of the bounding box''s width.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear relationship between the bounding box''s width and the `detector_offset`
    variable is learned from the annotated dataset in the `face_detector::train` function.
    The learning process is started by loading the training data into memory and assigning
    the reference shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the reference shape in the `patch_models` class, a convenient choice
    for the reference shape is the normalized average face shape in the dataset. The
    `cv::CascadeClassifier` is then applied to each image (and optionally its mirrored
    counterpart) in the dataset and the resulting detection is checked to ensure that
    enough annotated points lie within the detected bounding box (see the figure towards
    the end of this section) to prevent learning from misdetections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If more than a fraction of `frac` of the annotated points lie within the bounding
    box, the linear relationship between its width and the offset parameters for that
    image are added as a new entry in an STL `vector` class object. Here, the `face_detector::center_of_mass`
    function computes the center of mass of the annotated point set for that image
    and the `face_detector::calc_scale` function computes the scaling factor for transforming
    the reference shape to the centered annotated shape. Once all images have been
    processed, the `detector_offset` variable is set to the median over all of the
    image-specific offsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the *shape and patch* models, the simple program in `train_face_detector.cpp`
    is an example of how a `face_detector` object can be built and saved for later
    use in the tracker. It first loads the annotation data and the shape model, and
    sets the reference shape as the mean-centered average of the training data (that
    is, the identity shape of the `shape_model` class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Training and saving the face detector, then, consists of two function calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the performance of the resulting shape-placement procedure, the program
    in `visualize_face_detector.cpp` calls the `face_detector::detect` function for
    each image in the video or camera input stream and draws the results on screen.
    An example of the results using this approach is shown in the following figure.
    Although the placed shape does not match the individual in the image, its placement
    is close enough so that face tracking can proceed using the approach described
    in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Face tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem of face tracking can be posed as that of finding an efficient and
    robust way to combine the independent detections of various facial features with
    the geometrical dependencies they exhibit in order to arrive at an accurate estimate
    of facial feature locations in each image of a sequence. With this in mind, it
    is perhaps worth considering whether geometrical dependencies are at all necessary.
    In the following figure, the results of detecting the facial features with and
    without geometrical constraints are shown. These results clearly highlight the
    benefit of capturing the spatial inter-dependencies between facial features. The
    relative performance of these two approaches is typical, whereby relying strictly
    on the detections leads to overly noisy solutions. The reason for this is that
    the response maps for each facial feature cannot be expected to always peak at
    the correct location. Whether due to image noise, lighting changes, or expression
    variation, the only way to overcome the limitations of facial feature detectors
    is by leveraging the geometrical relationship they share with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A particularly simple, but surprisingly effective, way to incorporate facial
    geometry into the tracking procedure is by projecting the output of the feature
    detections onto the linear shape model's subspace. This amounts to minimizing
    the distance between the original points and their closest plausible shape that
    lies on the subspace. Thus, when the spatial noise in the feature detections is
    close to being Gaussian distributed, the projection yields the most likely solution.
    In practice, the distribution of detection errors on occasion does not follow
    a Gaussian distribution and additional mechanisms need to be introduced to account
    for this.
  prefs: []
  type: TYPE_NORMAL
- en: Face tracker implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An implementation of the face-tracking algorithm can be found in the `face_tracker`
    class (see `face_tracker.cpp` and `face_tracker.hpp`). The following code is a
    snippet of its header that highlights its primary functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The class has public member instances of the `shape_model`, `patch_models`,
    and `face_detector` classes. It uses the functionality of these three classes
    to effect tracking. The `timer` variable is an instance of the `fps_timer` class
    that keeps track of the frame rate at which the `face_tracker::track` function
    is called and is useful for analyzing the effects patch and shape model configurations
    on the computational complexity of the algorithm. The `tracking` member variable
    is a flag to indicate the current state of the tracking procedure. When this flag
    is set to `false`, as it is in the constructor and the `face_tracker::reset` function,
    the tracker enters a detection mode whereby the `face_detector::detect` function
    is applied to the next incoming image to initialize the model. When in the tracking
    mode, the initial estimate used for inferring facial feature locations in the
    next incoming image is simply their location in the previous frame. The complete
    tracking algorithm is implemented simply as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Other than bookkeeping operations, such as setting the appropriate `tracking`
    state and incrementing the tracking time, the core of the tracking algorithm is
    the multi-level fitting procedure, which is highlighted in the preceding code
    snippet. The fitting algorithm, implemented in the `face_tracker::fit` function,
    is applied multiple times with the different search window sizes stored in `face_tracker_params::ssize`,
    where the output of the previous stage is used as input to the next. In its simplest
    setting, the `face_tracker_params::ssize` function performs the facial feature
    detection around the current estimate of the shape in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'It also projects the result onto the face shape''s subspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: To account for gross outliers in the facial features' detected locations, a
    robust model's fitting procedure can be employed instead of a simple projection
    by setting the `robust` flag to `true`. However, in practice, when using a decaying
    search window size (that is, as set in `face_tracker_params::ssize`), this is
    often unnecessary as gross outliers typically remain far from its corresponding
    point in the projected shape, and will likely lie outside the search region of
    the next level of the fitting procedure. Thus, the rate at which the search region
    size is reduced acts as an incremental outlier rejection scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Training and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike the other classes detailed in this chapter, training a `face_tracker`
    object does not involve any learning process. It is implemented in `train_face_tracker.cpp`
    simply as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Here `arg[1]` to `argv[4]` contain the paths to the `shape_model`, `patch_model`,
    `face_detector`, and `face_tracker` objects, respectively. The visualization for
    the face tracker in `visualize_face_tracker.cpp` is equally simple. Obtaining
    its input image stream either from a camera or video file, through the `cv::VideoCapture`
    class, the program simply loops until the end of the stream or until the user
    presses the Q key, tracking each frame as it comes in. The user also has the option
    of resetting the tracker by pressing the D key at any time.
  prefs: []
  type: TYPE_NORMAL
- en: Generic versus person-specific models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of variables in the training and tracking process that can
    be tweaked to optimize the performance for a given application. However, one of
    the primary determinants of tracking quality is the range of shape and appearance
    variability the tracker has to model. As a case in point, consider the generic
    versus person-specific case. A generic model is trained using annotated data from
    multiple identities, expressions, lighting conditions, and other sources of variability.
    In contrast, person-specific models are trained specifically for a single individual.
    Thus, the amount of variability it needs to account for is far smaller. As a result,
    person-specific tracking is often more accurate than its generic counter part
    by a large magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration of this is shown in the following image. Here the generic model
    was trained using the MUCT dataset. The person-specific model was learned from
    data generated using the annotation tool described earlier in this chapter. The
    results clearly show a substantially better tracking offered by the person-specific
    model, capable of capturing complex expressions and head-pose changes, whereas
    the generic model appears to struggle even for some of the simpler expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It should be noted that the method for face tracking described in this chapter
    is a bare-bones approach that serves to highlight the various components utilized
    in most non-rigid face-tracking algorithms. The numerous approaches to remedy
  prefs: []
  type: TYPE_NORMAL
- en: some of the drawbacks of this method are beyond the scope of this book and
  prefs: []
  type: TYPE_NORMAL
- en: require specialized mathematical tools that are not yet supported by OpenCV's
    functionality. The relatively few commercial-grade face-tracking software
  prefs: []
  type: TYPE_NORMAL
- en: packages available are testament to the difficulty of this problem in the general
  prefs: []
  type: TYPE_NORMAL
- en: setting. Nonetheless, the simple approach described in this chapter can work
    remarkably well in constrained settings.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have built a simple face tracker that can work reasonably
    in constrained settings using only modest mathematical tools and OpenCV''s substantial
    functionality for basic image processing and linear algebraic operations. Improvements
    to this simple tracker can be achieved by employing more sophisticated techniques
    in each of the three components of the tracker: the shape model, the feature detectors,
    and the fitting algorithm. The modular design of the tracker described in this
    section should allow these three components to be modified without substantial
    disruptions to the functionality of the others.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Procrustes Problems, Gower, John C. and Dijksterhuis, Garmt B, Oxford University
    Press, 2004*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
