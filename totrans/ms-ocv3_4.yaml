- en: Non-Rigid Face Tracking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非刚性面部跟踪
- en: Non-rigid face tracking, which is the estimation of a quasi-dense set of facial
    features in each frame of a video stream, is a difficult problem for which modern
    approaches borrow ideas from a number of related fields, including Computer Vision,
    computational geometry, machine learning, and image processing. Non-rigidity here
    refers to the fact that relative distances between facial features vary between
    facial expression and across the population, and is distinct from face detection
    and tracking, which aims only to find the location of the face in each frame,
    rather than the configuration of facial features. Non-rigid face tracking is a
    popular research topic that has been pursued for over two decades, but it is only
    recently that various approaches have become robust enough, and processors fast
    enough, which makes the building of commercial applications possible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 非刚性面部跟踪，即在每个视频流帧中估计一个准密集的面部特征集，是一个难题，现代方法借鉴了多个相关领域的思想，包括计算机视觉、计算几何、机器学习和图像处理。这里的非刚性指的是面部特征之间的相对距离在面部表情和人群之间是变化的，并且与面部检测和跟踪不同，后者旨在仅找到每帧中面部位置，而不是面部特征配置。非刚性面部跟踪是一个热门的研究课题，已经持续了二十多年，但直到最近，各种方法才足够稳健，处理器足够快，使得商业应用的开发成为可能。
- en: Although commercial-grade face tracking can be highly sophisticated and pose
    a challenge even for experienced Computer Vision scientists, in this chapter we
    will see that a face tracker that performs reasonably well under constrained settings
    can be devised using modest mathematical tools and OpenCV's substantial functionality
    in linear algebra, image processing, and visualization. This is particularly the
    case when the person to be tracked is known ahead of time, and training data in
    the form of images and landmark annotations are available. The techniques described
    henceforth will act as a useful starting point and a guide for further pursuits
    towards a more elaborate face-tracking system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管商业级面部跟踪可以非常复杂，甚至对经验丰富的计算机视觉科学家来说也是一个挑战，但在本章中，我们将看到，在受限设置下表现合理的面部跟踪器可以使用适度的数学工具和OpenCV在线性代数、图像处理和可视化方面的强大功能来设计。这在需要跟踪的人事先已知，并且有图像和地标注释形式的训练数据可用时尤其如此。以下描述的技术将作为一个有用的起点和指南，以进一步追求更复杂的人脸跟踪系统。
- en: 'An outline of this chapter is as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的概述如下：
- en: '**Overview**: This section covers a brief history of face tracking.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：本节涵盖面部跟踪的简要历史。'
- en: '**Utilities**: This section outlines the common structures and conventions
    used in this chapter. It includes object-oriented design, data storage and representation,
    and a tool for data collection and annotation.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：本节概述了本章中使用的常见结构和约定。它包括面向对象设计、数据存储和表示，以及用于数据收集和注释的工具。'
- en: '**Geometrical constraints**: This section describes how facial geometry and
    its variations are learned from the training data and utilized during tracking
    to constrain the solution. This includes modeling the face as a linear shape model
    and how global transformations can be integrated into its representation.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**几何约束**：本节描述了如何从训练数据中学习面部几何及其变化，并在跟踪过程中利用这些变化来约束解决方案。这包括将面部建模为线性形状模型以及如何将全局变换集成到其表示中。'
- en: '**Facial feature detectors**: This section describes how to learn the appearance
    of facial features in order to detect them in an image where the face is to be
    tracked.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面部特征检测器**：本节描述了如何学习面部特征的外观以便在需要跟踪面部图像中检测它们。'
- en: '**Face detection and initialization**: This section describes how to use face
    detection to initialize the tracking process.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面部检测和初始化**：本节描述了如何使用面部检测来初始化跟踪过程。'
- en: '**Face tracking**: This section combines all components described previously
    into a tracking system through the process of image alignment. Discussion on the
    settings in which the system can be expected to work best.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面部跟踪**：本节将之前描述的所有组件通过图像配准的过程组合成一个跟踪系统。讨论系统在哪些设置下可以预期表现最佳。'
- en: 'The following block diagram illustrates the relationships between the various
    components of the system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方块图说明了系统各个组件之间的关系：
- en: '![](img/image_05_001.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_05_001.jpg)'
- en: 'Note that all methods employed in this chapter follow a data-driven paradigm
    whereby all models used are learned from data rather than designed by hand in
    a rule-based setting. As such, each component of the system will involve two components:
    training and testing. Training builds the models from data and testing employs
    these models on new unseen data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章中采用的所有方法都遵循数据驱动范式，即所有使用的模型都是通过数据学习得到的，而不是在基于规则的设置中手工设计。因此，系统的每个组件都将涉及两个部分：训练和测试。训练从数据中构建模型，测试则将这些模型应用于新的未见数据。
- en: Overview
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Non-rigid face tracking was first popularized in the early to mid-1990s with
    the advent of **Active Shape Models** (**ASM**) by Cootes and Taylor. Since then,
    a tremendous amount of research has been dedicated to solving the difficult problem
    of generic face tracking with many improvements over the original method that
    ASM proposed. The first milestone was the extension of ASM to **Active Appearance
    Models** (**AAM**) in 2001, also by Cootes and Taylor. This approach was later
    formalized though the principled treatment of image warps by Baker and colleges
    in the mid-2000s. Another strand of work along these lines was the **3D morphable
    model** (**3DMM**) by Blanz and Vetter, which like AAM, not only modeled image
    textures as opposed to profiles along object boundaries as in ASM, but took it
    one step further by representing the models with a highly dense 3D data learned
    from laser scans of faces. From the mid- to late 2000s, the focus of research
    on face tracking shifted away from how the face was parameterized to how the objective
    of the tracking algorithm was posed and optimized. Various techniques from the
    machine-learning community were applied with various degrees of success. Since
    the turn of the century, the focus has shifted once again, this time towards joint
    parameter and objective design strategies that guarantee global solutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 非刚性面部追踪首次在20世纪90年代初至中期流行起来，当时Cootes和Taylor推出了**主动形状模型**（**ASM**）。从那时起，大量研究致力于解决通用面部追踪的难题，并对ASM最初提出的方法进行了许多改进。第一个里程碑是在2001年将ASM扩展到**主动外观模型**（**AAM**），这也是由Cootes和Taylor完成的。这种方法后来在2005年中期通过Baker及其同事对图像扭曲的原理性处理而得到形式化。沿着这些方向的另一项工作是Blanz和Vetter的**3D可变形模型**（**3DMM**），它类似于AAM，不仅将图像纹理建模，而不是像ASM那样沿物体边界建模轮廓，而且更进一步，通过从面部激光扫描中学习的高度密集的3D数据来表示模型。从2000年代中期到晚期，面部追踪的研究重点从如何参数化面部转向了如何提出和优化跟踪算法的目标。机器学习社区的各种技术被应用于此，并取得了不同程度的成功。进入21世纪后，研究重点再次转移，这次转向了联合参数和目标设计策略，以确保全局解。
- en: Despite the continued intense research into face tracking, there have been relatively
    few commercial applications that use it. There has also been a lag in uptake by
    hobbyists and enthusiasts, despite there being a number of freely available source
    code packages for a number of common approaches. Nonetheless, in the past 2 years
    there has been a renewed interest in the public domain for the potential use of
    face tracking and commercial-grade products are beginning to emerge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对面部追踪的研究持续深入，但使用该技术的商业应用相对较少。尽管有大量免费开源代码包可用于多种常见方法，但业余爱好者和爱好者对该技术的采用也相对滞后。然而，在过去的两年里，公众对面部追踪的潜在用途重新产生了兴趣，并且商业级产品开始涌现。
- en: Utilities
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: Before diving into the intricacies of face tracking, a number of book-keeping
    tasks and conventions common to all face-tracking methods must first be introduced.
    The rest of this section will deal with these issues. An interested reader may
    want to skip this section at the first reading and go straight to the section
    on geometrical constraints.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究面部追踪的复杂性之前，必须首先介绍一些常见的账务任务和约定，这些任务和约定适用于所有面部追踪方法。本节的其余部分将处理这些问题。感兴趣的读者可能希望在第一次阅读时跳过本节，直接跳到关于几何约束的部分。
- en: Object-oriented design
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向对象设计
- en: 'As with face detection and recognition, programmatically, face tracking consists
    of two components: data and algorithms. The algorithms typically perform some
    kind of operation on the incoming (that is, online) data by referencing prestored
    (that is, offline) data as a guide. As such, an object-oriented design that couples
    algorithms with the data they rely on is a convenient design choice.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与面部检测和识别一样，从程序的角度来看，面部追踪由两个组件组成：数据和算法。算法通常通过参考预先存储的数据（即离线数据）作为指南，对传入的（即在线）数据进行某种操作。因此，将算法与它们依赖的数据相结合的面向对象设计是一种方便的设计选择。
- en: 'In OpenCV v2.x, a convenient XML/YAML file storage class was introduced that
    greatly simplifies the task of organizing offline data for use in the algorithms.
    To leverage this feature, all classes described in this chapter will implement
    read-and write-serialization functions. An example of this is shown as follows
    for an imaginary class `foo`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV v2.x版本中，引入了一个方便的XML/YAML文件存储类，它极大地简化了为算法组织离线数据的工作。为了利用这一功能，本章中描述的所有类都将实现读写序列化函数。以下是一个虚构的类`foo`的示例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `Mat` is OpenCV's matrix class and `type_b` is a (imaginary) user-defined
    class that also has the serialization functionality defined. The I/O functions
    `read` and `write` implement the serialization. The `FileStorage` class supports
    two types of data structures that can be serialized. For simplicity, in this chapter
    all classes will only utilize mappings, where each stored variable creates a `FileNode`
    object of type `FileNode::MAP`. This requires a unique key to be assigned to each
    element. Although the choice for this key is arbitrary, we will use the variable
    name as the label for consistency reasons. As illustrated in the preceding code
    snippet, the `read` and `write` functions take on a particularly simple form,
    whereby the streaming operators (`<<` and `>>`) are used to insert and extract
    data to the `FileStorage` object. Most OpenCV classes have implementations of
    the `read` and `write` functions, allowing the storage of the data that they contain
    to be done with ease.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Mat`是OpenCV的矩阵类，`type_b`是一个（虚构的）用户定义的类，它也定义了序列化功能。I/O函数`read`和`write`实现了序列化。`FileStorage`类支持两种可以序列化的数据结构。为了简单起见，在本章中，所有类将仅利用映射，其中每个存储的变量创建一个类型为`FileNode::MAP`的`FileNode`对象。这要求为每个元素分配一个唯一的键。尽管对键的选择是任意的，但为了保持一致性，我们将使用变量名作为标签。如前述代码片段所示，`read`和`write`函数采用了特别简单的形式，其中使用了流操作符（`<<`和`>>`）将数据插入和提取到`FileStorage`对象中。大多数OpenCV类都有`read`和`write`函数的实现，这使得存储它们包含的数据变得容易。
- en: 'In addition to defining the serialization functions, one must also define two
    additional functions for the serialization in the `FileStorage` class to work,
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定义序列化函数外，还必须定义两个额外的函数，以便在`FileStorage`类中实现序列化，如下所示：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As the functionality of these two functions remains the same for all classes
    we describe in this section, they are templated and defined in the `ft.hpp` header
    file found in the source code pertaining to this chapter. Finally, to easily save
    and load user-defined classes that utilize the serialization functionality, templated
    functions for these are also implemented in the header file as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些两个函数在我们本节描述的所有类中功能相同，因此它们被模板化并在源代码中与本章相关的`ft.hpp`头文件中定义。最后，为了轻松保存和加载使用序列化功能的自定义类，头文件中也实现了这些模板化函数，如下所示：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the label associated with the object is always the same (that is,
    `ft object`). With these functions defined, saving and loading object data is
    a painless process. This is shown with the help of the following example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与对象关联的标签始终相同（即，`ft object`）。定义了这些函数后，保存和加载对象数据就变得轻松愉快了。以下示例展示了这一点：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the `.xml` extension results in an XML-formatted data file. For any
    other extension, it defaults to the (more human-readable) YAML format.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`.xml`扩展名会导致生成XML格式的数据文件。对于任何其他扩展名，它默认为（更易于阅读的）YAML格式。
- en: Data collection - image and video annotation
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集 - 图像和视频标注
- en: Modern face-tracking techniques are almost entirely data driven, that is, the
    algorithms used to detect the locations of facial features in the image rely on
    models of the appearance of the facial features and the geometrical dependencies
    between their relative locations from a set of examples. The larger the set of
    examples, the more robust the algorithms behave, as they become more aware of
    the gamut of variability that faces can exhibit. Thus, the first step in building
    a face-tracking algorithm is to create an image/video annotation tool, where the
    user can specify the locations of the desired facial features in each example
    image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人脸追踪技术几乎完全是数据驱动的，也就是说，用于检测图像中面部特征位置的算法依赖于面部特征的外观模型以及它们相对位置之间的几何依赖关系。示例集越大，算法的行为就越稳健，因为它们对面部可以展现的变异性范围了解得越多。因此，构建人脸追踪算法的第一步是创建一个图像/视频标注工具，用户可以在每个示例图像中指定所需面部特征的位置。
- en: Training data types
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据类型
- en: 'The data for training face tracking algorithms generally consists of four components:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人脸跟踪算法的数据通常包括四个组成部分：
- en: '**Images**: This component is a collection of images (still images or video
    frames) that contain an entire face. For best results, this collection should
    be specialized to the types of conditions (that is, identity, lighting, distance
    from camera, capturing device, among others) in which the tracker is later deployed.
    It is also crucial that the faces in the collection exhibit the range of head
    poses and facial expressions that the intended application expects.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像**：该组成部分是一组包含整个面部的图像（静态图像或视频帧）。为了获得最佳效果，该集合应专门针对跟踪器随后部署的条件类型（即身份、照明、相机距离、捕获设备等）进行定制。此外，集合中的面部必须展现出预期应用所期望的头姿和面部表情的范围。'
- en: '**Annotations**: This component has ordered hand-labeled locations in each
    image that correspond to every facial feature to be tracked. More facial features
    often lead to a more robust tracker as the tracking algorithm can use their measurements
    to reinforce each other. The computational cost of common tracking algorithms
    typically scales linearly with the number of facial features.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注**：该组成部分在每个图像中都有有序的手动标注位置，对应于要跟踪的每个面部特征。更多的面部特征通常会导致跟踪器更加鲁棒，因为跟踪算法可以使用它们的测量值相互强化。常见跟踪算法的计算成本通常与面部特征的数量成线性关系。'
- en: '**Symmetry indices**: This component has an index for each facial feature point
    that defines its bilaterally symmetrical feature. This can be used to mirror the
    training images, effectively doubling the training set size and symmetrizing the
    data along the *y* axis.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对称指数**：该组成部分为每个面部特征点提供一个索引，用于定义其双边对称特征。这可以用来镜像训练图像，有效地将训练集大小加倍，并沿 *y* 轴对称化数据。'
- en: '**Connectivity indices**: This component has a set of index pairs of the annotations
    that define the semantic interpretation of the facial features. These connections
    are useful for visualizing the tracking results.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接指数**：该组成部分有一组标注的索引对，用于定义面部特征的语义解释。这些连接对于可视化跟踪结果非常有用。'
- en: 'A visualization of these four components is shown in the following image, where
    from left to right we have the raw image, facial feature annotations, color-coded
    bilateral symmetry points, mirrored image, and annotations and facial feature
    connectivity:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像展示了这四个组成部分的可视化，从左到右分别是原始图像、面部特征标注、颜色编码的双边对称点、镜像图像以及标注和面部特征连接：
- en: '![](img/image_05_002.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_002.jpg)'
- en: 'To conveniently manage such data, a class that implements storage and access
    functionality is a useful component. The `CvMLData` class in the `ml` module of
    OpenCV has the functionality for handling general data often used in machine-learning
    problems. However, it lacks the functionality required from the face-tracking
    data. As such, in this chapter, we will use the `ft_data` class, declared in the
    `ft_data.hpp` header file, which is designed specifically with the peculiarity
    of face-tracking data in mind. All data elements are defined as public members
    of the class, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便管理此类数据，实现存储和访问功能的类是一个有用的组成部分。OpenCV的`ml`模块中的`CvMLData`类具有处理机器学习问题中常用的一般数据的函数。然而，它缺少处理人脸跟踪数据所需的函数。因此，在本章中，我们将使用在`ft_data.hpp`头文件中声明的`ft_data`类，该类专门考虑了人脸跟踪数据的特殊性。所有数据元素都被定义为类的公共成员，如下所示：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `Vec2i` and `Point2f` types are OpenCV classes for vectors of two integers
    and 2D floating-point coordinates respectively. The `symmetry` vector has as many
    components as there are feature points on the face (as defined by the user). Each
    of the `connections` define a zero-based index pair of connected facial features.
    As the training set can potentially be very large, rather than storing the images
    directly, the class stores the filenames of each image in the `imnames` member
    variable (note that this requires the images to be located in the same relative
    path for the filenames to remain valid). Finally, for each training image, a collection
    of facial feature locations are stored as vectors of floating-point coordinates
    in the `points` member variable.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Vec2i` 和 `Point2f` 类型分别是 OpenCV 的两个整数字符串和二维浮点坐标类。`symmetry` 向量包含与面部特征点（由用户定义）数量相同数量的组件。每个
    `connections` 定义了一对连接面部特征的零索引。由于训练集可能非常大，而不是直接存储图像，该类将每个图像的文件名存储在 `imnames` 成员变量中（注意，这要求图像位于相同的相对路径，以便文件名保持有效）。最后，对于每个训练图像，将面部特征位置作为浮点坐标的向量存储在
    `points` 成员变量中。'
- en: 'The `ft_data` class implements a number of convenience methods for accessing
    the data. To access an image in the dataset, the `get_image` function loads the
    image at the specified index, `idx`, and optionally mirrors it around the y axis
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`ft_data` 类实现了一系列方便的方法来访问数据。要访问数据集中的图像，`get_image` 函数会加载指定索引 `idx` 的图像，并可选择沿
    y 轴进行镜像，如下所示：'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The (`0`,`1`) flag passed to OpenCV's `imread` function specifies whether the
    image is loaded as a three-channel color image or as a single-channel grayscale
    image. The flag passed to OpenCV's `flip` function specifies the mirroring around
    the *y* axis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 OpenCV 的 `imread` 函数的 (`0`,`1`) 标志指定图像是作为三通道彩色图像还是单通道灰度图像加载。传递给 OpenCV 的
    `flip` 函数的标志指定沿 *y* 轴的镜像。
- en: 'To access a point set corresponding to an image at a particular index, the
    `get_points` function returns a vector of floating-point coordinates with the
    option of mirroring their indices as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问特定索引处的图像对应的点集，`get_points` 函数返回一个浮点坐标向量，可以选择如下镜像它们的索引：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that when the mirroring flag is specified, this function calls the `get_image`
    function. This is required to determine the width of the image in order to correctly
    mirror the facial feature coordinates. A more efficient method could be devised
    by simply passing the image width as a variable. Finally, the utility of the `symmetry`
    member variable is illustrated in this function. The mirrored feature location
    of a particular index is simply the feature location at the index specified in
    the `symmetry` variable with its *x* coordinate flipped and biased.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当指定镜像标志时，此函数会调用 `get_image` 函数。这是为了确定图像的宽度，以便正确镜像面部特征坐标。可以设计一个更有效的方法，只需简单地将图像宽度作为变量传递。最后，此函数展示了
    `symmetry` 成员变量的实用性。特定索引的镜像特征位置只是 `symmetry` 变量中指定的索引处的特征位置，其 *x* 坐标翻转并偏移。
- en: 'Both the `get_image` and `get_points` functions return empty structures if
    the specified index is outside the one that exists for the dataset. It is also
    possible that not all images in the collection are annotated. Face-tracking algorithms
    can be designed to handle missing data; however, these implementations are often
    quite involved and are outside the scope of this chapter. The `ft_data` class
    implements a function for removing samples from its collection that do not have
    corresponding annotations, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定的索引超出了数据集存在的索引范围，`get_image` 和 `get_points` 函数都会返回空结构。也可能不是集合中的所有图像都有注释。可以设计处理缺失数据的面部跟踪算法；然而，这些实现通常相当复杂，并且超出了本章的范围。`ft_data`
    类实现了一个函数，用于从其集合中删除没有对应注释的样本，如下所示：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The sample instance that has the most number of annotations is assumed to be
    the canonical sample. All data instances that have a point set with less than
    that number of points are removed from the collection using the vector's `erase`
    function. Also notice that points with (*x, y*) coordinates less than 1 are considered
    missing in their corresponding image (possibly due to occlusion, poor visibility,
    or ambiguity).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设具有最多注释的样本实例是规范样本。使用向量的 `erase` 函数从集合中删除具有少于该数量点的所有数据实例。还请注意，具有 (*x, y*) 坐标小于
    1 的点在相应的图像中被视为缺失（可能是由于遮挡、可见性差或模糊）。
- en: 'The `ft_data` class implements the serialization functions `read` and `write`,
    and can thus be stored and loaded easily. For example, saving a dataset can be
    done as simply as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`ft_data` 类实现了 `read` 和 `write` 序列化函数，因此可以轻松存储和加载。例如，保存数据集可以像以下这样简单完成：'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For visualizing the dataset, `ft_data` implements a number of drawing functions.
    Their use is illustrated in the `visualize_annotations.cpp` file. This simple
    program loads annotation data stored in the file specified in the command-line,
    removes the incomplete samples, and displays the training images with their corresponding
    annotations, symmetry, and connections superimposed. A few notable features of
    OpenCV's `highgui` module are demonstrated here. Although quite rudimentary and
    not well suited for complex user interfaces, the functionality in OpenCV's `highgui`
    module is extremely useful for loading and visualizing data and algorithmic outputs
    in Computer Vision applications. This is perhaps one of OpenCV's distinguishing
    qualities compared to other Computer Vision libraries.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化数据集，`ft_data` 实现了多个绘图函数。它们的使用在 `visualize_annotations.cpp` 文件中有说明。这个简单的程序加载存储在命令行指定的文件中的注释数据，移除不完整的样本，并显示带有相应注释、对称性和连接的训练图像。这里展示了
    OpenCV 的 `highgui` 模块的一些显著特性。尽管非常基础且不适合复杂的用户界面，但 OpenCV 的 `highgui` 模块在计算机视觉应用中加载和可视化数据和算法输出方面非常实用。这可能是
    OpenCV 与其他计算机视觉库相比的一个区别特征。
- en: Annotation tool
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注释工具
- en: 'To aid in generating annotations for use with the code in this chapter, a rudimentary
    annotation tool can be found in the `annotate.cpp` file. The tool takes as input
    a video stream, either from a file or from the camera. The procedure for using
    the tool is listed in the following four steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助生成本章代码中使用的注释，可以在 `annotate.cpp` 文件中找到一个基本的注释工具。该工具接受视频流作为输入，无论是来自文件还是来自摄像头。使用该工具的步骤如下：
- en: '**Capture images**: In this first step, the image stream is displayed on the
    screen and the user chooses the images to annotate by pressing the `S` key. The
    best set of features to annotate are those that maximally span the range of facial
    behaviors that the face-tracking system will be required to track.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**捕获图像**：在这个第一步中，图像流显示在屏幕上，用户通过按 `S` 键选择要注释的图像。最适合注释的特征是那些最大限度地覆盖面部行为范围的特征，这些特征是面部追踪系统需要追踪的。'
- en: '**Annotate first image**: In this second step, the user is presented with the
    first image selected in the previous stage. The user then proceeds to click on
    the image at the locations pertaining to the facial features that require tracking.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注释第一张图像**：在这个第二步中，用户被展示在上一阶段选中的第一张图像。然后用户继续点击图像上与需要追踪的面部特征相关的位置。'
- en: '**Annotate connectivity**: In this third step, to better visualize a shape,
    the connectivity structure of points needs to be defined. Here, the user is presented
    with the same image as in the previous stage, where the task now is to click a
    set of point pairs, one after the other, to build the connectivity structure for
    the face model.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注释连接性**：在这个第三步中，为了更好地可视化形状，需要定义点的连接结构。在这里，用户被展示与之前阶段相同的图像，现在的任务是依次点击一系列点对，以构建面部模型的连接结构。'
- en: '**Annotate symmetry**: In this step, still with the same image, the user selects
    pairs of points that exhibit bilateral symmetry.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注释对称性**：在这个步骤中，仍然使用相同的图像，用户选择表现出双边对称性的点对。'
- en: '**Annotate remaining images**: In this final step, the procedure here is similar
    to that of *step 2*, except that the user can browse through the set of images
    and annotate them asynchronously.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注释剩余图像**：在这个最后一步中，这里的程序与 *步骤 2* 类似，但用户可以浏览图像集并对它们进行异步注释。'
- en: An interested reader may want to improve on this tool by improving its usability
    or may even integrate an incremental learning procedure, whereby a tracking model
    is updated after each additional image is annotated and is subsequently used to
    initialize the points to reduce the burden of annotation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对此感兴趣的读者可能希望通过改进其可用性或甚至集成增量学习过程来改进此工具，其中在注释每张额外的图像后更新追踪模型，并随后使用它来初始化点以减少注释负担。
- en: Although some publicly available datasets are available for use with the code
    developed in this chapter (see for example, the description in the following section),
    the annotation tool can be used to build person-specific face-tracking models,
    which often perform far better than their generic, person-independent, counterparts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些公开可用的数据集可以与本章开发的代码一起使用（例如，参见下节中的描述），但标注工具可以用来构建特定于个人的面部跟踪模型，这些模型通常比它们的通用、独立于个人的对应模型表现要好得多。
- en: Pre-annotated data (the MUCT dataset)
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预标注数据（MUCT数据集）
- en: One of the hindering factors of developing face-tracking systems is the tedious
    and error-prone process of manually annotating a large collection of images, each
    with a large number of points. To ease this process for the purpose of following
    the work in this chapter, the publicly available MUCT dataset can be downloaded
    from [h t t p ://w w w /m i l b o . o r g /m u c t](http://www/milbo.org/muct)
    .
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人脸跟踪系统的一个阻碍因素是手动标注大量图像的繁琐且易出错的过程，每张图像都有大量标注点。为了简化这一过程，以便跟随本章的工作，可以从公开可用的MUCT数据集下载 [h
    t t p ://w w w /m i l b o . o r g /m u c t](http://www.milbo.org/muct) 。
- en: The dataset consists of 3,755 face images annotated with 76 point landmarks.
    The subjects in the dataset vary in age and ethnicity and are captured under a
    number of different lighting conditions and head poses.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含3,755张带有76个关键点的面部图像。数据集中的受试者年龄和种族各异，并在多种不同的光照条件和头部姿势下进行拍摄。
- en: 'To use the MUCT dataset with the code in this chapter, perform the following
    steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中的代码与MUCT数据集，请执行以下步骤：
- en: '**Download the image set**: In this step, all the images in the dataset can
    be obtained by downloading the files `muct-a-jpg-v1.tar.gz` to `muct-e-jpg-v1.tar.gz`
    and uncompressing them. This will generate a new folder in which all the images
    will be stored.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载图像集**：在这一步中，可以通过下载文件 `muct-a-jpg-v1.tar.gz` 到 `muct-e-jpg-v1.tar.gz` 并解压它们来获取数据集中的所有图像。这将生成一个新文件夹，其中将存储所有图像。'
- en: '**Download the annotations**: In this step, download the file containing the
    annotations `muct-landmarks-v1.tar.gz`. Save and uncompress this file in the same
    folder as the one in which the images were downloaded.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载标注**：在这一步中，下载包含标注的文件 `muct-landmarks-v1.tar.gz`。将此文件保存并解压到与下载图像相同的文件夹中。'
- en: '**Define connections and symmetry using the annotation tool**: In this step,
    from the command-line, issue the command `./annotate -m $mdir -d $odir`, where
    `$mdir` denotes the folder where the MUCT dataset was saved and `$odir` denotes
    the folder to which the `annotations.yaml` file, containing the data stored as
    an `ft_data` object, will be written.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用标注工具定义连接和对称性**：在这一步中，从命令行发出命令 `./annotate -m $mdir -d $odir`，其中 `$mdir`
    表示保存MUCT数据集的文件夹，而 `$odir` 表示将包含作为 `ft_data` 对象存储的数据的 `annotations.yaml` 文件写入的文件夹。'
- en: Usage of the MUCT dataset is encouraged to get a quick introduction to the functionality
    of the face-tracking code described in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励使用MUCT数据集，以快速了解本章所述面部跟踪代码的功能。
- en: Geometrical constraints
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 几何约束
- en: In face tracking, geometry refers to the spatial configuration of a predefined
    set of points that correspond to physically consistent locations on the human
    face (such as eye corners, nose tips, and eyebrow edges). A particular choice
    of these points is application dependent, with some applications requiring a dense
    set of over 100 points and others requiring only a sparser selection. However,
    the robustness of face-tracking algorithms generally improves with an increased
    number of points, as their separate measurements can reinforce each other through
    their relative spatial dependencies. For example, the location of an eye corner
    is a good indication of where to expect the nose to be located. However, there
    are limits to improvements in robustness gained by increasing the number of points,
    where performance typically plateaus after around 100 points. Furthermore, increasing
    the point set used to describe a face carries with it a linear increase in computational
    complexity. Thus, applications with strict constraints on computational load may
    fare better with fewer points.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在人脸追踪中，几何指的是一组预定义点的空间配置，这些点对应于人类面部上的物理一致位置（例如眼角、鼻尖和眉毛边缘）。这些点的特定选择取决于应用，有些应用需要超过100个点的密集集合，而有些则只需要较稀疏的选择。然而，随着点数的增加，人脸追踪算法的鲁棒性通常会提高，因为它们各自的测量值可以通过它们之间的相对空间依赖性相互加强。例如，眼角的位置是预测鼻尖位置的良方。然而，通过增加点数来提高鲁棒性的改进是有极限的，性能通常在约100个点后趋于平稳。此外，用于描述面部的点集的增加会导致计算复杂性的线性增加。因此，对计算负载有严格限制的应用可能更适合使用较少的点。
- en: It is also the case that faster tracking often leads to more accurate tracking
    in the online setting. This is because, when frames are dropped, the perceived
    motion between frames increases, and the optimization algorithm used to find the
    configuration of the face in each frame has to search a larger space of possible
    configurations of feature points; a process that often fails when displacement
    between frames becomes too large. In summary, although there are general guidelines
    on how to best design the selection of facial feature points, to get an optimal
    performance, this selection should be specialized to the application's domain.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 也有这样的情况，更快的追踪往往会导致在线设置中更精确的追踪。这是因为，当帧被丢弃时，帧之间的感知运动增加，用于在每一帧中找到面部配置的优化算法必须搜索一个更大的特征点可能配置空间；当帧之间的位移变得过大时，这个过程通常会失败。总之，尽管有关于如何最佳设计面部特征点选择的通用指南，但要获得最佳性能，这种选择应该专门针对应用的领域进行定制。
- en: 'Facial geometry is often parameterized as a composition of two elements: a
    **global transformation** (rigid) and a **local deformation** (non-rigid). The
    global transformation accounts for the overall placement of the face in the image,
    which is often allowed to vary without constraint (that is, the face can appear
    anywhere in the image). This includes the (*x, y*) location of the face in the
    image, the in-plane head rotation, and the size of the face in the image. Local
    deformations, on the other hand, account for differences between facial shapes
    across identities and between expressions. In contrast to the global transformation,
    these local deformations are often far more constrained largely due to the highly
    structured configuration of facial features. Global transformations are generic
    functions of 2D coordinates, applicable to any type of object, whereas local deformations
    are object specific and must be learned from a training dataset.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 面部几何通常被参数化为两个元素的组合：一个**全局变换**（刚体）和一个**局部变形**（非刚体）。全局变换负责解释面部在图像中的整体位置，这通常允许其无约束地变化（也就是说，面部可以出现在图像的任何位置）。这包括面部在图像中的(*x,
    y*)位置、平面内头部旋转以及面部在图像中的大小。另一方面，局部变形负责解释不同身份之间的面部形状差异以及表情之间的差异。与全局变换相比，这些局部变形通常受到更多的约束，这主要归因于面部特征的复杂结构配置。全局变换是二维坐标的通用函数，适用于任何类型的对象，而局部变形是特定于对象的，必须从训练数据集中学习。
- en: 'In this section, we will describe the construction of a geometrical model of
    a facial structure, hereby referred to as the shape model. Depending on the application,
    it can capture expression variations of a single individual, differences between
    facial shapes across a population, or a combination of both. This model is implemented
    in the `shape_model` class which can be found in the `shape_model.hpp` and `shape_model.cpp`
    files. The following code snippet is a part of the header of the `shape_model`
    class that highlights its primary functionality:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述人脸结构的几何模型的构建，这里称为形状模型。根据应用的不同，它可以捕捉单个个体的表情变化、人群之间面部形状的差异，或者两者的组合。该模型在
    `shape_model` 类中实现，可以在 `shape_model.hpp` 和 `shape_model.cpp` 文件中找到。以下代码片段是 `shape_model`
    类头文件的一部分，突出了其主要功能：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The model that represents variations in face shapes is encoded in the subspace
    matrix `V` and variance vector `e`. The parameter vector `p` stores the encoding
    of a shape with respect to the model. The connectivity matrix `C` is also stored
    in this class as it pertains only to visualizing instances of the face's shape.
    The three functions of primary interest in this class are `calc_params`, `calc_shape`,
    and `train`. The `calc_params` function projects a set of points onto the space
    of plausible face shapes. It optionally provides separate confidence weights for
    each of the points to be projected. The `calc_shape` function generates a set
    of points by decoding the parameter vector `p` using the face model (encoded by
    `V` and `e`). The `train` function learns the encoding model from a dataset of
    face shapes, each of which consists of the same number of points. The parameters
    `frac` and `kmax` are parameters of the training procedure that can be specialized
    for the data at hand.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表示人脸形状变化的模型编码在子空间矩阵 `V` 和方差向量 `e` 中。参数向量 `p` 存储了相对于模型的形状编码。连接矩阵 `C` 也存储在这个类中，因为它仅与可视化人脸形状的实例相关。这个类中主要关注的三个函数是
    `calc_params`、`calc_shape` 和 `train`。`calc_params` 函数将一组点投影到可能的人脸形状空间。它可以选择为要投影的每个点提供单独的置信权重。`calc_shape`
    函数通过使用人脸模型（由 `V` 和 `e` 编码）解码参数向量 `p` 来生成一组点。`train` 函数从人脸形状的数据集中学习编码模型，每个数据集都由相同数量的点组成。`frac`
    和 `kmax` 是训练过程的参数，可以根据手头的数据进行专门化。
- en: The functionality of this class will be elaborated in the sections that follow,
    where we begin by describing **Procrustes analysis**, a method for rigidly registering
    a point set, followed by the linear model used to represent local deformations.
    The programs in the `train_shape_model.cpp` and `visualize_shape_model.cpp` files
    train and visualize the shape model respectively. Their usage will be outlined
    at the end of this section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细说明这个类的功能，首先描述**Procrustes 分析**，这是一种刚性注册点集的方法，然后是用于表示局部变形的线性模型。`train_shape_model.cpp`
    和 `visualize_shape_model.cpp` 文件中的程序分别训练和可视化形状模型。它们的用法将在本节末尾概述。
- en: Procrustes analysis
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Procrustes 分析
- en: In order to build a deformation model of face shapes, we must first process
    the raw annotated data to remove components pertaining to global rigid motion.
    When modeling geometry in 2D, a rigid motion is often represented as a similarity
    transform; this includes the scale, in-plane rotation, and translation. The following
    image illustrates the set of permissible motion types under a similarity transform.
    The process of removing global rigid motion from a collection of points is called
    **Procrustes analysis**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建人脸形状的变形模型，我们首先必须处理原始标注数据，以去除与全局刚性运动相关的成分。在 2D 几何建模中，刚性运动通常表示为相似变换；这包括缩放、平面旋转和平移。以下图像说明了在相似变换下的允许的运动类型集合。从点集集合中去除全局刚性运动的过程称为**Procrustes
    分析**。
- en: '![](img/image_05_003.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_05_003.jpg)'
- en: 'Mathematically, the objective of Procrustes analysis is to simultaneously find
    a canonical shape and similarity, and transform each data instance that brings
    them into alignment with the canonical shape. Here, alignment is measured as the
    least-squares distance between each transformed shape with the canonical shape.
    An iterative procedure for fulfilling this objective is implemented in the `shape_model`
    class as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，Procrustes 分析的目的是同时找到规范形状和相似性，并将每个数据实例转换，使其与规范形状对齐。在这里，对齐是通过每个转换形状与规范形状之间的最小二乘距离来衡量的。实现这一目标的迭代过程在
    `shape_model` 类中如下实现：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The algorithm begins by subtracting the center of mass of each shape''s instance
    followed by an iterative procedure that alternates between computing the canonical
    shape, as the normalized average of all shapes, and rotating and scaling each
    shape to best match the canonical shape. The normalization step of the estimated
    canonical shape is necessary to fix the scale of the problem and prevent it from
    shrinking all the shapes to zero. The choice of this anchor scale is arbitrary;
    here, we have chosen to enforce the length of the canonical shape vector `C` to
    1.0, as is the default behavior of OpenCV''s `normalize` function. Computing the
    in-plane rotation and scaling that best aligns each shape''s instance to the current
    estimate of the canonical shape is effected through the `rot_scale_align` function
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先从每个形状实例的质量中心减去，然后通过一个迭代过程，交替计算规范形状，即所有形状的归一化平均值，以及旋转和缩放每个形状以最佳匹配规范形状。估计规范形状的归一化步骤是必要的，以固定问题的比例并防止所有形状缩小到零。这个锚定比例的选择是任意的；在这里，我们选择强制规范形状向量
    `C` 的长度为 1.0，这是 OpenCV 的 `normalize` 函数的默认行为。通过 `rot_scale_align` 函数计算平面内旋转和缩放，以最佳对齐每个形状实例到当前规范形状的估计值如下：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function minimizes the following least squares difference between the
    rotated and canonical shapes. Mathematically this can be written as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数最小化旋转形状和规范形状之间的以下最小二乘差异。从数学上讲，这可以写成如下：
- en: '![](img/a7829_09_04-1.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a7829_09_04-1.png)'
- en: 'Here the solution to the least-squares problem takes on the closed-form solution
    shown in the following image on the right-hand side of the equation. Note that
    rather than solving for the scaling and in-plane rotation, which are nonlinearly
    related in the scaled 2D rotation matrix, we solve for the variables (`a`, `b`).
    These variables are related to the scale and rotation matrix as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最小二乘问题的解采用以下方程右侧所示方程的封闭形式解。请注意，我们不是求解与缩放 2D 旋转矩阵非线性相关的缩放和内平面旋转，而是求解变量（`a`，`b`）。这些变量与缩放和旋转矩阵的关系如下：
- en: '![](img/a7829_09_05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a7829_09_05.png)'
- en: 'A visualization of the effects of Procrustes analysis on raw annotated shape
    data is illustrated in the following image. Each facial feature is displayed with
    a unique color. After translation normalization, the structure of the face becomes
    apparent, where the locations of facial features cluster around their average
    locations. After the iterative scale and rotation normalization procedure, the
    feature clustering becomes more compact and their distribution becomes more representative
    of the variation induced by facial deformation. This last point is important as
    it is these deformations that we will attempt to model in the following section.
    Thus, the role of Procrustes analysis can be thought of as a preprocessing operation
    on the raw data that will allow better local deformation models of the face to
    be learned:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Procrustes 分析对原始标注形状数据的影响可视化如图所示。每个面部特征以独特的颜色显示。在平移归一化后，面部结构变得明显，面部特征的位置围绕其平均位置聚集。在迭代比例和旋转归一化过程之后，特征聚类变得更加紧凑，它们的分布更加代表面部变形引起的变异。这一点很重要，因为这是我们将在下一节尝试建模的变形。因此，Procrustes
    分析的作用可以被视为对原始数据进行预处理操作，这将允许学习更好的面部局部变形模型：
- en: '![](img/image_05_004.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_004.jpg)'
- en: Linear shape models
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性形状模型
- en: The aim of facial-deformation modeling is to find a compact parametric representation
    of how the face's shape varies across identities and between expressions. There
    are many ways of achieving this goal with various levels of complexity. The simplest
    of these is to use a linear representation of facial geometry. Despite its simplicity,
    it has been shown to accurately capture the space of facial deformations, particularly
    when the faces in the dataset are largely in a frontal pose. It also has the advantage
    that inferring the parameters of its representation is an extremely simple and
    cheap operation, in contrast to its nonlinear counterparts. This plays an important
    role when deploying it to constrain the search procedure during tracking.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 面部变形建模的目的是找到面部形状在身份和表情之间变化的紧凑参数表示。有许多方法可以实现这一目标，并且具有不同的复杂程度。其中最简单的一种是使用面部几何的线性表示。尽管它很简单，但它已被证明可以准确地捕捉面部变形的空间，尤其是在数据集中的面部大多处于正面姿态时。它还有优势，即推断其表示的参数是一个极其简单且成本极低的操作，与它的非线性对应物形成对比。这在将其部署到跟踪期间约束搜索过程时起着重要作用。
- en: The main idea of linearly modeling facial shapes is illustrated in the following
    image. Here, a face shape, which consists of *N* facial features, is modeled as
    a single point in a 2*N*-dimensional space. The aim of linear modeling is to find
    a low-dimensional hyperplane embedded within this 2*N*-dimensional space in which
    all the face shape points lie (that is, the green points in the image). As this
    hyperplane spans only a subset of the entire 2*N*-dimensional space, it is often
    referred to as the subspace. The lower the dimensionality of the subspace, the
    more compact the representation of the face is and the stronger the constraint
    that it places on the tracking procedure becomes. This often leads to more robust
    tracking. However, care should be taken in selecting the subspace's dimension
    so that it has enough capacity to span the space of all faces, but not so much
    that non-face shapes lie within its span (that is, the red points in the image).
    It should be noted that when modeling data from a single person, the subspace
    that captures the face's variability is often far more compact than the one that
    models multiple identities. This is one of the reasons why person-specific trackers
    perform much better than generic ones.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 线性建模面部形状的主要思想在以下图像中得到了说明。在这里，一个由*N*个面部特征组成的面部形状被建模为2*N*维空间中的一个单独的点。线性建模的目的是在2*N*维空间中找到一个低维超平面，其中所有面部形状点都位于该超平面内（即图像中的绿色点）。由于这个超平面仅覆盖整个2*N*维空间的一个子集，因此它通常被称为子空间。子空间的维度越低，面部表示就越紧凑，对跟踪过程施加的约束就越强。这通常会导致更鲁棒的跟踪。然而，在选择子空间的维度时应该小心，以确保它有足够的容量来涵盖所有面部的空间，但又不能太多以至于非面部形状位于其覆盖范围内（即图像中的红色点）。需要注意的是，当从单个人建模数据时，捕捉面部变异性的子空间通常比建模多个身份的子空间要紧凑得多。这就是为什么特定于个人的跟踪器比通用跟踪器表现更好的原因之一。
- en: '![](img/image_05_005.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_005.jpg)'
- en: 'The procedure for finding the best low-dimensional subspace that spans a dataset
    is called **Principal Component Analysis** (**PCA**). OpenCV implements a class
    for computing PCA; however, it requires the number of preserved subspace dimensions
    to be prespecified. As this is often difficult to determine a priori, a common
    heuristic is to choose it based on the fraction of the total amount of variation
    it accounts for. In the `shape_model::train` function, PCA is implemented as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳低维子空间以涵盖数据集的过程被称为**主成分分析**（**PCA**）。OpenCV实现了一个用于计算PCA的类；然而，它需要预先指定保留的子空间维度数。由于这通常很难事先确定，一个常见的启发式方法是根据它所解释的总变异量的一部分来选择它。在`shape_model::train`函数中，PCA的实现如下：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, each column of the `dY` variable denotes the mean-subtracted Procrustes-aligned
    shape. Thus, **Singular Value Decomposition** (**SVD**) is effectively applied
    to the covariance matrix of the shape data (that is, `dY.t()*dY`). The `w` member
    of OpenCV''s `SVD` class stores the variance in the major directions of variability
    of the data, ordered from largest to smallest. A common approach to choose the
    dimensionality of the subspace is to choose the smallest set of directions that
    preserve a fraction `frac` of the total energy of the data, which is represented
    by the entries of `svd.w`. As these entries are ordered from largest to smallest,
    it suffices to enumerate the subspace selection by greedily evaluating the energy
    in the top `k` directions of variability. The directions themselves are stored
    in the `u` member of the `SVD` class. The `svd.w` and `svd.u` components are generally
    referred to as the eigen spectrum and eigen vectors respectively. A visualization
    of these two components is shown in the following figure:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`dY` 变量的每一列表示减去均值后的Procrustes对齐形状。因此，**奇异值分解**（**SVD**）有效地应用于形状数据的协方差矩阵（即
    `dY.t()*dY`）。OpenCV的`SVD`类的`w`成员存储了数据主要变化方向上的方差，按从大到小的顺序排列。选择子空间维数的一种常见方法是选择能够保留数据总能量的一定分数
    `frac` 的最小方向集，这由 `svd.w` 的条目表示。由于这些条目是按从大到小的顺序排列的，因此只需通过贪婪地评估变化性的前 `k` 个方向中的能量来枚举子空间选择。这些方向本身存储在
    `SVD` 类的 `u` 成员中。`svd.w` 和 `svd.u` 成分通常分别被称为特征谱和特征向量。以下图显示了这两个组件的可视化：
- en: '![](img/image_05_006.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_006.jpg)'
- en: Note that the eigen spectrum decreases rapidly, which suggests that most of
    the variation contained in the data can be modeled with a low-dimensional subspace.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征谱迅速下降，这表明数据中的大部分变化可以用低维子空间来建模。
- en: A combined local-global representation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合局部-全局表示
- en: 'A shape in the image frame is generated by the composition of a local deformation
    and a global transformation. Mathematically, this parameterization can be problematic,
    as the composition of these transformations results in a nonlinear function that
    does not admit a closed-form solution. A common way to circumvent this problem
    is to model the global transformation as a linear subspace and append it to the
    deformation subspace. For a fixed shape, a similarity transform can be modeled
    with a subspace as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图像帧中的形状是通过局部变形和全局变换的组合生成的。从数学上讲，这种参数化可能存在问题，因为这些变换的组合产生了一个非线性函数，该函数没有封闭形式的解。绕过这个问题的常见方法是将全局变换建模为线性子空间，并将其附加到变形子空间上。对于固定的形状，可以使用子空间来建模相似变换，如下所示：
- en: '![](img/a7829_09_09.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a7829_09_09.png)'
- en: 'In the `shape_model` class, this subspace is generated using the `calc_rigid_basis`
    function. The shape from which the subspace is generated (that is, the `x` and
    `y` components in the preceding equation) is the mean shape over the Procustes-aligned
    shape (that is, the canonical shape). In addition to constructing the subspace
    in the aforementioned form, each column of the matrix is normalized to unit length.
    In the `shape_model::train` function, the variable `dY` described in the previous
    section is computed by projecting out the components of the data that pertain
    to rigid motion, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `shape_model` 类中，这个子空间是通过 `calc_rigid_basis` 函数生成的。从该子空间生成的形状（即前述方程中的 `x`
    和 `y` 分量）是Procustes对齐形状（即规范形状）的平均形状。除了以上所述的形式构建子空间外，矩阵的每一列都被归一化为单位长度。在 `shape_model::train`
    函数中，上一节中描述的变量 `dY` 通过以下方式计算，即投影掉与刚性运动相关的数据分量：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that this projection is implemented as a simple matrix multiplication.
    This is possible because the columns of the rigid subspace have been length normalized.
    This does not change the space spanned by the model, and means only that `R.t()*R`
    equals the identity matrix.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个投影是通过简单的矩阵乘法实现的。这是可能的，因为刚体子空间的列已经被长度归一化。这不会改变模型所涵盖的空间，仅仅意味着 `R.t()*R` 等于单位矩阵。
- en: 'As the directions of variability stemming from rigid transformations have been
    removed from the data before learning the deformation model, the resulting deformation
    subspace will be orthogonal to the rigid transformation subspace. Thus, concatenating
    the two subspaces results in a combined local-global linear representation of
    facial shapes that is also orthonormal. Concatenation here can be performed by
    assigning the two subspace matrices to submatrices of the combined subspace matrix
    through the ROI extraction mechanism implemented in OpenCV''s `Mat` class as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在学习变形模型之前已经从数据中移除了由刚性变换产生的可变性方向，因此得到的变形子空间将与刚性变换子空间正交。因此，将两个子空间连接起来，结果是一个面部形状的局部-全局线性表示，它也是正交归一的。这里的连接可以通过将两个子空间矩阵分配给组合子空间矩阵的子矩阵来实现，这是通过在OpenCV的`Mat`类中实现的ROI提取机制来完成的，如下所示：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The orthonormality of the resulting model means that the parameters describing
    a shape can be computed easily, as is done in the `shape_model::calc_params` function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型的正交性意味着可以轻松计算描述形状的参数，就像在`shape_model::calc_params`函数中所做的那样：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here `s` is a vectorized face shape and `p` stores the coordinates in the face
    subspace that represents it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`s`是一个向量化的面部形状，而`p`存储表示它的面部子空间中的坐标。
- en: A final point to note about linearly modeling facial shapes is how to constrain
    the subspace coordinates such that shapes generated using it remain valid. In
    the following image, instances of face shapes that lie within the subspace are
    shown for an increasing value of the coordinates in one of the directions of variability
    in increments of four standard deviations. Notice that for small values, the resulting
    shape remains face-like, but deteriorates as the values become too large.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于线性建模面部形状的一个需要注意的最终点是，如何约束子空间坐标，使得使用它生成的形状仍然有效。在下面的图像中，展示了随着一个方向的可变性坐标值以四个标准差为增量增加时，位于子空间内的面部形状实例。请注意，对于较小的值，生成的形状仍然类似面部，但随着值的增大，形状会变差。
- en: '![](img/image_05_007.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_007.jpg)'
- en: 'A simple way to prevent such deformation is to clamp the subspace coordinate
    values to lie within a permissible region as determined from the dataset. A common
    choice for this is a box constraint within &pm;3 standard deviations of the data,
    which accounts for 99.7 percent of variation in the data. These clamping values
    are computed in the `shape_model::train` function after the subspace is found,
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 防止这种变形的简单方法是将子空间坐标值夹具在从数据集确定的允许区域内。对此的一个常见选择是在数据加减3个标准差内的箱型约束，这解释了数据中99.7%的变化。这些夹具值是在找到子空间之后在`shape_model::train`函数中计算的，如下所示：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Notice that the variance is computed over the subspace coordinate `Q` after
    normalizing with respect to the coordinate of the first dimension (that is, scale).
    This prevents data samples that have relatively large scale from dominating the
    estimate. Also, notice that a negative value is assigned to the variance of the
    coordinates of the rigid subspace (that is, the first four columns of `V`). The
    clamping function `shape_model::clamp` checks to see if the variance of a particular
    direction is negative and only applies clamping if it is not, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，方差是在将第一维度的坐标（即尺度）归一化后，在子空间坐标`Q`上计算的。这防止了具有相对较大尺度的数据样本主导估计。此外，请注意，将负值分配给刚性子空间坐标的方差（即`V`的前四列）。`shape_model::clamp`函数检查特定方向的方差是否为负，并且只有在不是负值的情况下才应用夹具，如下所示：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The reason for this is that the training data is often captured under contrived
    settings where the face is upright and centered in the image at a particular scale.
    Clamping the rigid components of the shape model to adhere to the configurations
    in the training set would then be too restrictive. Finally, as the variance of
    each deformable coordinate is computed in the scale-normalized frame, the same
    scaling must be applied to the coordinates during clamping.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于训练数据通常是在人为设置下捕获的，其中面部在图像中垂直且居中，并且具有特定的尺度。将形状模型的刚性组件夹具到训练集中的配置将过于严格。最后，由于每个可变形坐标的方差是在尺度归一化框架中计算的，因此在夹具期间必须应用相同的缩放。
- en: Training and visualization
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和可视化
- en: 'An example program for training a shape model from the annotation data can
    be found in `train_shape_model.cpp`. With the command-line argument `argv[1]`
    containing the path to the annotation data, training begins by loading the data
    into memory and removing incomplete samples, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 `train_shape_model.cpp` 中找到一个从注释数据训练形状模型的示例程序。通过命令行参数 `argv[1]` 包含注释数据的路径，训练开始于将数据加载到内存中并移除不完整样本，如下所示：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The annotations for each example, and optionally their mirrored counterparts,
    are then stored in a vector before passing them to the training function as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将每个示例的注释以及可选的镜像对应物存储在一个向量中，然后再按照以下方式将它们传递给训练函数：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The shape model is then trained by a single function call to `shape_model::train` as
    follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过一个函数调用 `shape_model::train` 来训练形状模型，如下所示：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, `frac` (that is, the fraction of variation to retain) and `kmax` (that
    is, the maximum number of eigen vectors to retain) can be optionally set through
    command-line options, although the default settings of 0.95 and 20, respectively,
    tend to work well in most cases. Finally, with the command-line argument `argv[2]`
    containing the path to save the trained shape model to, saving can be performed
    by a single function call as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`frac`（即保留变化的分数）和 `kmax`（即保留的最大特征向量数）可以通过命令行选项进行设置，尽管在大多数情况下，默认设置 0.95 和
    20 通常效果很好。最后，通过命令行参数 `argv[2]` 包含保存训练好的形状模型的路径，可以通过单个函数调用执行保存，如下所示：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The simplicity of this step results from defining the `read` and `write` serialization
    functions for the `shape_model` class.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的简单性源于为 `shape_model` 类定义了 `read` 和 `write` 序列化函数。
- en: 'To visualize the trained shape model, the `visualize_shape_model.cpp` program
    animates the learned non-rigid deformations of each direction in turn. It begins
    by loading the shape model into memory as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化训练好的形状模型，`visualize_shape_model.cpp` 程序依次动画化每个方向学习到的非刚性变形。它首先按照以下方式将形状模型加载到内存中：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The rigid parameters that place the model at the center of the display window
    are computed as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型放置在显示窗口中心的刚性参数如下计算：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, the `calc_scale` function finds the scaling coefficient that would generate
    face shapes with a width of 200 pixels. The translation components are computed
    by finding the coefficients that generate a translation of 150 pixels (that is,
    the model is mean-centered and the display window is 300x300 pixels in size).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`calc_scale` 函数找到生成宽度为 200 像素的面部形状的缩放系数。通过找到生成 150 像素平移的系数来计算平移分量（即，模型是均值中心化的，显示窗口大小为
    300x300 像素）。
- en: Note that the first column of `shape_model::V` corresponds to scale and the
    third and fourth columns to `x` and `y` translations respectively.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`shape_model::V` 的第一列对应于缩放，第三和第四列分别对应于 `x` 和 `y` 平移。
- en: 'A trajectory of parameter values is then generated, which begins at zero, moves
    to the positive extreme, moves to the negative extreme, and then back to zero,
    as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后生成一个参数值轨迹，它从零开始，移动到正极限，然后移动到负极限，最后回到零，如下所示：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, each phase of the animation is composed of 50 increments. This trajectory
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，动画的每个阶段由 50 个增量组成。这个轨迹
- en: is then used to animate the face model and render the results in a display window
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后用于动画化面部模型并在显示窗口中渲染结果
- en: 'as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that the rigid coefficients (that is, those corresponding to the first
    four columns of `shape_model::V`) are always set to the values computed previously,
    to place the face at the center of the display window.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，刚性系数（即对应于 `shape_model::V` 的前四列）始终设置为之前计算出的值，以将面部放置在显示窗口的中心。
- en: Facial feature detectors
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部特征检测器
- en: 'Detecting facial features in images bares a strong resemblance to general object
    detection. OpenCV has a set of sophisticated functions for building general object
    detectors, the most well-known of which is the cascade of Haar-based feature detectors
    used in their implementation of the well-known **Viola-Jones** **face detector**.
    There are, however, a few distinguishing factors that make facial feature detection
    unique. These are as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中检测面部特征与通用对象检测有很强的相似性。OpenCV 有一个用于构建通用对象检测器的复杂函数集，其中最著名的是用于其实现中已知的 **Viola-Jones**
    **面部检测器** 的基于 Haar 特征检测器的级联。然而，有几个区分因素使得面部特征检测独特。这些如下：
- en: '**Precision versus robustness**: In generic object detection, the aim is to
    find the coarse position of the object in the image; facial feature detectors
    are required to give highly precise estimates of the location of the feature.
    An error of a few pixels is considered inconsequential in object detection but
    it can mean the difference between a smile and a frown in facial expression estimation
    through feature detections.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度与鲁棒性**：在通用目标检测中，目标是找到图像中对象的大致位置；面部特征检测器需要给出特征位置的高度精确估计。在目标检测中，几个像素的误差被认为是无关紧要的，但在通过特征检测估计面部表情时，这可能会意味着微笑和皱眉之间的区别。'
- en: '**Ambiguity from limited spatial support**: It is common to assume that the
    object of interest in generic object detection exhibits sufficient image structure
    such that it can be reliably discriminated from image regions that do not contain
    the object. This is often not the case for facial features, which typically have
    limited spatial support. This is because image regions that do not contain the
    object can often exhibit a very similar structure to facial features. For example,
    a feature on the periphery of the face, seen from a small bounding box centered
    at the feature, can be easily confused with any other image patch that contains
    a strong edge through its center.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限空间支持带来的歧义**：通常假设在通用目标检测中，感兴趣的对象在图像中表现出足够的结构，可以可靠地区分出不包含该对象的图像区域。对于面部特征来说，这通常不是情况，因为面部特征通常具有有限的空间支持。这是因为不包含对象的图像区域可以表现出与面部特征非常相似的结构。例如，从以特征为中心的小边界框中看到的面部边缘特征，很容易与其他任何包含通过其中心的强烈边缘的图像块混淆。'
- en: '**Computational complexity**: Generic object detection aims to find all instances
    of the object in an image. Face tracking, on the other hand, requires the locations
    of all facial features, which often ranges from around 20 to 100 features. Thus,
    the ability to evaluate each feature detector efficiently is paramount in building
    a face tracker that can run in real time.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度**：通用目标检测旨在找到图像中所有对象实例。另一方面，面部跟踪需要所有面部特征的位置，这通常在20到100个特征之间。因此，在构建一个能够实时运行的跟踪器时，能够高效地评估每个特征检测器的能力至关重要。'
- en: Due to these differences, the facial feature detectors used in face tracking
    are often specifically designed with that purpose in mind. There are, of course,
    many instances of generic object-detection techniques being applied to facial
    feature detectors in face tracking. However, there does not appear to be a consensus
    in the community about which representation is best suited for the problem.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些差异，用于面部跟踪的面部特征检测器通常是专门设计来满足这一目的的。当然，有许多通用目标检测技术被应用于面部特征检测器的例子。然而，在社区中似乎没有关于哪种表示最适合该问题的共识。
- en: 'In this section, we will build facial feature detectors using a representation
    that is perhaps the simplest model one would consider: a linear image patch. Despite
    its simplicity, with due care in designing its learning procedure, we will see
    that this representation can in fact give reasonable estimates of facial feature
    locations for use in a face-tracking algorithm. Furthermore, their simplicity
    enables an extremely rapid evaluation that makes real-time face tracking possible.
    Due to their representation as an image patch, the facial feature detectors are
    hereby referred to as patch models. This model is implemented in the `patch_model`
    class that can be found in the'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用可能是最简单的模型之一：线性图像块来构建面部特征检测器。尽管它很简单，但只要在设计其学习过程时足够小心，我们就会看到这种表示实际上可以给出合理的面部特征位置估计，用于面部跟踪算法。此外，它们的简单性使得评估速度极快，使得实时面部跟踪成为可能。由于它们的表示形式为图像块，面部特征检测器因此被称为块模型。此模型在
    `patch_model` 类中实现，可以在以下位置找到：
- en: '`patch_model.hpp` and `patch_model.cpp` files. The following code snippet is'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`patch_model.hpp` 和 `patch_model.cpp` 文件。以下代码片段是'
- en: 'of the header of the `patch_model` class that highlights its primary functionality:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`patch_model` 类的标题中突出其主要功能：'
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The patch model used to detect a facial feature is stored in the matrix `P`.
    The two functions of primary interest in this class are `calc_response` and `train`.
    The `calc_response` function evaluates the patch model's response at every integer
    displacement over the search region `im`. The `train` function learns the patch
    model `P` of size `psize` that, on an average, yields response maps over the training
    set that is as close as possible to the ideal response map. The parameters `var`,
    `lambda`, `mu_init`, and `nsamples` are parameters of the training procedure that
    can be tuned to optimize performance for the data at hand.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检测面部特征的图像块模型存储在矩阵`P`中。此类中两个主要的功能是`calc_response`和`train`。`calc_response`函数评估搜索区域`im`上每个整数位移处的图像块模型的响应。`train`函数学习大小为`psize`的图像块模型`P`，在平均情况下，在训练集上产生的响应图与理想响应图尽可能接近。`var`、`lambda`、`mu_init`和`nsamples`是训练过程的参数，可以根据手头的数据进行调整以优化性能。
- en: The functionality of this class will be elaborated in this section. We begin
    by discussing the correlation patch and its training procedure, which will be
    used to learn the patch model. Next, the `patch_models` class, which is a collection
    of the patch models for each facial feature and has functionality that accounts
    for global transformations will be described. The programs in `train_patch_model.cpp`
    and `visualize_patch_model.cpp` train and visualize the patch models, respectively,
    and their usage will be outlined at the end of this section on facial feature
    detectors.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将详细阐述此类功能。我们首先讨论相关图像块及其训练过程，这将用于学习图像块模型。接下来，将描述`patch_models`类，它是一组每个面部特征的图像块模型，并具有考虑全局变换的功能。`train_patch_model.cpp`和`visualize_patch_model.cpp`程序分别训练和可视化图像块模型，其用法将在本节末尾关于面部特征检测器的部分中概述。
- en: Correlation-based patch models
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于相关性的图像块模型
- en: 'In learning detectors, there are two primary competing paradigms: generative
    and discriminative. Generative methods learn an underlying representation of image
    patches that can best generate the object appearance in all its manifestations.
    Discriminative methods, on the other hand, learn a representation that best discriminates
    instances of the object from other objects that the model will likely encounter
    when deployed. Generative methods have the advantage that the resulting model
    encodes properties specific to the object, allowing novel instances of the object
    to be visually inspected. A popular approach that falls within the paradigm of
    generative methods is the famous `Eigenfaces` method. Discriminative methods have
    the advantage that the full capacity of the model is geared directly towards the
    problem at hand; discriminating instances of the object from all others. Perhaps
    the most well-known of all discriminative methods is the support vector machine.
    Although both paradigms can work well in many situations, we will see that when
    modeling facial features as an image patch, the discriminative paradigm is far
    superior.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习检测器中，存在两种主要的竞争性范式：生成性和判别性。生成性方法学习图像块的下层表示，可以最好地生成对象的所有表现形态。另一方面，判别性方法学习一种表示，可以最好地区分对象的实例与其他对象，这些对象是模型在部署时可能遇到的对象。生成性方法的优势在于，所得到的模型编码了特定于对象的特征，允许对对象的全新实例进行视觉检查。属于生成性方法范式的流行方法之一是著名的`Eigenfaces`方法。判别性方法的优势在于，模型的全部能力直接针对手头的问题；从所有其他对象中区分对象的实例。也许最著名的判别性方法就是支持向量机。尽管这两种范式在许多情况下都可以很好地工作，但我们将看到，当将面部特征建模为图像块时，判别性范式远优于生成性范式。
- en: Note that the `Eigenfaces` and support vector machine methods were originally
    developed for classification rather than detection or image alignment. However,
    their underlying mathematical concepts have been shown to be applicable to the
    face-tracking domain.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`Eigenfaces`和支持向量机方法最初是为分类而不是检测或图像对齐而开发的。然而，它们背后的数学概念已被证明适用于面部跟踪领域。
- en: Learning discriminative patch models
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习判别性图像块模型
- en: 'Given an annotated dataset, the feature detectors can be learned independently
    from each other. The learning objective of a discriminative patch model is to
    construct an image patch that, when cross-correlated with an image region containing
    the facial feature, yields a strong response at the fease. Mathematically, this
    can be expressed as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个标注数据集，特征检测器可以独立于彼此进行学习。判别性图像块模型的目的是构建一个图像块，当与包含面部特征的图像区域进行交叉相关时，在特征点处产生强烈的响应。从数学上讲，这可以表示为：
- en: '![](img/a7829_09_11-1.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/a7829_09_11-1.png)'
- en: 'Here, **P** denotes the patch model, **I** denotes the i^(th) training image,
    **I**(*a:b, c:d*) denotes the rectangular region whose top-left and bottom-right
    corners are located at *(a, c)* and *(b, d)*, respectively. The period symbol
    denotes the inner product operation and **R** denotes the ideal response map.
    The solution to this equation is a patch model that generates response maps that
    are, on average, closest to the ideal response map as measured using the least-squares
    criterion. An obvious choice for the ideal response map, **R**, is a matrix with
    zeros everywhere except at the center (assuming the training image patches are
    centered at the facial feature of interest). In practice, since the images are
    hand-labeled, there will always be an annotation error. To account for this, it
    is common to describe R as a decaying function of distance from the center. A
    good choice is the 2D-Gaussian distribution, which is equivalent to assuming the
    annotation error is Gaussian distributed. A visualization of this setup is shown
    in the following figure for the left outer eye corner:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**P**表示图像块模型，**I**表示第i个训练图像，**I**(*a:b, c:d*)表示其左上角和右下角分别位于*(a, c)*和*(b,
    d)*的矩形区域。周期符号表示内积运算，**R**表示理想响应图。该方程的解是一个图像块模型，它生成的响应图在平均意义上，使用最小二乘标准测量时，最接近理想响应图。理想响应图**R**的一个明显选择是一个除了中心外所有地方都是零的矩阵（假设训练图像块以感兴趣的面部特征为中心）。在实践中，由于图像是手工标注的，总会存在标注错误。为了解决这个问题，通常将R描述为中心距离的衰减函数。一个好的选择是二维高斯分布，这相当于假设标注错误是高斯分布的。以下图显示了左外眼角的这种设置的可视化：
- en: '![](img/a7829_09_12.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/a7829_09_12.png)'
- en: The learning objective as written previously is in a form commonly referred
    to as linear least squares. As such, it affords a closed-form solution. However,
    the degrees of freedom of this problem; that is, the number of ways the variables
    can vary to solve the problem, is equal to the number of pixels in the patch.
    Thus, the computational cost and memory requirements of solving for the optimal
    patch model can be prohibitive, even for a moderately sized patch; for example,
    a 40x40 patch model has 1,600 degrees of freedom.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的学习目标通常以线性最小二乘的形式表示。因此，它提供了一个封闭形式的解。然而，这个问题的自由度；也就是说，变量可以变化以解决问题的方式的数量，等于图像块中的像素数。因此，求解最优图像块模型的计算成本和内存需求可能是制约因素，即使是对于中等大小的图像块；例如，一个40x40的图像块模型有1,600个自由度。
- en: 'An efficient alternative to solving the learning problem as a linear system
    of equations is a method called stochastic gradient descent. By visualizing the
    learning objective as an error terrain over the degrees of freedom of the patch
    model, stochastic gradient descent iteratively makes an approximate estimate of
    the gradient direction of the terrain and takes a small step in the opposite direction.
    For our problem, the approximation to gradient can be computed by considering
    only the gradient of the learning objective for a single, randomly chosen image
    from the training set:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将学习问题作为线性方程组求解的一个有效替代方法是称为随机梯度下降的方法。通过将学习目标可视化为图像块模型自由度上的误差地形，随机梯度下降迭代地做出对地形梯度方向的近似估计，并朝相反方向迈出小步。对于我们的问题，梯度近似的计算可以通过仅考虑从训练集中随机选择的一个图像的学习目标梯度来完成：
- en: '![](img/a7829_09_13.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/a7829_09_13.png)'
- en: 'In the `patch_model` class, this learning process is implemented in the `train`
    function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在`patch_model`类中，这个学习过程在`train`函数中实现：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The first highlighted code snippet in the preceding code is where the ideal
    response map is computed. Since the images are centered on the facial feature
    of interest, the response map is the same for all samples. In the second highlighted
    code snippet, the decay rate, `step`, of the step sizes is determined such that
    after `nsamples` iterations, the step size would have decayed to a value close
    to zero. The third highlighted code snippet is where the stochastic gradient direction
    is computed and used to update the patch model. There are two things to note here.
    First, the images used in training are passed to the `patch_model::convert_image`
    function, which converts the image to a single-channel image (if it is a color
    image) and applies the natural logarithm to the image pixel intensities:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码中第一个突出显示的代码片段是计算理想响应图的地方。由于图像是以感兴趣的面部特征为中心的，因此响应图对所有样本都是相同的。在第二个突出显示的代码片段中，确定了步长大小的衰减率`step`，使得在`nsamples`次迭代后，步长将衰减到接近零的值。第三个突出显示的代码片段是计算随机梯度方向并用于更新补丁模型的地方。这里有两点需要注意。首先，用于训练的图像被传递到`patch_model::convert_image`函数，该函数将图像转换为单通道图像（如果它是彩色图像）并应用自然对数到图像像素强度：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: A bias value of 1 is added to each pixel before applying the logarithm since
    the logarithm of zero is undefined. The reason for performing this pre-processing
    on the training images is because log-scale images are more robust against differences
    in contrast and changes in illumination conditions. The following figure shows
    images of two faces with different degrees of contrast in the facial region. The
    difference between the images is much less pronounced in the log-scale images
    than it is in the raw images.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用对数之前，每个像素都添加了一个偏置值1，因为零的对数是未定义的。在训练图像上执行这种预处理的原因是因为对数尺度图像对对比度差异和光照条件变化更加鲁棒。以下图显示了两个面部图像，面部区域的对比度不同。在对数尺度图像中，图像之间的差异比在原始图像中要小得多。
- en: '![](img/image_05_009.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_009.jpg)'
- en: The second point to note about the update equation is the subtraction of `lambda*P`
    from the update direction. This effectively regularizes the solution from growing
    too large; a procedure that is often applied in machine-learning algorithms to
    promote generalization to unseen data. The scaling factor `lambda` is user defined
    and is usually problem dependent. However, a small value typically works well
    for learning patch models for facial feature detection.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 关于更新方程的第二个需要注意的问题是减去`lambda*P`。这有效地限制了解的增长，这是一种在机器学习算法中常用于促进对未见数据的泛化的过程。缩放因子`lambda`由用户定义，通常与问题相关。然而，对于学习面部特征检测的补丁模型，通常一个较小的值效果很好。
- en: Generative versus discriminative patch models
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成性补丁模型与判别性补丁模型
- en: 'Despite the ease of which discriminative patch models can be learned as described
    previously, it is worth considering whether generative patch models and their
    corresponding training regimes are simple enough to achieve similar results. The
    generative counterpart of the correlation patch model is the average patch. The
    learning objective for this model is to construct a single image patch that is
    as close as possible to all examples of the facial feature as measured via the
    least-squares criterion:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如前所述，判别性补丁模型可以很容易地学习，但考虑生成性补丁模型及其相应的训练机制是否足够简单以实现类似的结果是值得的。相关补丁模型的生成对应物是平均补丁。该模型的训练目标是构建一个尽可能接近通过最小二乘准则测量的所有面部特征示例的单个图像补丁：
- en: '![](img/a7829_09_15.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a7829_09_15.png)'
- en: The solution to this problem is exactly the average of all the feature-centered
    training image patches. Thus, in a way, the solution afforded by this objective
    is far simpler.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的解决方案正好是所有特征中心化训练图像补丁的平均值。因此，从这个目标提供的解决方案来看，它要简单得多。
- en: 'In the following figure, a comparison is shown for the response maps obtained
    by cross-correlating the average and correlation patch models with an example
    image. The respective average and correlation patch models are also shown, where
    the range of pixel values is normalized for visualization purposes. Although the
    two patch model types exhibit some similarities, the response maps they generate
    differ substantially. While the correlation patch model generates response maps
    that are highly peaked around the feature location, the response map generated
    by the average patch model is overly smooth and does not strongly distinguish
    the feature location from those close by. Inspecting the patch models'' appearance,
    the correlation patch model is mostly gray, which corresponds to zero in the un-normalized
    pixel range, with strong positive and negative values strategically placed around
    prominent areas of the facial feature. Thus, it preserves only those components
    of the training patches, useful for discriminating it from misaligned configuration,
    which leads to highly peaked responses. In contrast, the average patch model encodes
    no knowledge of misaligned data. As a result, it is not well suited to the task
    of facial feature localization, where the task is to discriminate an aligned image
    patch from locally shifted versions of itself:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，展示了通过交叉相关平均和关联补丁模型与示例图像获得的响应图比较。相应的平均和关联补丁模型也显示出来，其中像素值范围已归一化以供可视化。尽管两种补丁模型类型表现出一些相似之处，但它们生成的响应图差异很大。关联补丁模型生成的响应图在特征位置周围高度峰值，而平均补丁模型生成的响应图过于平滑，并且没有强烈地区分特征位置和附近的特征。检查补丁模型的外观，关联补丁模型主要是灰色，对应于未归一化像素范围中的零，而在面部特征显著区域周围放置了强烈的正负值。因此，它仅保留那些对区分错位配置有用的训练补丁组件，导致高度峰值响应。相比之下，平均补丁模型不编码错位数据的知识。因此，它不适合面部特征定位的任务，该任务的目的是区分对齐图像补丁与其局部平移版本：
- en: '![](img/image_05_010.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_05_010.jpg)'
- en: Accounting for global geometric transformations
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑全局几何变换
- en: So far, we have assumed that the training images are centered at the facial
    feature and are normalized with respect to global scale and rotation. In practice,
    the face can appear at any scale and rotation within the image during tracking.
    Thus, a mechanism must be devised to account for this discrepancy between the
    training and testing conditions. One approach is to synthetically perturb the
    training images in scale and rotation within the ranges one expects to encounter
    during deployment. However, the simplistic form of the detector as a correlation
    patch model often lacks the capacity to generate useful response maps for that
    kind of data. On the other hand, the correlation patch model does exhibit a degree
    of robustness against small perturbations in scale and rotation. Since motion
    between consecutive frames in a video sequence is relatively small, one can leverage
    the estimated global transformation of the face in the previous frame to normalize
    the current image with respect to scale and rotation. All that is needed to enable
    this procedure is to select a reference frame in which the correlation patch models
    are learned.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设训练图像是以面部特征为中心，并且相对于全局尺度和平移进行了归一化。在实践中，面部可以在图像中的任何尺度和平移下出现，因此在跟踪过程中。因此，必须设计一种机制来处理训练和测试条件之间的这种差异。一种方法是在部署期间预期遇到的范围内在尺度和平移上对训练图像进行人工扰动。然而，检测器作为关联补丁模型的形式过于简单，通常缺乏生成此类数据的有用响应图的能力。另一方面，关联补丁模型对尺度和平移的小扰动具有一定的鲁棒性。由于视频序列中连续帧之间的运动相对较小，可以利用前一帧中面部估计的全局变换来相对于尺度和平移归一化当前图像。要启用此过程，只需要选择一个学习关联补丁模型的参考帧。
- en: 'The `patch_models` class stores the correlation patch models for each facial
    feature as well as the reference frame in which they are trained. It is the `patch_models`
    class, rather than the `patch_model` class, that the face tracker code interfaces
    with directly, to obtain the feature detections. The following code snippet of
    the declaration of this class highlights its primary functionality:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`patch_models` 类存储了每个面部特征的关联补丁模型以及它们训练时的参考帧。直接与代码接口以获取特征检测的是 `patch_models`
    类，而不是 `patch_model` 类。以下代码片段展示了该类的核心功能：'
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `reference` shape is stored as an interleaved set of (*x, y*) coordinates
    that are used to normalize the scale and rotation of the training images, and
    later, during deployment, that of the test images. In the `patch_models::train`
    function, this is done by first computing the similarity transform between the
    `reference` shape and the annotated shape for a given image using the `patch_models::calc_simil`
    function, which solves a similar problem to that in the `shape_model::procrustes`
    function, albeit for a single pair of shapes. Since the rotation and scale is
    common across all facial features, the image normalization procedure only requires
    adjusting this similarity transform to account for the centers of each feature
    in the image and the center of the normalized image patch. In `patch_models::train`,
    this is implemented as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`reference` 形状存储为交织的 (*x, y*) 坐标集，这些坐标用于归一化训练图像的缩放和旋转，并在部署期间，测试图像的缩放和旋转。在 `patch_models::train`
    函数中，这是通过首先使用 `patch_models::calc_simil` 函数计算给定图像的 `reference` 形状和标注形状之间的相似性变换来完成的，该函数解决了一个类似于
    `shape_model::procrustes` 函数中的问题，尽管是针对一对形状。由于旋转和缩放在所有面部特征中是通用的，因此图像归一化过程只需要调整这个相似性变换，以考虑图像中每个特征的中心和归一化图像块的中心。在
    `patch_models::train` 中，这是通过以下方式实现的：'
- en: '[PRE30]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, `wsize` is the total size of the normalized training image, which is
    the sum of the patch size and the search region size. As just mentioned, the top-left
    (2x2) block of the similarity transform from the reference shape to the annotated
    shape `pt`, which corresponds to the scale and rotation component of the transformation,
    is preserved in the affine transform passed to OpenCV''s `warpAffine` function.
    The last column of the affine transform `A` is an adjustment that will render
    the i^(th) facial feature location centered in the normalized image after warping
    (that is, the normalizing translation). Finally, the `cv::warpAffine` function
    has the default setting of warping from the image to the reference frame. Since
    the similarity transform was computed for transforming the `reference` shape to
    the image-space annotations, the `pt`, the `WARP_INVERSE_MAP` flag needs to be
    set to ensure the function applies the warp in the desired direction. Exactly
    the same procedure is performed in the `patch_models::calc_peaks` function, with
    the additional step that the computed similarity transform between the reference
    and the current shape in the image-frame is re-used to un-normalize the detected
    facial features, placing them appropriately in the image:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`wsize` 是归一化训练图像的总大小，它是补丁大小和搜索区域大小的总和。正如刚才提到的，从参考形状到标注形状 `pt` 的相似性变换的左上角（2x2）块，对应于变换的缩放和旋转组件，被保留在传递给
    OpenCV 的 `warpAffine` 函数的仿射变换中。仿射变换 `A` 的最后一列是一个调整，它将在扭曲后使第 i 个面部特征位置在归一化图像中居中（即归一化平移）。最后，`cv::warpAffine`
    函数具有默认的从图像到参考框架的扭曲设置。由于相似性变换是为了将 `reference` 形状转换到图像空间标注而计算的，因此需要设置 `WARP_INVERSE_MAP`
    标志以确保函数按所需方向应用扭曲。在 `patch_models::calc_peaks` 函数中也执行了完全相同的程序，额外的步骤是重新使用在图像框架中参考形状和当前形状之间计算的相似性变换来反归一化检测到的面部特征，将它们适当地放置在图像中：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the first highlighted code snippet in the preceding code, both the forward
    and inverse similarity transforms are computed. The reason why the inverse transform
    is required here is so that the peaks of the response map for each feature can
    be adjusted according to the normalized locations of the current shape estimate.
    This must be performed before reapplying the similarity transform to place the
    new estimates of the facial feature locations back into the image frame using
    the
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中第一个高亮的代码片段中，计算了前向和逆相似性变换。这里需要逆变换的原因是为了根据当前形状估计的归一化位置调整每个特征的响应图峰值。这必须在重新应用相似性变换，将面部特征位置的新估计放回图像框架之前完成。
- en: '`patch_models::apply_simil` function.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`patch_models::apply_simil` 函数。'
- en: Training and visualization
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和可视化
- en: 'An example program for training the patch models from the annotation data can
    be found in `train_patch_model.cpp`. With the command-line argument `argv[1]`
    containing the path to the annotation data, training begins by loading the data
    into memory and removing incomplete samples:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从注释数据中训练补丁模型的示例程序可以在 `train_patch_model.cpp` 中找到。通过命令行参数 `argv[1]` 包含注释数据的路径，训练开始于将数据加载到内存中并移除不完整样本：
- en: '[PRE32]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The simplest choice for the reference shape in the `patch_models` class is
    the average shape of the training set, scaled to a desired size. Assuming that
    a shape model has previously been trained for this dataset, the reference shape
    is computed by first loading the shape model stored in `argv[2]` as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `patch_models` 类中，参考形状的最简单选择是训练集的平均形状，缩放到期望的大小。假设已经为该数据集训练了一个形状模型，则通过以下方式计算参考形状：首先加载存储在
    `argv[2]` 中的形状模型：
- en: '[PRE33]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is followed by the computation of the scaled-centered average shape:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这之后是计算缩放中心平均形状：
- en: '[PRE34]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `calc_scale` function computes the scaling factor to transform the average
    shape (that is, the first column of `shape_model::V`) to one with a width of `width`.
    Once the reference shape `r` is defined, training the set of patch models can
    be done with a single function call:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`calc_scale` 函数计算缩放因子，将平均形状（即 `shape_model::V` 的第一列）转换为宽度为 `width` 的形状。一旦定义了参考形状
    `r`，就可以通过单个函数调用来训练补丁模型集：'
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The optimal choices for the parameters `width`, `psize`, and `ssize` are application
    dependent; however, the default values of 100, 11, and 11, respectively, give
    reasonable results in general.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `width`、`psize` 和 `ssize` 的最佳选择取决于应用；然而，通常情况下，默认值 100、11 和 11 分别可以给出合理的结果。
- en: Although the training process is quite simple, it can still take some time to
    complete. Depending on the number of facial features, the size of the patches,
    and the number of stochastic samples in the optimization algorithm, the training
    process can take anywhere from between a few minutes to over an hour. However,
    since the training of each patch can be performed independently of all others,
    this process can be sped
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练过程相当简单，但仍可能需要一些时间来完成。根据面部特征的数量、补丁的大小以及优化算法中的随机样本数量，训练过程可能需要几分钟到超过一小时。然而，由于每个补丁的训练可以独立于所有其他补丁进行，这个过程可以通过在多个处理器核心或机器上并行化训练过程来显著加速。
- en: up substantially by parallelizing the training process across multiple processorcores
    or machines.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在多个处理器核心或机器上并行化训练过程，可以显著提高训练过程的效率。
- en: 'Once training has been completed, the program in `visualize_patch_model.cpp`
    can be used to visualize the resulting patch models. As with the `visualize_shape_model.cpp`
    program, the aim here is to visually inspect the results to verify if anything
    went wrong during the training process. The program generates a composite image
    of all the patch models, `patch_model::P`, each centered at their respective feature
    location in the reference shape, `patch_models::reference`, and displaying a bounding
    rectangle around the patch whose index is currently active. The `cv::waitKey`
    function is used to get user input for selecting the activee patch index and terminating
    the program. The following image shows three examples of composite patch images
    learned for patch models with varying spatial support. Despite using the same
    training data, modifying the spatial support of the patch model appears to change
    the structure of the patch models substantially. Visually inspecting the results
    in this way can lend intuition into how to modify the parameters of the training
    process, or even the training process itself, in order to optimize results for
    a particular application:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，就可以使用 `visualize_patch_model.cpp` 中的程序来可视化结果补丁模型。与 `visualize_shape_model.cpp`
    程序一样，这里的目的是通过视觉检查结果来验证训练过程中是否出现了错误。程序生成所有补丁模型的合成图像，`patch_model::P`，每个都在参考形状 `patch_models::reference`
    的相应特征位置中心，并显示围绕当前活动索引的补丁的边界矩形。使用 `cv::waitKey` 函数获取用户输入以选择活动补丁索引并终止程序。以下图像显示了为具有不同空间支持的补丁模型学习到的三个合成补丁图像示例。尽管使用了相同的训练数据，但修改补丁模型的空间支持似乎显著改变了补丁模型的结构。以这种方式检查结果可以提供对如何修改训练过程的参数，甚至整个训练过程本身，以优化特定应用的直觉：
- en: '![](img/image_05_011.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_011.jpg)'
- en: Face detection and initialization
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部检测和初始化
- en: The method for face tracking described thus far has assumed that the facial
    features in the image are located within a reasonable proximity to the current
    estimate. Although this assumption is reasonable during tracking, where face motion
    between frames is often quite small, we are still faced with the dilemma of how
    to initialize the model in the first frame of the sequence. An obvious choice
    for this is to use OpenCV's in-built cascade detector to find the face. However,
    the placement of the model within the detected bounding box will depend on the
    selection made for the facial features to track. In keeping with the data-driven
    paradigm we have followed so far in this chapter, a simple solution is to learn
    the geometrical relationship between the face detection's bounding box and the
    facial features.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止所描述的人脸跟踪方法假设图像中的面部特征位于当前估计的合理距离内。尽管在跟踪期间这个假设是合理的，因为帧与帧之间的面部运动通常很小，但我们仍然面临着如何在序列的第一帧中初始化模型的两难困境。对于这个选择，一个明显的选择是使用OpenCV的内置级联检测器来找到面部。然而，模型在检测到的边界框内的放置将取决于跟踪的面部特征的选择。遵循本章中迄今为止所遵循的数据驱动范式，一个简单的解决方案是学习人脸检测边界框与面部特征之间的几何关系。
- en: 'The `face_detector` class implements exactly this solution. A snippet of its
    declaration that highlights its functionality is given as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`face_detector`类实现了这个解决方案。以下是其声明的片段，突出了其功能：'
- en: '[PRE36]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The class has four public member variables: the path to an object of type `cv::CascadeClassifier`
    called `detector_fname`, a set of offsets from a detection bounding box to the
    location and scale of the face in the image `detector_offset`, a reference shape
    to place in the bounding box `reference`, and a face detector `detector`. The
    primary function of use to a face-tracking system is `face_detector::detect`,
    which takes an image as the input, along with standard options for the `cv::CascadeClassifier`
    class, and returns a rough estimate of the facial feature locations in the image.
    Its implementation is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 该类有四个公共成员变量：一个名为`detector_fname`的`cv::CascadeClassifier`类型对象的路径，一个从检测边界框到图像中面部位置和缩放的偏移量集`detector_offset`，一个放置在边界框中的参考形状`reference`，以及一个面部检测器`detector`。对面部跟踪系统最有用的主要功能是`face_detector::detect`，它接受一个图像作为输入，以及`cv::CascadeClassifier`类的标准选项，并返回图像中面部特征位置的粗略估计。其实现如下：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The face is detected in the image in the usual way, except that the `CV_HAAR_FIND_BIGGEST_OBJECT`
    flag is set so as to enable tracking the most prominent face in the image. The
    highlighted code is where the reference shape is placed in the image in accordance
    with the detected face''s bounding box. The `detector_offset` member variable
    consists of three components: an (x, y) offset of the center of the face from
    the center of the detection''s bounding box, and the scaling factor that resizes
    the reference shape to best fit the face in the image. All three components are
    a linear function of the bounding box''s width.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸以通常的方式在图像中被检测到，除了设置了`CV_HAAR_FIND_BIGGEST_OBJECT`标志，以便能够跟踪图像中最突出的面部。高亮显示的代码是按照检测到的面部边界框将参考形状放置在图像中的位置。`detector_offset`成员变量由三个部分组成：人脸中心与检测边界框中心的(x,
    y)偏移量，以及将参考形状调整大小以最佳匹配图像中人脸的缩放因子。所有这三个部分都是边界框宽度的线性函数。
- en: 'The linear relationship between the bounding box''s width and the `detector_offset`
    variable is learned from the annotated dataset in the `face_detector::train` function.
    The learning process is started by loading the training data into memory and assigning
    the reference shape:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在`face_detector::train`函数中，从标注的数据集中学习边界框宽度与`detector_offset`变量之间的线性关系。学习过程通过将训练数据加载到内存中并分配参考形状来启动：
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As with the reference shape in the `patch_models` class, a convenient choice
    for the reference shape is the normalized average face shape in the dataset. The
    `cv::CascadeClassifier` is then applied to each image (and optionally its mirrored
    counterpart) in the dataset and the resulting detection is checked to ensure that
    enough annotated points lie within the detected bounding box (see the figure towards
    the end of this section) to prevent learning from misdetections:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与`patch_models`类中的参考形状一样，对于参考形状的一个方便的选择是数据集中归一化的平均人脸形状。然后，`cv::CascadeClassifier`被应用于数据集中的每一张图像（以及可选的镜像图像），并对检测结果进行检查，以确保足够的标注点位于检测到的边界框内（参见本节末尾的图），以防止从误检中学习：
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If more than a fraction of `frac` of the annotated points lie within the bounding
    box, the linear relationship between its width and the offset parameters for that
    image are added as a new entry in an STL `vector` class object. Here, the `face_detector::center_of_mass`
    function computes the center of mass of the annotated point set for that image
    and the `face_detector::calc_scale` function computes the scaling factor for transforming
    the reference shape to the centered annotated shape. Once all images have been
    processed, the `detector_offset` variable is set to the median over all of the
    image-specific offsets:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果超过`frac`分数的注释点位于边界框内，则将该图像的宽度和偏移参数之间的线性关系添加为STL `vector`类对象的新条目。在这里，`face_detector::center_of_mass`函数计算该图像注释点集的质量中心，而`face_detector::calc_scale`函数计算将参考形状转换为居中注释形状的缩放因子。一旦处理完所有图像，`detector_offset`变量被设置为所有图像特定偏移的中位数：
- en: '[PRE40]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As with the *shape and patch* models, the simple program in `train_face_detector.cpp`
    is an example of how a `face_detector` object can be built and saved for later
    use in the tracker. It first loads the annotation data and the shape model, and
    sets the reference shape as the mean-centered average of the training data (that
    is, the identity shape of the `shape_model` class):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 与**形状和补丁**模型一样，`train_face_detector.cpp`中的简单程序是构建和保存`face_detector`对象以供跟踪器后续使用的示例。它首先加载注释数据和形状模型，并将参考形状设置为训练数据的均值中心平均（即`shape_model`类的身份形状）：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Training and saving the face detector, then, consists of two function calls:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练和保存面部检测器包括两个函数调用：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To test the performance of the resulting shape-placement procedure, the program
    in `visualize_face_detector.cpp` calls the `face_detector::detect` function for
    each image in the video or camera input stream and draws the results on screen.
    An example of the results using this approach is shown in the following figure.
    Although the placed shape does not match the individual in the image, its placement
    is close enough so that face tracking can proceed using the approach described
    in the following section:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试形状放置过程的性能，`visualize_face_detector.cpp`程序对视频或相机输入流中的每张图像调用`face_detector::detect`函数，并在屏幕上绘制结果。以下图显示了使用这种方法的结果示例。尽管放置的形状与图像中的个体不匹配，但其放置足够接近，可以使用下节描述的方法进行面部跟踪：
- en: '![](img/image_05_012.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_012.jpg)'
- en: Face tracking
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部跟踪
- en: 'The problem of face tracking can be posed as that of finding an efficient and
    robust way to combine the independent detections of various facial features with
    the geometrical dependencies they exhibit in order to arrive at an accurate estimate
    of facial feature locations in each image of a sequence. With this in mind, it
    is perhaps worth considering whether geometrical dependencies are at all necessary.
    In the following figure, the results of detecting the facial features with and
    without geometrical constraints are shown. These results clearly highlight the
    benefit of capturing the spatial inter-dependencies between facial features. The
    relative performance of these two approaches is typical, whereby relying strictly
    on the detections leads to overly noisy solutions. The reason for this is that
    the response maps for each facial feature cannot be expected to always peak at
    the correct location. Whether due to image noise, lighting changes, or expression
    variation, the only way to overcome the limitations of facial feature detectors
    is by leveraging the geometrical relationship they share with each other:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 面部跟踪问题可以表述为寻找一种高效且鲁棒的方法，将各种面部特征的独立检测与它们所表现出的几何依赖性结合起来，以便对序列中每张图像的面部特征位置进行准确估计。考虑到这一点，也许值得考虑几何依赖性是否真的必要。在下图中，显示了带有和不带有几何约束的面部特征检测结果。这些结果清楚地突出了捕捉面部特征之间空间相互依赖性的好处。这两种方法的相对性能是典型的，完全依赖检测会导致过度嘈杂的解决方案。原因在于，对于每个面部特征的响应图不能期望总是峰值出现在正确的位置。无论是因为图像噪声、光照变化还是表情变化，克服面部特征检测器局限性的唯一方法是通过利用它们之间共享的几何关系：
- en: '![](img/image_05_013.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_013.jpg)'
- en: A particularly simple, but surprisingly effective, way to incorporate facial
    geometry into the tracking procedure is by projecting the output of the feature
    detections onto the linear shape model's subspace. This amounts to minimizing
    the distance between the original points and their closest plausible shape that
    lies on the subspace. Thus, when the spatial noise in the feature detections is
    close to being Gaussian distributed, the projection yields the most likely solution.
    In practice, the distribution of detection errors on occasion does not follow
    a Gaussian distribution and additional mechanisms need to be introduced to account
    for this.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 将面部几何形状纳入跟踪过程的一种特别简单但出奇有效的方法是将特征检测的输出投影到线性形状模型的子空间。这相当于最小化原始点与其在子空间上最接近的合理形状之间的距离。因此，当特征检测中的空间噪声接近高斯分布时，投影会产生最可能的解。在实践中，检测错误的分布有时并不遵循高斯分布，需要引入额外的机制来解释这一点。
- en: Face tracker implementation
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部跟踪实现
- en: 'An implementation of the face-tracking algorithm can be found in the `face_tracker`
    class (see `face_tracker.cpp` and `face_tracker.hpp`). The following code is a
    snippet of its header that highlights its primary functionality:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 面部跟踪算法的实现可以在`face_tracker`类中找到（参见`face_tracker.cpp`和`face_tracker.hpp`）。以下代码是该头文件的一个片段，突出了其主要功能：
- en: '[PRE43]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The class has public member instances of the `shape_model`, `patch_models`,
    and `face_detector` classes. It uses the functionality of these three classes
    to effect tracking. The `timer` variable is an instance of the `fps_timer` class
    that keeps track of the frame rate at which the `face_tracker::track` function
    is called and is useful for analyzing the effects patch and shape model configurations
    on the computational complexity of the algorithm. The `tracking` member variable
    is a flag to indicate the current state of the tracking procedure. When this flag
    is set to `false`, as it is in the constructor and the `face_tracker::reset` function,
    the tracker enters a detection mode whereby the `face_detector::detect` function
    is applied to the next incoming image to initialize the model. When in the tracking
    mode, the initial estimate used for inferring facial feature locations in the
    next incoming image is simply their location in the previous frame. The complete
    tracking algorithm is implemented simply as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 该类具有`shape_model`、`patch_models`和`face_detector`类的公共成员实例。它使用这三个类的功能来实现跟踪。`timer`变量是`fps_timer`类的实例，用于跟踪`face_tracker::track`函数被调用的帧率，这对于分析补丁和形状模型配置对算法计算复杂性的影响是有用的。`tracking`成员变量是一个标志，用于指示跟踪过程的状态。当此标志设置为`false`时，如构造函数和`face_tracker::reset`函数中所示，跟踪器进入检测模式，其中`face_detector::detect`函数应用于下一个传入的图像以初始化模型。在跟踪模式下，用于推断下一个传入图像中面部特征位置的初始估计只是它们在上一帧中的位置。完整的跟踪算法简单地如下实现：
- en: '[PRE44]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Other than bookkeeping operations, such as setting the appropriate `tracking`
    state and incrementing the tracking time, the core of the tracking algorithm is
    the multi-level fitting procedure, which is highlighted in the preceding code
    snippet. The fitting algorithm, implemented in the `face_tracker::fit` function,
    is applied multiple times with the different search window sizes stored in `face_tracker_params::ssize`,
    where the output of the previous stage is used as input to the next. In its simplest
    setting, the `face_tracker_params::ssize` function performs the facial feature
    detection around the current estimate of the shape in the image:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 除了簿记操作，例如设置适当的`跟踪`状态和增加跟踪时间之外，跟踪算法的核心是多级拟合过程，这在前面代码片段中已突出显示。拟合算法在`face_tracker::fit`函数中实现，并多次应用，使用存储在`face_tracker_params::ssize`中的不同搜索窗口大小，其中前一个阶段的输出用作下一个阶段的输入。在其最简单设置中，`face_tracker_params::ssize`函数在图像中当前形状估计的周围执行面部特征检测：
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'It also projects the result onto the face shape''s subspace:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 它还将结果投影到面部形状的子空间：
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: To account for gross outliers in the facial features' detected locations, a
    robust model's fitting procedure can be employed instead of a simple projection
    by setting the `robust` flag to `true`. However, in practice, when using a decaying
    search window size (that is, as set in `face_tracker_params::ssize`), this is
    often unnecessary as gross outliers typically remain far from its corresponding
    point in the projected shape, and will likely lie outside the search region of
    the next level of the fitting procedure. Thus, the rate at which the search region
    size is reduced acts as an incremental outlier rejection scheme.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理面部特征检测位置中的粗大异常值，可以使用鲁棒模型拟合过程而不是简单的投影，通过将`robust`标志设置为`true`来实现。然而，在实践中，当使用衰减的搜索窗口大小时（即，在`face_tracker_params::ssize`中设置），这通常是不必要的，因为粗大异常值通常远离其在投影形状中的对应点，并且可能位于拟合过程下一级的搜索区域之外。因此，搜索区域大小减少的速率充当了一个增量异常值拒绝方案。
- en: Training and visualization
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和可视化
- en: 'Unlike the other classes detailed in this chapter, training a `face_tracker`
    object does not involve any learning process. It is implemented in `train_face_tracker.cpp`
    simply as:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章中详细描述的其他类别不同，训练`face_tracker`对象不涉及任何学习过程。它简单地通过`train_face_tracker.cpp`实现：
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here `arg[1]` to `argv[4]` contain the paths to the `shape_model`, `patch_model`,
    `face_detector`, and `face_tracker` objects, respectively. The visualization for
    the face tracker in `visualize_face_tracker.cpp` is equally simple. Obtaining
    its input image stream either from a camera or video file, through the `cv::VideoCapture`
    class, the program simply loops until the end of the stream or until the user
    presses the Q key, tracking each frame as it comes in. The user also has the option
    of resetting the tracker by pressing the D key at any time.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`arg[1]`到`argv[4]`包含`shape_model`、`patch_model`、`face_detector`和`face_tracker`对象的路径。`visualize_face_tracker.cpp`中面部跟踪器的可视化同样简单。通过`cv::VideoCapture`类从摄像头或视频文件中获取输入图像流，程序简单地循环直到流结束或用户按下Q键，跟踪每个进入的帧。用户还可以通过在任何时候按下D键来重置跟踪器。
- en: Generic versus person-specific models
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用模型与个人特定模型
- en: There are a number of variables in the training and tracking process that can
    be tweaked to optimize the performance for a given application. However, one of
    the primary determinants of tracking quality is the range of shape and appearance
    variability the tracker has to model. As a case in point, consider the generic
    versus person-specific case. A generic model is trained using annotated data from
    multiple identities, expressions, lighting conditions, and other sources of variability.
    In contrast, person-specific models are trained specifically for a single individual.
    Thus, the amount of variability it needs to account for is far smaller. As a result,
    person-specific tracking is often more accurate than its generic counter part
    by a large magnitude.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和跟踪过程中有许多变量可以调整以优化特定应用的性能。然而，跟踪质量的一个主要决定因素是跟踪器需要建模的形状和外观变化范围。作为一个例子，考虑通用模型与个人特定模型的情况。通用模型是使用来自多个身份、表情、光照条件和其他变化来源的标注数据进行训练的。相比之下，个人特定模型是专门为单个个体训练的。因此，它需要考虑的变化量要小得多。因此，个人特定跟踪通常比其通用对应物更准确，差异很大。
- en: 'An illustration of this is shown in the following image. Here the generic model
    was trained using the MUCT dataset. The person-specific model was learned from
    data generated using the annotation tool described earlier in this chapter. The
    results clearly show a substantially better tracking offered by the person-specific
    model, capable of capturing complex expressions and head-pose changes, whereas
    the generic model appears to struggle even for some of the simpler expressions:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了这一点的示例。在这里，通用的模型是使用MUCT数据集进行训练的。个人特定的模型是通过使用本章前面描述的标注工具生成数据学习得到的。结果清楚地表明，个人特定的模型提供了显著更好的跟踪效果，能够捕捉复杂的表情和头部姿态变化，而通用模型甚至在一些简单的表情上似乎也显得有些吃力：
- en: '![](img/image_05_014.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_05_014.jpg)'
- en: It should be noted that the method for face tracking described in this chapter
    is a bare-bones approach that serves to highlight the various components utilized
    in most non-rigid face-tracking algorithms. The numerous approaches to remedy
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，本章中描述的面部跟踪方法是基础的方法，旨在突出大多数非刚性面部跟踪算法中使用的各种组件。为了补救众多方法
- en: some of the drawbacks of this method are beyond the scope of this book and
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法的缺点超出了本书的范围
- en: require specialized mathematical tools that are not yet supported by OpenCV's
    functionality. The relatively few commercial-grade face-tracking software
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 需要专门的数学工具，而这些工具目前尚未得到OpenCV功能的支持。相对较少的商业级面部追踪软件
- en: packages available are testament to the difficulty of this problem in the general
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的包证明了这个问题在一般情况下的难度。
- en: setting. Nonetheless, the simple approach described in this chapter can work
    remarkably well in constrained settings.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。尽管如此，本章中描述的简单方法在受限环境中可以非常出色地工作。
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have built a simple face tracker that can work reasonably
    in constrained settings using only modest mathematical tools and OpenCV''s substantial
    functionality for basic image processing and linear algebraic operations. Improvements
    to this simple tracker can be achieved by employing more sophisticated techniques
    in each of the three components of the tracker: the shape model, the feature detectors,
    and the fitting algorithm. The modular design of the tracker described in this
    section should allow these three components to be modified without substantial
    disruptions to the functionality of the others.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了一个简单的面部追踪器，它仅使用适度的数学工具和OpenCV的基本图像处理和线性代数运算功能，在受限环境中可以合理工作。通过在每个追踪器的三个组成部分（形状模型、特征检测器和拟合算法）中采用更复杂的技术，可以改进这个简单的追踪器。本节中描述的追踪器的模块化设计应允许这三个组件被修改，而不会对其他组件的功能造成重大干扰。
- en: References
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Procrustes Problems, Gower, John C. and Dijksterhuis, Garmt B, Oxford University
    Press, 2004*.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*普鲁克鲁斯特问题，Gower, John C. 和 Dijksterhuis, Garmt B, 牛津大学出版社，2004*.'
