- en: '*Chapter 7*: Applying Machine Learning Algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we studied AWS services for data processing, including
    Glue, Athena, and Kinesis! It is now time to move on to the modeling phase and
    study machine learning algorithms. I am sure that, during the earlier chapters,
    you have realized that building machine learning models requires a lot of knowledge
    about AWS services, data engineering, data exploratory, data architecture, and
    much more. This time, we will go deeper into the algorithms that we have been
    talking about so far and many others.
  prefs: []
  type: TYPE_NORMAL
- en: Having a good sense of the different types of algorithms and machine learning
    approaches will put you in a very good position to make decisions during your
    projects. Of course, this type of knowledge is also crucial to the AWS machine
    learning specialty exam.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that there are thousands of algorithms out there and, by the way,
    you can even propose your own algorithm for a particular problem. Furthermore,
    we will cover the most relevant ones and, hopefully, the ones that you will probably
    face in the exam.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word about ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, let's do it!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter, we will talk about several algorithms, modeling concepts,
    and learning strategies. We think all these topics will be beneficial for you
    during the exam and your data scientist career.
  prefs: []
  type: TYPE_NORMAL
- en: We have structured this chapter in a way so that it covers not only the necessary
    topics of the exam but also gives you a good sense of the most important learning
    strategies out there. For example, the exam will check your knowledge regarding
    the basic concepts of K-means; however, we will cover it on a much deeper level,
    since this is an important topic for your career as a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will follow this approach, looking deeper into the logic of the algorithm,
    for some types of models that we feel every data scientist should master. So,
    keep that in mind: sometimes, we might go deeper than expected in the exam, but
    that will be extremely important for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Many times, during this chapter, we will use the term **built-in algorithms**.
    We will use this term when we want to refer to the list of algorithms that's implemented
    by AWS on their SageMaker SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me give you a concrete example: you can use scikit-learn''s KNN algorithm
    (if you don''t remember what scikit-learn is, refresh your memory by going back
    to [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014), *Machine Learning
    Fundamentals*) to create a classification model and deploy it to SageMaker. However,
    AWS also offers its own implementation of the KNN algorithm on their SDK, which
    is optimized to run in the AWS environment. Here, KNN is an example of a built-in
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the possibilities on AWS are endless, because you can either
    take advantage of built-in algorithms or bring in your own algorithm to create
    models on SageMaker. Finally, just to make this very clear, here is an example
    of how to import a built-in algorithm from the AWS SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You will learn how to create models on SageMaker in [*Chapter 9*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173),
    *Amazon SageMaker Modeling*. For now, just understand that AWS has its own set
    of libraries where those built-in algorithms are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: To train and evaluate a model, you need training and testing data. After instantiating
    your estimator, you should then feed it with those datasets. I don't want to spoil
    [*Chapter 9*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173), *Amazon SageMaker
    Modeling*, but you should know the concepts of **data channels** in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Data channels are configurations related to input data that you can pass to
    SageMaker when you're creating a training job. You should set these configurations
    just to inform SageMaker of how your input data is formatted.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173), *Amazon SageMaker
    Modeling*, you will learn how to create training jobs and how to set data channels.
    As of now, you should know that, while configuring data channels, you can set
    **content types** (**ContentType**) and an **input mode** (**TrainingInputMode**).
    Let's take a closer look at how and where the training data should be stored so
    that it can be integrated properly with AWS's built-in algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, you can use multiple AWS services to prepare data for machine
    learning, such as EMR, Redshift, Glue, and so on. After preprocessing the training
    data, you should store it in S3, in a format expected by the algorithm you are
    using. The following table shows the list of acceptable data formats per algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Data formats that are acceptable per AWS algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Data formats that are acceptable per AWS algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, many algorithms accept text`/.csv` format. Keep in mind that
    you should follow these rules if you want to use that format:'
  prefs: []
  type: TYPE_NORMAL
- en: Your CSV file *can't* have a header record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For supervised learning, the target variable must be in the first column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While configuring the training pipeline, set the input data channel as `content_type`
    equal to `text/csv`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For unsupervised learning, set the `label_size` within the `'content_type=text/csv;label_size=0'`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although text/.csv format is fine for many use cases, most of the time, AWS's
    built-in algorithms work better with **recordIO-protobuf**. This is an optimized
    data format that's used to train AWS's built-in algorithms, where SageMaker converts
    each observation in the dataset into a binary representation that's a set of 4-byte
    floats.
  prefs: []
  type: TYPE_NORMAL
- en: 'RecordIO-protobuf accepts two types of input mode: **pipe mode** and **file
    mode**. In pipe mode, the data will be streamed directly from S3, which helps
    optimize storage. In file mode, the data is copied from S3 to the training instance''s
    store volume.'
  prefs: []
  type: TYPE_NORMAL
- en: We are almost ready! Now, let's have a quick look at some modeling definitions
    that will help you understand some more advanced algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A word about ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start diving into the algorithms, there is an important modeling concept
    that you should be aware of, known as **ensemble**. The term ensemble is used
    to describe methods that use multiple algorithms to create a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, instead of creating just one model to predict fraudulent transactions,
    you could create multiple models that do the same thing and, using a vote sort
    of system, select the predicted outcome. The following table illustrates this
    simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – An example of a voting system on ensemble methods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – An example of a voting system on ensemble methods
  prefs: []
  type: TYPE_NORMAL
- en: The same approach works for regression problems, where, instead of voting, we
    could average the results of each model and use that as the final outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Voting and averaging are just two examples of ensemble approaches. Other powerful
    techniques include **blending** and **stacking**, where you can create multiple
    models and use the outcome of each model as features for a main model. Looking
    back at the preceding table, columns "Model A," "Model B," and "Model C" would
    be used as features to predict the final outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that many machine learning algorithms use ensemble methods while
    training, in an embedded way. These algorithms can be classified into two main
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrapping Aggregation** or **Bagging**: With this approach, several models
    are trained on top of different samples of the data. Then, predictions are made
    through the voting or averaging system. The main algorithm from this category
    is known as **Random Forest**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: With this approach, several models are trained on top of different
    samples of the data. Then, one model tries to correct the error of the next model
    by penalizing incorrect predictions. The main algorithms from this category are
    known as **Stochastic Gradient Boosting** and **AdaBoost**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you know what ensemble models are, let's move on and study some machine
    learning algorithms that are likely to be present in your exam. Not all of them
    use ensemble approaches, but I trust it is going to be easier for you to recognize
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will split the next few sections up based on AWS algorithm categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Textual analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will provide an overview of reinforcement learning in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS provides supervised learning algorithms for general purposes (regression
    and classification tasks) and for more specific purposes (forecasting and vectorization).
    The list of built-in algorithms that can be found in these sub-categories is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear learner algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization machines algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Nearest Neighbor algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepAR Forecasting algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with regression models and the linear learner algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Working with regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Okay; I know that real problems usually aren't linear nor simple. However, looking
    into **linear regression** models is a nice way to figure out what's going on
    inside **regression models** in general (yes, regression models can be linear
    and non-linear). This is mandatory knowledge for every data scientist and can
    help you solve real challenges as well. We'll take a closer look at this in the
    following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing regression algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression models aim to predict a numeric value (Y) according to one
    or more variables (X). Mathematically, we can define such a relationship as Y
    = f(X), where Y is known as the **dependent variable** and X is known as the **independent
    variable**.
  prefs: []
  type: TYPE_NORMAL
- en: With regression models, the component that we want to predict (Y) is always
    a continuous number; for example, the price of houses or the number of transactions.
    We saw that in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*, *Table 2*, when we were choosing the right type
    of supervised learning algorithm, given the target variable. Please, feel free
    to go back and review it.
  prefs: []
  type: TYPE_NORMAL
- en: '*When we use just one variable to predict Y*, we refer to this problem as **simple
    linear regression**. On the other hand, when we use *more than one variable to
    predict Y*, we say that we have a **multiple linear regression** problem.'
  prefs: []
  type: TYPE_NORMAL
- en: There is also another class of regression models, known as **non-linear regression**.
    However, let's put that aside for a moment and understand what simple linear regression
    means.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models belong to the supervised side of machine learning (the other
    side is non-supervised) because algorithms try to predict values according to
    existing correlations between independent and dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what does "f" mean in Y=f(X)? "f" is the regression function responsible
    for predicting Y based on X. In other words, this is the function that we want
    to figure out! When we start talking about simple linear regression, pay attention
    to the next three questions and answers:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the shape of "f" in linear regression?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear, sure!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can we represent a linear relationship?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a *straight* line (you will understand why in a few minutes).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, what's the function that defines a line?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ax + b (just check any *mathematics* book).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That's it! Linear regression models are given by **y = ax + b**. Once we are
    trying to predict Y given X, we just need to find out the values of "a" and "b".
    We can adopt the same logic to figure out what's going on inside other kinds of
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: And believe me, finding out the values of "a" and "b" isn't the only thing we're
    going to do. It's nice to know that "a" is also known as the **alpha coefficient**,
    or **slope**, and represents the line's inclination, while "b" is also known as
    the **beta coefficient**, or **y-intercept**, and represents the place where the
    line crosses the y-axis (into a two-dimensional plan consisting of x and y). You
    will see these two terms in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: It's also nice to know that there is an error associated with every predictor
    that we don't have control. Let's name it "e" and formally define simple linear
    regression as **y = ax + b + e**. Mathematically, this error is expressed by the
    difference between the prediction and the real value.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, let's find alpha and beta and give this section a happy ending!
  prefs: []
  type: TYPE_NORMAL
- en: Least square method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are different ways to find the slope and y-intercept of a line, but the
    most used method is known as the **least square method**. The principle behind
    this method is simple: we have to find the *best line that reduces the sum of
    squared error*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following graph, we can see a Cartesian plane with multiple points and
    lines in it. "Line a" represents the best fit for this data – in other words,
    that would be the best linear regression function for those points. But how do
    I know that? It''s simple: if we compute the error associated with each point
    (like the one we''ve zoomed in on in the following graph), we will realize that
    "line a" contains the least sum of square errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Visualizing the principle of the least square method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Visualizing the principle of the least square method
  prefs: []
  type: TYPE_NORMAL
- en: It is worth understanding linear regression from scratch, not only for the certification
    exam but mainly for your career as a data scientist. To provide you with a complete
    example, we have developed a spreadsheet containing all the calculations that
    we are going to see, step by step! We encourage you to jump on this support material
    and perform some simulations. Let's see this in action.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a linear regression model from scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are going to use a very simple dataset, with only two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X*: Represents the person''s number of years of work experience'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y*: Represents the person''s average salary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to understand the relationship between X and Y and, if possible, predict
    the salary (Y) based on years of experience (X). As I mentioned previously, often,
    real problems have far more independent variables and are not necessarily linear.
    However, I am sure this example will give you the baseline knowledge to master
    more complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out what the alpha and beta coefficients are (or slope and y-intercept,
    if you prefer), we need to find some statistics related to the dataset, so let''s
    take a look at the data and the auxiliary statistics shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 7.4 – Dataset to predict average salary based on amount of work experience'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Dataset to predict average salary based on amount of work experience
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, there is an almost perfect linear relationship between X and
    Y. As the amount of work experience increases, so does the salary. In addition
    to X and Y, we need to compute the following statistics: the number of records,
    the mean of X, the mean of Y, the covariance of X and Y, the variance of X, and
    the variance of Y. The following formulas provide a mathematical representation
    of variance and covariance (respectively), where *x bar*, *y bar*, and *n* represent
    the mean of X, the mean of Y, and the number of records, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image3.jpg)![](img/image4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you want to check the calculation details of the formulas for each of those
    auxiliary statistics in *Table 7.2*, please refer to the support material provided
    along with this book. There, you will find those formulas already implemented
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'These statistics are important because they will be used to compute our alpha
    and beta coefficients. The following image explains how we are going to compute
    both coefficients, along with the correlation coefficients **R** and **R squared**.
    These last two metrics will give us an idea about the quality of the model, where
    the closer the model is to 1, the better the model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Equations to calculate coefficients for simple linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – Equations to calculate coefficients for simple linear regression
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying these formulas, we will come up with the results shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Finding regression coefficients'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Finding regression coefficients
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding table already contains all the information that we need to make
    predictions on top of the new data. If we replace the coefficients in the original
    equation, **y = ax + b + e**, we will find the regression formula to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y = 1021,212 * X + 53,3
  prefs: []
  type: TYPE_NORMAL
- en: 'From this point on, to make predictions, all we have to do is replace X with
    the number of years of experience. As a result, we will find Y, which is the projected
    salary. We can see the model fit in the following graph and some model predictions
    in *Figure 7.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Fitting data in the regression equation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Fitting data in the regression equation
  prefs: []
  type: TYPE_NORMAL
- en: 'We see the prediction values here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Model predictions
  prefs: []
  type: TYPE_NORMAL
- en: While you are analyzing regression models, you should be able to understand
    whether your model is of a good quality or not. We talked about many modeling
    issues (such as overfitting) in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*, and you already know that you always have to
    check model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good approach to regression models is performing what is called **residual
    analysis**. This is where we plot the errors of the model in a scatter plot and
    check if they are randomly distributed (as expected) or not. If the errors are
    *not* randomly distributed, this means that your model was unable to generalize
    the data. The following graph shows a residual analysis of our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Residual analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Residual analysis
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway here is that the errors are randomly distributed. Such evidence,
    along with a high R squared rating, can be used as arguments to support the use
    of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162), *Evaluating
    and Optimizing Models*, we will learn about evaluation metrics. For instance,
    we will learn that each type of model may have its own set of evaluation metrics.
    Regression models are commonly evaluated with **Mean Square Error** (**MSE**)
    and **Root Mean Square Error** (**RMSE**). In other words, apart from R, R squared,
    and residual analysis, ideally, you will execute your model on test sets to extract
    other performance metrics. You can even use a cross-validation system to check
    model performance, as we learned in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*.
  prefs: []
  type: TYPE_NORMAL
- en: Very often, when the model residuals *do* present a pattern and are *not* randomly
    distributed, it's because the existing relationship in the data is not linear,
    but non-linear, so another modeling technique must be applied. Now, let's take
    a look at how we can interpret a model's results.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting regression models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is also good to know how to interpret a linear regression model. Sometimes,
    we use linear regression not necessarily to create a predictive model but to do
    a regression analysis, where we can understand the relationship between the independent
    and dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking back at our regression equation (Y = 1021,212 * X + 53,3), we can see
    our two terms: alpha or slope (1021.2) and beta or y-intercept (53.3). We can
    interpret this model as follows: *for each additional year of working experience,
    you will increase your salary by $1,021.3 dollars*. Also, note that when "years
    of experience" is equal to zero, the expected salary is going to be $53.3 dollars
    (this is the point where our straight line crosses the y-axis).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a generic perspective, your regression analysis should answer the following
    question: for each extra unit that''s added to the independent variable (slope),
    what is the average change in the dependent variable? Please take notes and make
    sure you know how to interpret simple linear regression models – this is a very
    important thing to do as one of your daily activities as a data scientist! Let''s
    move on and take a look at some final considerations about linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Checking R squared adjusted
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, I hope you have a much better idea of regression models! There
    is just another very important topic that you should be aware of, regardless of
    whether it will come up in the exam or not, which is the parsimony aspect of your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: We have talked about parsimony already, in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*. This is your ability to prioritize simple models
    over complex models. Looking into regression models, you might have to use more
    than one feature to predict your outcome. This is also known as a multiple regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: When that's the case, the R and R squared coefficients tend to reward more complex
    models that have more features. In other words, if you keep adding new features
    to a multiple regression model, you will come up with higher R and R squared coefficients.
    That's why you *can't* anchor your decisions *only* based on those two metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another additional metric that you could use (apart from R, R squared, MSE,
    and RMSE) is known as **R squared adjusted**. This metric is penalized when we
    add extra features to the model that do not bring any real gain. In the following
    table, we have illustrated a hypothetical example just to show you when you are
    starting to lose parsimony:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Comparing R squared and R squared adjusted'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Comparing R squared and R squared adjusted
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can conclude that maintaining three variables in the model is better
    than maintaining four or five variables. Adding four or five variables to that
    model will increase R squared (as expected), but decrease R squared adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: Alright; at this point, you should have a very good understanding of regression
    models. Now, let's check what AWS offers in terms of built-in algorithms for this
    class of models. That is going to be important for your exam, so let's take a
    look.
  prefs: []
  type: TYPE_NORMAL
- en: Regression modeling on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS has a built-in algorithm known as **linear learner**, where we can implement
    linear regression models. The built-in linear learner uses **Stochastic Gradient
    Descent** (**SGD**) to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We will learn more about SGD when we talk about neural networks. For now, we
    can look at SGD as an alternative to the popular least square error method that
    we just dissected.
  prefs: []
  type: TYPE_NORMAL
- en: The linear learn built-in algorithm provides a hyperparameter that can apply
    normalization to the data, prior to the training process. The name of this hyperparameter
    is `normalize_data`. This is very helpful since linear models are sensitive to
    the scale of the data and usually take advantage of data normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We talked about data normalization in [*Chapter 3*](B16735_03_Final_VK_ePub.xhtml#_idTextAnchor059),
    *Data Preparation and Transformation*. Please review that chapter if you need
    to.
  prefs: []
  type: TYPE_NORMAL
- en: Some other important hyperparameters of the linear learner algorithm are **L1**
    and **wd**, which play the roles of **L1 regularization** and **L2 regularization**,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 regularization help the linear learner (or any other regression algorithm
    implementation) avoid overfitting. Conventionally, we call regression models that
    implement L1 regularization **Lasso Regression** models, while for regression
    models with L2 regularization, we call them **Ridge Regression** models.
  prefs: []
  type: TYPE_NORMAL
- en: Although it might sound complex, it is not! Actually, the regression model equation
    is still the same; that is, **y = ax + b + e**. The change is in the loss function,
    which is used to find the coefficients that best minimize the error. If you look
    back at *Figure 7.3*, you will see that we have defined the error function as
    **e = (ŷ - y)2**, where **ŷ** is the regression function value and **y** is
    the real value.
  prefs: []
  type: TYPE_NORMAL
- en: 'L1 and L2 regularization add a penalty term to the loss function, as shown
    in the following formulas (note that we are replacing ŷ with ax + b):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The λ (lambda) parameter must be greater than 0 and manually tuned. A very high
    lambda value may result in an underfitting issue, while a very low lambda may
    not result in expressive changes in the final results (if your model is overfitted,
    it will stay overfitted).
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, the main difference between L1 and L2 regularization is
    that L1 will shrink the less important coefficients to zero, which will force
    the feature to be dropped (acting as a feature selector). In other words, if your
    model is overfitting due to it having a high number of features, L1 regularization
    should help you solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: During your exam, remember the basis of L1 and L2 regularization, especially
    the key difference between them, where L1 works well as a feature selector.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, many built-in algorithms can serve multiple modeling purposes.
    The linear learner algorithm can be used for regression, binary classification,
    and multi-classification. Make sure you remember this during your exam (it is
    *not only* about regression models).
  prefs: []
  type: TYPE_NORMAL
- en: Still going in that direction, AWS has other built-in algorithms that work for
    regression and classification problems; that is, **Factorization Machines**, **K-Nearest
    Neighbor** (**KNN**) and the **XGBoost** algorithm. Since these algorithms can
    also be used for classification purposes, we'll cover them in the section about
    classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'You just got a very important tip to remember during the exam: linear learner,
    Factorization Machines, K-Nearest Neighbor, and XGBoost are suitable for both
    regression and classification problems. These algorithms are often known as algorithms
    for general purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we have reached the end of this section on regression models. I
    hope you have enjoyed it; remember to check out our support material before you
    take your exam. By the way, you can use that reference material when you''re working
    on your daily activities! Now, let''s move on to another classical example of
    a machine learning problem: classification models.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with classification models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have been learning what classification models are throughout this book.
    However, now, we are going to discuss some algorithms that are suitable for classification
    problems. Keep in mind that there are hundreds of classification algorithms out
    there, but since we are preparing for the AWS machine learning specialty exam,
    we will cover the ones that have been pre-built by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: We already know what the linear learner does (and we know that it is suitable
    for both regression and classification tasks), so let's take a look at the other
    built-in algorithms for general purposes (which includes classification tasks).
  prefs: []
  type: TYPE_NORMAL
- en: We will start with **Factorization Machines**. Factorization machines are considered
    an extension of the linear learner, optimized to find the relationship between
    features within high-dimensional sparse datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A very traditional use case for Factorization machines is *recommendation systems*,
    where we usually have a high level of sparsity in the data. During the exam, if
    you are faced with a general-purpose problem (either a regression or binary classification
    task) where the underlying datasets are sparse, then Factorization Machines are
    probably the best answer from an algorithm perspective.
  prefs: []
  type: TYPE_NORMAL
- en: When we use Factorization Machines in a regression model, the **Root Mean Square
    Error** (**RMSE**) will be used to evaluate the model. On the other hand, in binary
    classification mode, the algorithm will use Log Loss, Accuracy, and F1 score to
    evaluate results. We will have a deeper discussion about evaluation metrics in
    [*Chapter 8*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162), *Evaluating and
    Optimizing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware that Factorization Machines only accept input data in **recordIO-protobuf**
    format. This is because of the data sparsity issue, in which recordIO-protobuf
    is supposed to do a better job on data processing than text/.csv format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next built-in algorithm suitable for classification problems is known as
    K-Nearest Neighbors, or KNN for short. As the name suggests, this algorithm will
    try to find the *K* closest points to the input data and return either of the
    following predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: The most repeated class of the k closest points, if it's a classification task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average value of the label of the k closest points, if it's a regression
    task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We say that KNN is an **index-based algorithm** because it computes distances
    between points, assigns indexes for these points, and then stores the sorted distances
    and their indexes. With that type of data structure, KNN can easily select the
    top K closest points to make the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Note that K is a hyperparameter of KNN and should be optimized during the modeling
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The other AWS built-in algorithm available for general purposes, including classification,
    is known as **eXtreme Gradient Boosting**, or **XGBoost** for short. This is an
    ensemble, decision tree-based model.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost uses a set of **weaker** models (decision trees) to predict the target
    variable, which can be a regression task, binary class, or multi-class. This is
    a very popular algorithm and has been used in machine learning competitions by
    the top performers.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost uses a boosting learning strategy when one model tries to correct the
    error of the prior model. It carries the name "gradient" because it uses the gradient
    descent algorithm to minimize the loss when adding new trees.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The term weaker is used in this context to describe very simple decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Although XGBoost is much more robust than a single decision tree, it is important
    to go into the exam with a clear understanding of what decision trees are and
    their main configurations. By the way, they are the base model of many ensemble
    algorithms, such as AdaBoost, Random Forest, Gradient Boost, and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees are rule-based algorithms that organize decisions in the form
    of a tree, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Example of what a decision tree model looks like'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.11 – Example of what a decision tree model looks like
  prefs: []
  type: TYPE_NORMAL
- en: They are formed by a root node (at the very top of the tree), intermediary or
    decision nodes (in the middle of the tree), and leaf nodes (bottom nodes with
    no splits). The depth of the tree is given by the difference between the root
    node and the very last leaf node. For example, in the preceding diagram, the depth
    of the tree is 3\.
  prefs: []
  type: TYPE_NORMAL
- en: The depth of the tree is one of the most important hyperparameters of this type
    of model and it is often known as the **max depth**. In other words, max depth
    controls the maximum depth that a decision tree can reach.
  prefs: []
  type: TYPE_NORMAL
- en: Another very important hyperparameter of decision tree models is known as the
    minimum number of samples/observations in the leaf nodes. It is also used to control
    the growth of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have many other types of hyperparameters, but these two are especially
    important for controlling how the model overfits. Decision trees with a high depth
    or very small number of observations in the leaf nodes are likely to face issues
    during extrapolation/prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for this is simple: decision trees use data from the leaf nodes
    to make predictions, based on the proportion (for classification tasks) or average
    value (for regression tasks) of each observation/target variable that belongs
    to that node. Thus, the node should have enough data to make good predictions
    outside the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: In case you face the term **CART** during the exam, you should know that it
    stands for **Classification and Regression Trees**, since decision trees can be
    used for classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To select the best variables to split the data in the tree, the model will choose
    the ones that maximize the separation of the target variables across the nodes.
    This task can be performed by different methods, such as **Gini** and **Information
    Gain**.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Time series**, or **TS** for short, refers to data points that are collected
    on a regular basis with an ordered dependency. Time series have a measure, a fact,
    and a time unit, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Time series statement'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – Time series statement
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, time series can be classified as **univariate** or **multivariate**.
    Univariate time series have just one variable connected across a period of time,
    while a multivariate time series have two or more variables connected across a
    period of time. The following graph shows the univariate time series we showed
    in the preceding image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Time series example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Time series example
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series can be decomposed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observed** or **level**: The average values of the series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trend**: Increasing, decreasing pattern (sometimes, there is no trend)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality**: Regular peaks at specific periods of time (sometimes, there
    is no seasonality)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise**: Something we cannot explain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sometimes, we can also find isolated peaks in the series that cannot be captured
    in a forecasting model. In such cases, we might want to consider those peaks as
    outliers. The following is a decomposition of the time series shown in the preceding
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Time series decomposition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Time series decomposition
  prefs: []
  type: TYPE_NORMAL
- en: It's also worth highlighting that we can use **additive** or **multiplicative**
    approaches to decompose time series. Additive models suggest that your time series
    *adds* each component to explain the target variable; that is, **y(t) = level
    + trend + seasonality + noise**.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplicative models, on the other hand, suggest that your time series *multiplies*
    each component to explain the target variable; that is, **y(t) = level * trend
    * seasonality * noise**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will have a closer look at time series components.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the stationarity of time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decomposing time series and understanding how their components interact with
    additive and multiplicative models is a great achievement! However, the more we
    learn, the more we question ourselves. Maybe you have realized that time series
    without trend and seasonality are easier to predict than the ones with all those
    components!
  prefs: []
  type: TYPE_NORMAL
- en: That is naturally right. If you don't have to understand trend and seasonality,
    and if you don't have control over the noise, all you have to do is explore the
    observed values and find their regression relationship.
  prefs: []
  type: TYPE_NORMAL
- en: We refer to time series with constant mean and variance across a period of time
    as **stationary**. In general, time series *with* trend and seasonality are *not*
    stationary. It is possible to apply data transformations to the series to transform
    it into a stationary time series so that the modeling task tends to be easier.
    This type of transformation is known as **differentiation**.
  prefs: []
  type: TYPE_NORMAL
- en: While you are exploring a time series, you can check stationarity by applying
    hypothesis tests, such as **Dickey-Fuller**, **KPSS**, and **Philips-Perron**,
    just to mention a few. If you find it non-stationary, then you can apply differentiation
    to make it a stationary time series. Some algorithms already have that capability
    embedded.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring, exploring, and exploring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, I am sure I don't have to remind you that exploration tasks happen
    all the time in data science. Nothing is different here. While you are building
    time series models, you might want to have a look at the data and check if it
    is suitable for this type of modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Autocorrelation plots** are one of the tools that you can use for time series
    analysis. Autocorrelation plots allow you to check the correlations between lags
    in the time series. The following graph shows an example of this type of visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Autocorrelation plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 – Autocorrelation plot
  prefs: []
  type: TYPE_NORMAL
- en: Remember, if you're playing with univariate time series, your time series just
    has one variable, so finding autocorrelation across the lags of your unique variable
    is crucial to understanding whether you can build a good model or not.
  prefs: []
  type: TYPE_NORMAL
- en: And yes, it turns out that, sometimes, it might happen that you don't have a
    time series in front of you. Furthermore, no matter your efforts, you will not
    be able to model this data as a time series. This type of data is often known
    as **white noise**.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of series that we cannot predict is known as **random walk**. Random
    walks are random by nature, but they have a dependency on the previous time step.
    For example, the next point of a random walk could be a random number between
    0 and 1, and also the last point of the series.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Be careful if you come across those terms in the exam and remember to relate
    them to randomness in time series.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have covered the main theory about time series modeling. You should
    also be aware that the most popular algorithms out there for working with time
    series are known as **Auto-Regressive Integrated Moving Average** (**ARIMA**)
    and **Exponential Smoothing** (**ETS**). We will not look at the details of these
    two models. Instead, we will see what AWS can offer us in terms of time series
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DeepAR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **DeepAR** forecasting algorithm is a built-in SageMaker algorithm that's
    used to forecast a one-dimensional time series using a **Recurrent Neural Network**
    (**RNN**).
  prefs: []
  type: TYPE_NORMAL
- en: Traditional time series algorithms, such as ARIMA and ETS, are designed to fit
    one model per time series. For example, if you want to forecast sales per region,
    you might have to create one model per region, since each region might have its
    own sales behaviors. DeepAR, on the other hand, allows you to operate more than
    one time series in a single model, which seems to be a huge advantage for more
    complex use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data for DeepAR, as expected, is one *or more* time series. Each
    of these time series can be associated with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A vector of static (time-independent) categorical features, controlled by the
    `cat` field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a vector of dynamic (time-dependent) time series, controlled by `dynamic_feat`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that the ability to train and make predictions on top of multiple time
    series is strictly related to the vector of static categorical features. While
    defining the time series that DeepAR will train on, you can set categorical variables
    to specify which group each time series belongs to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Two of the main hyperparameters of DeepAR are `context_length`, which is used
    to control how far in the past the model can see during the training process,
    and `prediction_length`, which is used to control how far in the future the model
    will output predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepAR can also handle missing values, which, in this case, refers to existing
    gaps in the time series. A very interesting functionality of DeepAR is its ability
    to create derived features from time series. These derived features, which are
    created from basic time frequencies, help the algorithm learn time-dependent patterns.
    The following table shows all the derived features created by DeepAR, according
    to each type of time series that it is trained on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – DeepAR derived features per frequency of time series'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.16 – DeepAR derived features per frequency of time series
  prefs: []
  type: TYPE_NORMAL
- en: We have now completed this section about forecasting models. Next, we will have
    a look at the last algorithm regarding supervised learning; that is, the **Object2Vec**
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Object2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Object2Vec is a built-in SageMaker algorithm that generalizes the well-known
    **word2vec** algorithm. Object2Vec is used to create **embedding spaces** for
    high multidimensional objects. These embedding spaces are, per the definition,
    compressed representatiosn of the original object and can be used for multiple
    purposes, such as feature engineering or object comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – A visual example of an embedding space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.17 – A visual example of an embedding space
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram illustrates what we mean by an embedding space. The first
    and the last layers of the neural network model just map the input data with itself
    (represented by the same vector size).
  prefs: []
  type: TYPE_NORMAL
- en: As we move on to the internal layers of the model, the data is compressed more
    and more until it hits the layer in the middle of this architecture, known as
    the embedding layer. On that particular layer, we have a smaller vector, which
    aims to be an accurate and compressed representation of the high-dimensional original
    vector from the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we just completed our first section about machine learning algorithms
    in AWS. Coming up next, we will have a look at some unsupervised algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS provides several unsupervised learning algorithms for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimension reduction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattern recognition:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anomaly detection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Cut Forest Algorithm** (**RCF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start by talking about clustering and how the most popular clustering
    algorithm works: K-means.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering algorithms are very popular in data science. Basically, they aim
    to identify groups in a given dataset. Technically, we call these findings or
    groups **clusters**. Clustering algorithms belong to the field of non-supervised
    learning, which means that they don't need a label or response variable to be
    trained.
  prefs: []
  type: TYPE_NORMAL
- en: This is just fantastic because labeled data used to be scarce. However, it comes
    with some limitations. The main one is that clustering algorithms provide clusters
    for you, but not the meaning of each cluster. Thus, someone, as a subject matter
    expert, has to analyze the properties of each cluster to define their meanings.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of clustering approaches, such as hierarchical clustering
    and partitional clustering. Inside each approach, we will find several algorithms.
    However, **K-Means** is probably the most popular clustering algorithm and you
    are likely to come across it in your exam, so let's take a closer look at it.
  prefs: []
  type: TYPE_NORMAL
- en: When we are playing with K-means, somehow, we have to specify the number of
    clusters that we want to create. Then, we have to allocate the data points across
    each cluster so that each data point will belong to a single cluster. This is
    exactly what we expect as a result at the end of the clustering process!
  prefs: []
  type: TYPE_NORMAL
- en: You, as a user, have to specify the number of clusters you want to create and
    pass this number to K-means. Then, the algorithm will randomly initiate the central
    point of each cluster, which is also known as **centroid** initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the centroids of each cluster, all we need to do is assign a cluster
    to each data point. To do that, we have to use a proximity or distance metric!
    Let's adopt the term distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: The **distance metric** is responsible for calculating the distance between
    data points and centroids. The data point will belong to the closer cluster centroid,
    according to the distance metric!
  prefs: []
  type: TYPE_NORMAL
- en: 'The most well-known and used distance metric is called **Euclidean distance**
    and the math behind it is really simple: imagine that the points of your dataset
    are composed of two dimensions, X and Y. So, we could consider points a and b
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: a (X=1, Y=1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b (X=2, Y=5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Euclidean distance between points a and b is given by the following formula,
    where X1 and Y1 refer to the values of point a and X2 and Y2 refer to the values
    of point b:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The same function can be generalized by the following equation:![](img/image18.png)
  prefs: []
  type: TYPE_NORMAL
- en: Once we have completed this process and assigned a cluster with each data point,
    we have to recalculate the cluster centroids. This process can be done by different
    methods, such as **single link**, **average link**, and **complete link**.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this centroid refreshment, we will have to keep checking the closest
    cluster for each data point and keep refreshing the centroids. We have to reexecute
    steps 1 and 2 **iteratively**, until the cluster centroids converge or the maximum
    number of allowed iterations is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright; let''s recap the components that compose the K-means method:'
  prefs: []
  type: TYPE_NORMAL
- en: Centroid initialization, cluster assignment, centroid refreshment, and then
    redo the last two steps until it converges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The algorithm itself:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A distance metric to assign data points to each cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have selected Euclidian distance here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And a linkage method to recalculate the cluster centroids:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sake of our demonstration, we'll select the average linkage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these definitions, we are ready to walk through our real example, step
    by step. As with our regression models, some support material is also available
    for your reference.
  prefs: []
  type: TYPE_NORMAL
- en: Computing K-means step by step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will simulate K-means in a very small dataset, with only
    two columns (x and y) and six data points (A, B, C, D, E, F), as defined in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Input data for K-means'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.18 – Input data for K-means
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding table, we created three clusters with the following centroids:
    (1,1), (2,2), (5,5). The number of clusters (3) was defined *a priori* and the
    centroid for each cluster was randomly defined. The following graph shows the
    stage of the algorithm we are at right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Plotting the K-means results before completing the first iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.19 – Plotting the K-means results before completing the first iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can''t see points A, B, and C since they overlap with cluster centroids,
    but don''t worry – they will appear soon. What we have to do now is compute the
    distance of each data point to each cluster centroid. Then, we need to choose
    the cluster that is the closest to each point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Processing iteration 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.20 – Processing iteration 1
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding table, we have the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Each row represents a data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first six columns represent the centroid axis (x and y) of each cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next three columns represent the distance of each data point to each cluster
    centroid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last column represents the clusters that are the closest to each data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at data point A (first row), we can see that it was assigned to cluster
    1 because the distance from data point A to cluster 1 is 0 (remember when I told
    you they were overlapping?). The same calculation happens to all other data points
    to define a cluster for each data point.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, you might want to see how we computed those distances between
    the clusters and the data points. As we stated previously, we have used the Euclidian
    distance, so let's see that in action. For demonstration purposes, let's check
    out how we came up with the distance between data point A and cluster 3 (the first
    row in *Figure 7.20*, column `distance-c3`, value 5,7).
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we applied the same equation from the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image171.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: X1 = X of data point A = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y1 = Y of data point A = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X2 = X of cluster 3 = 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y2 = Y of cluster 3 = 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying the formula step by step, we will come up with the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Computing the Euclidian distance step by step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.21 – Computing the Euclidian distance step by step
  prefs: []
  type: TYPE_NORMAL
- en: 'That is just fantastic, isn''t it? We have almost completed the first iteration
    of K-means. In the very last step of iteration 1, we have to refresh the cluster
    centroids. Remember: initially, we randomly defined those centroids, but now,
    we have just assigned some data points to each cluster, which means we should
    be able to identify where the central point of the cluster is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we have defined that we are going to use the **average linkage**
    method to refresh the cluster centroids. This is a very simple step, and the results
    are present in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – K-means results after iteration 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.22 – K-means results after iteration 1
  prefs: []
  type: TYPE_NORMAL
- en: The preceding table shows the same data points that we are dealing with (by
    the way, they will never change), and the centroids of clusters 1, 2, and 3\.
    Those centroids are quite different from what they were initially, as shown in
    *Figure 7.18*. This is because they were refreshed using average linkage! The
    method got the average value of all the x and y values of the data points of each
    cluster. For example, let's see how we came up with (1.5, 3.5) as centroids of
    cluster 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at *Figure 7.20*, you will see that cluster 2 only has two data
    points assigned to it: B and E. These are the second and fifth rows in that image.
    If we take the average values of the x-axis of each point, then we''ll have **(2
    + 1) / 2 = 1.5** and **(2 + 5) / 2 = 3.5**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we are done with iteration 1 of K-means and we can view the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Plotting the K-means results after the first iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.23 – Plotting the K-means results after the first iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can see almost all the data points, except for data point A because
    it is still overlapping with the centroid of cluster 1\. Moving on, we have to
    redo the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Recalculate the distance between each data point and each cluster centroid and
    reassign clusters, if needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recalculate the cluster centroids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We do those two tasks many times until the cluster centroids converge and they
    don''t change anymore *or* we reach the maximum number of allowed iterations,
    which can be set as a hyperparameter of K-means. For demonstration purposes, after
    four iterations, our clusters will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Plotting the K-means results after the fourth iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.24 – Plotting the K-means results after the fourth iteration
  prefs: []
  type: TYPE_NORMAL
- en: On the fourth iteration, our cluster centroids look pretty consistent, and we
    can clearly see that we can group our six data points according to their proximity.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have only set two dimensions for each data point (dimension
    x and y). In real use cases, we can see far more dimensions, and that's why clustering
    algorithms play a very important role in identifying groups in the data in a more
    automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have enjoyed how to compute K-means from scratch! I am sure this
    knowledge will be beneficial for your exam and for your career as a data scientist.
    By the way, I have told you many times that data scientists must be skeptical
    and curious, so you might be wondering why we defined three clusters in this example
    and not two or four. You may also be wondering how we measure the quality of the
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: You didn't think I wouldn't explain this to you, did you? In the next section,
    we will clarify those points together.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of clusters and measuring cluster quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although K-means is a great algorithm for finding patterns in your data, it
    will not provide the meaning of each cluster, nor the number of clusters you have
    to create to maximize cluster quality.
  prefs: []
  type: TYPE_NORMAL
- en: In clustering, cluster quality means that we want to create groups with a high
    homogeneity among the elements of the same cluster, and a high heterogeneity among
    the elements of different clusters. In other words, the elements of the same clusters
    should be close/similar, whereas the elements of different clusters should be
    well separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to compute the cluster''s homogeneity is by using a metric known as
    **Sum of Square Errors**, or **SSE** for short. This metric will compute the sum
    of squared differences between each data point and its cluster centroid. For example,
    when all the data points are located at the same point where the cluster centroid
    is, then SSE will be 0\. In other words, we want to minimize SSE. The following
    equation formally defines SSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know how to check cluster quality, it is easier to understand how
    to define the number of appropriated clusters for a given dataset. All we have
    to do is find several clusters that minimize SSE. A very popular method that works
    around that logic is known as the **Elbow method**.
  prefs: []
  type: TYPE_NORMAL
- en: The Elbow method proposes executing the clustering algorithm many times. In
    each execution, we will test a different number of clusters, *k*. After each execution,
    we compute the SSE related to that *k* number of cluster clusters. Finally, we
    can plot these results and select the number of *k* where the SSE stops to drastically
    decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Adding more clusters will naturally decrease the SSE. In the Elbow method, we
    want to find the point where that change becomes smoother.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, we decided to create three clusters. The following
    graph shows the Elbow analysis that supports this decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – The Elbow method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.25 – The Elbow method
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that adding more than three or four clusters will add unnecessary
    complexity to the clustering process.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you should always consider the business background while defining
    the number of clusters. For example, if you are creating a customer segmentation
    model and your company has prepared the commercial team and business processes
    to support four segments of customers, considering the preceding graph, there
    is no harm in setting four clusters instead of three.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should know that AWS has implemented K-means as part of their list
    of built-in algorithms. In other words, you don't have to use external libraries
    or bring your own algorithm to play with K-means on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'That was a really good accomplishment: you just mastered the basics of clustering
    algorithms and you should now be able to drive your own projects and research
    about this topic! For the exam, remember that clustering belongs to the unsupervised
    field of machine learning, so there is no need to have labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, make sure that you know how the most popular algorithm of this field works;
    that is, K-means. Although clustering algorithms do not provide the meaning of
    each group, they are very powerful for finding patterns in the data, either to
    model a particular problem or just to explore the data.
  prefs: []
  type: TYPE_NORMAL
- en: Coming up next, we'll keep studying unsupervised algorithms and see how AWS
    has built one of the most powerful algorithms out there for anomaly detection,
    known as **Random Cut Forest** (**RCF**).
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding anomalies in data is very common in modeling and data exploratory analysis.
    Sometimes, you might want to find anomalies in the data just to remove them before
    fitting a regression model, while other times, you might want to create a model
    that identifies anomalies as an end goal, for example, in fraud detection systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we can use many different methods to find anomalies in the data. With
    some creativity, the possibilities are endless. However, there is a particular
    algorithm that works around this problem that you should definitely be aware of
    for your exam: **Random Cut Forest** (**RCF**).'
  prefs: []
  type: TYPE_NORMAL
- en: RCF is an unsupervised decision tree-based algorithm that creates multiple decision
    trees (forests) using random subsamples of the training data. Technically, it
    randomizes the data and then creates samples according to the number of trees.
    Finally, these samples are distributed across each tree.
  prefs: []
  type: TYPE_NORMAL
- en: These sets of trees are used to assign an anomaly score tp the data points.
    This anomaly score is defined as the expected change in the complexity of the
    tree as a result of adding that point to the tree.
  prefs: []
  type: TYPE_NORMAL
- en: The most important hyperparameters of RCF are `num_trees` and `num_samples_per_tree`,
    which are the number of trees in the forest and the number of samples per tree,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another unsupervised algorithm that's implemented by AWS in their list of built-in
    algorithms is known as **Principal Component Analysis**, or **PCA** for short.
    PCA is a technique that's used to reduce the number of variables/dimensions in
    a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind PCA is plotting the data points to another set of coordinates,
    known as **principal components** (**PC**), which aims to explain the most variance
    in the data. By definition, the first component will capture more variance than
    the second component, then the second component will capture more variance than
    the third one, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set up as many principal components as you need as long as it does
    not surpass the number of variables in your dataset. The following graph shows
    how these principal components are drawn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Finding principal components in PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_07_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.26 – Finding principal components in PCA
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, the first principal component will be drawn in such
    a way that it will capture most of the variance in the data. That's why it passes
    close to the majority of the data points in the preceding graph.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the second principal component will be perpendicular to the first one,
    so that it will be the second component that explains the variance in the data.
    If you want to create more components (consequentially, capturing more variance),
    you just have to follow the same rule of adding perpendicular components. **Eigenvectors**
    and **eigenvalues** are the linear algebra concepts associated with PCA that compute
    the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: So, what's the story with dimension reduction here? In case it is not clear
    yet, these principal components can be used to replace your original variables.
    For example, let's say you have 10 variables in your dataset and you want to reduce
    this dataset to three variables that best represent the others. A potential solution
    for that would be applying PCA and extracting the first three principal components!
  prefs: []
  type: TYPE_NORMAL
- en: Do these three components explain 100% of your dataset? Probably not, but ideally,
    they will explain most of the variance. Adding more principal components will
    explain more variance, but at the cost of you adding extra dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS's built-in algorithm for PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In AWS, PCA works in two different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular**: For datasets with a moderate number of observations and features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Randomized**: For datasets with a large number of observations and features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference is that, in randomized mode, it is used an approximation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the main hyperparameter of PCA is the number of components that you
    want to extract, known as `num_components`.
  prefs: []
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IP Insights is an unsupervised algorithm that's used for pattern recognition.
    Essentially, it learns the usage pattern of IPv4 addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modus operandi of this algorithm is very intuitive: it is trained on top
    of pairs of events in the format of entity and IPv4 address so that it can understand
    the pattern of each entity that it was trained on.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can understand "entity" as user IDs or account numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Then, to make predictions, it receives a pair of events with the same data structure
    (entity, IPv4 address) and returns an anomaly score for that particular IP address
    regarding the input entity.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This anomaly score that's returned by IP Insight infers how anomalous the pattern
    of the event is.
  prefs: []
  type: TYPE_NORMAL
- en: We might come across many applications with IP Insights. For example, you can
    create an IP Insights model that was trained on top of your application login
    events (this is your entity). You should be able to expose this model through
    an API endpoint to make predictions in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Then, during the authentication process of your application, you could call
    your endpoint and pass the IP address that is trying to log in. If you got a high
    score (meaning this pattern of logging in looks anomalous), you can request extra
    information before authorizing access (even if the password was right).
  prefs: []
  type: TYPE_NORMAL
- en: This is just one of the many applications of IP Insights you could think about.
    Next, we will discuss textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Textual analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern applications use **Natural Language Processing** (**NLP**) for several
    purposes, such as **text translation**, **document classifications**, **web search**,
    **named entity recognition** (**NER**), and many others.
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a suite of algorithms for most NLP use cases. In the next few subsections,
    we will have a look at these built-in algorithms for textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Blazing Text algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Blazing Text does two different types of tasks: text classification, which
    is a supervised learning approach that extends the **fastText** text classifier,
    and **word2vec**, which is an unsupervised learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The Blazing Text's implementations of these two algorithms are optimized to
    run on large datasets. For example, you can train a model on top of billions of
    words in a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This scalability aspect of Blazing Text is possible due to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Its ability to use multi-core CPUs and a single GPU to accelerate text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its ability to use multi-core CPUs or GPUs, with custom CUDA kernels for GPU
    acceleration, when playing with the word2vec algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word2vec option supports a **batch_skipgram** mode, which allows Blazing
    Text to do distributed training across multiple CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The distributed training that's performed by Blazing Text uses a mini-batching
    approach to convert **level-1 BLAS** operations into **level-3 BLAS** operations.
    If you see these terms during your exam, you should know that they are related
    to Blazing Text in terms of word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: Still in word2vec mode, Blazing Text supports both the **skip-gram** and **continuous
    bag of words** (**CBOW**) architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last but not least, note the following configurations of Blazing Text, since
    they are likely to be present in your exam:'
  prefs: []
  type: TYPE_NORMAL
- en: In word2vec mode, only the train channel is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blazing Text expects a single text file with space-separated tokens. Each line
    of the file must contain a single sentence. This means you usually have to pre-process
    your corpus of data before using Blazing Text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a supervised algorithm that transforms an input sequence into an output
    sequence. This sequence can be a text sentence or even an audio recording.
  prefs: []
  type: TYPE_NORMAL
- en: The most common use cases for sequence-to-sequence are machine translation,
    text summarization, and speech-to-text. Anything that you think is a sequence-to-sequence
    problem can be approached by this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, AWS SageMaker''s Seq2Seq uses two types of neural networks to
    create models: a **Recurrent Neural Network** (**RNN**) and a **Convolutional
    Neural Network** (**CNN**) with an attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation**, or **LDA** for short, is used for topic modeling.
    Topic modeling is a textual analysis technique where you can extract a set of
    topics from a corpus of text data. LDA learns these topics based on the probability
    distribution of the words in the corpus of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an unsupervised algorithm, there is no need to set a target variable.
    Also, the number of topics must be specified up-front and you will have to analyze
    each topic to find their domain meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Topic Model (NTM) algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like the LDA algorithm, the **Neural Topic Model** (**NTM**) also aims
    to extract topics from a corpus of data. However, the difference between LDA and
    NTM is their learning logic. While LDA learns from probability distributions of
    the words in the documents, NTM is built on top of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The NTM network architecture has a bottleneck layer, which creates an embedding
    representation of the documents. This bottleneck layer contains all the necessary
    information to predict document composition, and its coefficients can be considered
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have completed this section on textual analysis. In the next section,
    we will learn about image processing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image processing is a very popular topic in machine learning. The idea is pretty
    self-explanatory: creating models that can analyze images and make inferences
    on top of them. By inference, you can understand this as detecting objects in
    an image, classifying images, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a set of built-in algorithms we can use to train image processing
    models. In the next few sections, we will have a look at those algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the image classification algorithm is used to classify
    images using supervised learning. In other words, it needs a label within each
    image. It supports multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way it operates is simple: during training, it receives an image and its
    associated labels. During inference, it receives an image and returns all the
    predicted labels. The image classification algorithm uses a CNN (**ResNet**) for
    training. It can either train the model from scratch or take advantage of transfer
    learning to pre-load the first few layers of the neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: According to AWS's documentation, the `.jpg` and `.png` file formats are supported,
    but the recommended format is **MXNet RecordIO**.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The semantic segmentation algorithm provides a pixel-level capability for creating
    computer vision applications. It tags each pixel of the image with a class, which
    is an important feature for complex applications such as self-driving and medical
    image diagnostics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of its implementation, the semantic segmentation algorithm uses the
    **MXNet Gluon framework** and the **Gluon CV toolkit**. You can choose any of
    the following algorithms to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully convolutional network** (**FCN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pyramid scene parsing** (**PSP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepLabV3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these options work as an **encoder-decoder** neural network architecture.
    The output of the network is known as a **segmentation mask**.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like the image classification algorithm, the main goal of the object detection
    algorithm is also self-explanatory: it detects and classifies objects in images.
    It uses a supervised approach to train a deep neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the inference process, this algorithm returns the identified objects
    and a score of confidence regarding the prediction. The object detection algorithm
    uses a **Single Shot MultiBox Detector** (**SSD**) and supports two types of network
    architecture: **VGG** and **ResNet**.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That was such a journey! Let''s take a moment to highlight what we have just
    learned. We broke this chapter into four main sections: supervised learning, unsupervised
    learning, textual analysis, and image processing. Everything that we have learned
    fits those subfields of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of supervised learning algorithms that we have studied includes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear learner algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization machines algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Nearest Neighbors algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object2Vec algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepAR forecasting algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that you can use linear learner, factorization machines, XGBoost, and
    KNN for multiple purposes, including to solve regression and classification problems.
    Linear learner is probably the simplest algorithm out of these four; factorization
    machines extend linear learner and are good for sparse datasets, XGBoost uses
    an ensemble method based on decision trees, and KNN is an index-based algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The other two algorithms, Object2Vec and DeepAR, are used for specific purposes.
    Object2Vec is used to create vector representations of the data, while DeepAR
    is used to create forecast models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of unsupervised learning algorithms that we have studied includes
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal** **Component** **Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP Insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random** **Cut** **Forest** (**RCF**) algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means is a very popular algorithm that's used for clustering. PCA is used
    for dimensionality reduction, IP Insights is used for pattern recognition, and
    RCF is used for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at regression models and K-means in more detail. We did this
    because, as a data scientist, we think you should at least master these two very
    popular algorithms so that you can go deeper into other algorithms by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we moved on to the second half of this chapter, where we talked about
    textual analysis and the following algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Blazing Text algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent** **Dirichlet** **Allocation** (**LDA**) algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural** **Topic** **Model** (**NTM**) algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we talked about image processing and looked at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image classification algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this is a very important chapter regarding the AWS machine learning specialty
    exam, we encourage you to jump onto the AWS website and search for machine learning
    algorithms. There, you will find the most recent information about the algorithms
    that we have just covered.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of this quick refresher and the end of this chapter.
    In the next chapter, we will have a look at the existing mechanisms provided by
    AWS that we can use to optimize these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are working as a lead data scientist for a retail company. Your team is
    building a regression model and using the linear learner built-in algorithm to
    predict the optimal price of a particular product. The model is clearly overfitting
    to the training data and you suspect that this is due to the excessive number
    of variables being used. Which of the following approaches would best suit a solution
    that addresses your suspicion?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Implementing a cross-validation process to reduce overfitting during the
    training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Applying L1 regularization and changing the `wd` hyperparameter of the linear
    learner algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Applying L2 regularization and changing the `wd` hyperparameter of the linear
    learner algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Applying L1 and L2 regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C, This question prompts about to the problem of overfitting due an excessive
    number of features being used. L2 regularization, which is available in linear
    learner through the `wd` hyperparameter, will work as a feature selector. Some
    less important features will be penalized by receiving very low weights, which
    will, in practical terms, eliminate the variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RecordIO-protobuf is an optimized data format that''s used to train AWS, built-in
    algorithms, where SageMaker converts each observation in the dataset into a binary
    representation as a set of 4-byte floats. RecordIO-protobuf can operate in two
    modes: pipe and file mode. What is the difference between them?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Pipe mode accepts encryption at rest, while file mode does not.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to the training instance's store
    volume.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) In pipe mode, the data is copied from S3 to the training instance store volume.
    In file mode, the data will be streamed directly from S3, which helps optimize
    storage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to another temporary S3 bucket.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B, Remember that RecordIO-protobuf has a pipe mode, which allows us to stream
    data directly from S3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are the cloud administrator of your company. You have done great work creating
    and managing user access and you have fine-grained control of daily activities
    in the cloud. However, you want to add an extra layer of security by identifying
    accounts that are attempting to create cloud resources from an unusual IP address.
    What would be the fastest solution to address this use case (choose all the correct
    answers)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Create an IP Insights model to identity anomalous accesses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Create a clustering model to identify anomalies in the application connections.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Integrate your IP Insights with existing rules from Amazon Guard Duty.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Integrate your anomaly detection model with existing rules from Amazon Guard
    Duty.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A,C, Remember: You can always come up with different approaches to solve problems.
    However, taking advantage of SageMaker''s built-in algorithms is usually the fastest
    way to do things.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a data scientist for a large company. One of your internal
    clients has requested that you improve a regression model that they have implemented
    in production. You have added a few features to the model and now you want to
    know if the model's performance has improved due to this change. Which of the
    following options best describes the evaluation metrics that you should use to
    evaluate your change?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Check if the R squared of the new model is better than the R squared of the
    current model in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Check if the R squared adjusted of the new model is better than the R squared
    of the current model in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Check if the R squared and the RMSE of the new model are better than the
    R squared of the current model in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Check if the R squared adjusted RMSE of the new model is better than the
    R squared of the current model in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D, In this case, you have been exposed to a particular behavior of regression
    model evaluation, where, by adding new features, you will always increase R squared.
    You should use R squared adjusted to understand if the new features are adding
    value to the model or not. Additionally, RMSE will give you a business perspective
    of the model's performance. Although option b is correct, option d best describes
    the optimal decision you should make.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following algorithms is optimized to work with sparse data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Factorization machines
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) XGBoost
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Linear Learner
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) KNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A, Factorization machines is a general-purpose algorithm that is optimized for
    sparse data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following algorithms uses an ensemble method based on decision
    trees during the training process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Factorization machines
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) XGBoost
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Linear learner
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) KNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B, XGBoost is a very popular algorithm that uses an ensemble of decision trees
    to train the model. XGBoost uses a boosting approach, where decision trees try
    to correct the error of the prior model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following options is considered an index-based algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Factorization machines
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) XGBoost
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Linear learner
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) KNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D, We say that KNN is an index-based algorithm because it has to compute distances
    between points, assign indexes for these points, and then store the sorted distances
    and their indexes. With that type of data structure, KNN can easily select the
    top K closest points to make the final prediction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are a data scientist in a big retail company that wants to predict their
    sales per region on a monthly basis. You have done some exploratory work and discovered
    that the sales pattern per region is different. Your team has decided to approach
    this project as a time series model, and now, you have to select the best approach
    to create a solution. Which of the following options would potentially give you
    a good solution with the least effort?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Approach this problem using the ARIMA algorithm. Since each region might
    have different sales behavior, you would have to create one independent model
    per region.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Approach this problem with the RNN algorithm. Since neural networks are robust,
    you could create one single model to predict sales in any region.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Develop a DeepAR model and set the region, associated with each time series,
    as a vector of static categorical features. You can use the **cat** field to set
    up this option.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Develop a DeepAR model and set the region, associated with each time series,
    as a vector of static categorical features. You can use the **dynamic_feat** field
    to set this option.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C, Options a and c are potentially right. However, the problem states that we
    want the solution with the least effort. In this case, setting up a DeepAR model
    and separating the time series by region would be the expected solution (option
    c). We can set up that type of configuration by passing a vector of static categorical
    features into the cat field of the DeepAR class model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working on a dataset that contains nine numerical variables. You want
    to create a scatter plot to see if those variables could be potentially grouped
    on clusters of high similarity. How could you achieve this goal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Execute the K-means algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Compute the two **principal components** (**PCs**) using PCA. Then, plot
    PC1 and PC2 in the scatter plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Execute the KNN algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Compute the three **principal components** (**PCs**) using PCA. Then, plot
    PC1, PC2, and PC3 in the scatter plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B, Using K-means or KNN will not solve this question. You have to apply PCA
    to reduce the number of features and then plot the results in a scatter plot.
    Since scatter plots only accept two variables, option b is the right one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How should you preprocess your data in order to train a Blazing Text model on
    top of 100 text files?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) You should create a text file with space-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should concatenate all of them into a single one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) You should create a text file with space-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should apply the same transformation to each one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) You should create a text file with comma-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should concatenate all of them into a single one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) You should create a text file with comma-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should apply the same transformation to each one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Option a is the right one. You should provide just a single file to Blazing
    Text with space-separated tokens, where each line of the file must contain a single
    sentence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
