- en: '*Chapter 7*: Applying Machine Learning Algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：应用机器学习算法'
- en: In the previous chapter, we studied AWS services for data processing, including
    Glue, Athena, and Kinesis! It is now time to move on to the modeling phase and
    study machine learning algorithms. I am sure that, during the earlier chapters,
    you have realized that building machine learning models requires a lot of knowledge
    about AWS services, data engineering, data exploratory, data architecture, and
    much more. This time, we will go deeper into the algorithms that we have been
    talking about so far and many others.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了AWS的数据处理服务，包括Glue、Athena和Kinesis！现在是时候进入建模阶段，研究机器学习算法了。我相信，在前面几章中，你已经意识到构建机器学习模型需要大量关于AWS服务、数据工程、数据探索、数据架构等方面的知识。这次，我们将更深入地探讨我们之前讨论过的算法以及许多其他算法。
- en: Having a good sense of the different types of algorithms and machine learning
    approaches will put you in a very good position to make decisions during your
    projects. Of course, this type of knowledge is also crucial to the AWS machine
    learning specialty exam.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对不同类型的算法和机器学习方法的良好理解将使你在项目决策中处于非常有利的地位。当然，这种知识对于AWS机器学习专业考试也是至关重要的。
- en: Bear in mind that there are thousands of algorithms out there and, by the way,
    you can even propose your own algorithm for a particular problem. Furthermore,
    we will cover the most relevant ones and, hopefully, the ones that you will probably
    face in the exam.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，有成千上万的算法，顺便说一句，你甚至可以为特定问题提出自己的算法。此外，我们将涵盖最相关的算法，并希望这些算法你可能在考试中遇到。
- en: 'The main topics of this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容包括：
- en: Storing the training data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储训练数据
- en: A word about ensemble models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于集成模型的说明
- en: Regression models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型
- en: Classification models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型
- en: Forecasting models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测模型
- en: Object2Vec
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec
- en: Clustering
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Anomaly detection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: Dimensionality reduction
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度约简
- en: IP Insights
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP洞察
- en: Natural language processing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Reinforcement learning
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Alright, let's do it!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始吧！
- en: Introducing this chapter
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍本章
- en: During this chapter, we will talk about several algorithms, modeling concepts,
    and learning strategies. We think all these topics will be beneficial for you
    during the exam and your data scientist career.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论几个算法、建模概念和学习策略。我们认为所有这些主题都将对你的考试和数据科学家职业生涯有益。
- en: We have structured this chapter in a way so that it covers not only the necessary
    topics of the exam but also gives you a good sense of the most important learning
    strategies out there. For example, the exam will check your knowledge regarding
    the basic concepts of K-means; however, we will cover it on a much deeper level,
    since this is an important topic for your career as a data scientist.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章结构化，不仅涵盖考试所需的必要主题，还让你对最重要的学习策略有一个良好的认识。例如，考试将检查你对K-means基本概念的了解；然而，我们将对其进行更深入的探讨，因为这是作为数据科学家职业的重要主题。
- en: 'We will follow this approach, looking deeper into the logic of the algorithm,
    for some types of models that we feel every data scientist should master. So,
    keep that in mind: sometimes, we might go deeper than expected in the exam, but
    that will be extremely important for you.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取这种方法，更深入地研究某些类型的模型，我们认为每个数据科学家都应该掌握其算法逻辑。所以，请记住：有时，我们可能在考试中比预期走得更深，但这对你来说将极其重要。
- en: Many times, during this chapter, we will use the term **built-in algorithms**.
    We will use this term when we want to refer to the list of algorithms that's implemented
    by AWS on their SageMaker SDK.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们多次使用术语**内置算法**。当我们想要提及AWS在他们的SageMaker SDK上实现的算法列表时，我们将使用这个术语。
- en: 'Let me give you a concrete example: you can use scikit-learn''s KNN algorithm
    (if you don''t remember what scikit-learn is, refresh your memory by going back
    to [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014), *Machine Learning
    Fundamentals*) to create a classification model and deploy it to SageMaker. However,
    AWS also offers its own implementation of the KNN algorithm on their SDK, which
    is optimized to run in the AWS environment. Here, KNN is an example of a built-in
    algorithm.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你一个具体的例子：你可以使用scikit-learn的KNN算法（如果你不记得什么是scikit-learn，可以通过回到[*第1章*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)，*机器学习基础*)来创建一个分类模型并将其部署到SageMaker。然而，AWS还提供了他们SDK上的KNN算法的自身实现，该实现针对AWS环境进行了优化。在这里，KNN是一个内置算法的例子。
- en: 'As you can see, the possibilities on AWS are endless, because you can either
    take advantage of built-in algorithms or bring in your own algorithm to create
    models on SageMaker. Finally, just to make this very clear, here is an example
    of how to import a built-in algorithm from the AWS SDK:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在AWS上的可能性是无限的，因为你可以利用内置算法，或者引入自己的算法来在SageMaker上创建模型。最后，为了使这一点非常明确，这里有一个从AWS
    SDK导入内置算法的示例：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will learn how to create models on SageMaker in [*Chapter 9*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173),
    *Amazon SageMaker Modeling*. For now, just understand that AWS has its own set
    of libraries where those built-in algorithms are implemented.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在[*第9章*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173)，“Amazon SageMaker建模”中学习如何在SageMaker上创建模型。目前，你只需了解AWS有一套自己的库，其中实现了这些内置算法。
- en: To train and evaluate a model, you need training and testing data. After instantiating
    your estimator, you should then feed it with those datasets. I don't want to spoil
    [*Chapter 9*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173), *Amazon SageMaker
    Modeling*, but you should know the concepts of **data channels** in advance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练和评估一个模型，你需要训练数据和测试数据。在实例化你的估计器之后，你应该用这些数据集来喂养它。我不想破坏[*第9章*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173)，“Amazon
    SageMaker建模”，但你应该事先了解**数据通道**的概念。
- en: Data channels are configurations related to input data that you can pass to
    SageMaker when you're creating a training job. You should set these configurations
    just to inform SageMaker of how your input data is formatted.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通道是与输入数据相关的配置，你可以在创建训练作业时将其传递给SageMaker。你应该设置这些配置，只是为了通知SageMaker你的输入数据格式。
- en: In [*Chapter 9*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173), *Amazon SageMaker
    Modeling*, you will learn how to create training jobs and how to set data channels.
    As of now, you should know that, while configuring data channels, you can set
    **content types** (**ContentType**) and an **input mode** (**TrainingInputMode**).
    Let's take a closer look at how and where the training data should be stored so
    that it can be integrated properly with AWS's built-in algorithms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第9章*](B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173)，“Amazon SageMaker建模”中，你将学习如何创建训练作业以及如何设置数据通道。到目前为止，你应该知道，在配置数据通道时，你可以设置**内容类型**（**ContentType**）和**输入模式**（**TrainingInputMode**）。让我们更详细地看看训练数据应该如何存储，以便能够与AWS的内置算法正确集成。
- en: Storing the training data
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储训练数据
- en: 'First of all, you can use multiple AWS services to prepare data for machine
    learning, such as EMR, Redshift, Glue, and so on. After preprocessing the training
    data, you should store it in S3, in a format expected by the algorithm you are
    using. The following table shows the list of acceptable data formats per algorithm:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以使用多个AWS服务来准备机器学习的数据，例如EMR、Redshift、Glue等。预处理完训练数据后，你应该将其存储在S3中，以算法期望的格式存储。以下表格显示了每个算法可接受的数据格式列表：
- en: '![Figure 7.1 – Data formats that are acceptable per AWS algorithm'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.1 – 每个AWS算法可接受的数据格式'
- en: '](img/B16735_07_001.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_001.jpg)'
- en: Figure 7.1 – Data formats that are acceptable per AWS algorithm
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 每个AWS算法可接受的数据格式
- en: 'As we can see, many algorithms accept text`/.csv` format. Keep in mind that
    you should follow these rules if you want to use that format:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，许多算法接受text`/.csv`格式。请注意，如果你想使用该格式，你应该遵循以下规则：
- en: Your CSV file *can't* have a header record.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的CSV文件*不能*有标题记录。
- en: For supervised learning, the target variable must be in the first column.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于监督学习，目标变量必须在第一列。
- en: While configuring the training pipeline, set the input data channel as `content_type`
    equal to `text/csv`.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在配置训练管道时，将输入数据通道的`content_type`设置为`text/csv`。
- en: For unsupervised learning, set the `label_size` within the `'content_type=text/csv;label_size=0'`.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无监督学习，在`'content_type=text/csv;label_size=0'`中设置`label_size`。
- en: Although text/.csv format is fine for many use cases, most of the time, AWS's
    built-in algorithms work better with **recordIO-protobuf**. This is an optimized
    data format that's used to train AWS's built-in algorithms, where SageMaker converts
    each observation in the dataset into a binary representation that's a set of 4-byte
    floats.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然text/.csv格式对许多用例来说都很好，但大多数情况下，AWS的内置算法与**recordIO-protobuf**配合得更好。这是一种用于训练AWS内置算法的优化数据格式，其中SageMaker将数据集中的每个观测值转换为二进制表示，即一组4字节的浮点数。
- en: 'RecordIO-protobuf accepts two types of input mode: **pipe mode** and **file
    mode**. In pipe mode, the data will be streamed directly from S3, which helps
    optimize storage. In file mode, the data is copied from S3 to the training instance''s
    store volume.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RecordIO-protobuf 接受两种类型的输入模式：**管道模式**和**文件模式**。在管道模式下，数据将直接从 S3 流出，这有助于优化存储。在文件模式下，数据将从
    S3 复制到训练实例的存储卷。
- en: We are almost ready! Now, let's have a quick look at some modeling definitions
    that will help you understand some more advanced algorithms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好了！现在，让我们快速浏览一些建模定义，这将帮助你理解一些更高级的算法。
- en: A word about ensemble models
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于集成模型的一些话
- en: Before we start diving into the algorithms, there is an important modeling concept
    that you should be aware of, known as **ensemble**. The term ensemble is used
    to describe methods that use multiple algorithms to create a model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入算法之前，有一个重要的建模概念你应该了解，称为**集成**。集成这个术语用来描述使用多个算法创建模型的方法。
- en: 'For example, instead of creating just one model to predict fraudulent transactions,
    you could create multiple models that do the same thing and, using a vote sort
    of system, select the predicted outcome. The following table illustrates this
    simple example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，与其只创建一个模型来预测欺诈交易，你还可以创建多个执行相同任务的模型，并通过一种投票系统选择预测结果。下表展示了这个简单的例子：
- en: '![Figure 7.2 – An example of a voting system on ensemble methods'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 集成方法上的投票系统示例'
- en: '](img/B16735_07_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_002.jpg)'
- en: Figure 7.2 – An example of a voting system on ensemble methods
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 集成方法上的投票系统示例
- en: The same approach works for regression problems, where, instead of voting, we
    could average the results of each model and use that as the final outcome.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，这种方法同样适用，我们不是通过投票，而是通过平均每个模型的预测结果，并将其作为最终结果。
- en: Voting and averaging are just two examples of ensemble approaches. Other powerful
    techniques include **blending** and **stacking**, where you can create multiple
    models and use the outcome of each model as features for a main model. Looking
    back at the preceding table, columns "Model A," "Model B," and "Model C" would
    be used as features to predict the final outcome.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 投票和平均只是集成方法中的两个例子。其他强大的技术包括**混合**和**堆叠**，其中你可以创建多个模型，并将每个模型的输出作为主模型的特征。回顾前面的表格，"模型
    A"、"模型 B"和"模型 C"将被用作预测最终结果的特征。
- en: 'It turns out that many machine learning algorithms use ensemble methods while
    training, in an embedded way. These algorithms can be classified into two main
    categories:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，许多机器学习算法在训练过程中使用集成方法，以嵌入式的方式。这些算法可以分为两大类：
- en: '**Bootstrapping Aggregation** or **Bagging**: With this approach, several models
    are trained on top of different samples of the data. Then, predictions are made
    through the voting or averaging system. The main algorithm from this category
    is known as **Random Forest**.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助聚合**或**Bagging**：这种方法中，在数据的不同样本上训练多个模型。然后，通过投票或平均系统进行预测。这个类别的主要算法被称为**随机森林**。'
- en: '**Boosting**: With this approach, several models are trained on top of different
    samples of the data. Then, one model tries to correct the error of the next model
    by penalizing incorrect predictions. The main algorithms from this category are
    known as **Stochastic Gradient Boosting** and **AdaBoost**.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升法**：这种方法中，在数据的不同样本上训练多个模型。然后，一个模型试图通过惩罚错误预测来纠正下一个模型的错误。这个类别的主要算法被称为**随机梯度提升**和**AdaBoost**。'
- en: Now that you know what ensemble models are, let's move on and study some machine
    learning algorithms that are likely to be present in your exam. Not all of them
    use ensemble approaches, but I trust it is going to be easier for you to recognize
    that.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经知道了集成模型是什么，那么让我们继续学习一些可能出现在你考试中的机器学习算法。虽然并非所有这些算法都使用集成方法，但我相信这将更容易让你识别出来。
- en: 'We will split the next few sections up based on AWS algorithm categories:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据 AWS 算法类别划分接下来的几个部分：
- en: Supervised learning
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Textual analysis
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析
- en: Image processing
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Finally, we will provide an overview of reinforcement learning in AWS.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将概述 AWS 中的强化学习。
- en: Supervised learning
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'AWS provides supervised learning algorithms for general purposes (regression
    and classification tasks) and for more specific purposes (forecasting and vectorization).
    The list of built-in algorithms that can be found in these sub-categories is as
    follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 为通用目的（回归和分类任务）以及更具体的目的（预测和向量化）提供了监督学习算法。这些子类别中可以找到的内置算法列表如下：
- en: Linear learner algorithm
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习算法
- en: Factorization machines algorithm
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分解机算法
- en: XGBoost algorithm
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 算法
- en: K-Nearest Neighbor algorithm
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-最近邻算法
- en: Object2Vec algorithm
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec 算法
- en: DeepAR Forecasting algorithm
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAR 预测算法
- en: Let's start with regression models and the linear learner algorithm.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从回归模型和线性学习算法开始。
- en: Working with regression models
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与回归模型一起工作
- en: Okay; I know that real problems usually aren't linear nor simple. However, looking
    into **linear regression** models is a nice way to figure out what's going on
    inside **regression models** in general (yes, regression models can be linear
    and non-linear). This is mandatory knowledge for every data scientist and can
    help you solve real challenges as well. We'll take a closer look at this in the
    following subsections.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 好的；我知道现实问题通常既不是线性的也不是简单的。然而，研究**线性回归**模型是了解一般**回归模型**内部发生什么的好方法（是的，回归模型可以是线性的和非线性的）。这是每位数据科学家必须掌握的知识，也可以帮助你解决现实挑战。我们将在以下小节中更详细地探讨这一点。
- en: Introducing regression algorithms
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍回归算法
- en: Linear regression models aim to predict a numeric value (Y) according to one
    or more variables (X). Mathematically, we can define such a relationship as Y
    = f(X), where Y is known as the **dependent variable** and X is known as the **independent
    variable**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型旨在根据一个或多个变量（X）预测一个数值（Y）。从数学上讲，我们可以将这种关系定义为 Y = f(X)，其中 Y 被称为**依赖变量**，X
    被称为**独立变量**。
- en: With regression models, the component that we want to predict (Y) is always
    a continuous number; for example, the price of houses or the number of transactions.
    We saw that in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*, *Table 2*, when we were choosing the right type
    of supervised learning algorithm, given the target variable. Please, feel free
    to go back and review it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归模型中，我们想要预测的组件（Y）始终是一个连续的数字；例如，房价或交易数量。我们在[*第一章*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)，“机器学习基础”，*表2*中看到了这一点，当我们根据目标变量选择合适的监督学习算法时。请随意回去复习它。
- en: '*When we use just one variable to predict Y*, we refer to this problem as **simple
    linear regression**. On the other hand, when we use *more than one variable to
    predict Y*, we say that we have a **multiple linear regression** problem.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我们只用一个变量来预测 Y 时*，我们称这个问题为**简单线性回归**。另一方面，当我们使用**多个变量**来预测 Y 时，我们说我们有一个**多重线性回归**问题。'
- en: There is also another class of regression models, known as **non-linear regression**.
    However, let's put that aside for a moment and understand what simple linear regression
    means.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一类回归模型，称为**非线性回归**。然而，让我们暂时把它放在一边，先了解简单线性回归是什么意思。
- en: Regression models belong to the supervised side of machine learning (the other
    side is non-supervised) because algorithms try to predict values according to
    existing correlations between independent and dependent variables.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型属于机器学习的监督学习方面（另一方面是非监督学习），因为算法试图根据独立变量和依赖变量之间的现有相关性来预测值。
- en: 'But what does "f" mean in Y=f(X)? "f" is the regression function responsible
    for predicting Y based on X. In other words, this is the function that we want
    to figure out! When we start talking about simple linear regression, pay attention
    to the next three questions and answers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但 Y=f(X)中的“f”是什么意思？“f”是负责根据 X 预测 Y 的回归函数。换句话说，这正是我们想要弄清楚的功能！当我们开始谈论简单线性回归时，请注意以下三个问题和答案：
- en: What is the shape of "f" in linear regression?
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性回归中的“f”的形状是什么？
- en: Linear, sure!
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性，当然！
- en: How can we represent a linear relationship?
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何表示线性关系？
- en: Using a *straight* line (you will understand why in a few minutes).
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用一条**直线**（你将在几分钟内理解原因）。
- en: So, what's the function that defines a line?
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那么，定义直线的函数是什么？
- en: ax + b (just check any *mathematics* book).
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ax + b（只需查看任何**数学**书籍）。
- en: That's it! Linear regression models are given by **y = ax + b**. Once we are
    trying to predict Y given X, we just need to find out the values of "a" and "b".
    We can adopt the same logic to figure out what's going on inside other kinds of
    regression.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！线性回归模型由**y = ax + b**给出。一旦我们试图根据X预测Y，我们只需要找出“a”和“b”的值。我们可以采用相同的逻辑来了解其他类型回归的内部情况。
- en: And believe me, finding out the values of "a" and "b" isn't the only thing we're
    going to do. It's nice to know that "a" is also known as the **alpha coefficient**,
    or **slope**, and represents the line's inclination, while "b" is also known as
    the **beta coefficient**, or **y-intercept**, and represents the place where the
    line crosses the y-axis (into a two-dimensional plan consisting of x and y). You
    will see these two terms in the next subsection.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相信我，找出“a”和“b”的值并不是我们要做的唯一事情。知道“a”也被称为**alpha系数**，或**斜率**，代表线的倾斜度，而“b”也被称为**beta系数**，或**y截距**，代表线与y轴交叉的位置（进入由x和y组成的二维平面）。你将在下一个子节中看到这两个术语。
- en: It's also nice to know that there is an error associated with every predictor
    that we don't have control. Let's name it "e" and formally define simple linear
    regression as **y = ax + b + e**. Mathematically, this error is expressed by the
    difference between the prediction and the real value.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得知道，每个我们无法控制的预测因子都存在误差。让我们称它为“e”，并将简单线性回归正式定义为**y = ax + b + e**。从数学上讲，这个误差是通过预测值和真实值之间的差异来表达的。
- en: Alright, let's find alpha and beta and give this section a happy ending!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们找到alpha和beta，为这一部分画上一个圆满的句号！
- en: Least square method
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小二乘法
- en: 'There are different ways to find the slope and y-intercept of a line, but the
    most used method is known as the **least square method**. The principle behind
    this method is simple: we have to find the *best line that reduces the sum of
    squared error*.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来找到直线的斜率和y截距，但最常用的方法是称为**最小二乘法**。这种方法背后的原理很简单：我们必须找到*最佳直线，以减少平方误差的总和*。
- en: 'In the following graph, we can see a Cartesian plane with multiple points and
    lines in it. "Line a" represents the best fit for this data – in other words,
    that would be the best linear regression function for those points. But how do
    I know that? It''s simple: if we compute the error associated with each point
    (like the one we''ve zoomed in on in the following graph), we will realize that
    "line a" contains the least sum of square errors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们可以看到一个带有多个点和线的笛卡尔平面。“线a”代表这些数据的最优拟合线——换句话说，那将是这些点的最佳线性回归函数。但我是怎么知道的呢？很简单：如果我们计算每个点（就像下面图中我们放大查看的那个点）相关的误差，我们会意识到“线a”包含了最小的平方误差总和：
- en: '![Figure 7.3 – Visualizing the principle of the least square method'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.3 – 最小二乘法原理的可视化]'
- en: '](img/B16735_07_003.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_07_003.jpg]'
- en: Figure 7.3 – Visualizing the principle of the least square method
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 最小二乘法原理的可视化
- en: It is worth understanding linear regression from scratch, not only for the certification
    exam but mainly for your career as a data scientist. To provide you with a complete
    example, we have developed a spreadsheet containing all the calculations that
    we are going to see, step by step! We encourage you to jump on this support material
    and perform some simulations. Let's see this in action.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 值得从头开始理解线性回归，这不仅是为了认证考试，而且主要是为了你作为数据科学家的职业生涯。为了给你提供一个完整的例子，我们已经开发了一个包含我们将要逐步看到的所有计算的电子表格！我们鼓励你使用这个支持材料进行一些模拟。让我们看看它是如何运作的。
- en: Creating a linear regression model from scratch
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始创建线性回归模型
- en: 'We are going to use a very simple dataset, with only two variables:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个非常简单的数据集，只包含两个变量：
- en: '*X*: Represents the person''s number of years of work experience'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*：代表一个人的工作经验年数'
- en: '*Y*: Represents the person''s average salary'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*：代表一个人的平均薪水'
- en: We want to understand the relationship between X and Y and, if possible, predict
    the salary (Y) based on years of experience (X). As I mentioned previously, often,
    real problems have far more independent variables and are not necessarily linear.
    However, I am sure this example will give you the baseline knowledge to master
    more complex algorithms.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要理解X和Y之间的关系，并且如果可能的话，根据工作经验（X）预测薪水（Y）。正如我之前提到的，现实中的问题往往有更多的独立变量，并且不一定呈线性。然而，我相信这个例子会给你提供掌握更复杂算法的基础知识。
- en: 'To find out what the alpha and beta coefficients are (or slope and y-intercept,
    if you prefer), we need to find some statistics related to the dataset, so let''s
    take a look at the data and the auxiliary statistics shown here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出alpha和beta系数（或者如果您愿意，斜率和y截距），我们需要找到与数据集相关的某些统计量，因此让我们看看这里显示的数据和辅助统计量：
- en: '![ Figure 7.4 – Dataset to predict average salary based on amount of work experience'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![ 图7.4 – 基于工作经验数量预测平均工资的数据集'
- en: '](img/B16735_07_004.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_004.jpg)'
- en: Figure 7.4 – Dataset to predict average salary based on amount of work experience
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 基于工作经验数量预测平均工资的数据集
- en: 'As we can see, there is an almost perfect linear relationship between X and
    Y. As the amount of work experience increases, so does the salary. In addition
    to X and Y, we need to compute the following statistics: the number of records,
    the mean of X, the mean of Y, the covariance of X and Y, the variance of X, and
    the variance of Y. The following formulas provide a mathematical representation
    of variance and covariance (respectively), where *x bar*, *y bar*, and *n* represent
    the mean of X, the mean of Y, and the number of records, respectively:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，X和Y之间存在几乎完美的线性关系。随着工作经验的增加，工资也随之增加。除了X和Y之外，我们还需要计算以下统计量：记录数、X的平均值、Y的平均值、X和Y的协方差、X的方差和Y的方差。以下公式提供了方差和协方差的数学表示（分别），其中*x
    bar*、*y bar*和*n*分别代表X的平均值、Y的平均值和记录数：
- en: '![](img/image3.jpg)![](img/image4.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image3.jpg)![](img/image4.jpg)'
- en: If you want to check the calculation details of the formulas for each of those
    auxiliary statistics in *Table 7.2*, please refer to the support material provided
    along with this book. There, you will find those formulas already implemented
    for you.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想检查*表7.2*中每个辅助统计量的公式计算细节，请参阅本书附带的支持材料。在那里，您将找到这些公式已经为您实现。
- en: 'These statistics are important because they will be used to compute our alpha
    and beta coefficients. The following image explains how we are going to compute
    both coefficients, along with the correlation coefficients **R** and **R squared**.
    These last two metrics will give us an idea about the quality of the model, where
    the closer the model is to 1, the better the model is:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统计量很重要，因为它们将被用来计算我们的alpha和beta系数。以下图像解释了我们将如何计算这两个系数，以及相关系数**R**和**R平方**。后两个指标将给我们一个关于模型质量的概念，模型越接近1，模型就越好：
- en: '![Figure 7.5 – Equations to calculate coefficients for simple linear regression'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.5 – 计算简单线性回归系数的公式'
- en: '](img/B16735_07_005.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_005.jpg)'
- en: Figure 7.5 – Equations to calculate coefficients for simple linear regression
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 计算简单线性回归系数的公式
- en: 'After applying these formulas, we will come up with the results shown here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些公式后，我们将得到以下结果：
- en: '![Figure 7.6 – Finding regression coefficients'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.6 – 寻找回归系数'
- en: '](img/B16735_07_006.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_006.jpg)'
- en: Figure 7.6 – Finding regression coefficients
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 寻找回归系数
- en: 'The preceding table already contains all the information that we need to make
    predictions on top of the new data. If we replace the coefficients in the original
    equation, **y = ax + b + e**, we will find the regression formula to be as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格已经包含了我们用于在新的数据上做出预测所需的所有信息。如果我们用原始方程中的系数替换，**y = ax + b + e**，我们将找到回归公式如下：
- en: Y = 1021,212 * X + 53,3
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Y = 1021,212 * X + 53,3
- en: 'From this point on, to make predictions, all we have to do is replace X with
    the number of years of experience. As a result, we will find Y, which is the projected
    salary. We can see the model fit in the following graph and some model predictions
    in *Figure 7.8*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，为了进行预测，我们只需要将X替换为经验年数。结果，我们将找到Y，即预测的工资。我们可以在以下图表中看到模型拟合和*图7.8*中的某些模型预测：
- en: '![Figure 7.7 – Fitting data in the regression equation'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 – 将数据拟合到回归方程中'
- en: '](img/B16735_07_007.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_007.jpg)'
- en: Figure 7.7 – Fitting data in the regression equation
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 将数据拟合到回归方程中
- en: 'We see the prediction values here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到预测值：
- en: '![Figure 7.8 – Model predictions'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.8 – 模型预测'
- en: '](img/B16735_07_008.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_008.jpg)'
- en: Figure 7.8 – Model predictions
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 模型预测
- en: While you are analyzing regression models, you should be able to understand
    whether your model is of a good quality or not. We talked about many modeling
    issues (such as overfitting) in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*, and you already know that you always have to
    check model performance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在分析回归模型时，你应该能够理解你的模型是否具有良好的质量。我们在[*第一章*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)，“机器学习基础”中讨论了许多建模问题（例如过拟合），你已经知道你总是需要检查模型性能。
- en: 'A good approach to regression models is performing what is called **residual
    analysis**. This is where we plot the errors of the model in a scatter plot and
    check if they are randomly distributed (as expected) or not. If the errors are
    *not* randomly distributed, this means that your model was unable to generalize
    the data. The following graph shows a residual analysis of our example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对回归模型的一个良好方法是执行所谓的**残差分析**。这就是我们在散点图中绘制模型的误差，并检查它们是否随机分布（如预期）的地方。如果误差不是随机分布的，这意味着你的模型无法泛化数据。以下图表显示了我们的示例的残差分析：
- en: '![Figure 7.9 – Residual analysis'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.9 – 残差分析'
- en: '](img/B16735_07_009.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_009.jpg)'
- en: Figure 7.9 – Residual analysis
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 残差分析
- en: The takeaway here is that the errors are randomly distributed. Such evidence,
    along with a high R squared rating, can be used as arguments to support the use
    of this model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的要点是误差是随机分布的。这样的证据，加上高R平方评分，可以用作支持使用此模型的论据。
- en: Important note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In [*Chapter 8*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162), *Evaluating
    and Optimizing Models*, we will learn about evaluation metrics. For instance,
    we will learn that each type of model may have its own set of evaluation metrics.
    Regression models are commonly evaluated with **Mean Square Error** (**MSE**)
    and **Root Mean Square Error** (**RMSE**). In other words, apart from R, R squared,
    and residual analysis, ideally, you will execute your model on test sets to extract
    other performance metrics. You can even use a cross-validation system to check
    model performance, as we learned in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第八章*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162)，“评估和优化模型”中，我们将学习关于评估指标的内容。例如，我们将了解到每种类型的模型可能都有自己的评估指标集。回归模型通常使用**均方误差**（MSE）和**均方根误差**（RMSE）进行评估。换句话说，除了R、R平方和残差分析之外，理想情况下，你将在测试集上执行你的模型以提取其他性能指标。你甚至可以使用交叉验证系统来检查模型性能，正如我们在[*第一章*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)，“机器学习基础”中学到的。
- en: Very often, when the model residuals *do* present a pattern and are *not* randomly
    distributed, it's because the existing relationship in the data is not linear,
    but non-linear, so another modeling technique must be applied. Now, let's take
    a look at how we can interpret a model's results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的是，当模型残差确实呈现模式并且不是随机分布时，这是因为数据中现有的关系不是线性的，而是非线性的，因此必须应用另一种建模技术。现在，让我们看看我们如何解释模型的结果。
- en: Interpreting regression models
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释回归模型
- en: It is also good to know how to interpret a linear regression model. Sometimes,
    we use linear regression not necessarily to create a predictive model but to do
    a regression analysis, where we can understand the relationship between the independent
    and dependent variables.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何解释线性回归模型也是很好的。有时，我们使用线性回归不一定是为了创建预测模型，而是为了进行回归分析，这样我们可以理解自变量和因变量之间的关系。
- en: 'Looking back at our regression equation (Y = 1021,212 * X + 53,3), we can see
    our two terms: alpha or slope (1021.2) and beta or y-intercept (53.3). We can
    interpret this model as follows: *for each additional year of working experience,
    you will increase your salary by $1,021.3 dollars*. Also, note that when "years
    of experience" is equal to zero, the expected salary is going to be $53.3 dollars
    (this is the point where our straight line crosses the y-axis).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的回归方程（Y = 1021,212 * X + 53,3），我们可以看到我们的两个术语：alpha或斜率（1021.2）和beta或y轴截距（53.3）。我们可以这样解释这个模型：*对于每增加一年工作经验，你的薪水将增加1,021.3美元*。此外，请注意，当“工作经验年数”为零时，预期的薪水将是53.3美元（这是我们的直线与y轴相交的点）。
- en: 'From a generic perspective, your regression analysis should answer the following
    question: for each extra unit that''s added to the independent variable (slope),
    what is the average change in the dependent variable? Please take notes and make
    sure you know how to interpret simple linear regression models – this is a very
    important thing to do as one of your daily activities as a data scientist! Let''s
    move on and take a look at some final considerations about linear regression.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个通用角度来看，你的回归分析应该回答以下问题：对于独立变量（斜率）中每增加一个额外单位，因变量的平均变化是多少？请做笔记并确保你知道如何解释简单线性回归模型——这对于你作为数据科学家日常活动中的一个重要事情来说是非常重要的事情！让我们继续前进，看看关于线性回归的一些最终考虑。
- en: Checking R squared adjusted
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查调整后的R平方
- en: At this point, I hope you have a much better idea of regression models! There
    is just another very important topic that you should be aware of, regardless of
    whether it will come up in the exam or not, which is the parsimony aspect of your
    model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我希望你对回归模型有了更好的理解！还有一个非常重要的主题你应该知道，无论它是否会在考试中出现，那就是你模型的简约性方面。
- en: We have talked about parsimony already, in [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning Fundamentals*. This is your ability to prioritize simple models
    over complex models. Looking into regression models, you might have to use more
    than one feature to predict your outcome. This is also known as a multiple regression
    model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[*第一章*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)《机器学习基础》中讨论了简约性。这是你优先考虑简单模型而不是复杂模型的能力。在查看回归模型时，你可能需要使用多个特征来预测你的结果。这也被称为多元回归模型。
- en: When that's the case, the R and R squared coefficients tend to reward more complex
    models that have more features. In other words, if you keep adding new features
    to a multiple regression model, you will come up with higher R and R squared coefficients.
    That's why you *can't* anchor your decisions *only* based on those two metrics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种情况发生时，R和R平方系数往往会奖励具有更多特征的更复杂模型。换句话说，如果你继续向多元回归模型添加新特征，你会得到更高的R和R平方系数。这就是为什么你不能仅仅基于这两个指标来做决定。
- en: 'Another additional metric that you could use (apart from R, R squared, MSE,
    and RMSE) is known as **R squared adjusted**. This metric is penalized when we
    add extra features to the model that do not bring any real gain. In the following
    table, we have illustrated a hypothetical example just to show you when you are
    starting to lose parsimony:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的一个额外指标（除了R、R平方、MSE和RMSE之外）被称为**调整后的R平方**。当我们向模型添加不带来任何实际收益的额外特征时，这个指标会受到惩罚。在下面的表中，我们提供了一个假设的例子，只是为了展示当你开始失去简约性时的情况：
- en: '![Figure 7.10 – Comparing R squared and R squared adjusted'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.10 – 比较R平方和调整后的R平方'
- en: '](img/B16735_07_010.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_010.jpg)'
- en: Figure 7.10 – Comparing R squared and R squared adjusted
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 比较R平方和调整后的R平方
- en: Here, we can conclude that maintaining three variables in the model is better
    than maintaining four or five variables. Adding four or five variables to that
    model will increase R squared (as expected), but decrease R squared adjusted.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以得出结论，在模型中保持三个变量比保持四个或五个变量更好。向该模型添加四个或五个变量会增加R平方（正如预期的那样），但会降低调整后的R平方。
- en: Alright; at this point, you should have a very good understanding of regression
    models. Now, let's check what AWS offers in terms of built-in algorithms for this
    class of models. That is going to be important for your exam, so let's take a
    look.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧；到目前为止，你应该对回归模型有了非常好的理解。现在，让我们看看AWS为这类模型提供的内置算法有哪些。这对你的考试很重要，所以让我们来看看。
- en: Regression modeling on AWS
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS上的回归建模
- en: AWS has a built-in algorithm known as **linear learner**, where we can implement
    linear regression models. The built-in linear learner uses **Stochastic Gradient
    Descent** (**SGD**) to train the model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: AWS有一个内置的算法称为**线性学习器**，其中我们可以实现线性回归模型。内置的线性学习器使用**随机梯度下降**（**SGD**）来训练模型。
- en: Important note
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We will learn more about SGD when we talk about neural networks. For now, we
    can look at SGD as an alternative to the popular least square error method that
    we just dissected.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈到神经网络时，我们将了解更多关于SGD的内容。现在，我们可以将SGD视为我们刚刚剖析的流行的最小二乘误差方法的替代方案。
- en: The linear learn built-in algorithm provides a hyperparameter that can apply
    normalization to the data, prior to the training process. The name of this hyperparameter
    is `normalize_data`. This is very helpful since linear models are sensitive to
    the scale of the data and usually take advantage of data normalization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 线性学习内置算法提供了一个超参数，可以在训练过程之前对数据进行归一化处理。这个超参数的名称是`normalize_data`。这非常有帮助，因为线性模型对数据的规模很敏感，通常利用数据归一化。
- en: Important note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We talked about data normalization in [*Chapter 3*](B16735_03_Final_VK_ePub.xhtml#_idTextAnchor059),
    *Data Preparation and Transformation*. Please review that chapter if you need
    to.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第3章*](B16735_03_Final_VK_ePub.xhtml#_idTextAnchor059)中讨论了数据归一化，*数据准备和转换*。如果你需要复习，请查看该章节。
- en: Some other important hyperparameters of the linear learner algorithm are **L1**
    and **wd**, which play the roles of **L1 regularization** and **L2 regularization**,
    respectively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 线性学习算法的一些其他重要超参数是**L1**和**wd**，它们分别扮演**L1正则化**和**L2正则化**的角色。
- en: L1 and L2 regularization help the linear learner (or any other regression algorithm
    implementation) avoid overfitting. Conventionally, we call regression models that
    implement L1 regularization **Lasso Regression** models, while for regression
    models with L2 regularization, we call them **Ridge Regression** models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化有助于线性学习器（或任何其他回归算法实现）避免过拟合。传统上，我们称实现L1正则化的回归模型为**Lasso回归**模型，而对于具有L2正则化的回归模型，我们称它们为**岭回归**模型。
- en: Although it might sound complex, it is not! Actually, the regression model equation
    is still the same; that is, **y = ax + b + e**. The change is in the loss function,
    which is used to find the coefficients that best minimize the error. If you look
    back at *Figure 7.3*, you will see that we have defined the error function as
    **e = (ŷ - y)2**, where **ŷ** is the regression function value and **y** is
    the real value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能听起来很复杂，但实际上并不复杂！实际上，回归模型方程仍然是相同的；即**y = ax + b + e**。变化在于损失函数，它用于找到最佳最小化误差的系数。如果你回顾*图7.3*，你会看到我们定义误差函数为**e
    = (ŷ - y)²**，其中**ŷ**是回归函数值，**y**是真实值。
- en: 'L1 and L2 regularization add a penalty term to the loss function, as shown
    in the following formulas (note that we are replacing ŷ with ax + b):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化将惩罚项添加到损失函数中，如下公式所示（注意，我们将ŷ替换为ax + b）：
- en: '![](img/image10.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image10.jpg)'
- en: The λ (lambda) parameter must be greater than 0 and manually tuned. A very high
    lambda value may result in an underfitting issue, while a very low lambda may
    not result in expressive changes in the final results (if your model is overfitted,
    it will stay overfitted).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: λ（lambda）参数必须大于0，并且需要手动调整。一个非常高的λ值可能会导致欠拟合问题，而一个非常低的λ值可能不会导致最终结果有显著的变化（如果你的模型已经过拟合，它将保持过拟合）。
- en: In practical terms, the main difference between L1 and L2 regularization is
    that L1 will shrink the less important coefficients to zero, which will force
    the feature to be dropped (acting as a feature selector). In other words, if your
    model is overfitting due to it having a high number of features, L1 regularization
    should help you solve this problem.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，L1和L2正则化之间的主要区别是，L1会将不那么重要的系数缩小到零，这将迫使特征被删除（充当特征选择器）。换句话说，如果你的模型因为具有大量特征而过拟合，L1正则化应该可以帮助你解决这个问题。
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: During your exam, remember the basis of L1 and L2 regularization, especially
    the key difference between them, where L1 works well as a feature selector.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的考试中，记住L1和L2正则化的基础，特别是它们之间的关键区别，其中L1作为一个特征选择器效果很好。
- en: Last but not least, many built-in algorithms can serve multiple modeling purposes.
    The linear learner algorithm can be used for regression, binary classification,
    and multi-classification. Make sure you remember this during your exam (it is
    *not only* about regression models).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，许多内置算法可以服务于多种建模目的。线性学习算法可以用于回归、二分类和多分类。在考试中确保你记住这一点（这不仅仅是关于回归模型）。
- en: Still going in that direction, AWS has other built-in algorithms that work for
    regression and classification problems; that is, **Factorization Machines**, **K-Nearest
    Neighbor** (**KNN**) and the **XGBoost** algorithm. Since these algorithms can
    also be used for classification purposes, we'll cover them in the section about
    classification algorithms.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然沿着这个方向，AWS 还内置了其他适用于回归和分类问题的算法；即**因子分解机**、**K最近邻**（**KNN**）和**XGBoost**算法。由于这些算法也可以用于分类目的，我们将在关于分类算法的章节中介绍它们。
- en: Important note
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You just got a very important tip to remember during the exam: linear learner,
    Factorization Machines, K-Nearest Neighbor, and XGBoost are suitable for both
    regression and classification problems. These algorithms are often known as algorithms
    for general purposes.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您在考试期间得到了一个非常重要的提示：线性学习者、因子分解机、K最近邻和XGBoost都适合回归和分类问题。这些算法通常被称为通用算法。
- en: 'With that, we have reached the end of this section on regression models. I
    hope you have enjoyed it; remember to check out our support material before you
    take your exam. By the way, you can use that reference material when you''re working
    on your daily activities! Now, let''s move on to another classical example of
    a machine learning problem: classification models.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们已经到达了关于回归模型的这一节结束。希望您喜欢它；记得在您参加考试之前查看我们的支持材料。顺便说一句，您可以在日常活动中使用那份参考资料！现在，让我们继续探讨另一个经典的机器学习问题示例：分类模型。
- en: Working with classification models
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与分类模型一起工作
- en: You have been learning what classification models are throughout this book.
    However, now, we are going to discuss some algorithms that are suitable for classification
    problems. Keep in mind that there are hundreds of classification algorithms out
    there, but since we are preparing for the AWS machine learning specialty exam,
    we will cover the ones that have been pre-built by AWS.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您在这本书中一直在学习分类模型是什么。然而，现在，我们将讨论一些适合分类问题的算法。请记住，有数百种分类算法，但由于我们正在为AWS机器学习专业考试做准备，我们将介绍AWS预先构建的算法。
- en: We already know what the linear learner does (and we know that it is suitable
    for both regression and classification tasks), so let's take a look at the other
    built-in algorithms for general purposes (which includes classification tasks).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道线性学习者做什么（我们知道它适合回归和分类任务），让我们看看其他用于通用目的（包括分类任务）的内置算法。
- en: We will start with **Factorization Machines**. Factorization machines are considered
    an extension of the linear learner, optimized to find the relationship between
    features within high-dimensional sparse datasets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从**因子分解机**开始。因子分解机被认为是线性学习者的扩展，优化以在具有高维稀疏数据集的特征之间找到关系。
- en: Important note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A very traditional use case for Factorization machines is *recommendation systems*,
    where we usually have a high level of sparsity in the data. During the exam, if
    you are faced with a general-purpose problem (either a regression or binary classification
    task) where the underlying datasets are sparse, then Factorization Machines are
    probably the best answer from an algorithm perspective.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分解机的一个非常传统的用例是*推荐系统*，其中我们通常有很高的数据稀疏度。在考试中，如果您面临一个通用问题（无论是回归还是二元分类任务）且底层数据集是稀疏的，那么从算法角度来看，因子分解机可能是最好的答案。
- en: When we use Factorization Machines in a regression model, the **Root Mean Square
    Error** (**RMSE**) will be used to evaluate the model. On the other hand, in binary
    classification mode, the algorithm will use Log Loss, Accuracy, and F1 score to
    evaluate results. We will have a deeper discussion about evaluation metrics in
    [*Chapter 8*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162), *Evaluating and
    Optimizing Models*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在回归模型中使用因子分解机时，将使用**均方根误差**（**RMSE**）来评估模型。另一方面，在二元分类模式下，算法将使用对数损失、准确率和F1分数来评估结果。我们将在[*第8章*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162)中更深入地讨论评估指标，*评估和优化模型*。
- en: You should be aware that Factorization Machines only accept input data in **recordIO-protobuf**
    format. This is because of the data sparsity issue, in which recordIO-protobuf
    is supposed to do a better job on data processing than text/.csv format.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该知道，因子分解机只接受**recordIO-protobuf**格式的输入数据。这是因为数据稀疏性问题，recordIO-protobuf被认为在数据处理方面比文本/.csv格式做得更好。
- en: 'The next built-in algorithm suitable for classification problems is known as
    K-Nearest Neighbors, or KNN for short. As the name suggests, this algorithm will
    try to find the *K* closest points to the input data and return either of the
    following predictions:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于分类问题的下一个内置算法被称为K-最近邻，简称KNN。正如其名所示，该算法将尝试找到输入数据的**K**个最近点，并返回以下预测之一：
- en: The most repeated class of the k closest points, if it's a classification task
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是分类任务，则k个最近点的最频繁出现的类别
- en: The average value of the label of the k closest points, if it's a regression
    task
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是回归任务，则k个最近点的标签的平均值
- en: We say that KNN is an **index-based algorithm** because it computes distances
    between points, assigns indexes for these points, and then stores the sorted distances
    and their indexes. With that type of data structure, KNN can easily select the
    top K closest points to make the final prediction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称KNN为**基于索引的算法**，因为它计算点之间的距离，为这些点分配索引，然后存储排序后的距离及其索引。有了这种类型的数据结构，KNN可以轻松选择最接近的K个点来进行最终预测。
- en: Note that K is a hyperparameter of KNN and should be optimized during the modeling
    process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，K是KNN的一个超参数，应该在建模过程中进行优化。
- en: The other AWS built-in algorithm available for general purposes, including classification,
    is known as **eXtreme Gradient Boosting**, or **XGBoost** for short. This is an
    ensemble, decision tree-based model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个AWS内置算法，适用于包括分类在内的通用目的，被称为**极端梯度提升**，简称**XGBoost**。这是一个基于集成和决策树的模型。
- en: XGBoost uses a set of **weaker** models (decision trees) to predict the target
    variable, which can be a regression task, binary class, or multi-class. This is
    a very popular algorithm and has been used in machine learning competitions by
    the top performers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost使用一系列**较弱的**模型（决策树）来预测目标变量，这可能是一个回归任务、二分类或多分类。这是一个非常流行的算法，并且已被顶尖选手在机器学习竞赛中使用。
- en: XGBoost uses a boosting learning strategy when one model tries to correct the
    error of the prior model. It carries the name "gradient" because it uses the gradient
    descent algorithm to minimize the loss when adding new trees.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost在添加新树时使用提升学习策略来纠正先前模型的错误。它被称为“梯度”，因为它使用梯度下降算法来最小化损失。
- en: Important note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The term weaker is used in this context to describe very simple decision trees.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，术语“较弱”用来描述非常简单的决策树。
- en: Although XGBoost is much more robust than a single decision tree, it is important
    to go into the exam with a clear understanding of what decision trees are and
    their main configurations. By the way, they are the base model of many ensemble
    algorithms, such as AdaBoost, Random Forest, Gradient Boost, and XGBoost.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然XGBoost比单个决策树更稳健，但在考试中有一个清晰的决策树及其主要配置的理解是很重要的。顺便说一下，它们是许多集成算法（如AdaBoost、随机森林、梯度提升和XGBoost）的基础模型。
- en: 'Decision trees are rule-based algorithms that organize decisions in the form
    of a tree, as shown in the following diagram:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是基于规则的算法，以树的形式组织决策，如下面的图所示：
- en: '![Figure 7.11 – Example of what a decision tree model looks like'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.11 – 决策树模型示例'
- en: '](img/B16735_07_011.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_011.jpg)'
- en: Figure 7.11 – Example of what a decision tree model looks like
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 决策树模型示例
- en: They are formed by a root node (at the very top of the tree), intermediary or
    decision nodes (in the middle of the tree), and leaf nodes (bottom nodes with
    no splits). The depth of the tree is given by the difference between the root
    node and the very last leaf node. For example, in the preceding diagram, the depth
    of the tree is 3\.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 它们由根节点（位于树的顶部）、中间或决策节点（位于树的中间）和叶节点（底部节点，没有分支）组成。树的深度由根节点和最后一个叶节点之间的差异给出。例如，在前面的图中，树的深度是3。
- en: The depth of the tree is one of the most important hyperparameters of this type
    of model and it is often known as the **max depth**. In other words, max depth
    controls the maximum depth that a decision tree can reach.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 树的深度是此类模型最重要的超参数之一，通常被称为**最大深度**。换句话说，最大深度控制决策树可以达到的最大深度。
- en: Another very important hyperparameter of decision tree models is known as the
    minimum number of samples/observations in the leaf nodes. It is also used to control
    the growth of the tree.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型另一个非常重要的超参数被称为叶节点中的最小样本数/观测数。它也用于控制树的生长。
- en: Decision trees have many other types of hyperparameters, but these two are especially
    important for controlling how the model overfits. Decision trees with a high depth
    or very small number of observations in the leaf nodes are likely to face issues
    during extrapolation/prediction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有许多其他类型的超参数，但这两个对于控制模型过拟合尤为重要。深度较高或叶子节点观察值非常少的决策树在预测/外推过程中可能会遇到问题。
- en: 'The reason for this is simple: decision trees use data from the leaf nodes
    to make predictions, based on the proportion (for classification tasks) or average
    value (for regression tasks) of each observation/target variable that belongs
    to that node. Thus, the node should have enough data to make good predictions
    outside the training set.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 原因很简单：决策树使用叶子节点中的数据来做出预测，基于每个观察值/目标变量属于该节点的比例（用于分类任务）或平均值（用于回归任务）。因此，节点应该有足够的数据来在训练集之外做出良好的预测。
- en: In case you face the term **CART** during the exam, you should know that it
    stands for **Classification and Regression Trees**, since decision trees can be
    used for classification and regression tasks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在考试中遇到术语**CART**，你应该知道它代表**分类和回归树**，因为决策树可以用于分类和回归任务。
- en: To select the best variables to split the data in the tree, the model will choose
    the ones that maximize the separation of the target variables across the nodes.
    This task can be performed by different methods, such as **Gini** and **Information
    Gain**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择在树中分割数据的最佳变量，模型将选择最大化节点间目标变量分离的变量。这项任务可以通过不同的方法执行，例如**基尼系数**和**信息增益**。
- en: Forecasting models
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测模型
- en: '**Time series**, or **TS** for short, refers to data points that are collected
    on a regular basis with an ordered dependency. Time series have a measure, a fact,
    and a time unit, as shown in the following image:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间序列**，或简称**TS**，指的是按顺序依赖关系定期收集的数据点。时间序列具有度量、事实和时间单位，如下面的图像所示：'
- en: '![Figure 7.12 – Time series statement'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.12 – 时间序列陈述'
- en: '](img/B16735_07_012.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_07_012.jpg](img/B16735_07_012.jpg)'
- en: Figure 7.12 – Time series statement
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 时间序列陈述
- en: 'Additionally, time series can be classified as **univariate** or **multivariate**.
    Univariate time series have just one variable connected across a period of time,
    while a multivariate time series have two or more variables connected across a
    period of time. The following graph shows the univariate time series we showed
    in the preceding image:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，时间序列可以分为**单变量**或**多变量**。单变量时间序列在一段时间内只有一个变量，而多变量时间序列在一段时间内有两个或更多变量。以下图表显示了我们在前面图像中所示的单变量时间序列：
- en: '![Figure 7.13 – Time series example'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.13 – 时间序列示例'
- en: '](img/B16735_07_013.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_07_013.jpg](img/B16735_07_013.jpg)'
- en: Figure 7.13 – Time series example
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 时间序列示例
- en: 'Time series can be decomposed as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列可以按以下方式分解：
- en: '**Observed** or **level**: The average values of the series'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察值**或**水平**：序列的平均值'
- en: '**Trend**: Increasing, decreasing pattern (sometimes, there is no trend)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**趋势**：增加或减少的模式（有时没有趋势）'
- en: '**Seasonality**: Regular peaks at specific periods of time (sometimes, there
    is no seasonality)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**季节性**：在特定时间段的规律性高峰（有时没有季节性）'
- en: '**Noise**: Something we cannot explain'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声**：我们无法解释的东西'
- en: 'Sometimes, we can also find isolated peaks in the series that cannot be captured
    in a forecasting model. In such cases, we might want to consider those peaks as
    outliers. The following is a decomposition of the time series shown in the preceding
    graph:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们也可以在序列中找到一些孤立的高峰，这些高峰在预测模型中无法捕捉到。在这种情况下，我们可能希望将这些高峰视为异常值。以下是对前面图表所示时间序列的分解：
- en: '![Figure 7.14 – Time series decomposition'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.14 – 时间序列分解'
- en: '](img/B16735_07_014.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_07_014.jpg](img/B16735_07_014.jpg)'
- en: Figure 7.14 – Time series decomposition
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 时间序列分解
- en: It's also worth highlighting that we can use **additive** or **multiplicative**
    approaches to decompose time series. Additive models suggest that your time series
    *adds* each component to explain the target variable; that is, **y(t) = level
    + trend + seasonality + noise**.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，我们可以使用**加法**或**乘法**方法来分解时间序列。加法模型表明，你的时间序列将每个组成部分**加**起来以解释目标变量；也就是说，**y(t)
    = 水平 + 趋势 + 季节性 + 噪声**。
- en: Multiplicative models, on the other hand, suggest that your time series *multiplies*
    each component to explain the target variable; that is, **y(t) = level * trend
    * seasonality * noise**.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法模型另一方面表明，你的时间序列将每个组成部分**乘**起来以解释目标变量；也就是说，**y(t) = 水平 * 趋势 * 季节性 * 噪声**。
- en: In the next section, we will have a closer look at time series components.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地研究时间序列的组成部分。
- en: Checking the stationarity of time series
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查时间序列的平稳性
- en: Decomposing time series and understanding how their components interact with
    additive and multiplicative models is a great achievement! However, the more we
    learn, the more we question ourselves. Maybe you have realized that time series
    without trend and seasonality are easier to predict than the ones with all those
    components!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 分解时间序列并理解它们的组成部分如何通过加法和乘法模型相互作用是一项伟大的成就！然而，我们学得越多，我们对自己的疑问就越多。也许你已经意识到，没有趋势和季节性的时间序列比具有所有这些成分的时间序列更容易预测！
- en: That is naturally right. If you don't have to understand trend and seasonality,
    and if you don't have control over the noise, all you have to do is explore the
    observed values and find their regression relationship.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然而然的事情。如果你不需要理解趋势和季节性，如果你无法控制噪声，你所要做的就是探索观察到的值并找到它们的回归关系。
- en: We refer to time series with constant mean and variance across a period of time
    as **stationary**. In general, time series *with* trend and seasonality are *not*
    stationary. It is possible to apply data transformations to the series to transform
    it into a stationary time series so that the modeling task tends to be easier.
    This type of transformation is known as **differentiation**.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将具有在一段时间内保持恒定均值和方差的时序称为**平稳的**。一般来说，具有趋势和季节性的时序**不是**平稳的。可以通过对序列应用数据转换来将其转换为平稳时序，从而使建模任务变得更容易。这种转换被称为**微分**。
- en: While you are exploring a time series, you can check stationarity by applying
    hypothesis tests, such as **Dickey-Fuller**, **KPSS**, and **Philips-Perron**,
    just to mention a few. If you find it non-stationary, then you can apply differentiation
    to make it a stationary time series. Some algorithms already have that capability
    embedded.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当你探索时间序列时，你可以通过应用假设检验来检查平稳性，例如**Dickey-Fuller**、**KPSS**和**Philips-Perron**，仅举几例。如果你发现它是非平稳的，那么你可以应用微分来使其成为平稳时间序列。一些算法已经内置了这种能力。
- en: Exploring, exploring, and exploring
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索，探索，再探索
- en: At this point, I am sure I don't have to remind you that exploration tasks happen
    all the time in data science. Nothing is different here. While you are building
    time series models, you might want to have a look at the data and check if it
    is suitable for this type of modeling.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我确信我无需提醒你，探索任务在数据科学中一直都在发生。这里没有什么不同。当你构建时间序列模型时，你可能想查看数据并检查它是否适合这种类型的建模。
- en: '**Autocorrelation plots** are one of the tools that you can use for time series
    analysis. Autocorrelation plots allow you to check the correlations between lags
    in the time series. The following graph shows an example of this type of visualization:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**自相关图**是你可以用于时间序列分析的工具之一。自相关图允许你检查时间序列中滞后之间的相关性。以下图表展示了这种可视化类型的示例：'
- en: '![Figure 7.15 – Autocorrelation plot'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.15 – 自相关图](img/B16735_07_015.jpg)'
- en: '](img/B16735_07_015.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.15 – 自相关图](img/B16735_07_015.jpg)'
- en: Figure 7.15 – Autocorrelation plot
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 自相关图
- en: Remember, if you're playing with univariate time series, your time series just
    has one variable, so finding autocorrelation across the lags of your unique variable
    is crucial to understanding whether you can build a good model or not.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，如果你在处理单变量时间序列，你的时间序列只有一个变量，因此找到你独特变量的滞后之间的自相关对于理解你是否能构建一个好的模型至关重要。
- en: And yes, it turns out that, sometimes, it might happen that you don't have a
    time series in front of you. Furthermore, no matter your efforts, you will not
    be able to model this data as a time series. This type of data is often known
    as **white noise**.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，有时候可能会发生这样的情况，你面前没有时间序列。而且，无论你多么努力，你都无法将这类数据建模为时间序列。这类数据通常被称为**白噪声**。
- en: Another type of series that we cannot predict is known as **random walk**. Random
    walks are random by nature, but they have a dependency on the previous time step.
    For example, the next point of a random walk could be a random number between
    0 and 1, and also the last point of the series.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种我们无法预测的序列类型被称为**随机游走**。随机游走本质上是随机的，但它们依赖于前一时间步。例如，随机游走的下一个点可能是在0和1之间的随机数，也可能是序列的最后一个点。
- en: Important note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Be careful if you come across those terms in the exam and remember to relate
    them to randomness in time series.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在考试中遇到这些术语，请记住将它们与时间序列中的随机性联系起来。
- en: With that, we have covered the main theory about time series modeling. You should
    also be aware that the most popular algorithms out there for working with time
    series are known as **Auto-Regressive Integrated Moving Average** (**ARIMA**)
    and **Exponential Smoothing** (**ETS**). We will not look at the details of these
    two models. Instead, we will see what AWS can offer us in terms of time series
    modeling.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经涵盖了时间序列建模的主要理论。你还应该知道，目前最流行的用于处理时间序列的算法被称为**自回归积分移动平均**（**ARIMA**）和**指数平滑**（**ETS**）。我们不会查看这两个模型的细节。相反，我们将看看AWS在时间序列建模方面能为我们提供什么。
- en: Understanding DeepAR
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解DeepAR
- en: The **DeepAR** forecasting algorithm is a built-in SageMaker algorithm that's
    used to forecast a one-dimensional time series using a **Recurrent Neural Network**
    (**RNN**).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepAR**预测算法是SageMaker内置的算法，用于使用**循环神经网络**（**RNN**）预测一维时间序列。'
- en: Traditional time series algorithms, such as ARIMA and ETS, are designed to fit
    one model per time series. For example, if you want to forecast sales per region,
    you might have to create one model per region, since each region might have its
    own sales behaviors. DeepAR, on the other hand, allows you to operate more than
    one time series in a single model, which seems to be a huge advantage for more
    complex use cases.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的时序算法，如ARIMA和ETS，是为每个时间序列拟合一个模型而设计的。例如，如果你想预测每个地区的销售额，你可能需要为每个地区创建一个模型，因为每个地区可能有自己独特的销售行为。另一方面，DeepAR允许你在单个模型中操作多个时间序列，这在更复杂的使用案例中似乎是一个巨大的优势。
- en: 'The input data for DeepAR, as expected, is one *or more* time series. Each
    of these time series can be associated with the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR的输入数据，正如预期的那样，是一个或多个时间序列。这些时间序列中的每一个都可以与以下内容相关联：
- en: A vector of static (time-independent) categorical features, controlled by the
    `cat` field
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由`cat`字段控制的静态（时间无关性）分类特征向量
- en: a vector of dynamic (time-dependent) time series, controlled by `dynamic_feat`
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由`dynamic_feat`控制的动态（时间依赖性）时间序列向量
- en: Important note
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that the ability to train and make predictions on top of multiple time
    series is strictly related to the vector of static categorical features. While
    defining the time series that DeepAR will train on, you can set categorical variables
    to specify which group each time series belongs to.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在多个时间序列上训练和进行预测的能力与静态分类特征向量密切相关。在定义DeepAR将要训练的时间序列时，你可以设置分类变量来指定每个时间序列属于哪个组。
- en: Two of the main hyperparameters of DeepAR are `context_length`, which is used
    to control how far in the past the model can see during the training process,
    and `prediction_length`, which is used to control how far in the future the model
    will output predictions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR的两个主要超参数是`context_length`，它用于控制在训练过程中模型可以看到多远的历史，以及`prediction_length`，它用于控制模型将输出预测的多远未来。
- en: 'DeepAR can also handle missing values, which, in this case, refers to existing
    gaps in the time series. A very interesting functionality of DeepAR is its ability
    to create derived features from time series. These derived features, which are
    created from basic time frequencies, help the algorithm learn time-dependent patterns.
    The following table shows all the derived features created by DeepAR, according
    to each type of time series that it is trained on:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR还可以处理缺失值，在这种情况下，指的是时间序列中存在的缺口。DeepAR的一个非常有趣的功能是它能够从时间序列中创建派生特征。这些派生特征，由基本时间频率创建，有助于算法学习时间依赖性模式。以下表格显示了DeepAR根据其训练的每种类型的时间序列创建的所有派生特征：
- en: '![Figure 7.16 – DeepAR derived features per frequency of time series'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.16 – 按时间序列频率提取的DeepAR特征'
- en: '](img/B16735_07_016.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_016.jpg)'
- en: Figure 7.16 – DeepAR derived features per frequency of time series
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 按时间序列频率提取的DeepAR特征
- en: We have now completed this section about forecasting models. Next, we will have
    a look at the last algorithm regarding supervised learning; that is, the **Object2Vec**
    algorithm.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了关于预测模型的这一章节。接下来，我们将查看关于监督学习的最后一个算法；即**Object2Vec**算法。
- en: Object2Vec
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Object2Vec
- en: 'Object2Vec is a built-in SageMaker algorithm that generalizes the well-known
    **word2vec** algorithm. Object2Vec is used to create **embedding spaces** for
    high multidimensional objects. These embedding spaces are, per the definition,
    compressed representatiosn of the original object and can be used for multiple
    purposes, such as feature engineering or object comparison:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Object2Vec是SageMaker内置算法，它泛化了众所周知的**word2vec**算法。Object2Vec用于为高维对象创建**嵌入空间**。根据定义，这些嵌入空间是原始对象的压缩表示，可用于多种目的，例如特征工程或对象比较：
- en: '![Figure 7.17 – A visual example of an embedding space'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.17 – 嵌入空间的视觉示例'
- en: '](img/B16735_07_017.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片/B16735_07_017.jpg]'
- en: Figure 7.17 – A visual example of an embedding space
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 嵌入空间的视觉示例
- en: The preceding diagram illustrates what we mean by an embedding space. The first
    and the last layers of the neural network model just map the input data with itself
    (represented by the same vector size).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图展示了我们所说的嵌入空间。神经网络模型的第一层和最后一层只是将输入数据映射到自身（由相同大小的向量表示）。
- en: As we move on to the internal layers of the model, the data is compressed more
    and more until it hits the layer in the middle of this architecture, known as
    the embedding layer. On that particular layer, we have a smaller vector, which
    aims to be an accurate and compressed representation of the high-dimensional original
    vector from the first layer.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入到模型的内部层时，数据被越来越压缩，直到它达到这个架构中间的层，也就是嵌入层。在这一特定层，我们有一个更小的向量，其目的是准确且压缩地表示来自第一层的高维原始向量。
- en: With this, we just completed our first section about machine learning algorithms
    in AWS. Coming up next, we will have a look at some unsupervised algorithms.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们刚刚完成了关于AWS中机器学习算法的第一部分。接下来，我们将查看一些无监督算法。
- en: Unsupervised learning
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'AWS provides several unsupervised learning algorithms for the following tasks:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为以下任务提供了几种无监督学习算法：
- en: 'Clustering:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类：
- en: K-means algorithm
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means算法
- en: 'Dimension reduction:'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低：
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: 'Pattern recognition:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式识别：
- en: IP Insights
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP洞察
- en: 'Anomaly detection:'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测：
- en: '**Random Cut Forest Algorithm** (**RCF**)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机切割森林算法**（**RCF**）'
- en: 'Let''s start by talking about clustering and how the most popular clustering
    algorithm works: K-means.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论聚类以及最流行的聚类算法K-means的工作原理开始。
- en: Clustering
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Clustering algorithms are very popular in data science. Basically, they aim
    to identify groups in a given dataset. Technically, we call these findings or
    groups **clusters**. Clustering algorithms belong to the field of non-supervised
    learning, which means that they don't need a label or response variable to be
    trained.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在数据科学中非常流行。基本上，它们旨在识别给定数据集中的组。技术上，我们称这些发现或组为**簇**。聚类算法属于非监督学习领域，这意味着它们不需要标签或响应变量来训练。
- en: This is just fantastic because labeled data used to be scarce. However, it comes
    with some limitations. The main one is that clustering algorithms provide clusters
    for you, but not the meaning of each cluster. Thus, someone, as a subject matter
    expert, has to analyze the properties of each cluster to define their meanings.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太棒了，因为以前标记的数据很稀缺。然而，它也有一些限制。主要的一个是，聚类算法为你提供簇，但不提供每个簇的含义。因此，必须有人，作为领域专家，分析每个簇的特性来定义它们的含义。
- en: There are many types of clustering approaches, such as hierarchical clustering
    and partitional clustering. Inside each approach, we will find several algorithms.
    However, **K-Means** is probably the most popular clustering algorithm and you
    are likely to come across it in your exam, so let's take a closer look at it.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种聚类方法，例如层次聚类和划分聚类。在每种方法内部，我们会找到几种算法。然而，**K-Means**可能是最流行的聚类算法，你很可能在考试中会遇到它，所以让我们更详细地看看它。
- en: When we are playing with K-means, somehow, we have to specify the number of
    clusters that we want to create. Then, we have to allocate the data points across
    each cluster so that each data point will belong to a single cluster. This is
    exactly what we expect as a result at the end of the clustering process!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们玩K-means时，我们必须要指定我们想要创建的簇的数量。然后，我们必须将数据点分配到每个簇中，以便每个数据点只属于一个簇。这正是我们在聚类过程结束时期望得到的结果！
- en: You, as a user, have to specify the number of clusters you want to create and
    pass this number to K-means. Then, the algorithm will randomly initiate the central
    point of each cluster, which is also known as **centroid** initialization.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 您作为用户，必须指定您想要创建的簇的数量，并将此数字传递给 K-means。然后，算法将随机初始化每个簇的中心点，这也被称为**质心初始化**。
- en: Once we have the centroids of each cluster, all we need to do is assign a cluster
    to each data point. To do that, we have to use a proximity or distance metric!
    Let's adopt the term distance metric.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了每个簇的质心，我们只需要为每个数据点分配一个簇。为此，我们必须使用邻近度或距离度量！让我们采用距离度量的术语。
- en: The **distance metric** is responsible for calculating the distance between
    data points and centroids. The data point will belong to the closer cluster centroid,
    according to the distance metric!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离度量**负责计算数据点与质心之间的距离。根据距离度量，数据点将属于最近的簇质心！'
- en: 'The most well-known and used distance metric is called **Euclidean distance**
    and the math behind it is really simple: imagine that the points of your dataset
    are composed of two dimensions, X and Y. So, we could consider points a and b
    as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名且使用最广泛的距离度量称为**欧几里得距离**，其背后的数学非常简单：想象一下，您的数据集点由两个维度组成，X 和 Y。因此，我们可以将点 a 和
    b 考虑如下：
- en: a (X=1, Y=1)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a (X=1, Y=1)
- en: b (X=2, Y=5)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b (X=2, Y=5)
- en: 'The Euclidean distance between points a and b is given by the following formula,
    where X1 and Y1 refer to the values of point a and X2 and Y2 refer to the values
    of point b:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 点 a 和 b 之间的欧几里得距离由以下公式给出，其中 X1 和 Y1 代表点 a 的值，X2 和 Y2 代表点 b 的值：
- en: '![](img/image17.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image17.jpg)'
- en: The same function can be generalized by the following equation:![](img/image18.png)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的函数可以通过以下方程进行推广！[](img/image18.png)
- en: Once we have completed this process and assigned a cluster with each data point,
    we have to recalculate the cluster centroids. This process can be done by different
    methods, such as **single link**, **average link**, and **complete link**.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成这个过程，并为每个数据点分配了一个簇，我们必须重新计算簇质心。这个过程可以通过不同的方法完成，例如**单链**、**平均链**和**完整链**。
- en: Due to this centroid refreshment, we will have to keep checking the closest
    cluster for each data point and keep refreshing the centroids. We have to reexecute
    steps 1 and 2 **iteratively**, until the cluster centroids converge or the maximum
    number of allowed iterations is reached.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种质心更新，我们必须为每个数据点持续检查最近的簇，并持续更新质心。我们必须**迭代地**重新执行步骤 1 和 2，直到簇质心收敛或达到允许的最大迭代次数。
- en: 'Alright; let''s recap the components that compose the K-means method:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 好的；让我们回顾一下构成 K-means 方法的组件：
- en: Centroid initialization, cluster assignment, centroid refreshment, and then
    redo the last two steps until it converges.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心初始化、簇分配、质心更新，然后重复最后两个步骤，直到收敛。
- en: 'The algorithm itself:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法本身：
- en: 'A distance metric to assign data points to each cluster:'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个簇分配数据点的距离度量：
- en: We have selected Euclidian distance here.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在这里选择了欧几里得距离。
- en: 'And a linkage method to recalculate the cluster centroids:'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及一种重新计算簇质心的链接方法：
- en: For the sake of our demonstration, we'll select the average linkage.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了我们的演示，我们将选择平均链接。
- en: With these definitions, we are ready to walk through our real example, step
    by step. As with our regression models, some support material is also available
    for your reference.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些定义，我们可以一步一步地通过我们的真实示例。就像我们的回归模型一样，也有一些支持材料可供您参考。
- en: Computing K-means step by step
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步计算 K-means
- en: 'In this example, we will simulate K-means in a very small dataset, with only
    two columns (x and y) and six data points (A, B, C, D, E, F), as defined in the
    following table:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在一个非常小的数据集中模拟 K-means，只有两列（x 和 y）和六个数据点（A、B、C、D、E、F），如下表所示：
- en: '![Figure 7.18 – Input data for K-means'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.18 – K-means 的输入数据'
- en: '](img/B16735_07_018.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_018.jpg)'
- en: Figure 7.18 – Input data for K-means
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – K-means 的输入数据
- en: 'In the preceding table, we created three clusters with the following centroids:
    (1,1), (2,2), (5,5). The number of clusters (3) was defined *a priori* and the
    centroid for each cluster was randomly defined. The following graph shows the
    stage of the algorithm we are at right now:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表中，我们创建了三个簇，其质心如下：(1,1)，(2,2)，(5,5)。簇的数量（3）是**预先定义的*，每个簇的质心是随机定义的。以下图表显示了我们现在算法所处的阶段：
- en: '![Figure 7.19 – Plotting the K-means results before completing the first iteration'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.19 – 完成第一次迭代前的 K-means 结果'
- en: '](img/B16735_07_019.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_019.jpg)'
- en: Figure 7.19 – Plotting the K-means results before completing the first iteration
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 在完成第一次迭代前绘制K-means结果
- en: 'Here, you can''t see points A, B, and C since they overlap with cluster centroids,
    but don''t worry – they will appear soon. What we have to do now is compute the
    distance of each data point to each cluster centroid. Then, we need to choose
    the cluster that is the closest to each point:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你无法看到点A、B和C，因为它们与聚类中心重叠，但不用担心 – 它们很快就会出现。我们现在必须做的是计算每个数据点到每个聚类中心的距离。然后，我们需要选择每个点最近的聚类：
- en: '![Figure 7.20 – Processing iteration 1'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.20 – 处理迭代1'
- en: '](img/B16735_07_020.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_020.jpg)'
- en: Figure 7.20 – Processing iteration 1
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 – 处理迭代1
- en: 'In the preceding table, we have the following elements:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表中，我们有以下元素：
- en: Each row represents a data point.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行代表一个数据点。
- en: The first six columns represent the centroid axis (x and y) of each cluster.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前六列代表每个聚类的质心轴（x和y）。
- en: The next three columns represent the distance of each data point to each cluster
    centroid.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的三列代表每个数据点到每个聚类质心的距离。
- en: The last column represents the clusters that are the closest to each data point.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一列代表每个数据点最近的聚类。
- en: Looking at data point A (first row), we can see that it was assigned to cluster
    1 because the distance from data point A to cluster 1 is 0 (remember when I told
    you they were overlapping?). The same calculation happens to all other data points
    to define a cluster for each data point.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 看数据点A（第一行），我们可以看到它被分配到聚类1，因为数据点A到聚类1的距离是0（记得我告诉过你们它们是重叠的吗？）。同样的计算发生在所有其他数据点上，以定义每个数据点的聚类。
- en: Before we move on, you might want to see how we computed those distances between
    the clusters and the data points. As we stated previously, we have used the Euclidian
    distance, so let's see that in action. For demonstration purposes, let's check
    out how we came up with the distance between data point A and cluster 3 (the first
    row in *Figure 7.20*, column `distance-c3`, value 5,7).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，你可能想看看我们是如何计算聚类和数据点之间的距离的。正如我们之前所述，我们使用了欧几里得距离，所以让我们看看它是如何起作用的。为了演示目的，让我们检查一下数据点A和聚类3（图7.20的第一行，`distance-c3`列，值5,7）之间的距离是如何计算出来的。
- en: 'First of all, we applied the same equation from the following formula:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应用了以下公式中的相同方程：
- en: '![](img/image171.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '](img/image171.jpg)'
- en: 'Here, we have the following:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: X1 = X of data point A = 1
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X1 = 数据点A的X = 1
- en: Y1 = Y of data point A = 1
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y1 = 数据点A的Y = 1
- en: X2 = X of cluster 3 = 5
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X2 = 第3个聚类的X = 5
- en: Y2 = Y of cluster 3 = 5
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y2 = 第3个聚类的Y = 5
- en: 'Applying the formula step by step, we will come up with the following results:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步应用公式，我们将得到以下结果：
- en: '![Figure 7.21 – Computing the Euclidian distance step by step'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.21 – 逐步计算欧几里得距离'
- en: '](img/image22.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image22.jpg)'
- en: Figure 7.21 – Computing the Euclidian distance step by step
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 – 逐步计算欧几里得距离
- en: 'That is just fantastic, isn''t it? We have almost completed the first iteration
    of K-means. In the very last step of iteration 1, we have to refresh the cluster
    centroids. Remember: initially, we randomly defined those centroids, but now,
    we have just assigned some data points to each cluster, which means we should
    be able to identify where the central point of the cluster is.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太棒了，不是吗？我们几乎完成了K-means的第一次迭代。在迭代1的最后一步，我们必须刷新聚类中心。记住：最初，我们随机定义了这些中心，但现在，我们已经将一些数据点分配给每个聚类，这意味着我们应该能够识别出聚类的中心点在哪里。
- en: 'In this example, we have defined that we are going to use the **average linkage**
    method to refresh the cluster centroids. This is a very simple step, and the results
    are present in the following table:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们定义了我们将使用**平均链连接**方法来刷新聚类中心。这是一个非常简单的步骤，结果如下表所示：
- en: '![Figure 7.22 – K-means results after iteration 1'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.22 – 第一次迭代后的K-means结果'
- en: '](img/B16735_07_022.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_022.jpg)'
- en: Figure 7.22 – K-means results after iteration 1
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 – 第一次迭代后的K-means结果
- en: The preceding table shows the same data points that we are dealing with (by
    the way, they will never change), and the centroids of clusters 1, 2, and 3\.
    Those centroids are quite different from what they were initially, as shown in
    *Figure 7.18*. This is because they were refreshed using average linkage! The
    method got the average value of all the x and y values of the data points of each
    cluster. For example, let's see how we came up with (1.5, 3.5) as centroids of
    cluster 2.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格显示了我们在处理的数据点（顺便说一句，它们永远不会改变），以及簇1、2和3的中心。这些中心与*图7.18*中显示的初始中心相当不同。这是因为它们使用了平均链接来更新！这种方法得到了每个簇的数据点的所有x和y值的平均值。例如，让我们看看我们是如何得到(1.5,
    3.5)作为簇2的中心。
- en: 'If you look at *Figure 7.20*, you will see that cluster 2 only has two data
    points assigned to it: B and E. These are the second and fifth rows in that image.
    If we take the average values of the x-axis of each point, then we''ll have **(2
    + 1) / 2 = 1.5** and **(2 + 5) / 2 = 3.5**.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看*图7.20*，你会看到只有两个数据点分配给了簇2：B和E。这些是图像中的第二行和第五行。如果我们取每个点的x轴的平均值，那么我们将得到**(2
    + 1) / 2 = 1.5**和**(2 + 5) / 2 = 3.5**。
- en: 'With that, we are done with iteration 1 of K-means and we can view the results:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了K-means的第一次迭代，我们可以查看结果：
- en: '![Figure 7.23 – Plotting the K-means results after the first iteration'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.23 – 第一次迭代后的K-means结果'
- en: '](img/B16735_07_023.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_023.jpg)'
- en: Figure 7.23 – Plotting the K-means results after the first iteration
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23 – 第一次迭代后的K-means结果
- en: 'Now, we can see almost all the data points, except for data point A because
    it is still overlapping with the centroid of cluster 1\. Moving on, we have to
    redo the following steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到几乎所有的数据点，除了数据点A，因为它仍然与簇1的中心重叠。继续前进，我们必须重新执行以下步骤：
- en: Recalculate the distance between each data point and each cluster centroid and
    reassign clusters, if needed
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新计算每个数据点与每个簇中心的距离，并在必要时重新分配簇
- en: Recalculate the cluster centroids
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新计算簇中心
- en: 'We do those two tasks many times until the cluster centroids converge and they
    don''t change anymore *or* we reach the maximum number of allowed iterations,
    which can be set as a hyperparameter of K-means. For demonstration purposes, after
    four iterations, our clusters will look as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会多次执行这两项任务，直到簇中心收敛并且不再改变，或者我们达到允许的最大迭代次数，这可以作为K-means的超参数设置。为了演示目的，经过四次迭代后，我们的簇将看起来如下：
- en: '![Figure 7.24 – Plotting the K-means results after the fourth iteration'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.24 – 第四次迭代后的K-means结果'
- en: '](img/B16735_07_024.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_024.jpg)'
- en: Figure 7.24 – Plotting the K-means results after the fourth iteration
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 – 第四次迭代后的K-means结果
- en: On the fourth iteration, our cluster centroids look pretty consistent, and we
    can clearly see that we can group our six data points according to their proximity.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四次迭代中，我们的簇中心看起来相当一致，我们可以清楚地看到我们可以根据它们的邻近性将六个数据点分组。
- en: Important note
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In this example, we have only set two dimensions for each data point (dimension
    x and y). In real use cases, we can see far more dimensions, and that's why clustering
    algorithms play a very important role in identifying groups in the data in a more
    automated fashion.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们只为每个数据点设置了两个维度（维度x和y）。在实际应用中，我们可以看到更多的维度，这就是为什么聚类算法在以更自动化的方式识别数据中的组时扮演着非常重要的角色。
- en: I hope you have enjoyed how to compute K-means from scratch! I am sure this
    knowledge will be beneficial for your exam and for your career as a data scientist.
    By the way, I have told you many times that data scientists must be skeptical
    and curious, so you might be wondering why we defined three clusters in this example
    and not two or four. You may also be wondering how we measure the quality of the
    clusters.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你喜欢从头计算K-means的过程！我相信这些知识对你的考试和作为数据科学家的职业生涯都会有所帮助。顺便说一句，我已经告诉你很多次，数据科学家必须持怀疑态度和好奇心，所以你可能想知道为什么我们在这个例子中定义了三个簇而不是两个或四个。你也可能想知道我们如何衡量簇的质量。
- en: You didn't think I wouldn't explain this to you, did you? In the next section,
    we will clarify those points together.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你不会认为我不会向你解释这一点，对吧？在下一节中，我们将一起澄清这些点。
- en: Defining the number of clusters and measuring cluster quality
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义簇的数量和衡量簇的质量
- en: Although K-means is a great algorithm for finding patterns in your data, it
    will not provide the meaning of each cluster, nor the number of clusters you have
    to create to maximize cluster quality.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然K-means是一个在数据中寻找模式的好算法，但它不会提供每个簇的含义，也不会提供你必须创建以最大化簇质量的簇数量。
- en: In clustering, cluster quality means that we want to create groups with a high
    homogeneity among the elements of the same cluster, and a high heterogeneity among
    the elements of different clusters. In other words, the elements of the same clusters
    should be close/similar, whereas the elements of different clusters should be
    well separated.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，簇质量意味着我们希望创建具有高同质性的簇内元素和具有高异质性的簇间元素。换句话说，同一簇的元素应该接近/相似，而不同簇的元素应该很好地分离。
- en: 'One way to compute the cluster''s homogeneity is by using a metric known as
    **Sum of Square Errors**, or **SSE** for short. This metric will compute the sum
    of squared differences between each data point and its cluster centroid. For example,
    when all the data points are located at the same point where the cluster centroid
    is, then SSE will be 0\. In other words, we want to minimize SSE. The following
    equation formally defines SSE:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 计算簇同质性的一个方法是通过使用一个称为**平方误差和**的度量，或简称为**SSE**。这个度量将计算每个数据点与其簇质心的平方差之和。例如，当所有数据点都位于簇质心所在的同一点时，SSE将为0。换句话说，我们希望最小化SSE。以下方程正式定义了SSE：
- en: '![](img/image26.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image26.jpg)'
- en: Now that we know how to check cluster quality, it is easier to understand how
    to define the number of appropriated clusters for a given dataset. All we have
    to do is find several clusters that minimize SSE. A very popular method that works
    around that logic is known as the **Elbow method**.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何检查簇质量，就更容易理解如何为给定的数据集定义适当的簇数量。我们只需要找到几个最小化SSE的簇。一个围绕该逻辑工作的非常流行的方法被称为**肘部方法**。
- en: The Elbow method proposes executing the clustering algorithm many times. In
    each execution, we will test a different number of clusters, *k*. After each execution,
    we compute the SSE related to that *k* number of cluster clusters. Finally, we
    can plot these results and select the number of *k* where the SSE stops to drastically
    decrease.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部方法建议多次执行聚类算法。在每次执行中，我们将测试不同数量的簇，*k*。在每次执行后，我们计算与该*k*数量簇相关的SSE。最后，我们可以绘制这些结果，并选择SSE急剧下降的*k*数量。
- en: Important note
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Adding more clusters will naturally decrease the SSE. In the Elbow method, we
    want to find the point where that change becomes smoother.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多的簇将自然会降低SSE。在肘部方法中，我们希望找到这种变化变得更为平滑的点。
- en: 'In the previous example, we decided to create three clusters. The following
    graph shows the Elbow analysis that supports this decision:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们决定创建三个簇。以下图表显示了支持这一决策的肘部分析：
- en: '![Figure 7.25 – The Elbow method'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.25 – 肘部方法'
- en: '](img/B16735_07_025.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_07_025.jpg)'
- en: Figure 7.25 – The Elbow method
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.25 – 肘部方法
- en: We can conclude that adding more than three or four clusters will add unnecessary
    complexity to the clustering process.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，添加超过三个或四个簇将给聚类过程增加不必要的复杂性。
- en: Of course, you should always consider the business background while defining
    the number of clusters. For example, if you are creating a customer segmentation
    model and your company has prepared the commercial team and business processes
    to support four segments of customers, considering the preceding graph, there
    is no harm in setting four clusters instead of three.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在定义簇的数量时，你应该始终考虑业务背景。例如，如果你正在创建客户细分模型，而你的公司已经为四个客户细分准备了商业团队和业务流程，考虑到前面的图表，将簇设置为四个而不是三个是没有害处的。
- en: Finally, you should know that AWS has implemented K-means as part of their list
    of built-in algorithms. In other words, you don't have to use external libraries
    or bring your own algorithm to play with K-means on AWS.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该知道AWS已经将K-means算法作为其内置算法列表的一部分实现了。换句话说，你不需要使用外部库或自己带来算法来在AWS上使用K-means。
- en: Conclusion
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: 'That was a really good accomplishment: you just mastered the basics of clustering
    algorithms and you should now be able to drive your own projects and research
    about this topic! For the exam, remember that clustering belongs to the unsupervised
    field of machine learning, so there is no need to have labeled data.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 那真是一个了不起的成就：你刚刚掌握了聚类算法的基础，现在你应该能够自己开展关于这个主题的项目和研究！对于考试，记住聚类属于机器学习的无监督领域，因此不需要有标记的数据。
- en: Also, make sure that you know how the most popular algorithm of this field works;
    that is, K-means. Although clustering algorithms do not provide the meaning of
    each group, they are very powerful for finding patterns in the data, either to
    model a particular problem or just to explore the data.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，确保你知道这个领域最流行的算法是如何工作的；那就是K-means。尽管聚类算法不提供每个组的含义，但它们在寻找数据中的模式方面非常强大，无论是为了建模特定问题还是仅仅为了探索数据。
- en: Coming up next, we'll keep studying unsupervised algorithms and see how AWS
    has built one of the most powerful algorithms out there for anomaly detection,
    known as **Random Cut Forest** (**RCF**).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续研究无监督算法，看看AWS是如何构建出目前最强大的异常检测算法之一的，这个算法被称为**随机切割森林**（**RCF**）。
- en: Anomaly detection
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测
- en: Finding anomalies in data is very common in modeling and data exploratory analysis.
    Sometimes, you might want to find anomalies in the data just to remove them before
    fitting a regression model, while other times, you might want to create a model
    that identifies anomalies as an end goal, for example, in fraud detection systems.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模和数据探索性分析中，发现数据中的异常情况非常常见。有时，你可能只想在拟合回归模型之前找到数据中的异常并去除它们，而有时，你可能想创建一个将识别异常作为最终目标的模型，例如在欺诈检测系统中。
- en: 'Again, we can use many different methods to find anomalies in the data. With
    some creativity, the possibilities are endless. However, there is a particular
    algorithm that works around this problem that you should definitely be aware of
    for your exam: **Random Cut Forest** (**RCF**).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们可以使用许多不同的方法来找到数据中的异常。只要有些创意，可能性是无限的。然而，有一个特定的算法可以解决这个问题，你在考试中应该一定要了解：**随机切割森林**（**RCF**）。
- en: RCF is an unsupervised decision tree-based algorithm that creates multiple decision
    trees (forests) using random subsamples of the training data. Technically, it
    randomizes the data and then creates samples according to the number of trees.
    Finally, these samples are distributed across each tree.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: RCF是一种基于无监督决策树的算法，它使用训练数据的随机子样本创建多个决策树（森林）。技术上，它随机化数据，然后根据树的数量创建样本。最后，这些样本被分配到每个树中。
- en: These sets of trees are used to assign an anomaly score tp the data points.
    This anomaly score is defined as the expected change in the complexity of the
    tree as a result of adding that point to the tree.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树集被用来为数据点分配一个异常分数。这个异常分数定义为将这个点添加到树中后，树的复杂度预期的变化。
- en: The most important hyperparameters of RCF are `num_trees` and `num_samples_per_tree`,
    which are the number of trees in the forest and the number of samples per tree,
    respectively.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: RCF最重要的超参数是`num_trees`和`num_samples_per_tree`，分别代表森林中的树的数量和每棵树中的样本数量。
- en: Dimensionality reduction
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低
- en: Another unsupervised algorithm that's implemented by AWS in their list of built-in
    algorithms is known as **Principal Component Analysis**, or **PCA** for short.
    PCA is a technique that's used to reduce the number of variables/dimensions in
    a given dataset.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个AWS在其内置算法列表中实现的非监督算法被称为**主成分分析**，简称**PCA**。PCA是一种用于减少给定数据集中变量/维度的数量的技术。
- en: The main idea behind PCA is plotting the data points to another set of coordinates,
    known as **principal components** (**PC**), which aims to explain the most variance
    in the data. By definition, the first component will capture more variance than
    the second component, then the second component will capture more variance than
    the third one, and so on.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: PCA背后的主要思想是将数据点绘制到另一组坐标上，称为**主成分**（**PC**），目的是解释数据中的最大方差。根据定义，第一个成分将比第二个成分捕获更多的方差，然后第二个成分将比第三个成分捕获更多的方差，依此类推。
- en: 'You can set up as many principal components as you need as long as it does
    not surpass the number of variables in your dataset. The following graph shows
    how these principal components are drawn:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 只要不超过你的数据集中的变量数量，你就可以设置你需要的任何主成分。以下图表显示了这些主成分是如何绘制的：
- en: '![Figure 7.26 – Finding principal components in PCA'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.26 – 在PCA中找到主成分]'
- en: '](img/B16735_07_026.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_07_026.jpg]'
- en: Figure 7.26 – Finding principal components in PCA
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26 – 在PCA中找到主成分
- en: As we mentioned previously, the first principal component will be drawn in such
    a way that it will capture most of the variance in the data. That's why it passes
    close to the majority of the data points in the preceding graph.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，第一个主成分将以这种方式绘制，以便它能捕捉到数据中的大部分方差。这就是为什么它在先前的图表中接近大多数数据点。
- en: Then, the second principal component will be perpendicular to the first one,
    so that it will be the second component that explains the variance in the data.
    If you want to create more components (consequentially, capturing more variance),
    you just have to follow the same rule of adding perpendicular components. **Eigenvectors**
    and **eigenvalues** are the linear algebra concepts associated with PCA that compute
    the principal components.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，第二个主成分将垂直于第一个，因此它将成为解释数据中方差的第二组件。如果你想创建更多的组件（从而捕获更多的方差），你只需遵循添加垂直组件的相同规则。**特征向量**和**特征值**是与PCA相关的线性代数概念，用于计算主成分。
- en: So, what's the story with dimension reduction here? In case it is not clear
    yet, these principal components can be used to replace your original variables.
    For example, let's say you have 10 variables in your dataset and you want to reduce
    this dataset to three variables that best represent the others. A potential solution
    for that would be applying PCA and extracting the first three principal components!
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这里的降维故事是什么呢？如果还不清楚，这些主成分可以用来替换你的原始变量。例如，假设你的数据集中有10个变量，你想要将这个数据集减少到三个最能代表其他变量的变量。解决这个问题的潜在方法就是应用PCA并提取前三个主成分！
- en: Do these three components explain 100% of your dataset? Probably not, but ideally,
    they will explain most of the variance. Adding more principal components will
    explain more variance, but at the cost of you adding extra dimensions.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个组件能解释你数据集的100%吗？可能不是，但理想情况下，它们将解释大部分的方差。添加更多的主成分将解释更多的方差，但代价是增加了额外的维度。
- en: Using AWS's built-in algorithm for PCA
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用AWS内置的PCA算法
- en: 'In AWS, PCA works in two different modes:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，PCA以两种不同的模式工作：
- en: '**Regular**: For datasets with a moderate number of observations and features'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常规**：对于具有适度观察和特征的集合'
- en: '**Randomized**: For datasets with a large number of observations and features'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机化**：对于具有大量观察和特征的集合'
- en: The difference is that, in randomized mode, it is used an approximation algorithm.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于，在随机模式下，它使用了一个近似算法。
- en: Of course, the main hyperparameter of PCA is the number of components that you
    want to extract, known as `num_components`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，PCA的主要超参数是你想要提取的组件数量，称为`num_components`。
- en: IP Insights
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IP洞察
- en: IP Insights is an unsupervised algorithm that's used for pattern recognition.
    Essentially, it learns the usage pattern of IPv4 addresses.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: IP洞察是一种无监督算法，用于模式识别。本质上，它学习IPv4地址的使用模式。
- en: 'The modus operandi of this algorithm is very intuitive: it is trained on top
    of pairs of events in the format of entity and IPv4 address so that it can understand
    the pattern of each entity that it was trained on.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的操作方式非常直观：它是在实体和IPv4地址格式的成对事件上训练的，这样它就能理解它所训练的每个实体的模式。
- en: Important note
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For instance, we can understand "entity" as user IDs or account numbers.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将“实体”理解为用户ID或账户号码。
- en: Then, to make predictions, it receives a pair of events with the same data structure
    (entity, IPv4 address) and returns an anomaly score for that particular IP address
    regarding the input entity.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了进行预测，它接收一对具有相同数据结构（实体，IPv4地址）的事件，并返回关于该特定IP地址相对于输入实体的异常分数。
- en: Important note
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This anomaly score that's returned by IP Insight infers how anomalous the pattern
    of the event is.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这个由IP Insight返回的异常分数推断出事件的模式有多异常。
- en: We might come across many applications with IP Insights. For example, you can
    create an IP Insights model that was trained on top of your application login
    events (this is your entity). You should be able to expose this model through
    an API endpoint to make predictions in real time.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会遇到许多与IP洞察相关的应用。例如，你可以创建一个基于你的应用程序登录事件（这是你的实体）训练的IP洞察模型。你应该能够通过API端点公开这个模型以进行实时预测。
- en: Then, during the authentication process of your application, you could call
    your endpoint and pass the IP address that is trying to log in. If you got a high
    score (meaning this pattern of logging in looks anomalous), you can request extra
    information before authorizing access (even if the password was right).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在应用程序的认证过程中，你可以调用你的端点并传递尝试登录的IP地址。如果你得到了高分（这意味着这种登录模式看起来异常），在授权访问之前（即使密码是正确的），你可以请求更多信息。
- en: This is just one of the many applications of IP Insights you could think about.
    Next, we will discuss textual analysis.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是你可以考虑的IP Insights的许多应用之一。接下来，我们将讨论文本分析。
- en: Textual analysis
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析
- en: Modern applications use **Natural Language Processing** (**NLP**) for several
    purposes, such as **text translation**, **document classifications**, **web search**,
    **named entity recognition** (**NER**), and many others.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现代应用使用**自然语言处理**（**NLP**）进行多种目的，例如**文本翻译**、**文档分类**、**网络搜索**、**命名实体识别**（**NER**）等。
- en: AWS offers a suite of algorithms for most NLP use cases. In the next few subsections,
    we will have a look at these built-in algorithms for textual analysis.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为大多数NLP用例提供了一套算法。在接下来的几个小节中，我们将查看这些内置的文本分析算法。
- en: Blazing Text algorithm
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blazing Text算法
- en: 'Blazing Text does two different types of tasks: text classification, which
    is a supervised learning approach that extends the **fastText** text classifier,
    and **word2vec**, which is an unsupervised learning algorithm.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Blazing Text执行两种不同类型的任务：文本分类，这是一种扩展**fastText**文本分类器的监督学习方法，以及**word2vec**，这是一种无监督学习算法。
- en: The Blazing Text's implementations of these two algorithms are optimized to
    run on large datasets. For example, you can train a model on top of billions of
    words in a few minutes.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: Blazing Text实现这两种算法已针对在大数据集上运行进行优化。例如，你可以在几分钟内训练一个在数十亿个单词之上的模型。
- en: 'This scalability aspect of Blazing Text is possible due to the following:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: Blazing Text的这种可扩展性是由于以下原因：
- en: Its ability to use multi-core CPUs and a single GPU to accelerate text classification
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够利用多核CPU和单个GPU来加速文本分类
- en: Its ability to use multi-core CPUs or GPUs, with custom CUDA kernels for GPU
    acceleration, when playing with the word2vec algorithm
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在处理word2vec算法时能够使用多核CPU或GPU，并使用定制的CUDA内核进行GPU加速。
- en: The word2vec option supports a **batch_skipgram** mode, which allows Blazing
    Text to do distributed training across multiple CPUs.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec选项支持**批量跳字图**模式，这允许Blazing Text在多个CPU上执行分布式训练。
- en: Important note
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: The distributed training that's performed by Blazing Text uses a mini-batching
    approach to convert **level-1 BLAS** operations into **level-3 BLAS** operations.
    If you see these terms during your exam, you should know that they are related
    to Blazing Text in terms of word2vec.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Blazing Text执行的分布式训练采用小批量方法将**一级BLAS**操作转换为**三级BLAS**操作。如果在考试中遇到这些术语，你应该知道它们与Blazing
    Text在word2vec方面有关。
- en: Still in word2vec mode, Blazing Text supports both the **skip-gram** and **continuous
    bag of words** (**CBOW**) architectures.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在word2vec模式下，Blazing Text支持**跳字图**和**连续词袋**（**CBOW**）架构。
- en: 'Last but not least, note the following configurations of Blazing Text, since
    they are likely to be present in your exam:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，请注意以下Blazing Text的配置，因为它们很可能出现在你的考试中：
- en: In word2vec mode, only the train channel is available.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在word2vec模式下，只有训练通道可用。
- en: Blazing Text expects a single text file with space-separated tokens. Each line
    of the file must contain a single sentence. This means you usually have to pre-process
    your corpus of data before using Blazing Text.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blazing Text期望一个单独的文本文件，其中包含空格分隔的标记。文件的每一行必须包含一个句子。这意味着在使用Blazing Text之前，你通常需要预处理你的数据集。
- en: Sequence-to-sequence algorithm
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列到序列算法
- en: This is a supervised algorithm that transforms an input sequence into an output
    sequence. This sequence can be a text sentence or even an audio recording.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个监督算法，它将输入序列转换为输出序列。这个序列可以是文本句子，甚至是音频记录。
- en: The most common use cases for sequence-to-sequence are machine translation,
    text summarization, and speech-to-text. Anything that you think is a sequence-to-sequence
    problem can be approached by this algorithm.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列最常见的用例是机器翻译、文本摘要和语音到文本。你认为的任何序列到序列问题都可以通过这个算法来处理。
- en: 'Technically, AWS SageMaker''s Seq2Seq uses two types of neural networks to
    create models: a **Recurrent Neural Network** (**RNN**) and a **Convolutional
    Neural Network** (**CNN**) with an attention mechanism.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，AWS SageMaker的Seq2Seq使用两种类型的神经网络来创建模型：一个**循环神经网络**（**RNN**）和一个具有注意力机制的**卷积神经网络**（**CNN**）。
- en: '**Latent Dirichlet Allocation**, or **LDA** for short, is used for topic modeling.
    Topic modeling is a textual analysis technique where you can extract a set of
    topics from a corpus of text data. LDA learns these topics based on the probability
    distribution of the words in the corpus of text.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**，简称**LDA**，用于主题建模。主题建模是一种文本分析技术，可以从文本数据集中提取一组主题。LDA根据文本数据集中单词的概率分布来学习这些主题。'
- en: Since this is an unsupervised algorithm, there is no need to set a target variable.
    Also, the number of topics must be specified up-front and you will have to analyze
    each topic to find their domain meaning.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个无监督算法，因此不需要设置目标变量。此外，必须事先指定主题数量，并且您将不得不分析每个主题以找到它们的领域意义。
- en: Neural Topic Model (NTM) algorithm
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经主题模型（NTM）算法
- en: Just like the LDA algorithm, the **Neural Topic Model** (**NTM**) also aims
    to extract topics from a corpus of data. However, the difference between LDA and
    NTM is their learning logic. While LDA learns from probability distributions of
    the words in the documents, NTM is built on top of neural networks.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 就像LDA算法一样，**神经主题模型**（**NTM**）也旨在从数据集中提取主题。然而，LDA和NTM之间的区别在于它们的学习逻辑。虽然LDA从文档中单词的概率分布中学习，但NTM建立在神经网络之上。
- en: The NTM network architecture has a bottleneck layer, which creates an embedding
    representation of the documents. This bottleneck layer contains all the necessary
    information to predict document composition, and its coefficients can be considered
    topics.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: NTM网络架构有一个瓶颈层，它创建了文档的嵌入表示。这个瓶颈层包含预测文档组成所需的所有必要信息，其系数可以被视为主题。
- en: With that, we have completed this section on textual analysis. In the next section,
    we will learn about image processing algorithms.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经完成了关于文本分析的这一章节。在下一章，我们将学习图像处理算法。
- en: Image processing
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: 'Image processing is a very popular topic in machine learning. The idea is pretty
    self-explanatory: creating models that can analyze images and make inferences
    on top of them. By inference, you can understand this as detecting objects in
    an image, classifying images, and so on.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理在机器学习中是一个非常热门的话题。这个想法相当直观：创建可以分析图像并在其上做出推断的模型。通过推断，您可以将其理解为检测图像中的对象、对图像进行分类等等。
- en: AWS offers a set of built-in algorithms we can use to train image processing
    models. In the next few sections, we will have a look at those algorithms.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一套内置算法，我们可以使用这些算法来训练图像处理模型。在接下来的几节中，我们将查看这些算法。
- en: Image classification algorithm
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分类算法
- en: As the name suggests, the image classification algorithm is used to classify
    images using supervised learning. In other words, it needs a label within each
    image. It supports multi-label classification.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，图像分类算法是使用监督学习对图像进行分类的。换句话说，每个图像都需要一个标签。它支持多标签分类。
- en: 'The way it operates is simple: during training, it receives an image and its
    associated labels. During inference, it receives an image and returns all the
    predicted labels. The image classification algorithm uses a CNN (**ResNet**) for
    training. It can either train the model from scratch or take advantage of transfer
    learning to pre-load the first few layers of the neural network.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 它的操作方式很简单：在训练期间，它接收一个图像及其相关的标签。在推理期间，它接收一个图像并返回所有预测的标签。图像分类算法使用CNN（**ResNet**）进行训练。它可以从头开始训练模型，或者利用迁移学习预先加载神经网络的前几层。
- en: According to AWS's documentation, the `.jpg` and `.png` file formats are supported,
    but the recommended format is **MXNet RecordIO**.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 根据AWS的文档，支持`.jpg`和`.png`文件格式，但推荐格式是**MXNet RecordIO**。
- en: Semantic segmentation algorithm
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分割算法
- en: The semantic segmentation algorithm provides a pixel-level capability for creating
    computer vision applications. It tags each pixel of the image with a class, which
    is an important feature for complex applications such as self-driving and medical
    image diagnostics.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割算法为创建计算机视觉应用提供了像素级的能力。它将图像的每个像素标记为类别，这对于自动驾驶和医学图像诊断等复杂应用来说是一个重要特征。
- en: 'In terms of its implementation, the semantic segmentation algorithm uses the
    **MXNet Gluon framework** and the **Gluon CV toolkit**. You can choose any of
    the following algorithms to train a model:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在其实现方面，语义分割算法使用**MXNet Gluon框架**和**Gluon CV工具包**。你可以选择以下任何算法来训练模型：
- en: '**Fully convolutional network** (**FCN**)'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全卷积网络**（**FCN**）'
- en: '**Pyramid scene parsing** (**PSP**)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金字塔场景解析**（**PSP**）'
- en: DeepLabV3
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLabV3
- en: All these options work as an **encoder-decoder** neural network architecture.
    The output of the network is known as a **segmentation mask**.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些选项都作为**编码器-解码器**神经网络架构工作。网络的输出被称为**分割掩码**。
- en: Object detection algorithm
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测算法
- en: 'Just like the image classification algorithm, the main goal of the object detection
    algorithm is also self-explanatory: it detects and classifies objects in images.
    It uses a supervised approach to train a deep neural network.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 就像图像分类算法一样，目标检测算法的主要目标也是不言而喻的：它在图像中检测和分类对象。它使用监督方法来训练深度神经网络。
- en: 'During the inference process, this algorithm returns the identified objects
    and a score of confidence regarding the prediction. The object detection algorithm
    uses a **Single Shot MultiBox Detector** (**SSD**) and supports two types of network
    architecture: **VGG** and **ResNet**.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，此算法返回识别的对象和关于预测的置信度分数。目标检测算法使用**单次多框检测器**（**SSD**）并支持两种类型的网络架构：**VGG**和**ResNet**。
- en: Summary
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'That was such a journey! Let''s take a moment to highlight what we have just
    learned. We broke this chapter into four main sections: supervised learning, unsupervised
    learning, textual analysis, and image processing. Everything that we have learned
    fits those subfields of machine learning.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是一次难忘的旅程！让我们花点时间来强调我们刚刚学到的东西。我们将本章分为四个主要部分：监督学习、无监督学习、文本分析和图像处理。我们所学的所有内容都适合机器学习的这些子领域。
- en: 'The list of supervised learning algorithms that we have studied includes the
    following:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究过的监督学习算法列表包括以下内容：
- en: Linear learner algorithm
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习器算法
- en: Factorization machines algorithm
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解机算法
- en: XGBoost algorithm
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost算法
- en: K-Nearest Neighbors algorithm
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-最近邻算法
- en: Object2Vec algorithm
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec算法
- en: DeepAR forecasting algorithm
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAR预测算法
- en: Remember that you can use linear learner, factorization machines, XGBoost, and
    KNN for multiple purposes, including to solve regression and classification problems.
    Linear learner is probably the simplest algorithm out of these four; factorization
    machines extend linear learner and are good for sparse datasets, XGBoost uses
    an ensemble method based on decision trees, and KNN is an index-based algorithm.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你可以使用线性学习器、分解机、XGBoost和KNN来解决多种目的，包括回归和分类问题。在这四个算法中，线性学习器可能是最简单的一个；分解机扩展了线性学习器，适用于稀疏数据集，XGBoost使用基于决策树的集成方法，而KNN是一个基于索引的算法。
- en: The other two algorithms, Object2Vec and DeepAR, are used for specific purposes.
    Object2Vec is used to create vector representations of the data, while DeepAR
    is used to create forecast models.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 其他两个算法，Object2Vec和DeepAR，用于特定目的。Object2Vec用于创建数据的向量表示，而DeepAR用于创建预测模型。
- en: 'The list of unsupervised learning algorithms that we have studied includes
    the following:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究过的无监督学习算法列表包括以下内容：
- en: K-means algorithm
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means算法
- en: '**Principal** **Component** **Analysis** (**PCA**)'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: IP Insights
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP洞察
- en: '**Random** **Cut** **Forest** (**RCF**) algorithm'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机切割森林**（**RCF**）算法'
- en: K-means is a very popular algorithm that's used for clustering. PCA is used
    for dimensionality reduction, IP Insights is used for pattern recognition, and
    RCF is used for anomaly detection.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是一个非常流行的算法，用于聚类。PCA用于降维，IP洞察用于模式识别，RCF用于异常检测。
- en: We then looked at regression models and K-means in more detail. We did this
    because, as a data scientist, we think you should at least master these two very
    popular algorithms so that you can go deeper into other algorithms by yourself.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来更详细地研究了回归模型和K-means。我们这样做是因为，作为数据科学家，我们认为你应该至少掌握这两个非常流行的算法，这样你就可以自己深入研究其他算法。
- en: 'Then, we moved on to the second half of this chapter, where we talked about
    textual analysis and the following algorithms:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续本章的后半部分，其中我们讨论了文本分析和以下算法：
- en: Blazing Text algorithm
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blazing Text算法
- en: Sequence-to-sequence algorithm
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列算法
- en: '**Latent** **Dirichlet** **Allocation** (**LDA**) algorithm'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）算法'
- en: '**Neural** **Topic** **Model** (**NTM**) algorithm'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经** **主题** **模型**（**NTM**）算法'
- en: 'Finally, we talked about image processing and looked at the following:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了图像处理，并查看以下内容：
- en: Image classification algorithm
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类算法
- en: Semantic segmentation algorithm
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分割算法
- en: Object detection algorithm
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测算法
- en: Since this is a very important chapter regarding the AWS machine learning specialty
    exam, we encourage you to jump onto the AWS website and search for machine learning
    algorithms. There, you will find the most recent information about the algorithms
    that we have just covered.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是关于AWS机器学习专业考试非常重要的一章，我们鼓励您访问AWS网站并搜索机器学习算法。在那里，您将找到我们刚刚覆盖的算法的最新信息。
- en: That brings us to the end of this quick refresher and the end of this chapter.
    In the next chapter, we will have a look at the existing mechanisms provided by
    AWS that we can use to optimize these algorithms.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了这个快速复习和本章的内容。在下一章中，我们将探讨AWS提供的现有机制，我们可以使用这些机制来优化这些算法。
- en: Questions
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: You are working as a lead data scientist for a retail company. Your team is
    building a regression model and using the linear learner built-in algorithm to
    predict the optimal price of a particular product. The model is clearly overfitting
    to the training data and you suspect that this is due to the excessive number
    of variables being used. Which of the following approaches would best suit a solution
    that addresses your suspicion?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您正在一家零售公司担任首席数据科学家。您的团队正在构建一个回归模型，并使用内置的线性学习算法来预测特定产品的最优价格。该模型明显对训练数据过拟合，您怀疑这是由于使用了过多的变量。以下哪种方法最适合解决您的怀疑？
- en: a) Implementing a cross-validation process to reduce overfitting during the
    training process.
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 在训练过程中实施交叉验证过程以减少过拟合。
- en: b) Applying L1 regularization and changing the `wd` hyperparameter of the linear
    learner algorithm.
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 应用L1正则化和改变线性学习算法的`wd`超参数。
- en: c) Applying L2 regularization and changing the `wd` hyperparameter of the linear
    learner algorithm.
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 应用L2正则化和改变线性学习算法的`wd`超参数。
- en: d) Applying L1 and L2 regularization.
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 应用L1和L2正则化。
- en: Answers
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: C, This question prompts about to the problem of overfitting due an excessive
    number of features being used. L2 regularization, which is available in linear
    learner through the `wd` hyperparameter, will work as a feature selector. Some
    less important features will be penalized by receiving very low weights, which
    will, in practical terms, eliminate the variable.
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C，这个问题涉及到由于使用了过多的特征而导致的过拟合问题。线性学习器中通过`wd`超参数提供的L2正则化将作为一个特征选择器。一些不太重要的特征将因获得非常低的权重而受到惩罚，这在实际操作中相当于消除了变量。
- en: 'RecordIO-protobuf is an optimized data format that''s used to train AWS, built-in
    algorithms, where SageMaker converts each observation in the dataset into a binary
    representation as a set of 4-byte floats. RecordIO-protobuf can operate in two
    modes: pipe and file mode. What is the difference between them?'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RecordIO-protobuf是一种优化的数据格式，用于训练AWS内置算法，其中SageMaker将数据集中的每个观测值转换为二进制表示，作为一组4字节的浮点数。RecordIO-protobuf可以在两种模式下操作：管道模式和文件模式。它们之间有什么区别？
- en: a) Pipe mode accepts encryption at rest, while file mode does not.
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 管道模式接受静态加密，而文件模式则不接受。
- en: b) In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to the training instance's store
    volume.
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 在管道模式下，数据将直接从S3流式传输，这有助于优化存储。在文件模式下，数据从S3复制到训练实例的存储卷。
- en: c) In pipe mode, the data is copied from S3 to the training instance store volume.
    In file mode, the data will be streamed directly from S3, which helps optimize
    storage.
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在管道模式下，数据从S3复制到训练实例存储卷。在文件模式下，数据将直接从S3流式传输，这有助于优化存储。
- en: d) In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to another temporary S3 bucket.
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 在管道模式下，数据将直接从S3流式传输，这有助于优化存储。在文件模式下，数据从S3复制到另一个临时S3存储桶。
- en: Answers
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: B, Remember that RecordIO-protobuf has a pipe mode, which allows us to stream
    data directly from S3.
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B，请记住，RecordIO-protobuf具有管道模式，这允许我们直接从S3流式传输数据。
- en: You are the cloud administrator of your company. You have done great work creating
    and managing user access and you have fine-grained control of daily activities
    in the cloud. However, you want to add an extra layer of security by identifying
    accounts that are attempting to create cloud resources from an unusual IP address.
    What would be the fastest solution to address this use case (choose all the correct
    answers)?
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您是您公司的云管理员。您在创建和管理用户访问以及控制云中的日常活动方面已经做了大量出色的工作。然而，您希望通过识别尝试从异常IP地址创建云资源的账户来添加一个额外的安全层。针对这个用例，什么是最快的解决方案（选择所有正确答案）？
- en: a) Create an IP Insights model to identity anomalous accesses.
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 创建一个IP洞察模型来识别异常访问。
- en: b) Create a clustering model to identify anomalies in the application connections.
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 创建一个聚类模型来识别应用程序连接中的异常。
- en: c) Integrate your IP Insights with existing rules from Amazon Guard Duty.
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 将您的IP洞察与Amazon Guard Duty的现有规则集成。
- en: d) Integrate your anomaly detection model with existing rules from Amazon Guard
    Duty.
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 将您的异常检测模型与Amazon Guard Duty的现有规则集成。
- en: Answers
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: 'A,C, Remember: You can always come up with different approaches to solve problems.
    However, taking advantage of SageMaker''s built-in algorithms is usually the fastest
    way to do things.'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A,C, 记住：您总是可以想出不同的方法来解决问题。然而，利用SageMaker的内置算法通常是做事最快的方式。
- en: You are working as a data scientist for a large company. One of your internal
    clients has requested that you improve a regression model that they have implemented
    in production. You have added a few features to the model and now you want to
    know if the model's performance has improved due to this change. Which of the
    following options best describes the evaluation metrics that you should use to
    evaluate your change?
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您正在为一家大型公司担任数据科学家。您的内部客户之一要求您改进他们在生产中实施的一个回归模型。您已经向模型添加了一些特征，现在您想了解由于这种变化，模型的表现是否有所改善。以下哪个选项最好地描述了您应该使用的评估指标来评估您的更改？
- en: a) Check if the R squared of the new model is better than the R squared of the
    current model in production.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 检查新模型的R平方是否优于生产中当前模型的R平方。
- en: b) Check if the R squared adjusted of the new model is better than the R squared
    of the current model in production.
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 检查新模型的调整后的R平方是否优于生产中当前模型的R平方。
- en: c) Check if the R squared and the RMSE of the new model are better than the
    R squared of the current model in production.
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 检查新模型的R平方和RMSE是否优于生产中当前模型的R平方。
- en: d) Check if the R squared adjusted RMSE of the new model is better than the
    R squared of the current model in production.
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 检查新模型的调整后的R平方RMSE是否优于生产中当前模型的R平方。
- en: Answers
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: D, In this case, you have been exposed to a particular behavior of regression
    model evaluation, where, by adding new features, you will always increase R squared.
    You should use R squared adjusted to understand if the new features are adding
    value to the model or not. Additionally, RMSE will give you a business perspective
    of the model's performance. Although option b is correct, option d best describes
    the optimal decision you should make.
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D, 在这种情况下，您已经接触到了回归模型评估的特定行为，即通过添加新特征，您总是会提高R平方。您应该使用调整后的R平方来了解新特征是否为模型增加了价值。此外，RMSE将为您提供模型性能的商业视角。尽管选项b是正确的，但选项d最好地描述了您应该做出的最佳决策。
- en: Which of the following algorithms is optimized to work with sparse data?
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个算法是针对稀疏数据进行优化的？
- en: a) Factorization machines
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 因子分解机
- en: b) XGBoost
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) XGBoost
- en: c) Linear Learner
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 线性学习器
- en: d) KNN
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) KNN
- en: Answers
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: A, Factorization machines is a general-purpose algorithm that is optimized for
    sparse data.
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A, 因子分解机是一种通用算法，针对稀疏数据进行了优化。
- en: Which of the following algorithms uses an ensemble method based on decision
    trees during the training process?
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个算法在训练过程中使用基于决策树的集成方法？
- en: a) Factorization machines
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 因子分解机
- en: b) XGBoost
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) XGBoost
- en: c) Linear learner
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 线性学习器
- en: d) KNN
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) KNN
- en: Answers
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: B, XGBoost is a very popular algorithm that uses an ensemble of decision trees
    to train the model. XGBoost uses a boosting approach, where decision trees try
    to correct the error of the prior model.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B, XGBoost是一个非常流行的算法，它使用决策树的集成来训练模型。XGBoost使用提升方法，其中决策树试图纠正先前模型的错误。
- en: Which of the following options is considered an index-based algorithm?
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个选项被认为是基于索引的算法？
- en: a) Factorization machines
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 因子分解机
- en: b) XGBoost
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) XGBoost
- en: c) Linear learner
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 线性学习器
- en: d) KNN
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) KNN
- en: Answers
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: D, We say that KNN is an index-based algorithm because it has to compute distances
    between points, assign indexes for these points, and then store the sorted distances
    and their indexes. With that type of data structure, KNN can easily select the
    top K closest points to make the final prediction.
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D，我们说 KNN 是一个基于索引的算法，因为它必须计算点之间的距离，为这些点分配索引，然后存储排序后的距离及其索引。有了这种类型的数据结构，KNN 可以轻松选择最接近的
    K 个点来进行最终预测。
- en: You are a data scientist in a big retail company that wants to predict their
    sales per region on a monthly basis. You have done some exploratory work and discovered
    that the sales pattern per region is different. Your team has decided to approach
    this project as a time series model, and now, you have to select the best approach
    to create a solution. Which of the following options would potentially give you
    a good solution with the least effort?
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是一家大型零售公司的数据科学家，该公司希望预测其按月度区域销售额。你已经进行了一些探索性工作，并发现每个地区的销售模式不同。你的团队已经决定将此项目作为一个时间序列模型来处理，现在你必须选择最佳方法来创建解决方案。以下哪个选项可能会以最少的努力提供一个良好的解决方案？
- en: a) Approach this problem using the ARIMA algorithm. Since each region might
    have different sales behavior, you would have to create one independent model
    per region.
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用 ARIMA 算法来解决这个问题。由于每个地区可能有不同的销售行为，你将不得不为每个地区创建一个独立的模型。
- en: b) Approach this problem with the RNN algorithm. Since neural networks are robust,
    you could create one single model to predict sales in any region.
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用 RNN 算法来解决这个问题。由于神经网络具有鲁棒性，你可以创建一个单一模型来预测任何地区的销售额。
- en: c) Develop a DeepAR model and set the region, associated with each time series,
    as a vector of static categorical features. You can use the **cat** field to set
    up this option.
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 开发一个 DeepAR 模型，并将与每个时间序列相关联的区域设置为一个静态分类特征的向量。你可以使用 **cat** 字段来设置此选项。
- en: d) Develop a DeepAR model and set the region, associated with each time series,
    as a vector of static categorical features. You can use the **dynamic_feat** field
    to set this option.
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 开发一个 DeepAR 模型，并将与每个时间序列相关联的区域设置为一个静态分类特征的向量。你可以使用 **dynamic_feat** 字段来设置此选项。
- en: Answers
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: C, Options a and c are potentially right. However, the problem states that we
    want the solution with the least effort. In this case, setting up a DeepAR model
    and separating the time series by region would be the expected solution (option
    c). We can set up that type of configuration by passing a vector of static categorical
    features into the cat field of the DeepAR class model.
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C，选项 a 和 c 可能是正确的。然而，问题指出我们想要的是最省力的解决方案。在这种情况下，设置一个 DeepAR 模型并按区域分离时间序列将是预期的解决方案（选项
    c）。我们可以通过将静态分类特征的向量传递到 DeepAR 类模型的 cat 字段来设置这种类型的配置。
- en: You are working on a dataset that contains nine numerical variables. You want
    to create a scatter plot to see if those variables could be potentially grouped
    on clusters of high similarity. How could you achieve this goal?
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在处理一个包含九个数值变量的数据集。你想要创建一个散点图来查看这些变量是否可能被潜在地分组在高度相似性的簇中。你该如何实现这个目标？
- en: a) Execute the K-means algorithm.
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 执行 K-means 算法。
- en: b) Compute the two **principal components** (**PCs**) using PCA. Then, plot
    PC1 and PC2 in the scatter plot.
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用 PCA 计算两个**主成分**（**PCs**）。然后，在散点图中绘制 PC1 和 PC2。
- en: c) Execute the KNN algorithm.
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 执行 KNN 算法。
- en: d) Compute the three **principal components** (**PCs**) using PCA. Then, plot
    PC1, PC2, and PC3 in the scatter plot.
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 使用 PCA 计算三个**主成分**（**PCs**）。然后，在散点图中绘制 PC1、PC2 和 PC3。
- en: Answers
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: B, Using K-means or KNN will not solve this question. You have to apply PCA
    to reduce the number of features and then plot the results in a scatter plot.
    Since scatter plots only accept two variables, option b is the right one.
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B，使用 K-means 或 KNN 无法解决这个问题。你必须应用 PCA 来减少特征数量，然后在散点图中绘制结果。由于散点图只接受两个变量，选项 b
    是正确的。
- en: How should you preprocess your data in order to train a Blazing Text model on
    top of 100 text files?
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该如何预处理你的数据，以便在 100 个文本文件上训练一个 Blazing Text 模型？
- en: a) You should create a text file with space-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should concatenate all of them into a single one.
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 你应该创建一个包含空格分隔标记的文本文件。文件的每一行必须包含一个单独的句子。如果你有多个用于训练的文件，你应该将它们全部连接成一个单一的文件。
- en: b) You should create a text file with space-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should apply the same transformation to each one.
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 你应该创建一个以空格分隔标记的文本文件。文件的每一行必须包含一个句子。如果你有多个用于训练的文件，你应该对每个文件应用相同的转换。
- en: c) You should create a text file with comma-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should concatenate all of them into a single one.
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 你应该创建一个以逗号分隔标记的文本文件。文件的每一行必须包含一个句子。如果你有多个用于训练的文件，你应该将它们全部连接成一个单一的文件。
- en: d) You should create a text file with comma-separated tokens. Each line of the
    file must contain a single sentence. If you have multiple files for training,
    you should apply the same transformation to each one.
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 你应该创建一个以逗号分隔标记的文本文件。文件的每一行必须包含一个句子。如果你有多个用于训练的文件，你应该对每个文件应用相同的转换。
- en: Answers
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: Option a is the right one. You should provide just a single file to Blazing
    Text with space-separated tokens, where each line of the file must contain a single
    sentence.
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选项 a 是正确的。你应该向 Blazing Text 提供一个包含空格分隔标记的单个文件，其中文件的每一行必须包含一个句子。
