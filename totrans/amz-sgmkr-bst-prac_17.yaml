- en: Chapter 13:Well-Architected Machine Learning with Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章：使用Amazon SageMaker进行架构良好的机器学习
- en: When running workloads in the cloud, you want to make sure that the workload
    is architected correctly to take advantage of all that the cloud can offer. AWS
    Well-Architected Framework helps you with this, by providing a formal approach
    for learning best practices across five critical pillars applicable to any workload
    deployed to AWS. The pillars are operational excellence, security, reliability,
    performance efficiency, and cost optimization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中运行工作负载时，您希望确保工作负载的架构正确，以便充分利用云所能提供的一切。AWS架构良好框架可以帮助您做到这一点，通过提供一种正式的方法来学习适用于部署到AWS的任何工作负载的五个关键支柱的最佳实践。这些支柱是运营卓越、安全性、可靠性、性能效率和成本优化。
- en: The framework provides guidance on how to improve your architecture and make
    trade-offs between the pillars both during the initial development and continued
    updates of the workload. While you can use Well-Architected Framework to evaluate
    your workload from a general technology perspective, while building **machine
    learning** (**ML**) applications, it would be great to have focused guidance across
    the five pillars specific to ML. AWS Machine Learning Lens provides this focused
    guidance, which you can use to compare and measure your ML workload on AWS against
    best practices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架提供了在初始开发和持续更新工作负载过程中如何改进架构以及如何在支柱之间进行权衡的指导。虽然您可以使用架构良好框架从一般技术角度评估您的负载，但在构建**机器学习**（**ML**）应用程序时，拥有针对ML五个支柱的特定指导将非常有益。AWS机器学习透镜提供了这种专注的指导，您可以使用它来比较和衡量您的AWS上机器学习工作负载与最佳实践的差距。
- en: Important Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'For an in-depth review of the Well-Architected Framework and Machine Learning
    Lens, please review these two white papers from AWS: [https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf](https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf)
    and [https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解架构良好框架和机器学习透镜，请查阅AWS提供的以下两篇白皮书：[https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf](https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf)
    和 [https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf)。
- en: So far in this book, we have discussed how to use different Amazon SageMaker
    capabilities across all phases of ML workloads. In this chapter, we will learn
    how to combine guidance from both the generic Well-Architected Framework and Machine
    Learning Lens and apply it to the end-to-end ML workloads built on SageMaker.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书中我们已经讨论了如何使用不同的Amazon SageMaker功能跨越机器学习工作负载的所有阶段。在本章中，我们将学习如何结合通用架构良好框架和机器学习透镜的指导，并将其应用于基于SageMaker构建的端到端机器学习工作负载。
- en: Please note that this chapter does not introduce any new SageMaker features,
    but rather dives into how you can apply the capabilities you already know to build
    a well-architected ML workload. You will learn how SageMaker's specific capabilities
    are combined with other AWS services across the five pillars, with some of the
    capabilities playing a key role in multiple pillars.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章不介绍任何新的SageMaker功能，而是深入探讨如何将您已知的技能应用于构建一个架构良好的机器学习工作负载。您将了解SageMaker的具体功能如何与其他AWS服务结合，在五个支柱中发挥作用，其中一些功能在多个支柱中扮演关键角色。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Best practices for operationalizing ML workloads
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运营机器学习工作负载的最佳实践
- en: Best practices for securing ML workloads
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护机器学习工作负载的最佳实践
- en: Best practices for building reliable ML workloads
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建可靠机器学习工作负载的最佳实践
- en: Best practices for building performant ML workloads
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建高性能机器学习工作负载的最佳实践
- en: Best practices for building cost-optimized ML workloads
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建成本优化的机器学习工作负载的最佳实践
- en: Best practices for operationalizing ML workloads
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运营机器学习工作负载的最佳实践
- en: Many organizations start their ML journey with a few experiments of building
    models to solve one or more business problems. Cloud platforms, in general, and
    ML platforms such as SageMaker make this experimentation easy by providing seamless
    access to elastic compute infrastructure and built-in support for various ML frameworks
    and algorithms. Once these experiments have proven successful, the next natural
    step is to move the models into production. Typically, at this time, organizations
    want to move out of the research-and-development phase and into operationalizing
    ML.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织从构建模型来解决一个或多个业务问题的几个实验开始他们的机器学习之旅。通常，云平台，如SageMaker，通过提供无缝访问弹性计算基础设施和内置对各种机器学习框架和算法的支持，使这种实验变得容易。一旦这些实验证明成功，下一步自然的步骤就是将模型投入生产。通常，在这个时候，组织希望从研发阶段转向机器学习的运营。
- en: The idea of MLOps is gaining popularity these days. MLOps, at a very high level,
    involves bringing together people, processes, and technology to integrate ML workloads
    into release management, CI/CD, and operations. Without diving into all the details
    of MLOps, in this section, we will discuss best practices for operationalizing
    ML workloads using technology. We will also discuss which SageMaker features play
    a role in various aspects of operationalizing ML workloads.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps的概念最近越来越受欢迎。在非常高的层面上，MLOps涉及将人员、流程和技术结合起来，将机器学习工作负载集成到发布管理、CI/CD和运营中。在不深入MLOps的所有细节的情况下，在本节中，我们将讨论使用技术来运营机器学习工作负载的最佳实践。我们还将讨论哪些SageMaker功能在运营机器学习工作负载的各个方面发挥作用。
- en: Let's now look at best practices for operationalizing ML workloads on AWS in
    the following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在以下部分中查看在AWS上运营机器学习工作负载的最佳实践。
- en: Ensuring reproducibility
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保可重复性
- en: To successfully operationalize the end-to-end ML system, you must first ensure
    its reproducibility through versioned data, code, and artifacts. Best practice
    is to version all inputs used to create models, including training data, data
    preparation code, algorithm implementation code, parameters, and hyperparameters,
    in addition to all trained model artifacts. A versioning strategy is also about
    helping in the model-update phase and allowing for easy rollback to a specific
    known working version if a model update fails or if the updated model does not
    meet your requirements.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功地将端到端机器学习系统投入运营，您必须首先通过版本化的数据、代码和工件来确保其可重复性。最佳实践是版本化创建模型所使用的所有输入，包括训练数据、数据准备代码、算法实现代码、参数和超参数，以及所有训练模型工件。版本化策略还涉及在模型更新阶段提供帮助，并在模型更新失败或更新的模型不符合您的要求时，允许轻松回滚到特定的已知有效版本。
- en: Tracking ML artifacts
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪机器学习工件
- en: Iterative development of ML models using different algorithms and hyperparameters
    for each algorithm results in many training experiments and multiple model versions.
    Keeping track of these experiments and resulting models along with each model's
    lineage is important to meet auditing and compliance requirements. Model lineage
    also helps with root-cause analysis in case of degrading model performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的算法和超参数对每个算法进行迭代开发，会导致许多训练实验和多个模型版本的产生。跟踪这些实验和结果模型，以及每个模型的血统，对于满足审计和合规要求非常重要。模型血统还有助于在模型性能下降的情况下进行根本原因分析。
- en: While you can certainly build a custom tracking solution, best practice is to
    use a managed service such as SageMaker Experiments. Experiments allows you to
    track, organize, visualize, and compare ML models across all phases of the ML
    lifecycle including feature engineering, model training, model tuning, and model
    deployment. With SageMaker Experiments, you can easily choose to deploy or update
    the model with a specific version. Experiments also provides you with the model
    lineage capability. For a detailed discussion of SageMaker Experiments' capabilities,
    please refer to the *Amazon SageMaker Experiments* section of [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)*,
    Training and Tuning at Scale*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您当然可以构建定制的跟踪解决方案，但最佳实践是使用如SageMaker Experiments之类的托管服务。Experiments允许您跟踪、组织、可视化和比较机器学习模型在整个机器学习生命周期中的所有阶段，包括特征工程、模型训练、模型调优和模型部署。使用SageMaker
    Experiments，您可以轻松选择部署或更新特定版本的模型。Experiments还为您提供了模型血统功能。有关SageMaker Experiments功能的详细讨论，请参阅[*第6章*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)的*“Amazon
    SageMaker Experiments”部分*，*“大规模训练和调优”*。
- en: Additionally, you can also use the Amazon SageMaker ML Lineage Tracking capability,
    which keeps track of information about the individual steps of an ML workflow
    from data preparation to model deployment. With the information tracked, you can
    reproduce the workflow steps, track model and dataset lineage, and establish model
    governance and audit standards.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Automating deployment pipelines
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated pipelines minimize human intervention in moving a trained ML model
    from lower-level environments such as development and staging into a production
    environment. The aim is to have a codified deployment pipeline created with Infrastructure-as-Code
    and Configuration-as-Code, with manual and automated quality gates incorporated
    into the pipeline. Manual quality gates can ensure that models are promoted to
    the production environment only after ensuring that there are no operational concerns
    such as security exposure. Automated quality gates, on the other hand, can be
    used to evaluate model metrics such as precision, recall, or accuracy. Pipelines
    result in consistent deployment as well as providing the ability to reliably recreate
    ML-related resources across multiple environments with minimal human intervention.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon SageMaker Pipelines, you can build automated model workflows. You
    can build every step of the ML lifecycle as a pipeline step to develop and deploy
    models and monitor the pipelines. You can further manage dependencies between
    each step, build the correct sequence, and execute the steps automatically. A
    service that brings in CI/CD practices to ML workloads is SageMaker Projects.
    This service helps you move models from concept to production. Additionally, you
    can easily meet governance and audit standards using a combination of SageMaker
    Projects and SageMaker Pipelines, by automatically tracking code, datasets, and
    model versions through each step of the ML lifecycle. This enables you to go back
    and replay model-generation steps, troubleshoot problems, and reliably track the
    lineage of models at scale. For a detailed discussion of automated workflows and
    MLOps, please refer to [*Chapter 12*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*,
    Machine Learning Automated Workflows*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring production models
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continued monitoring of deployed models is a critical step in operationalizing
    ML workloads, since a model's performance and effectiveness may degrade over time.
    Ensuring that the model continues to meet your business needs starts with the
    identification of the metrics that measure both model-related metrics and business
    metrics. Ensure that all metrics critical to model evaluation against your business
    KPIs are defined early on and collected during monitoring.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Once the metrics are identified, to ensure the continued high quality of the
    deployed model, use the Amazon SageMaker Model Monitor capabilities and its integration
    with CloudWatch to proactively detect issues, raise alerts, and automate remediation
    actions. In addition to detecting model-quality degradation, you can monitor data
    drift, bias drift, and feature attribution drift to meet your reliability, regulatory,
    and model explainability requirements.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: CloudWatch alerts that have been triggered because of model monitoring can be
    used to automate activities such as invalidating the current model, reverting
    to an older model version, or retraining a new model based on new ground truth
    data. Updates to production models should consider trade-offs between the risk
    of introducing changes, the cost of retraining, and the potential value of having
    a newer model in production. For a detailed discussion of model monitoring, please
    refer to [*Chapter 11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring
    Production Models with Amazon SageMaker Model Monitor and Clarify*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: While this section has focused on SageMaker-native approaches for operationalizing
    ML workloads, please note that similar automated pipelines can be built using
    a combination of SageMaker APIs and other AWS services such as CodePipeline, Step
    Functions, Lambda, and SageMaker Data Science SDK. Multiple MLOps architectures
    are documented along with sample code at [https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml](https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various AWS services and features applicable
    to operationalizing ML workloads:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – AWS Services used for operationalizing ML workloads'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '](img/012.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – AWS Services used for operationalizing ML workloads
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to enable secure ML workloads.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for securing ML workloads
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When securing an ML workload, you should take into consideration infrastructure
    and network security, authentication and authorization, encrypting data and model
    artifacts, logging and auditing, and meeting regulatory requirements. In this
    section, we will discuss best practices for security ML workloads using a combination
    of SageMaker and related AWS services.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for securing ML workloads on AWS in the following
    sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Isolating the ML environment
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build secure ML workloads, you need an isolated compute and network environment.
    To achieve this for ML on SageMaker, deploy all resources such as notebooks, studio
    domain, training jobs, processing jobs, and endpoints within a **Virtual** **Private**
    **Cloud** (**VPC**). A VPC provides an isolated environment where all traffic
    between various SageMaker components flows within the network. You can add another
    layer of isolation by using security groups that include rules for both inbound
    and outbound traffic allowed by subnets within the VPC, thereby isolating your
    ML resources further.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建安全的机器学习工作负载，您需要一个隔离的计算和网络环境。为了在SageMaker上实现这一点，请在**虚拟** **私有** **云**（**VPC**）中部署所有资源，例如笔记本、工作室域、训练作业、处理作业和端点。VPC提供了一个隔离的环境，其中所有SageMaker组件之间的流量都在网络内部流动。您可以通过使用包含VPC内子网允许的入站和出站流量的规则的网络安全组，添加另一层隔离，从而进一步隔离您的机器学习资源。
- en: Even if you use SageMaker without a VPC, all resources run in an environment
    managed by AWS on single-tenancy EC2 instances, which ensures that your ML environments
    are isolated from other customers. However, deploying ML resources, such as training
    containers, in a VPC allows you to monitor all network traffic in and out of these
    resources using VPC Flow Logs. Additionally, you can use VPC endpoints and AWS
    PrivateLink to enable communication between SageMaker and other AWS services such
    as S3 or CloudWatch. This keeps all traffic flowing between the various services
    within the AWS network without exposing the traffic to the public internet.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您在没有VPC的情况下使用SageMaker，所有资源也在由AWS管理的单租户EC2实例的环境中运行，这确保了您的机器学习环境与其他客户隔离。然而，在VPC中部署机器学习资源，例如训练容器，允许您使用VPC
    Flow Logs监控这些资源的进出网络流量。此外，您可以使用VPC端点和AWS PrivateLink来启用SageMaker与其他AWS服务（如S3或CloudWatch）之间的通信。这保持了AWS网络内各种服务之间所有流量的流动，同时不会将流量暴露给公共互联网。
- en: Disabling internet and root access
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用互联网和root访问
- en: By default, SageMaker notebook instances are internet-enabled to allow you to
    download external libraries and customize your working environment. Additionally,
    root access is enabled on these notebooks, giving you the flexibility to leverage
    external libraries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SageMaker笔记本实例已启用互联网访问，以便您下载外部库并自定义工作环境。此外，这些笔记本启用了root访问，这使您能够利用外部库。
- en: Only use these default settings in a lower-level sandbox and development environments
    to figure out the optimal working notebook environment. In all other non-production
    and production environments, launch SageMaker resources in your own VPC and turn
    off root access to prevent downloading and installing unauthorized software. Import
    all necessary libraries into a private repository such as AWS CodeArtifact before
    you isolate your environment. This allows you to seamlessly download specific
    versions of libraries without having to reach out to the internet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在低级沙盒和开发环境中，仅使用这些默认设置来找出最佳工作笔记本环境。在其他所有非生产环境和生产环境中，请在自己的VPC中启动SageMaker资源，并关闭root访问以防止下载和安装未经授权的软件。在隔离您的环境之前，将所有必要的库导入到私有仓库，例如AWS
    CodeArtifact。这允许您无缝下载特定版本的库，而无需访问互联网。
- en: Additionally, use codified lifecycle configurations to automate setting up the
    notebook environment. Similarly, training and deployed inference containers managed
    by SageMaker are internet-enabled by default. When launching training and inference
    resources, use `VPCConfig` and `EnableNetworkIsolation` flags to protect these
    resources from external network traffic. In this case, all downloads and uploads
    of data and model artifacts are routed through your VPC. At the same time, the
    training and inference containers remain isolated from the network and do not
    have access to any resource within your VPC or on the internet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用编码化的生命周期配置来自动设置笔记本环境。同样，由SageMaker管理的训练和部署推理容器默认启用互联网访问。在启动训练和推理资源时，使用`VPCConfig`和`EnableNetworkIsolation`标志来保护这些资源免受外部网络流量的影响。在这种情况下，所有数据和模型工件的上传和下载都通过您的VPC路由。同时，训练和推理容器保持与网络的隔离，并且无法访问VPC内或互联网上的任何资源。
- en: Enforcing authentication and authorization
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制执行身份验证和授权
- en: Implement a strong mechanism to determine who can access the ML resources (authentication)
    and what resources authenticated users can access (authorization). SageMaker is
    natively integrated with AWS IAM, a service used to manage access to all AWS services
    and resources. IAM allows you to define fine-grained access controls using IAM
    users, groups, roles, and policies. You can implement least-privilege access using
    a combination of identity-based policies to specify what an IAM user, role, or
    group can do and resource-based policies to specify who has access to the resource
    and what actions they can perform on it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: When designing these IAM policies, it is tempting to start with wide-open IAM
    policies with good intentions of tightening them as you go. However, best practice
    is to start with tight policies that grant minimal required access and add additional
    permissions when required. Periodically review and refine policies to ensure that
    no unnecessary permissions are granted. The IAM service provides the Access Advisor
    capability, which shows you when various AWS services are last accessed by different
    entities such as IAM groups, users, roles, and policies. Use this information
    to refine the policies. All the service API calls are also logged by CloudTrail,
    and you can use the CloudTrail history to determine which permissions can be removed
    based on the usage patterns.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Securing data and model artifacts
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IAM policies can also be used for access-control of data and models in S3\.
    Additionally, you can use a security service called Amazon Macie to protect and
    classify data in S3\. Macie internally uses ML to automatically discover, classify,
    and protect sensitive data. It automatically recognizes sensitive data such as
    **personally** **identifiable** **information** (**PII**) or **intellectual**
    **property** (**IP**), providing visibility into data access and movement patterns.
    Macie continuously monitors for anomalies in data-access patterns and proactively
    generates alerts on unauthorized access and data leaks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The next important aspects to secure are data and model artifacts of an ML system,
    both at rest and in transit. To secure data in transit within a VPC, use **Transport**
    **Layer** **Security** (**TLS**). To secure data at rest, best practice is to
    use encryption to block malicious actors from reading your data and model artifacts.
    You can use either client-side or server-side encryption. SageMaker comes with
    built-in encryption capabilities to protect training data and model artifacts
    both at rest and in transit. For example, when launching a training job, you can
    specify the encryption key to be used. You have the flexibility of using SageMaker-managed
    keys, AWS-managed keys, or your own customer-managed keys.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Logging, monitoring, and auditing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker is natively integrated with CloudWatch and CloudTrail. You can capture
    logs from SageMaker training, processing, and inference in CloudWatch, which can
    further be used for troubleshooting. All SageMaker (and other AWS services) API
    calls are logged by CloudTrail, allowing you to track down which IAM user, AWS
    account, or source IP address made the API call along with when the call occurred.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 与 CloudWatch 和 CloudTrail 原生集成。你可以在 CloudWatch 中捕获 SageMaker 训练、处理和推理的日志，这可以进一步用于故障排除。所有
    SageMaker（以及其他 AWS 服务）的 API 调用都由 CloudTrail 记录，允许你追踪哪个 IAM 用户、AWS 账户或源 IP 地址执行了
    API 调用，以及调用发生的时间。
- en: Meeting regulatory requirements
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 满足监管要求
- en: For many organizations, ML solutions need to comply with regulatory standards
    and pass compliance certifications that vary significantly across countries and
    industries. Amazon SageMaker complies with a wide range of compliance programs,
    including PCI, HIPAA, SOC 1/2/3, FedRAMP, and ISO 9001/27001/27017/27018.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多组织来说，机器学习解决方案需要符合监管标准，并通过各国和行业差异很大的合规认证。Amazon SageMaker 符合广泛的合规计划，包括 PCI、HIPAA、SOC
    1/2/3、FedRAMP 和 ISO 9001/27001/27017/27018。
- en: 'The following table summarizes the various AWS services applicable to securing
    ML workloads:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了适用于保护机器学习工作负载的各种 AWS 服务：
- en: '![Figure 13.2 – AWS services used for securing ML workloads'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.2 – 用于保护机器学习工作负载的 AWS 服务'
- en: '](img/021.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/021.jpg)'
- en: Figure 13.2 – AWS services used for securing ML workloads
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 用于保护机器学习工作负载的 AWS 服务
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build reliable ML workloads.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解 SageMaker 如何与其他 AWS 服务集成，以构建可靠的机器学习工作负载。
- en: Best practices for reliable ML workloads
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可靠机器学习工作负载的最佳实践
- en: 'For a reliable system, there are two considerations at the core:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个可靠系统，有两个核心考虑因素：
- en: First, the ability to recover from planned and unplanned disruptions
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，能够从计划内和计划外的中断中恢复
- en: Second, the ability to meet unpredictable increases in traffic demands
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，能够满足不可预测的流量需求增加
- en: Ideally, the system should achieve both without affecting downstream applications
    and end consumers. In this section, we will discuss best practices for building
    reliable ML workloads using a combination of SageMaker and related AWS services.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，系统应在不影响下游应用程序和最终用户的情况下实现这两点。在本节中，我们将讨论使用 SageMaker 和相关 AWS 服务构建可靠机器学习工作负载的最佳实践。
- en: Let's now look at some best practices for securing ML workloads on AWS in the
    following sections.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看以下章节中关于在 AWS 上保护机器学习工作负载的一些最佳实践。
- en: Recovering from failure
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从故障中恢复
- en: For an ML workload, the ability to recover gracefully should be part of all
    the steps that make up the iterative ML process. A failure can occur with data
    storage, data processing, model training, or model hosting, which may result from
    a variety of events ranging from system failure to human error.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习工作负载，优雅地恢复的能力应该是构成迭代机器学习过程的各个步骤的一部分。故障可能发生在数据存储、数据处理、模型训练或模型托管中，这可能是从系统故障到人为错误的各种事件的结果。
- en: For ML on SageMaker, all data (and model artifacts) is typically saved in S3\.
    This ensures decoupling between ML data and the computation processing. To prevent
    an inadvertent loss of data, best practice is to use a combination of IAM and
    S3 policies to ensure least privilege-based access to data. Additionally, use
    S3 versioning and object tagging to enable versioning and traceability of data
    (and model artifacts) for easy recovery or recreation in the event of failure.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SageMaker 上的机器学习，所有数据（以及模型工件）通常都保存在 S3 中。这确保了机器学习数据与计算处理的解耦。为了防止数据意外丢失，最佳实践是使用
    IAM 和 S3 策略的组合来确保基于最小权限的数据访问。此外，使用 S3 版本控制和对象标记来启用数据（以及模型工件）的版本控制和可追溯性，以便在发生故障时易于恢复或重建。
- en: Next, consider the reliability of ML training, which is often a long, time-consuming
    process. It is not uncommon to see training jobs that run over multiple hours
    and even multiple days. If these long-running training jobs are disrupted due
    to a power outage, OS fault, or other unexpected error, having the ability to
    reliably resume from where the job stopped is critical. ML checkpointing should
    be used in this situation. On SageMaker, a few built-in algorithms and all supported
    deep learning frameworks provide the capability of turning on checkpointing when
    a training job is launched. When you enable checkpointing, SageMaker automatically
    saves snapshots of the model state during training. This enables you to reliably
    restart a training job from the last saved checkpoint.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑机器学习训练的可靠性，这通常是一个漫长且耗时的过程。看到运行数小时甚至数天的训练作业并不罕见。如果这些长时间运行的训练作业因断电、操作系统故障或其他意外错误而中断，能够可靠地从作业停止的地方继续进行至关重要。在这种情况下应使用机器学习检查点。在
    SageMaker 上，一些内置算法和所有支持的深度学习框架在启动训练作业时提供启用检查点的功能。当您启用检查点时，SageMaker 会自动在训练过程中保存模型状态的快照。这使得您可以从最后一个保存的检查点可靠地重新启动训练作业。
- en: Tracking model origin
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪模型来源
- en: Let's say your training goes off without a hitch and you have a trained model
    artifact saved in an S3 bucket. What happens if you lose this model artifact due
    to human error, such as someone in your team deleting it by mistake? In a reliable
    ML system, you need to be able to recreate this model using the same data, version
    of the code, and parameters as the original model. Hence, it is important to keep
    track of all these aspects during training. Using SageMaker Experiments, you can
    keep track of all the steps and artifacts that went into creating a model so you
    can easily recreate the model as necessary. Another benefit of tracking with SageMaker
    Experiments is the ability to troubleshoot issues in production for reliable operation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的训练过程顺利进行，并且您已将训练好的模型工件保存在一个 S3 桶中。如果由于人为错误（例如，您的团队中有人不小心删除了它）而丢失了这个模型工件，会发生什么？在一个可靠的机器学习系统中，您需要能够使用与原始模型相同的数据、代码版本和参数来重新创建此模型。因此，在训练过程中跟踪所有这些方面非常重要。使用
    SageMaker 实验，您可以跟踪创建模型的所有步骤和工件，以便您可以根据需要轻松地重新创建模型。使用 SageMaker 实验跟踪的另一个好处是能够对生产中的问题进行故障排除，以确保可靠运行。
- en: In addition to relying on Experiments to be able to recreate a specific version
    of a model artifact, use a combination of IAM and S3 policies to ensure least
    privilege-based access to minimize the risk of accidental model-artifact deletion.
    Implement measures such as requiring MFA for model artifact deletion and storing
    a secondary copy of the artifact as required by your organization's disaster recovery
    strategy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了依赖实验来重新创建模型工件的具体版本外，还应结合使用 IAM 和 S3 策略，以确保基于最小权限的访问，以最大限度地降低意外删除模型工件的风险。实施诸如要求对模型工件删除进行多因素认证以及根据您组织的灾难恢复策略存储工件副本等措施。
- en: Automating deployment pipelines
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化部署管道
- en: To ensure that all steps leading up to model deployment are executed consistently,
    use a CI/CD pipeline with access controls to enforce least privilege-based access.
    Deployment automation combined with manual and automated quality gates ensures
    that all changes can be effectively validated with dependent systems prior to
    deployment. Amazon SageMaker Pipelines has the capability to bring CI/CD practices
    to ML workloads for improved reliability. Codifying the CI/CD pipelines using
    SageMaker Pipelines provides you with an additional capability of dealing with
    the model endpoint itself being deleted inadvertently. Using the Infrastructure-as-Code
    approach, the endpoint can be recreated. This requires a well-defined versioning
    strategy in place for your data, code, algorithms, hyperparameters, model artifacts,
    container images, and more. Version everything and document your versioning strategy.
    For a detailed discussion of SageMaker Pipelines capabilities, please refer to
    the *Amazon SageMaker Pipelines* section of [*Chapter 12*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*,
    Machine Learning Automated Workflows*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, follow the *train once and deploy everywhere* strategy. Because
    of the decoupled nature of the training process and results, you can share the
    trained model artifact across multiple environments. This prevents retraining
    in multiple environments and introducing unexpected changes to the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Handling unexpected traffic patterns
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is deployed, you must ensure the reliability of the deployed
    model in serving the inference requests. The model should be able to handle spikes
    in inference traffic and continue to operate at the quality necessary to meet
    the business requirements.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: To handle traffic spikes, deploy the model with the Autoscaling-enabled SageMaker
    real-time endpoint. With Autoscaling enabled, SageMaker automatically increases
    (and decreases) the computation capacity behind the hosted model in response to
    the dynamic shifts in the inference traffic. Autoscaling provided by SageMaker
    is horizontal scaling, meaning it adds new instances or removes existing instances
    to handle the inference traffic variations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Continuous monitoring of deployed model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To ensure the continued high quality of the deployed model, use the Amazon SageMaker
    Model Monitor capabilities and its integration with CloudWatch to proactively
    detect issues, raise alerts, and automate remediation actions when a production
    model is not performing as expected. In addition to model quality, you can monitor
    data drift, bias drift, and feature-attribution drift to meet your reliability,
    regulatory, and model explainability requirements. Ensure that all metrics critical
    to model evaluation against your business KPIs are defined and monitored. For
    a detailed discussion of model monitoring, please refer to [*Chapter 11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*,
    Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Updating model with new versions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, you must consider how to update a production model reliably. SageMaker
    endpoint production variants can be used to implement multiple deployment strategies
    such as A/B, Blue/Green, Canary, and Shadow deployments. The advanced deployment
    strategies along with detailed implementation are discussed in [*Chapter 9*](B17249_09_Final_JM_ePub.xhtml#_idTextAnchor163)*,
    Updating Production Models Using Amazon SageMaker Endpoint Production Variants*.
    Depending on the model consumer's tolerance for risk and downtime, choose an appropriate
    deployment strategy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various AWS services applicable to building
    reliable ML workloads:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – AWS service capabilities used for reliable ML workloads'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '](img/03.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – AWS service capabilities used for reliable ML workloads
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build reliable, performance-efficient workloads.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for building performant ML workloads
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the compute- and time-intensive nature of ML workloads, it is important
    to choose the most performant resources appropriate for each individual phase
    of the workload. Computation, memory, and network bandwidth requirements are unique
    to each phase of the ML process. Besides the performance of the infrastructure,
    the performance of the model as measured by metrics such as accuracy is also important.
    In this section, we will discuss best practices to apply in selecting the most
    performant resources for building ML workloads on SageMaker.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for building performant ML workloads on AWS
    in the following sections.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Rightsizing ML resources
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker supports a variety of ML instance types with a varying combination
    of CPU, GPU, FPGA, memory, storage, and networking capacity. Each instance type,
    in turn, supports multiple instance sizes. So, you have a range of choices to
    choose from to suit your specific workload. The best practice is to choose different
    compute resource configurations for data processing, building, training, and hosting
    your ML model. This is made possible by the decoupled nature of SageMaker, which
    allows you to choose different instance types and sizes for different APIs. For
    example, you can choose `ml.c5.medium` for a notebook instance as your working
    environment, use a cluster of four `ml.p3.large` GPU instances for training, and
    finally host the trained model on two `ml.m5.4xlarge` instances with Elastic Inference
    attached. Additionally, in the SageMaker Studio environment, you can change the
    notebook instance type seamlessly without any interruption to your work.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: While you have the flexibility of choosing different compute options for different
    ML phases, how do you choose the specific instance types and sizes to use? This
    comes down to understanding your workload and experimentation. For example, if
    you know that the training framework and algorithm of your choice will need GPU
    support, choose a GPU cluster to train on. While it may be tempting to use GPUs
    for all training, traditional algorithms may not work well on GPUs due to the
    communication overheads involved. Some built-in algorithms, such as XGBoost, implement
    an open source algorithm that has been optimized for CPU computations. SageMaker
    also provides optimized versions of frameworks, such as TensorFlow and PyTorch,
    which include optimizations for high-performance training across Amazon EC2 instance
    families.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring resource utilization
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you make your initial choice of instances and kick off training, SageMaker
    training jobs emit CloudWatch metrics for resource utilization that you can use
    to improve your training runs the next time. Additionally, when you enable Debugger
    with your training jobs, SageMaker Debugger provides visibility into training
    jobs and the infrastructure a training job is executing on. Debugger also monitors
    and reports on the system resources such as CPU, GPU, and memory, providing you
    with insights into resource underutilization and bottlenecks. If you use TensorFlow
    or PyTorch for your deep learning training jobs, Debugger provides you with a
    view into framework metrics that can be used to speed up your training jobs. For
    a detailed discussion of Debugger's capabilities, please refer to [*Chapter 7*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*,*
    *Profile Training Jobs with Amazon SageMaker Debugger.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Rightsizing hosting infrastructure
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is trained and ready to be deployed to choose instances for real-time
    endpoints, consider what your target performance is. Target performance is a combination
    of how many requests to serve in each period and the desired latency for each
    request, for example, 10,000 requests per minute with a maximum of a 1 millisecond
    response time. Once you have the target performance in mind, perform load testing
    in a non-production environment to figure out the instance type, instance size,
    and number of instances to host the model on. Recommended best practice is to
    deploy the endpoint with at least two instances across two availability zones
    for high availability.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Once you decide on the instance type to use, start with the minimum number of
    instances necessary to meet your steady-state traffic and take advantage of the
    Autoscaling capability of SageMaker hosting. Using Autoscaling, SageMaker can
    automatically scale the inference capacity depending on the utilization and request
    traffic thresholds you configure. Capacity adjustments to meet your performance
    requirements are done by updating the endpoint configuration without any downtime.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can scale up the hosting infrastructure for deep learning
    models using Amazon `Inf1` instances, which are best suited to applications such
    as search, recommendation engines, and computer vision, at a low cost.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: While real-time endpoints provide access to models deployed on SageMaker, some
    workloads may warrant inference at the edge due to latency requirements, for example,
    models used to determine defective product parts in a manufacturing plant. In
    such cases, the model needs to be deployed on cameras within the manufacturing
    plant. For such use cases, use SageMaker Neo and SageMaker Edge Manager to optimize,
    deploy, and manage models at the edge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: While real-time endpoints and models deployed at the edge provide synchronous
    predictions, batch transform is used for asynchronous inferences with more tolerance
    for longer response times. Use experimentation to determine the right instance
    type, size, and number of instances to be used for batch transform with job completion
    time in mind.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Continuous monitoring of deployed model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is actively serving inference traffic, use SageMaker Model Monitor
    to continuously monitor ML models for data drift, model-quality performance, feature-importance
    drift, and bias drift. Behind the scenes, Model Monitor uses distributed processing
    jobs. As with batch processing, use experimentation and load testing to determine
    the processing job resources necessary to complete each scheduled monitoring job
    execution. For a detailed discussion of Model Monitor, please refer to [*Chapter
    11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring Production Models
    with Amazon SageMaker Model Monitor and Clarify*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various SageMaker features and how they
    are applicable for building performant ML workloads:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – AWS service capabilities for building performant ML workloads.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '](img/04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – AWS service capabilities for building performant ML workloads.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build cost-optimized workloads.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for cost-optimized ML workloads
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many organizations, the lost opportunity cost of not embracing disruptive
    technologies such as ML outweighs the ML costs. By implementing a few best practices,
    these organizations can get the best possible returns on their ML investment.
    In this section, we will discuss best practices to apply for cost-optimized ML
    workloads on SageMaker.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for building cost-optimized ML workloads on
    AWS in the following sections.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing data labeling costs
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Labeling of data used for ML training, typically done at the very beginning
    of the ML process, can be tedious, error-prone, and time-consuming. Labeling at
    scale consumes many working hours, making this an expensive task, too. To optimize
    cost for data labeling, use SageMaker Ground Truth. Ground Truth provides capabilities
    for data labeling at scale using a combination of human workforce and active learning.
    When active learning is enabled, a labeling task is routed to humans only if a
    model cannot confidently finish the task. The human-labeled data is then used
    to train the model to improve accuracy. Therefore, as the labeling job progresses,
    less and less data needs to be labeled by humans. This results in faster completion
    of the job at reduced costs. For a detailed discussion of Ground Truth capabilities,
    please refer to [*Chapter 3*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*,
    Data Labeling with Amazon SageMaker Ground Truth*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Reducing experimentation costs with models from AWS Marketplace
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML is inherently iterative and experimental. Having to run multiple algorithms
    with different sets of hyperparameters each time leads to several training jobs
    before you can determine a model that meets your needs. All this training adds
    up in terms of time and costs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: A big part of experimentation is the research and reuse of readily available
    pre-trained models that may suit your needs. AWS Marketplace for ML gives you
    a catalog of datasets and models made available by vendors vetted by AWS. You
    can subscribe to models that meet your needs and potentially save the time and
    costs involved in developing your own models. If you do, however, end up developing
    your own models, you can use the marketplace to monetize your models by making
    them available to others.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Using AutoML to reduce experimentation time
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the marketplace models don't meet your needs or if your organization has
    the **build rather than buy** policy, first check whether your dataset and use
    case are suitable for AutoPilot. At the time of writing this book, AutoPilot supports
    tabular data and classification and regression problems. AutoPilot automatically
    analyzes datasets and builds multiple models with different combinations of algorithms
    and hyperparameters and finally selects the best algorithm for the list. This
    saves both time and cost. Additionally, the service provides transparency through
    two notebooks – a data preparation notebook and a model candidate selection notebook,
    which details all the behind-the-scenes steps performed by AutoPilot. So, even
    if you don't end up using the model built and recommended by AutoPilot, you can
    use these notebooks as a starting point for your own experimentation and modify
    them using your business domain knowledge.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: However, at the time of publication of this book, AutoPilot only supports regression
    and classification using tabular data. For other data types and problems, you
    will have to build and train your model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Iterating locally with small datasets
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During ML experimentation, iterate with a smaller dataset in the SageMaker notebook's
    local environment first. Once you iron out details such as code bugs and data
    issues, you can scale up with the full dataset and distributed training clusters
    managed by SageMaker. This phased approach will let you iterate faster at lower
    costs. SageMaker SDK makes this easy by supporting `instance-type = "local"` for
    the training API so that you can reuse the same code in the local environment
    or on the distributed cluster. Note that at the time of publication, local mode
    only works in SageMaker notebook instances, not in the Studio environment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Rightsizing training infrastructure
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are ready to launch a distributed training cluster, it is important
    to choose the right number and type of instances in the cluster. For built-in
    or custom algorithms that do not support distributed training, your cluster will
    always have a single instance. For algorithms and frameworks that do support distributed
    training, take advantage of data parallelism and model parallelism as discussed
    in [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)*, Training and
    Tuning at Scale,* to complete training faster, thereby reducing the overall training
    costs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: While there are various instance types with different capacity configurations
    available, it is important to rightsize the training instances based on the ML
    algorithm used. For example, simple algorithms may not train faster on the larger
    instance types since they cannot take advantage of hardware parallelism. Even
    worse, they may even train slower due to high GPU communication overhead. Best
    practice for cost optimization is to start with a smaller instance, scale out
    first by adding more instances to the training cluster, and then scale up to more
    powerful instances. However, if you are using a deep learning framework and distributed
    training, best practice would be to scale up to more GPUs/CPUs on a single instance
    before scaling out because the network I/O involved may negatively impact the
    training performance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: In addition to selecting the right infrastructure, you can also use optimized
    versions of ML frameworks that result in faster training. SageMaker provides optimized
    versions of multiple open source ML frameworks including TensorFlow, Chainer,
    Keras, and Theano. SageMaker versions of these popular frameworks are optimized
    for high performance on all SageMaker ML instances.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing hyperparameter-tuning costs
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperparameter tuning is also an expensive task, using sophisticated search
    and algorithms. Best practice is to rely on the automated model tuning capability
    provided by managed SageMaker Automatic Model Tuning, also known as **hyperparameter
    tuning** (**HPT**). Automatic model tuning finds the best version of a model by
    running multiple training jobs using the algorithm and hyperparameter ranges specified
    by you. HPT then chooses the hyperparameter values that result in the best model
    as measured by the objective metric you specify. Behind the scenes, HPT uses ML
    techniques that can determine optimal hyperparameters with a limited number of
    training jobs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: You can further speed up the HPT jobs using warm start mode. With warm start,
    you no longer must start an HPT job from scratch; instead, you can create a new
    HPT job based on one or more parent jobs. This allows you to reuse the training
    jobs conducted in the parent jobs as prior knowledge. Warm start allows you to
    reduce the time and cost associated with model tuning.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Saving training costs with Managed Spot Training
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Managed Spot Training applies the cost-saving construct of Spot Instances
    and applies it to hyperparameter tuning and training. The Managed Spot Training
    capability takes advantage of checkpointing, to resume training jobs easily. Since
    you don't have to run the training from the start again, this reduces your overall
    training costs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Using insights and recommendations from Debugger
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to deep learning on SageMaker, training with GPU is very powerful,
    but training costs can add up quickly. SageMaker Debugger provides insight into
    deep learning training both into the ML framework in use and the underlying compute
    resources. The deep profiler capability provides you with recommendations that
    you can implement to improve training performance and reduce resource wastage.
    For a detailed discussion of Debugger's capabilities, please refer to [*Chapter
    7*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*,* *Profile Training Jobs
    with Amazon SageMaker Debugger*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Saving ML infrastructure costs with SavingsPlan
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you enable SavingsPlan in your AWS account, it analyzes your ML resource
    usage within a time of your choice – the past 7, 30, or up to 60 days. The service
    then recommends the right plan to use to optimize costs. You can also select a
    pre-payment option from three different options: no upfront costs, partial upfront
    (50% or more), or all upfront. Once you configure these options, SavingsPlan provides
    you with details of how your monthly spend can be optimized. Additionally, it
    also suggests an hourly usage commitment that maximizes your savings. The plans
    cover all ML instance families, notebook instances, Studio instances, training
    instances, batch transform instances, real-time endpoint instances, Data Wrangler
    instances, and SageMaker Processing instances, thereby helping to optimize costs
    across various phases of ML workloads.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: While Managed Spot Training and SavingsPlan are both cost-saving approaches,
    they are not meant to be combined. With SavingsPlan, you are billed every hour
    of the commitment regardless of whether it is fully used. Best practice is to
    use SavingsPlan and Managed Spot Training usages separately. For example, use
    SavingsPlan for predictable steady-state recurring training workloads and Managed
    Spot Training for new training workloads and prototyping where you do not have
    a clear idea of monthly costs yet.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing inference costs
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inference costs typically make up most ML costs. Inference costs are discussed
    in detail in [*Chapter 10*](B17249_10_Final_JM_ePub.xhtml#_idTextAnchor179)*,
    Optimizing Model Hosting and Inference Costs*, which details several ways to improve
    inference performance while reducing inference costs. These methods include using
    batch inference where possible, deploying several models behind a single inference
    endpoint to reduce cost and help with advanced canary or blue/green deployments,
    scaling inference endpoints to meet demand, and using EI and SageMaker Neo to
    provide better inference performance at a lower cost.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Stopping or terminating resources
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you terminate or at least stop the ML resources once you are done.
    While the instances for training, hyperparameter tuning, batch inferences, and
    processing jobs will be managed and automatically deleted by SageMaker, you are
    responsible for notebook instances, endpoint, and monitoring schedules. Stop or
    delete these resources to avoid unnecessary costs using automation with scripts
    that stop resources based on idle time or a schedule.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various SageMaker features and how they
    are applicable for building cost-optimized ML workloads:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – AWS service capabilities for cost-optimized ML workloads'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '](img/05.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – AWS service capabilities for cost-optimized ML workloads
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: This section concludes the discussion on applying best practices to build well-architected
    ML workloads on AWS.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you reviewed the five pillars – operational excellence, security,
    reliability, performance, and cost optimization – that make up the Well-Architected
    Framework. You then dove into the best practices for each of these pillars, with
    an eye to applying these best practices to ML workloads. You learned how to use
    the SageMaker capabilities with related AWS services to build well-architected
    ML workloads on AWS.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: As you architect your ML applications, you typically must make trade-offs between
    the pillars depending on your organization's priorities. For example, when getting
    started with ML, cost-optimization may not be at the top of your mind but establishing
    operational standards may be important. However, as the number of ML workloads
    scale, cost-optimization could become an important consideration. By applying
    the best practices you learned in this chapter, you can architect and implement
    ML applications that meet your organization's needs and periodically evaluate
    your applications against the best practices.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will apply all these best practices and see how to
    operate in multiple AWS environments that reflect the real world.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS service/feature** | **How you should use it for securing ML** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| IAM | By implementing authentication, authorization, and access control through
    IAM users, groups, roles, and policies. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| IAM access advisor/CloudWatch Event history | By identifying opportunities
    to refine IAM policies. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| CloudWatch/CloudTrail | By collecting logs, implementing monitoring, and
    auditing. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| VPC | By providing infrastructure and network isolation. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| VPC endpoints | By routing traffic through the AWS network and avoiding exposure
    to the public internet. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Private links | By routing traffic through the AWS network and avoiding exposure
    to the public internet. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
