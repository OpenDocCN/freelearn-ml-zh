- en: Chapter 13:Well-Architected Machine Learning with Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章：使用Amazon SageMaker进行架构良好的机器学习
- en: When running workloads in the cloud, you want to make sure that the workload
    is architected correctly to take advantage of all that the cloud can offer. AWS
    Well-Architected Framework helps you with this, by providing a formal approach
    for learning best practices across five critical pillars applicable to any workload
    deployed to AWS. The pillars are operational excellence, security, reliability,
    performance efficiency, and cost optimization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中运行工作负载时，您希望确保工作负载的架构正确，以便充分利用云所能提供的一切。AWS架构良好框架可以帮助您做到这一点，通过提供一种正式的方法来学习适用于部署到AWS的任何工作负载的五个关键支柱的最佳实践。这些支柱是运营卓越、安全性、可靠性、性能效率和成本优化。
- en: The framework provides guidance on how to improve your architecture and make
    trade-offs between the pillars both during the initial development and continued
    updates of the workload. While you can use Well-Architected Framework to evaluate
    your workload from a general technology perspective, while building **machine
    learning** (**ML**) applications, it would be great to have focused guidance across
    the five pillars specific to ML. AWS Machine Learning Lens provides this focused
    guidance, which you can use to compare and measure your ML workload on AWS against
    best practices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架提供了在初始开发和持续更新工作负载过程中如何改进架构以及如何在支柱之间进行权衡的指导。虽然您可以使用架构良好框架从一般技术角度评估您的负载，但在构建**机器学习**（**ML**）应用程序时，拥有针对ML五个支柱的特定指导将非常有益。AWS机器学习透镜提供了这种专注的指导，您可以使用它来比较和衡量您的AWS上机器学习工作负载与最佳实践的差距。
- en: Important Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'For an in-depth review of the Well-Architected Framework and Machine Learning
    Lens, please review these two white papers from AWS: [https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf](https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf)
    and [https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解架构良好框架和机器学习透镜，请查阅AWS提供的以下两篇白皮书：[https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf](https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf)
    和 [https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf)。
- en: So far in this book, we have discussed how to use different Amazon SageMaker
    capabilities across all phases of ML workloads. In this chapter, we will learn
    how to combine guidance from both the generic Well-Architected Framework and Machine
    Learning Lens and apply it to the end-to-end ML workloads built on SageMaker.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书中我们已经讨论了如何使用不同的Amazon SageMaker功能跨越机器学习工作负载的所有阶段。在本章中，我们将学习如何结合通用架构良好框架和机器学习透镜的指导，并将其应用于基于SageMaker构建的端到端机器学习工作负载。
- en: Please note that this chapter does not introduce any new SageMaker features,
    but rather dives into how you can apply the capabilities you already know to build
    a well-architected ML workload. You will learn how SageMaker's specific capabilities
    are combined with other AWS services across the five pillars, with some of the
    capabilities playing a key role in multiple pillars.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章不介绍任何新的SageMaker功能，而是深入探讨如何将您已知的技能应用于构建一个架构良好的机器学习工作负载。您将了解SageMaker的具体功能如何与其他AWS服务结合，在五个支柱中发挥作用，其中一些功能在多个支柱中扮演关键角色。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Best practices for operationalizing ML workloads
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运营机器学习工作负载的最佳实践
- en: Best practices for securing ML workloads
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护机器学习工作负载的最佳实践
- en: Best practices for building reliable ML workloads
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建可靠机器学习工作负载的最佳实践
- en: Best practices for building performant ML workloads
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建高性能机器学习工作负载的最佳实践
- en: Best practices for building cost-optimized ML workloads
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建成本优化的机器学习工作负载的最佳实践
- en: Best practices for operationalizing ML workloads
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运营机器学习工作负载的最佳实践
- en: Many organizations start their ML journey with a few experiments of building
    models to solve one or more business problems. Cloud platforms, in general, and
    ML platforms such as SageMaker make this experimentation easy by providing seamless
    access to elastic compute infrastructure and built-in support for various ML frameworks
    and algorithms. Once these experiments have proven successful, the next natural
    step is to move the models into production. Typically, at this time, organizations
    want to move out of the research-and-development phase and into operationalizing
    ML.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织从构建模型来解决一个或多个业务问题的几个实验开始他们的机器学习之旅。通常，云平台，如SageMaker，通过提供无缝访问弹性计算基础设施和内置对各种机器学习框架和算法的支持，使这种实验变得容易。一旦这些实验证明成功，下一步自然的步骤就是将模型投入生产。通常，在这个时候，组织希望从研发阶段转向机器学习的运营。
- en: The idea of MLOps is gaining popularity these days. MLOps, at a very high level,
    involves bringing together people, processes, and technology to integrate ML workloads
    into release management, CI/CD, and operations. Without diving into all the details
    of MLOps, in this section, we will discuss best practices for operationalizing
    ML workloads using technology. We will also discuss which SageMaker features play
    a role in various aspects of operationalizing ML workloads.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps的概念最近越来越受欢迎。在非常高的层面上，MLOps涉及将人员、流程和技术结合起来，将机器学习工作负载集成到发布管理、CI/CD和运营中。在不深入MLOps的所有细节的情况下，在本节中，我们将讨论使用技术来运营机器学习工作负载的最佳实践。我们还将讨论哪些SageMaker功能在运营机器学习工作负载的各个方面发挥作用。
- en: Let's now look at best practices for operationalizing ML workloads on AWS in
    the following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在以下部分中查看在AWS上运营机器学习工作负载的最佳实践。
- en: Ensuring reproducibility
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保可重复性
- en: To successfully operationalize the end-to-end ML system, you must first ensure
    its reproducibility through versioned data, code, and artifacts. Best practice
    is to version all inputs used to create models, including training data, data
    preparation code, algorithm implementation code, parameters, and hyperparameters,
    in addition to all trained model artifacts. A versioning strategy is also about
    helping in the model-update phase and allowing for easy rollback to a specific
    known working version if a model update fails or if the updated model does not
    meet your requirements.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功地将端到端机器学习系统投入运营，您必须首先通过版本化的数据、代码和工件来确保其可重复性。最佳实践是版本化创建模型所使用的所有输入，包括训练数据、数据准备代码、算法实现代码、参数和超参数，以及所有训练模型工件。版本化策略还涉及在模型更新阶段提供帮助，并在模型更新失败或更新的模型不符合您的要求时，允许轻松回滚到特定的已知有效版本。
- en: Tracking ML artifacts
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪机器学习工件
- en: Iterative development of ML models using different algorithms and hyperparameters
    for each algorithm results in many training experiments and multiple model versions.
    Keeping track of these experiments and resulting models along with each model's
    lineage is important to meet auditing and compliance requirements. Model lineage
    also helps with root-cause analysis in case of degrading model performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的算法和超参数对每个算法进行迭代开发，会导致许多训练实验和多个模型版本的产生。跟踪这些实验和结果模型，以及每个模型的血统，对于满足审计和合规要求非常重要。模型血统还有助于在模型性能下降的情况下进行根本原因分析。
- en: While you can certainly build a custom tracking solution, best practice is to
    use a managed service such as SageMaker Experiments. Experiments allows you to
    track, organize, visualize, and compare ML models across all phases of the ML
    lifecycle including feature engineering, model training, model tuning, and model
    deployment. With SageMaker Experiments, you can easily choose to deploy or update
    the model with a specific version. Experiments also provides you with the model
    lineage capability. For a detailed discussion of SageMaker Experiments' capabilities,
    please refer to the *Amazon SageMaker Experiments* section of [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)*,
    Training and Tuning at Scale*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您当然可以构建定制的跟踪解决方案，但最佳实践是使用如SageMaker Experiments之类的托管服务。Experiments允许您跟踪、组织、可视化和比较机器学习模型在整个机器学习生命周期中的所有阶段，包括特征工程、模型训练、模型调优和模型部署。使用SageMaker
    Experiments，您可以轻松选择部署或更新特定版本的模型。Experiments还为您提供了模型血统功能。有关SageMaker Experiments功能的详细讨论，请参阅[*第6章*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)的*“Amazon
    SageMaker Experiments”部分*，*“大规模训练和调优”*。
- en: Additionally, you can also use the Amazon SageMaker ML Lineage Tracking capability,
    which keeps track of information about the individual steps of an ML workflow
    from data preparation to model deployment. With the information tracked, you can
    reproduce the workflow steps, track model and dataset lineage, and establish model
    governance and audit standards.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以使用Amazon SageMaker ML Lineage Tracking功能，该功能跟踪从数据准备到模型部署的ML工作流程的各个步骤的信息。通过跟踪的信息，您可以重现工作流程步骤，跟踪模型和数据集的谱系，并建立模型治理和审计标准。
- en: Automating deployment pipelines
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化部署流程
- en: Automated pipelines minimize human intervention in moving a trained ML model
    from lower-level environments such as development and staging into a production
    environment. The aim is to have a codified deployment pipeline created with Infrastructure-as-Code
    and Configuration-as-Code, with manual and automated quality gates incorporated
    into the pipeline. Manual quality gates can ensure that models are promoted to
    the production environment only after ensuring that there are no operational concerns
    such as security exposure. Automated quality gates, on the other hand, can be
    used to evaluate model metrics such as precision, recall, or accuracy. Pipelines
    result in consistent deployment as well as providing the ability to reliably recreate
    ML-related resources across multiple environments with minimal human intervention.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化流程将最小化将训练好的机器学习模型从开发、预发布等低级环境移动到生产环境时的人工干预。目标是创建一个编码化的部署流程，使用基础设施即代码和配置即代码，并将手动和自动的质量关卡纳入流程中。手动质量关卡可以确保在模型被提升到生产环境之前，不存在诸如安全暴露等运营问题。另一方面，自动质量关卡可以用来评估模型指标，如精确度、召回率或准确度。流程可以实现一致的部署，并提供在多个环境中以最小的人工干预可靠地重新创建机器学习相关资源的能力。
- en: Using Amazon SageMaker Pipelines, you can build automated model workflows. You
    can build every step of the ML lifecycle as a pipeline step to develop and deploy
    models and monitor the pipelines. You can further manage dependencies between
    each step, build the correct sequence, and execute the steps automatically. A
    service that brings in CI/CD practices to ML workloads is SageMaker Projects.
    This service helps you move models from concept to production. Additionally, you
    can easily meet governance and audit standards using a combination of SageMaker
    Projects and SageMaker Pipelines, by automatically tracking code, datasets, and
    model versions through each step of the ML lifecycle. This enables you to go back
    and replay model-generation steps, troubleshoot problems, and reliably track the
    lineage of models at scale. For a detailed discussion of automated workflows and
    MLOps, please refer to [*Chapter 12*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*,
    Machine Learning Automated Workflows*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker Pipelines，您可以构建自动化的模型工作流程。您可以将机器学习生命周期的每个步骤作为一个流程步骤来开发和部署模型，并监控流程。您可以进一步管理每个步骤之间的依赖关系，构建正确的顺序，并自动执行步骤。将CI/CD实践引入机器学习工作负载的服务是SageMaker
    Projects。此服务帮助您将模型从概念转移到生产。此外，您可以通过结合使用SageMaker Projects和SageMaker Pipelines，通过在机器学习生命周期的每个步骤自动跟踪代码、数据集和模型版本，轻松满足治理和审计标准。这使您能够回溯并重新播放模型生成步骤，解决问题，并可靠地跟踪模型的规模级谱系。有关自动化工作流程和MLOps的详细讨论，请参阅[*第12章*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*，机器学习自动化工作流程*。
- en: Monitoring production models
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控生产模型
- en: Continued monitoring of deployed models is a critical step in operationalizing
    ML workloads, since a model's performance and effectiveness may degrade over time.
    Ensuring that the model continues to meet your business needs starts with the
    identification of the metrics that measure both model-related metrics and business
    metrics. Ensure that all metrics critical to model evaluation against your business
    KPIs are defined early on and collected during monitoring.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对部署的模型进行持续监控是使机器学习工作负载投入运营的关键步骤，因为模型的表现和有效性可能会随时间退化。确保模型持续满足您的业务需求，首先从识别衡量模型相关指标和业务指标的指标开始。确保在监控过程中早期定义并收集所有对模型评估您的业务关键绩效指标至关重要的指标。
- en: Once the metrics are identified, to ensure the continued high quality of the
    deployed model, use the Amazon SageMaker Model Monitor capabilities and its integration
    with CloudWatch to proactively detect issues, raise alerts, and automate remediation
    actions. In addition to detecting model-quality degradation, you can monitor data
    drift, bias drift, and feature attribution drift to meet your reliability, regulatory,
    and model explainability requirements.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了指标，为了确保部署的模型持续保持高质量，使用 Amazon SageMaker Model Monitor 功能及其与 CloudWatch
    的集成，主动检测问题、发出警报并自动化修复操作。除了检测模型质量下降外，你还可以监控数据漂移、偏差漂移和特征归因漂移，以满足你的可靠性、监管和模型可解释性要求。
- en: CloudWatch alerts that have been triggered because of model monitoring can be
    used to automate activities such as invalidating the current model, reverting
    to an older model version, or retraining a new model based on new ground truth
    data. Updates to production models should consider trade-offs between the risk
    of introducing changes, the cost of retraining, and the potential value of having
    a newer model in production. For a detailed discussion of model monitoring, please
    refer to [*Chapter 11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring
    Production Models with Amazon SageMaker Model Monitor and Clarify*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型监控而触发的 CloudWatch 警报可用于自动化以下活动：使当前模型无效、回滚到较旧模型版本或根据新的真实数据重新训练新模型。生产模型的更新应考虑引入更改的风险、重新训练的成本以及在生产中拥有较新模型可能带来的潜在价值。有关模型监控的详细讨论，请参阅[*第
    11 章*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*，使用 Amazon SageMaker Model
    Monitor 和 Clarify 进行生产模型监控*。
- en: Important note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: While this section has focused on SageMaker-native approaches for operationalizing
    ML workloads, please note that similar automated pipelines can be built using
    a combination of SageMaker APIs and other AWS services such as CodePipeline, Step
    Functions, Lambda, and SageMaker Data Science SDK. Multiple MLOps architectures
    are documented along with sample code at [https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml](https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节重点介绍了用于操作 ML 工作负载的 SageMaker 原生方法，但请注意，可以使用 SageMaker API 和其他 AWS 服务（如 CodePipeline、Step
    Functions、Lambda 和 SageMaker Data Science SDK）的组合来构建类似的自动化管道。在 [https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml](https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml)
    上记录了多个 MLOps 架构和示例代码。
- en: 'The following table summarizes the various AWS services and features applicable
    to operationalizing ML workloads:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了适用于操作 ML 工作负载的各种 AWS 服务和功能：
- en: '![Figure 13.1 – AWS Services used for operationalizing ML workloads'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.1 – 用于操作 ML 工作负载的 AWS 服务'
- en: '](img/012.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.1 – 用于操作 ML 工作负载的 AWS 服务](img/012.jpg)'
- en: Figure 13.1 – AWS Services used for operationalizing ML workloads
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – 用于操作 ML 工作负载的 AWS 服务
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to enable secure ML workloads.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解 SageMaker 如何与其他 AWS 服务集成，以实现安全的 ML 工作负载。
- en: Best practices for securing ML workloads
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护 ML 工作负载的最佳实践
- en: When securing an ML workload, you should take into consideration infrastructure
    and network security, authentication and authorization, encrypting data and model
    artifacts, logging and auditing, and meeting regulatory requirements. In this
    section, we will discuss best practices for security ML workloads using a combination
    of SageMaker and related AWS services.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在保护 ML 工作负载时，你应该考虑基础设施和网络安全、身份验证和授权、加密数据和模型工件、日志记录和审计以及满足监管要求。在本节中，我们将讨论使用 SageMaker
    和相关 AWS 服务组合来保护 ML 工作负载的最佳实践。
- en: Let's now look at best practices for securing ML workloads on AWS in the following
    sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在以下章节中查看在 AWS 上保护 ML 工作负载的最佳实践。
- en: Isolating the ML environment
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隔离 ML 环境
- en: To build secure ML workloads, you need an isolated compute and network environment.
    To achieve this for ML on SageMaker, deploy all resources such as notebooks, studio
    domain, training jobs, processing jobs, and endpoints within a **Virtual** **Private**
    **Cloud** (**VPC**). A VPC provides an isolated environment where all traffic
    between various SageMaker components flows within the network. You can add another
    layer of isolation by using security groups that include rules for both inbound
    and outbound traffic allowed by subnets within the VPC, thereby isolating your
    ML resources further.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建安全的机器学习工作负载，您需要一个隔离的计算和网络环境。为了在SageMaker上实现这一点，请在**虚拟** **私有** **云**（**VPC**）中部署所有资源，例如笔记本、工作室域、训练作业、处理作业和端点。VPC提供了一个隔离的环境，其中所有SageMaker组件之间的流量都在网络内部流动。您可以通过使用包含VPC内子网允许的入站和出站流量的规则的网络安全组，添加另一层隔离，从而进一步隔离您的机器学习资源。
- en: Even if you use SageMaker without a VPC, all resources run in an environment
    managed by AWS on single-tenancy EC2 instances, which ensures that your ML environments
    are isolated from other customers. However, deploying ML resources, such as training
    containers, in a VPC allows you to monitor all network traffic in and out of these
    resources using VPC Flow Logs. Additionally, you can use VPC endpoints and AWS
    PrivateLink to enable communication between SageMaker and other AWS services such
    as S3 or CloudWatch. This keeps all traffic flowing between the various services
    within the AWS network without exposing the traffic to the public internet.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您在没有VPC的情况下使用SageMaker，所有资源也在由AWS管理的单租户EC2实例的环境中运行，这确保了您的机器学习环境与其他客户隔离。然而，在VPC中部署机器学习资源，例如训练容器，允许您使用VPC
    Flow Logs监控这些资源的进出网络流量。此外，您可以使用VPC端点和AWS PrivateLink来启用SageMaker与其他AWS服务（如S3或CloudWatch）之间的通信。这保持了AWS网络内各种服务之间所有流量的流动，同时不会将流量暴露给公共互联网。
- en: Disabling internet and root access
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用互联网和root访问
- en: By default, SageMaker notebook instances are internet-enabled to allow you to
    download external libraries and customize your working environment. Additionally,
    root access is enabled on these notebooks, giving you the flexibility to leverage
    external libraries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SageMaker笔记本实例已启用互联网访问，以便您下载外部库并自定义工作环境。此外，这些笔记本启用了root访问，这使您能够利用外部库。
- en: Only use these default settings in a lower-level sandbox and development environments
    to figure out the optimal working notebook environment. In all other non-production
    and production environments, launch SageMaker resources in your own VPC and turn
    off root access to prevent downloading and installing unauthorized software. Import
    all necessary libraries into a private repository such as AWS CodeArtifact before
    you isolate your environment. This allows you to seamlessly download specific
    versions of libraries without having to reach out to the internet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在低级沙盒和开发环境中，仅使用这些默认设置来找出最佳工作笔记本环境。在其他所有非生产环境和生产环境中，请在自己的VPC中启动SageMaker资源，并关闭root访问以防止下载和安装未经授权的软件。在隔离您的环境之前，将所有必要的库导入到私有仓库，例如AWS
    CodeArtifact。这允许您无缝下载特定版本的库，而无需访问互联网。
- en: Additionally, use codified lifecycle configurations to automate setting up the
    notebook environment. Similarly, training and deployed inference containers managed
    by SageMaker are internet-enabled by default. When launching training and inference
    resources, use `VPCConfig` and `EnableNetworkIsolation` flags to protect these
    resources from external network traffic. In this case, all downloads and uploads
    of data and model artifacts are routed through your VPC. At the same time, the
    training and inference containers remain isolated from the network and do not
    have access to any resource within your VPC or on the internet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用编码化的生命周期配置来自动设置笔记本环境。同样，由SageMaker管理的训练和部署推理容器默认启用互联网访问。在启动训练和推理资源时，使用`VPCConfig`和`EnableNetworkIsolation`标志来保护这些资源免受外部网络流量的影响。在这种情况下，所有数据和模型工件的上传和下载都通过您的VPC路由。同时，训练和推理容器保持与网络的隔离，并且无法访问VPC内或互联网上的任何资源。
- en: Enforcing authentication and authorization
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制执行身份验证和授权
- en: Implement a strong mechanism to determine who can access the ML resources (authentication)
    and what resources authenticated users can access (authorization). SageMaker is
    natively integrated with AWS IAM, a service used to manage access to all AWS services
    and resources. IAM allows you to define fine-grained access controls using IAM
    users, groups, roles, and policies. You can implement least-privilege access using
    a combination of identity-based policies to specify what an IAM user, role, or
    group can do and resource-based policies to specify who has access to the resource
    and what actions they can perform on it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实施一个强大的机制来确定谁可以访问机器学习资源（身份验证）以及认证用户可以访问哪些资源（授权）。SageMaker与AWS IAM服务原生集成，该服务用于管理对所有AWS服务和资源的访问。IAM允许您使用IAM用户、组、角色和策略定义细粒度的访问控制。您可以使用基于身份的策略组合来实现最小权限访问，以指定IAM用户、角色或组可以执行的操作，以及基于资源的策略来指定谁可以访问资源以及他们可以在其上执行哪些操作。
- en: When designing these IAM policies, it is tempting to start with wide-open IAM
    policies with good intentions of tightening them as you go. However, best practice
    is to start with tight policies that grant minimal required access and add additional
    permissions when required. Periodically review and refine policies to ensure that
    no unnecessary permissions are granted. The IAM service provides the Access Advisor
    capability, which shows you when various AWS services are last accessed by different
    entities such as IAM groups, users, roles, and policies. Use this information
    to refine the policies. All the service API calls are also logged by CloudTrail,
    and you can use the CloudTrail history to determine which permissions can be removed
    based on the usage patterns.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计这些IAM策略时，可能会倾向于从开放性较高的IAM策略开始，并怀有随着实施过程逐步收紧策略的良好意图。然而，最佳实践是从严格的策略开始，仅授予最小必需的访问权限，并在需要时添加额外的权限。定期审查和精炼策略，以确保不会授予不必要的权限。IAM服务提供了访问顾问功能，该功能显示各种AWS服务最后一次被不同的实体（如IAM组、用户、角色和策略）访问的时间。使用这些信息来精炼策略。所有服务API调用也由CloudTrail记录，您可以使用CloudTrail历史记录来确定基于使用模式可以删除哪些权限。
- en: Securing data and model artifacts
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护数据和模型工件
- en: IAM policies can also be used for access-control of data and models in S3\.
    Additionally, you can use a security service called Amazon Macie to protect and
    classify data in S3\. Macie internally uses ML to automatically discover, classify,
    and protect sensitive data. It automatically recognizes sensitive data such as
    **personally** **identifiable** **information** (**PII**) or **intellectual**
    **property** (**IP**), providing visibility into data access and movement patterns.
    Macie continuously monitors for anomalies in data-access patterns and proactively
    generates alerts on unauthorized access and data leaks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: IAM策略也可以用于S3中数据和模型的访问控制。此外，您还可以使用名为Amazon Macie的安全服务来保护并分类S3中的数据。Macie内部使用机器学习来自动发现、分类和保护敏感数据。它自动识别敏感数据，如**个人身份信息**（**PII**）或**知识产权**（**IP**），提供对数据访问和移动模式的可视性。Macie持续监控数据访问模式中的异常，并主动生成关于未经授权的访问和数据泄露的警报。
- en: The next important aspects to secure are data and model artifacts of an ML system,
    both at rest and in transit. To secure data in transit within a VPC, use **Transport**
    **Layer** **Security** (**TLS**). To secure data at rest, best practice is to
    use encryption to block malicious actors from reading your data and model artifacts.
    You can use either client-side or server-side encryption. SageMaker comes with
    built-in encryption capabilities to protect training data and model artifacts
    both at rest and in transit. For example, when launching a training job, you can
    specify the encryption key to be used. You have the flexibility of using SageMaker-managed
    keys, AWS-managed keys, or your own customer-managed keys.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要的安全方面是确保机器学习系统的数据和模型工件的安全，无论是在静止状态还是在传输过程中。为了在VPC内部传输过程中保护数据，请使用**传输层安全**（**TLS**）。为了在静止状态下保护数据，最佳实践是使用加密来阻止恶意行为者读取您的数据和模型工件。您可以使用客户端或服务器端加密。SageMaker内置了加密功能，可以保护静止状态和传输过程中的训练数据和模型工件。例如，在启动训练作业时，您可以指定要使用的加密密钥。您可以选择使用SageMaker管理的密钥、AWS管理的密钥或您自己的客户管理的密钥。
- en: Logging, monitoring, and auditing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录、监控和审计
- en: SageMaker is natively integrated with CloudWatch and CloudTrail. You can capture
    logs from SageMaker training, processing, and inference in CloudWatch, which can
    further be used for troubleshooting. All SageMaker (and other AWS services) API
    calls are logged by CloudTrail, allowing you to track down which IAM user, AWS
    account, or source IP address made the API call along with when the call occurred.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 与 CloudWatch 和 CloudTrail 原生集成。你可以在 CloudWatch 中捕获 SageMaker 训练、处理和推理的日志，这可以进一步用于故障排除。所有
    SageMaker（以及其他 AWS 服务）的 API 调用都由 CloudTrail 记录，允许你追踪哪个 IAM 用户、AWS 账户或源 IP 地址执行了
    API 调用，以及调用发生的时间。
- en: Meeting regulatory requirements
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 满足监管要求
- en: For many organizations, ML solutions need to comply with regulatory standards
    and pass compliance certifications that vary significantly across countries and
    industries. Amazon SageMaker complies with a wide range of compliance programs,
    including PCI, HIPAA, SOC 1/2/3, FedRAMP, and ISO 9001/27001/27017/27018.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多组织来说，机器学习解决方案需要符合监管标准，并通过各国和行业差异很大的合规认证。Amazon SageMaker 符合广泛的合规计划，包括 PCI、HIPAA、SOC
    1/2/3、FedRAMP 和 ISO 9001/27001/27017/27018。
- en: 'The following table summarizes the various AWS services applicable to securing
    ML workloads:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了适用于保护机器学习工作负载的各种 AWS 服务：
- en: '![Figure 13.2 – AWS services used for securing ML workloads'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.2 – 用于保护机器学习工作负载的 AWS 服务'
- en: '](img/021.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/021.jpg)'
- en: Figure 13.2 – AWS services used for securing ML workloads
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 用于保护机器学习工作负载的 AWS 服务
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build reliable ML workloads.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解 SageMaker 如何与其他 AWS 服务集成，以构建可靠的机器学习工作负载。
- en: Best practices for reliable ML workloads
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可靠机器学习工作负载的最佳实践
- en: 'For a reliable system, there are two considerations at the core:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个可靠系统，有两个核心考虑因素：
- en: First, the ability to recover from planned and unplanned disruptions
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，能够从计划内和计划外的中断中恢复
- en: Second, the ability to meet unpredictable increases in traffic demands
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，能够满足不可预测的流量需求增加
- en: Ideally, the system should achieve both without affecting downstream applications
    and end consumers. In this section, we will discuss best practices for building
    reliable ML workloads using a combination of SageMaker and related AWS services.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，系统应在不影响下游应用程序和最终用户的情况下实现这两点。在本节中，我们将讨论使用 SageMaker 和相关 AWS 服务构建可靠机器学习工作负载的最佳实践。
- en: Let's now look at some best practices for securing ML workloads on AWS in the
    following sections.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看以下章节中关于在 AWS 上保护机器学习工作负载的一些最佳实践。
- en: Recovering from failure
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从故障中恢复
- en: For an ML workload, the ability to recover gracefully should be part of all
    the steps that make up the iterative ML process. A failure can occur with data
    storage, data processing, model training, or model hosting, which may result from
    a variety of events ranging from system failure to human error.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习工作负载，优雅地恢复的能力应该是构成迭代机器学习过程的各个步骤的一部分。故障可能发生在数据存储、数据处理、模型训练或模型托管中，这可能是从系统故障到人为错误的各种事件的结果。
- en: For ML on SageMaker, all data (and model artifacts) is typically saved in S3\.
    This ensures decoupling between ML data and the computation processing. To prevent
    an inadvertent loss of data, best practice is to use a combination of IAM and
    S3 policies to ensure least privilege-based access to data. Additionally, use
    S3 versioning and object tagging to enable versioning and traceability of data
    (and model artifacts) for easy recovery or recreation in the event of failure.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SageMaker 上的机器学习，所有数据（以及模型工件）通常都保存在 S3 中。这确保了机器学习数据与计算处理的解耦。为了防止数据意外丢失，最佳实践是使用
    IAM 和 S3 策略的组合来确保基于最小权限的数据访问。此外，使用 S3 版本控制和对象标记来启用数据（以及模型工件）的版本控制和可追溯性，以便在发生故障时易于恢复或重建。
- en: Next, consider the reliability of ML training, which is often a long, time-consuming
    process. It is not uncommon to see training jobs that run over multiple hours
    and even multiple days. If these long-running training jobs are disrupted due
    to a power outage, OS fault, or other unexpected error, having the ability to
    reliably resume from where the job stopped is critical. ML checkpointing should
    be used in this situation. On SageMaker, a few built-in algorithms and all supported
    deep learning frameworks provide the capability of turning on checkpointing when
    a training job is launched. When you enable checkpointing, SageMaker automatically
    saves snapshots of the model state during training. This enables you to reliably
    restart a training job from the last saved checkpoint.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑机器学习训练的可靠性，这通常是一个漫长且耗时的过程。看到运行数小时甚至数天的训练作业并不罕见。如果这些长时间运行的训练作业因断电、操作系统故障或其他意外错误而中断，能够可靠地从作业停止的地方继续进行至关重要。在这种情况下应使用机器学习检查点。在
    SageMaker 上，一些内置算法和所有支持的深度学习框架在启动训练作业时提供启用检查点的功能。当您启用检查点时，SageMaker 会自动在训练过程中保存模型状态的快照。这使得您可以从最后一个保存的检查点可靠地重新启动训练作业。
- en: Tracking model origin
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪模型来源
- en: Let's say your training goes off without a hitch and you have a trained model
    artifact saved in an S3 bucket. What happens if you lose this model artifact due
    to human error, such as someone in your team deleting it by mistake? In a reliable
    ML system, you need to be able to recreate this model using the same data, version
    of the code, and parameters as the original model. Hence, it is important to keep
    track of all these aspects during training. Using SageMaker Experiments, you can
    keep track of all the steps and artifacts that went into creating a model so you
    can easily recreate the model as necessary. Another benefit of tracking with SageMaker
    Experiments is the ability to troubleshoot issues in production for reliable operation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的训练过程顺利进行，并且您已将训练好的模型工件保存在一个 S3 桶中。如果由于人为错误（例如，您的团队中有人不小心删除了它）而丢失了这个模型工件，会发生什么？在一个可靠的机器学习系统中，您需要能够使用与原始模型相同的数据、代码版本和参数来重新创建此模型。因此，在训练过程中跟踪所有这些方面非常重要。使用
    SageMaker 实验，您可以跟踪创建模型的所有步骤和工件，以便您可以根据需要轻松地重新创建模型。使用 SageMaker 实验跟踪的另一个好处是能够对生产中的问题进行故障排除，以确保可靠运行。
- en: In addition to relying on Experiments to be able to recreate a specific version
    of a model artifact, use a combination of IAM and S3 policies to ensure least
    privilege-based access to minimize the risk of accidental model-artifact deletion.
    Implement measures such as requiring MFA for model artifact deletion and storing
    a secondary copy of the artifact as required by your organization's disaster recovery
    strategy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了依赖实验来重新创建模型工件的具体版本外，还应结合使用 IAM 和 S3 策略，以确保基于最小权限的访问，以最大限度地降低意外删除模型工件的风险。实施诸如要求对模型工件删除进行多因素认证以及根据您组织的灾难恢复策略存储工件副本等措施。
- en: Automating deployment pipelines
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化部署管道
- en: To ensure that all steps leading up to model deployment are executed consistently,
    use a CI/CD pipeline with access controls to enforce least privilege-based access.
    Deployment automation combined with manual and automated quality gates ensures
    that all changes can be effectively validated with dependent systems prior to
    deployment. Amazon SageMaker Pipelines has the capability to bring CI/CD practices
    to ML workloads for improved reliability. Codifying the CI/CD pipelines using
    SageMaker Pipelines provides you with an additional capability of dealing with
    the model endpoint itself being deleted inadvertently. Using the Infrastructure-as-Code
    approach, the endpoint can be recreated. This requires a well-defined versioning
    strategy in place for your data, code, algorithms, hyperparameters, model artifacts,
    container images, and more. Version everything and document your versioning strategy.
    For a detailed discussion of SageMaker Pipelines capabilities, please refer to
    the *Amazon SageMaker Pipelines* section of [*Chapter 12*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*,
    Machine Learning Automated Workflows*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保所有导致模型部署的步骤都得到一致执行，请使用具有访问控制的CI/CD管道来强制执行最小权限访问。部署自动化与手动和自动质量门相结合，确保在部署之前所有更改都可以有效地与依赖系统进行验证。Amazon
    SageMaker Pipelines具有将CI/CD实践应用于机器学习工作负载以改进可靠性的能力。使用SageMaker Pipelines编码CI/CD管道为您提供了处理模型端点意外删除的额外能力。采用基础设施即代码（IaC）方法，可以重新创建端点。这需要对您的数据、代码、算法、超参数、模型工件、容器镜像等实施良好的版本控制策略。对一切进行版本控制，并记录您的版本控制策略。有关SageMaker
    Pipelines功能的详细讨论，请参阅[*第12章*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*，机器学习自动化工作流程*中的*Amazon
    SageMaker Pipelines*部分。
- en: Additionally, follow the *train once and deploy everywhere* strategy. Because
    of the decoupled nature of the training process and results, you can share the
    trained model artifact across multiple environments. This prevents retraining
    in multiple environments and introducing unexpected changes to the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，遵循*训练一次，部署到任何地方*的策略。由于训练过程和结果的去耦合性质，您可以在多个环境中共享训练好的模型工件。这可以防止在多个环境中重新训练并引入对模型的不期望变化。
- en: Handling unexpected traffic patterns
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理意外的流量模式
- en: Once the model is deployed, you must ensure the reliability of the deployed
    model in serving the inference requests. The model should be able to handle spikes
    in inference traffic and continue to operate at the quality necessary to meet
    the business requirements.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署后，您必须确保部署的模型在服务推理请求时的可靠性。模型应能够处理推理流量的峰值，并继续以符合业务要求的质量运行。
- en: To handle traffic spikes, deploy the model with the Autoscaling-enabled SageMaker
    real-time endpoint. With Autoscaling enabled, SageMaker automatically increases
    (and decreases) the computation capacity behind the hosted model in response to
    the dynamic shifts in the inference traffic. Autoscaling provided by SageMaker
    is horizontal scaling, meaning it adds new instances or removes existing instances
    to handle the inference traffic variations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理流量峰值，请使用启用自动扩展的SageMaker实时端点部署模型。启用自动扩展后，SageMaker会根据推理流量的动态变化自动增加（和减少）托管模型背后的计算能力。SageMaker提供的自动扩展是水平扩展，这意味着它会添加新实例或删除现有实例来处理推理流量的变化。
- en: Continuous monitoring of deployed model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型的持续监控
- en: To ensure the continued high quality of the deployed model, use the Amazon SageMaker
    Model Monitor capabilities and its integration with CloudWatch to proactively
    detect issues, raise alerts, and automate remediation actions when a production
    model is not performing as expected. In addition to model quality, you can monitor
    data drift, bias drift, and feature-attribution drift to meet your reliability,
    regulatory, and model explainability requirements. Ensure that all metrics critical
    to model evaluation against your business KPIs are defined and monitored. For
    a detailed discussion of model monitoring, please refer to [*Chapter 11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*,
    Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保部署的模型持续保持高质量，请使用Amazon SageMaker Model Monitor的功能及其与CloudWatch的集成，以主动检测问题、发出警报，并在生产模型未按预期表现时自动执行修复操作。除了模型质量外，您还可以监控数据漂移、偏差漂移和特征归因漂移，以满足您的可靠性、监管和模型可解释性要求。确保定义并监控所有对模型评估至关重要的指标，以符合您的业务关键绩效指标（KPIs）。有关模型监控的详细讨论，请参阅[*第11章*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*，使用Amazon
    SageMaker Model Monitor和Clarify监控生产模型*。
- en: Updating model with new versions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新模型的新版本
- en: Finally, you must consider how to update a production model reliably. SageMaker
    endpoint production variants can be used to implement multiple deployment strategies
    such as A/B, Blue/Green, Canary, and Shadow deployments. The advanced deployment
    strategies along with detailed implementation are discussed in [*Chapter 9*](B17249_09_Final_JM_ePub.xhtml#_idTextAnchor163)*,
    Updating Production Models Using Amazon SageMaker Endpoint Production Variants*.
    Depending on the model consumer's tolerance for risk and downtime, choose an appropriate
    deployment strategy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您必须考虑如何可靠地更新生产模型。SageMaker端点生产变体可用于实现多种部署策略，如A/B、蓝绿、金丝雀和影子部署。高级部署策略及其详细实现将在[*第9章*](B17249_09_Final_JM_ePub.xhtml#_idTextAnchor163)*，使用Amazon
    SageMaker端点生产变体更新生产模型*中讨论。根据模型消费者的风险容忍度和停机时间，选择合适的部署策略。
- en: 'The following table summarizes the various AWS services applicable to building
    reliable ML workloads:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了适用于构建可靠机器学习工作负载的各种AWS服务：
- en: '![Figure 13.3 – AWS service capabilities used for reliable ML workloads'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.3 – 用于可靠机器学习工作负载的AWS服务功能'
- en: '](img/03.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/03.jpg)'
- en: Figure 13.3 – AWS service capabilities used for reliable ML workloads
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 – 用于可靠机器学习工作负载的AWS服务功能
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build reliable, performance-efficient workloads.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将了解SageMaker如何与其他AWS服务集成，以构建可靠、性能高效的工作负载。
- en: Best practices for building performant ML workloads
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建高性能机器学习工作负载的最佳实践
- en: Given the compute- and time-intensive nature of ML workloads, it is important
    to choose the most performant resources appropriate for each individual phase
    of the workload. Computation, memory, and network bandwidth requirements are unique
    to each phase of the ML process. Besides the performance of the infrastructure,
    the performance of the model as measured by metrics such as accuracy is also important.
    In this section, we will discuss best practices to apply in selecting the most
    performant resources for building ML workloads on SageMaker.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习工作负载的计算和时间密集型特性，选择适合工作负载每个单独阶段的最高性能资源非常重要。计算、内存和网络带宽需求是机器学习过程每个阶段的独特需求。除了基础设施的性能外，模型性能（如通过准确率等指标衡量）也非常重要。在本节中，我们将讨论在SageMaker上构建机器学习工作负载时应应用的最佳实践。
- en: Let's now look at best practices for building performant ML workloads on AWS
    in the following sections.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在以下章节中查看在AWS上构建高性能机器学习工作负载的最佳实践。
- en: Rightsizing ML resources
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习资源的合理配置
- en: SageMaker supports a variety of ML instance types with a varying combination
    of CPU, GPU, FPGA, memory, storage, and networking capacity. Each instance type,
    in turn, supports multiple instance sizes. So, you have a range of choices to
    choose from to suit your specific workload. The best practice is to choose different
    compute resource configurations for data processing, building, training, and hosting
    your ML model. This is made possible by the decoupled nature of SageMaker, which
    allows you to choose different instance types and sizes for different APIs. For
    example, you can choose `ml.c5.medium` for a notebook instance as your working
    environment, use a cluster of four `ml.p3.large` GPU instances for training, and
    finally host the trained model on two `ml.m5.4xlarge` instances with Elastic Inference
    attached. Additionally, in the SageMaker Studio environment, you can change the
    notebook instance type seamlessly without any interruption to your work.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker支持多种机器学习实例类型，这些实例类型具有不同的CPU、GPU、FPGA、内存、存储和网络容量组合。每个实例类型反过来又支持多个实例大小。因此，您有一系列选择来满足您特定的需求。最佳实践是为数据处理、构建、训练和托管您的机器学习模型选择不同的计算资源配置。这是由SageMaker的解耦特性实现的，它允许您为不同的API选择不同的实例类型和大小。例如，您可以选择`ml.c5.medium`作为笔记本实例的工作环境，使用四个`ml.p3.large`
    GPU实例的集群进行训练，最后在两个附加了弹性推理的`ml.m5.4xlarge`实例上托管训练好的模型。此外，在SageMaker Studio环境中，您可以在不中断您的工作的情况下无缝更改笔记本实例类型。
- en: While you have the flexibility of choosing different compute options for different
    ML phases, how do you choose the specific instance types and sizes to use? This
    comes down to understanding your workload and experimentation. For example, if
    you know that the training framework and algorithm of your choice will need GPU
    support, choose a GPU cluster to train on. While it may be tempting to use GPUs
    for all training, traditional algorithms may not work well on GPUs due to the
    communication overheads involved. Some built-in algorithms, such as XGBoost, implement
    an open source algorithm that has been optimized for CPU computations. SageMaker
    also provides optimized versions of frameworks, such as TensorFlow and PyTorch,
    which include optimizations for high-performance training across Amazon EC2 instance
    families.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有选择不同计算选项的灵活性来应对不同的机器学习阶段时，你如何选择具体的实例类型和大小呢？这取决于你对工作负载的理解和实验。例如，如果你知道你选择的训练框架和算法将需要GPU支持，请选择一个用于训练的GPU集群。虽然使用GPU进行所有训练可能很有吸引力，但传统的算法可能由于涉及到的通信开销而在GPU上运行不佳。一些内置算法，如XGBoost，实现了针对CPU计算优化的开源算法。SageMaker还提供了针对TensorFlow和PyTorch等框架的优化版本，这些版本包括针对Amazon
    EC2实例家族的高性能训练优化。
- en: Monitoring resource utilization
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控资源利用率
- en: Once you make your initial choice of instances and kick off training, SageMaker
    training jobs emit CloudWatch metrics for resource utilization that you can use
    to improve your training runs the next time. Additionally, when you enable Debugger
    with your training jobs, SageMaker Debugger provides visibility into training
    jobs and the infrastructure a training job is executing on. Debugger also monitors
    and reports on the system resources such as CPU, GPU, and memory, providing you
    with insights into resource underutilization and bottlenecks. If you use TensorFlow
    or PyTorch for your deep learning training jobs, Debugger provides you with a
    view into framework metrics that can be used to speed up your training jobs. For
    a detailed discussion of Debugger's capabilities, please refer to [*Chapter 7*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*,*
    *Profile Training Jobs with Amazon SageMaker Debugger.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你做出了初始的实例选择并启动了训练，SageMaker训练作业会发出用于资源利用率的CloudWatch指标，你可以使用这些指标来改进下一次的训练运行。此外，当你为训练作业启用调试器时，SageMaker调试器提供了对训练作业及其执行的基础设施的可见性。调试器还监控并报告系统资源，如CPU、GPU和内存，为你提供资源低利用率瓶颈的见解。如果你使用TensorFlow或PyTorch进行深度学习训练作业，调试器为你提供了框架指标的视图，这些指标可以用来加速你的训练作业。有关调试器功能的详细讨论，请参阅[*第7章*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*，*使用Amazon
    SageMaker调试器分析训练作业性能*。
- en: Rightsizing hosting infrastructure
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整托管基础设施的大小
- en: Once the model is trained and ready to be deployed to choose instances for real-time
    endpoints, consider what your target performance is. Target performance is a combination
    of how many requests to serve in each period and the desired latency for each
    request, for example, 10,000 requests per minute with a maximum of a 1 millisecond
    response time. Once you have the target performance in mind, perform load testing
    in a non-production environment to figure out the instance type, instance size,
    and number of instances to host the model on. Recommended best practice is to
    deploy the endpoint with at least two instances across two availability zones
    for high availability.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成并准备好部署到用于实时端点的实例，考虑一下你的目标性能。目标性能是每个周期要服务的请求数量和每个请求期望的延迟的组合，例如，每分钟10,000个请求，最大响应时间为1毫秒。一旦你有了目标性能的概念，在非生产环境中进行负载测试，以确定用于托管模型的实例类型、实例大小和实例数量。推荐的最佳实践是，为了高可用性，在两个可用区部署至少两个实例的端点。
- en: Once you decide on the instance type to use, start with the minimum number of
    instances necessary to meet your steady-state traffic and take advantage of the
    Autoscaling capability of SageMaker hosting. Using Autoscaling, SageMaker can
    automatically scale the inference capacity depending on the utilization and request
    traffic thresholds you configure. Capacity adjustments to meet your performance
    requirements are done by updating the endpoint configuration without any downtime.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定了要使用的实例类型，就从满足你的稳定状态流量所需的最小实例数量开始，并利用SageMaker托管服务的自动扩展功能。使用自动扩展，SageMaker可以根据你配置的利用率和请求流量阈值自动调整推理容量。为了满足你的性能要求，通过更新端点配置来调整容量，而无需停机。
- en: Additionally, you can scale up the hosting infrastructure for deep learning
    models using Amazon `Inf1` instances, which are best suited to applications such
    as search, recommendation engines, and computer vision, at a low cost.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以使用 Amazon `Inf1` 实例扩展深度学习模型的托管基础设施，这些实例非常适合搜索、推荐引擎和计算机视觉等应用，且成本较低。
- en: While real-time endpoints provide access to models deployed on SageMaker, some
    workloads may warrant inference at the edge due to latency requirements, for example,
    models used to determine defective product parts in a manufacturing plant. In
    such cases, the model needs to be deployed on cameras within the manufacturing
    plant. For such use cases, use SageMaker Neo and SageMaker Edge Manager to optimize,
    deploy, and manage models at the edge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实时端点提供了对 SageMaker 上部署的模型的访问，但由于延迟要求，某些工作负载可能需要在边缘进行推理，例如用于确定制造工厂中缺陷产品部件的模型。在这种情况下，模型需要部署在制造工厂内的摄像头中。对于此类用例，请使用
    SageMaker Neo 和 SageMaker Edge Manager 来优化、部署和管理边缘模型。
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: While real-time endpoints and models deployed at the edge provide synchronous
    predictions, batch transform is used for asynchronous inferences with more tolerance
    for longer response times. Use experimentation to determine the right instance
    type, size, and number of instances to be used for batch transform with job completion
    time in mind.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实时端点和边缘部署的模型提供同步预测，但批量转换用于异步推理，对较长的响应时间有更大的容忍度。使用实验来确定合适的实例类型、大小和实例数量，同时考虑到作业完成时间。
- en: Continuous monitoring of deployed model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续监控已部署模型
- en: Once the model is actively serving inference traffic, use SageMaker Model Monitor
    to continuously monitor ML models for data drift, model-quality performance, feature-importance
    drift, and bias drift. Behind the scenes, Model Monitor uses distributed processing
    jobs. As with batch processing, use experimentation and load testing to determine
    the processing job resources necessary to complete each scheduled monitoring job
    execution. For a detailed discussion of Model Monitor, please refer to [*Chapter
    11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring Production Models
    with Amazon SageMaker Model Monitor and Clarify*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型开始主动处理推理流量，请使用 SageMaker 模型监控器来持续监控机器学习模型的数据漂移、模型质量性能、特征重要性漂移和偏差漂移。在幕后，模型监控器使用分布式处理作业。与批量处理一样，使用实验和负载测试来确定完成每个计划监控作业执行所需的处理作业资源。有关模型监控器的详细讨论，请参阅[*第
    11 章*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*，使用 Amazon SageMaker 模型监控器和
    Clarify 监控生产模型*。
- en: 'The following table summarizes the various SageMaker features and how they
    are applicable for building performant ML workloads:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了各种 SageMaker 功能及其在构建高性能机器学习工作负载中的应用：
- en: '![Figure 13.4 – AWS service capabilities for building performant ML workloads.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.4 – AWS 服务能力，用于构建高性能机器学习工作负载。'
- en: '](img/04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/04.jpg)'
- en: Figure 13.4 – AWS service capabilities for building performant ML workloads.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – AWS 服务能力，用于构建高性能机器学习工作负载。
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build cost-optimized workloads.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将了解 SageMaker 如何与其他 AWS 服务集成以构建成本优化工作负载。
- en: Best practices for cost-optimized ML workloads
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本优化机器学习工作负载的最佳实践
- en: For many organizations, the lost opportunity cost of not embracing disruptive
    technologies such as ML outweighs the ML costs. By implementing a few best practices,
    these organizations can get the best possible returns on their ML investment.
    In this section, we will discuss best practices to apply for cost-optimized ML
    workloads on SageMaker.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多组织来说，不采用如机器学习等颠覆性技术所失去的机会成本超过了机器学习的成本。通过实施一些最佳实践，这些组织可以从他们的机器学习投资中获得最佳回报。在本节中，我们将讨论适用于
    SageMaker 上成本优化机器学习工作负载的最佳实践。
- en: Let's now look at best practices for building cost-optimized ML workloads on
    AWS in the following sections.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在 AWS 上构建成本优化机器学习工作负载的最佳实践。
- en: Optimizing data labeling costs
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化数据标注成本
- en: Labeling of data used for ML training, typically done at the very beginning
    of the ML process, can be tedious, error-prone, and time-consuming. Labeling at
    scale consumes many working hours, making this an expensive task, too. To optimize
    cost for data labeling, use SageMaker Ground Truth. Ground Truth provides capabilities
    for data labeling at scale using a combination of human workforce and active learning.
    When active learning is enabled, a labeling task is routed to humans only if a
    model cannot confidently finish the task. The human-labeled data is then used
    to train the model to improve accuracy. Therefore, as the labeling job progresses,
    less and less data needs to be labeled by humans. This results in faster completion
    of the job at reduced costs. For a detailed discussion of Ground Truth capabilities,
    please refer to [*Chapter 3*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*,
    Data Labeling with Amazon SageMaker Ground Truth*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 用于机器学习训练的数据标注，通常在机器学习过程的初期进行，可能非常繁琐、容易出错且耗时。大规模标注消耗大量工作时间，这使得这项任务也变得成本高昂。为了优化数据标注的成本，请使用SageMaker
    Ground Truth。Ground Truth通过结合人力和主动学习提供大规模数据标注的能力。当启用主动学习时，只有当模型无法自信地完成标注任务时，标注任务才会路由给人类。然后，人工标注的数据被用来训练模型以提高准确性。因此，随着标注工作的进行，需要人工标注的数据越来越少。这导致工作完成速度加快，成本降低。有关Ground
    Truth功能的详细讨论，请参阅[*第3章*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*，使用Amazon
    SageMaker Ground Truth进行数据标注*。
- en: Reducing experimentation costs with models from AWS Marketplace
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AWS Marketplace中的模型降低实验成本
- en: ML is inherently iterative and experimental. Having to run multiple algorithms
    with different sets of hyperparameters each time leads to several training jobs
    before you can determine a model that meets your needs. All this training adds
    up in terms of time and costs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习本质上是迭代的和实验性的。每次都需要运行多个算法，每个算法都有不同的超参数集，这会导致在确定满足您需求的模型之前，需要执行多个训练任务。所有这些训练在时间和成本上都会累积。
- en: A big part of experimentation is the research and reuse of readily available
    pre-trained models that may suit your needs. AWS Marketplace for ML gives you
    a catalog of datasets and models made available by vendors vetted by AWS. You
    can subscribe to models that meet your needs and potentially save the time and
    costs involved in developing your own models. If you do, however, end up developing
    your own models, you can use the marketplace to monetize your models by making
    them available to others.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的一个很大部分是研究和重用现成的预训练模型，这些模型可能符合您的需求。AWS Marketplace for ML为您提供了由AWS审核的供应商提供的数据集和模型目录。您可以订阅满足您需求的模型，并可能节省开发自己模型所需的时间和成本。如果您最终开发了自己的模型，您可以使用市场将您的模型货币化，使其可供他人使用。
- en: Using AutoML to reduce experimentation time
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoML减少实验时间
- en: If the marketplace models don't meet your needs or if your organization has
    the **build rather than buy** policy, first check whether your dataset and use
    case are suitable for AutoPilot. At the time of writing this book, AutoPilot supports
    tabular data and classification and regression problems. AutoPilot automatically
    analyzes datasets and builds multiple models with different combinations of algorithms
    and hyperparameters and finally selects the best algorithm for the list. This
    saves both time and cost. Additionally, the service provides transparency through
    two notebooks – a data preparation notebook and a model candidate selection notebook,
    which details all the behind-the-scenes steps performed by AutoPilot. So, even
    if you don't end up using the model built and recommended by AutoPilot, you can
    use these notebooks as a starting point for your own experimentation and modify
    them using your business domain knowledge.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果市场中的模型不符合您的需求，或者您的组织有**购买而非构建**的政策，首先检查您的数据集和用例是否适合AutoPilot。在撰写本书时，AutoPilot支持表格数据以及分类和回归问题。AutoPilot自动分析数据集，并构建多个模型，这些模型具有不同的算法和超参数组合，并最终选择最佳算法。这节省了时间和成本。此外，该服务通过两个笔记本提供透明度——一个数据准备笔记本和一个模型候选选择笔记本，这些笔记本详细说明了AutoPilot执行的幕后步骤。因此，即使您最终没有使用AutoPilot构建和推荐的模型，您也可以将这些笔记本作为您自己实验的起点，并使用您的业务领域知识进行修改。
- en: However, at the time of publication of this book, AutoPilot only supports regression
    and classification using tabular data. For other data types and problems, you
    will have to build and train your model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本书出版时，AutoPilot仅支持使用表格数据进行的回归和分类。对于其他数据类型和问题，您将不得不构建和训练自己的模型。
- en: Iterating locally with small datasets
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用小数据集进行本地迭代
- en: During ML experimentation, iterate with a smaller dataset in the SageMaker notebook's
    local environment first. Once you iron out details such as code bugs and data
    issues, you can scale up with the full dataset and distributed training clusters
    managed by SageMaker. This phased approach will let you iterate faster at lower
    costs. SageMaker SDK makes this easy by supporting `instance-type = "local"` for
    the training API so that you can reuse the same code in the local environment
    or on the distributed cluster. Note that at the time of publication, local mode
    only works in SageMaker notebook instances, not in the Studio environment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习实验过程中，首先在SageMaker笔记本的本地环境中使用较小的数据集进行迭代。一旦解决了代码错误和数据问题等细节，您可以使用SageMaker管理的完整数据集和分布式训练集群进行扩展。这种分阶段的方法将使您以更低的成本更快地迭代。SageMaker
    SDK通过支持训练API中的`instance-type = "local"`来简化这一过程，这样您就可以在本地环境或分布式集群上重用相同的代码。请注意，在发布时，本地模式仅在SageMaker笔记本实例中工作，不在Studio环境中。
- en: Rightsizing training infrastructure
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整训练基础设施
- en: When you are ready to launch a distributed training cluster, it is important
    to choose the right number and type of instances in the cluster. For built-in
    or custom algorithms that do not support distributed training, your cluster will
    always have a single instance. For algorithms and frameworks that do support distributed
    training, take advantage of data parallelism and model parallelism as discussed
    in [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)*, Training and
    Tuning at Scale,* to complete training faster, thereby reducing the overall training
    costs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当您准备启动分布式训练集群时，选择集群中正确的实例数量和类型非常重要。对于不支持分布式训练的内置或自定义算法，您的集群将始终只有一个实例。对于支持分布式训练的算法和框架，利用如[*第6章*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)中讨论的数据并行性和模型并行性，以更快地完成训练，从而降低整体训练成本。
- en: While there are various instance types with different capacity configurations
    available, it is important to rightsize the training instances based on the ML
    algorithm used. For example, simple algorithms may not train faster on the larger
    instance types since they cannot take advantage of hardware parallelism. Even
    worse, they may even train slower due to high GPU communication overhead. Best
    practice for cost optimization is to start with a smaller instance, scale out
    first by adding more instances to the training cluster, and then scale up to more
    powerful instances. However, if you are using a deep learning framework and distributed
    training, best practice would be to scale up to more GPUs/CPUs on a single instance
    before scaling out because the network I/O involved may negatively impact the
    training performance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有各种不同容量配置的实例类型可供选择，但根据所使用的机器学习算法，正确调整训练实例的大小非常重要。例如，简单的算法可能无法在更大的实例类型上更快地训练，因为它们无法利用硬件并行性。更糟糕的是，由于高GPU通信开销，它们甚至可能训练得更慢。成本优化的最佳实践是从小实例开始，首先通过向训练集群添加更多实例进行扩展，然后升级到更强大的实例。然而，如果您正在使用深度学习框架和分布式训练，最佳实践是在扩展之前将单个实例上的GPU/CPUs数量升级更多，因为涉及的网络安全I/O可能会对训练性能产生负面影响。
- en: In addition to selecting the right infrastructure, you can also use optimized
    versions of ML frameworks that result in faster training. SageMaker provides optimized
    versions of multiple open source ML frameworks including TensorFlow, Chainer,
    Keras, and Theano. SageMaker versions of these popular frameworks are optimized
    for high performance on all SageMaker ML instances.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择正确的基础设施外，您还可以使用优化版本的机器学习框架，从而实现更快的训练。SageMaker提供了包括TensorFlow、Chainer、Keras和Theano在内的多个开源机器学习框架的优化版本。这些流行框架的SageMaker版本针对所有SageMaker机器学习实例的高性能进行了优化。
- en: Optimizing hyperparameter-tuning costs
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化超参数调优成本
- en: Hyperparameter tuning is also an expensive task, using sophisticated search
    and algorithms. Best practice is to rely on the automated model tuning capability
    provided by managed SageMaker Automatic Model Tuning, also known as **hyperparameter
    tuning** (**HPT**). Automatic model tuning finds the best version of a model by
    running multiple training jobs using the algorithm and hyperparameter ranges specified
    by you. HPT then chooses the hyperparameter values that result in the best model
    as measured by the objective metric you specify. Behind the scenes, HPT uses ML
    techniques that can determine optimal hyperparameters with a limited number of
    training jobs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整也是一个成本高昂的任务，需要使用复杂的搜索和算法。最佳实践是依赖由托管 SageMaker 自动模型调优提供的自动化模型调优功能，也称为**超参数调整**（**HPT**）。自动模型调优通过运行多个训练作业来找到最佳模型版本，这些作业使用您指定的算法和超参数范围。HPT
    然后选择导致最佳模型（根据您指定的目标指标衡量）的超参数值。在幕后，HPT 使用可以在有限数量的训练作业中确定最佳超参数的 ML 技术。
- en: You can further speed up the HPT jobs using warm start mode. With warm start,
    you no longer must start an HPT job from scratch; instead, you can create a new
    HPT job based on one or more parent jobs. This allows you to reuse the training
    jobs conducted in the parent jobs as prior knowledge. Warm start allows you to
    reduce the time and cost associated with model tuning.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用预热启动模式进一步加快 HPT 作业的速度。使用预热启动，您不再需要从头开始启动 HPT 作业；相反，您可以根据一个或多个父作业创建一个新的
    HPT 作业。这允许您重用父作业中进行的训练作业作为先验知识。预热启动可以帮助您减少与模型调优相关的时耗和成本。
- en: Saving training costs with Managed Spot Training
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用管理式 Spot 训练节省训练成本
- en: SageMaker Managed Spot Training applies the cost-saving construct of Spot Instances
    and applies it to hyperparameter tuning and training. The Managed Spot Training
    capability takes advantage of checkpointing, to resume training jobs easily. Since
    you don't have to run the training from the start again, this reduces your overall
    training costs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 管理式 Spot 训练将 Spot 实例的成本节省结构应用于超参数调整和训练。管理式 Spot 训练功能利用检查点功能，以便轻松恢复训练作业。由于您不必再次从头开始运行训练，这降低了您的整体训练成本。
- en: Using insights and recommendations from Debugger
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用调试器的洞察和建议
- en: When it comes to deep learning on SageMaker, training with GPU is very powerful,
    but training costs can add up quickly. SageMaker Debugger provides insight into
    deep learning training both into the ML framework in use and the underlying compute
    resources. The deep profiler capability provides you with recommendations that
    you can implement to improve training performance and reduce resource wastage.
    For a detailed discussion of Debugger's capabilities, please refer to [*Chapter
    7*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*,* *Profile Training Jobs
    with Amazon SageMaker Debugger*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到在 SageMaker 上的深度学习时，使用 GPU 进行训练非常强大，但训练成本可能会迅速增加。SageMaker Debugger 提供了对正在使用的
    ML 框架和底层计算资源的深度学习训练洞察。深度分析功能为您提供改进训练性能和减少资源浪费的建议。有关调试器功能的详细讨论，请参阅[*第 7 章*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*，*使用
    Amazon SageMaker Debugger 分析训练作业*。
- en: Saving ML infrastructure costs with SavingsPlan
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SavingsPlan 节省 ML 基础设施成本
- en: 'Once you enable SavingsPlan in your AWS account, it analyzes your ML resource
    usage within a time of your choice – the past 7, 30, or up to 60 days. The service
    then recommends the right plan to use to optimize costs. You can also select a
    pre-payment option from three different options: no upfront costs, partial upfront
    (50% or more), or all upfront. Once you configure these options, SavingsPlan provides
    you with details of how your monthly spend can be optimized. Additionally, it
    also suggests an hourly usage commitment that maximizes your savings. The plans
    cover all ML instance families, notebook instances, Studio instances, training
    instances, batch transform instances, real-time endpoint instances, Data Wrangler
    instances, and SageMaker Processing instances, thereby helping to optimize costs
    across various phases of ML workloads.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在 AWS 账户中启用 SavingsPlan，它将在您选择的时间范围内分析您的 ML 资源使用情况——过去 7 天、30 天或最多 60 天。然后，该服务推荐合适的计划以优化成本。您还可以从三种不同的选项中选择预付费选项：无前期费用、部分前期（50%
    或更多）或全部前期。一旦您配置了这些选项，SavingsPlan 将为您提供如何优化月度支出的详细信息。此外，它还建议一个每小时使用承诺，以最大化您的节省。这些计划涵盖了所有
    ML 实例系列、笔记本实例、Studio 实例、训练实例、批量转换实例、实时端点实例、Data Wrangler 实例和 SageMaker Processing
    实例，从而帮助优化 ML 工作负载各个阶段的成本。
- en: While Managed Spot Training and SavingsPlan are both cost-saving approaches,
    they are not meant to be combined. With SavingsPlan, you are billed every hour
    of the commitment regardless of whether it is fully used. Best practice is to
    use SavingsPlan and Managed Spot Training usages separately. For example, use
    SavingsPlan for predictable steady-state recurring training workloads and Managed
    Spot Training for new training workloads and prototyping where you do not have
    a clear idea of monthly costs yet.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Managed Spot Training 和 SavingsPlan 都是节省成本的方法，但它们并不适合结合使用。在使用 SavingsPlan
    时，无论是否完全使用，你都需要为承诺的每小时付费。最佳实践是分别使用 SavingsPlan 和 Managed Spot Training。例如，使用 SavingsPlan
    来处理可预测的稳定状态重复训练工作负载，而使用 Managed Spot Training 来处理新的训练工作负载和原型设计，在这些情况下，你还没有明确的月度成本概念。
- en: Optimizing inference costs
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化推理成本
- en: Inference costs typically make up most ML costs. Inference costs are discussed
    in detail in [*Chapter 10*](B17249_10_Final_JM_ePub.xhtml#_idTextAnchor179)*,
    Optimizing Model Hosting and Inference Costs*, which details several ways to improve
    inference performance while reducing inference costs. These methods include using
    batch inference where possible, deploying several models behind a single inference
    endpoint to reduce cost and help with advanced canary or blue/green deployments,
    scaling inference endpoints to meet demand, and using EI and SageMaker Neo to
    provide better inference performance at a lower cost.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 推理成本通常占机器学习成本的大部分。推理成本在[*第 10 章*](B17249_10_Final_JM_ePub.xhtml#_idTextAnchor179)“优化模型托管和推理成本”中进行了详细讨论，该章节详细介绍了几种提高推理性能同时降低推理成本的方法。这些方法包括尽可能使用批量推理，在单个推理端点后面部署多个模型以降低成本并帮助进行高级金丝雀或蓝/绿部署，根据需求扩展推理端点，以及使用
    EI 和 SageMaker Neo 以较低的成本提供更好的推理性能。
- en: Stopping or terminating resources
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止或终止资源
- en: Ensure that you terminate or at least stop the ML resources once you are done.
    While the instances for training, hyperparameter tuning, batch inferences, and
    processing jobs will be managed and automatically deleted by SageMaker, you are
    responsible for notebook instances, endpoint, and monitoring schedules. Stop or
    delete these resources to avoid unnecessary costs using automation with scripts
    that stop resources based on idle time or a schedule.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你在完成工作后终止或至少停止机器学习资源。虽然训练、超参数调整、批量推理和处理作业的实例将由 SageMaker 管理并自动删除，但你负责笔记本实例、端点和监控计划。停止或删除这些资源，以避免使用自动化脚本（基于空闲时间或计划）停止资源而产生不必要的成本。
- en: 'The following table summarizes the various SageMaker features and how they
    are applicable for building cost-optimized ML workloads:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了各种 SageMaker 功能及其在构建成本优化的机器学习工作负载中的应用：
- en: '![Figure 13.5 – AWS service capabilities for cost-optimized ML workloads'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.5 – AWS 服务能力，用于成本优化的机器学习工作负载'
- en: '](img/05.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.5 – AWS 服务能力，用于成本优化的机器学习工作负载'
- en: Figure 13.5 – AWS service capabilities for cost-optimized ML workloads
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – AWS 服务能力，用于成本优化的机器学习工作负载
- en: This section concludes the discussion on applying best practices to build well-architected
    ML workloads on AWS.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了在 AWS 上构建良好架构的机器学习工作负载的最佳实践讨论。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you reviewed the five pillars – operational excellence, security,
    reliability, performance, and cost optimization – that make up the Well-Architected
    Framework. You then dove into the best practices for each of these pillars, with
    an eye to applying these best practices to ML workloads. You learned how to use
    the SageMaker capabilities with related AWS services to build well-architected
    ML workloads on AWS.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你回顾了构成 Well-Architected 框架的五个支柱——运营卓越、安全性、可靠性、性能和成本优化。然后你深入研究了这些支柱的最佳实践，目的是将这些最佳实践应用于机器学习工作负载。你学习了如何使用
    SageMaker 功能与相关 AWS 服务一起构建 AWS 上的良好架构的机器学习工作负载。
- en: As you architect your ML applications, you typically must make trade-offs between
    the pillars depending on your organization's priorities. For example, when getting
    started with ML, cost-optimization may not be at the top of your mind but establishing
    operational standards may be important. However, as the number of ML workloads
    scale, cost-optimization could become an important consideration. By applying
    the best practices you learned in this chapter, you can architect and implement
    ML applications that meet your organization's needs and periodically evaluate
    your applications against the best practices.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当你设计你的机器学习应用时，你通常必须在根据你组织的优先级进行权衡的支柱之间做出权衡。例如，当你开始使用机器学习时，成本优化可能不是你首先考虑的，但建立操作标准可能很重要。然而，随着机器学习工作负载数量的增加，成本优化可能成为一个重要的考虑因素。通过应用你在本章中学到的最佳实践，你可以设计和实施满足你组织需求的机器学习应用，并定期评估你的应用与最佳实践。
- en: In the next chapter, you will apply all these best practices and see how to
    operate in multiple AWS environments that reflect the real world.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将应用所有这些最佳实践，并了解如何在多个反映现实世界的AWS环境中进行操作。
- en: '| **AWS service/feature** | **How you should use it for securing ML** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **AWS服务/功能** | **您应该如何使用它来保护机器学习** |'
- en: '| IAM | By implementing authentication, authorization, and access control through
    IAM users, groups, roles, and policies. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| IAM | 通过IAM用户、组、角色和政策实现身份验证、授权和访问控制。|'
- en: '| IAM access advisor/CloudWatch Event history | By identifying opportunities
    to refine IAM policies. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| IAM访问顾问/CloudWatch事件历史 | 通过识别优化IAM策略的机会。|'
- en: '| CloudWatch/CloudTrail | By collecting logs, implementing monitoring, and
    auditing. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| CloudWatch/CloudTrail | 通过收集日志、实施监控和审计。|'
- en: '| VPC | By providing infrastructure and network isolation. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| VPC | 通过提供基础设施和网络隔离。|'
- en: '| VPC endpoints | By routing traffic through the AWS network and avoiding exposure
    to the public internet. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| VPC端点 | 通过路由流量通过AWS网络并避免暴露于公共互联网。|'
- en: '| Private links | By routing traffic through the AWS network and avoiding exposure
    to the public internet. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 私有链接 | 通过路由流量通过AWS网络并避免暴露于公共互联网。|'
