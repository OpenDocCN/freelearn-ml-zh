- en: Chapter 13:Well-Architected Machine Learning with Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running workloads in the cloud, you want to make sure that the workload
    is architected correctly to take advantage of all that the cloud can offer. AWS
    Well-Architected Framework helps you with this, by providing a formal approach
    for learning best practices across five critical pillars applicable to any workload
    deployed to AWS. The pillars are operational excellence, security, reliability,
    performance efficiency, and cost optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The framework provides guidance on how to improve your architecture and make
    trade-offs between the pillars both during the initial development and continued
    updates of the workload. While you can use Well-Architected Framework to evaluate
    your workload from a general technology perspective, while building **machine
    learning** (**ML**) applications, it would be great to have focused guidance across
    the five pillars specific to ML. AWS Machine Learning Lens provides this focused
    guidance, which you can use to compare and measure your ML workload on AWS against
    best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For an in-depth review of the Well-Architected Framework and Machine Learning
    Lens, please review these two white papers from AWS: [https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf](https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf)
    and [https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, we have discussed how to use different Amazon SageMaker
    capabilities across all phases of ML workloads. In this chapter, we will learn
    how to combine guidance from both the generic Well-Architected Framework and Machine
    Learning Lens and apply it to the end-to-end ML workloads built on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this chapter does not introduce any new SageMaker features,
    but rather dives into how you can apply the capabilities you already know to build
    a well-architected ML workload. You will learn how SageMaker's specific capabilities
    are combined with other AWS services across the five pillars, with some of the
    capabilities playing a key role in multiple pillars.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for operationalizing ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for securing ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for building reliable ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for building performant ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for building cost-optimized ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for operationalizing ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many organizations start their ML journey with a few experiments of building
    models to solve one or more business problems. Cloud platforms, in general, and
    ML platforms such as SageMaker make this experimentation easy by providing seamless
    access to elastic compute infrastructure and built-in support for various ML frameworks
    and algorithms. Once these experiments have proven successful, the next natural
    step is to move the models into production. Typically, at this time, organizations
    want to move out of the research-and-development phase and into operationalizing
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of MLOps is gaining popularity these days. MLOps, at a very high level,
    involves bringing together people, processes, and technology to integrate ML workloads
    into release management, CI/CD, and operations. Without diving into all the details
    of MLOps, in this section, we will discuss best practices for operationalizing
    ML workloads using technology. We will also discuss which SageMaker features play
    a role in various aspects of operationalizing ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for operationalizing ML workloads on AWS in
    the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To successfully operationalize the end-to-end ML system, you must first ensure
    its reproducibility through versioned data, code, and artifacts. Best practice
    is to version all inputs used to create models, including training data, data
    preparation code, algorithm implementation code, parameters, and hyperparameters,
    in addition to all trained model artifacts. A versioning strategy is also about
    helping in the model-update phase and allowing for easy rollback to a specific
    known working version if a model update fails or if the updated model does not
    meet your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking ML artifacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Iterative development of ML models using different algorithms and hyperparameters
    for each algorithm results in many training experiments and multiple model versions.
    Keeping track of these experiments and resulting models along with each model's
    lineage is important to meet auditing and compliance requirements. Model lineage
    also helps with root-cause analysis in case of degrading model performance.
  prefs: []
  type: TYPE_NORMAL
- en: While you can certainly build a custom tracking solution, best practice is to
    use a managed service such as SageMaker Experiments. Experiments allows you to
    track, organize, visualize, and compare ML models across all phases of the ML
    lifecycle including feature engineering, model training, model tuning, and model
    deployment. With SageMaker Experiments, you can easily choose to deploy or update
    the model with a specific version. Experiments also provides you with the model
    lineage capability. For a detailed discussion of SageMaker Experiments' capabilities,
    please refer to the *Amazon SageMaker Experiments* section of [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)*,
    Training and Tuning at Scale*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can also use the Amazon SageMaker ML Lineage Tracking capability,
    which keeps track of information about the individual steps of an ML workflow
    from data preparation to model deployment. With the information tracked, you can
    reproduce the workflow steps, track model and dataset lineage, and establish model
    governance and audit standards.
  prefs: []
  type: TYPE_NORMAL
- en: Automating deployment pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated pipelines minimize human intervention in moving a trained ML model
    from lower-level environments such as development and staging into a production
    environment. The aim is to have a codified deployment pipeline created with Infrastructure-as-Code
    and Configuration-as-Code, with manual and automated quality gates incorporated
    into the pipeline. Manual quality gates can ensure that models are promoted to
    the production environment only after ensuring that there are no operational concerns
    such as security exposure. Automated quality gates, on the other hand, can be
    used to evaluate model metrics such as precision, recall, or accuracy. Pipelines
    result in consistent deployment as well as providing the ability to reliably recreate
    ML-related resources across multiple environments with minimal human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon SageMaker Pipelines, you can build automated model workflows. You
    can build every step of the ML lifecycle as a pipeline step to develop and deploy
    models and monitor the pipelines. You can further manage dependencies between
    each step, build the correct sequence, and execute the steps automatically. A
    service that brings in CI/CD practices to ML workloads is SageMaker Projects.
    This service helps you move models from concept to production. Additionally, you
    can easily meet governance and audit standards using a combination of SageMaker
    Projects and SageMaker Pipelines, by automatically tracking code, datasets, and
    model versions through each step of the ML lifecycle. This enables you to go back
    and replay model-generation steps, troubleshoot problems, and reliably track the
    lineage of models at scale. For a detailed discussion of automated workflows and
    MLOps, please refer to [*Chapter 12*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*,
    Machine Learning Automated Workflows*.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring production models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continued monitoring of deployed models is a critical step in operationalizing
    ML workloads, since a model's performance and effectiveness may degrade over time.
    Ensuring that the model continues to meet your business needs starts with the
    identification of the metrics that measure both model-related metrics and business
    metrics. Ensure that all metrics critical to model evaluation against your business
    KPIs are defined early on and collected during monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Once the metrics are identified, to ensure the continued high quality of the
    deployed model, use the Amazon SageMaker Model Monitor capabilities and its integration
    with CloudWatch to proactively detect issues, raise alerts, and automate remediation
    actions. In addition to detecting model-quality degradation, you can monitor data
    drift, bias drift, and feature attribution drift to meet your reliability, regulatory,
    and model explainability requirements.
  prefs: []
  type: TYPE_NORMAL
- en: CloudWatch alerts that have been triggered because of model monitoring can be
    used to automate activities such as invalidating the current model, reverting
    to an older model version, or retraining a new model based on new ground truth
    data. Updates to production models should consider trade-offs between the risk
    of introducing changes, the cost of retraining, and the potential value of having
    a newer model in production. For a detailed discussion of model monitoring, please
    refer to [*Chapter 11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring
    Production Models with Amazon SageMaker Model Monitor and Clarify*.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While this section has focused on SageMaker-native approaches for operationalizing
    ML workloads, please note that similar automated pipelines can be built using
    a combination of SageMaker APIs and other AWS services such as CodePipeline, Step
    Functions, Lambda, and SageMaker Data Science SDK. Multiple MLOps architectures
    are documented along with sample code at [https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml](https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various AWS services and features applicable
    to operationalizing ML workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – AWS Services used for operationalizing ML workloads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – AWS Services used for operationalizing ML workloads
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to enable secure ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for securing ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When securing an ML workload, you should take into consideration infrastructure
    and network security, authentication and authorization, encrypting data and model
    artifacts, logging and auditing, and meeting regulatory requirements. In this
    section, we will discuss best practices for security ML workloads using a combination
    of SageMaker and related AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for securing ML workloads on AWS in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating the ML environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build secure ML workloads, you need an isolated compute and network environment.
    To achieve this for ML on SageMaker, deploy all resources such as notebooks, studio
    domain, training jobs, processing jobs, and endpoints within a **Virtual** **Private**
    **Cloud** (**VPC**). A VPC provides an isolated environment where all traffic
    between various SageMaker components flows within the network. You can add another
    layer of isolation by using security groups that include rules for both inbound
    and outbound traffic allowed by subnets within the VPC, thereby isolating your
    ML resources further.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you use SageMaker without a VPC, all resources run in an environment
    managed by AWS on single-tenancy EC2 instances, which ensures that your ML environments
    are isolated from other customers. However, deploying ML resources, such as training
    containers, in a VPC allows you to monitor all network traffic in and out of these
    resources using VPC Flow Logs. Additionally, you can use VPC endpoints and AWS
    PrivateLink to enable communication between SageMaker and other AWS services such
    as S3 or CloudWatch. This keeps all traffic flowing between the various services
    within the AWS network without exposing the traffic to the public internet.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling internet and root access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, SageMaker notebook instances are internet-enabled to allow you to
    download external libraries and customize your working environment. Additionally,
    root access is enabled on these notebooks, giving you the flexibility to leverage
    external libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Only use these default settings in a lower-level sandbox and development environments
    to figure out the optimal working notebook environment. In all other non-production
    and production environments, launch SageMaker resources in your own VPC and turn
    off root access to prevent downloading and installing unauthorized software. Import
    all necessary libraries into a private repository such as AWS CodeArtifact before
    you isolate your environment. This allows you to seamlessly download specific
    versions of libraries without having to reach out to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, use codified lifecycle configurations to automate setting up the
    notebook environment. Similarly, training and deployed inference containers managed
    by SageMaker are internet-enabled by default. When launching training and inference
    resources, use `VPCConfig` and `EnableNetworkIsolation` flags to protect these
    resources from external network traffic. In this case, all downloads and uploads
    of data and model artifacts are routed through your VPC. At the same time, the
    training and inference containers remain isolated from the network and do not
    have access to any resource within your VPC or on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing authentication and authorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implement a strong mechanism to determine who can access the ML resources (authentication)
    and what resources authenticated users can access (authorization). SageMaker is
    natively integrated with AWS IAM, a service used to manage access to all AWS services
    and resources. IAM allows you to define fine-grained access controls using IAM
    users, groups, roles, and policies. You can implement least-privilege access using
    a combination of identity-based policies to specify what an IAM user, role, or
    group can do and resource-based policies to specify who has access to the resource
    and what actions they can perform on it.
  prefs: []
  type: TYPE_NORMAL
- en: When designing these IAM policies, it is tempting to start with wide-open IAM
    policies with good intentions of tightening them as you go. However, best practice
    is to start with tight policies that grant minimal required access and add additional
    permissions when required. Periodically review and refine policies to ensure that
    no unnecessary permissions are granted. The IAM service provides the Access Advisor
    capability, which shows you when various AWS services are last accessed by different
    entities such as IAM groups, users, roles, and policies. Use this information
    to refine the policies. All the service API calls are also logged by CloudTrail,
    and you can use the CloudTrail history to determine which permissions can be removed
    based on the usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Securing data and model artifacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IAM policies can also be used for access-control of data and models in S3\.
    Additionally, you can use a security service called Amazon Macie to protect and
    classify data in S3\. Macie internally uses ML to automatically discover, classify,
    and protect sensitive data. It automatically recognizes sensitive data such as
    **personally** **identifiable** **information** (**PII**) or **intellectual**
    **property** (**IP**), providing visibility into data access and movement patterns.
    Macie continuously monitors for anomalies in data-access patterns and proactively
    generates alerts on unauthorized access and data leaks.
  prefs: []
  type: TYPE_NORMAL
- en: The next important aspects to secure are data and model artifacts of an ML system,
    both at rest and in transit. To secure data in transit within a VPC, use **Transport**
    **Layer** **Security** (**TLS**). To secure data at rest, best practice is to
    use encryption to block malicious actors from reading your data and model artifacts.
    You can use either client-side or server-side encryption. SageMaker comes with
    built-in encryption capabilities to protect training data and model artifacts
    both at rest and in transit. For example, when launching a training job, you can
    specify the encryption key to be used. You have the flexibility of using SageMaker-managed
    keys, AWS-managed keys, or your own customer-managed keys.
  prefs: []
  type: TYPE_NORMAL
- en: Logging, monitoring, and auditing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker is natively integrated with CloudWatch and CloudTrail. You can capture
    logs from SageMaker training, processing, and inference in CloudWatch, which can
    further be used for troubleshooting. All SageMaker (and other AWS services) API
    calls are logged by CloudTrail, allowing you to track down which IAM user, AWS
    account, or source IP address made the API call along with when the call occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Meeting regulatory requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For many organizations, ML solutions need to comply with regulatory standards
    and pass compliance certifications that vary significantly across countries and
    industries. Amazon SageMaker complies with a wide range of compliance programs,
    including PCI, HIPAA, SOC 1/2/3, FedRAMP, and ISO 9001/27001/27017/27018.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various AWS services applicable to securing
    ML workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – AWS services used for securing ML workloads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – AWS services used for securing ML workloads
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build reliable ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for reliable ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a reliable system, there are two considerations at the core:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the ability to recover from planned and unplanned disruptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, the ability to meet unpredictable increases in traffic demands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, the system should achieve both without affecting downstream applications
    and end consumers. In this section, we will discuss best practices for building
    reliable ML workloads using a combination of SageMaker and related AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at some best practices for securing ML workloads on AWS in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering from failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For an ML workload, the ability to recover gracefully should be part of all
    the steps that make up the iterative ML process. A failure can occur with data
    storage, data processing, model training, or model hosting, which may result from
    a variety of events ranging from system failure to human error.
  prefs: []
  type: TYPE_NORMAL
- en: For ML on SageMaker, all data (and model artifacts) is typically saved in S3\.
    This ensures decoupling between ML data and the computation processing. To prevent
    an inadvertent loss of data, best practice is to use a combination of IAM and
    S3 policies to ensure least privilege-based access to data. Additionally, use
    S3 versioning and object tagging to enable versioning and traceability of data
    (and model artifacts) for easy recovery or recreation in the event of failure.
  prefs: []
  type: TYPE_NORMAL
- en: Next, consider the reliability of ML training, which is often a long, time-consuming
    process. It is not uncommon to see training jobs that run over multiple hours
    and even multiple days. If these long-running training jobs are disrupted due
    to a power outage, OS fault, or other unexpected error, having the ability to
    reliably resume from where the job stopped is critical. ML checkpointing should
    be used in this situation. On SageMaker, a few built-in algorithms and all supported
    deep learning frameworks provide the capability of turning on checkpointing when
    a training job is launched. When you enable checkpointing, SageMaker automatically
    saves snapshots of the model state during training. This enables you to reliably
    restart a training job from the last saved checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model origin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's say your training goes off without a hitch and you have a trained model
    artifact saved in an S3 bucket. What happens if you lose this model artifact due
    to human error, such as someone in your team deleting it by mistake? In a reliable
    ML system, you need to be able to recreate this model using the same data, version
    of the code, and parameters as the original model. Hence, it is important to keep
    track of all these aspects during training. Using SageMaker Experiments, you can
    keep track of all the steps and artifacts that went into creating a model so you
    can easily recreate the model as necessary. Another benefit of tracking with SageMaker
    Experiments is the ability to troubleshoot issues in production for reliable operation.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to relying on Experiments to be able to recreate a specific version
    of a model artifact, use a combination of IAM and S3 policies to ensure least
    privilege-based access to minimize the risk of accidental model-artifact deletion.
    Implement measures such as requiring MFA for model artifact deletion and storing
    a secondary copy of the artifact as required by your organization's disaster recovery
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Automating deployment pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To ensure that all steps leading up to model deployment are executed consistently,
    use a CI/CD pipeline with access controls to enforce least privilege-based access.
    Deployment automation combined with manual and automated quality gates ensures
    that all changes can be effectively validated with dependent systems prior to
    deployment. Amazon SageMaker Pipelines has the capability to bring CI/CD practices
    to ML workloads for improved reliability. Codifying the CI/CD pipelines using
    SageMaker Pipelines provides you with an additional capability of dealing with
    the model endpoint itself being deleted inadvertently. Using the Infrastructure-as-Code
    approach, the endpoint can be recreated. This requires a well-defined versioning
    strategy in place for your data, code, algorithms, hyperparameters, model artifacts,
    container images, and more. Version everything and document your versioning strategy.
    For a detailed discussion of SageMaker Pipelines capabilities, please refer to
    the *Amazon SageMaker Pipelines* section of [*Chapter 12*](B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222)*,
    Machine Learning Automated Workflows*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, follow the *train once and deploy everywhere* strategy. Because
    of the decoupled nature of the training process and results, you can share the
    trained model artifact across multiple environments. This prevents retraining
    in multiple environments and introducing unexpected changes to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Handling unexpected traffic patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is deployed, you must ensure the reliability of the deployed
    model in serving the inference requests. The model should be able to handle spikes
    in inference traffic and continue to operate at the quality necessary to meet
    the business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: To handle traffic spikes, deploy the model with the Autoscaling-enabled SageMaker
    real-time endpoint. With Autoscaling enabled, SageMaker automatically increases
    (and decreases) the computation capacity behind the hosted model in response to
    the dynamic shifts in the inference traffic. Autoscaling provided by SageMaker
    is horizontal scaling, meaning it adds new instances or removes existing instances
    to handle the inference traffic variations.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous monitoring of deployed model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To ensure the continued high quality of the deployed model, use the Amazon SageMaker
    Model Monitor capabilities and its integration with CloudWatch to proactively
    detect issues, raise alerts, and automate remediation actions when a production
    model is not performing as expected. In addition to model quality, you can monitor
    data drift, bias drift, and feature-attribution drift to meet your reliability,
    regulatory, and model explainability requirements. Ensure that all metrics critical
    to model evaluation against your business KPIs are defined and monitored. For
    a detailed discussion of model monitoring, please refer to [*Chapter 11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*,
    Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify*.
  prefs: []
  type: TYPE_NORMAL
- en: Updating model with new versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, you must consider how to update a production model reliably. SageMaker
    endpoint production variants can be used to implement multiple deployment strategies
    such as A/B, Blue/Green, Canary, and Shadow deployments. The advanced deployment
    strategies along with detailed implementation are discussed in [*Chapter 9*](B17249_09_Final_JM_ePub.xhtml#_idTextAnchor163)*,
    Updating Production Models Using Amazon SageMaker Endpoint Production Variants*.
    Depending on the model consumer's tolerance for risk and downtime, choose an appropriate
    deployment strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various AWS services applicable to building
    reliable ML workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – AWS service capabilities used for reliable ML workloads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – AWS service capabilities used for reliable ML workloads
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build reliable, performance-efficient workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for building performant ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the compute- and time-intensive nature of ML workloads, it is important
    to choose the most performant resources appropriate for each individual phase
    of the workload. Computation, memory, and network bandwidth requirements are unique
    to each phase of the ML process. Besides the performance of the infrastructure,
    the performance of the model as measured by metrics such as accuracy is also important.
    In this section, we will discuss best practices to apply in selecting the most
    performant resources for building ML workloads on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for building performant ML workloads on AWS
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Rightsizing ML resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker supports a variety of ML instance types with a varying combination
    of CPU, GPU, FPGA, memory, storage, and networking capacity. Each instance type,
    in turn, supports multiple instance sizes. So, you have a range of choices to
    choose from to suit your specific workload. The best practice is to choose different
    compute resource configurations for data processing, building, training, and hosting
    your ML model. This is made possible by the decoupled nature of SageMaker, which
    allows you to choose different instance types and sizes for different APIs. For
    example, you can choose `ml.c5.medium` for a notebook instance as your working
    environment, use a cluster of four `ml.p3.large` GPU instances for training, and
    finally host the trained model on two `ml.m5.4xlarge` instances with Elastic Inference
    attached. Additionally, in the SageMaker Studio environment, you can change the
    notebook instance type seamlessly without any interruption to your work.
  prefs: []
  type: TYPE_NORMAL
- en: While you have the flexibility of choosing different compute options for different
    ML phases, how do you choose the specific instance types and sizes to use? This
    comes down to understanding your workload and experimentation. For example, if
    you know that the training framework and algorithm of your choice will need GPU
    support, choose a GPU cluster to train on. While it may be tempting to use GPUs
    for all training, traditional algorithms may not work well on GPUs due to the
    communication overheads involved. Some built-in algorithms, such as XGBoost, implement
    an open source algorithm that has been optimized for CPU computations. SageMaker
    also provides optimized versions of frameworks, such as TensorFlow and PyTorch,
    which include optimizations for high-performance training across Amazon EC2 instance
    families.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring resource utilization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you make your initial choice of instances and kick off training, SageMaker
    training jobs emit CloudWatch metrics for resource utilization that you can use
    to improve your training runs the next time. Additionally, when you enable Debugger
    with your training jobs, SageMaker Debugger provides visibility into training
    jobs and the infrastructure a training job is executing on. Debugger also monitors
    and reports on the system resources such as CPU, GPU, and memory, providing you
    with insights into resource underutilization and bottlenecks. If you use TensorFlow
    or PyTorch for your deep learning training jobs, Debugger provides you with a
    view into framework metrics that can be used to speed up your training jobs. For
    a detailed discussion of Debugger's capabilities, please refer to [*Chapter 7*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*,*
    *Profile Training Jobs with Amazon SageMaker Debugger.*
  prefs: []
  type: TYPE_NORMAL
- en: Rightsizing hosting infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is trained and ready to be deployed to choose instances for real-time
    endpoints, consider what your target performance is. Target performance is a combination
    of how many requests to serve in each period and the desired latency for each
    request, for example, 10,000 requests per minute with a maximum of a 1 millisecond
    response time. Once you have the target performance in mind, perform load testing
    in a non-production environment to figure out the instance type, instance size,
    and number of instances to host the model on. Recommended best practice is to
    deploy the endpoint with at least two instances across two availability zones
    for high availability.
  prefs: []
  type: TYPE_NORMAL
- en: Once you decide on the instance type to use, start with the minimum number of
    instances necessary to meet your steady-state traffic and take advantage of the
    Autoscaling capability of SageMaker hosting. Using Autoscaling, SageMaker can
    automatically scale the inference capacity depending on the utilization and request
    traffic thresholds you configure. Capacity adjustments to meet your performance
    requirements are done by updating the endpoint configuration without any downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can scale up the hosting infrastructure for deep learning
    models using Amazon `Inf1` instances, which are best suited to applications such
    as search, recommendation engines, and computer vision, at a low cost.
  prefs: []
  type: TYPE_NORMAL
- en: While real-time endpoints provide access to models deployed on SageMaker, some
    workloads may warrant inference at the edge due to latency requirements, for example,
    models used to determine defective product parts in a manufacturing plant. In
    such cases, the model needs to be deployed on cameras within the manufacturing
    plant. For such use cases, use SageMaker Neo and SageMaker Edge Manager to optimize,
    deploy, and manage models at the edge.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While real-time endpoints and models deployed at the edge provide synchronous
    predictions, batch transform is used for asynchronous inferences with more tolerance
    for longer response times. Use experimentation to determine the right instance
    type, size, and number of instances to be used for batch transform with job completion
    time in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous monitoring of deployed model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is actively serving inference traffic, use SageMaker Model Monitor
    to continuously monitor ML models for data drift, model-quality performance, feature-importance
    drift, and bias drift. Behind the scenes, Model Monitor uses distributed processing
    jobs. As with batch processing, use experimentation and load testing to determine
    the processing job resources necessary to complete each scheduled monitoring job
    execution. For a detailed discussion of Model Monitor, please refer to [*Chapter
    11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring Production Models
    with Amazon SageMaker Model Monitor and Clarify*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various SageMaker features and how they
    are applicable for building performant ML workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – AWS service capabilities for building performant ML workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – AWS service capabilities for building performant ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how SageMaker integrates with other AWS
    services to build cost-optimized workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for cost-optimized ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many organizations, the lost opportunity cost of not embracing disruptive
    technologies such as ML outweighs the ML costs. By implementing a few best practices,
    these organizations can get the best possible returns on their ML investment.
    In this section, we will discuss best practices to apply for cost-optimized ML
    workloads on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at best practices for building cost-optimized ML workloads on
    AWS in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing data labeling costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Labeling of data used for ML training, typically done at the very beginning
    of the ML process, can be tedious, error-prone, and time-consuming. Labeling at
    scale consumes many working hours, making this an expensive task, too. To optimize
    cost for data labeling, use SageMaker Ground Truth. Ground Truth provides capabilities
    for data labeling at scale using a combination of human workforce and active learning.
    When active learning is enabled, a labeling task is routed to humans only if a
    model cannot confidently finish the task. The human-labeled data is then used
    to train the model to improve accuracy. Therefore, as the labeling job progresses,
    less and less data needs to be labeled by humans. This results in faster completion
    of the job at reduced costs. For a detailed discussion of Ground Truth capabilities,
    please refer to [*Chapter 3*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*,
    Data Labeling with Amazon SageMaker Ground Truth*.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing experimentation costs with models from AWS Marketplace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML is inherently iterative and experimental. Having to run multiple algorithms
    with different sets of hyperparameters each time leads to several training jobs
    before you can determine a model that meets your needs. All this training adds
    up in terms of time and costs.
  prefs: []
  type: TYPE_NORMAL
- en: A big part of experimentation is the research and reuse of readily available
    pre-trained models that may suit your needs. AWS Marketplace for ML gives you
    a catalog of datasets and models made available by vendors vetted by AWS. You
    can subscribe to models that meet your needs and potentially save the time and
    costs involved in developing your own models. If you do, however, end up developing
    your own models, you can use the marketplace to monetize your models by making
    them available to others.
  prefs: []
  type: TYPE_NORMAL
- en: Using AutoML to reduce experimentation time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the marketplace models don't meet your needs or if your organization has
    the **build rather than buy** policy, first check whether your dataset and use
    case are suitable for AutoPilot. At the time of writing this book, AutoPilot supports
    tabular data and classification and regression problems. AutoPilot automatically
    analyzes datasets and builds multiple models with different combinations of algorithms
    and hyperparameters and finally selects the best algorithm for the list. This
    saves both time and cost. Additionally, the service provides transparency through
    two notebooks – a data preparation notebook and a model candidate selection notebook,
    which details all the behind-the-scenes steps performed by AutoPilot. So, even
    if you don't end up using the model built and recommended by AutoPilot, you can
    use these notebooks as a starting point for your own experimentation and modify
    them using your business domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: However, at the time of publication of this book, AutoPilot only supports regression
    and classification using tabular data. For other data types and problems, you
    will have to build and train your model.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating locally with small datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During ML experimentation, iterate with a smaller dataset in the SageMaker notebook's
    local environment first. Once you iron out details such as code bugs and data
    issues, you can scale up with the full dataset and distributed training clusters
    managed by SageMaker. This phased approach will let you iterate faster at lower
    costs. SageMaker SDK makes this easy by supporting `instance-type = "local"` for
    the training API so that you can reuse the same code in the local environment
    or on the distributed cluster. Note that at the time of publication, local mode
    only works in SageMaker notebook instances, not in the Studio environment.
  prefs: []
  type: TYPE_NORMAL
- en: Rightsizing training infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are ready to launch a distributed training cluster, it is important
    to choose the right number and type of instances in the cluster. For built-in
    or custom algorithms that do not support distributed training, your cluster will
    always have a single instance. For algorithms and frameworks that do support distributed
    training, take advantage of data parallelism and model parallelism as discussed
    in [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117)*, Training and
    Tuning at Scale,* to complete training faster, thereby reducing the overall training
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: While there are various instance types with different capacity configurations
    available, it is important to rightsize the training instances based on the ML
    algorithm used. For example, simple algorithms may not train faster on the larger
    instance types since they cannot take advantage of hardware parallelism. Even
    worse, they may even train slower due to high GPU communication overhead. Best
    practice for cost optimization is to start with a smaller instance, scale out
    first by adding more instances to the training cluster, and then scale up to more
    powerful instances. However, if you are using a deep learning framework and distributed
    training, best practice would be to scale up to more GPUs/CPUs on a single instance
    before scaling out because the network I/O involved may negatively impact the
    training performance.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to selecting the right infrastructure, you can also use optimized
    versions of ML frameworks that result in faster training. SageMaker provides optimized
    versions of multiple open source ML frameworks including TensorFlow, Chainer,
    Keras, and Theano. SageMaker versions of these popular frameworks are optimized
    for high performance on all SageMaker ML instances.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing hyperparameter-tuning costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperparameter tuning is also an expensive task, using sophisticated search
    and algorithms. Best practice is to rely on the automated model tuning capability
    provided by managed SageMaker Automatic Model Tuning, also known as **hyperparameter
    tuning** (**HPT**). Automatic model tuning finds the best version of a model by
    running multiple training jobs using the algorithm and hyperparameter ranges specified
    by you. HPT then chooses the hyperparameter values that result in the best model
    as measured by the objective metric you specify. Behind the scenes, HPT uses ML
    techniques that can determine optimal hyperparameters with a limited number of
    training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: You can further speed up the HPT jobs using warm start mode. With warm start,
    you no longer must start an HPT job from scratch; instead, you can create a new
    HPT job based on one or more parent jobs. This allows you to reuse the training
    jobs conducted in the parent jobs as prior knowledge. Warm start allows you to
    reduce the time and cost associated with model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Saving training costs with Managed Spot Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Managed Spot Training applies the cost-saving construct of Spot Instances
    and applies it to hyperparameter tuning and training. The Managed Spot Training
    capability takes advantage of checkpointing, to resume training jobs easily. Since
    you don't have to run the training from the start again, this reduces your overall
    training costs.
  prefs: []
  type: TYPE_NORMAL
- en: Using insights and recommendations from Debugger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to deep learning on SageMaker, training with GPU is very powerful,
    but training costs can add up quickly. SageMaker Debugger provides insight into
    deep learning training both into the ML framework in use and the underlying compute
    resources. The deep profiler capability provides you with recommendations that
    you can implement to improve training performance and reduce resource wastage.
    For a detailed discussion of Debugger's capabilities, please refer to [*Chapter
    7*](B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133)*,* *Profile Training Jobs
    with Amazon SageMaker Debugger*.
  prefs: []
  type: TYPE_NORMAL
- en: Saving ML infrastructure costs with SavingsPlan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you enable SavingsPlan in your AWS account, it analyzes your ML resource
    usage within a time of your choice – the past 7, 30, or up to 60 days. The service
    then recommends the right plan to use to optimize costs. You can also select a
    pre-payment option from three different options: no upfront costs, partial upfront
    (50% or more), or all upfront. Once you configure these options, SavingsPlan provides
    you with details of how your monthly spend can be optimized. Additionally, it
    also suggests an hourly usage commitment that maximizes your savings. The plans
    cover all ML instance families, notebook instances, Studio instances, training
    instances, batch transform instances, real-time endpoint instances, Data Wrangler
    instances, and SageMaker Processing instances, thereby helping to optimize costs
    across various phases of ML workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: While Managed Spot Training and SavingsPlan are both cost-saving approaches,
    they are not meant to be combined. With SavingsPlan, you are billed every hour
    of the commitment regardless of whether it is fully used. Best practice is to
    use SavingsPlan and Managed Spot Training usages separately. For example, use
    SavingsPlan for predictable steady-state recurring training workloads and Managed
    Spot Training for new training workloads and prototyping where you do not have
    a clear idea of monthly costs yet.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing inference costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inference costs typically make up most ML costs. Inference costs are discussed
    in detail in [*Chapter 10*](B17249_10_Final_JM_ePub.xhtml#_idTextAnchor179)*,
    Optimizing Model Hosting and Inference Costs*, which details several ways to improve
    inference performance while reducing inference costs. These methods include using
    batch inference where possible, deploying several models behind a single inference
    endpoint to reduce cost and help with advanced canary or blue/green deployments,
    scaling inference endpoints to meet demand, and using EI and SageMaker Neo to
    provide better inference performance at a lower cost.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping or terminating resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you terminate or at least stop the ML resources once you are done.
    While the instances for training, hyperparameter tuning, batch inferences, and
    processing jobs will be managed and automatically deleted by SageMaker, you are
    responsible for notebook instances, endpoint, and monitoring schedules. Stop or
    delete these resources to avoid unnecessary costs using automation with scripts
    that stop resources based on idle time or a schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the various SageMaker features and how they
    are applicable for building cost-optimized ML workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – AWS service capabilities for cost-optimized ML workloads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – AWS service capabilities for cost-optimized ML workloads
  prefs: []
  type: TYPE_NORMAL
- en: This section concludes the discussion on applying best practices to build well-architected
    ML workloads on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you reviewed the five pillars – operational excellence, security,
    reliability, performance, and cost optimization – that make up the Well-Architected
    Framework. You then dove into the best practices for each of these pillars, with
    an eye to applying these best practices to ML workloads. You learned how to use
    the SageMaker capabilities with related AWS services to build well-architected
    ML workloads on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: As you architect your ML applications, you typically must make trade-offs between
    the pillars depending on your organization's priorities. For example, when getting
    started with ML, cost-optimization may not be at the top of your mind but establishing
    operational standards may be important. However, as the number of ML workloads
    scale, cost-optimization could become an important consideration. By applying
    the best practices you learned in this chapter, you can architect and implement
    ML applications that meet your organization's needs and periodically evaluate
    your applications against the best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will apply all these best practices and see how to
    operate in multiple AWS environments that reflect the real world.
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS service/feature** | **How you should use it for securing ML** |'
  prefs: []
  type: TYPE_TB
- en: '| IAM | By implementing authentication, authorization, and access control through
    IAM users, groups, roles, and policies. |'
  prefs: []
  type: TYPE_TB
- en: '| IAM access advisor/CloudWatch Event history | By identifying opportunities
    to refine IAM policies. |'
  prefs: []
  type: TYPE_TB
- en: '| CloudWatch/CloudTrail | By collecting logs, implementing monitoring, and
    auditing. |'
  prefs: []
  type: TYPE_TB
- en: '| VPC | By providing infrastructure and network isolation. |'
  prefs: []
  type: TYPE_TB
- en: '| VPC endpoints | By routing traffic through the AWS network and avoiding exposure
    to the public internet. |'
  prefs: []
  type: TYPE_TB
- en: '| Private links | By routing traffic through the AWS network and avoiding exposure
    to the public internet. |'
  prefs: []
  type: TYPE_TB
