- en: Chapter 5. Recommendation Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommendation systems find their natural application whenever a user is exposed
    to a wide choice of products or services that they cannot evaluate in a reasonable
    timeframe. These engines are an important part of an e-commerce business because
    they assist the clients on the web to facilitate the task of deciding the appropriate
    items to buy or choose over a large number of candidates not relevant to the end
    user. Typical examples are Amazon, Netflix, eBay, and Google Play stores that
    suggest each user the items they may like to buy using the historical data they
    have collected. Different techniques have been developed in the past 20 years
    and we will focus on the most important (and employed) methods used in the industry
    to date, specifying the advantages and disadvantages that characterize each of
    these methods. The recommendation systems are classified in **Content-based Filtering**
    ( **CBF** ) and **Collaborative Filtering** ( **CF** ) techniques and other different
    approaches (association rules, the log-likelihood method, and hybrid methods)
    will be discussed together with different ways to evaluate their accuracy. The
    methods will be tested on the MovieLens database (from [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)
    ) consisting of 100,000 movie ratings (1 to 5 values) from 943 users on 1,682
    movies. Each user has at least 20 ratings and each movie has a list of genres
    that it belongs to. All the codes shown in this chapter are available, as usual,
    at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_5](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_5)
    in the `rec_sys_methods.ipynb` file.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by introducing the main matrix used to arrange the dataset employed
    by the recommendation system and the metric measures typically used before starting
    to discuss the algorithms in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Utility matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data used in a recommendation system is divided in two categories: the
    users and the items. Each user likes certain items, and the rating value *r[ij]*
    (from 1 to 5) is the data associated with each user *i* and item *j* and represents
    how much the user appreciates the item. These rating values are collected in matrix,
    called utility matrix *R* , in which each row *i* represents the list of rated
    items for user *i* while each column *j* lists all the users who have rated item
    *j* . In our case, the data folder `ml-100k` contains a file called `u.data` (and
    also `u.item` with the list of movie titles) that has been converted into a Pandas
    DataFrame (and saved into a `csv, utilitymatrix.csv` ) by the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utility matrix](img/Image00452.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the first two lines is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utility matrix](img/Image00453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each column name, apart from the first (which is the user id), defines the
    name of the movie and the ID of the movie in the MovieLens database (separated
    by a semicolon). The `0` values represent the missing values and we expect to
    have a large number of them because the users evaluated far fewer than 1,600 movies.
    Note that the movies with less than 50 ratings have been removed from the utility
    matrix, so the number of columns is 604 (603 movies rated more than 50 times).
    The goal of the recommendation system is to predict these values, but for some
    techniques to work properly it will be necessary for us to initially set these
    values (imputation). Usually, two imputation approaches are used: ratings average
    per user or ratings average per item, and both of them are implemented in the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utility matrix](img/Image00454.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This function will be called by many of the algorithms implemented in this chapter,
    so we decided to discuss it here as a reference for future use. Furthermore, in
    this chapter the utility matrix *R* will have dimensions *N* × *M* with *N* number
    of users and *M* number of items. Due to the recurrent use of the similarity measures
    by different algorithms, we will define the most commonly used definitions hereafter.
  prefs: []
  type: TYPE_NORMAL
- en: Similarities measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to compute similarity *s* between two different vectors *x* and *y*
    , which can be users (rows of utility matrix) or items (columns of utility matrix),
    two measures are typically used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cosine similarity: ![Similarities measures](img/Image00455.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pearson correlation: ![Similarities measures](img/Image00456.jpg) , where *x*
    and *y* are the averages of the two vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the two measures coincide if the average is 0\. We can now start
    discussing the different algorithms, starting from the CF category. The following
    `sim()` function will be used to evaluate the similarity between two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarities measures](img/Image00457.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `SciPy` library has been used to compute both similarities (note that the
    cosine scipy definition is the opposite of what has been defined previously, so
    the value is subtracted from 1).
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Filtering methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This class of methods is based on the idea that any user will like items appreciated
    by other users similar to them. In simple terms, the fundamental hypothesis is
    that a user *A* , who is similar to user *B* , will likely rate an item as *B*
    did rather than in another way. In practice, this concept is implemented by either
    comparing the taste of different user's and inferring the future rating for a
    given user using the most similar users taste (memory-based) or by extracting
    some rating patterns from what the users like (model-based) and trying to predict
    the future rating following these patterns. All these methods require a large
    amount of data to work because the recommendations to a given user rely on how
    many similar users can be found in the data. This problem is called **cold start**
    and it is very well studied in literature, which usually suggests using some hybrid
    method between CF and CBF to overcome the issue. In our MovieLens database example
    we assume we have enough data to avoid the cold start problem. Other common problems
    of CF algorithms are the scalability, because the computation grows with the number
    of users and products (it may be necessary some parallelization technique), and
    the sparsity of the utility matrix due to small number of items that any user
    usually rates (imputation is usually an attempt to handle the problem).
  prefs: []
  type: TYPE_NORMAL
- en: Memory-based Collaborative Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This subclass employs the utility matrix to calculate either the similarity
    between users or items. The methods suffer from scalability and cold start issues,
    but when they are applied to a large or too small utility matrix, they are currently
    used in many commercial systems today. We are going to discuss user-based Collaborative
    Filtering and iteFiased Collaborative Filtering hereafter.
  prefs: []
  type: TYPE_NORMAL
- en: User-based Collaborative Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The approach uses a `k-NN` method (see [Chapter 3](text00024.html#page "Chapter 3. Supervised
    Machine Learning") , *Supervised Machine Learning* ) to find the users whose past
    ratings are similar to the ratings of the chosen user so that their ratings can
    be combined in a weighted average to return the current user's missing ratings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For any given user *i* and item not yet rated *j* :'
  prefs: []
  type: TYPE_NORMAL
- en: Find the *K* that is most similar users that have rate *j* using a similarity
    metric *s* .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the predicted rating for each item *j* not yet rated by *i* as a weighted
    average over the ratings of the users *K* :![User-based Collaborative Filtering](img/Image00458.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here ![User-based Collaborative Filtering](img/Image00459.jpg) are the average
    ratings for users *i* and *k* to compensate for subjective judgment (some users
    are generous and some are picky) and *s(i* , *k)* is the similarity metric, as
    seen in the previous paragraph. Note that we can even normalize by the spread
    of the ratings per user to compare more homogeneous ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![User-based Collaborative Filtering](img/Image00460.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, σ[i] and σ[k] are the standard deviations of ratings of users *i* and
    *k* .
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm has as an input parameter, the number of neighbors, *K* but usually
    a value between `20` and `50` is sufficient in most applications. The Pearson
    correlation has been found to return better results than cosine similarity, probably
    because the subtraction of the user ratings means that the correlation formula
    makes the users more comparable. The following code is used to predict the missing
    ratings of each user.
  prefs: []
  type: TYPE_NORMAL
- en: The `u_vec` represents the user ratings values from which the most similar other
    users *K* are found by the function `FindKNeighbours` . `CalcRating` just computes
    the predicted rating using the formula discussed earlier (without the spreading
    correction). Note that in case the utility matrix is so sparse that no neighbors
    are found, the mean rating of the user is predicted. It may happen that the predicted
    rating is beyond `5` or below `1` , so in such situations the predicted rating
    is set to `5` or `1` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![User-based Collaborative Filtering](img/Image00461.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Item-based Collaborative Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach is conceptually the same as user-based CF except that the similarity
    is calculated on the items rather than the users. Since most of the time the number
    of users can become much larger than the number of items, this method offers a
    more scalable recommendation system because the items' similarities can be precomputed
    and they will not change much when new users arrive (if the number of users *N*
    is significantly large).
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm for each user *i* and item *j* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the *K* most similar items using a similarity metric *s* that *i* has already
    rated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the predicted rating as a weighted average of the ratings of the *K*
    items:![Item-based Collaborative Filtering](img/Image00462.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the similarity metric may have a negative value, so we need to restrict
    the summation to only positive similarities in order to have meaningful (that
    is, positive) *P[ij]* (the relative ordering of items will be correct anyway if
    we are only interested in the best item to recommend instead of the ratings).
    Even in this case, a *K* value between `20` and `50` is usually fine in most applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is implemented using a class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Item-based Collaborative Filtering](img/Image00463.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The constructor of the class `CF_itembased` calculates the item similarity
    matrix `simmatrix` to use any time we want to evaluate missing ratings for a user
    through the function `CalcRatings` . The function `GetKSimItemsperUser` finds
    *K* : most similar users to the chosen user (given by `u_vec` ) and `CalcRating`
    just implements the weighted average rating calculations discussed previously.
    Note that in case no neighbors are found, the rating is set to the average or
    the item''s ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: Simplest item-based Collaborative Filtering – slope one
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of computing the similarity using the metric discussed previously,
    a very simple but effective method can be used. We can compute a matrix *D* in
    which each entry *d[ij]* is the average difference between the ratings of items
    *i* and *j* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simplest item-based Collaborative Filtering – slope one](img/Image00464.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Simplest item-based Collaborative Filtering – slope one](img/Image00465.jpg)
    is a variable that counts if the user *k* has rated both *i* and *j* items, so
    ![Simplest item-based Collaborative Filtering – slope one](img/Image00466.jpg)
    is the number of users who have rated both *i* and *j* items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the algorithm is as explained in the *Item-based Collaborative Filtering*
    section. For each user *i* and item *j* :'
  prefs: []
  type: TYPE_NORMAL
- en: Find the *K* items with the smallest differences from *j* , ![Simplest item-based
    Collaborative Filtering – slope one](img/Image00467.jpg) (the `*` indicates the
    possible index values, but for simplicity we relabel them from `1` to *K* ).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the predicted rating as a weighted average:![Simplest item-based Collaborative
    Filtering – slope one](img/Image00468.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although this algorithm is much simpler than the other CF algorithms, it often
    matches their accuracy, is computationally less expensive, and is easy to implement.
    The implementation is very similar to the class used for item-based CF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simplest item-based Collaborative Filtering – slope one](img/Image00469.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The only difference is the matrix: now `difmatrix` is used to calculate the
    differences *d(i* , *j)* between items *i* , *j* , as explained earlier, and the
    function `GetKSimItemsperUser` now looks for the smallest `difmatrix` values to
    determine the *K* nearest neighbors. Since it is possible (although unlikely)
    that two items have not been rated by at least one user, `difmatrix` can have
    undefined values that are set to `1000` by default. Note that it is also possible
    that the predicted rating is beyond `5` or below `1` , so in such situations the
    predicted rating must be set to `5` or `1` appropriately.'
  prefs: []
  type: TYPE_NORMAL
- en: Model-based Collaborative Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This class of methods uses the utility matrix to generate a model to extract
    the pattern of how the users rate the items. The pattern model returns the predicted
    ratings, filling or approximating the original matrix (matrix factorization).Various
    models have been studied in the literature and we will discuss particular *matrix
    factorization* algorithms—the **Singular Value Decomposition** ( **SVD** , also
    with expectation maximization), the **Alternating Least Square** ( **ALS** ),
    the **Stochastic Gradient Descent** ( **SGD** ), and the general **Non-negative
    matrix factorization** ( **NMF** ) class of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative least square (ALS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the simplest method to factorize the matrix *R* . Each user and each
    item can be represented in a feature space of dimension *K* so that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative least square (ALS)](img/Image00470.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *P N×K* is the new matrix of users in the feature space, and *Q M×K*
    is the projection of the items in the same space. So the problem is reduced to
    minimize a regularized cost function *J* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative least square (ALS)](img/Image00471.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, λ is the regularization parameter, which is useful to avoid overfitting
    by penalizing the learned parameters and ensuring that the magnitudes of the vectors
    *p[i]* and *[q] ^T [j]* are not too large. The matrix entries *Mc[ij]* are needed
    to check that the pair of user *i* and item *j* are actually rated, so *Mc[ij]*
    is `1` if *r[ij] >0* , and it''s `0` otherwise. Setting the derivatives of *J*
    to `0` for each user vector *p[i]* and item vector *q[j]* , we obtain the following
    two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative least square (ALS)](img/Image00472.jpg)![Alternative least square
    (ALS)](img/Image00473.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here *R[i]* and *Mc[i]* refer to the row *i* of the matrices *R* and *Mc* ,
    and *R[j]* and *Mc[j]* refer to the column *j* of the matrices *Mc* and *R* .
    Alternating the fixing of the matrix *P* , *Q* , the previous equations can be
    solved directly using a least square algorithm and the following function implements
    the ALS algorithm in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative least square (ALS)](img/Image00474.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The matrix *Mc* is called `mask` , the variable `l` represents the regularization
    parameter lambda and is set to `0.001` by default, and the least square problem
    has been solved using the `linalg.solve` function of the `Numpy` library. This
    method usually is less precise than both **Stochastic Gradient Descent** ( **SGD**
    ) and **Singular Value Decomposition** ( **SVD** ) (see the following sections)
    but it is very easy to implement and easy to parallelize (so it can be fast).
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent (SGD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This method also belongs to the matrix factorization subclass because it relies
    on the approximation of the utility matrix *R* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic gradient descent (SGD)](img/Image00475.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the matrices *P(N×K)* and *Q(M×K)* represent the users and the items
    in a latent feature space of *K* dimensions. Each approximated rating ![Stochastic
    gradient descent (SGD)](img/Image00476.jpg) can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic gradient descent (SGD)](img/Image00477.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix ![Stochastic gradient descent (SGD)](img/Image00478.jpg) is found,
    solving the minimization problem of the regularized squared errors *e² [ij] *
    as with the ALS method (cost function *J* as in [Chapter 3](text00024.html#page
    "Chapter 3. Supervised Machine Learning") , *Supervised Machine Learning* ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic gradient descent (SGD)](img/Image00479.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This minimization problem is solved using the gradient descent (see [Chapter
    3](text00024.html#page "Chapter 3. Supervised Machine Learning") , *Supervised
    Machine Learning* ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic gradient descent (SGD)](img/Image00480.jpg)![Stochastic gradient
    descent (SGD)](img/Image00481.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, α is the learning rate (see [Chapter 3](text00024.html#page "Chapter 3. Supervised
    Machine Learning") , *Supervised Machine Learning* ) and ![Stochastic gradient
    descent (SGD)](img/Image00482.jpg) . The technique finds *R* alternating between
    the two previous equations (fixing *q[kj] * and solving *P[ik] * , and vice versa)
    until convergence. SGD is usually easier to parallelize (so it can be faster)
    than SVD (see the following section) but is less precise at finding good ratings.
    The implementation in Python of this method is given by the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic gradient descent (SGD)](img/Image00483.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This SGD function has default parameters that are learning rate *α = 0.0001*
    , regularization parameter *λ = l =0.001* , maximum number of iterations `1000`
    , and convergence tolerance `tol = 0.001` . Note also that the items not rated
    (`0` rating values) are not considered in the computation, so an initial filling
    (imputation) is not necessary when using this method.
  prefs: []
  type: TYPE_NORMAL
- en: Non-negative matrix factorization (NMF)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a group of methods that finds the decomposition of the matrix *R* again
    as a product of two matrices *P* ( *N* × *K* ) and *Q* ( *M* × *K* ) (where *K*
    is a dimension of the feature space), but their elements are required to be non-negative.
    The general minimization problem is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-negative matrix factorization (NMF)](img/Image00484.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, α is a parameter that defines which regularization term to use (`0` squared,
    `1` a lasso regularization, or a mixture of them) and λ is the regularization
    parameter. Several techniques have been developed to solve this problem, such
    as projected gradient, coordinate descent, and non-negativity constrained least
    squares. It is beyond the scope of this book to discuss the details of these techniques,
    but we are going to use the coordinate descent method implemented in `sklearn
    NFM` wrapped in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-negative matrix factorization (NMF)](img/Image00485.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that an imputation may be performed before the actual factorization takes
    place and that the function `fit_transform` returns the *P* matrix while the *Q^T*
    matrix is stored in the `nmf.components_` object. The *α* value is assumed to
    be `0` (squared regularization) and *λ = l =0.01* by default. Since the utility
    matrix has positive values (ratings), this class of methods is certainly a good
    fit to predict these values.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition (SVD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have already discussed this algorithm in [Chapter 2](text00020.html#ch02
    "Chapter 2. Unsupervised Machine Learning") , *Unsupervised Machine Learning*
    , as a dimensionality reduction technique to approximate a matrix by decomposition
    into matrices *U* , Σ, *V* (you should read the related section in [Chapter 2](text00020.html#ch02
    "Chapter 2. Unsupervised Machine Learning") , *Unsupervised Machine Learning*
    , for further technical details). In this case, SVD is used as a matrix factorization
    technique, but an imputation method is required to initially estimate the missing
    data for each user; typically, the average of each utility matrix row (or column)
    or a combination of both (instead of leaving the zero values) is used. Apart from
    directly applying the SVD to the utility matrix, another algorithm that exploits
    an expectation-maximization (see [Chapter 2](text00020.html#ch02 "Chapter 2. Unsupervised
    Machine Learning") , *Unsupervised Machine Learning* ) can be used as follows,
    starting from the matrix ![Singular value decomposition (SVD)](img/Image00486.jpg)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '**m-step** : Perform ![Singular value decomposition (SVD)](img/Image00487.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**e-step** : ![Singular value decomposition (SVD)](img/Image00488.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This procedure is repeated until the sum of squared errors ![Singular value
    decomposition (SVD)](img/Image00489.jpg) is less than a chosen tolerance. The
    code that implements this algorithm and the simple SVD factorization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition (SVD)](img/Image00490.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the SVD is given by the `sklearn` library and both imputation average
    methods (user ratings' average and item ratings' average) have been implemented,
    although the function default is *none* , which means that the zero values are
    left as initial values. For the expect-maximization SVD, the other default parameters
    are the convergence tolerance (0.0001) and the maximum number of iterations (10,000).
    This method (especially with expectation-maximization) is slower than the ALS,
    but the accuracy is generally higher. Also note that the SVD method decomposes
    the utility matrix subtracted by the user ratings' mean since this approach usually
    performs better (the user ratings' mean is then added after the SVD matrix has
    been computed).
  prefs: []
  type: TYPE_NORMAL
- en: We finish remarking that SVD factorization can also be used in memory-based
    CF to compare users or items in the reduced space (matrix *U* or *V^T* ) and then
    the ratings are taken from the original utility matrix (SVD with k-NN approach).
  prefs: []
  type: TYPE_NORMAL
- en: CBF methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This class of method relies on the data that describes the items, which is
    then used to extract the features of the users. In our MovieLens example, each
    movie *j* has a set of *G* binary fields to indicate if it belongs to one of the
    following genres: unknown, action, adventure, animation, children''s, comedy,
    crime, documentary, drama, fantasy, film noir, horror, musical, mystery, romance,
    sci-fi, thriller, war, or western.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these features (genres), each movie is described by a binary vector
    *m[j]* with *G* dimensions (number of movie genres) with entries equal to `1`
    for all the genres contained in movie *j* , or `0` otherwise. Given the `dataframe`
    that stores the utility matrix called `dfout` in the *Utility matrix* section
    mentioned earlier, these binary vectors *m[j]* are collected from the MoviesLens
    `database` into a dataframe using the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CBF methods](img/Image00491.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The movies content matrix has been saved in the `movies_content.csv` file ready
    to be used by the CBF methods.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the content-based recommendation system is to generate the user's
    profile with the same fields to indicate how much the user likes each genre. The
    problem with this method is that the content description of the item is not always
    available, so it is not always possible to employ this technique in the e-commerce
    environment. The advantage is that the recommendations to a specific user are
    independent of the other users' ratings, so it does not suffer from cold start
    problems due to an insufficient number of users' ratings for particular items.
    Two approaches are going to be discussed to find the best recommendation methodologies.
    The first methodology simply generates the user's profile associated with the
    average ratings of the movies seen by each user to each genre and the cosine similarity
    is used to find the movies most similar to the user preferences. The second methodology
    is a regularized linear regression model to generate the user's profile features
    from the ratings and the movie features so that the ratings of the movies not
    yet seen by each user can be predicted using these users' profiles.
  prefs: []
  type: TYPE_NORMAL
- en: Item features average method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The approach is really simple and we are going to explain it using the features
    that describe the movies in the MovieLens example, as discussed previously. The
    objective of the method is to generate the movie genres'' preferences vector ![Item
    features average method](img/Image00492.jpg) for each user *i* (length equal to
    *G* ). This is done by calculating the average rating ![Item features average
    method](img/Image00493.jpg) and each genre entry *g* ; ![Item features average
    method](img/Image00494.jpg) is given by the sum of ratings of the movies seen
    by user *i* ( *Mi* ) containing the genre *g* , minus the average ![Item features
    average method](img/Image00493.jpg) and divided by the number of movies containing
    genre *g* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Item features average method](img/Image00495.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *I[kg]* is 1 if the movie *k* contains genre *g* ; otherwise it is `0`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The vectors ![Item features average method](img/Image00496.jpg) are then compared
    to the binary vectors m *j* using the cosine similarity and the movies with the
    highest similarity values are recommended to the user *i* . The implementation
    of the method is given by the following Python class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Item features average method](img/Image00497.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The constructor stores the list of the movie titles in `Movieslist` and the
    movie features in the `Movies` vector, and the `GetRecMovies` function generates
    the user genres' preferences vector, that is, ![Item features average method](img/Image00496.jpg)
    (applying the preceding formula) called `features_u` , and returns the most similar
    items to this vector.
  prefs: []
  type: TYPE_NORMAL
- en: Regularized linear regression method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The method learns the movie preferences of the users as parameters ![Regularized
    linear regression method](img/Image00498.jpg) of a linear model, with ![Regularized
    linear regression method](img/Image00499.jpg) , where *N* is the number of users
    and *G* is the number of features (movie genres) of each item. We add an intercept
    value on the user parameters *θ[i] (θ[i0] = 1* ) and also the movie vector *m[j]
    * that has the same value *m[j0] =1* , and so ![Regularized linear regression
    method](img/Image00500.jpg) . To learn the vectors of parameters q * [i] * , we
    solve the following regularized minimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularized linear regression method](img/Image00501.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *I[ij]* is `1` ; that is, user *i* watched the movie, otherwise *j* is
    `0` and λ is the regularization parameter (see [Chapter 3](text00024.html#page
    "Chapter 3. Supervised Machine Learning") , *Supervised Machine Learning* ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is given by applying gradient descent (see [Chapter 3](text00024.html#page
    "Chapter 3. Supervised Machine Learning") , *Supervised Machine Learning* ). For
    each user *i* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularized linear regression method](img/Image00502.jpg) (k=0)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Regularized linear regression method](img/Image00503.jpg) (k>0)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Since we are adding `1` entry to the movie and user vectors respectively, the
    distinction between learning the intercept parameter ( *k=0* ) and the others
    is necessary (there is no possibility of overfitting on the intercept, so no need
    to regularize on it). After the parameters q *[i]* are learned, the recommendation
    is performed by simply applying for any missing rating *r[ij]* in the formula
    ![Regularized linear regression method](img/Image00504.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is implemented by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularized linear regression method](img/Image00505.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The constructor of the class `CBF_regression` just performs the gradient descent
    to find the parameters *θ[i]* (called `Pmatrix` ) while the function `CalcRatings`
    finds the most similar rating vector in the stored utility matrix *R* (in case
    the user is not present in the utility matrix) and then it uses the corresponding
    parameters' vector to predict the missing ratings.
  prefs: []
  type: TYPE_NORMAL
- en: Association rules for learning recommendation system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although this method is not used often in many commercial recommendation systems,
    association rules learning is certainly a method worth knowing about because of
    historical data reasons, and it can be employed to solve a wide range of problems
    in real-world examples. The main concept of this method is to find relationships
    among items based on some statistical measure of the occurrences of the items
    in the database of transactions *T* (for example, a transaction could be the movies
    seen by a user *i* or the products bought by *i* ). More formally, a rule could
    be *{item1,item2} => {item3}* , that is, a set of items *({item1,item2})* implies
    the presence of another set *({item3})* . Two definitions are used to characterize
    each *X=>Y* rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support** : Given a set of items *X* , the support *supp(X)* is the portion
    of transactions that contains the set *X* over the total transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidence** : It is the fraction of transactions that contains the set *X*
    that also contains the set *Y: conf(X=>Y)=supp(X U Y)/supp(X)* . Note that the
    confidence *conf(X=>Y)* can have a very different value than *conf(Y=>X)* .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support represents the frequency of a certain rule on the transaction database,
    while the confidence indicates the probability that set *Y* will occur if set
    *X* is present. In other words, the support value is chosen to filter the number
    of rules we want to mine from the database (the higher the support, the fewer
    rules will satisfy the condition), while the confidence can be thought of as a
    *similarity* metric between sets *X* and *Y* . In the case of the movie recommendation
    system, the transaction database can be generated from the utility matrix *R*
    considering the movies each user likes, and we look for rules composed by sets
    *X* and *Y* that contain only one item (movie). These rules are collected in a
    matrix, `ass_matrix` , in which each entry *ass_matrixij* represents the confidence
    of the rule *i =>j* . The recommendations for the given user are obtained by simply
    multiplying the `ass_matrix` by his ratings `u_vec` : ![Association rules for
    learning recommendation system](img/Image00506.jpg) , and sorting all the values
    ![Association rules for learning recommendation system](img/Image00507.jpg) by
    the largest value corresponding to the most recommended movie to the least. Therefore,
    this method does not predict the ratings, but the list of movie recommendations;
    however, it is fast and it also works well with a sparse utility matrix. Note
    that to find all the possible combinations of items to form sets X and Y as fast
    as possible, two algorithms have been developed in the literature: *apriori* and
    *fp-growth* (not discussed here since we only require rules with one item per
    set *X* and *Y* ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class that implements the method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rules for learning recommendation system](img/Image00508.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The class constructor takes as input parameters the utility matrix `Umatrix`
    , the movie titles list `Movieslist` , the support `min_support` , confidence
    `min_confidence` thresholds (default `0.1` ), and the `likethreshold` , which
    is the minimum rating value to consider a movie in a transaction (default `3`
    ). The function `combine_lists` finds all the possible rules, while `filterSet`
    just reduces the rules to the subset that satisfies the minimum support threshold.
    `calc_confidence_matrix` fills the `ass_matrix` with the confidence value that
    satisfies the minimum threshold (otherwise `0` is set by default) and `GetRecItems`
    returns the list of recommended movies given the user ratings `u_vec` .
  prefs: []
  type: TYPE_NORMAL
- en: Log-likelihood ratios recommendation system method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **log-likelihood ratio** ( **LLR** ) is a measure of how two events *A*
    and *B* are unlikely to be independent but occur together more than by chance
    (more than the single event frequency). In other words, the LLR indicates where
    a significant co-occurrence might exist between two events *A* and *B* with a
    frequency higher than a normal distribution (over the two events variables) would
    predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been shown by Ted Dunning ([http://tdunning.blogspot.it/2008/03/surprise-and-coincidence.html](http://tdunning.blogspot.it/2008/03/surprise-and-coincidence.html)
    ) that the LLR can be expressed based on binomial distributions for events A and
    B using a matrix *k* with the following entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | A | Not A |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **B** | *k11* | *k12* |'
  prefs: []
  type: TYPE_TB
- en: '| **Not B** | *k21* | *k22* |'
  prefs: []
  type: TYPE_TB
- en: '![Log-likelihood ratios recommendation system method](img/Image00509.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Log-likelihood ratios recommendation system method](img/Image00510.jpg)
    and ![Log-likelihood ratios recommendation system method](img/Image00511.jpg)
    is the **Shannon** entropy that measures the information contained in the vector
    *p* .
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: ![Log-likelihood ratios recommendation system method](img/Image00512.jpg)
    is also called the **Mutual Information** ( **MI** ) of the two event variables
    *A* and *B* , measuring how the occurrence of the two events depend on each other.'
  prefs: []
  type: TYPE_NORMAL
- en: This test is also called *G2* , and it has been proven effective to detect co-occurrence
    of rare events (especially in text analysis), so it's useful with sparse databases
    (or a utility matrix, in our case).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the events *A* and *B* are the like or dislike of two movies *A*
    and *B* by a user, where the event of *like a movie* is defined when the rating
    is greater than `3` (and vice versa for dislike). Therefore, the implementation
    of the algorithm is given by the following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Log-likelihood ratios recommendation system method](img/Image00513.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The constructor takes as input the utility matrix, the movie titles list, and
    the `likethreshold` that is used to define if a user likes a movie or not (default
    `3` ). The function `loglikelihood_ratio` generates the matrix with all the LLR
    values for each pair of movies *i* and *j* calculating the matrix *k* (`calc_k`
    ) and the corresponding LLR (`calc_llr` ). The function `GetRecItems` returns
    the recommended movie list for the user with ratings given by `u_vec` (the method
    does not predict the rating values).
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a class of methods that combine both CBF and CF in a single recommender
    to achieve better results. Several approaches have been tried and can be summarized
    in the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted** : The CBF and CF predicted ratings are combined in to some weighted
    mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mixed** : CF and CBF predicted movies are found separately and then merged
    in to a single list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Switched** : Based on certain criteria, the CF predictions or CBF predictions
    are used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature combination** : CF and CBF features are considered together to find
    the most similar users or items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature augmentation** : Similar to feature combination, but the additional
    features are used to predict some ratings and then the main recommender uses these
    ratings to produce the recommendation list. For example, Content-Boosted Collaborative
    Filtering learns the ratings of unrated movies by a content-based model and then
    a collaborative approach is employed to define the recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, we implement two hybrid feature combination methods merging
    an item''s features CBF method with a user-based CF method. The first method employs
    a user-based CF to the expanded utility matrix that now also contains the average
    rating per genre per user. The Python class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hybrid recommendation systems](img/Image00514.jpg)![Hybrid recommendation
    systems](img/Image00515.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The constructor generates the expanded utility matrix with the movies' genres
    average rating features associated to each user, `Umatrix_mfeats` . The function
    `CalcRatings` finds the K-NN using the Pearson correlation comparing the expanded
    feature vectors of the users. The second method applies and SVD factorization
    to the expanded utility matrix that contains the genre preferences for each user.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hybrid recommendation systems](img/Image00516.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As the SVD method, the ratings are subtracted with the user rating's average,
    and genre preferences are subtracted from the same user rating's average.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of the recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have discussed all of the most relevant methods used in the commercial environment
    to date. The evaluation of a recommendation system can be executed offline (using
    only the data in the utility matrix) or online (using the utility matrix data
    and the new data provided in real time by each user using the website). The online
    evaluation procedures are discussed in [Chapter 7](text00050.html#page "Chapter 7. Movie
    Recommendation System Web Application") , *Movie Recommendation System Web Application*
    , together with a proper online movie recommendation system website. In this section,
    we will evaluate the performances of the methods using two offline tests often
    used to evaluate recommendation systems: root mean square error on ratings and
    ranking accuracy. For all the evaluations in which k-fold cross-validation (see
    [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning") , *Supervised
    Machine Learning* ) is applicable, a 5-fold cross-validation has been performed
    to obtain more objective results. The utility matrix has been divided in to 5
    folds using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation of the recommendation systems](img/Image00517.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here `df` is a data frame object that stores the utility matrix and *k* is the
    number of folds. In the validation set, for each user ratings' vector `u_vec`
    , half of the ratings have been hidden so that the real value can be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation of the recommendation systems](img/Image00518.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`u_vals` stores the values to predict while `u_test` contains the ratings for
    testing the algorithms. Before we start to compare the different algorithms with
    the different measures, we load the utility matrix and the movie content matrix
    into data frames and split the data into 5 folds for cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation of the recommendation systems](img/Image00519.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`df_vals` contains the validation sets so the `HideRandomRatings` function
    presented in this section needs to be applied.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation of the recommendation systems](img/Image00520.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The data available in the `movies` matrix, the `movieslist` list, and the data
    frames `df_trains` , `vals_vecs_folds` , `tests_vecs_folds` are now ready to be
    used for training and validating all the methods discussed in the previous sections.
    We can start evaluating the **root mean square error** ( **RMSE** ).
  prefs: []
  type: TYPE_NORMAL
- en: Root mean square error (RMSE) evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This validation technique is applicable only on CF methods and linear regression
    CBF since the predicted ratings are generated only by these algorithms. Given
    each rating *rij* in `u_vals` in the validation sets, the predicted rating ![Root
    mean square error (RMSE) evaluation](img/Image00521.jpg) is calculated using each
    method and the root mean square error is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: RMSE = ![Root mean square error (RMSE) evaluation](img/Image00522.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Nval* is the number of ratings in the `u_vals` vectors. The presence
    of the square factor in this formula highly penalizes the large errors, so the
    methods with low RMSE (best values) are characterized by small errors spread over
    all the predicted ratings instead of large errors on few ratings, like the mean
    absolute error MAE=![Root mean square error (RMSE) evaluation](img/Image00523.jpg)
    would prefer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to calculate the RMSE for the memory-based CF user-based and item-based
    methods is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Root mean square error (RMSE) evaluation](img/Image00524.jpg)![Root mean
    square error (RMSE) evaluation](img/Image00525.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For each method, the SE function is called to compute the error for each fold
    and then the total RMSE of the folds is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using 5 nearest-neighbors for item-based CF with slope one and 20 for user-based
    CF, the methods have the following errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | RMSE | Number of Predicted Ratings |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **CF user-based** | 1.01 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| **CF item-based** | 1.03 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| **Slope one** | 1.08 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| **CF-CBF user-based** | 1.01 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: All have similar RMSE values but the best method is item-based Collaborative
    Filtering.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the model-based methods, instead of not hidden validation ratings, `u_test`
    are included in the utility matrix for training and then the RMSE is calculated
    using the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Root mean square error (RMSE) evaluation](img/Image00526.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The code calculates the RMSE only for CBF regression and SVD, and the reader
    can easily replicate the code to calculate the error for the other algorithms
    since most of the required code is just commented (SVD expect-maximization, SGD,
    ALS, and NMF). The results are shown in the following table ( *K* dimension feature
    space):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | RMSE | Number Predicted Ratings |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CBF linear regression(a= 0.01, l =0.0001, its=50) | 1.09 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD ( K=20, 50 its, a =0.00001, l=0.001) | 1.35 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| ALS ( K=20, 50 its, l =0.001) | 2.58 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| SVD (`imputation` =`useraverage` , *K* =20) | 1.02 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| SVD EM (`imputation` =`itemaverage` , iterations=30, *K* =20) | 1.03 | 39,972
    |'
  prefs: []
  type: TYPE_TB
- en: '| HYBRID SVD (`imputation` =`useraverage` , *K* =20) | 1.01 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: '| NMF ( *K* =20 `imputation` =`useraverage` ) | 0.97 | 39,972 |'
  prefs: []
  type: TYPE_TB
- en: As expected, the ALS and SGD are the worst methods but they are discussed because
    they are instructive from a didactic point of view (they are also slow because
    the implementation is not as optimized as the methods from `sklearn` library).
  prefs: []
  type: TYPE_NORMAL
- en: All the others have similar results. However, just note that the hybrid methods
    have slightly better results than the corresponding SVD and CF user-based algorithms.
    Note that the movies to predict are chosen randomly so the results may vary.
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rating error RMSE does not really indicate the quality of a method but
    is an academic measure that is not really used in a commercial environment. The
    goal of a website is to present content that is relevant to the user regardless
    of the exact rating the user gives. In order to evaluate the relevance of the
    recommended items, the `precision` , `recall` , and `f1` (see [Chapter 2](text00020.html#ch02
    "Chapter 2. Unsupervised Machine Learning") , *Unsupervised Machine Learning*
    ) measures are used where the correct predictions are the items with ratings greater
    than 3\. These measures are calculated on the first 50 items returned by each
    algorithm (if the algorithm return a recommended list or the 50 items with the
    highest predicted ratings for the other methods). The function that calculates
    the measures is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification metrics](img/Image00527.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, Boolean `ratingsval` indicates if the method returns ratings or recommended
    list. We use the function `ClassificationMetrics` in the same way we compute the
    RMSE for all the methods, so the actual code to evaluate the measures is not shown
    (you can write it as an exercise). The following table summarizes the results
    for all the methods ( *neighs* is number of nearest-neighbors, *K* dimension feature
    space):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Precision | Recall | f1 | Number of Predicted Ratings |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CF user-based ( *neighs* =20) | 0.6 | 0.18 | 0.26 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| CBFCF user-based ( *neighs* =20) | 0.6 | 0.18 | 0.26 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| HYBRID SVD ( *K* =20, `imputation` =`useraverage` ) | 0.54 | 0.12 | 0.18
    | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| CF item-based ( *neighs* =5) | 0.57 | 0.15 | 0.22 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| Slope one ( *neighs* =5) | 0.57 | 0.17 | 0.24 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| SVD EM ( *K* =20, iterations=30, `imputation` =`useraverage` ) | 0.58 | 0.16
    | 0.24 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| SVD ( *K* =20, `imputation` =`itemaverage` ) | 0.53 | 0.12 | 0.18 | 39,786
    |'
  prefs: []
  type: TYPE_TB
- en: '| CBF regression (a = 0.01, l =0.0001, iterations=50) | 0.54 | 0.13 | 0.2 |
    39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD (K=20, a =0.00001, l =0.001) | 0.52 | 0.12 | 0.18 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| ALS ( *K* =20, λ =0.001, iterations=50) | 0.57 | 0.15 | 0.23 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| CBF average | 0.56 | 0.12 | 0.19 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| LLR | 0.63 | 0.3 | 0.39 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: '| NMF ( *K* =20, λ =0.001, `imputation` =`ssss` ) | 0.53 | 0.13 | 0.19 | 39,786
    |'
  prefs: []
  type: TYPE_TB
- en: '| Association rules | 0.68 | 0.31 | 0.4 | 39,786 |'
  prefs: []
  type: TYPE_TB
- en: From the results you can see that the best method is association rules, and
    there is good precision also for the LLR, hybrid CBFCF user-based, and CF user-based
    methods. Note that the results may vary since the movies to predict have been
    randomly chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the most commonly used recommendation system methods
    from Collaborative Filtering and content-based filtering to two simple hybrid
    algorithms. Note also that in the literature are present *modal* recommendation
    systems in which different data (user gender, demographics, views, locations,
    devices, and so on) are incorporated in to the same algorithm. These methods are
    more advanced and more different data is needed to use them.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](text00050.html#page "Chapter 7. Movie Recommendation System Web
    Application") , *Movie Recommendation System Web Application* , we will implement
    a web recommendation system using the methods discussed in this chapter, but before
    that we will present the Django framework to build web applications in [Chapter
    6](text00046.html#ch06 "Chapter 6. Getting Started with Django") , *Getting Started
    with Django* .
  prefs: []
  type: TYPE_NORMAL
- en: 读累了记得休息一会哦~
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**公众号：古德猫宁李**'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书单分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书友学习交流
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 电子书打包资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 学习资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
