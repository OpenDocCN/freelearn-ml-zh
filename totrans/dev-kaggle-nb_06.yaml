- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Can You Predict Bee Subspecies?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你能预测蜜蜂的亚种吗？
- en: In this chapter, we will learn how to work with image data and start building
    models to classify images. Computer vision’s part in data science and data analysis
    has grown in an exponential way over the years. Some of the most high-profile
    (with a large number of upvotes and forks, e.g., copying and editing) notebooks
    on Kaggle are not **Exploratory Data Analysis** (**EDA**) notebooks or just EDAs
    but, instead, notebooks to build models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何处理图像数据并开始构建用于图像分类的模型。多年来，计算机视觉在数据科学和数据分析中的应用呈指数增长。在 Kaggle 上一些最引人注目的（拥有大量点赞和复制的，例如复制和编辑）笔记本并不是
    **探索性数据分析**（**EDA**）笔记本，而是构建模型的笔记本。
- en: 'In this chapter, we will demonstrate how to use your in-depth data analysis
    to prepare to build a model, and we will also give you some insights into the
    process of iteratively refining a model. It will be not for a competition but,
    instead, for an image dataset. The dataset is the *BeeImage Dataset: Annotated
    Honey Bee Images* (see *Reference 1*). In the previous chapter, we also started
    to use Plotly as a visualization library. In this chapter, we will continue to
    use Plotly for visualization of the dataset features. We grouped a few useful
    functions for visualization with Plotly in a utility script, `plotly-utils` (see
    *Reference 2*). The notebook associated with this chapter is *Honeybee Subspecies
    Classification* (see *Reference 3*).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将演示如何使用您深入的数据分析来准备构建模型，并且我们还将向您介绍模型迭代优化过程的一些见解。这不仅仅是为了比赛，而是为了一个图像数据集。数据集是
    *BeeImage Dataset: Annotated Honey Bee Images*（参见 *参考文献 1*）。在前一章中，我们也开始使用 Plotly
    作为可视化库。在本章中，我们将继续使用 Plotly 来可视化数据集特征。我们将一些有用的可视化函数与 Plotly 一起放在一个实用脚本中，名为 `plotly-utils`（参见
    *参考文献 2*）。与本章相关的笔记本是 *Honeybee Subspecies Classification*（参见 *参考文献 3*）。'
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: 'A comprehensive data exploration of the *BeeImage Dataset: Annotated Honey
    Bee Images*.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对 *BeeImage Dataset: Annotated Honey Bee Images* 的全面数据探索。'
- en: Preparation of a model baseline followed by step-by-step model refinement, analyzing
    the effect of the changes performed on the evolution of train and validation metrics,
    and taking new actions to further improve the model.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在准备模型基线之后，逐步优化模型，分析对训练和验证指标变化的影响，并采取新措施进一步改进模型。
- en: Data exploration
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'The *BeeImage Dataset: Annotated Honey Bee Images* contains one **comma-delimited
    format** (.**csv**) file, `bee_data.csv`, with 5172 rows and 9 columns, along
    with a folder with 5172 images:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*BeeImage Dataset: Annotated Honey Bee Images* 包含一个 **逗号分隔格式** (.**csv**) 文件，`bee_data.csv`，包含
    5172 行和 9 列，以及一个包含 5172 张图片的文件夹：'
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成，置信度低](img/B20963_06_01.png)'
- en: 'Figure 6.1: Sample of the bee_data.csv data file'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：bee_data.csv 数据文件的样本
- en: 'As you can see, the preceding dataframe contains the following columns:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，前面的数据框包含以下列：
- en: '**file**: the image filename'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**file**: 图片文件名'
- en: '**date**: the date when the picture was taken'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**date**: 拍摄图片的日期'
- en: '**time**: the time when the picture was taken'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**time**: 拍摄图片的时间'
- en: '**location**: the US location, with city, state, and country names'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**location**: 美国位置，包括城市、州和国家名称'
- en: '**zip code**: the ZIP code associated with the location'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**zip code**: 与位置相关的邮政编码'
- en: '**subspecies**: the subspecies to whom the bee in the current image belongs'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**subspecies**: 当前图像中蜜蜂所属的亚种'
- en: '**health**: the health state of the bee in the current image'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**health**: 当前图像中蜜蜂的健康状态'
- en: '**pollen_carrying**: indicates whether the picture shows the bee with pollen
    attached to its legs or not'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pollen_carrying**: 表示图片中蜜蜂是否带有花粉附着在其腿上'
- en: '**caste**: the bee’s caste'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**caste**: 蜜蜂的社会阶层'
- en: We will start the data exploration journey with a few quality checks, focusing
    on the `bee_data.csv` file, followed by the images. For the data quality checks,
    we will use one of the utility scripts previously introduced in *Chapter 4*, `data_quality_stats`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始数据探索之旅，进行一些质量检查，重点关注 `bee_data.csv` 文件，然后是图像。对于数据质量检查，我们将使用在 *第 4 章* 中之前介绍的一个实用脚本，`data_quality_stats`。
- en: Data quality checks
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量检查
- en: The dataset does not have any missing values, as you can see in the following
    figure. All the features are of the `string` type.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，数据集没有任何缺失值。所有特征都是 `string` 类型。
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Missing values in the bee_data.csv file. The result was obtained
    using the data_quality_stats functions'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.3*, we show the unique values of the dataset features. The data
    was collected:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: at 6 different dates and 35 different times
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in 8 locations with 7 different ZIP codes
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the data, there are seven subspecies represented with six different health
    problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, line  Description automatically
    generated](img/B20963_06_03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Unique values in the bee_data.csv file. The result was obtained
    using the data_quality_stats functions'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: From the data shown in *Figure 6.4*, 21% of the images are from one single date
    (out of 16 different dates). There was a time when 11% of images were collected.
    There is one single location (Saratoga, California, with the ZIP code 95070) where
    2000 (or 39% of the) images were collected. Italian honeybee is the most frequent
    species. 65% of the images represent healthy bees. Almost all the images show
    bees that do not carry pollen, and all are from the worker caste.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, font, number, line  Description automatically
    generated](img/B20963_06_04.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The most frequent values in the bee_data.csv file. The result was
    obtained using data_quality_stats functions'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will review the image data in parallel with the features in `bee_data.csv`.
    We will also introduce functions to read and visualize the images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Exploring image data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we check that all the image names present in the dataset are also present
    in the folder with images:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The result is that all the images indexed in the .`csv` file are present in
    the `images` folder. Next, we check the image sizes. For this, we can use the
    following code to read images:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Alternatively, we can use the following code to read images based on the OpenCSV
    (`cv2`) library:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The code below is used to measure the execution time to read the images, using
    the method based on opencv:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The comparison shows that the execution was faster using the `opencv`-based
    method:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'With `skimage.io`:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`129 ms ± 4.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'With `opencv`:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`127 ms ± 6.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we apply the fastest approach to extract the shape of each image (the
    width, height, and depth, or the number of color dimensions) and add it to the
    dataset for each image:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of executing the preceding code is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The result is plotted in *Figure 6.5*. Median values for width and height are
    61 and 62, respectively. There are many outliers for both width (the maximum value
    being 520) and height (the maximum value is 392).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_05.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Width and height distribution of images'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'In our analysis, we include all the features in the dataset, not only the ones
    related to images. We want to understand, before we start building a baseline
    for the prediction model, all aspects related to *The BeeImage Dataset: Annotated
    Honey Bee Images*.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们包括了数据集中的所有特征，而不仅仅是与图像相关的特征。在我们开始构建预测模型的基线之前，我们希望了解与*蜜蜂图像数据集：标注的蜜蜂图像*相关的所有方面。
- en: Locations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置
- en: 'By grouping the data in our dataset by locations where the pictures were shot
    and ZIP code, we can observe that there is one location with the same ZIP code
    and similar name:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按拍摄图片的位置和ZIP代码对数据集中的数据进行分组，我们可以观察到有一个位置具有相同的ZIP代码和类似的名字：
- en: '![](img/B20963_06_06.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_06_06.png)'
- en: 'Figure 6.6: Locations and ZIP codes where the pictures with bees were taken'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：拍摄带有蜜蜂的图片的位置和ZIP代码
- en: 'We can observe that Athens, Georgia, USA appears with two slightly different
    names. We just merge them using the following lines of code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，美国乔治亚州的雅典出现了两个略有不同的名称。我们只是使用以下代码将它们合并：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let’s visualize, using one of the functions in the Plotly utility script
    module, the distribution of the resulting location data:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Plotly实用脚本模块中的一个函数来可视化结果位置数据的分布：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The code for the function `plotly_barplot` is given here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`plotly_barplot`的代码如下：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In *Figure 6.7*, we show the distribution of the locations where the bee images
    were taken. Most of the images are from Saratoga, CA (2000 images), followed by
    Athens, GA, and Des Moines, IA.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.7*中，我们展示了拍摄蜜蜂图像的位置分布。大多数图像来自加利福尼亚州的萨拉托加（2000张图像），其次是乔治亚州的雅典和爱荷华州的迪莫因。
- en: '![](img/B20963_06_07.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_06_07.png)'
- en: 'Figure 6.7: Location distribution'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：位置分布
- en: 'We also build a function for the visualization of subsets of images, based
    on a selected criterion. The following code is to select images based on location
    and display a subset of them (five images in a row, from the same location):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还基于一个选定的标准构建了一个用于可视化图像子集的函数。以下代码是根据位置选择图像并显示它们的子集（一行五张，来自同一位置）：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In *Figure 6.8*, a fraction of this selection is shown (just for the first
    two locations). The full image can be seen in the associated notebook:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.8*中，展示了这个选择的一部分（仅限于前两个位置）。完整的图像可以在相关的笔记本中查看：
- en: '![](img/B20963_06_08.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_06_08.png)'
- en: 'Figure 6.8: Bee images from two locations (a selection from the full picture,
    obtained with the preceding code)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：来自两个地点的蜜蜂图像（从完整图片中选择，使用前面的代码获取）
- en: Date and time
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日期和时间
- en: 'Let’s continue with the detailed analysis of the features in our dataset. We
    start now to analyze the `date` and `time` data. We will convert the `date` to
    `datetime` and extract the year, month, and day. We also convert `time` and extract
    the hour and minute:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续详细分析我们数据集中的特征。我们现在开始分析`date`和`time`数据。我们将`date`转换为`datetime`并提取年、月和日。我们还转换`time`并提取小时和分钟：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A visualization of the number of bee images per date and approximative hour
    and location is shown in *Figure 6.9*. The code for this visualization starts
    by grouping the data by `date_time` and `hour` and calculating the number of images
    collected on each date and the time of day:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.9*中展示了每天和大约的小时及位置的蜜蜂图像数量的可视化。这个可视化的代码首先通过`date_time`和`hour`对数据进行分组，并计算每个日期和一天中的时间收集到的图像数量：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we build the text displayed when we hover over one point displayed in
    the graph. This text will include the hour, location, and the number of images.
    We then add the hover texts as a new column in the new dataset:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建当我们在图中的某个点上悬停时显示的文本。这个文本将包括小时、位置和图像数量。然后我们将悬停文本作为新数据集中的一个新列添加：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we represent, for each location, a scatter plot with the time and hour
    when pictures were collected. Each point’s size is proportional to the number
    of images taken in that location, at a certain time of day, and on a certain date:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为每个位置绘制一个散点图，表示图片收集的时间和小时。每个点的尺寸与在该位置、一天中的某个时间点和某个日期拍摄的图像数量成比例：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next image, *Figure 6.9*, we see the result of running the aforementioned
    code. Most pictures were taken around August. Most of the pictures were also taken
    in the afternoon hours.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一张图像，*图6.9*中，我们看到运行上述代码的结果。大多数图片是在八月份拍摄的。大多数图片也是在下午时段拍摄的。
- en: '![](img/B20963_06_09.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_06_09.png)'
- en: 'Figure 6.9: Number of bee images per date and the approximate hour and location'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：每天和大约的小时及位置的蜜蜂图像数量
- en: Subspecies
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚种
- en: We use the same `plotly_barplot` function to visualize the distribution of subspecies.
    Most of the bees are Italian honey bees, followed by Russian honey bees and Carniolan
    honey bees (see *Figure 6.10*). Some of the 428 images are not classified (with
    a label value of **-1**). We will keep the images not classified as one subspecies
    category.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, line  Description automatically
    generated](img/B20963_06_10.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Number of bee images per date and the approximate hour and location'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.11*, we show a selection of images, with samples for just a few
    of the subspecies:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of images of a bee  Description automatically generated](img/B20963_06_11.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Samples of bee images from a few subspecies'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s represent the number of images per subspecies and location, as well
    as the number of images per subspecies and hour (see *Figure 6.12*). The largest
    number of images were collected from Saratoga, CA, and all were Italian honey
    bees (1972 images). The largest number of images were collected at hour 13, and
    all were of Italian honey bees (909 images).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_12.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Number of images per subspecies and location (upper) and the number
    of images per subspecies and hour (lower)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The `Subspecies` images have a large variety of weights and heights. *Figure
    6.13* shows this distribution, using a boxplot, for the weight and the height.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_13.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Image size distribution per each subspecies – width (upper) and
    height (lower)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '**VSH Italian honey bee** has the largest average and also the largest variance
    for both width and height. **Western honey bee**, **Carniolan honey be***e*, and
    **Mixed local stock 2** have the most compact distribution of weight and height
    (and lower variance). The *Italian honey bee*, the most numerous subspecies, shows
    both a small median and a large variance, with a lot of outliers. In the next
    figure, we show the weight and the height distribution combined on the same scatter
    plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_14.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Image size distribution per each subspecies – scatter plot'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure shows this distribution, using a scatter plot, for the
    weight and the height. The code for this visualization is shown in the following.
    First, we start by defining a function to draw a scatter plot, with the image
    width on the `x` scale and the image height on the `y` scale:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We now use the function defined above to draw the scatter plot for each subspecies.
    Each function call will create a trace, and we add the traces to the Plotly plot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Health
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 6.15* shows the distribution of images with various health problems.
    The majority of images are for **healthy** bees (3384), followed by **few varrao,
    hive beetles** (579), **Varroa, Small Hive Beetles** (472), and **ant problems**
    (457):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, line  Description automatically
    generated](img/B20963_06_15.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Number of images with different health problems'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15：不同健康问题的图像数量
- en: If we analyze the number of images per subspecies and health (see *Figure 6.16*),
    we can observe that only a reduced number of combinations of health and subspecies
    values are present. Most images are **healthy** **Italian honey bee** (1972),
    followed by **few varroa, hive beetles**, then **Italian honey bee** (579), and
    lastly, **healthy** **Russian honey bee** (527). The unknown subspecies are either
    **healthy** (177) or **hive being robbed** (251).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析每个亚种和健康问题的图像数量（见*图6.16*），我们可以观察到只有少量健康和亚种值组合存在。大多数图像是**健康的意大利蜜蜂**（1972），其次是**少量瓦螨、蜂箱甲虫**，然后是**意大利蜜蜂**（579），最后是**健康的俄罗斯蜜蜂**（527）。未知亚种要么是**健康的**（177）要么是**蜂群被盗**（251）。
- en: '![A screenshot of a computer screen  Description automatically generated with
    low confidence](img/B20963_06_16.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成，置信度低](img/B20963_06_16.png)'
- en: 'Figure 6.16: Number of images per subspecies and bees with different health
    problems'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16：每个亚种和不同健康问题的蜜蜂图像数量
- en: 'In *Figure 6.17*, we plot the number of images per location, subspecies, and
    health problems:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.17*中，我们绘制了每个地点、亚种和健康问题的图像数量：
- en: '![A screenshot of a graph  Description automatically generated with low confidence](img/B20963_06_17.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图表截图，描述自动生成，置信度低](img/B20963_06_17.png)'
- en: 'Figure 6.17: Number of images per location, subspecies, and health problems'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：每个地点、亚种和健康问题的图像数量
- en: Others
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他
- en: 'There is a small number of images with bees carrying pollen. *Figure 6.18*
    shows a few images of bees carrying pollen and some not carrying pollen. All the
    bees are from only one caste: the worker caste.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 携带花粉的蜜蜂图像数量很少。*图6.18*显示了一些携带花粉和不携带花粉的蜜蜂图像。所有蜜蜂都来自同一个等级：工蜂等级。
- en: '![A picture containing collage, screenshot, text  Description automatically
    generated](img/B20963_06_18.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![包含拼贴、截图、文本的图片，描述自动生成](img/B20963_06_18.png)'
- en: 'Figure 6.18: Selection of images with bees carrying pollen and not carrying
    pollen'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18：携带和不携带花粉的蜜蜂图像的选择
- en: Conclusion
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'We use a Sankey diagram, drawn with the `plotly_sankey` script from the `plotly_utils`
    utility script module, to draw the summary graph in *Figure 6.19*. A Sankey diagram
    is used mainly to visualize processes or flows, for example, the production of
    energy, with its sources and consumers, in an economy. I use it here with another
    purpose, to summarize the distribution of data with multiple features. It shows
    on the same plot the distribution of images per date, time, location, ZIP code,
    subspecies, and health. The adaptation code for the Sankey diagram is not given
    here for limited space reasons (refer *Reference 2* for the code samples associated
    with the chapter); we just include the code to adapt the honey bee data for the
    use of this function:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`plotly_sankey`脚本从`plotly_utils`实用脚本模块中绘制的桑基图，来绘制*图6.19*中的摘要图。桑基图主要用于可视化流程或流动，例如，在经济学中能源的生产及其来源和消费者。我这里用它来达到另一个目的，即总结具有多个特征的数据分布。它显示了同一图表中按日期、时间、地点、ZIP代码、亚种和健康分布的图像。由于空间限制，这里没有给出桑基图的适配代码（请参考*参考文献2*获取与本章相关的代码示例）；我们只包含了适配蜜蜂数据以使用此功能的代码：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The visualization in *Figure 6.19*, a funnel-like graph, allows us to capture
    in one single graph the relationship between multiple features:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.19*中的可视化，一个漏斗形图，使我们能够在一个单一的图表中捕捉多个特征之间的关系：'
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_19.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图，描述自动生成，置信度低](img/B20963_06_19.png)'
- en: 'Figure 6.19: Summary of images'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19：图像摘要
- en: Until now, we analyzed the distribution of features in the dataset. Now, we
    have a better understanding of the data in the dataset. In the following section
    of this chapter, we will start preparation to build a machine learning model to
    classify images on subspecies, which is the second and more important objective
    of this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们分析了数据集中特征分布。现在，我们对数据集中的数据有了更好的理解。在本章接下来的部分，我们将开始准备构建一个机器学习模型，以对亚种进行图像分类，这是本章的第二大且更为重要的目标。
- en: Subspecies classification
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚种分类
- en: 'The objective of this section will be to use the images investigated until
    now to build a machine learning model that will correctly predict the subspecies.
    Since we have one dataset only, we will start by splitting the data into three
    subsets: for train, validation, and test data. We will use train and validation
    during the training process: the train data to feed the model and the validation
    data to verify how well the model predicts the class (i.e., the subspecies) with
    the new data. Then, the model trained and validated will be used to predict the
    class in the test set, which was not used either in train or validation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标将是使用迄今为止调查的图像构建一个机器学习模型，该模型可以正确预测亚种。由于我们只有一个数据集，我们将首先将数据分割成三个子集：用于训练、验证和测试数据。我们将在训练过程中使用训练和验证数据：训练数据用于向模型提供数据，验证数据用于验证模型如何使用新数据预测类别（即亚种）。然后，训练和验证后的模型将用于预测测试集中的类别，该类别既未用于训练也未用于验证。
- en: Splitting the data
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'First, we split the data into `train` and `test`, using an 80%–20% split. Then,
    the `train` data is split again, in to train and validation, using the same 80%–20%
    split. The splits are performed using `stratify` with `subspecies` as a parameter,
    ensuring balanced subsets that respect the overall distribution of classes in
    the subsampled sets of train, validation, and test. The percent values for train/validation/test
    splits are chosen arbitrarily here and are not the result of a study or optimization.
    In your experiments, you can work with different values for train/validation/test
    subsets, and you can also choose to not use `stratify`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据分割成`train`和`test`，使用80%–20%的分割。然后，再次将`train`数据分割成训练和验证，使用相同的80%–20%分割。分割使用`stratify`和`subspecies`作为参数执行，确保平衡的子集，同时尊重训练、验证和测试子样本集中类的整体分布。这里选择的训练/验证/测试分割的百分比是任意选择的，并不是研究或优化的结果。在您的实验中，您可以处理不同的训练/验证/测试子集值，也可以选择不使用`stratify`：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Ultimately, we will have three subsets, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将有三个子集，如下所示：
- en: 'Train set rows: 3309'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集行数：3309
- en: 'Validation set rows: 828'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集行数：828
- en: 'Test set rows: 1035'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集行数：1035
- en: We will split the images into subsets, corresponding to the image names’ subsets.
    We create functions to read images and rescale them all to the same dimension,
    as specified in the configuration structure we defined, using `skimage.io` and
    `opencv`. We decided to rescale all images to 100 x 100 pixels. Our decision was
    based on the analysis of the images’ size distribution. You can choose to modify
    the code in the notebook given at *Reference 3* and experiment with different
    image sizes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将图像分割成子集，对应于图像名称的子集。我们创建了读取图像并将它们全部调整到配置中定义的相同维度的函数，使用`skimage.io`和`opencv`。我们决定将所有图像调整到100
    x 100像素。我们的决定是基于对图像尺寸分布的分析。您可以选择修改笔记本中提供的代码（*参考3*）并尝试不同的图像尺寸。
- en: 'The following code reads an image (using `skimage.io`) and resizes it according
    to the size set in the configuration. You can change the configuration and resize
    the image, using different values for the image height and width:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用`skimage.io`读取图像并根据配置中设置的尺寸调整大小。您可以更改配置并调整图像大小，使用不同的图像高度和宽度值：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The code below reads and resizes an image using opencv. The function differs
    from the previous one shown above just by the method to read the image file:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码使用OpenCV读取并调整图像大小。该函数与上面展示的之前的函数不同之处仅在于读取图像文件的方法：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We then apply one of these functions to all dataset files to read and rescale
    the images in the dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些函数应用于所有数据集文件，以读取和调整数据集中的图像大小。
- en: 'We also create dummy variables corresponding to the categorical target variables.
    We prefer to use this approach because we will prepare a model for multiclass
    classification that outputs probabilities per class:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了与分类目标变量对应的虚拟变量。我们更喜欢使用这种方法，因为我们将为多类分类准备一个模型，该模型为每个类别输出概率：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With that, we’ve shown how to read and rescale our images. Next, we will see
    how we can multiply our images to have more data in the train set so that we present
    to the model a larger variety of data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们已经展示了如何读取和调整我们的图像大小。接下来，我们将看到如何通过乘以我们的图像来增加训练集中的数据量，以便向模型展示更多种类的数据。
- en: Data augmentation
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: We will use a deep learning model to classify the subspecies in our images.
    Typically, deep learning models perform better when trained with a larger amount
    of data. Using data augmentation, we also create a larger variety in the data,
    which is also beneficial for the model quality. Our model will improve its generalization
    if we expose it to more diverse data during training.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用深度学习模型来对图像中的亚种进行分类。通常，深度学习模型在训练数据量较大时表现更好。使用数据增强，我们还创建了更多样化的数据，这对模型质量也有益。如果我们在训练过程中让模型接触到更多样化的数据，我们的模型将提高其泛化能力。
- en: 'We define a component for data augmentation, based on `ImageDataGenerator`
    from `keras.preprocessing.image`. In this notebook, we will use Keras to build
    various components of the model. The `ImageDataGenerator` component is initialized
    with various parameters to create random variations of the training dataset, by
    applying:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于`keras.preprocessing.image`中的`ImageDataGenerator`定义了一个数据增强组件。在本笔记本中，我们将使用Keras构建模型的各个组件。`ImageDataGenerator`组件通过应用以下参数初始化，以创建训练数据集的随机变化：
- en: a rotation (in a range of 0 to 180 degrees) of the original images
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对原始图像进行旋转（0到180度范围内）
- en: a zoom (10%)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放（10%）
- en: a shift in horizontal and vertical directions (10%)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平和垂直方向上的平移（10%）
- en: a horizontal and vertical shift (10%)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平和垂直方向的平移（10%）
- en: 'These variations can be controlled separately. Not all use cases will allow,
    or benefit from applying, all the transformations listed above (think about, for
    example, images with buildings or other landmarks, for which rotations do not
    make too much sense). The following code can be used in our case to initialize
    and fit the image generator:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化可以分别控制。并非所有用例都允许或从应用上述所有转换中受益（例如，考虑具有建筑或其他地标图像的情况，对于这些图像，旋转并不太有意义）。以下代码可以用于我们的情况来初始化和拟合图像生成器：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We then move on to building and training a baseline model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续构建和训练基线模型。
- en: Building a baseline model
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建基线模型
- en: It is almost always recommended to start your work with a simple model and then
    conduct an error analysis. Based on the error analysis, you will have to further
    refine your model. If, for example, you observe that you obtained, with your baseline
    model, a large error for training, you need to start by improving the training.
    You can do this by perhaps adding more data, improving your data labeling, or
    creating better features. If your training error is small but you have instead
    a high validation error, it means that your model probably overfits on the training
    data. In such a case, you need to try to improve your model generalization. You
    can try various techniques to improve your model generalization. See more about
    this kind of analysis in *Reference 4* at the end of this chapter.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎总是建议您从一个简单的模型开始，然后进行错误分析。根据错误分析，您将需要进一步细化您的模型。例如，如果您观察到您的基线模型在训练中获得了很大的误差，您需要首先改进训练。您可以通过添加更多数据、改进您的数据标注或创建更好的特征来实现这一点。如果您的训练误差很小，但您有较高的验证误差，这意味着您的模型可能过度拟合了训练数据。在这种情况下，您需要尝试提高模型泛化能力。您可以尝试各种技术来提高模型泛化能力。关于此类分析，请参阅本章末尾的*参考4*。
- en: 'We will use the `Keras` library to define our models. Keras (see *Reference
    5*) is a wrapper over the TensorFlow machine learning platform (see *Reference
    6*). It allows you to create powerful deep learning models by defining a sequential
    structure with specialized layers. We will add the following layers to our model:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Keras`库来定义我们的模型。Keras（见*参考5*）是TensorFlow机器学习平台（见*参考6*）的包装器。它允许您通过定义具有专用层的顺序结构来创建强大的深度学习模型。我们将向我们的模型添加以下层：
- en: One `Conv2D` layer, with 16 filters of dimension 3
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有3维度的16个滤波器的`Conv2D`层
- en: One `MaxPooling2D` layer, with reduction factor 2
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有2倍缩减因子的`MaxPooling2D`层
- en: One convolutional layer, with 16 filters of dimension 3
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有3维度的16个滤波器的卷积层
- en: A `Flatten` layer
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Flatten`层
- en: A Dense layer, with the dimension the number of classes of the subspecies target
    feature
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有亚种目标特征类别数量的`Dense`层
- en: The preceding architecture is a very simple example of a **convolutional neural
    network**. The role of the `convolutional2d` layer is to apply sliding convolutional
    filters to a 2D input. The `maxpool2d` layer will down-sample the input along
    its spatial dimension (width and height) by taking the maximum value over an input
    window (see *Reference 5* for more details).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构是一个非常简单的**卷积神经网络**的例子。`convolutional2d`层的作用是对2D输入应用滑动卷积滤波器。`maxpool2d`层将通过在输入窗口（参见*参考文献5*获取更多详细信息）上取最大值来沿其空间维度（宽度和高度）对输入进行下采样。
- en: 'The code to build the described architecture will be:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 构建所述架构的代码如下：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In *Figure 6.20*, we show the summary information of the model. As you can
    see, the total number of trainable parameters is 282,775:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.20*中，我们展示了模型的摘要信息。如您所见，可训练参数的总数是282,775：
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B20963_06_20.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图，描述自动生成，中等置信度](img/B20963_06_20.png)'
- en: 'Figure 6.20: Summary of the baseline model'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20：基准模型的摘要
- en: 'For the baseline, we start with a small model and train for a reduced number
    of epochs. The size of the input images is 100 x 100 x 3 (as we explained previously).
    We will train this model for five epochs. The batch size is set to 32\. The code
    to run the training is shown here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基准模型，我们从一个小模型开始，并减少epoch的数量进行训练。输入图像的大小是100 x 100 x 3（如我们之前解释的）。我们将对这个模型进行五次epoch的训练。批大小设置为32。运行训练的代码如下：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Figure 6.21* shows the training log for the baseline model. We are not saving
    the best model version during training; the model weights at the last step will
    be used for the test.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.21*显示了基准模型的训练日志。在训练过程中，我们没有保存最佳模型版本；最后一步的模型权重将用于测试。'
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B20963_06_21.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、字体、数字的图片，描述自动生成](img/B20963_06_21.png)'
- en: 'Figure 6.21: The training log for the baseline model. The training loss and
    accuracy and validation loss and accuracy for each step are shown'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21：基准模型的训练日志。显示了每个步骤的训练损失和准确率以及验证损失和准确率。
- en: 'The training loss and accuracy are updated after each batch, and the validation
    loss and accuracy are calculated at the end of each epoch. Next, with the model
    trained and validated, we will evaluate the test set loss and accuracy:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批次之后更新训练损失和准确率，在每个epoch结束时计算验证损失和准确率。接下来，在模型训练和验证之后，我们将评估测试集的损失和准确率：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In *Figure 6.22*, we show the training and validation loss and the training
    and validation accuracy:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.22*中，我们展示了训练和验证损失以及训练和验证准确率：
- en: '![A graph of a graph of a graph of a graph of a graph of a graph of a graph
    of a graph of a graph of a graph of a graph of a graph of a graph of  Description
    automatically generated with low confidence](img/B20963_06_22.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文本、截图、字体、数字的图片，描述自动生成](img/B20963_06_22.png)'
- en: 'Figure 6.22: Baseline model – the training and validation accuracy (left) and
    the training and validation loss (right)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22：基准模型 – 训练和验证准确率（左）和训练和验证损失（右）
- en: 'The results obtained are as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的结果如下：
- en: 'Test loss: **0.42**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试损失：**0.42**
- en: 'Test accuracy: **0.82**'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试准确率：**0.82**
- en: The test loss refers to the loss function, a mathematical function that measures
    the difference between the predicted values and the true values. By measuring
    this value during training, for both the train and validation sets, we can monitor
    how the model learns and improves its predictions.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 测试损失指的是损失函数，这是一个数学函数，用于衡量预测值与真实值之间的差异。在训练过程中，通过测量这个值，对于训练集和验证集，我们可以监控模型的学习和预测的改进情况。
- en: 'Using a metrics classification report (`metrics.classification_report`) from
    `sklearn`, we calculate the precision, recall, f1-score, and accuracy per each
    class in train data. The code for that is given here:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`中的`metrics.classification_report`指标分类报告，我们计算了训练数据中每个类的精确度、召回率、f1分数和准确率。相应的代码如下：
- en: '[PRE27]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In *Figure 6.23*, we show this classification report for the test set, where
    we applied the baseline model fit with train data. The macro average of the precision,
    recall, and f1-score are **0.78**, **0.72**, and **0.74**, respectively (the support
    data is **1035**). The weighted average precision, recall, and f1-score are **0.82**,
    **0.83**, and **0.82**, respectively.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_23.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Classification report for test data using the baseline model'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: These weighted average scores are higher because unlike the simple averages
    of all class scores, these are weighted averages, so the higher scores associated
    with better-represented classes will have a higher contribution to the overall
    average. The worst scores for precision/class are for **1 Mixed local stock 2**
    (0.52) and **VSH Italian honey bee** (0.68). The worst overall score is for **VSH
    Italian honey bee,** where the recall is 0.33.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Iteratively refining the model
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we go back now to the training and validation errors, we can see that validation
    and training accuracies are around 0.81 and 0.82.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to train the model, and to avoid overfitting, we will also
    introduce two `Dropout` layers, each with a coefficient of 0.4\. A `Dropout` layer
    is used as a regularization method in neural networks. Its purpose is to prevent
    overfitting and improve generalization of the model. The coefficient given as
    a parameter is the percentage of inputs randomly selected during training to be
    set to zero at each training epoch. The structure of the model is described in
    *Figure 6.24*. The number of trainable parameters will remain the same.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B20963_06_24.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Refined model summary. Two Dropout layers were added'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We also extend the number of epochs to 10\. Let’s see the results in *Figure
    6.25*, where we show the training and validation accuracy and the training and
    validation loss.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a graph of a graph of a graph of a graph of a graph of a graph
    of a graph of a graph of a graph of a graph of a graph of a graph of  Description
    automatically generated with low confidence](img/B20963_06_25.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.25: Refined model (version 2) – the training and validation accuracy
    (left) and the training and validation loss (right)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The final training loss is 0.32, and the final training accuracy is 0.87\. The
    final validation loss is 0.28, and the final validation accuracy is 0.88\. These
    are improved results. Of course, training accuracy is mainly improved due to the
    fact that we trained for more epochs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated with
    low confidence](img/B20963_06_26.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Classification report for test data using the second refined model
    (the training epochs increased to 10 and Dropout layers added)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: As per validation accuracy, the result is due to both more training epochs as
    well as adding `Dropout` layers, which kept overfitting under control. Let’s now
    check the test loss and accuracy as well as look at the entire classification
    report for test data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Both macro averaged and weighted averaged metrics improved for precision, recall,
    and f1-score. Also, we can see much-improved precision, recall, and f1-scores
    for the classes with small scores obtained with the baseline. **1 Mixed local
    stock 2** had 0.52 precision, and now, precision is 0.63\. As for **VSH Italian
    honey bee**, precision was 0.68 and is now 0.97\. We see a degradation of the
    precision of **Western honey bee**, but the support for this minority class is
    only 7, so this result is expected.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue to refine our model to improve the validation and test metrics
    – in other words, to improve the model performance. In the next iteration, we
    will increase the number of training epochs to 50\. Also, we will add three callback
    functions, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: A learning rate scheduler, to implement a nonlinear function for variation of
    the learning rate. By changing the learning rate at each epoch, we can improve
    the training process. The function we introduce to control the learning rate will
    gradually decrease the learning function value.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An early stopper to stop training epochs, based on the loss function evolution
    (if loss does not improve for a certain number of epochs) and a patience factor
    (the number of epochs, after which, if we don’t see any improvement of the monitored
    function, we stop training).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A check pointer to save the best-performing models every time we obtain the
    best accuracy. This will allow us to use not the model parameters at the last
    training epoch but, instead, the best-performing model of all the epochs.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for the three callback functions is given below:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The code to fit the model is also given below:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The training can take as many as the maximum number of epochs assigned or, if
    the early stopping criteria are met (i.e., no loss function improvement after
    a number of epochs equal to the patience factor), it might end earlier. Either
    way, the model for which the best validation accuracy was achieved is saved and
    will be used for testing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6.27*, we show the evolution of the training and validation accuracy
    and the training and validation loss for this further refined model. The final
    training loss obtained is 0.18, and the final training accuracy is 0.93\. For
    validation, the last validation loss is 0.21, and the last validation accuracy
    is 0.91\. The learning rate at the final epoch is 6.08e-4\. The best validation
    accuracy obtained was for epoch 46, 0.92.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, plot, diagram  Description automatically
    generated](img/B20963_06_27.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Refined model (version 3, with a learning rate scheduler, early
    stop, and checkpoint) – the training and validation accuracy (left) and the training
    and validation loss (right)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: We use the model checkpoint saved (for epoch 46) to predict for the test data.
    In *Figure 6.28*, we show the classification report for the third model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Macro average metrics are further improved to the precision, recall, and f1-score
    values of 0.88, 0.90, and 0.89, respectively. The weighted average values for
    precision, recall, and f1-score are also improved to 0.91, 0.90, and 0.90, respectively.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated with
    low confidence](img/B20963_06_28.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Classification report for test data using the third refined model
    (the training epochs increased to 50 and a learning rate scheduler, early stopper,
    and model checkpoint added)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will stop the process of iteratively improving the model. You can continue
    to refine it. You can try to add more convolutional and maxpool layers, using
    a different number of kernels and values of stride to work with different hyperparameters,
    including a different batch size or learning rate scheduler. Also, you can change
    the optimization scheme. Another way to change the model is to control, through
    data augmentation, the balance of the class images (currently, the bee images
    are unbalanced with respect to the `subspecies` class).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: You can also experiment with various data augmentation parameters and try to
    use a different data augmentation solution. See *Reference 5* for one example
    of an image data augmentation library that is very much used currently, **Albumentations**,
    created by a group of data scientists, researchers, and computer vision engineers,
    including the famous Kaggle Grandmaster Vladimir Iglovikov.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by introducing a new dataset, with metadata about
    images collected at different dates and in different locations, including various
    bee subspecies that have various diseases. We also introduced a few functions
    to read, rescale, and extract features from images, based on `skimage.io` and
    opencv (`cv2`).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We used a newly created utility script to visualize tabular data, based on Plotly,
    and created insightful visualizations by leveraging the flexibility of Plotly
    to create customized graphics. We also created visualization functions for images.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'After the detailed EDA, we moved on to building a predictive model for bee
    subspecies. Here, we introduced a method for data augmentation to multiply the
    initial available training data, by creating variations (rotations, zoom, shift,
    and mirroring) from the original image sets. We split the data into train, validation,
    and test subsets, using `stratify` to account for the class imbalance when randomly
    sampling the three subsets. We started by training and validating a baseline model,
    and then, after performing error analysis, we gradually improved the initial model,
    by adding more steps, introducing `Dropout` layers, and then using several callbacks:
    the learning rate scheduler, early stopper, and model checkpoints. We analyzed
    the iterative improvement for the training, validation, and test errors, looking
    not only at the training and validation loss and accuracy but also at the classification
    report for test data.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will introduce techniques and tools for text data
    analysis, showing you how to prepare your data to create baseline models using
    text data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The BeeImage Dataset: Annotated Honey Bee Images: [https://www.kaggle.com/datasets/jenny18/honey-bee-annotated-images](https://www.kaggle.com/datasets/jenny18/honey-bee-annotated-images)'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`plotly-script` and Kaggle Utility Script: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/plotly-utils.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/plotly-utils.ipynb)'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Honeybee Subspecies Classification, Kaggle Notebook: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/honeybee-subspecies-classification.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/honeybee-subspecies-classification.ipynb)'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Andrew Ng, Machine Learning Yearning: [https://info.deeplearning.ai/machine-learning-yearning-book](https://info.deeplearning.ai/machine-learning-yearning-book)'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keras: [https://keras.io/](https://keras.io/)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'TensorFlow: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using Albumentations with Tensorflow: [https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/tensorflow-example.ipynb](https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/tensorflow-example.ipynb)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code9220780366773140.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
