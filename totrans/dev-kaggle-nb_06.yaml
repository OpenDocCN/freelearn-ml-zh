- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can You Predict Bee Subspecies?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to work with image data and start building
    models to classify images. Computer vision’s part in data science and data analysis
    has grown in an exponential way over the years. Some of the most high-profile
    (with a large number of upvotes and forks, e.g., copying and editing) notebooks
    on Kaggle are not **Exploratory Data Analysis** (**EDA**) notebooks or just EDAs
    but, instead, notebooks to build models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will demonstrate how to use your in-depth data analysis
    to prepare to build a model, and we will also give you some insights into the
    process of iteratively refining a model. It will be not for a competition but,
    instead, for an image dataset. The dataset is the *BeeImage Dataset: Annotated
    Honey Bee Images* (see *Reference 1*). In the previous chapter, we also started
    to use Plotly as a visualization library. In this chapter, we will continue to
    use Plotly for visualization of the dataset features. We grouped a few useful
    functions for visualization with Plotly in a utility script, `plotly-utils` (see
    *Reference 2*). The notebook associated with this chapter is *Honeybee Subspecies
    Classification* (see *Reference 3*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A comprehensive data exploration of the *BeeImage Dataset: Annotated Honey
    Bee Images*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparation of a model baseline followed by step-by-step model refinement, analyzing
    the effect of the changes performed on the evolution of train and validation metrics,
    and taking new actions to further improve the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *BeeImage Dataset: Annotated Honey Bee Images* contains one **comma-delimited
    format** (.**csv**) file, `bee_data.csv`, with 5172 rows and 9 columns, along
    with a folder with 5172 images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Sample of the bee_data.csv data file'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the preceding dataframe contains the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**file**: the image filename'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**date**: the date when the picture was taken'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**time**: the time when the picture was taken'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**location**: the US location, with city, state, and country names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**zip code**: the ZIP code associated with the location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**subspecies**: the subspecies to whom the bee in the current image belongs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**health**: the health state of the bee in the current image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pollen_carrying**: indicates whether the picture shows the bee with pollen
    attached to its legs or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**caste**: the bee’s caste'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start the data exploration journey with a few quality checks, focusing
    on the `bee_data.csv` file, followed by the images. For the data quality checks,
    we will use one of the utility scripts previously introduced in *Chapter 4*, `data_quality_stats`.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset does not have any missing values, as you can see in the following
    figure. All the features are of the `string` type.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Missing values in the bee_data.csv file. The result was obtained
    using the data_quality_stats functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.3*, we show the unique values of the dataset features. The data
    was collected:'
  prefs: []
  type: TYPE_NORMAL
- en: at 6 different dates and 35 different times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in 8 locations with 7 different ZIP codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the data, there are seven subspecies represented with six different health
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, line  Description automatically
    generated](img/B20963_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Unique values in the bee_data.csv file. The result was obtained
    using the data_quality_stats functions'
  prefs: []
  type: TYPE_NORMAL
- en: From the data shown in *Figure 6.4*, 21% of the images are from one single date
    (out of 16 different dates). There was a time when 11% of images were collected.
    There is one single location (Saratoga, California, with the ZIP code 95070) where
    2000 (or 39% of the) images were collected. Italian honeybee is the most frequent
    species. 65% of the images represent healthy bees. Almost all the images show
    bees that do not carry pollen, and all are from the worker caste.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, font, number, line  Description automatically
    generated](img/B20963_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The most frequent values in the bee_data.csv file. The result was
    obtained using data_quality_stats functions'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will review the image data in parallel with the features in `bee_data.csv`.
    We will also introduce functions to read and visualize the images.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring image data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we check that all the image names present in the dataset are also present
    in the folder with images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is that all the images indexed in the .`csv` file are present in
    the `images` folder. Next, we check the image sizes. For this, we can use the
    following code to read images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use the following code to read images based on the OpenCSV
    (`cv2`) library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The code below is used to measure the execution time to read the images, using
    the method based on opencv:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The comparison shows that the execution was faster using the `opencv`-based
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With `skimage.io`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`129 ms ± 4.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'With `opencv`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`127 ms ± 6.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we apply the fastest approach to extract the shape of each image (the
    width, height, and depth, or the number of color dimensions) and add it to the
    dataset for each image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of executing the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The result is plotted in *Figure 6.5*. Median values for width and height are
    61 and 62, respectively. There are many outliers for both width (the maximum value
    being 520) and height (the maximum value is 392).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Width and height distribution of images'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our analysis, we include all the features in the dataset, not only the ones
    related to images. We want to understand, before we start building a baseline
    for the prediction model, all aspects related to *The BeeImage Dataset: Annotated
    Honey Bee Images*.'
  prefs: []
  type: TYPE_NORMAL
- en: Locations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By grouping the data in our dataset by locations where the pictures were shot
    and ZIP code, we can observe that there is one location with the same ZIP code
    and similar name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Locations and ZIP codes where the pictures with bees were taken'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe that Athens, Georgia, USA appears with two slightly different
    names. We just merge them using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s visualize, using one of the functions in the Plotly utility script
    module, the distribution of the resulting location data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for the function `plotly_barplot` is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 6.7*, we show the distribution of the locations where the bee images
    were taken. Most of the images are from Saratoga, CA (2000 images), followed by
    Athens, GA, and Des Moines, IA.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Location distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also build a function for the visualization of subsets of images, based
    on a selected criterion. The following code is to select images based on location
    and display a subset of them (five images in a row, from the same location):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 6.8*, a fraction of this selection is shown (just for the first
    two locations). The full image can be seen in the associated notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Bee images from two locations (a selection from the full picture,
    obtained with the preceding code)'
  prefs: []
  type: TYPE_NORMAL
- en: Date and time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s continue with the detailed analysis of the features in our dataset. We
    start now to analyze the `date` and `time` data. We will convert the `date` to
    `datetime` and extract the year, month, and day. We also convert `time` and extract
    the hour and minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'A visualization of the number of bee images per date and approximative hour
    and location is shown in *Figure 6.9*. The code for this visualization starts
    by grouping the data by `date_time` and `hour` and calculating the number of images
    collected on each date and the time of day:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we build the text displayed when we hover over one point displayed in
    the graph. This text will include the hour, location, and the number of images.
    We then add the hover texts as a new column in the new dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we represent, for each location, a scatter plot with the time and hour
    when pictures were collected. Each point’s size is proportional to the number
    of images taken in that location, at a certain time of day, and on a certain date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the next image, *Figure 6.9*, we see the result of running the aforementioned
    code. Most pictures were taken around August. Most of the pictures were also taken
    in the afternoon hours.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Number of bee images per date and the approximate hour and location'
  prefs: []
  type: TYPE_NORMAL
- en: Subspecies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use the same `plotly_barplot` function to visualize the distribution of subspecies.
    Most of the bees are Italian honey bees, followed by Russian honey bees and Carniolan
    honey bees (see *Figure 6.10*). Some of the 428 images are not classified (with
    a label value of **-1**). We will keep the images not classified as one subspecies
    category.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, line  Description automatically
    generated](img/B20963_06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Number of bee images per date and the approximate hour and location'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.11*, we show a selection of images, with samples for just a few
    of the subspecies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of images of a bee  Description automatically generated](img/B20963_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Samples of bee images from a few subspecies'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s represent the number of images per subspecies and location, as well
    as the number of images per subspecies and hour (see *Figure 6.12*). The largest
    number of images were collected from Saratoga, CA, and all were Italian honey
    bees (1972 images). The largest number of images were collected at hour 13, and
    all were of Italian honey bees (909 images).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Number of images per subspecies and location (upper) and the number
    of images per subspecies and hour (lower)'
  prefs: []
  type: TYPE_NORMAL
- en: The `Subspecies` images have a large variety of weights and heights. *Figure
    6.13* shows this distribution, using a boxplot, for the weight and the height.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Image size distribution per each subspecies – width (upper) and
    height (lower)'
  prefs: []
  type: TYPE_NORMAL
- en: '**VSH Italian honey bee** has the largest average and also the largest variance
    for both width and height. **Western honey bee**, **Carniolan honey be***e*, and
    **Mixed local stock 2** have the most compact distribution of weight and height
    (and lower variance). The *Italian honey bee*, the most numerous subspecies, shows
    both a small median and a large variance, with a lot of outliers. In the next
    figure, we show the weight and the height distribution combined on the same scatter
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_06_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Image size distribution per each subspecies – scatter plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure shows this distribution, using a scatter plot, for the
    weight and the height. The code for this visualization is shown in the following.
    First, we start by defining a function to draw a scatter plot, with the image
    width on the `x` scale and the image height on the `y` scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use the function defined above to draw the scatter plot for each subspecies.
    Each function call will create a trace, and we add the traces to the Plotly plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Health
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 6.15* shows the distribution of images with various health problems.
    The majority of images are for **healthy** bees (3384), followed by **few varrao,
    hive beetles** (579), **Varroa, Small Hive Beetles** (472), and **ant problems**
    (457):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, line  Description automatically
    generated](img/B20963_06_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Number of images with different health problems'
  prefs: []
  type: TYPE_NORMAL
- en: If we analyze the number of images per subspecies and health (see *Figure 6.16*),
    we can observe that only a reduced number of combinations of health and subspecies
    values are present. Most images are **healthy** **Italian honey bee** (1972),
    followed by **few varroa, hive beetles**, then **Italian honey bee** (579), and
    lastly, **healthy** **Russian honey bee** (527). The unknown subspecies are either
    **healthy** (177) or **hive being robbed** (251).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated with
    low confidence](img/B20963_06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Number of images per subspecies and bees with different health
    problems'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.17*, we plot the number of images per location, subspecies, and
    health problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a graph  Description automatically generated with low confidence](img/B20963_06_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: Number of images per location, subspecies, and health problems'
  prefs: []
  type: TYPE_NORMAL
- en: Others
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a small number of images with bees carrying pollen. *Figure 6.18*
    shows a few images of bees carrying pollen and some not carrying pollen. All the
    bees are from only one caste: the worker caste.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing collage, screenshot, text  Description automatically
    generated](img/B20963_06_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Selection of images with bees carrying pollen and not carrying
    pollen'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use a Sankey diagram, drawn with the `plotly_sankey` script from the `plotly_utils`
    utility script module, to draw the summary graph in *Figure 6.19*. A Sankey diagram
    is used mainly to visualize processes or flows, for example, the production of
    energy, with its sources and consumers, in an economy. I use it here with another
    purpose, to summarize the distribution of data with multiple features. It shows
    on the same plot the distribution of images per date, time, location, ZIP code,
    subspecies, and health. The adaptation code for the Sankey diagram is not given
    here for limited space reasons (refer *Reference 2* for the code samples associated
    with the chapter); we just include the code to adapt the honey bee data for the
    use of this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The visualization in *Figure 6.19*, a funnel-like graph, allows us to capture
    in one single graph the relationship between multiple features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Summary of images'
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we analyzed the distribution of features in the dataset. Now, we
    have a better understanding of the data in the dataset. In the following section
    of this chapter, we will start preparation to build a machine learning model to
    classify images on subspecies, which is the second and more important objective
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Subspecies classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective of this section will be to use the images investigated until
    now to build a machine learning model that will correctly predict the subspecies.
    Since we have one dataset only, we will start by splitting the data into three
    subsets: for train, validation, and test data. We will use train and validation
    during the training process: the train data to feed the model and the validation
    data to verify how well the model predicts the class (i.e., the subspecies) with
    the new data. Then, the model trained and validated will be used to predict the
    class in the test set, which was not used either in train or validation.'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we split the data into `train` and `test`, using an 80%–20% split. Then,
    the `train` data is split again, in to train and validation, using the same 80%–20%
    split. The splits are performed using `stratify` with `subspecies` as a parameter,
    ensuring balanced subsets that respect the overall distribution of classes in
    the subsampled sets of train, validation, and test. The percent values for train/validation/test
    splits are chosen arbitrarily here and are not the result of a study or optimization.
    In your experiments, you can work with different values for train/validation/test
    subsets, and you can also choose to not use `stratify`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Ultimately, we will have three subsets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train set rows: 3309'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation set rows: 828'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Test set rows: 1035'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will split the images into subsets, corresponding to the image names’ subsets.
    We create functions to read images and rescale them all to the same dimension,
    as specified in the configuration structure we defined, using `skimage.io` and
    `opencv`. We decided to rescale all images to 100 x 100 pixels. Our decision was
    based on the analysis of the images’ size distribution. You can choose to modify
    the code in the notebook given at *Reference 3* and experiment with different
    image sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code reads an image (using `skimage.io`) and resizes it according
    to the size set in the configuration. You can change the configuration and resize
    the image, using different values for the image height and width:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The code below reads and resizes an image using opencv. The function differs
    from the previous one shown above just by the method to read the image file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We then apply one of these functions to all dataset files to read and rescale
    the images in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also create dummy variables corresponding to the categorical target variables.
    We prefer to use this approach because we will prepare a model for multiclass
    classification that outputs probabilities per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With that, we’ve shown how to read and rescale our images. Next, we will see
    how we can multiply our images to have more data in the train set so that we present
    to the model a larger variety of data.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a deep learning model to classify the subspecies in our images.
    Typically, deep learning models perform better when trained with a larger amount
    of data. Using data augmentation, we also create a larger variety in the data,
    which is also beneficial for the model quality. Our model will improve its generalization
    if we expose it to more diverse data during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a component for data augmentation, based on `ImageDataGenerator`
    from `keras.preprocessing.image`. In this notebook, we will use Keras to build
    various components of the model. The `ImageDataGenerator` component is initialized
    with various parameters to create random variations of the training dataset, by
    applying:'
  prefs: []
  type: TYPE_NORMAL
- en: a rotation (in a range of 0 to 180 degrees) of the original images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a zoom (10%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a shift in horizontal and vertical directions (10%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a horizontal and vertical shift (10%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These variations can be controlled separately. Not all use cases will allow,
    or benefit from applying, all the transformations listed above (think about, for
    example, images with buildings or other landmarks, for which rotations do not
    make too much sense). The following code can be used in our case to initialize
    and fit the image generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We then move on to building and training a baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a baseline model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is almost always recommended to start your work with a simple model and then
    conduct an error analysis. Based on the error analysis, you will have to further
    refine your model. If, for example, you observe that you obtained, with your baseline
    model, a large error for training, you need to start by improving the training.
    You can do this by perhaps adding more data, improving your data labeling, or
    creating better features. If your training error is small but you have instead
    a high validation error, it means that your model probably overfits on the training
    data. In such a case, you need to try to improve your model generalization. You
    can try various techniques to improve your model generalization. See more about
    this kind of analysis in *Reference 4* at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `Keras` library to define our models. Keras (see *Reference
    5*) is a wrapper over the TensorFlow machine learning platform (see *Reference
    6*). It allows you to create powerful deep learning models by defining a sequential
    structure with specialized layers. We will add the following layers to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: One `Conv2D` layer, with 16 filters of dimension 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One `MaxPooling2D` layer, with reduction factor 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One convolutional layer, with 16 filters of dimension 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Flatten` layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer, with the dimension the number of classes of the subspecies target
    feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding architecture is a very simple example of a **convolutional neural
    network**. The role of the `convolutional2d` layer is to apply sliding convolutional
    filters to a 2D input. The `maxpool2d` layer will down-sample the input along
    its spatial dimension (width and height) by taking the maximum value over an input
    window (see *Reference 5* for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to build the described architecture will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 6.20*, we show the summary information of the model. As you can
    see, the total number of trainable parameters is 282,775:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B20963_06_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Summary of the baseline model'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the baseline, we start with a small model and train for a reduced number
    of epochs. The size of the input images is 100 x 100 x 3 (as we explained previously).
    We will train this model for five epochs. The batch size is set to 32\. The code
    to run the training is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 6.21* shows the training log for the baseline model. We are not saving
    the best model version during training; the model weights at the last step will
    be used for the test.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B20963_06_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: The training log for the baseline model. The training loss and
    accuracy and validation loss and accuracy for each step are shown'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loss and accuracy are updated after each batch, and the validation
    loss and accuracy are calculated at the end of each epoch. Next, with the model
    trained and validated, we will evaluate the test set loss and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 6.22*, we show the training and validation loss and the training
    and validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a graph of a graph of a graph of a graph of a graph of a graph
    of a graph of a graph of a graph of a graph of a graph of a graph of  Description
    automatically generated with low confidence](img/B20963_06_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: Baseline model – the training and validation accuracy (left) and
    the training and validation loss (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results obtained are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test loss: **0.42**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Test accuracy: **0.82**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test loss refers to the loss function, a mathematical function that measures
    the difference between the predicted values and the true values. By measuring
    this value during training, for both the train and validation sets, we can monitor
    how the model learns and improves its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a metrics classification report (`metrics.classification_report`) from
    `sklearn`, we calculate the precision, recall, f1-score, and accuracy per each
    class in train data. The code for that is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 6.23*, we show this classification report for the test set, where
    we applied the baseline model fit with train data. The macro average of the precision,
    recall, and f1-score are **0.78**, **0.72**, and **0.74**, respectively (the support
    data is **1035**). The weighted average precision, recall, and f1-score are **0.82**,
    **0.83**, and **0.82**, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B20963_06_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Classification report for test data using the baseline model'
  prefs: []
  type: TYPE_NORMAL
- en: These weighted average scores are higher because unlike the simple averages
    of all class scores, these are weighted averages, so the higher scores associated
    with better-represented classes will have a higher contribution to the overall
    average. The worst scores for precision/class are for **1 Mixed local stock 2**
    (0.52) and **VSH Italian honey bee** (0.68). The worst overall score is for **VSH
    Italian honey bee,** where the recall is 0.33.
  prefs: []
  type: TYPE_NORMAL
- en: Iteratively refining the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we go back now to the training and validation errors, we can see that validation
    and training accuracies are around 0.81 and 0.82.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to train the model, and to avoid overfitting, we will also
    introduce two `Dropout` layers, each with a coefficient of 0.4\. A `Dropout` layer
    is used as a regularization method in neural networks. Its purpose is to prevent
    overfitting and improve generalization of the model. The coefficient given as
    a parameter is the percentage of inputs randomly selected during training to be
    set to zero at each training epoch. The structure of the model is described in
    *Figure 6.24*. The number of trainable parameters will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B20963_06_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Refined model summary. Two Dropout layers were added'
  prefs: []
  type: TYPE_NORMAL
- en: We also extend the number of epochs to 10\. Let’s see the results in *Figure
    6.25*, where we show the training and validation accuracy and the training and
    validation loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a graph of a graph of a graph of a graph of a graph of a graph
    of a graph of a graph of a graph of a graph of a graph of a graph of  Description
    automatically generated with low confidence](img/B20963_06_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.25: Refined model (version 2) – the training and validation accuracy
    (left) and the training and validation loss (right)'
  prefs: []
  type: TYPE_NORMAL
- en: The final training loss is 0.32, and the final training accuracy is 0.87\. The
    final validation loss is 0.28, and the final validation accuracy is 0.88\. These
    are improved results. Of course, training accuracy is mainly improved due to the
    fact that we trained for more epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated with
    low confidence](img/B20963_06_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Classification report for test data using the second refined model
    (the training epochs increased to 10 and Dropout layers added)'
  prefs: []
  type: TYPE_NORMAL
- en: As per validation accuracy, the result is due to both more training epochs as
    well as adding `Dropout` layers, which kept overfitting under control. Let’s now
    check the test loss and accuracy as well as look at the entire classification
    report for test data.
  prefs: []
  type: TYPE_NORMAL
- en: Both macro averaged and weighted averaged metrics improved for precision, recall,
    and f1-score. Also, we can see much-improved precision, recall, and f1-scores
    for the classes with small scores obtained with the baseline. **1 Mixed local
    stock 2** had 0.52 precision, and now, precision is 0.63\. As for **VSH Italian
    honey bee**, precision was 0.68 and is now 0.97\. We see a degradation of the
    precision of **Western honey bee**, but the support for this minority class is
    only 7, so this result is expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue to refine our model to improve the validation and test metrics
    – in other words, to improve the model performance. In the next iteration, we
    will increase the number of training epochs to 50\. Also, we will add three callback
    functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A learning rate scheduler, to implement a nonlinear function for variation of
    the learning rate. By changing the learning rate at each epoch, we can improve
    the training process. The function we introduce to control the learning rate will
    gradually decrease the learning function value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An early stopper to stop training epochs, based on the loss function evolution
    (if loss does not improve for a certain number of epochs) and a patience factor
    (the number of epochs, after which, if we don’t see any improvement of the monitored
    function, we stop training).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A check pointer to save the best-performing models every time we obtain the
    best accuracy. This will allow us to use not the model parameters at the last
    training epoch but, instead, the best-performing model of all the epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for the three callback functions is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The code to fit the model is also given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The training can take as many as the maximum number of epochs assigned or, if
    the early stopping criteria are met (i.e., no loss function improvement after
    a number of epochs equal to the patience factor), it might end earlier. Either
    way, the model for which the best validation accuracy was achieved is saved and
    will be used for testing.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6.27*, we show the evolution of the training and validation accuracy
    and the training and validation loss for this further refined model. The final
    training loss obtained is 0.18, and the final training accuracy is 0.93\. For
    validation, the last validation loss is 0.21, and the last validation accuracy
    is 0.91\. The learning rate at the final epoch is 6.08e-4\. The best validation
    accuracy obtained was for epoch 46, 0.92.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, plot, diagram  Description automatically
    generated](img/B20963_06_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Refined model (version 3, with a learning rate scheduler, early
    stop, and checkpoint) – the training and validation accuracy (left) and the training
    and validation loss (right)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the model checkpoint saved (for epoch 46) to predict for the test data.
    In *Figure 6.28*, we show the classification report for the third model.
  prefs: []
  type: TYPE_NORMAL
- en: Macro average metrics are further improved to the precision, recall, and f1-score
    values of 0.88, 0.90, and 0.89, respectively. The weighted average values for
    precision, recall, and f1-score are also improved to 0.91, 0.90, and 0.90, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated with
    low confidence](img/B20963_06_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Classification report for test data using the third refined model
    (the training epochs increased to 50 and a learning rate scheduler, early stopper,
    and model checkpoint added)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will stop the process of iteratively improving the model. You can continue
    to refine it. You can try to add more convolutional and maxpool layers, using
    a different number of kernels and values of stride to work with different hyperparameters,
    including a different batch size or learning rate scheduler. Also, you can change
    the optimization scheme. Another way to change the model is to control, through
    data augmentation, the balance of the class images (currently, the bee images
    are unbalanced with respect to the `subspecies` class).
  prefs: []
  type: TYPE_NORMAL
- en: You can also experiment with various data augmentation parameters and try to
    use a different data augmentation solution. See *Reference 5* for one example
    of an image data augmentation library that is very much used currently, **Albumentations**,
    created by a group of data scientists, researchers, and computer vision engineers,
    including the famous Kaggle Grandmaster Vladimir Iglovikov.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by introducing a new dataset, with metadata about
    images collected at different dates and in different locations, including various
    bee subspecies that have various diseases. We also introduced a few functions
    to read, rescale, and extract features from images, based on `skimage.io` and
    opencv (`cv2`).
  prefs: []
  type: TYPE_NORMAL
- en: We used a newly created utility script to visualize tabular data, based on Plotly,
    and created insightful visualizations by leveraging the flexibility of Plotly
    to create customized graphics. We also created visualization functions for images.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the detailed EDA, we moved on to building a predictive model for bee
    subspecies. Here, we introduced a method for data augmentation to multiply the
    initial available training data, by creating variations (rotations, zoom, shift,
    and mirroring) from the original image sets. We split the data into train, validation,
    and test subsets, using `stratify` to account for the class imbalance when randomly
    sampling the three subsets. We started by training and validating a baseline model,
    and then, after performing error analysis, we gradually improved the initial model,
    by adding more steps, introducing `Dropout` layers, and then using several callbacks:
    the learning rate scheduler, early stopper, and model checkpoints. We analyzed
    the iterative improvement for the training, validation, and test errors, looking
    not only at the training and validation loss and accuracy but also at the classification
    report for test data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will introduce techniques and tools for text data
    analysis, showing you how to prepare your data to create baseline models using
    text data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The BeeImage Dataset: Annotated Honey Bee Images: [https://www.kaggle.com/datasets/jenny18/honey-bee-annotated-images](https://www.kaggle.com/datasets/jenny18/honey-bee-annotated-images)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`plotly-script` and Kaggle Utility Script: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/plotly-utils.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/plotly-utils.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Honeybee Subspecies Classification, Kaggle Notebook: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/honeybee-subspecies-classification.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-06/honeybee-subspecies-classification.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Andrew Ng, Machine Learning Yearning: [https://info.deeplearning.ai/machine-learning-yearning-book](https://info.deeplearning.ai/machine-learning-yearning-book)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keras: [https://keras.io/](https://keras.io/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'TensorFlow: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using Albumentations with Tensorflow: [https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/tensorflow-example.ipynb](https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/tensorflow-example.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code9220780366773140.png)'
  prefs: []
  type: TYPE_IMG
