["```py\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\nnp.random.seed(1000)\n\nnb_samples = 1000\nnsb = int(nb_samples / 4)\n\nX = np.zeros((nb_samples, 2))\nY = np.zeros((nb_samples, ))\n\nX[0:nsb, :] = np.random.multivariate_normal([1.0, -1.0], np.diag([0.1, 0.1]), size=nsb)\nY[0:nsb] = 0.0\n\nX[nsb:(2 * nsb), :] = np.random.multivariate_normal([1.0, 1.0], np.diag([0.1, 0.1]), size=nsb)\nY[nsb:(2 * nsb)] = 1.0\n\nX[(2 * nsb):(3 * nsb), :] = np.random.multivariate_normal([-1.0, 1.0], np.diag([0.1, 0.1]), size=nsb)\nY[(2 * nsb):(3 * nsb)] = 0.0\n\nX[(3 * nsb):, :] = np.random.multivariate_normal([-1.0, -1.0], np.diag([0.1, 0.1]), size=nsb)\nY[(3 * nsb):] = 1.0\n\nss = StandardScaler()\nX = ss.fit_transform(X)\n\nX, Y = shuffle(X, Y, random_state=1000)\n```", "```py\nimport numpy as np\n\nfrom multiprocessing import cpu_count\n\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import cross_val_score\n\npc = Perceptron(penalty='l2', alpha=0.1, max_iter=1000, n_jobs=cpu_count(), random_state=1000)\nprint(np.mean(cross_val_score(pc, X, Y, cv=10)))\n0.498\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\n\nmodel.add(Dense(4, input_dim=2))\nmodel.add(Activation('tanh'))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n```", "```py\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1000)\n\nmodel.fit(X_train, \n          to_categorical(Y_train, num_classes=2), \n          epochs=100, \n          batch_size=32,\n          validation_data=(X_test, to_categorical(Y_test, num_classes=2)))\n\nTrain on 700 samples, validate on 300 samples\nEpoch 1/100\n700/700 [==============================] - 1s 2ms/step - loss: 0.7227 - acc: 0.4929 - val_loss: 0.6943 - val_acc: 0.5933\nEpoch 2/100\n700/700 [==============================] - 0s 267us/step - loss: 0.7037 - acc: 0.5371 - val_loss: 0.6801 - val_acc: 0.6100\nEpoch 3/100\n700/700 [==============================] - 0s 247us/step - loss: 0.6875 - acc: 0.5871 - val_loss: 0.6675 - val_acc: 0.6733\n\n...\n\nEpoch 98/100\n700/700 [==============================] - 0s 236us/step - loss: 0.0385 - acc: 0.9986 - val_loss: 0.0361 - val_acc: 1.0000\nEpoch 99/100\n700/700 [==============================] - 0s 261us/step - loss: 0.0378 - acc: 0.9986 - val_loss: 0.0355 - val_acc: 1.0000\nEpoch 100/100\n700/700 [==============================] - 0s 250us/step - loss: 0.0371 - acc: 0.9986 - val_loss: 0.0347 - val_acc: 1.0000\n```", "```py\nfrom keras.optimizers import SGD\n\n...\n\nsgd = SGD(lr=0.0001, momentum=0.8, nesterov=True)\n\nmodel.compile(optimizer=sgd,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nfrom keras.optimizers import RMSprop\n\n...\n\nrms_prop = RMSprop(lr=0.0001, rho=0.8, epsilon=1e-6, decay=1e-2)\n\nmodel.compile(optimizer=rms_prop,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nfrom keras.optimizers import Adam\n\n...\n\nadam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.9, epsilon=1e-6, decay=1e-2)\n\nmodel.compile(optimizer=adam,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nfrom keras.optimizers import Adagrad\n\n...\n\nadagrad = Adagrad(lr=0.0001, epsilon=1e-6, decay=1e-2)\n\nmodel.compile(optimizer=adagrad,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nfrom keras.optimizers import Adadelta\n\n...\n\nadadelta = Adadelta(lr=0.0001, rho=0.9, epsilon=1e-6, decay=1e-2)\n\nmodel.compile(optimizer=adadelta,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nfrom keras.layers import Dense\nfrom keras.regularizers import l2\n\n...\n\nmodel.add(Dense(128, kernel_regularizer=l2(0.05)))\n```", "```py\nfrom keras.layers import Dense\nfrom keras.constraints import maxnorm\n\n...\n\nmodel.add(Dense(128, kernel_constraint=maxnorm(1.5)))\n```", "```py\nimport numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nwidth = height = X_train.shape[1]\n\nX_train = X_train.reshape((X_train.shape[0], width * height)).astype(np.float32) / 255.0\nX_test = X_test.reshape((X_test.shape[0], width * height)).astype(np.float32) / 255.0\n\nY_train = to_categorical(Y_train, num_classes=10)\nY_test = to_categorical(Y_test, num_classes=10)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Dense(2048, input_shape=(width * height, )))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer=Adam(lr=0.0001, decay=1e-6),\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, Y_train,\n                    epochs=200,\n                    batch_size=256,\n                    validation_data=(X_test, Y_test))\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/200\n60000/60000 [==============================] - 11s 189us/step - loss: 0.4026 - acc: 0.8980 - val_loss: 0.1601 - val_acc: 0.9523\nEpoch 2/200\n60000/60000 [==============================] - 7s 116us/step - loss: 0.1338 - acc: 0.9621 - val_loss: 0.1062 - val_acc: 0.9669\nEpoch 3/200\n60000/60000 [==============================] - 7s 124us/step - loss: 0.0872 - acc: 0.9744 - val_loss: 0.0869 - val_acc: 0.9732\n\n...\n\nEpoch 199/200\n60000/60000 [==============================] - 7s 114us/step - loss: 1.1935e-07 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 0.9838\nEpoch 200/200\n60000/60000 [==============================] - 7s 116us/step - loss: 1.1935e-07 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 0.9840\n```", "```py\nfrom keras.constraints import maxnorm\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n\nmodel.add(Dropout(0.25, input_shape=(width * height, ), seed=1000))\n\nmodel.add(Dense(2048, kernel_initializer='uniform', kernel_constraint=maxnorm(2.0)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(Dense(1024, kernel_initializer='uniform', kernel_constraint=maxnorm(2.0)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(Dense(1024, kernel_initializer='uniform', kernel_constraint=maxnorm(2.0)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer=SGD(lr=0.1, momentum=0.9),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, Y_train,\n                    epochs=200,\n                    batch_size=256,\n                    validation_data=(X_test, Y_test))\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/200\n60000/60000 [==============================] - 11s 189us/step - loss: 0.4964 - acc: 0.8396 - val_loss: 0.1592 - val_acc: 0.9511\nEpoch 2/200\n60000/60000 [==============================] - 6s 97us/step - loss: 0.2300 - acc: 0.9300 - val_loss: 0.1081 - val_acc: 0.9645\nEpoch 3/200\n60000/60000 [==============================] - 6s 93us/step - loss: 0.1867 - acc: 0.9435 - val_loss: 0.0941 - val_acc: 0.9713\n\n...\n\nEpoch 199/200\n60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.0473 - val_acc: 0.9884\nEpoch 200/200\n60000/60000 [==============================] - 6s 101us/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0484 - val_acc: 0.9883\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, BatchNormalization\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Dense(2048, input_shape=(width * height, )))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(10))\nmodel.add(BatchNormalization())\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer=Adam(lr=0.001, decay=1e-6),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, Y_train,\n                    epochs=200,\n                    batch_size=256,\n                    validation_data=(X_test, Y_test))\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/200\n60000/60000 [==============================] - 16s 274us/step - loss: 0.3848 - acc: 0.9558 - val_loss: 0.3338 - val_acc: 0.9736\nEpoch 2/200\n60000/60000 [==============================] - 8s 139us/step - loss: 0.1977 - acc: 0.9844 - val_loss: 0.1904 - val_acc: 0.9789\nEpoch 3/200\n60000/60000 [==============================] - 8s 137us/step - loss: 0.1292 - acc: 0.9903 - val_loss: 0.1397 - val_acc: 0.9835\n\n...\n\nEpoch 199/200\n60000/60000 [==============================] - 8s 132us/step - loss: 4.7805e-05 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9877\nEpoch 200/200\n60000/60000 [==============================] - 8s 133us/step - loss: 2.6056e-05 - acc: 1.0000 - val_loss: 0.0593 - val_acc: 0.9879\n```"]