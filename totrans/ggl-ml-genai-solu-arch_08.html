<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer117" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-125"><a id="_idTextAnchor187" class="calibre6 pcalibre pcalibre1"/>6</h1>
<h1 id="_idParaDest-126" class="calibre5"><a id="_idTextAnchor188" class="calibre6 pcalibre pcalibre1"/>Diving Deeper – Preparing and Processing Data for AI/ML Workloads on Google Cloud</h1>
<p class="calibre3">In the previous chapter, we did some very rudimentary data exploration by looking at a few details relating to our dataset, using functions such as <strong class="source-inline">pandas.DataFrame.info()</strong> and <strong class="source-inline">pandas.DataFrame.head()</strong>. In this chapter, we will dive deeper into the realm of data exploration and preparation for data science workloads, as represented by the section highlighted in blue in the data science life-cycle diagram shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.1</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer101">
<img alt="Figure 6.1: Data exploration and processing" src="image/B18143_06_1.jpg" class="calibre105"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.1: Data exploration and processing</p>
<p class="calibre3">In the early stages of a typical<a id="_idIndexMarker609" class="calibre6 pcalibre pcalibre1"/> data science project, you would likely perform many of the data exploration and preparation steps in Jupyter notebooks, which, as we have seen, are useful for experimenting with small datasets. When you bring your workload into production, however, you are likely to use much larger datasets, in which case you would usually need to use different tools for processing your data. Google is seen as an industry leader when it comes to large-scale data processing, analytics, and AI/ML. For example, Google Cloud BigQuery is well established as one of the most popular data warehouse services in the industry, and Google Cloud has many additional industry-leading services for data processing and <span>analytics workloads.</span></p>
<p class="calibre3">In this chapter, we will learn how to explore, visualize, and prepare data for ML use cases with tools such as Vertex AI, BigQuery, Dataproc, and Cloud Composer. Furthermore, we will dive into processing streaming data on the fly with Dataflow, and introduce the fundamentals of data pipelines. By the end of this chapter, you will be able to create, build, and run data pipelines on Google Cloud, equipping you with the necessary skills to take on complex data processing tasks in today’s fast-paced, <span>data-driven world.</span></p>
<p class="calibre3">This chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Prerequisites and <span>basic concepts</span></li>
<li class="calibre8">Ingesting data into <span>Google Cloud</span></li>
<li class="calibre8">Exploring and <span>visualizing data</span></li>
<li class="calibre8">Cleaning and preparing the data for <span>ML workloads</span></li>
<li class="calibre8">An introduction to <span>data pipelines</span></li>
<li class="calibre8">Processing batch and <span>streaming data</span></li>
<li class="calibre8">Building and running data pipelines on <span>Google Cloud</span></li>
</ul>
<p class="calibre3">Let’s begin by discussing the prerequisites for <span>this chapter.</span></p>
<h1 id="_idParaDest-127" class="calibre5"><a id="_idTextAnchor189" class="calibre6 pcalibre pcalibre1"/>Prerequisites for this chapter</h1>
<p class="calibre3">The activities in this section need to be completed before we can start to perform the primary activities in <span>this chapter.</span></p>
<h2 id="_idParaDest-128" class="calibre9"><a id="_idTextAnchor190" class="calibre6 pcalibre pcalibre1"/>Enabling APIs</h2>
<p class="calibre3">In addition to the methods we discussed<a id="_idIndexMarker610" class="calibre6 pcalibre pcalibre1"/> in earlier chapters to enable Google Cloud APIs, such as the Google Cloud Shell or being prompted in the Google Cloud console, you can also proactively search for an API in order to enable it in the console. To do that, you would perform the following steps, in which <em class="italic">[service/API name]</em> is the name of the Service/API you wish <span>to enable:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu → <strong class="bold">APIs &amp; Services</strong> → <span><strong class="bold">Library</strong></span><span>.</span></li>
<li class="calibre8">Search for <em class="italic">[service name]</em> in the <span>search box.</span></li>
<li class="calibre8">Select the API from the list <span>of results.</span></li>
<li class="calibre8">On the page that displays information about the API, <span>click </span><span><strong class="bold">Enable</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Perform the preceding steps for each of the following <span>service/API names:</span></p>
<ul class="calibre16">
<li class="calibre8">Compute <span>Engine API</span></li>
<li class="calibre8">Cloud <span>Scheduler API</span></li>
<li class="calibre8"><span>Dataflow API</span></li>
<li class="calibre8">Data <span>Pipelines API</span></li>
<li class="calibre8">Cloud <span>Dataproc API</span></li>
<li class="calibre8">Cloud <span>Composer API</span></li>
<li class="calibre8">Cloud <span>Pub/Sub API</span></li>
</ul>
<p class="calibre3">After you have enabled<a id="_idIndexMarker611" class="calibre6 pcalibre pcalibre1"/> the required APIs, we’re ready to move on to the <span>next section.</span></p>
<h2 id="_idParaDest-129" class="calibre9"><a id="_idTextAnchor191" class="calibre6 pcalibre pcalibre1"/>IAM permissions</h2>
<p class="calibre3">In this section, we will set up the <strong class="bold">identity and access management</strong> (<strong class="bold">IAM</strong>) permissions required<a id="_idIndexMarker612" class="calibre6 pcalibre pcalibre1"/> to enable the activities that come later in <span>this chapter.</span></p>
<h3 class="calibre11">Service accounts</h3>
<p class="calibre3">In previous chapters, I mentioned<a id="_idIndexMarker613" class="calibre6 pcalibre pcalibre1"/> that there are multiple<a id="_idIndexMarker614" class="calibre6 pcalibre pcalibre1"/> ways to authenticate with Google Cloud services and that the API keys you used in <a href="B18143_04.xhtml#_idTextAnchor146" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 4</em></span></a> were the simplest authentication methods. Now that we’re advancing to more complex use cases, we will begin to use a more advanced form of authentication, referred to as <span>service accounts.</span></p>
<p class="calibre3">Google Cloud service accounts<a id="_idIndexMarker615" class="calibre6 pcalibre pcalibre1"/> are special accounts used by Google Cloud services, applications, and <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) to interact with and authenticate to other Google Cloud resources. They are an interesting concept because, in addition to being resources, they are also considered to be identities or principals, just like people, and just like people, they have their own email addresses and permissions associated with them. However, these email addresses and permissions apply to the machines and applications that use them, rather than to people. Service accounts provide a way for machines and applications to have an identity when those systems want to perform an activity that requires authentication with Google Cloud APIs <span>and resources.</span></p>
<p class="calibre3">We’re going to create<a id="_idIndexMarker616" class="calibre6 pcalibre pcalibre1"/> a service account with<a id="_idIndexMarker617" class="calibre6 pcalibre pcalibre1"/> the required permissions for the activities we will perform in <span>this chapter.</span></p>
<h4 class="calibre20">Data processing service account</h4>
<p class="calibre3">Considering that we’re going<a id="_idIndexMarker618" class="calibre6 pcalibre pcalibre1"/> to use multiple Google Cloud services<a id="_idIndexMarker619" class="calibre6 pcalibre pcalibre1"/> to process data in various ways in this chapter, we will create a service account with permissions mainly related to Google Cloud data processing<a id="_idIndexMarker620" class="calibre6 pcalibre pcalibre1"/> services, such as BigQuery, Cloud Composer, Dataflow, Dataproc, <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>), <span>and Pub/Sub.</span></p>
<p class="calibre3">Perform the following steps to create the required <span>service account:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <strong class="bold">IAM &amp; Admin</strong> → <span><strong class="bold">Service accounts</strong></span><span>.</span></li>
<li class="calibre8">Select <strong class="bold">Create </strong><span><strong class="bold">service account</strong></span><span>.</span></li>
<li class="calibre8">For the service account name, <span>enter </span><span><strong class="source-inline">data-processing-sa</strong></span><span>.</span></li>
<li class="calibre8">In the section titled <strong class="bold">Grant this service account access to project</strong>, add the roles shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.2</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer102">
<img alt="Figure 6.2: Dataflow worker service account permissions" src="image/B18143_06_2.jpg" class="calibre106"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.2: Dataflow worker service account permissions</p>
<ol class="calibre7">
<li value="5" class="calibre8"><span>Select </span><span><strong class="bold">Done</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Our service account<a id="_idIndexMarker621" class="calibre6 pcalibre pcalibre1"/> is now ready to be used later<a id="_idIndexMarker622" class="calibre6 pcalibre pcalibre1"/> in <span>this chapter.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In the service account we just created, we also added the <strong class="bold">Service Account User</strong> permission. This is because, in some of the use cases we will implement, our service account will also need to temporarily impersonate or “act as” other service accounts or identities. For more information on the concept <a id="_idIndexMarker623" class="calibre6 pcalibre pcalibre1"/>of impersonating service accounts, see the following <span>documentation: </span><a href="https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate</span></a><span>.</span></p>
<h2 id="_idParaDest-130" class="calibre9"><a id="_idTextAnchor192" class="calibre6 pcalibre pcalibre1"/>Cloud Storage bucket folders</h2>
<p class="calibre3">We will use a Cloud Storage bucket <a id="_idIndexMarker624" class="calibre6 pcalibre pcalibre1"/>to store data for the activities later in this chapter. We already created a bucket in <a href="B18143_04.xhtml#_idTextAnchor146" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 4</em></span></a>, so we can simply add some folders to the bucket for storing our data. Perform the following steps to create <span>the folders:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <strong class="bold">Cloud Storage</strong> → <span><strong class="bold">Buckets</strong></span><span>.</span></li>
<li class="calibre8">Click on the name of the bucket you created in <a href="B18143_04.xhtml#_idTextAnchor146" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 4</em></span></a><span>.</span></li>
<li class="calibre8">Select <span><strong class="bold">Create folder</strong></span><span>.</span></li>
<li class="calibre8">Name <span>it </span><span><strong class="source-inline">data</strong></span><span>.</span></li>
<li class="calibre8"><span>Select </span><span><strong class="bold">Create</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Repeat the preceding steps with the following additional <span>folder names:</span></p>
<ul class="calibre16">
<li class="calibre8"><span><strong class="source-inline">code</strong></span></li>
<li class="calibre8"><span><strong class="source-inline">dataflow</strong></span></li>
<li class="calibre8"><span><strong class="source-inline">pyspark-airbnb</strong></span></li>
</ul>
<p class="calibre3">Our folders are now ready to store <span>our data.</span></p>
<h2 id="_idParaDest-131" class="calibre9"><a id="_idTextAnchor193" class="calibre6 pcalibre pcalibre1"/>Uploading data</h2>
<p class="calibre3">We will use a file<a id="_idIndexMarker625" class="calibre6 pcalibre pcalibre1"/> named <strong class="source-inline">AB_NYC_2019.csv</strong> as a dataset for some of the activities in this chapter. In the clone of our GitHub repository that you created on your local machine in <a href="B18143_04.xhtml#_idTextAnchor146" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 4</em></span></a>, you will find that file in a directory named <strong class="source-inline">data</strong>, which exists within the directory <span>named </span><span><strong class="source-inline">Chapter-06</strong></span><span>.</span></p>
<p class="calibre3">Therefore, you should find the file at the following path on your local machine (the slashes will be reversed if you’re using <span>Microsoft Windows):</span></p>
<p class="calibre3"><strong class="source-inline">[Location in which you cloned our </strong><span><strong class="source-inline">GitHub repository]/</strong></span><span><strong class="source-inline">Chapter-06</strong></span><span><strong class="source-inline">/data/AB_NYC_2019.csv</strong></span></p>
<p class="calibre3">In order to upload this file, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Navigate into the <strong class="source-inline">data</strong> folder you created in GCS in the <span>previous section.</span></li>
<li class="calibre8">Click <span><strong class="bold">Upload files</strong></span><span>.</span></li>
<li class="calibre8">Now, select the <strong class="source-inline">AB_NYC_2019.csv</strong> file by navigating to the <strong class="source-inline">Chapter-06</strong><strong class="source-inline">/data</strong> directory in the clone of our GitHub repository that you created on your <span>local machine.</span></li>
</ol>
<p class="calibre3">Now that we’ve completed<a id="_idIndexMarker626" class="calibre6 pcalibre pcalibre1"/> the prerequisites, let’s discuss some important industry concepts that we will dive into in <span>this chapter.</span></p>
<h1 id="_idParaDest-132" class="calibre5"><a id="_idTextAnchor194" class="calibre6 pcalibre pcalibre1"/>Fundamental concepts in this chapter</h1>
<p class="calibre3">In this book, we aim to provide you with knowledge not only regarding how to use the relevant Google Cloud services for various types of workloads but also regarding important industry concepts that relate to each of the relevant technologies. In this section, we briefly cover concepts that provide additional context for this chapter’s <span>learning activities.</span></p>
<h2 id="_idParaDest-133" class="calibre9"><a id="_idTextAnchor195" class="calibre6 pcalibre pcalibre1"/>Ingesting data into Google Cloud</h2>
<p class="calibre3">In previous chapters, you already<a id="_idIndexMarker627" class="calibre6 pcalibre pcalibre1"/> performed steps to upload data to GCS<a id="_idIndexMarker628" class="calibre6 pcalibre pcalibre1"/> and Google Cloud BigQuery. In addition to performing bulk uploads to GCS and BigQuery, it’s also possible to stream data into those services. You will see this in action in <span>this chapter.</span></p>
<p class="calibre3">This section provides a more holistic overview of Google Cloud’s data ingestion options. We will only cover Google Cloud’s services here, but there are also countless options of third-party database and data management services that you can run on Google Cloud, such as MongoDB, Cassandra, Neo4j, and many others that are available through the Google <span>Cloud Marketplace.</span></p>
<p class="calibre3">For streaming data use cases, which we will describe in more detail later in this chapter, you can use Cloud Pub/Sub to ingest data from sources such as IoT devices or website clickstream feeds. Dataflow can also be used to ingest data from various sources, transform it, and write it to other Google Cloud services such as BigQuery, Bigtable, or Cloud Storage. As we will discuss later in this chapter, Dataflow also supports batch data processing <span>use cases.</span></p>
<p class="calibre3">Google Cloud Dataproc can be used to ingest data from <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>)-compatible sources or other distributed<a id="_idIndexMarker629" class="calibre6 pcalibre pcalibre1"/> storage systems, and Google Cloud Data Fusion can also be used to ingest data from various sources, transform it, and write it to most Google Cloud storage and <span>database services.</span></p>
<p class="calibre3">For relational database data, you can use the Google Cloud <strong class="bold">Database Migration Service</strong> (<strong class="bold">DMS</strong>) to ingest data into Google Cloud <a id="_idIndexMarker630" class="calibre6 pcalibre pcalibre1"/>SQL, which is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server. You can also use standard SQL clients, import/export<a id="_idIndexMarker631" class="calibre6 pcalibre pcalibre1"/> tools, and third-party database migration tools<a id="_idIndexMarker632" class="calibre6 pcalibre pcalibre1"/> to ingest data into Cloud <span>SQL instances.</span></p>
<p class="calibre3">For non-relational database data, there are multiple<a id="_idIndexMarker633" class="calibre6 pcalibre pcalibre1"/> options, including <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">Google Cloud Bigtable, which is a fully managed, scalable<a id="_idIndexMarker634" class="calibre6 pcalibre pcalibre1"/> NoSQL database for large-scale, low-latency workloads. You can use the Bigtable HBase API or the Cloud Bigtable client libraries to ingest data <span>into Bigtable.</span></li>
<li class="calibre8">Google Cloud Firestore, which is a fully managed, serverless<a id="_idIndexMarker635" class="calibre6 pcalibre pcalibre1"/> NoSQL document database for web and mobile applications. Firestore provides client libraries, REST APIs, or gRPC APIs to <span>ingest data.</span></li>
<li class="calibre8">Some of you may also be familiar with Google Cloud Datastore, which was a separate Google Cloud non-relational database offering but has been somewhat merged with Google Cloud Firestore. To see more details regarding how these database options relate to each other, please see the Google Cloud documentation<a id="_idIndexMarker636" class="calibre6 pcalibre pcalibre1"/> at the following <span>link: </span><a href="https://cloud.google.com/datastore/docs/firestore-or-datastore" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/datastore/docs/firestore-or-datastore</span></a><span>.</span></li>
</ul>
<p class="calibre3">Now that we’ve covered many<a id="_idIndexMarker637" class="calibre6 pcalibre pcalibre1"/> of the options for ingesting data into Google Cloud, let’s discuss what kinds of things we may want to do with that data after ingesting it, starting with data transformation approaches, such as <strong class="bold">Extract, Transform, Load</strong> (<strong class="bold">ETL</strong>) and <strong class="bold">Extract, Load, </strong><span><strong class="bold">Transform</strong></span><span> (</span><span><strong class="bold">ELT</strong></span><span>).</span></p>
<h2 id="_idParaDest-134" class="calibre9"><a id="_idTextAnchor196" class="calibre6 pcalibre pcalibre1"/>ETL and ELT</h2>
<p class="calibre3">ETL and ELT are two approaches<a id="_idIndexMarker638" class="calibre6 pcalibre pcalibre1"/> for data integration, processing, and storage. In ETL, data<a id="_idIndexMarker639" class="calibre6 pcalibre pcalibre1"/> is first extracted from source systems and then transformed into another format. Such transformations could include cleansing, enrichment, aggregation, or deduplication. These transformations usually take place within an intermediary processing engine or staging area. Once the data is transformed, it is loaded into the target system, which is typically a data warehouse or a data lake. ETL is a traditional approach that is well suited for environments where data consistency and quality are critical. However, it can be time-consuming and resource-intensive due to the processing steps happening before the data <span>is loaded.</span></p>
<p class="calibre3">On the other hand, ELT reverses the order of the last two steps. Data is first extracted from source systems and then directly loaded into the target system, such as a modern data warehouse or data lake. The transformation step is performed within the target system itself, using the processing capabilities of the destination system. ELT has gained popularity in recent years due to the increased scalability and processing power of modern cloud-based data warehouses, as it enables faster loading and allows users to perform complex transformations <span>on demand.</span></p>
<p class="calibre3">Choosing between ETL and ELT depends on the specific requirements of the data processing environment, data quality needs, and the target <span>system’s capabilities.</span></p>
<h2 id="_idParaDest-135" class="calibre9"><a id="_idTextAnchor197" class="calibre6 pcalibre pcalibre1"/>Batch and streaming data processing</h2>
<p class="calibre3">At a high level, there are two main ways in which we can process data: batch and streaming. In this section, we explain each of these approaches, which will equip us with the knowledge we need for the activities that follow in <span>this chapter.</span></p>
<h3 class="calibre11">Batch data processing</h3>
<p class="calibre3">Batch data processing usually refers<a id="_idIndexMarker640" class="calibre6 pcalibre pcalibre1"/> to performing a series of transformations on large datasets in a bulk manner. These kinds of pipelines are suitable for use cases in which you need to process large amounts of data in parallel over a period of hours or even days. As an example, imagine that your company runs an online retail business that has separate deployments in different geographical regions, such as a <em class="italic">www.example.com</em> website in North America, a <em class="italic">www.example.co.uk</em> website in the UK, and a <em class="italic">www.example.cn</em> website in China. Every night, you may want to run a workload that takes all of the items purchased by all customers in each region that day, merges that data across regions, and then feeds that data into an ML model. This would be an example of a batch workload. Other examples <a id="_idIndexMarker641" class="calibre6 pcalibre pcalibre1"/>of batch data processing use cases include <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">Processing daily sales transactions to generate reports for <span>business decision-making</span></li>
<li class="calibre8">Analyzing log files from a web server to understand user behavior over a specific time window (for example, a day <span>or week)</span></li>
<li class="calibre8">Running large-scale data transformation jobs, such as converting raw data into a structured format for downstream <span>data systems</span></li>
</ul>
<p class="calibre3">In addition to data processing tasks that only need to be performed periodically, companies may also need to process data in real time or near real time, which brings us to the topic of streaming <span>data processing.</span></p>
<h3 class="calibre11">Streaming data processing</h3>
<p class="calibre3">Streaming data processing, as the name implies, involves<a id="_idIndexMarker642" class="calibre6 pcalibre pcalibre1"/> working on a stream of data that continuously flows into a system, usually on an ongoing basis. Rather than processing an enormous dataset as a single job, the data in a streaming processing use case usually consists of small pieces of data that are processed in flight. Examples<a id="_idIndexMarker643" class="calibre6 pcalibre pcalibre1"/> of streaming data processing include <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">Analyzing social media<a id="_idIndexMarker644" class="calibre6 pcalibre pcalibre1"/> feeds to identify trends or detect events (for example, <strong class="bold">sentiment analysis</strong> (<strong class="bold">SA</strong>), <span>hashtag tracking)</span></li>
<li class="calibre8">Monitoring and analyzing IoT sensor data in real time to detect anomalies, trigger alerts, or <span>optimize processes</span></li>
<li class="calibre8">Processing financial transactions<a id="_idIndexMarker645" class="calibre6 pcalibre pcalibre1"/> in real time for fraud detection <span>and prevention</span></li>
</ul>
<p class="calibre3">Now that we’ve discussed the two main high-level categories of data processing use cases, let’s start diving into how we actually implement these <span>use cases.</span></p>
<h2 id="_idParaDest-136" class="calibre9"><a id="_idTextAnchor198" class="calibre6 pcalibre pcalibre1"/>Data pipelines</h2>
<p class="calibre3">Data pipelines are how we automate<a id="_idIndexMarker646" class="calibre6 pcalibre pcalibre1"/> the concepts we just described, such as large-scale ETL/ELT or streaming data transformations. Data pipelines are essential for organizations that handle large volumes of data or require complex data processing workloads, as they help to streamline data management at scale. Without such automation pipelines, employees would spend a lot of time performing mundane and repetitive but complex and error-prone data processing activities. There are multiple Google Cloud services that can be used for batch and streaming data pipelines, which you will learn to use in <span>this chapter.</span></p>
<p class="calibre3">Now that we’ve covered some of the fundamental concepts, it’s time to start diving into some practical data <span>processing activities.</span></p>
<p class="calibre3">Before we start to build automated data processing pipelines, however, we will first need to explore our data so that we can understand what kinds of transformations we would want to implement in <span>our pipelines.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">While, in previous chapters, we included the code directly in the pages of this book, we are now advancing to more complex use cases that require a lot of code that would not be suitable to include directly in the pages of this book. Please review the code artifacts in the GitHub repository related to this chapter to understand the code we are using to implement these <span>steps: </span><a href="https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects" class="calibre6 pcalibre pcalibre1"><span>https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects</span></a><span>.</span></p>
<p class="callout">Throughout the rest of the book, I will continue to include code directly where it <span>makes sense.</span></p>
<h1 id="_idParaDest-137" class="calibre5"><a id="_idTextAnchor199" class="calibre6 pcalibre pcalibre1"/>Exploring, visualizing, and preparing data</h1>
<p class="calibre3"><em class="italic">Use case</em>: We’re planning a trip<a id="_idIndexMarker647" class="calibre6 pcalibre pcalibre1"/> to New York City, and we want to get an idea of what the best <a id="_idIndexMarker648" class="calibre6 pcalibre pcalibre1"/>accommodation options<a id="_idIndexMarker649" class="calibre6 pcalibre pcalibre1"/> would be. Rather than perusing through and evaluating lots of individual Airbnb postings, we’re going to download lots of the reviews and do some bulk data analysis and data processing to get <span>some insights.</span></p>
<p class="calibre3">We can use the Vertex AI Workbench notebook that we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a> for this purpose. Please open JupyterLab on that notebook instance. In the directory explorer on the left side of the screen, navigate to the <strong class="source-inline">Chapter-6</strong> directory and open the<strong class="bold"> </strong><strong class="source-inline">Chapter-6-Airbnb.ipynb</strong> notebook. You can choose <strong class="bold">Python (Local)</strong> as the kernel. As you did in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>, run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on <span>your keyboard.</span></p>
<p class="calibre3">In the notebook, we use markdown cells to describe each step in detail so that you can understand each step in the process. We use libraries such as <strong class="source-inline">pandas</strong>, <strong class="source-inline">matplotlib</strong>, and <strong class="source-inline">seaborn</strong> to summarize and visualize the contents of our dataset, and then we perform data cleaning and preparation activities such as filling in missing values, removing outliers, and removing features that are not likely to be useful for training a regression model to predict accommodation prices. <span><em class="italic">Figure 6</em></span><em class="italic">.3</em> shows an example of one of our data visualization graphs, in which we view the range and distribution of prices for the listings in <span>our dataset:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer103">
<img alt="Figure 6.3: Distribution of prices in the dataset" src="image/B18143_06_3.jpg" class="calibre107"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.3: Distribution of prices in the dataset</p>
<p class="calibre3">As we can see, the majority<a id="_idIndexMarker650" class="calibre6 pcalibre pcalibre1"/> of the accommodation options <a id="_idIndexMarker651" class="calibre6 pcalibre pcalibre1"/>cost less than $200 per night, but there<a id="_idIndexMarker652" class="calibre6 pcalibre pcalibre1"/> are some data points (although not many) between $600 and $1,000 per night. Those are either very expensive accommodation options or they could be potential outliers/errors in the data. You can see additional data visualization graphs in <span>the notebook.</span></p>
<p class="calibre3">Regarding data cleanup activities, to clean up potential pricing outliers, for example, we use the following piece of code to set a limit of $800 (although still high) and remove any listings above that <span>nightly rate:</span></p>
<pre class="source-code">
price_range = (data_cleaned['price'] &gt;= 10) &amp; (
    data_cleaned['price'] &lt;= 800)
data_cleaned = data_cleaned.loc[price_range]</pre> <p class="calibre3">To remove features that are not likely to be useful for training a regression model to predict accommodation prices, we use the following piece <span>of code:</span></p>
<pre class="source-code">
columns_to_drop = ['id', 'name', 'host_name', 'last_review', 
    'reviews_per_month']
data_cleaned = data.drop(columns=columns_to_drop)</pre> <p class="calibre3">These are just a couple of examples of the data preparation steps we perform in <span>the notebook.</span></p>
<p class="calibre3">When you have completed<a id="_idIndexMarker653" class="calibre6 pcalibre pcalibre1"/> executing all of the activities<a id="_idIndexMarker654" class="calibre6 pcalibre pcalibre1"/> in the notebook, we will move<a id="_idIndexMarker655" class="calibre6 pcalibre pcalibre1"/> on to see how we can turn those activities into an automated pipeline in production. We will start by implementing a batch <span>data pipeline.</span></p>
<h1 id="_idParaDest-138" class="calibre5"><a id="_idTextAnchor200" class="calibre6 pcalibre pcalibre1"/>Batch data pipelines</h1>
<p class="calibre3">Now that we’ve used our Jupyter notebook<a id="_idIndexMarker656" class="calibre6 pcalibre pcalibre1"/> to explore our data and figure out what kinds of transformations we want to perform on our dataset, let’s imagine that we want to turn this into a production workload that can run automatically on very large files, without needing any further human effort. As I mentioned earlier, this is essential in any company that implements large-scale data analytics and AI/ML workloads. It’s simply not feasible to get somebody to manually perform those transformations every time, and for very large data volumes, the transformations could not be performed on a notebook instance. For example, imagine we get thousands of new postings every day, and we want to automatically prepare that data for an ML model. We can do this by creating an automated pipeline to perform data transformations every night (or however often <span>we wish).</span></p>
<h2 id="_idParaDest-139" class="calibre9"><a id="_idTextAnchor201" class="calibre6 pcalibre pcalibre1"/>Batch data pipeline concepts and tools</h2>
<p class="calibre3">Before we start diving in and building our batch data processing pipeline, let’s first cover some important concepts and tools in <span>this domain.</span></p>
<h3 class="calibre11">Apache Spark</h3>
<p class="calibre3">Apache Spark is a highly<a id="_idIndexMarker657" class="calibre6 pcalibre pcalibre1"/> popular development<a id="_idIndexMarker658" class="calibre6 pcalibre pcalibre1"/> and execution framework that can be used to implement very large-scale data processing workloads and other types of large-scale computing use cases such as ML. Its power lies both in its in-memory processing capabilities and its ability to implement multiple large computing and data processing tasks <span>in parallel.</span></p>
<p class="calibre3">While Spark can be used for both batch and streaming (or micro-batching) data processing workloads, we’re going<a id="_idIndexMarker659" class="calibre6 pcalibre pcalibre1"/> to use it to perform our batch data transformations<a id="_idIndexMarker660" class="calibre6 pcalibre pcalibre1"/> in <span>this chapter.</span></p>
<h3 class="calibre11">Google Cloud Dataproc</h3>
<p class="calibre3">As we discussed<a id="_idIndexMarker661" class="calibre6 pcalibre pcalibre1"/> in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a>, Google Cloud Dataproc<a id="_idIndexMarker662" class="calibre6 pcalibre pcalibre1"/> is a fully managed, fast, and easy-to-use service for running Apache Spark and Apache Hadoop clusters on GCP. In this chapter, we will use it to execute our Spark <span>processing jobs.</span></p>
<h3 class="calibre11">Apache Airflow</h3>
<p class="calibre3">Apache Airflow is an<a id="_idIndexMarker663" class="calibre6 pcalibre pcalibre1"/> open source platform used<a id="_idIndexMarker664" class="calibre6 pcalibre pcalibre1"/> for orchestrating complex data workflows. It was created by Airbnb and later contributed to the <strong class="bold">Apache Software Foundation</strong> (<strong class="bold">ASF</strong>). Airflow is designed to help developers<a id="_idIndexMarker665" class="calibre6 pcalibre pcalibre1"/> and data engineers create, schedule, monitor, and manage workflows, making it easier to handle tasks that depend on one another. It is commonly used in data engineering and data science projects for tasks such as ETL, ML pipelines, and data analytics, and it is widely used by organizations across various industries, making it a popular choice for managing complex <span>data workflows.</span></p>
<h4 class="calibre20">Directed acyclic graphs</h4>
<p class="calibre3">Airflow represents workflows as <strong class="bold">directed acyclic graphs</strong> (<strong class="bold">DAGs</strong>), which consist of tasks and their dependencies. Each task<a id="_idIndexMarker666" class="calibre6 pcalibre pcalibre1"/> in the workflow is represented as a node, and the dependencies between tasks are represented as directed edges. This structure ensures that tasks are executed in a specific order without creating loops, as depicted in <span><em class="italic">Figure 6</em></span><span><em class="italic">.4</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer104">
<img alt="Figure 6.4: A simple DAG  (excerpted from source:  https://www.flickr.com/photos/dullhunk/4647369097)" src="image/B18143_06_4.jpg" class="calibre108"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.4: A simple DAG (excerpted from source:  https://www.flickr.com/photos/dullhunk/4647369097)</p>
<p class="calibre3">In <span><em class="italic">Figure 6</em></span><em class="italic">.4</em>, we can see that tasks <em class="italic">b</em>, <em class="italic">c</em>, <em class="italic">d</em>, and <em class="italic">e</em> all depend on task <em class="italic">a</em>. Similarly, task <em class="italic">d</em> also depends on tasks <em class="italic">b</em> and <em class="italic">c</em>, and task <em class="italic">e</em> also depends on tasks <em class="italic">c</em> <span>and </span><span><em class="italic">d</em></span><span>.</span></p>
<p class="calibre3">In Airflow, a DAG is defined<a id="_idIndexMarker667" class="calibre6 pcalibre pcalibre1"/> in a Python script, which represents tasks and their dependencies <span>as code.</span></p>
<h3 class="calibre11">Google Cloud Composer</h3>
<p class="calibre3"><strong class="bold">Google Cloud Composer</strong> (<strong class="bold">GCC</strong>) is a fully managed workflow orchestration<a id="_idIndexMarker668" class="calibre6 pcalibre pcalibre1"/> service built on Apache<a id="_idIndexMarker669" class="calibre6 pcalibre pcalibre1"/> Airflow. It allows you to create, schedule, and monitor data workflows across various Google Cloud services, as well as on-premises or <span>multi-cloud environments.</span></p>
<p class="calibre3">Cloud Composer simplifies the process of setting up and managing Apache Airflow by providing an easy-to-use interface and automating the infrastructure management. This allows you to focus on creating and maintaining your workflows while Google takes care of the underlying infrastructure, scaling, <span>and updates.</span></p>
<p class="calibre3">Now that we’ve covered the important concepts for implementing batch data pipelines, let’s start building <span>our pipeline.</span></p>
<h1 id="_idParaDest-140" class="calibre5"><a id="_idTextAnchor202" class="calibre6 pcalibre pcalibre1"/>Building our batch data pipeline</h1>
<p class="calibre3">In this section, we will create<a id="_idIndexMarker670" class="calibre6 pcalibre pcalibre1"/> our Spark job and run it on Google Cloud Dataproc and will use GCC to orchestrate our job. What this means is that we can get GCC to automatically run our job every day. Each time it runs our job, it will create a Dataproc cluster, execute our Spark job, and then delete the Dataproc cluster when our job completes. This is a standard best practice that companies use to save money because you should not have computing resources running when you are not using them. The architecture of our pipeline on Google Cloud is shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer105">
<img alt="Figure 6.5: Batch data pipeline architecture" src="image/B18143_06_5.jpg" class="calibre109"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.5: Batch data pipeline architecture</p>
<p class="calibre3">Let’s begin by setting up <span>Cloud Composer.</span></p>
<h2 id="_idParaDest-141" class="calibre9"><a id="_idTextAnchor203" class="calibre6 pcalibre pcalibre1"/>Cloud Composer</h2>
<p class="calibre3">In this section, we will <a id="_idIndexMarker671" class="calibre6 pcalibre pcalibre1"/>set up Cloud Composer<a id="_idIndexMarker672" class="calibre6 pcalibre pcalibre1"/> to schedule and run our batch data <span>processing pipeline.</span></p>
<h3 class="calibre11">Cloud Composer environment</h3>
<p class="calibre3">Everything we do in Cloud Composer happens within a Cloud Composer environment. To set up our Cloud Composer<a id="_idIndexMarker673" class="calibre6 pcalibre pcalibre1"/> environment, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <strong class="bold">Composer</strong> → <span><strong class="bold">Environments</strong></span><span>.</span></li>
<li class="calibre8">Select <span><strong class="bold">Create Environment</strong></span><span>.</span></li>
<li class="calibre8">If prompted, select <span><strong class="bold">Composer 2</strong></span><span>.</span></li>
<li class="calibre8">On the screen that appears, enter a name for your Composer environment. See <span><em class="italic">Figure 6</em></span><em class="italic">.6</em> <span>for reference:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer106">
<img alt="Figure 6.6: Creating a Composer environment" src="image/B18143_06_6.jpg" class="calibre110"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.6: Creating a Composer environment</p>
<ol class="calibre7">
<li value="5" class="calibre8">Select your preferred<a id="_idIndexMarker674" class="calibre6 pcalibre pcalibre1"/> region. (Remember that it’s better if you use the same region for each activity throughout this book, <span>if possible.)</span></li>
<li class="calibre8">Select the latest image version. See <span><em class="italic">Figure 6</em></span><em class="italic">.6</em> <span>for reference.</span></li>
<li class="calibre8"><em class="italic">IMPORTANT</em>: Select the service account you created earlier in this chapter (if you used the suggested name, then it will include <strong class="source-inline">data-processing-sa</strong> in <span>the name).</span></li>
<li class="calibre8">In the <strong class="bold">Environment resources</strong> section, select a <span>small environment.</span></li>
<li class="calibre8">Leave all other options at their default values and <span>select </span><span><strong class="bold">Create</strong></span><span>.</span></li>
<li class="calibre8">The environment will take up to 25 minutes to <span>spin up.</span></li>
</ol>
<p class="calibre3">While waiting for the environment<a id="_idIndexMarker675" class="calibre6 pcalibre pcalibre1"/> to spin up, let’s move on to the <span>next section.</span></p>
<h3 class="calibre11">Cloud Composer Python code</h3>
<p class="calibre3">In this section, we will review<a id="_idIndexMarker676" class="calibre6 pcalibre pcalibre1"/> and prepare the Python code for our Cloud Composer Spark workload. There are two code resources that we will use with Cloud Composer, which can be found in our GitHub <span>repository (</span><a href="https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/Chapter-6" class="calibre6 pcalibre pcalibre1"><span>https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/</span></a><span>Chapter-06)</span><span>:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="source-inline">composer-dag.py</strong>, which contains the Python code that defines our Cloud Composer <span>Airflow DAG</span></li>
<li class="calibre8"><strong class="source-inline">chapter-6-pyspark.py</strong>, which contains the PySpark code that defines our <span>Spark job</span></li>
</ul>
<p class="calibre3">Perform the following steps to start preparing those files for use with <span>Cloud Composer:</span></p>
<ol class="calibre7">
<li class="calibre8">Locate those files in the clone of our GitHub repository that you created on your local machine, and open them <span>for editing.</span></li>
<li class="calibre8">In the <strong class="source-inline">chapter-6-pyspark.py</strong> file, you just need to update the storage locations for the source and destination datasets. To do that, search for the <strong class="source-inline">GCS-BUCKET-NAME</strong> string in the file and replace it with your own GCS bucket name that you <span>created earlier.</span></li>
</ol>
<p class="callout-heading">IMPORTANT</p>
<p class="callout">The <strong class="source-inline1">GCS-BUCKET-NAME</strong> string exists in two locations in the file (once near the beginning, and again near the end). One location specifies the source dataset, and the other location specifies the destination where our Spark job will save the processed data. Replace both occurrences of the string with your own GCS <span>bucket name.</span></p>
<ol class="calibre7">
<li value="3" class="calibre8">In the <strong class="source-inline">composer-dag.py</strong> file, you will <a id="_idIndexMarker677" class="calibre6 pcalibre pcalibre1"/>see the following block of variables near the beginning of <span>the file:</span><pre class="source-code">
PROJECT_ID = "YOUR PROJECT ID"
REGION = "us-central1"
ZONE = "us-central1-a"
SERVICE_ACCOUNT_EMAIL = "data-processing-sa@YOUR-PROJECT-ID.iam.gserviceaccount.com"
PYSPARK_URI = "gs://GCS-BUCKET-NAME/code/chapter-6-pyspark.py"</pre><p class="calibre3">All of those variables need to be updated with specific values from your GCP project. The comments in the file provide additional details on <span>the replacements.</span></p></li> <li class="calibre8">In addition to making the aforementioned changes, review the contents of the code to get an understanding of how Cloud Composer will execute our jobs. The code contains comments to describe what it’s doing in each section so that you can understand how <span>it works.</span></li>
<li class="calibre8">When you have completed the preceding steps, we’re ready to upload the resources to be used by <span>Cloud Composer.</span></li>
<li class="calibre8">Cloud Composer requires the preceding code resources to be stored in GCS, but in two <span>separate locations:</span><ul class="calibre70"><li class="calibre8">For <strong class="source-inline">chapter-6-pyspark.py</strong>: Upload this file in the <strong class="source-inline">code</strong> folder you created earlier in GCS. To do that, navigate into the <strong class="source-inline">code</strong> folder you created, select <strong class="bold">Upload files</strong>, then select the <strong class="source-inline">chapter-6-pyspark.py</strong><strong class="bold"> </strong>file from the clone of our GitHub repository that you created on your local machine (that is, the file you edited in the <span>previous steps).</span></li><li class="calibre8"><strong class="source-inline">composer-dag.py</strong>: This file will be uploaded to a special folder that Cloud Composer will create. When your Cloud Composer environment is fully created, click on the name of your newly created environment in the Cloud Composer console, and your environment details screen will open. Near the top of the screen, select <strong class="bold">OPEN DAGS FOLDER</strong>. Then, you can click <strong class="bold">Upload files</strong> and select the <strong class="source-inline">composer-dag.py</strong> file from the clone of our GitHub repository that you created on your local machine (that is, the file you edited in the <span>previous steps).</span></li></ul></li>
</ol>
<p class="calibre3">That’s it! As soon as the <strong class="source-inline">composer-dag.py</strong> file is uploaded, Cloud Composer will completely automate everything for you. It will take a few minutes, but Cloud Composer will create your DAG, and you will see it appearing in the Cloud Composer console. It will then execute the DAG, meaning that it will create a Dataproc cluster, execute a Spark job to perform the data transformations we specified, and delete the Dataproc cluster when the <span>job completes.</span></p>
<p class="calibre3">You can see the various tasks happening in the Composer and Dataproc consoles (give it some time in each case), and the final test will be to verify that the processed data appears in the GCS destination you specified in the <span>PySpark code.</span></p>
<p class="calibre3">When you have completed<a id="_idIndexMarker678" class="calibre6 pcalibre pcalibre1"/> all of the aforementioned steps and you no longer need your Composer environment, you can delete <span>the environment.</span></p>
<h3 class="calibre11">Deleting the Cloud Composer environment</h3>
<p class="calibre3">Perform the following<a id="_idIndexMarker679" class="calibre6 pcalibre pcalibre1"/> steps to delete the <span>Composer environment:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <strong class="bold">Composer</strong> → <span><strong class="bold">Environments</strong></span><span>.</span></li>
<li class="calibre8">Select the checkbox next to the name of <span>your environment.</span></li>
<li class="calibre8">Select <strong class="bold">DELETE</strong> at the top of the screen. See <span><em class="italic">Figure 6</em></span><em class="italic">.7</em> <span>for reference:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer107">
<img alt="Figure 6.7: Deleting the Composer environment" src="image/B18143_06_7.jpg" class="calibre111"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.7: Deleting the Composer environment</p>
<ol class="calibre7">
<li value="4" class="calibre8">Select <strong class="bold">DELETE</strong> in the confirmation screen <span>that appears.</span></li>
</ol>
<p class="calibre3">It will take a few minutes for the environment to delete, after which it will disappear from your list <span>of environments.</span></p>
<p class="calibre3">Awesome work! You have officially created your first data processing pipeline on <span>Google Cloud!</span></p>
<p class="calibre3">Note that the method we used<a id="_idIndexMarker680" class="calibre6 pcalibre pcalibre1"/> in this chapter is an example of one (very popular) pattern for using multiple Google Cloud services to implement a data pipeline. Google Cloud also provides other products that could be used to implement similar outcomes, such as Google Cloud Data Fusion, which enables you to create pipelines using a visual user interface. We will explore other services in later chapters of this book, and one other important offering that was launched by Google Cloud in 2023 is Serverless Spark, which we’ll briefly <span>discuss next.</span></p>
<h2 id="_idParaDest-142" class="calibre9"><a id="_idTextAnchor204" class="calibre6 pcalibre pcalibre1"/>Google Cloud Serverless Spark</h2>
<p class="calibre3">Google Cloud Serverless Spark<a id="_idIndexMarker681" class="calibre6 pcalibre pcalibre1"/> is a fully managed, serverless<a id="_idIndexMarker682" class="calibre6 pcalibre pcalibre1"/> Apache Spark service that makes it easy to run Spark jobs without having to provision or manage any infrastructure. It also automatically scales your jobs up or down based on demand, so you only pay for the resources that you use, which makes it a cost-effective way to run Spark jobs, even for short-lived or <span>intermittent workloads.</span></p>
<p class="calibre3">It’s also integrated with other Google Cloud services, such as BigQuery, Dataflow, Dataproc, and Vertex AI, and popular open-source tools such as Zeppelin and Jupyter Notebook, making it easy<a id="_idIndexMarker683" class="calibre6 pcalibre pcalibre1"/> to explore and analyze data and build and run <strong class="bold">end-to-end</strong> (<strong class="bold">E2E</strong>) data pipelines directly with <span>those services.</span></p>
<p class="calibre3">With Serverless Spark on Dataproc, for example, you can select from pre-made templates to easily perform common<a id="_idIndexMarker684" class="calibre6 pcalibre pcalibre1"/> tasks such as moving and transforming data between <strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>) or Apache Hive data stores and GCS or BigQuery, or you can build your own Docker containers that Serverless Spark will run in order to implement custom data processing workloads. For more information<a id="_idIndexMarker685" class="calibre6 pcalibre pcalibre1"/> on how to develop such custom containers, see the following Google<a id="_idIndexMarker686" class="calibre6 pcalibre pcalibre1"/> Cloud <span>documentation: </span><a href="https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers</span></a><span>.</span></p>
<p class="calibre3">Now that we’ve learned<a id="_idIndexMarker687" class="calibre6 pcalibre pcalibre1"/> how to build a batch data processing pipeline, we will move on to implementing streaming <span>data pipelines.</span></p>
<h1 id="_idParaDest-143" class="calibre5"><a id="_idTextAnchor205" class="calibre6 pcalibre pcalibre1"/>Streaming data pipelines</h1>
<p class="calibre3">In this section, we will deal<a id="_idIndexMarker688" class="calibre6 pcalibre pcalibre1"/> with a different kind of data source and will learn about the differences in processing data in real time versus the batch-oriented methods we used in the <span>previous sections.</span></p>
<h2 id="_idParaDest-144" class="calibre9"><a id="_idTextAnchor206" class="calibre6 pcalibre pcalibre1"/>Streaming data pipeline concepts and tools</h2>
<p class="calibre3">Again, before we start to build a streaming data processing pipeline, there are some important concepts and tools that we need to introduce <span>and understand.</span></p>
<h3 class="calibre11">Apache Beam</h3>
<p class="calibre3">Apache Beam is an<a id="_idIndexMarker689" class="calibre6 pcalibre pcalibre1"/> open source, unified<a id="_idIndexMarker690" class="calibre6 pcalibre pcalibre1"/> programming model for processing and analyzing large-scale data in batch and streaming modes. It was initially developed by Google as a part of its internal data processing tools, and later, it was donated to the ASF. Beam provides a unified way to write data processing pipelines that can be executed on various distributed processing backends such as Apache Flink, Apache Samza, Apache Spark, Google Cloud Dataflow, and others. It supports multiple programming languages, including Java, Python, and Go, and it allows developers to write both batch and streaming data processing pipelines using a single API, which simplifies the development process and enables seamless switching between batch and streaming modes. It also provides a rich set of built-in I/O connectors for various data sources and sinks, including Kafka, Hadoop, Google Cloud Pub/Sub, BigQuery, and others. Additionally, developers can build their own custom connectors if needed. In this chapter, we will use Apache Beam on Google Cloud Dataflow to create<a id="_idIndexMarker691" class="calibre6 pcalibre pcalibre1"/> a pipeline to process<a id="_idIndexMarker692" class="calibre6 pcalibre pcalibre1"/> data in <span>real time.</span></p>
<h4 class="calibre20">Apache Beam concepts</h4>
<p class="calibre3">In this section, we discuss some basic concepts of the Apache Beam programming model, which include <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Pipelines</strong>: Just like the concept of a pipeline<a id="_idIndexMarker693" class="calibre6 pcalibre pcalibre1"/> in Apache Airflow, which<a id="_idIndexMarker694" class="calibre6 pcalibre pcalibre1"/> we used earlier in this chapter, an Apache Beam pipeline is a DAG representing a sequence of data processing steps or the overall data processing workflow in Apache <span>Beam workloads.</span></li>
<li class="calibre8"><strong class="bold">PCollections</strong>: A <strong class="bold">Parallel Collection</strong> (<strong class="bold">PCollection</strong>) is an immutable distributed <a id="_idIndexMarker695" class="calibre6 pcalibre pcalibre1"/>dataset representing<a id="_idIndexMarker696" class="calibre6 pcalibre pcalibre1"/> a collection of data elements. It is the primary data structure used in Apache Beam pipelines to hold and <span>manipulate data.</span></li>
<li class="calibre8"><strong class="bold">PTransforms</strong>: A <strong class="bold">Parallel Transform</strong> (<strong class="bold">PTransform</strong>) is a user-defined operation that takes<a id="_idIndexMarker697" class="calibre6 pcalibre pcalibre1"/> one or more PCollections <a id="_idIndexMarker698" class="calibre6 pcalibre pcalibre1"/>as input, processes the data, and produces one or more PCollections as output. PTransforms are the building blocks of a pipeline and define the data <span>processing logic.</span></li>
<li class="calibre8"><strong class="bold">Windowing</strong>: Windowing is a mechanism<a id="_idIndexMarker699" class="calibre6 pcalibre pcalibre1"/> that allows grouping data elements in a PCollection based on timestamps or other criteria. This concept is particularly useful for processing unbounded datasets in streaming applications, where data elements need to be processed in <span>finite</span><span><a id="_idIndexMarker700" class="calibre6 pcalibre pcalibre1"/></span><span> windows.</span></li>
<li class="calibre8"><strong class="bold">Watermarks</strong>: Watermarks are a way to estimate<a id="_idIndexMarker701" class="calibre6 pcalibre pcalibre1"/> the progress of time<a id="_idIndexMarker702" class="calibre6 pcalibre pcalibre1"/> in a streaming pipeline, and they help to determine when it’s safe to emit results for a <span>particular window.</span></li>
<li class="calibre8"><strong class="bold">Triggers</strong>: A trigger determines<a id="_idIndexMarker703" class="calibre6 pcalibre pcalibre1"/> when to aggregate<a id="_idIndexMarker704" class="calibre6 pcalibre pcalibre1"/> the results of each window, based on factors such as the arrival of a certain number of data elements, the passage of a certain amount of time, or the advancement <span>of watermarks.</span></li>
<li class="calibre8"><strong class="bold">Runners</strong>: Runners are the components<a id="_idIndexMarker705" class="calibre6 pcalibre pcalibre1"/> responsible for executing<a id="_idIndexMarker706" class="calibre6 pcalibre pcalibre1"/> a Beam pipeline on a specific execution engine or distributed <span>processing platform.</span></li>
</ul>
<p class="calibre3">The Beam model decouples the pipeline definition from the underlying execution engine, allowing users to choose the most suitable platform for their <span>use case.</span></p>
<h3 class="calibre11">Google Cloud Dataflow</h3>
<p class="calibre3">We introduced Dataflow in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a>, and now<a id="_idIndexMarker707" class="calibre6 pcalibre pcalibre1"/> we’ll dive deeper<a id="_idIndexMarker708" class="calibre6 pcalibre pcalibre1"/> into it. Dataflow is a Google Cloud service on which you can run Apache Beam. In other words, it provides one of the execution environments or runners on which you can run Apache <span>Beam workloads.</span></p>
<p class="calibre3">Dataflow provides multiple features for various types of use cases, and in this section, we’ll briefly discuss the various options and <span>their applications.</span></p>
<h4 class="calibre20">Dataflow Data Pipelines and Dataflow Jobs</h4>
<p class="calibre3">People often get confused about the difference <a id="_idIndexMarker709" class="calibre6 pcalibre pcalibre1"/>between Dataflow Data Pipelines<a id="_idIndexMarker710" class="calibre6 pcalibre pcalibre1"/> and Dataflow Jobs. The best way to look<a id="_idIndexMarker711" class="calibre6 pcalibre pcalibre1"/> at this is that Dataflow Data Pipelines refer to the definition of a data pipeline, which could be executed on a recurring basis, whereas Dataflow Jobs refer to a single<a id="_idIndexMarker712" class="calibre6 pcalibre pcalibre1"/> execution of a data pipeline. The reason this can be confusing is that you can create a new pipeline definition either in the Dataflow Jobs console or in the Dataflow Data <span>Pipelines console.</span></p>
<p class="calibre3">In either case, when creating a pipeline definition, we have the option to use predefined templates that cover common types of tasks that people often want to do with Dataflow, such as transferring data from BigQuery to Bigtable, or from Cloud Spanner to Pub/Sub, and there are lots of different templates to choose from, covering a wide variety of data sources and destinations. These templates make it very easy for us to implement a data transfer workload without requiring much or any development effort on our part. Alternatively, if we have more complex data processing needs that are not included in one of the standard templates, then we can create our own custom pipeline definitions. We will look at both options later in <span>this chapter.</span></p>
<h4 class="calibre20">Dataflow Workbench notebooks</h4>
<p class="calibre3">One way in which we can develop<a id="_idIndexMarker713" class="calibre6 pcalibre pcalibre1"/> custom data processing pipelines<a id="_idIndexMarker714" class="calibre6 pcalibre pcalibre1"/> is by using Dataflow Workbench notebooks. This may sound somewhat familiar, because you may remember creating and using a Vertex AI Workbench notebook in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>. In the Dataflow Workbench console, we can create notebooks that come with Apache Beam already installed. We will create a notebook in the Dataflow Workbench console later in <span>this chapter</span><span>.</span></p>
<h4 class="calibre20">Dataflow snapshots</h4>
<p class="calibre3">Dataflow snapshots <a id="_idIndexMarker715" class="calibre6 pcalibre pcalibre1"/>save the state of a streaming pipeline, which<a id="_idIndexMarker716" class="calibre6 pcalibre pcalibre1"/> allows you to start a new version of your Dataflow job without losing state. This is useful for backup and recovery, testing, and rolling back updates to <span>streaming pipelines.</span></p>
<h4 class="calibre20">SQL Workspace</h4>
<p class="calibre3">The Dataflow console also includes<a id="_idIndexMarker717" class="calibre6 pcalibre pcalibre1"/> a built-in SQL Workspace that enables<a id="_idIndexMarker718" class="calibre6 pcalibre pcalibre1"/> you to run SQL queries directly from the console and send the results to BigQuery or Pub/Sub. This can be useful for ad hoc use cases in which you want to simply run a SQL query to fetch information from a given source and store the results in one of the <span>supported destinations.</span></p>
<p class="calibre3">Now that we’ve covered the important concepts for implementing streaming data pipelines, let’s start building <span>our pipeline.</span></p>
<h1 id="_idParaDest-145" class="calibre5"><a id="_idTextAnchor207" class="calibre6 pcalibre pcalibre1"/>Building our streaming data pipeline</h1>
<p class="calibre3">Sticking with the theme<a id="_idIndexMarker719" class="calibre6 pcalibre pcalibre1"/> of planning our trip to New York City, the activities in the previous sections of this chapter gave us some good insights into what kinds of accommodation options are available to us, and now we want to assess our transportation options; specifically, how much it’s likely to cost us to travel around in taxis while <span>we’re there.</span></p>
<p class="calibre3">Our streaming data pipeline will take input data from Google Cloud Pub/Sub, perform some processing in Dataflow, and place the outputs into BigQuery for analysis. The architecture of our pipeline on Google Cloud is shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.8</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer108">
<img alt="Figure 6.8: Streaming data pipeline" src="image/B18143_06_8.jpg" class="calibre112"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.8: Streaming data pipeline</p>
<p class="calibre3">Google Cloud provides a public stream of data that can be used to test these kinds of stream processing workloads, which contains information relating to New York City taxi rides, and which we will use in our example in this section. Let’s start by creating a destination for our streamed<a id="_idIndexMarker720" class="calibre6 pcalibre pcalibre1"/> data, also referred<a id="_idIndexMarker721" class="calibre6 pcalibre pcalibre1"/> to as a <strong class="bold">sink</strong> for <span>our pipeline.</span></p>
<h2 id="_idParaDest-146" class="calibre9"><a id="_idTextAnchor208" class="calibre6 pcalibre pcalibre1"/>Creating a BigQuery dataset</h2>
<p class="calibre3">We will use Google Cloud BigQuery<a id="_idIndexMarker722" class="calibre6 pcalibre pcalibre1"/> as a storage system<a id="_idIndexMarker723" class="calibre6 pcalibre pcalibre1"/> for our data. To get started, we first need to define a dataset in BigQuery that we will use as our pipeline destination. To do this, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <span><strong class="bold">BigQuery</strong></span><span>.</span></li>
<li class="calibre8">In the top-left corner of the screen, you will see your project name. Click on the symbol of three vertical dots to the right of your project name (see <span><em class="italic">Figure 6</em></span><em class="italic">.9</em> <span>for reference):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer109">
<img alt="Figure 6.9: BigQuery project menu" src="image/B18143_06_9.jpg" class="calibre78"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.9: BigQuery project menu</p>
<ol class="calibre7">
<li value="3" class="calibre8">In the menu that gets displayed, select <span><strong class="bold">Create dataset</strong></span><span>.</span></li>
<li class="calibre8">Give your dataset a <span>name</span><span>: </span><span><strong class="source-inline">taxirides</strong></span><span>.</span></li>
<li class="calibre8">Select your preferred<a id="_idIndexMarker724" class="calibre6 pcalibre pcalibre1"/> region, and<a id="_idIndexMarker725" class="calibre6 pcalibre pcalibre1"/> select <span><strong class="bold">Create dataset</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Now that we’ve created our dataset, we will need to create a table within <span>that dataset.</span></p>
<h2 id="_idParaDest-147" class="calibre9"><a id="_idTextAnchor209" class="calibre6 pcalibre pcalibre1"/>Creating a BigQuery table</h2>
<p class="calibre3">A BigQuery dataset generally<a id="_idIndexMarker726" class="calibre6 pcalibre pcalibre1"/> contains one or more tables<a id="_idIndexMarker727" class="calibre6 pcalibre pcalibre1"/> that contain the actual data. Let’s create a table into which our data will be streamed. To do this, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the BigQuery editor, click on the <strong class="source-inline">taxirides</strong> dataset you just created and select <span><strong class="bold">Create table</strong></span><span>.</span></li>
<li class="calibre8">In the <strong class="bold">Table</strong> field, enter <strong class="source-inline">realtime</strong> as the <span>table name.</span></li>
<li class="calibre8">In the <strong class="bold">Schema</strong> section, click the plus sign (<strong class="bold">+</strong>) to add a new field. Our first field has the following properties (leave all other options for each field at their <span>default values):</span></li>
</ol>
<table class="no-table-style" id="table001-1">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Field name</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Type</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Mode</strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline">ride_id</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline">STRING</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline">NULLABLE</strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 6.1: Properties for the first field in the BigQuery table</p>
<p class="calibre3">4.	Repeat <em class="italic">step 3</em> to add more fields, until the schema looks like that shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.10</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer110">
<img alt="Figure 6.10: Table schema" src="image/B18143_06_10.jpg" class="calibre113"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.10: Table schema</p>
<p class="calibre3">5.	Select <span><strong class="bold">Create table</strong></span><span>.</span></p>
<p class="calibre3">Now our table is ready for us to stream<a id="_idIndexMarker728" class="calibre6 pcalibre pcalibre1"/> data into it, let’s move<a id="_idIndexMarker729" class="calibre6 pcalibre pcalibre1"/> on to creating our data <span>streaming pipeline.</span></p>
<h2 id="_idParaDest-148" class="calibre9"><a id="_idTextAnchor210" class="calibre6 pcalibre pcalibre1"/>Creating a Dataflow job from a template</h2>
<p class="calibre3">We’re going to use a Dataflow template<a id="_idIndexMarker730" class="calibre6 pcalibre pcalibre1"/> to create our first<a id="_idIndexMarker731" class="calibre6 pcalibre pcalibre1"/> data streaming pipeline. To do this, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <strong class="bold">Dataflow</strong> → <span><strong class="bold">Jobs</strong></span><span>.</span></li>
<li class="calibre8">Select <strong class="bold">Create job </strong><span><strong class="bold">from template</strong></span><span>.</span></li>
<li class="calibre8">For <strong class="bold">Job name</strong>, <span>enter </span><span><strong class="source-inline">taxi-data-raw</strong></span><span>.</span></li>
<li class="calibre8">Select your <span>preferred region.</span></li>
<li class="calibre8">In the drop-down menu for <strong class="bold">Dataflow template</strong>, select the <strong class="bold">Pub/Sub to </strong><span><strong class="bold">BigQuery</strong></span><span> template.</span></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Pub/Sub has also released a direct integration with BigQuery, but considering that we want to illustrate how to use a Dataflow template, we’re using the Dataflow connection method in <span>this chapter.</span></p>
<ol class="calibre7">
<li value="6" class="calibre8">Next, in the <strong class="bold">Input Pub/Sub topic</strong> field, select <strong class="bold">Enter </strong><span><strong class="bold">topic manually</strong></span><span>.</span></li>
<li class="calibre8">Enter the<a id="_idIndexMarker732" class="calibre6 pcalibre pcalibre1"/> <span>following</span><span><a id="_idIndexMarker733" class="calibre6 pcalibre pcalibre1"/></span><span> topic:</span><pre class="source-code">
projects/pubsub-public-data/topics/taxirides-realtime</pre></li> <li class="calibre8">In the <strong class="bold">BigQuery output table</strong> field, click <strong class="bold">BROWSE</strong>, and then select the <strong class="source-inline">realtime</strong> table you created in the <span>previous section.</span></li>
<li class="calibre8">Click <strong class="bold">SELECT</strong> at the bottom of <span>the screen.</span></li>
<li class="calibre8">In the <strong class="bold">Temporary location</strong> field, enter your desired storage location path in the following format (replace [BUCKET-NAME] with your bucket <span>name): gs://[BUCKET-NAME]/dataflow</span><span>.</span></li>
<li class="calibre8">Expand the <strong class="bold">Optional Parameters</strong> section and scroll down until you find the <strong class="bold">Service account </strong><span><strong class="bold">email</strong></span><span> field.</span></li>
<li class="calibre8">In that field, select the service account you created earlier in this chapter (it will contain <strong class="source-inline">data-processing-sa</strong> in the name if you used the <span>suggested name).</span></li>
<li class="calibre8">Leave all other options at their <span>default values.</span></li>
<li class="calibre8">Select <span><strong class="bold">RUN JOB</strong></span><span>.</span></li>
<li class="calibre8">After a few minutes, you will see<a id="_idIndexMarker734" class="calibre6 pcalibre pcalibre1"/> the job details appearing, and a graph that looks similar to the one shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.11</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer111">
<img alt="Figure 6.11: Dataflow execution graph" src="image/B18143_06_11.jpg" class="calibre114"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.11: Dataflow execution graph</p>
<ol class="calibre7">
<li value="16" class="calibre8">Try clicking around on each of the steps and sub-steps in the graph to get a better understanding of what each step <span>is doing.</span></li>
</ol>
<p class="calibre3">After some time, you can head<a id="_idIndexMarker735" class="calibre6 pcalibre pcalibre1"/> over to the BigQuery console<a id="_idIndexMarker736" class="calibre6 pcalibre pcalibre1"/> and verify that the data is streaming into the table. Move on to the next section in order to do that. However, don’t close the Dataflow console yet because you will come back here after verifying the data <span>in BigQuery.</span></p>
<h2 id="_idParaDest-149" class="calibre9"><a id="_idTextAnchor211" class="calibre6 pcalibre pcalibre1"/>Verifying data in BigQuery</h2>
<p class="calibre3">To verify the data<a id="_idIndexMarker737" class="calibre6 pcalibre pcalibre1"/> in BigQuery, perform<a id="_idIndexMarker738" class="calibre6 pcalibre pcalibre1"/> the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <span><strong class="bold">BigQuery</strong></span><span>.</span></li>
<li class="calibre8">In the top-left corner of the screen, you will see your project name. Click on the arrow symbol to the left of your project name to expand it (see <span><em class="italic">Figure 6</em></span><em class="italic">.12</em> <span>for reference).</span></li>
<li class="calibre8">Then, click on the arrow symbol to the left of your dataset name (<strong class="source-inline">taxirides</strong>) to <span>expand it.</span></li>
<li class="calibre8">Select your <span><strong class="source-inline">realtime</strong></span><span> table.</span></li>
<li class="calibre8">Select the <span><strong class="bold">PREVIEW</strong></span><span> tab.</span></li>
<li class="calibre8">You should then see a screen that looks like the one shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.12</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer112">
<img alt="Figure 6.12: BigQuery data" src="image/B18143_06_12.jpg" class="calibre115"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.12: BigQuery data</p>
<ol class="calibre7">
<li value="7" class="calibre8">When you have verified the data in BigQuery, you can go back to the Dataflow console and stop the Dataflow job by clicking <strong class="bold">STOP</strong> at the top of <span>the screen.</span></li>
</ol>
<p class="calibre3">Now that you’ve seen how easy<a id="_idIndexMarker739" class="calibre6 pcalibre pcalibre1"/> it is to set up a Dataflow job<a id="_idIndexMarker740" class="calibre6 pcalibre pcalibre1"/> from a template, let’s move on to more complex Dataflow <span>use cases.</span></p>
<h2 id="_idParaDest-150" class="calibre9"><a id="_idTextAnchor212" class="calibre6 pcalibre pcalibre1"/>Creating a Dataflow notebook</h2>
<p class="calibre3">Because we want<a id="_idIndexMarker741" class="calibre6 pcalibre pcalibre1"/> to use a customized Apache Beam<a id="_idIndexMarker742" class="calibre6 pcalibre pcalibre1"/> notebook, we will create a notebook in the Dataflow Workbench console. Perform the following steps to create <span>the notebook:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu<strong class="bold"> </strong>→ <strong class="bold">Dataflow</strong> → <span><strong class="bold">Workbench</strong></span><span>.</span></li>
<li class="calibre8">At the top of the screen, select the <span><strong class="bold">Instances</strong></span><span> tab.</span></li>
<li class="calibre8">Now, at the top of the screen , select <span><strong class="bold">Create New</strong></span><span>.</span></li>
<li class="calibre8">In the screen that appears (see <span><em class="italic">Figure 6</em></span><em class="italic">.13</em> for reference), you can either accept the default notebook name or create a name of <span>your preference:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer113">
<img alt="Figure 6.13: Creating a user-managed notebook" src="image/B18143_06_13.jpg" class="calibre116"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.13: Creating a user-managed notebook</p>
<ol class="calibre7">
<li value="5" class="calibre8">Select your preferred<a id="_idIndexMarker743" class="calibre6 pcalibre pcalibre1"/> region and zone. The zone selection<a id="_idIndexMarker744" class="calibre6 pcalibre pcalibre1"/> doesn’t really matter in this case, but I recommend selecting the same region you have been using in previous activities in <span>this book.</span></li>
<li class="calibre8">Select <strong class="bold">Continue</strong>, and then <span><strong class="bold">Continue</strong></span><span> again.</span></li>
<li class="calibre8">Select E2 as the compute <span>instance type.</span></li>
<li class="calibre8">You can also configure an idle timeout period after which the machine will automatically shut down if it is idle for that amount of time. This helps to <span>save costs.</span></li>
<li class="calibre8">Select <strong class="bold">Continue</strong> multiple times until you reach the <strong class="bold">IAM</strong> and <span><strong class="bold">security</strong></span><span> screen</span></li>
<li class="calibre8">In the <strong class="bold">IAM and security</strong> screen, select the <strong class="bold">Single user</strong> option. See <span><em class="italic">Figure 6</em></span><em class="italic">.14</em> <span>for reference:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer114">
<img alt="Figure 6.14: Dataflow notebook – IAM and security" src="image/B18143_06_14.jpg" class="calibre117"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.14: Dataflow notebook – IAM and security</p>
<p class="callout-heading">Note</p>
<p class="callout">You may remember performing a similar step (that is, selecting the <strong class="bold">Single user authentication</strong> option) when we created our managed notebook in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>. This allows us to directly use the notebook without needing to authenticate it as a service <span>account beforehand.</span></p>
<ol class="calibre7">
<li value="11" class="calibre8">The <strong class="bold">User email</strong> box that appears<a id="_idIndexMarker745" class="calibre6 pcalibre pcalibre1"/> should automatically<a id="_idIndexMarker746" class="calibre6 pcalibre pcalibre1"/> be populated with your login email address. If not, enter the email address that you use for logging in to the Google <span>Cloud console.</span></li>
<li class="calibre8">Also, we want our notebook instance to use the service account we created earlier in this chapter, so <strong class="bold">uncheck</strong> the option that says <strong class="bold">Use default Compute Engine service account on the VM to call Google </strong><span><strong class="bold">Cloud APIs</strong></span><span>.</span></li>
<li class="calibre8">In the <strong class="bold">Service account email</strong> box that appears, start typing <strong class="source-inline">data</strong>, and you should see the name of the service account you created earlier in this chapter appearing in the list of available service accounts (assuming that you named it <strong class="source-inline">data-processing-sa</strong>, as recommended in <span>that section).</span></li>
<li class="calibre8">Select that service account in <span>the list.</span></li>
<li class="calibre8">Also, take note of the <strong class="bold">Pricing summary</strong> option in the top-right corner of the screen. This is an estimate of how much it would cost if you were to leave the notebook running all month. Fortunately, you will only use it for a short time in this chapter. If you did not configure an idle shutdown period when creating the notebook, remember to shut it down when you’re finished <span>using it.</span></li>
<li class="calibre8">You can leave all other options at their default values and select <strong class="bold">Create</strong> at the bottom of <span>the screen.</span></li>
<li class="calibre8">It will take a few minutes for the notebook instance to be created. When the instance creation has completed, you will see it in the list of user-managed notebooks, and an option to open JupyterLab <span>will appear.</span></li>
<li class="calibre8">Select <span><strong class="bold">Open Jupyterlab</strong></span><span>.</span></li>
<li class="calibre8">When the JupyterLab screen<a id="_idIndexMarker747" class="calibre6 pcalibre pcalibre1"/> opens, it’s time to clone our repository into your<a id="_idIndexMarker748" class="calibre6 pcalibre pcalibre1"/> notebook. This is similar to the process you performed in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>, but you have created a separate notebook instance in this chapter, so we need to clone the repository into this instance. The steps, again, to clone the repository, are <span>as follows.</span></li>
<li class="calibre8">Click on the <strong class="bold">Git</strong> symbol in the menu on the left of the screen. The symbol will look like the one shown in <span><em class="italic">Figure 6</em></span><span><em class="italic">.15</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer115">
<img alt="Figure 6.15: Git symbol" src="image/B18143_06_15.jpg" class="calibre118"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.15: Git symbol</p>
<ol class="calibre7">
<li value="21" class="calibre8">Select <span><strong class="bold">Clone Repository</strong></span><span>.</span></li>
<li class="calibre8">Enter our repository <span>URL: </span><a href="https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects" class="calibre6 pcalibre pcalibre1"><span>https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects</span></a><span>.</span></li>
<li class="calibre8">If any options are displayed, leave<a id="_idIndexMarker749" class="calibre6 pcalibre pcalibre1"/> them at their<a id="_idIndexMarker750" class="calibre6 pcalibre pcalibre1"/> <span>default values.</span></li>
<li class="calibre8"><span>Select </span><span><strong class="bold">Clone</strong></span><span>.</span></li>
<li class="calibre8">You should see a new folder appear in your notebook, <span>named </span><span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span>.</span></li>
<li class="calibre8">Double-click on that folder, double-click on the <strong class="source-inline">Chapter-06</strong> folder within it, and then double-click on the <strong class="source-inline">Streaming_NYC_Taxi_Data.ipynb</strong> file to <span>open it.</span></li>
<li class="calibre8">In the <strong class="bold">Select Kernel</strong> screen that appears, select the latest version of Apache Beam. At the time of writing this, the latest available option in the launcher is Apache Beam 2.4.6 (see <span><em class="italic">Figure 6</em></span><em class="italic">.16</em> <span>for reference):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer116">
<img alt="Figure 6.16: Selecting a notebook kernel" src="image/B18143_06_16.jpg" class="calibre119"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.16: Selecting a notebook kernel</p>
<ol class="calibre7">
<li value="28" class="calibre8">The notebook we have opened contains a lot of Apache Beam Python code that we can use to process the data that’s streaming in from the public <span>Pub/Sub topic.</span></li>
<li class="calibre8">Run each of the cells in the notebook, and read the explanations in the markdown and comments to understand what we’re doing. We’re using Apache Beam to define a streaming data processing pipeline that we will run in Google <span>Cloud Dataflow.</span></li>
</ol>
<p class="calibre3">When you have completed<a id="_idIndexMarker751" class="calibre6 pcalibre pcalibre1"/> running the cells in the notebook, you can <a id="_idIndexMarker752" class="calibre6 pcalibre pcalibre1"/>go to the Dataflow Jobs console, and you will see your new pipeline running there. Give it a few minutes for the pipeline to start up and for data to flow through the pipeline. Just like before, I recommend that you click on various parts of the pipeline execution graph in order to get a better understanding of how the pipeline is structured. Next, let’s verify this new pipeline data in BigQuery, but again, don’t close the Dataflow console yet, because you will come back here after verifying the data <span>in BigQuery.</span></p>
<h2 id="_idParaDest-151" class="calibre9"><a id="_idTextAnchor213" class="calibre6 pcalibre pcalibre1"/>Verifying data in BigQuery</h2>
<p class="calibre3">To verify the data in BigQuery, open<a id="_idIndexMarker753" class="calibre6 pcalibre pcalibre1"/> the BigQuery console, and under<a id="_idIndexMarker754" class="calibre6 pcalibre pcalibre1"/> the <strong class="source-inline">taxirides</strong> dataset, you will see a new table that has been created by our custom Dataflow pipeline, called <strong class="source-inline">run_rates</strong>. Click on the <strong class="bold">PREVIEW</strong> tab again to see the data that is streaming in. As you will see, in this case, we just have a single column in our database, which contains the aggregated <strong class="source-inline">run_rates</strong> values that were computed by <span>our pipeline.</span></p>
<p class="calibre3">When you have verified the data in BigQuery, you can go back to the Dataflow console and stop the Dataflow job by clicking <strong class="bold">STOP</strong> at the top of <span>the screen.</span></p>
<p class="calibre3">Now that we have completed the activities in this section, you can shut down your user-managed notebook by performing the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to the <strong class="bold">Google Cloud services</strong> menu → <strong class="bold">Dataflow</strong> → <span><strong class="bold">Workbench</strong></span><span>.</span></li>
<li class="calibre8">At the top of the screen, select the <strong class="bold">User-managed </strong><span><strong class="bold">notebooks</strong></span><span> tab.</span></li>
<li class="calibre8">Select the checkbox next to your notebook name, and click <strong class="bold">STOP</strong> at the top of the screen (just above the <strong class="bold">User-managed </strong><span><strong class="bold">notebooks</strong></span><span> tab).</span></li>
</ol>
<p class="calibre3">The notebook will shut down after a <span>few minutes.</span></p>
<p class="calibre3">If everything worked as expected, you have now successfully created a custom pipeline that processes and transforms streaming data <span>in flight!</span></p>
<h1 id="_idParaDest-152" class="calibre5"><a id="_idTextAnchor214" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, you learned how to ingest data into Google Cloud from various sources, and you discovered important concepts on how to process data in <span>Google Cloud.</span></p>
<p class="calibre3">You then learned about exploring and visualizing data using Vertex AI and BigQuery. Next, you learned how to clean and prepare data for ML workloads using Jupyter notebooks, and then how to create an automated data pipeline to perform the same transformations at a production scale in a batch method using Apache Spark on Google Cloud Dataproc, as well as how to automatically orchestrate that entire process using Apache Airflow <span>in GCC.</span></p>
<p class="calibre3">We then covered important concepts and tools related to processing streaming data, and you finally built your own streaming data processing pipelines using Apache Beam on Google <span>Cloud Dataflow.</span></p>
<p class="calibre3">In the next chapter, we will spend additional time on data processing and preparation, with a specific focus on the concept of <span>feature engineering.</span></p>
</div>
</div></body></html>