["```py\n  > library(rpart) #classification and regression trees\n  > library(partykit) #treeplots\n  > library(MASS) #breast and pima indian data\n  > library(ElemStatLearn) #prostate data\n  > library(randomForest) #random forests\n  > library(xgboost) #gradient boosting\n  > library(caret) #tune hyper-parameters\n\n```", "```py\n  > data(prostate)\n  > prostate$gleason <- ifelse(prostate$gleason == 6, 0, 1)\n  > pros.train <- subset(prostate, train == TRUE)[, 1:9]\n  > pros.test <- subset(prostate, train == FALSE)[, 1:9]\n\n```", "```py\n  > tree.pros <- rpart(lpsa ~ ., data = pros.train)\n\n```", "```py\n  > print(tree.pros$cptable)\n     CP nsplit rel error  xerror   xstd\n  1 0.35852251   0 1.0000000 1.0364016 0.1822698\n  2 0.12295687   1 0.6414775 0.8395071 0.1214181\n  3 0.11639953   2 0.5185206 0.7255295 0.1015424\n  4 0.05350873   3 0.4021211 0.7608289 0.1109777\n  5 0.01032838   4 0.3486124 0.6911426 0.1061507\n  6 0.01000000   5 0.3382840 0.7102030 0.1093327\n\n```", "```py\n  > plotcp(tree.pros)\n\n```", "```py\n  > cp <- min(tree.pros$cptable[5, ])\n  > prune.tree.pros <- prune(tree.pros, cp = cp)\n\n```", "```py\n  > plot(as.party(tree.pros))\n\n```", "```py\n  > plot(as.party(prune.tree.pros))\n\n```", "```py\n  > party.pros.test <- predict(prune.tree.pros, newdata = pros.test)\n  > rpart.resid <- party.pros.test - pros.test$lpsa \n  > mean(rpart.resid^2) #caluclate MSE\n  [1] 0.5267748\n\n```", "```py\n  > data(biopsy)\n  > biopsy <- biopsy[, -1] #delete ID\n  > names(biopsy) <- c(\"thick\", \"u.size\", \"u.shape\", \"adhsn\", \n    \"s.size\", \"nucl\",\n  \"chrom\", \"n.nuc\", \"mit\", \"class\") #change the feature names\n  > biopsy.v2 <- na.omit(biopsy) #delete the observations with \n    missing values\n  > set.seed(123) #random number generator\n  > ind <- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, \n    0.3))\n  > biop.train <- biopsy.v2[ind == 1, ] #the training data set\n  > biop.test <- biopsy.v2[ind == 2, ] #the test data set\n\n```", "```py\n  > str(biop.test[, 10])\n   Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 2 1 1 ...\n\n```", "```py\n  > set.seed(123)\n  > tree.biop <- rpart(class ~ ., data = biop.train)\n  > tree.biop$cptable\n    CP nsplit rel error  xerror    xstd\n  1 0.79651163   0 1.0000000 1.0000000 0.06086254\n  2 0.07558140   1 0.2034884 0.2674419 0.03746996\n  3 0.01162791   2 0.1279070 0.1453488 0.02829278\n  4 0.01000000   3 0.1162791 0.1744186 0.03082013\n\n```", "```py\n  > cp <- min(tree.biop$cptable[3, ])\n  > prune.tree.biop <- prune(tree.biop, cp = cp)\n  > plot(as.party(prune.tree.biop))\n\n```", "```py\n  > rparty.test <- predict(prune.tree.biop, newdata = biop.test, type \n   =\"class\")  \n  > table(rparty.test, biop.test$class)\n  rparty.test benign malignant\n   benign    136     3\n   malignant   6    64\n  > (136+64)/209\n  [1] 0.9569378\n\n```", "```py\n  > set.seed(123)\n  > rf.pros <- randomForest(lpsa ~ ., data = pros.train)\n  > rf.pros\n  Call:\n randomForest(formula = lpsa ~ ., data = pros.train) \n Type of random forest: regression\n Number of trees: 500\n No. of variables tried at each split: 2\n Mean of squared residuals: 0.6792314\n % Var explained: 52.73 \n\n```", "```py\n  > plot(rf.pros)\n\n```", "```py\n  > which.min(rf.pros$mse)\n  [1] 75\n\n```", "```py\n  > set.seed(123)\n  > rf.pros.2 <- randomForest(lpsa ~ ., data = pros.train, ntree \n   =75)\n  > rf.pros.2\n  Call:\n randomForest(formula = lpsa ~ ., data = pros.train, ntree = 75) \n Type of random forest: regression\n Number of trees: 75\n No. of variables tried at each split: 2\n Mean of squared residuals: 0.6632513\n % Var explained: 53.85 \n\n```", "```py\n  > varImpPlot(rf.pros.2, scale = T, \n   main = \"Variable Importance Plot - PSA Score\")\n\n```", "```py\n  > importance(rf.pros.2)\n      IncNodePurity\n    lcavol  24.108641\n lweight  15.721079\n age  6.363778\n lbph  8.842343\n svi  9.501436\n lcp  9.900339\n gleason  0.000000\n pgg45  8.088635 \n\n```", "```py\n  > rf.pros.test <- predict(rf.pros.2, newdata = pros.test)\n  > rf.resid = rf.pros.test - pros.test$lpsa #calculate residual\n  > mean(rf.resid^2)\n  [1] 0.5136894\n\n```", "```py\n  > set.seed(123) \n  > rf.biop <- randomForest(class ~. , data = biop.train)\n  > rf.biop\n  Call:\n   randomForest(formula = class ~ ., data = biop.train)\n          Type of random forest: classification\n             Number of trees: 500\n  No. of variables tried at each split: 3\n      OOB estimate of error rate: 3.16%\n  Confusion matrix:\n       benign malignant class.error\n  benign    294     8 0.02649007\n  malignant   7    165 0.04069767\n\n```", "```py\n  > plot(rf.biop)\n\n```", "```py\n  > which.min(rf.biop$err.rate[, 1])\n  [1] 19\n\n```", "```py\n > set.seed(123)\n > rf.biop.2 <- randomForest(class~ ., data = biop.train, ntree = \n   19)\n  > print(rf.biop.2) \n  Call:\n   randomForest(formula = class ~ ., data = biop.train, ntree = 19)\n          Type of random forest: classification\n             Number of trees: 19\n  No. of variables tried at each split: 3\n      OOB estimate of error rate: 2.95%\n  Confusion matrix:\n       benign malignant class.error\n  benign    294     8 0.02649007\n  malignant   6    166 0.03488372\n  > rf.biop.test <- predict(rf.biop.2, newdata = biop.test, type = \n   \"response\")\n  > table(rf.biop.test, biop.test$class)\n  rf.biop.test benign malignant\n    benign    139     0\n    malignant   3    67\n  > (139 + 67) / 209\n  [1] 0.9856459\n\n```", "```py\n  > varImpPlot(rf.biop.2)\n\n```", "```py\n  > data(Pima.tr)\n  > data(Pima.te)\n  > pima <- rbind(Pima.tr, Pima.te)\n  > set.seed(502)\n  > ind <- sample(2, nrow(pima), replace = TRUE, prob = c(0.7, 0.3))\n  > pima.train <- pima[ind == 1, ]\n  > pima.test <- pima[ind == 2, ]\n\n```", "```py\n  > set.seed(321) \n  > rf.pima = randomForest(type~., data=pima.train)\n  > rf.pima\n  Call:\n   randomForest(formula = type ~ ., data = pima.train)\n          Type of random forest: classification\n             Number of trees: 500\n  No. of variables tried at each split: 2\n      OOB estimate of error rate: 20%\n  Confusion matrix:\n     No Yes class.error\n  No 233 29  0.1106870\n  Yes 48 75  0.3902439\n\n```", "```py\n  > which.min(rf.pima$err.rate[, 1])\n  [1] 80\n  > set.seed(321)\n  > rf.pima.2 = randomForest(type~., data=pima.train, ntree=80)\n  > print(rf.pima.2)\n  Call:\n   randomForest(formula = type ~ ., data = pima.train, ntree = 80)\n          Type of random forest: classification\n             Number of trees: 80\n  No. of variables tried at each split: 2\n      OOB estimate of error rate: 19.48%\n  Confusion matrix:\n     No Yes class.error\n  No 230 32  0.1221374\n  Yes 43 80  0.3495935\n\n```", "```py\n  > rf.pima.test <- predict(rf.pima.2, newdata= pima.test, \n   type = \"response\")\n  > table(rf.pima.test, pima.test$type)\n  rf.pima.test No Yes\n       No 75 21\n       Yes 18 33\n  > (75+33)/147\n  [1] 0.7346939\n\n```", "```py\n > grid = expand.grid(\n nrounds = c(75, 100),\n colsample_bytree = 1,\n min_child_weight = 1,\n eta = c(0.01, 0.1, 0.3), #0.3 is default,\n gamma = c(0.5, 0.25),\n subsample = 0.5,\n max_depth = c(2, 3)\n ) \n\n```", "```py\n > cntrl = trainControl(\n   method = \"cv\",\n   number = 5,\n   verboseIter = TRUE,\n   returnData = FALSE,\n   returnResamp = \"final\" \n   )\n\n```", "```py\n > set.seed(1)\n > train.xgb = train(\n x = pima.train[, 1:7],\n y = ,pima.train[, 8],\n trControl = cntrl,\n tuneGrid = grid,\n method = \"xgbTree\"\n ) \n\n```", "```py\n > train.xgb\n eXtreme Gradient Boosting \n No pre-processing\n Resampling: Cross-Validated (5 fold) \n Summary of sample sizes: 308, 308, 309, 308, 307 \n Resampling results across tuning parameters:\n eta max_depth gamma nrounds Accuracy  Kappa \n 0.01 2     0.25  75    0.7924286 0.4857249\n 0.01 2     0.25  100    0.7898321 0.4837457\n 0.01 2     0.50  75    0.7976243 0.5005362\n ...................................................\n 0.30 3     0.50  75    0.7870664 0.4949317\n 0.30 3     0.50  100    0.7481703 0.3936924\n Tuning parameter 'colsample_bytree' was held constant at a \n  value of 1\n Tuning parameter 'min_child_weight' was held constant at a \n  value of 1\n Tuning parameter 'subsample' was held constant at a value of 0.5\n Accuracy was used to select the optimal model using the largest \n   value.\n The final values used for the model were nrounds = 75, max_depth = \n   2,\n eta = 0.1, gamma = 0.5, colsample_bytree = 1, min_child_weight = 1\n and subsample = 0.5.\n\n```", "```py\n > param <- list( objective = \"binary:logistic\", \n   booster = \"gbtree\",\n   eval_metric = \"error\",\n   eta = 0.1, \n   max_depth = 2, \n   subsample = 0.5,\n   colsample_bytree = 1,\n   gamma = 0.5\n   )  \n  > x <- as.matrix(pima.train[, 1:7])\n > y <- ifelse(pima.train$type == \"Yes\", 1, 0)\n > train.mat <- xgb.DMatrix(data = x, label = y) \n\n```", "```py\n > set.seed(1)\n > xgb.fit <- xgb.train(params = param, data = train.mat, nrounds = \n   75) \n\n```", "```py\n > impMatrix <- xgb.importance(feature_names = dimnames(x)[[2]], \n   model = xgb.fit) > impMatrix\n Feature    Gain   Cover Frequency\n 1:   glu 0.40000548 0.31701688 0.24509804\n 2:   age 0.16177609 0.15685050 0.17156863\n 3:   bmi 0.12074049 0.14691325 0.14705882\n 4:   ped 0.11717238 0.15400331 0.16666667\n 5:  npreg 0.07642333 0.05920868 0.06862745\n 6:  skin 0.06389969 0.08682105 0.10294118\n 7:   bp 0.05998254 0.07918634 0.09803922 > xgb.plot.importance(impMatrix, main = \"Gain by Feature\") \n\n```", "```py\n > library(InformationValue)\n  > pred <- predict(xgb.fit, x)\n  > optimalCutoff(y, pred)\n  [1] 0.3899574\n > pima.testMat <- as.matrix(pima.test[, 1:7])\n > xgb.pima.test <- predict(xgb.fit, pima.testMat)\n > y.test <- ifelse(pima.test$type == \"Yes\", 1, 0) > confusionMatrix(y.test, xgb.pima.test, threshold = 0.39)\n 0 1\n 0 72 16\n 1 20 39\n > 1 - misClassError(y.test, xgb.pima.test, threshold = 0.39)\n [1] 0.7551\n\n```", "```py\n > plotROC(y.test, xgb.pima.test)\n\n```", "```py\n > data(Sonar, package=\"mlbench\")\n > dim(Sonar)\n [1] 208 61\n > table(Sonar$Class)\n M R \n 111 97\n\n```", "```py\n > library(Boruta)\n > set.seed(1)\n > feature.selection <- Boruta(Class ~ ., data = Sonar, doTrace = 1)\n\n```", "```py\n > feature.selection$timeTaken\n Time difference of 25.78468 secs\n\n```", "```py\n > table(feature.selection$finalDecision)\n Tentative Confirmed Rejected \n 12    31    17\n\n```", "```py\n > fNames <- getSelectedAttributes(feature.selection) # withTentative = TRUE\n > fNames\n [1] \"V1\" \"V4\" \"V5\" \"V9\" \"V10\" \"V11\" \"V12\" \"V13\" \"V15\" \"V16\"\n [11] \"V17\" \"V18\" \"V19\" \"V20\" \"V21\" \"V22\" \"V23\" \"V27\" \"V28\" \"V31\"\n [21] \"V35\" \"V36\" \"V37\" \"V44\" \"V45\" \"V46\" \"V47\" \"V48\" \"V49\" \"V51\"\n [31] \"V52\"\n\n```", "```py\n > Sonar.features <- Sonar[, fNames]\n > dim(Sonar.features)\n [1] 208 31\n\n```"]