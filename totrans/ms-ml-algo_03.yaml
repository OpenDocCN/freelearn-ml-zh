- en: Graph-Based Semi-Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图的半监督学习
- en: 'In this chapter, we continue our discussion about semi-supervised learning,
    considering a family of algorithms that is based on the graph obtained from the
    dataset and the existing relationships among samples. The problems that we are
    going to discuss belong to two main categories: the propagation of class labels
    to unlabeled samples and the use of non-linear techniques based on the manifold
    assumption to reduce the dimensionality of the original dataset. In particular,
    this chapter covers the following propagation algorithms:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续讨论半监督学习，考虑基于从数据集获得的图和样本之间现有关系的一系列算法。我们将讨论的问题属于两大类：将类别标签传播到未标记样本以及使用基于流形假设的非线性技术来降低原始数据集的维度。特别是，本章涵盖了以下传播算法：
- en: Label propagation based on the weight matrix
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于权重矩阵的标签传播
- en: Label propagation in Scikit-Learn (based on transition probabilities)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-Learn中的标签传播（基于转移概率）
- en: Label spreading
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签传播
- en: Propagation based on Markov random walks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于马尔可夫随机游走的传播
- en: 'For the manifold learning section, we''re discussing:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流形学习部分，我们正在讨论：
- en: Isomap algorithm and multidimensional scaling approach
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isomap算法和多维尺度方法
- en: Locally linear embedding
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部线性嵌入
- en: Laplacian Spectral Embedding
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉普拉斯谱嵌入
- en: t-SNE
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE
- en: Label propagation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签传播
- en: '**Label propagation** is a family of semi-supervised algorithms based on a
    graph representation of the dataset. In particular, if we have *N* labeled points
    (with bipolar labels +1 and -1) and *M* unlabeled points (denoted by *y=0*), it''s
    possible to build an undirected graph based on a measure of geometric affinity
    among samples. If *G = {V, E}* is the formal definition of the graph, the set
    of vertices is made up of sample labels *V = { -1, +1, 0 }*, while the edge set
    is based on an **affinity matrix** *W* (often called **adjacency matrix** when
    the graph is unweighted), which depends only on the *X* values, not on the labels.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签传播**是一系列基于数据集图表示的半监督算法。特别是，如果我们有*N*个标记点（带有双极性标签+1和-1）和*M*个未标记点（用*y=0*表示），则可以根据样本之间的几何亲和度度量构建一个无向图。如果*G
    = {V, E}*是图的正式定义，则顶点集由样本标签*V = { -1, +1, 0 }*组成，而边集基于**亲和度矩阵***W*（当图无权重时通常称为**邻接矩阵**），它只依赖于*X*值，而不依赖于标签。'
- en: 'In the following graph, there''s an example of such a structure:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，有一个此类结构的示例：
- en: '![](img/a857d71f-f48b-4a65-92f3-112d69183aa5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a857d71f-f48b-4a65-92f3-112d69183aa5.png)'
- en: Example of binary graph
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 二元图的示例
- en: 'In the preceding example graph, there are four labeled points (two with *y=+1* and
    two with *y=-1*), and two unlabeled points (*y=0*). The affinity matrix is normally
    symmetric and square with dimensions equal to *(N+M) x (N+M)*. It can be obtained
    with different approaches. The most common ones, also adopted by Scikit-Learn,
    are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例图中，有四个标记点（两个*y=+1*和两个*y=-1*），以及两个未标记点(*y=0*)。亲和度矩阵通常是对称的，是方阵，其维度等于*(N+M)
    x (N+M)*。它可以通过不同的方法获得。最常见的方法之一，也被Scikit-Learn采用，是：
- en: '***k*-Nearest Neighbors** (we are going to discuss this algorithm with further
    details in [Chapter 8](59f765c2-2ad0-4605-826e-349080f85f1f.xhtml), *Clustering
    Algorithms*):'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***k*-最近邻**（我们将在第8章[59f765c2-2ad0-4605-826e-349080f85f1f.xhtml]中详细讨论此算法，*聚类算法*）：'
- en: '![](img/3c0b235e-449d-4c59-8046-6130be025841.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c0b235e-449d-4c59-8046-6130be025841.png)'
- en: '**Radial basis function kernel**:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**径向基函数核**：'
- en: '![](img/34ccb44e-6279-4bcd-8b1a-49853b3cfdcf.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34ccb44e-6279-4bcd-8b1a-49853b3cfdcf.png)'
- en: Sometimes, in the radial basis function kernel, the parameter *γ* is represented
    as the reciprocal of *2σ²*; however, small *γ* values corresponding to a large
    variance increase the radius, including farther points and *smoothing* the class
    over a number of samples, while large *γ* values restrict the boundaries to a
    subset that tends to a single sample. Instead, in the k-nearest neighbors kernel,
    the parameter *k* controls the number of samples to consider as neighbors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在径向基函数核中，参数*γ*表示为*2σ²*的倒数；然而，对应于大方差的小*γ*值会增加半径，包括更远的点，并在多个样本上*平滑*类别，而大的*γ*值将边界限制在趋向于单个样本的子集。相反，在k-最近邻核中，参数*k*控制要考虑作为邻居的样本数量。
- en: 'To describe the basic algorithm, we also need to introduce the **degree matrix**
    (*D*):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述基本算法，我们还需要介绍**度矩阵**(*D*)：
- en: '![](img/7084e81a-bdeb-4eca-8e57-a56a7302bbb7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7084e81a-bdeb-4eca-8e57-a56a7302bbb7.png)'
- en: It is a diagonal matrix where each non-null element represents the *degree*
    of the corresponding vertex. This can be the number of incoming edges, or a measure
    proportional to it (as in the case of *W* based on the radial basis function).
    The general idea of label propagation is to let each node propagate its label
    to its neighbors and iterate the procedure until convergence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个对角矩阵，其中每个非零元素代表相应顶点的 *度*。这可以是入边数，或者与其成比例的度量（如在基于径向基函数的 *W* 的情况下）。标签传播的一般思想是让每个节点将其标签传播到其邻居，并迭代该过程直到收敛。
- en: 'Formally, if we have a dataset containing both labeled and unlabeled samples:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，如果我们有一个包含标记和无标记样本的数据集：
- en: '![](img/d61fa3a2-6d63-46ed-a068-f89324c21af7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d61fa3a2-6d63-46ed-a068-f89324c21af7.png)'
- en: 'The complete steps of the **label propagation** algorithm (as proposed by Zhu
    and Ghahramani in *Learning from Labeled and Unlabeled Data with Label Propagation*,
    *Zhu X.*, *Ghahramani Z.*, *CMU-CALD-02-107*) are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签传播**算法的完整步骤（如 Zhu 和 Ghahramani 在 *Learning from Labeled and Unlabeled Data
    with Label Propagation* 中提出，*Zhu X.*，*Ghahramani Z.*，*CMU-CALD-02-107*）是：'
- en: Select an affinity matrix type (KNN or RBF) and compute *W*
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择亲和矩阵类型（KNN 或 RBF）并计算 *W*
- en: Compute the degree matrix *D*
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算度矩阵 *D*
- en: Define *Y^((0)) = Y*
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 *Y^((0)) = Y*
- en: Define *Y[L] = {y[0], y[1], ..., y[N]}*
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 *Y[L] = {y[0], y[1], ..., y[N]}*
- en: 'Iterate until convergence of the following steps:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代以下步骤直到收敛：
- en: '![](img/892bd875-805e-429d-b1ab-f9a0b86176ee.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/892bd875-805e-429d-b1ab-f9a0b86176ee.png)'
- en: 'The first update performs a propagation step with both labeled and unlabeled
    points. Each label is spread from a node through its outgoing edges, and the corresponding
    weight, normalized with the degree, increases or decreases the *effect* of each
    contribution. The second command instead resets all *y* values for the labeled
    samples. The final labels can be obtained as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次更新执行了带有标记和无标记点的传播步骤。每个标签从一个节点通过其出边传播，相应的权重，与度数归一化后，增加或减少每个贡献的 *影响*。第二个命令则重置所有标记样本的
    *y* 值。最终的标签可以如下获得：
- en: '![](img/68bc4da1-a9a8-4ed4-b39e-bbd3bceb56eb.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68bc4da1-a9a8-4ed4-b39e-bbd3bceb56eb.png)'
- en: 'The proof of convergence is very easy. If we partition the matrix *D^(-1)W*
    according to the relationship among labeled and unlabeled samples, we get:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛的证明非常简单。如果我们根据标记和无标记样本之间的关系对矩阵 *D^(-1)W* 进行划分，我们得到：
- en: '![](img/a787530c-3ecc-4854-9855-63b2401ae8bf.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a787530c-3ecc-4854-9855-63b2401ae8bf.png)'
- en: 'If we consider that only the first *N* components of *Y* are non-null and they
    are clamped at the end of each iteration, the matrix can be rewritten as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑只有 *Y* 的前 *N* 个分量是非零的，并且在每次迭代的末尾被固定，则矩阵可以重写为：
- en: '![](img/e0f9ae2a-763c-4924-b8a5-99531e422ca7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0f9ae2a-763c-4924-b8a5-99531e422ca7.png)'
- en: 'We are interested in proving the convergence for the part regarding the unlabeled
    samples (the labeled ones are fixed), so we can write the update rule as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对证明无标记样本部分（标记样本是固定的）的收敛性感兴趣，因此我们可以将更新规则写为：
- en: '![](img/0b64dca0-af08-46d3-968f-bae377f18c79.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b64dca0-af08-46d3-968f-bae377f18c79.png)'
- en: 'Transforming the recursion into an iterative process, the previous formula
    becomes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将递归转换为迭代过程，前面的公式变为：
- en: '![](img/834692a0-7f1e-4423-aaa6-c62595755203.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/834692a0-7f1e-4423-aaa6-c62595755203.png)'
- en: 'In the previous expression, the second term is null, so we need to prove that
    the first term converges; however, it''s easy to recognize a truncated matrix
    geometrical series (Neumann series), and *A[UU]* is constructed to have all eigenvalues
    *|λ[i]| < 1*, therefore the series converges to:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，第二项为零，因此我们需要证明第一项收敛；然而，很容易识别一个截断的矩阵几何级数（Neumann 级数），并且 *A[UU]* 被构建以具有所有特征值
    *|λ[i]| < 1*，因此级数收敛到：
- en: '![](img/7abc27fc-c438-4f50-bba7-8bba0c25d66d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7abc27fc-c438-4f50-bba7-8bba0c25d66d.png)'
- en: Example of label propagation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签传播示例
- en: 'We can implement the algorithm in Python, using a test bidimensional dataset:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Python 实现该算法，使用一个测试的二维数据集：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As in the other examples, we set *y = 0* for all unlabeled samples (75 out
    of 100). The corresponding plot is shown in the following graph:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如其他示例一样，我们将所有无标记样本（100 个中的 75 个）的 *y* 设置为 0。相应的图如下所示：
- en: '![](img/6daecfc9-002c-40ce-86d5-d33559e5fb02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6daecfc9-002c-40ce-86d5-d33559e5fb02.png)'
- en: Partially labeled dataset
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 部分标记数据集
- en: 'The dots marked with a cross are unlabeled. At this point, we can define the
    affinity matrix. In this case, we compute it using both methods:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 带有交叉标记的点是无标记的。在此阶段，我们可以定义亲和矩阵。在这种情况下，我们使用两种方法来计算它：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The KNN matrix is obtained using the Scikit-Learn function `kneighbors_graph()`
    with the parameters `n_neighbors=2` and `mode='connectivity'`; the alternative
    is `'distance'`, which returns the distances instead of 0 and 1 to indicate the
    absence/presence of an edge. The `include_self=True` parameter is useful, as we
    want to have *W[ii] = 1*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 矩阵是通过 Scikit-Learn 函数 `kneighbors_graph()` 获得的，参数为 `n_neighbors=2` 和 `mode='connectivity'`；另一种选择是
    `'distance'`，它返回距离而不是 0 和 1 来表示边的存在/不存在。`include_self=True` 参数很有用，因为我们希望 *W[ii]
    = 1*。
- en: 'For the RBF matrix, we need to define it manually:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RBF 矩阵，我们需要手动定义它：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The default value for *γ* is *10*, corresponding to a standard deviation *σ*
    equal to *0.22*. When using this method, it''s important to set a correct value
    for *γ*; otherwise, the propagation can degenerate in the predominance of a class
    (*γ* too small). Now, we can compute the degree matrices and its inverse. As the
    procedure is identical, from this point on we continue using the RBF affinity
    matrix:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*γ* 的默认值是 *10*，对应的标准差 *σ* 等于 *0.22*。在使用此方法时，设置正确的 *γ* 值非常重要；否则，在某一类占主导地位的情况下（*γ*
    过小），传播可能会退化。现在，我们可以计算度矩阵及其逆矩阵。由于过程相同，从现在起我们将继续使用 RBF 相似度矩阵：'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The algorithm is implemented using a variable threshold. The value adopted
    here is `0.01`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用一个可变阈值。这里采用的值是 `0.01`：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The final result is shown in the following double plot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果在以下双图中显示：
- en: '![](img/6d34f804-4d77-4999-bc98-8a0f5ca99ae5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d34f804-4d77-4999-bc98-8a0f5ca99ae5.png)'
- en: Original dataset (left); dataset after a complete label propagation (right)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）；完整标签传播后的数据集（右）
- en: 'As it''s possible to see, in the original dataset there''s a round dot surrounded
    by square ones (-0.9, -1). As this algorithm keeps the original labels, we find
    the same situation after the propagation of labels. This condition could be acceptable,
    even if both the smoothness and clustering assumptions are contradicted. Assuming
    that it''s reasonable, it''s possible to force a *correction* by relaxing the
    algorithm:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在原始数据集中有一个圆形点被方形点包围（-0.9, -1）。由于此算法保留了原始标签，在标签传播后我们发现了相同的情况。这种条件可能是可以接受的，即使平滑性和聚类假设被违反。假设这是合理的，我们可以通过放松算法来强制进行
    *校正*：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this way, we don''t reset the original labels, letting the propagation change
    all those values that disagree with the neighborhood. The result is shown in the
    following plot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做，我们不重置原始标签，让传播改变所有与邻域不一致的值。结果在以下图中显示：
- en: '![](img/38d6bb5b-425c-41d3-9bfa-94ccb560e004.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/38d6bb5b-425c-41d3-9bfa-94ccb560e004.png)'
- en: Original dataset (left); dataset after a complete label propagation with overwrite
    (right)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）；完整标签传播后带有覆盖的数据集（右）
- en: Label propagation in Scikit-Learn
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的标签传播
- en: 'Scikit-Learn implements a slightly different algorithm proposed by Zhu and
    Ghahramani (in the aforementioned paper) where the affinity matrix *W* can be
    computed using both methods (KNN and RBF), but it is normalized to become a probability
    transition matrix:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 实现了 Zhu 和 Ghahramani 提出的略有不同的算法（在上述论文中提到），其中亲和度矩阵 *W* 可以使用两种方法（KNN
    和 RBF）计算，但被归一化成为一个概率转移矩阵：
- en: '![](img/14a975e7-8317-4b6b-9476-6aab77891f64.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14a975e7-8317-4b6b-9476-6aab77891f64.png)'
- en: 'The algorithm operates like a Markov random walk, with the following sequence
    (assuming that there are *Q* different labels):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法像马尔可夫随机游走一样操作，以下是一个序列（假设有 *Q* 个不同的标签）：
- en: Define a matrix *Y^M[i] = [P(label=y[0]), P(label=y[1]), ..., and P(label=y[Q])]*,
    where *P(label=yi)* is the probability of the label *yi*, and each row is normalized
    so that all the elements sum up to *1*
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个矩阵 *Y^M[i] = [P(label=y[0]), P(label=y[1]), ..., and P(label=y[Q])]*)，其中
    *P(label=yi)* 是标签 *yi* 的概率，并且每一行都归一化，使得所有元素之和等于 *1*
- en: Define *Y^((0)) = Y^M*
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 *Y^((0)) = Y^M*
- en: 'Iterate until convergence of the following steps:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代直到以下步骤收敛：
- en: '![](img/9595ccab-77ee-404e-8a4d-d631644d7591.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9595ccab-77ee-404e-8a4d-d631644d7591.png)'
- en: 'The first update performs a label propagation step. As we''re working with
    probabilities, it''s necessary (second step) to renormalize the rows so that their
    element sums up to *1*. The last update resets the original labels for all labeled
    samples. In this case, it means imposing a *P(label=y[i]) = 1* to the corresponding
    label, and setting all the others to zero. The proof of convergence is very similar
    to the one for label propagation algorithms, and can be found in *Learning from
    Labeled and Unlabeled Data with Label Propagation*,*Zhu X.*, *Ghahramani Z.*,
    *CMU-CALD-02-107. *The most important result is that the solution can be obtained
    in closed form (without any iteration) through this formula:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次更新执行标签传播步骤。由于我们处理的是概率，因此有必要（第二步）重新归一化行，使它们的元素之和为*1*。最后一次更新重置所有标记样本的原始标签。在这种情况下，这意味着对相应的标签施加*P(label=y[i])
    = 1*，并将所有其他设置为零。收敛的证明与标签传播算法的证明非常相似，可以在*《基于标签传播的带标签和无标签数据的机器学习》，*Zhu X.*，*Ghahramani
    Z.*，*CMU-CALD-02-107.*中最重要结果是，可以通过这个公式（无需任何迭代）获得封闭形式的解：
- en: '![](img/fc8a69f4-f2cd-4e5c-b878-219bc03ef988.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc8a69f4-f2cd-4e5c-b878-219bc03ef988.png)'
- en: The first term is the sum of a generalized geometric series, where *P[uu]* is
    the unlabeled-unlabeled part of the transition matrix *P*. *P[ul]*, instead, is
    the unlabeled-labeled part of the same matrix.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项是一个广义几何级数的和，其中*P[uu]*是转移矩阵*P*中未标记-未标记的部分。*P[ul]*，相反，是同一矩阵中未标记-标记的部分。
- en: 'For our Python example, we need to build the dataset differently, because Scikit-Learn
    considers a sample unlabeled if *y=-1*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的Python示例，我们需要以不同的方式构建数据集，因为Scikit-Learn将标签为*y=-1*的样本视为未标记：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now train a `LabelPropagation` instance with an RBF kernel and `gamma=10.0`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用具有RBF核和`gamma=10.0`的`LabelPropagation`实例进行训练：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result is shown in the following double plot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下双图所示：
- en: '![](img/3871338c-0515-4545-b288-00b1cf2319e9.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3871338c-0515-4545-b288-00b1cf2319e9.png)'
- en: Original dataset (left). Dataset after a Scikit-Learn label propagation (right)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）。经过Scikit-Learn标签传播后的数据集（右）
- en: As expected, the propagation converged to a solution that respects both the
    smoothness and the clustering assumption.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，传播收敛到一个既满足平滑性又满足聚类假设的解。
- en: Label spreading
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签传播
- en: 'The last algorithm (proposed by Zhou et al.) that we need to analyze is called
    **label spreading**, and it''s based on the normalized graph Laplacian:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个需要分析的算法（由Zhou等人提出）称为**标签传播**，它基于归一化图拉普拉斯算子：
- en: '![](img/86147c4f-38c0-4a51-af9d-81278e3fe790.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/86147c4f-38c0-4a51-af9d-81278e3fe790.png)'
- en: 'This matrix has each a diagonal element *l[ii]* equal to *1*, if the degree
    *deg(l[ii]) > 0* (0 otherwise) and all the other elements equal to:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的每个对角元素*l[ii]*等于*1*，如果度*deg(l[ii]) > 0*（否则为0），所有其他元素等于：
- en: '![](img/30c5dc27-5c4b-4b06-92bc-3c4c24322f69.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/30c5dc27-5c4b-4b06-92bc-3c4c24322f69.png)'
- en: 'The behavior of this matrix is analogous to a discrete Laplacian operator,
    whose real-value version is the fundamental element of all diffusion equations.
    To better understand this concept, let''s consider the generic heat equation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的行为类似于离散拉普拉斯算子，其实值版本是所有扩散方程的基本元素。为了更好地理解这个概念，让我们考虑一个通用的热方程：
- en: '![](img/25203c70-650d-4ac9-8d41-f9b06964de5d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/25203c70-650d-4ac9-8d41-f9b06964de5d.png)'
- en: 'This equation describes the behavior of the temperature of a room when a point
    is suddenly heated. From basic physics concepts, we know that heat will spread
    until the temperature reaches an equilibrium point and the speed of variation
    is proportional to the Laplacian of the distribution. If we consider a bidimensional
    grid at the equilibrium (the derivative with respect to when time becomes null)
    and we discretize the Laplacian operator (*∇² = ∇ · ∇*) considering the incremental
    ratios, we obtain:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程描述了当某一点突然加热时房间温度的行为。从基本的物理概念中，我们知道热量会传播，直到温度达到平衡点，变化的速率与分布的拉普拉斯算子成正比。如果我们考虑平衡状态下的二维网格（当时间变为零时的导数）并离散化拉普拉斯算子（*∇²
    = ∇ · ∇*），考虑增量比率，我们得到：
- en: '![](img/d6c5dfc2-8f95-4cf8-883f-a5035e5aa388.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6c5dfc2-8f95-4cf8-883f-a5035e5aa388.png)'
- en: Therefore, at the equilibrium, each point has a value that is the mean of the
    direct neighbors. It's possible to prove the finite-difference equation has a
    single fixed point that can be found iteratively, starting from every initial
    condition. In addition to this idea, label spreading adopts a clamping factor *α*
    for the labeled samples. If *α=0*, the algorithm will always reset the labels
    to the original values (like for label propagation), while with a value in the
    interval *(0, 1]*, the percentage of clamped labels decreases progressively until *α=1*,
    when all the labels are overwritten.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在平衡状态下，每个点都有一个值，它是直接邻居的平均值。可以证明有限差分方程有一个唯一的固定点，可以从每个初始条件迭代找到。除了这个想法之外，标签传播还采用一个夹紧因子
    *α* 来处理标记样本。如果 *α=0*，则算法将始终将标签重置为原始值（类似于标签传播），而当 *α* 在区间 *(0, 1)* 内时，夹紧标签的百分比会逐渐减少，直到
    *α=1*，此时所有标签都将被覆盖。
- en: 'The complete steps of the **label spreading** algorithm are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签传播**算法的完整步骤是：'
- en: Select an affinity matrix type (KNN or RBF) and compute *W*
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择亲和矩阵类型（KNN 或 RBF）并计算 *W*
- en: Compute the degree matrix *D*
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算度矩阵 *D*
- en: Compute the normalized graph Laplacian *L*
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算归一化图拉普拉斯矩阵 *L*
- en: Define *Y^((0)) = Y*
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 *Y^((0)) = Y*
- en: Define *α* in the interval *[0, 1]*
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在区间 *[0, 1]* 中定义 *α*
- en: 'Iterate until convergence of the following step:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代直到以下步骤收敛：
- en: '![](img/f1abf280-e4cf-4d61-9f5a-75043a721e23.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1abf280-e4cf-4d61-9f5a-75043a721e23.png)'
- en: 'It''s possible to show (as demonstrated in *Semi-Supervised Learning*, *Chapelle
    O.*,* Schölkopf B*., *Zien A.*, (edited by), *The MIT Press*) that this algorithm
    is equivalent to the minimization of a quadratic cost function with the following
    structure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能证明（正如在 *《半监督学习》* 中所展示的，*Chapelle O.*，*Schölkopf B*., *Zien A.* 编著，*麻省理工学院出版社*）该算法等价于最小化具有以下结构的二次成本函数：
- en: '![](img/b4b79495-24d9-4ce3-a660-bb9ba37f65c5.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4b79495-24d9-4ce3-a660-bb9ba37f65c5.png)'
- en: The first term imposes consistency between original labels and estimated ones
    (for the labeled samples). The second term acts as a normalization factor, forcing
    the unlabeled terms to become zero, while the third term, which is probably the
    least intuitive, is needed to guarantee geometrical coherence in terms of smoothness.
    As we have seen in the previous paragraph, when a hard-clamping is adopted, the
    smoothness assumption could be violated. By minimizing this term (*μ* is proportional
    to *α*), it's possible to penalize the rapid changes inside the high-density regions.
    Also in this case, the proof of convergence is very similar to the one for label
    propagation algorithms, and will be omitted. The interested reader can find it
    in *Semi-Supervised Learning*,*Chapelle O.*,* Schölkopf B.*, *Zien A.*, (edited
    by), *The MIT Press.*
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项强制原始标签和估计标签（对于标记样本）之间的一致性。第二项作为归一化因子，迫使未标记项变为零，而第三项，可能是最不直观的，需要保证在平滑性方面的几何一致性。正如我们在上一段中看到的，当采用硬限制时，平滑性假设可能会被违反。通过最小化这一项（*μ*
    与 *α* 成正比），可以在高密度区域内惩罚快速变化。在这种情况下，收敛性的证明与标签传播算法的证明非常相似，因此将省略。感兴趣的读者可以在 *《半监督学习》*，*Chapelle
    O.*，*Schölkopf B.*，*Zien A.*，(编者)，*麻省理工学院出版社* 中找到。
- en: Example of label spreading
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签传播的示例
- en: 'We can test this algorithm using the Scikit-Learn implementation. Let''s start
    by creating a very dense dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Scikit-Learn 的实现来测试这个算法。让我们首先创建一个非常密集的数据集：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can train a `LabelSpreading` instance with a clamping factor `alpha=0.2`.
    We want to preserve 80% of the original labels but, at the same time, we need
    a smooth solution:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用具有夹紧因子 `alpha=0.2` 的 `LabelSpreading` 实例进行训练。我们希望保留 80% 的原始标签，但同时也需要一个平滑的解决方案：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result is shown, as usual, together with the original dataset:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结果通常与原始数据集一起展示：
- en: '![](img/40b3c130-fc58-47bd-b456-230f36cbd7e9.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40b3c130-fc58-47bd-b456-230f36cbd7e9.png)'
- en: Original dataset (left). Dataset after a complete label spreading (right)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集（左）。完全标签传播后的数据集（右）
- en: As it's possible to see in the first figure (left), in the central part of the
    cluster *(x [-1, 0])*, there's an area of circle dots. Using a hard-clamping,
    this *aisle *would remain unchanged, violating both the smoothness and clustering
    assumptions. Setting *α > 0*, it's possible to avoid this problem. Of course,
    the choice of *α* is strictly correlated with each single problem. If we know
    that the original labels are absolutely correct, allowing the algorithm to change
    them can be counterproductive. In this case, for example, it would be better to
    preprocess the dataset, filtering out all those samples that violate the semi-supervised
    assumptions. If, instead, we are not sure that all samples are drawn from the
    same *p[data]*, and it's possible to be in the presence of spurious elements,
    using a higher *α* value can smooth the dataset without any other operation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如第一幅图（左）所示，在簇的中央部分 *(x [-1, 0])* 中，有一个圆形点的区域。使用硬限制，这个 *通道* 将保持不变，这违反了平滑性和聚类假设。设置
    *α > 0*，可以避免这个问题。当然，*α* 的选择与每个单独的问题严格相关。如果我们知道原始标签绝对正确，允许算法更改它们可能是适得其反的。在这种情况下，例如，最好预处理数据集，过滤掉所有违反半监督假设的样本。如果我们不确定所有样本是否都来自相同的
    *p[data]*，并且可能存在虚假元素，使用更高的 *α* 值可以在不进行其他操作的情况下平滑数据集。
- en: Label propagation based on Markov random walks
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于马尔可夫随机游走的标签传播
- en: The goal of this algorithm proposed by Zhu and Ghahramani is to find the probability
    distribution of target labels for unlabeled samples given a mixed dataset. This
    objective is achieved through the simulation of a stochastic process, where each
    unlabeled sample walks through the graph until it reaches a stationary absorbing
    state, a labeled sample where it stops acquiring the corresponding label. The
    main difference with other similar approaches is that in this case, we consider
    the probability of reaching a labeled sample. In this way, the problem acquires
    a closed form and can be easily solved.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Zhu 和 Ghahramani 提出的这个算法的目标是在一个混合数据集给定的情况下，找到未标记样本的目标标签的概率分布。这个目标是通过模拟一个随机过程来实现的，在这个过程中，每个未标记样本在图中行走，直到达到一个稳定的吸收状态，即停止获取相应标签的标记样本。与其他类似方法的主要区别在于，在这种情况下，我们考虑到达标记样本的概率。通过这种方式，问题获得了一个封闭形式，并且可以很容易地解决。
- en: 'The first step is to always build a k-nearest neighbors graph with all *N*
    samples, and define a weight matrix *W* based on an RBF kernel:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是始终构建一个包含所有 *N* 样本的 k-最近邻图，并基于 RBF 内核定义一个权重矩阵 *W*：
- en: '![](img/19f63010-f444-401f-bcd3-b9e9172a32e9.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19f63010-f444-401f-bcd3-b9e9172a32e9.png)'
- en: '*W[ij] = 0* is *x*[*i*, ]and *x[j]* are not neighbors and *W[ii] = 1*. The
    transition probability matrix, similarly to the Scikit-Learn label propagation
    algorithm, is built as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*W[ij] = 0* 是 *x*[*i*, ]和 *x[j]* 不是邻居，*W[ii] = 1*。与 Scikit-Learn 标签传播算法类似，转换概率矩阵是构建的：'
- en: '![](img/4f1890c2-e927-4afb-9fa3-27ef6492dbe3.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f1890c2-e927-4afb-9fa3-27ef6492dbe3.png)'
- en: 'In a more compact way, it can be rewritten as *P = D^(-1)W*. If we now consider
    a *test sample*, starting from the state *x[i]* and randomly walking until an
    absorbing labeled state is found (we call this label *y^∞*), the probability (referred
    to as **binary classification**) can be expressed as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以更紧凑的方式，它可以重写为 *P = D^(-1)W*。如果我们现在考虑一个 *测试样本*，从状态 *x[i]* 开始，随机行走直到找到一个吸收的标记状态（我们称之为
    *y^∞*），这个概率（称为**二元分类**）可以表示为：
- en: '![](img/8b36e8bb-1b5c-4ec3-bebb-8512091ca2f7.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b36e8bb-1b5c-4ec3-bebb-8512091ca2f7.png)'
- en: When *x[i]* is labeled, the state is final, and it is represented by the indicator
    function based on the condition *y[i]=1*. When the sample is unlabeled, we need
    to consider the sum of all possible transitions starting from *x[i]* and ending
    in the closest absorbing state, with label *y=1* weighted by the relative transition
    probabilities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *x[i]* 被标记时，状态是最终的，它由基于条件 *y[i]=1* 的指示函数表示。当样本未标记时，我们需要考虑从 *x[i]* 开始并结束在最近的吸收状态的所有可能的转换的总和，该状态带有标签
    *y=1*，并按相对转换概率加权。
- en: 'We can rewrite this expression in matrix form. If we create a vector *P^∞ =
    [ P[L](y^∞=1|X[L])*, *P[U](y^∞=1|X[U]) ]*, where the first component is based
    on labeled samples and the second on the unlabeled ones, we can write:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个表达式重写为矩阵形式。如果我们创建一个向量 *P^∞ = [ P[L](y^∞=1|X[L])*, *P[U](y^∞=1|X[U]) ]*，其中第一个分量基于标记样本，第二个基于未标记样本，我们可以写出：
- en: '![](img/693fff6a-7cf3-47ca-b449-706521663575.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/693fff6a-7cf3-47ca-b449-706521663575.png)'
- en: 'If we now expand the matrices, we get:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在扩展矩阵，我们得到：
- en: '![](img/81ec3959-f4ac-4ee5-ba07-e2b65dded4ff.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/81ec3959-f4ac-4ee5-ba07-e2b65dded4ff.png)'
- en: 'As we are interested only in the unlabeled samples, we can consider only the
    second equation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只对未标记的样本感兴趣，我们可以只考虑第二个方程：
- en: '![](img/24164fb4-ff31-4d99-af72-d5d67c3fc335.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/24164fb4-ff31-4d99-af72-d5d67c3fc335.png)'
- en: 'Simplifying the expression, we get the following linear system:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 简化表达式，我们得到以下线性系统：
- en: '![](img/dee84fb5-6771-46b1-a953-51816cfec912.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dee84fb5-6771-46b1-a953-51816cfec912.png)'
- en: The term *(D[uu] - W[uu])* is the unlabeled-unlabeled part of the unnormalized
    graph Laplacian *L = D - W*. By solving this system, we can get the probabilities
    for the class *y=1* for all unlabeled samples.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 项 *(D[uu] - W[uu])* 是未归一化图拉普拉斯矩阵 *L = D - W* 的未标记-未标记部分。通过解这个系统，我们可以得到所有未标记样本对于类别
    *y=1* 的概率。
- en: Example of label propagation based on Markov random walks
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于马尔可夫随机游走的标签传播示例
- en: 'For this Python example of label propagation based on Markov random walks,
    we are going to use a bidimensional dataset containing 50 labeled samples belonging
    to two different classes, and 1,950 unlabeled samples:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个基于马尔可夫随机游走的 Python 标签传播示例，我们将使用一个包含 50 个标记样本（属于两个不同的类别）和 1,950 个未标记样本的二维数据集：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The plot of the dataset is shown in the following diagram (the crosses represent
    the unlabeled samples):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的图示如下（交叉点代表未标记的样本）：
- en: '![](img/4eb4f91d-25b3-4345-be04-e504f66fe33f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4eb4f91d-25b3-4345-be04-e504f66fe33f.png)'
- en: Partially labeled dataset
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 部分标记的数据集
- en: 'We can now create the graph (using `n_neighbors=15`) and the weight matrix:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建图（使用 `n_neighbors=15`）和权重矩阵：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we need to compute the unlabeled part of the unnormalized graph Laplacian
    and the unlabeled-labeled part of the matrix *W*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算未归一化图拉普拉斯矩阵的未标记部分和矩阵 *W* 的未标记-标记部分：
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, it''s possible to solve the linear system using the NumPy function
    `np.linalg.solve()`, which accepts as parameters the matrix *A* and the vector
    *b* of a generic system in the form *Ax=b*. Once we have the solution, we can
    merge the new labels with the original ones (where the unlabeled samples have
    been marked with *-1*). In this case, we don''t need to convert the probabilities,
    because we are using *0* and *1* as labels. In general, it''s necessary to use
    a threshold (0.5) to select the right label:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以使用 NumPy 函数 `np.linalg.solve()` 解这个线性系统，该函数接受一个形式为 *Ax=b* 的通用系统的矩阵 *A*
    和向量 *b* 作为参数。一旦我们得到解，我们可以将新的标签与原始标签合并（其中未标记的样本被标记为 *-1*）。在这种情况下，我们不需要转换概率，因为我们使用
    *0* 和 *1* 作为标签。通常，需要使用一个阈值（0.5）来选择正确的标签：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Replotting the dataset, we get:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 重新绘制数据集，我们得到：
- en: '![](img/f7f28e6c-b0ab-4ec9-9f9c-ffbcf24992f5.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f7f28e6c-b0ab-4ec9-9f9c-ffbcf24992f5.png)'
- en: Dataset after a complete Markov random walk label propagation
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 完整马尔可夫随机游走标签传播后的数据集
- en: 'As expected, without any iteration, the labels have been successfully propagated
    to all samples in perfect compliance with the clustering assumption. Both this
    algorithm and label propagation can work using a closed-form solution, so they
    are very fast even when the number of samples is high; however, there''s a fundamental
    problem regarding the choice of *σ/γ* for the RBF kernel. As the same authors Zhu
    and Ghahramani remark, there is no standard solution, but it''s possible to consider
    when *σ → 0* and when *σ → ∞*. In the first case, only the nearest point has an
    influence, while in the second case, the influence is extended to the whole sample
    space, and the unlabeled points tend to acquire the same label. The authors suggest
    considering the entropy of all samples, trying to find the best σ value that minimizes
    it. This solution can be very effective, but sometimes the minimum entropy corresponds
    to a label configuration that isn''t impossible to achieve using these algorithms.
    The best approach is to try different values (at different scales) and select
    the one corresponding to a valid configuration with the lowest entropy. In our
    case, it''s possible to compute the entropy of the unlabeled samples as:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，没有进行任何迭代，标签已经成功传播到所有样本，完全符合聚类假设。这个算法和标签传播都可以使用闭式解来工作，因此即使样本数量很高，它们也非常快；然而，关于RBF核中*σ/γ*的选择存在一个基本问题。正如同一作者Zhu和Ghahramani所指出的，没有标准解决方案，但可以考虑当*σ →
    0*和当*σ → ∞*时的情况。在前一种情况下，只有最近邻点有影响，而在后一种情况下，影响扩展到整个样本空间，未标记的点倾向于获得相同的标签。作者建议考虑所有样本的熵，试图找到最小化它的最佳σ值。这种解决方案可能非常有效，但有时最小熵对应于使用这些算法不可能实现的标签配置。最佳方法是尝试不同的值（在不同尺度上），并选择对应于具有最低熵的有效配置的那个值。在我们的情况下，可以计算未标记样本的熵如下：
- en: '![](img/14d06359-bf07-4221-8044-e51d200428de.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14d06359-bf07-4221-8044-e51d200428de.png)'
- en: 'The Python code to perform this computation is:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此计算的Python代码如下：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The term `1e-6` has been added to avoid numerical problems when the probability
    is null. Repeating this process for different values allows us to find a set of
    candidates that can be restricted to a single value with a direct evaluation of
    the labeling accuracy (for example, when there is no precise information about
    the real distribution, it''s possible to consider the coherence of each cluster
    and the separation between them). Another approach is called **class rebalancing**,
    and it''s based on the idea of reweighting the probabilities of unlabeled samples
    to rebalance the number of points belonging to each class when the new unlabeled
    samples are added to the set. If we have *N* labeled points and *M* unlabeled
    ones, with *K* classes, the weight factor *w[j]* for the class *j* can be obtained
    as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免概率为零时的数值问题，已经添加了`1e-6`这个项。对不同的值重复这个过程，我们可以找到一组候选值，可以通过直接评估标签准确性来限制为单个值（例如，当没有关于真实分布的精确信息时，可以考虑每个簇的连贯性和它们之间的分离）。另一种方法称为**类别重平衡**，它基于重新加权未标记样本的概率，以在将新的未标记样本添加到集合时重新平衡每个类别的点数。如果我们有*N*个标记点和*M*个未标记点，以及*K*个类别，类别*j*的权重因子*w[j]*可以表示为：
- en: '![](img/5ae97f0c-83f4-434d-bcf1-c0b4abf09c59.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ae97f0c-83f4-434d-bcf1-c0b4abf09c59.png)'
- en: 'The numerator is the average computed over the labeled samples belonging to
    class *k*, while the denominator is the average over the unlabeled ones whose
    estimated class is *k*. The final decision about a class is no longer based only
    on the highest probability, but on:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 分子是对属于类别*k*的标记样本的平均值，而分母是对估计类别为*k*的未标记样本的平均值。关于类别的最终决定不再仅仅基于最高的概率，而是基于：
- en: '![](img/d0c00d0b-2fd0-4c13-93d8-67f3c04f34fd.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0c00d0b-2fd0-4c13-93d8-67f3c04f34fd.png)'
- en: Manifold learning
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形学习
- en: In [Chapter 02](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction to
    Semi-Supervised Learning*, we discussed the manifold assumption, saying that high-dimensional
    data normally lies on low-dimensional manifolds. Of course, this is not a theorem,
    but in many real cases, the assumption is proven to be correct, and it allows
    us to work with non-linear dimensionality reduction algorithms that would be otherwise
    unacceptable. In this section, we're going to analyze some of these algorithms.
    They are all implemented in Scikit-Learn, therefore it's easy to try them with
    complex datasets.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml)《半监督学习导论》中，我们讨论了流形假设，即高维数据通常位于低维流形上。当然，这并不是一个定理，但在许多实际情况下，这个假设已被证明是正确的，并且它允许我们使用在其它情况下不可接受的非线性降维算法。在本节中，我们将分析一些这些算法。它们都已在Scikit-Learn中实现，因此使用复杂数据集尝试它们很容易。
- en: Isomap
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Isomap
- en: '**Isomap** is one of the simplest algorithms, and it''s based on the idea of
    reducing the dimensionality while trying to preserve the geodesic distances measured
    on the original manifold where the input data lies. The algorithm works in three
    steps. The first operation is a k-nearest neighbors clustering and the construction
    of the following graph. The vertices will be the samples, while the edges represent
    the connections among nearest neighbors, and their weight is proportional to the
    distance to the corresponding neighbor.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**Isomap**是最简单的算法之一，它基于在尝试保留原始流形上测量的测地距离的同时降低维度的想法。该算法分为三个步骤。第一步是k-最近邻聚类和以下图的构建。顶点将是样本，而边表示最近邻之间的连接，它们的权重与对应邻居的距离成正比。'
- en: 'The second step adopts the **Dijkstra algorithm** to compute the shortest pairwise
    distances on the graph of all couples of samples. In the following graph, there''s
    a portion of a graph, where some shortest distances are marked:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步采用**Dijkstra算法**计算所有样本对在图上的最短距离。在下面的图中，有一部分图，其中标记了一些最短距离：
- en: '![](img/030d05e4-9518-4067-8ca2-8431a703a413.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/030d05e4-9518-4067-8ca2-8431a703a413.png)'
- en: Example of a graph with marked shortest distances
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 标记最短距离的图示例
- en: For example, as *x[3]* is a neighbor of *x[5]* and *x[7]*, applying the Dijkstra
    algorithm, we could get the shortest paths *d(x[3], x[5]) = w[53]* and *d(x[3], x[7])
    = w[73]*. The computational complexity of this step is about *O(n²log n + n²k)*,
    which is lower than *O(n³)* when *k << n* (a condition normally met); however,
    for large graphs (with *n >> 1*), this is often the most expensive part of the
    whole algorithm.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于*x[3]*是*x[5]*和*x[7]*的邻居，应用Dijkstra算法，我们可以得到最短路径*d(x[3], x[5]) = w[53]*和*d(x[3],
    x[7]) = w[73]*。这一步骤的计算复杂度大约为*O(n²log n + n²k)*，当*k << n*（通常满足的条件）时，低于*O(n³)*；然而，对于大型图（*n
    >> 1*），这通常是整个算法中最昂贵的部分。
- en: 'The third step is called **metric multidimensional scaling**, which is a technique
    for finding a low-dimensional representation while trying to preserve the inner
    product among samples. If we have a *P*-dimensional dataset *X*, the algorithm
    must find a *Q*-dimensional set *Φ* with *Q < P* minimizing the function:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步被称为**度量多维尺度**，这是一种在尝试保留样本之间的内积的同时寻找低维表示的技术。如果我们有一个*P*维数据集*X*，算法必须找到一个*Q*维集合*Φ*，其中*Q
    < P*，以最小化以下函数：
- en: '![](img/a316c204-dade-4948-8693-15f2d01c96bf.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a316c204-dade-4948-8693-15f2d01c96bf.png)'
- en: 'As proven in *Semi-Supervised Learning*  *Chapelle O.*,* Schölkopf B.*, *Zien
    A.*, (edited by), *The MIT Press*,the optimization is achieved by taking the top
    *Q* eigenvectors of the Gram matrix *G[ij] = x[i] · x*[*j* ](or in matrix form,
    *G=XX^T* if *X ∈ ℜ^(n × M)*); however, as the **Isomap** algorithm works with
    pairwise distances, we need to compute the matrix *D* of squared distances:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*Semi-Supervised Learning* *Chapelle O.*,* Schölkopf B.*, *Zien A.* (编辑)，*麻省理工学院出版社*中证明，优化是通过取Gram矩阵*G[ij]
    = x[i] · x[*j*](或以矩阵形式，如果*X* ∈ ℜ^(n × M)，则*G=XX^T*)的前*Q*个特征向量来实现的；然而，由于**Isomap**算法使用成对距离，我们需要计算平方距离矩阵*D*：
- en: '![](img/4a0e1b64-7619-4716-b7c1-a03f465db905.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a0e1b64-7619-4716-b7c1-a03f465db905.png)'
- en: 'If the *X* dataset is zero-centered, it''s possible to derive a simplified
    Gram matrix from *D*, as described by M. A. A. Cox and T. F. Cox:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X*数据集是零中心的，则可以从*D*中推导出一个简化的Gram矩阵，如M. A. A. Cox和T. F. Cox所述：
- en: '![](img/10897a36-bcd6-411b-8743-f727a6a29cc8.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10897a36-bcd6-411b-8743-f727a6a29cc8.png)'
- en: '**Isomap** computes the top *Q* eigenvalues *λ[1], λ2, ..., λ[Q]* of *G*[*D* ]and
    the corresponding eigenvectors *ν[1], ν[2]**, ..., ν[Q]* and determines the *Q*-dimensional
    vectors as:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**Isomap** 计算出 *G[*D*] 的前 *Q* 个特征值 *λ[1]，λ2，...，λ[Q]* 和相应的特征向量 *ν[1]，ν[2]**，...，ν[Q]*，并确定
    *Q*-维向量：'
- en: '![](img/b2983ed5-32f1-4d17-af07-2367b64f4cdb.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2983ed5-32f1-4d17-af07-2367b64f4cdb.png)'
- en: 'As we''re going to discuss in [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml),
    *EM Algorithm and Applications* (and also as pointed out by Saul, Weinberger,
    Sha, Ham, and Lee in *Spectral Methods for Dimensionality Reduction*,*Saul L.
    K., Weinberger K. Q.*, *Sha F.*, *Ham J.*, *and Lee D. D.*), this kind of projection
    is also exploited by **Principal Component Analysis** (**PCA**), which finds out
    the direction with the highest variance, corresponding to the top k eigenvectors
    of the covariance matrix. In fact, when applying the SVD to the dataset *X*, we
    get:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在第 5 章[EM 算法和应用](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)中讨论的（以及 Saul、Weinberger、Sha、Ham
    和 Lee 在《降维的谱方法》*Spectral Methods for Dimensionality Reduction*，Saul L. K.、Weinberger
    K. Q.、Sha F.、Ham J.、Lee D. D. 指出的），这种投影也被主成分分析（**PCA**）所利用，它找出协方差矩阵的最高方差方向，对应于协方差矩阵的前
    k 个特征向量。实际上，当对数据集 *X* 应用奇异值分解（SVD）时，我们得到：
- en: '![](img/ae41243f-3035-4652-93bf-969ff0301b9a.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae41243f-3035-4652-93bf-969ff0301b9a.png)'
- en: The diagonal matrix *Λ* contains the eigenvalues of both *XX^T* and *X^TX*; therefore,
    the eigenvalues *λ[Gi]* of *G* are equal to *Mλ[^Σ][i]* where *λ[^Σ]*[*i* ]are
    the eigenvalues of the covariance matrix *Σ = M^(-1)X^TX*. Hence, Isomap achieves
    the dimensionality reduction, trying to preserve the pairwise distances, while
    projecting the dataset in the subspace determined by a group of eigenvectors,
    where the maximum explained variance is achieved. In terms of information theory,
    this condition guarantees the minimum loss with an effective reduction of dimensionality.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对角矩阵 *Λ* 包含了 *XX^T* 和 *X^TX* 的特征值；因此，*G* 的特征值 *λ[Gi]* 等于 *Mλ[^Σ][i]*，其中 *λ[^Σ]*[*i*]
    是协方差矩阵 *Σ = M^(-1)X^TX* 的特征值。因此，Isomap 通过在由一组特征向量确定的子空间中投影数据集，尝试保留成对距离，从而实现降维，同时达到最大解释方差。从信息论的角度来看，这个条件保证了最小损失和有效的降维。
- en: Scikit-Learn also implements the Floyd-Warshall algorithm, which is slightly slower.
    For further information, please refer to *Introduction to Algorithms*, *Cormen
    T. H.*, *Leiserson C. E.*, *Rivest R. L.*, *The MIT Press*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 还实现了 Floyd-Warshall 算法，它稍微慢一些。有关更多信息，请参阅 *算法导论*，Cormen T. H.、Leiserson
    C. E.、Rivest R. L.、MIT 出版社。
- en: Example of Isomap
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Isomap 示例
- en: 'We can now test the Scikit-Learn **Isomap** implementation using the Olivetti
    faces dataset (provided by AT&T Laboratories, Cambridge), which is made up of
    400 64 × 64 grayscale portraits belonging to 40 different people. Examples of
    these images are shown here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 Olivetti 人脸数据集（由 AT&T 实验室，剑桥提供）测试 Scikit-Learn 的 **Isomap** 实现，该数据集由
    400 个 64 × 64 灰度肖像组成，属于 40 个不同的人。这些图像的示例如下所示：
- en: '![](img/bc559a7c-5399-4618-a868-bc6956689dc2.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc559a7c-5399-4618-a868-bc6956689dc2.png)'
- en: Subset of the Olivetti faces dataset
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Olivetti 人脸数据集的子集
- en: The original dimensionality is 4096, but we want to visualize the dataset in
    two dimensions. It's important to understand that using the Euclidean distance
    for measuring the similarity of images might not the best choice, and it's surprising
    to see how well the samples are clustered by such a simple algorithm.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 原始维度是 4096，但我们希望将数据集可视化在二维空间中。重要的是要理解，使用欧几里得距离来衡量图像的相似性可能不是最佳选择，而且令人惊讶的是，这些样本如何被这样一个简单的算法很好地聚类。
- en: 'The first step is loading the dataset:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载数据集：
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `faces` dictionary contains three main elements:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`faces` 字典包含三个主要元素：'
- en: '`images`: Image array with shape 400 × 64 × 64'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images`: 形状为 400 × 64 × 64 的图像数组'
- en: '`data`: Flattened array with shape 400 × 4096'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`: 形状为 400 × 4096 的展平数组'
- en: '`target`: Array with shape 400 × 1 containing the labels (0, 39)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`: 形状为 400 × 1 的数组，包含标签（0, 39）'
- en: 'At this point, we can instantiate the `Isomap` class provided by Scikit-Learn,
    setting `n_components=2` and `n_neighbors=5` (the reader can try different configurations),
    and then fitting the model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以实例化 Scikit-Learn 提供的 `Isomap` 类，设置 `n_components=2` 和 `n_neighbors=5`（读者可以尝试不同的配置），然后拟合模型：
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As the resulting plot with 400 elements is very dense, I preferred to show
    in the following plot only the first 100 samples:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成的包含 400 个元素的图非常密集，我更喜欢在下面的图中只展示前 100 个样本：
- en: '![](img/fb25fa4e-0e12-4f89-b6d3-8fd8fdc74b16.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb25fa4e-0e12-4f89-b6d3-8fd8fdc74b16.png)'
- en: Isomap applied to 100 samples drawn from the Olivetti faces dataset
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 将Isomap应用于从Olivetti人脸数据集中抽取的100个样本
- en: 'As it''s possible to see, samples belonging to the same class are grouped in
    rather dense agglomerates. The classes that seem better separated are 7 and 1\.
    Checking the corresponding faces, for class 7, we get:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，属于同一类的样本被分组在相当密集的聚团中。看起来分离得更好的类是7和1。检查相应的面孔，对于第7类，我们得到：
- en: '![](img/26a4a525-f57f-4fd0-9a2e-b2da1046cd20.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/26a4a525-f57f-4fd0-9a2e-b2da1046cd20.png)'
- en: Samples belonging to class 7
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 属于第7类的样本
- en: 'The set contains portraits of a young woman with a fair complexion, quite different
    from the majority of other people. Instead, for class 1, we get:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该集合包含一位肤色白皙的年轻女性的肖像，与其他大多数人截然不同。而对于第1类，我们得到：
- en: '![](img/0aedec99-9ac9-4df5-816a-2a68bdb52129.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0aedec99-9ac9-4df5-816a-2a68bdb52129.png)'
- en: Samples belonging to class 1
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 属于第1类的样本
- en: In this case, it's a man with big glasses and a particular mouth expression.
    In the dataset, there are only a few people with glasses, and one of them has
    a dark beard. We can conclude that **Isomap** created a low-dimensional representation
    that is really coherent with the original geodesic distances. In some cases, there's
    a partial clustering overlap that can be mitigated by increasing the dimensionality
    or adopting a more complex strategy.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这是一个戴着大眼镜并且有特定嘴型表情的男人。在数据集中，只有少数人戴眼镜，其中一个人有浓密的胡须。我们可以得出结论，**Isomap**创建了一个与原始测地距离高度一致的低维表示。在某些情况下，存在部分聚类重叠，可以通过增加维度或采用更复杂的策略来缓解。
- en: Locally linear embedding
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部线性嵌入
- en: Contrary to Isomap, which works with the pairwise distances, this algorithm
    is based on the assumption that a high-dimensional dataset lying on a smooth manifold
    can have local linear structures that it tries to preserve during the dimensionality
    reduction process. **Locally Linear Embedding** (**LLE**), like Isomap, is based
    on three steps. The first one is applying the *k*-nearest neighbor algorithm to
    create a directed graph (in Isomap, it was undirected), where the vertices are
    the input samples and the edges represent a neighborhood relationship. As the
    graph is direct, a point *x[i]* can be a neighbor of *x[j]*, but the opposite
    could be false. It means that the weight matrix can be asymmetric.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于成对距离的Isomap相反，该算法基于这样一个假设：一个高维数据集位于光滑流形上，可以在降维过程中具有局部线性结构，它试图在降维过程中保持这些结构。**局部线性嵌入**（**LLE**），像Isomap一样，基于三个步骤。第一步是应用*k*最近邻算法来创建一个有向图（在Isomap中是未定向的），其中顶点是输入样本，边代表邻域关系。由于图是有向的，一个点*x[i]*可以是*x[j]*的邻居，但反之则不然。这意味着权重矩阵可以是不对称的。
- en: 'The second step is based on the main assumption of local linearity. For example,
    consider the following graph:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步基于局部线性的主要假设。例如，考虑以下图：
- en: '![](img/17a66576-7e7d-4da4-a1bc-1bdf45b9dda7.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17a66576-7e7d-4da4-a1bc-1bdf45b9dda7.png)'
- en: Graph where a neighborhood is marked with a shaded rectangle
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 标记了邻域的阴影矩形图
- en: 'The rectangle delimits a small neighboorhood. If we consider the point *x[5]*,
    the local linearity assumption allows us to think that *x[5] = w[56]x[6] + w[53]x*[*3*, ]without
    considering the cyclic relationship. This concept can be formalized for all *N*
    *P*-dimensional points through the minimization of the following function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形界定了一个小邻域。如果我们考虑点*x[5]*，局部线性假设允许我们认为*x[5] = w[56]x[6] + w[53]x[*3*, ]而不考虑循环关系。这个概念可以通过以下函数的最小化来形式化，对于所有*N*
    *P*-维度的点：
- en: '![](img/ec133037-488c-4333-a614-fab4cc445bfe.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec133037-488c-4333-a614-fab4cc445bfe.png)'
- en: In order to address the problem of low-rank neighborhood matrices (think about
    the previous example, with a number of neighbors equal to 20), Scikit-Learn also
    implements a regularizer that is based on a small arbitrary additive constant
    that is added to the local weights (according to a variant called **Modified LLE**
    or **MLLE***)*. At the end of this step, the matrix W that better matches the
    linear relationships among neighbors will be selected for the next phase.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决低秩邻域矩阵的问题（想想之前的例子，邻居的数量等于20），Scikit-Learn还实现了一个基于小任意加性常数的正则化器，该常数被添加到局部权重中（根据称为**修改后的LLE**或**MLLE**的变体）*。在这一步结束时，将选择与邻居之间的线性关系匹配更好的矩阵W，用于下一阶段。
- en: 'In the third step, locally linear embedding tries to determine the low-dimensional
    (*Q < P*) representation that best reproduces the original relationship among
    nearest neighbors. This is achieved by minimizing the following function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步中，局部线性嵌入试图确定最佳的低维（*Q < P*）表示，以最好地再现原始最近邻之间的关系。这是通过最小化以下函数来实现的：
- en: '![](img/298f8db2-e6a9-419a-a88f-16eede731038.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/298f8db2-e6a9-419a-a88f-16eede731038.png)'
- en: 'The solution for this problem is obtained through the adoption of the **Rayleigh-Ritz
    method**, an algorithm to extract a subset of eigenvectors and eigenvalues from
    a very large sparse matrix. For further details, read *A spectrum slicing method
    for the Kohn–Sham problem, Schofield G. Chelikowsky J. R.; Saad Y., Computer Physics
    Communications. 183*. The initial part of the final procedure consists of determining
    the matrix *D*:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题的解是通过采用**Rayleigh-Ritz方法**获得的，这是一种从非常大的稀疏矩阵中提取特征向量和特征值的算法。有关更多详细信息，请阅读*A
    spectrum slicing method for the Kohn–Sham problem, Schofield G. Chelikowsky J.
    R.; Saad Y., Computer Physics Communications. 183*。最终过程的初始部分包括确定矩阵*D*：
- en: '![](img/451a151a-82cd-493e-b5b7-151bd5fbcbd7.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/451a151a-82cd-493e-b5b7-151bd5fbcbd7.png)'
- en: It's possible to prove the last eigenvector (if the eigenvalues are sorted in
    descending order, it's the bottom one) has all components *v[1]^((N)), v[2]^((N))**,
    ..., v[N]^((N) )= v*, and the corresponding eigenvalue is null. As Saul and Roweis
    (*An introduction to locally linear embedding*,*Saul L. K.*, *Roweis S. T.*) pointed
    out, all the other *Q* eigenvectors (from the bottom) are orthogonal, and this
    allows them to have zero-centered embedding. Hence, the last eigenvector is discarded,
    while the remaining Q eigenvectors determine the embedding vectors *φ[i]*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明最后一个特征向量（如果特征值按降序排列，则是底部的一个）具有所有分量*v[1]^((N)), v[2]^((N))**, ..., v[N]^((N) )=
    v*，并且对应的特征值是零。正如Saul和Roweis（*局部线性嵌入的介绍*，*Saul L. K.*，*Roweis S. T.*）所指出的，所有其他*Q*特征向量（从底部开始）是正交的，这使得它们可以具有零中心嵌入。因此，最后一个特征向量被舍弃，而剩余的Q个特征向量决定了嵌入向量*φ[i]*。
- en: 'For further details about MLLE, please refer to *MLLE: Modified Locally Linear
    Embedding Using Multiple Weights*,*Zhang Z., Wang J.*,[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有关MLLE的更多详细信息，请参阅*MLLE：使用多个权重的改进局部线性嵌入*，*张Z.*，*王J.*，[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382)。
- en: Example of locally linear embedding
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部线性嵌入的示例
- en: 'We can now apply this algorithm to the Olivetti faces dataset, instantiating
    the Scikit-Learn class `LocallyLinearEmbedding` with `n_components=2` and `n_neighbors=15`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将此算法应用于Olivetti人脸数据集，通过实例化Scikit-Learn类`LocallyLinearEmbedding`并设置`n_components=2`和`n_neighbors=15`：
- en: '[PRE17]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result (limited to the first 100 samples) is shown in the following plot:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 结果（限于前100个样本）如下所示：
- en: '![](img/dc32a388-61c7-4671-8b2b-3d5c2c123179.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dc32a388-61c7-4671-8b2b-3d5c2c123179.png)'
- en: Locally linear embedding applied to 100 samples drawn from the Olivetti faces
    dataset
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 将局部线性嵌入应用于从Olivetti人脸数据集中抽取的100个样本
- en: Even if the strategy is different from Isomap, we can determine some coherent
    clusters. In this case, the similarity is obtained through the conjunction of
    small linear blocks; for the faces, they can represent particular micro-features,
    like the shape of the nose or the presence of glasses, that remain invariant in
    the different portraits of the same person. LLE is, in general, preferable when
    the original dataset is intrinsically locally linear, possibly lying on a smooth
    manifold. In other words, LLE is a reasonable choice when small parts of a sample
    are structured in a way that allows the reconstruction of a point given the neighbors
    and the weights. This is often true for images, but it can be difficult to determine
    for a generic dataset. When the result doesn't reproduce the original clustering,
    it's possible to employ the next algorithm or **t-SNE**, which is one the most
    advanced.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 即使策略与Isomap不同，我们也可以确定一些有意义的聚类。在这种情况下，相似性是通过小线性块的结合获得的；对于人脸，它们可以代表特定的微观特征，如鼻子的形状或眼镜的存在，这些特征在不同的人像中保持不变。一般来说，当原始数据集本质上是局部线性的，可能位于光滑流形上时，LLE更可取。换句话说，当样本的小部分以允许根据邻居和权重重建点的方式结构化时，LLE是一个合理的选择。这通常适用于图像，但对于通用数据集来说可能很难确定。当结果不能再现原始聚类时，可以采用下一个算法或**t-SNE**，这是最先进的算法之一。
- en: Laplacian Spectral Embedding
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拉普拉斯谱嵌入
- en: This algorithm, based on the spectral decomposition of a graph Laplacian, has
    been proposed in order to perform a non-linear dimensionality reduction to try
    to preserve the nearness of points in the *P*-dimensional manifold when remapping
    on a *Q*-dimensional (with *Q < P*) subspace.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法基于图拉普拉斯算子的谱分解，旨在执行非线性降维，以尝试在将点重新映射到具有 *Q* 维（*Q < P*）子空间时保留 *P* 维流形上点的邻近性。
- en: 'The procedure is very similar to the other algorithms. The first step is a
    *k*-nearest neighbor clustering to generate a graph where the vertices (we can
    assume to have *N* elements) are the samples, and the edges are weighted using
    an RBF kernel:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程与其他算法非常相似。第一步是 *k* 近邻聚类，以生成一个图，其中顶点（我们可以假设有 *N* 个元素）是样本，边使用径向基函数核进行加权：
- en: '![](img/4b995de2-674b-45f0-8e21-137a7cedd0fb.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b995de2-674b-45f0-8e21-137a7cedd0fb.png)'
- en: 'The resulting graph is undirected and symmetric. We can now define a pseudo-degree
    matrix *D*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图是无向和对称的。我们现在可以定义一个伪度矩阵 *D*：
- en: '![](img/924b2158-8338-476f-9d88-854b753d8a0f.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/924b2158-8338-476f-9d88-854b753d8a0f.png)'
- en: 'The low-dimensional representation *Φ* is obtained by minimizing the function:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化以下函数获得低维表示 *Φ*：
- en: '![](img/1e635be3-4000-4606-853c-3d53eb27a48c.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e635be3-4000-4606-853c-3d53eb27a48c.png)'
- en: 'If the two points *x[i]* and *x[j]* are near, the corresponding *W[ij]* is
    close to *1*, while it tends to 0 when the distance tends to *∞*. *D[ii]* is the
    sum of all weights originating from *x[i]* (and the same for *D[jj]*). Now, let''s
    suppose that *x[i]* is very close only to *x*[*j* ]so, to approximate *D[ii] =
    D[jj] ≈ W[ij]*. The resulting formula is a square loss based on the difference
    between the vectors *φ[i]* and *φ[j]*. When instead there are multiple *closeness*
    relationships to consider, the factor *W[ij]* divided by the square root of *D[ii]D[jj]* allows
    reweighting the new distances to find the best trade-off for the whole dataset.
    In practice, *L[Φ]* is not minimized directly. In fact, it''s possible to prove
    that the minimum can be obtained through the spectral decomposition of the symmetric
    normalized graph Laplacian (the name derives from this procedure):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个点 *x[i]* 和 *x[j]* 靠近，相应的 *W[ij]* 接近于 *1*，而当距离趋向于 *∞* 时，它趋向于 0。*D[ii]* 是从
    *x[i]*（以及 *D[jj]* 同样）发出的所有权重的总和。现在，假设 *x[i]* 只非常接近于 *x[j]*，因此，为了近似 *D[ii] = D[jj]
    ≈ W[ij]*。得到的公式是基于向量 *φ[i]* 和 *φ[j]* 之间差异的平方损失。当需要考虑多个 *邻近性* 关系时，*W[ij]* 除以 *D[ii]D[jj]*
    的平方根允许重新加权新距离，以找到整个数据集的最佳权衡。在实践中，*L[Φ]* 不是直接最小化的。事实上，可以证明最小值可以通过对称归一化图拉普拉斯算子的谱分解（该名称来源于此过程）来获得：
- en: '![](img/d93c1574-7601-48d6-9ecc-9b79d84be4d6.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d93c1574-7601-48d6-9ecc-9b79d84be4d6.png)'
- en: Just like for the LLE algorithm, Laplacian Spectral Embedding also works with
    the bottom *Q + 1* eigenvectors. The mathematical theory behind the last step
    is always based on the application of the Rayleigh-Ritz method. The last one is
    discarded, and the remaining *Q* determines the low-dimensional representation *φ*[*i*.]
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 LLE 算法一样，拉普拉斯谱嵌入也使用底部的 *Q + 1* 个特征向量。最后一步背后的数学理论始终基于应用 Rayleigh-Ritz 方法。最后一个被丢弃，剩下的
    *Q* 确定了低维表示 *φ[i]*。
- en: Example of Laplacian Spectral Embedding
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拉普拉斯谱嵌入的示例
- en: 'Let''s apply this algorithm to the same dataset using the Scikit-Learn class
    `SpectralEmbedding`, with `n_components=2` and `n_neighbors=15`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Scikit-Learn 类 `SpectralEmbedding` 将此算法应用于相同的数据集，其中 `n_components=2` 和
    `n_neighbors=15`：
- en: '[PRE18]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resulting plot (zoomed in due to the presence of a high-density region)
    is shown in the following graph:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在高密度区域，以下图中显示了生成的图（已放大）：
- en: '![](img/b627d15a-85ad-4aa7-a113-4f9468463063.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b627d15a-85ad-4aa7-a113-4f9468463063.png)'
- en: Laplacian Spectral Embedding applied to the Olivetti faces dataset
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 将拉普拉斯谱嵌入应用于奥利维蒂人脸数据集
- en: Even in this case, we can see that some classes are grouped into small clusters,
    but at the same time, we observe many agglomerates where there are mixed samples.
    Both this and the previous method work with local pieces of information, trying
    to find low-dimensional representations that could preserve the geometrical structure
    of micro-features. This condition drives to a mapping where close points *share*
    local features (this is almost always true for images, but it's very difficult
    to prove for generic samples). Therefore, we can observe small clusters containing
    elements belonging to the same class, but also some *apparent* outliers, which,
    on the original manifold, can be globally different even if they share local *patches*.
    Instead, methods like Isomap or t-SNE work with the whole distribution, and try
    to determine a representation that is almost isometric with the original dataset
    considering its global properties.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这种情况下，我们也可以看到一些类别被分组到小的簇中，但与此同时，我们观察到许多混合样本的聚集。这两种方法都使用局部信息片段，试图找到能够保留微特征几何结构的低维表示。这种条件导致了一种映射，其中接近的点*共享*局部特征（这在图像中几乎是总是成立的，但对于通用样本来说很难证明）。因此，我们可以观察到包含属于同一类别的元素的微小簇，但也存在一些*明显的*异常值，在原始流形上，即使它们共享局部*块*，也可能在全局上不同。相反，像Isomap或t-SNE这样的方法处理整个分布，并试图确定一个表示，该表示在考虑其全局属性的情况下几乎与原始数据集等距。
- en: t-SNE
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE
- en: 'This algorithm, proposed by Van der Mateen and Hinton and formally known as
    **t-Distributed Stochastic Neighbor Embedding** (**t-SNE**), is one of the most
    powerful manifold dimensionality reduction techniques. Contrary to the other methods,
    this algorithm starts with a fundamental assumption: the similarity between two
    *N*-dimensional points *x[i]* and *x[j]* can be represented as the conditional
    probability *p(x[j]|x[i])* where each point is represented by a Gaussian distribution
    centered in *x[i]* and with variance *σ[i]*. The variances are selected starting
    from the desired perplexity, defined as:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法由Van der Mateen和Hinton提出，正式称为**t分布随机邻域嵌入**（**t-SNE**），是最强大的流形降维技术之一。与其他方法相反，这个算法从一个基本假设开始：两个*N*-维点*x[i]*和*x[j]*之间的相似性可以表示为条件概率*p(x[j]|x[i]*)，其中每个点由以*x[i]*为中心、方差*σ[i]*的高斯分布表示。方差是从所需的困惑度开始的，定义为：
- en: '![](img/e19f867d-dc23-4af6-aaeb-85b043b4b402.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e19f867d-dc23-4af6-aaeb-85b043b4b402.png)'
- en: Low-perplexity values indicate a low uncertainty, and are normally preferable.
    In common t-SNE tasks, values in the range *10÷50* are normally acceptable.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 低困惑度值表示低不确定性，通常更可取。在常见的t-SNE任务中，*10÷50*范围内的值通常是可以接受的。
- en: 'The assumption on the conditional probabilities can be interpreted thinking
    that if two samples are very similar, the probability associated with the first
    sample conditioned to the second one is high, while dissimilar points yield low
    conditional probabilities. For example, thinking about images, a point centered
    in the pupil can have as neighbors some points belonging to an eyelash. In terms
    of probabilities, we can think that *p(eyelash|pupil)* is quite high, while *p(nose|pupil)*
    is obviously lower. t-SNE models these conditional probabilities as:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对条件概率的假设可以这样解释：如果两个样本非常相似，则与第二个样本相关的第一个样本的条件概率很高，而不同的点产生低条件概率。例如，考虑图像，瞳孔中心的一个点可以有睫毛属于的点作为邻居。在概率方面，我们可以认为*p(eyelash|pupil)*相当高，而*p(nose|pupil)*显然较低。t-SNE将这些条件概率建模为：
- en: '![](img/c6b5d716-61c2-49a9-ab85-8b46e8267481.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6b5d716-61c2-49a9-ab85-8b46e8267481.png)'
- en: 'The probabilities *p(x[i]|x[i]**)* are set to zero, so the previous formula
    can be extended to the whole graph. In order to solve the problem in an easier
    way, the conditional probabilities are also symmetrized:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 将*p(x[i]|x[i]**)*的概率设置为零，因此前面的公式可以扩展到整个图。为了更容易地解决这个问题，条件概率也被对称化：
- en: '![](img/d1ab9e9c-a17a-49f4-8013-cc6fe8f9aefa.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1ab9e9c-a17a-49f4-8013-cc6fe8f9aefa.png)'
- en: 'The probability distribution so obtained represents the high-dimensional input
    relationship. As our goal is to reduce the dimensionality to a value *M < N*,
    we can think about a similar probabilistic representation for the target points *φ[i]*,
    using a student-t distribution with one degree of freedom:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 所获得的概率分布代表了高维输入关系。由于我们的目标是降低维度到一个值*M < N*，我们可以考虑对目标点*φ[i]*使用类似的概率表示，使用一个自由度为1的学生t分布：
- en: '![](img/3a1f8af3-c855-4f07-9696-b95dc7c9d696.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3a1f8af3-c855-4f07-9696-b95dc7c9d696.png)'
- en: 'We want the low-dimensional distribution *Q* to be as close as possible to
    the high-dimensional distribution *P*; therefore, the aim of the **t-SNE** algorithm
    is to minimize the Kullback-Leibler divergence between *P* and *Q*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望低维分布*Q*尽可能接近高维分布*P*；因此，**t-SNE**算法的目的是最小化*P*和*Q*之间的Kullback-Leibler散度：
- en: '![](img/5f8a6945-e821-4be0-a118-8b6efc69d58d.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f8a6945-e821-4be0-a118-8b6efc69d58d.png)'
- en: The first term is the entropy of the original distribution *P*, while the second
    one is the cross-entropy *H(P, Q)*, which has to be minimized to solve the problem.
    The best approach is based on a gradient-descent algorithm, but there are also
    some useful variations that can improve the performance discussed in *Visualizing
    High-Dimensional Data Using t-SNE*, *Van der Maaten L.J.P., Hinton G.E., Journal
    of Machine Learning Research 9 (Nov), 2008.*
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项是原始分布*P*的熵，而第二个项是交叉熵*H(P, Q)*，必须最小化以解决问题。最佳方法基于梯度下降算法，但也有一些有用的变体可以在*《使用t-SNE可视化高维数据》*，*Van
    der Maaten L.J.P., Hinton G.E., Journal of Machine Learning Research 9 (Nov),
    2008.*中讨论，以改善性能。
- en: Example of t-distributed stochastic neighbor embedding
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-distributed随机邻域嵌入的示例
- en: 'We can apply this powerful algorithm to the same Olivetti faces dataset, using
    the Scikit-Learn class `TSNE` with `n_components=2` and `perplexity=20`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个强大的算法应用于相同的Olivetti面部数据集，使用Scikit-Learn类`TSNE`，`n_components=2`和`perplexity=20`：
- en: '[PRE19]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result for all 400 samples is shown in the following graph:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所有400个样本的结果显示在下图中：
- en: '![](img/2b23c4bd-e002-43c1-be89-8fd83ab1a5e1.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b23c4bd-e002-43c1-be89-8fd83ab1a5e1.png)'
- en: t-SNE applied to the Olivetti faces dataset
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE应用于Olivetti面部数据集
- en: A visual inspection of the label distribution can confirm that t-SNE recreated
    the optimal clustering starting from the original high-dimensional distribution.
    This algorithm can be employed in several non-linear dimensionality reduction
    tasks, such as images, word embeddings, or complex feature vectors. Its main strength
    is hidden in the assumption to consider the similarities as probabilities, without
    the need to impose any constraint on the pairwise distances, either global or
    local. Under a certain viewpoint, it's possible to consider t-SNE as a reverse
    multiclass classification problem based on a cross-entropy cost function. Our
    goal is to find the labels (low-dimensional representation) given the original
    distribution and an assumption about the output distribution.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对标签分布的视觉检查可以确认，t-SNE从原始高维分布重新创建了最佳聚类。此算法可用于多个非线性降维任务，例如图像、词嵌入或复杂特征向量。其主要优势在于考虑相似性为概率的假设，无需对成对距离施加任何约束，无论是全局的还是局部的。从某个角度来看，可以将t-SNE视为基于交叉熵成本函数的反向多类分类问题。我们的目标是给定原始分布和关于输出分布的假设，找到标签（低维表示）。
- en: 'At this point, we could try to answer a natural question: which algorithm must
    be employed? The obvious answer is it depends on the single problem. When it''s
    useful to reduce the dimensionality, preserving the global similarity among vectors
    (this is the case when the samples are long feature vectors without local properties,
    such as word embeddings or data encodings), t-SNE or Isomap are good choices.
    When instead it''s necessary to keep the local distances (for example, the structure
    of a visual patch that can be shared by different samples also belonging to different
    classes) as close as possible to the original representation, locally linear embedding
    or spectral embedding algorithms are preferable.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以尝试回答一个自然的问题：必须使用哪种算法？显然的答案是这取决于单个问题。当需要降低维度，同时保留向量之间的全局相似性时（这是当样本是长特征向量且没有局部属性，如词嵌入或数据编码时的情况），t-SNE或Isomap是不错的选择。当需要尽可能保持局部距离（例如，一个视觉块的结构，可以被属于不同类别的不同样本共享）与原始表示尽可能接近时，局部线性嵌入或谱嵌入算法更可取。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have introduced the most important label propagation techniques.
    In particular, we have seen how to build a dataset graph based on a weighting
    kernel, and how to use the geometric information provided by unlabeled samples
    to determine the most likely class. The basic approach works by iterating the
    multiplication of the label vector times the weight matrix until a stable point
    is reached and we have proven that, under simple assumptions, it is always possible.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了最重要的标签传播技术。特别是，我们看到了如何基于加权核构建数据集图，以及如何使用未标记样本提供的几何信息来确定最可能的类别。基本方法通过迭代标签向量与权重矩阵的乘积，直到达到稳定点并证明，在简单假设下，这总是可能的。
- en: Another approach, implemented by Scikit-Learn, is based on the transition probability
    from a state (represented by a sample) to another one, until the convergence to
    a labeled point. The probability matrix is obtained using a normalized weight
    matrix to encourage transitions associated to close points and discourage all
    the *long jumps*. The main drawback of these two methods is the hard-clamping
    of labeled samples; this constraint can be useful if we *trust* our dataset, but
    it can be a limitation in the presence of outliers whose label has been wrongly
    assigned.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，由Scikit-Learn实现，基于从状态（由样本表示）到另一个状态的转换概率，直到收敛到一个标记点。概率矩阵通过使用归一化权重矩阵来获得，以鼓励与接近点相关的转换并阻止所有*长跳跃*。这两种方法的主要缺点是硬限制标记样本；如果我们*信任*我们的数据集，这个约束可能是有用的，但在存在标签错误分配的异常值的情况下，它可能是一个限制。
- en: Label spreading solves this problem by introducing a clamping factor that determines
    the percentage of clamped labels. The algorithm is very similar to label propagation,
    but it's based on graph Laplacian and can be employed in all those problems where
    the data-generating distribution is not well-determined and the probability of
    noise is high.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播通过引入一个限制系数来解决此问题，该系数决定了限制标签的百分比。该算法与标签传播非常相似，但它基于图拉普拉斯，可以应用于所有那些数据生成分布未确定且噪声概率高的问题。
- en: The propagation based on Markov random walks is a very simple algorithm that
    can estimate the class distribution of unlabeled samples through a stochastic
    process. It's possible to imagine it as a *test sample* that walks through the
    graph until it reaches a final labeled state (acquiring the corresponding label).
    The algorithm is very fast and it has a closed-form solution that can be found
    by solving a linear system.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于马尔可夫随机游走的传播是一个非常简单的算法，可以通过随机过程估计未标记样本的类别分布。可以想象它为一个*测试样本*在图中行走，直到它达到一个最终标记状态（获得相应的标签）。该算法非常快，并且有一个封闭形式的解，可以通过求解线性系统找到。
- en: The next topic was the introduction of manifold learning with the Isomap algorithm,
    which is a simple but powerful solution based on a graph built using a *k*-nearest
    neighbors algorithm (this is a common step in most of these algorithms). The original
    pairwise distances are processed using the multidimensional scaling technique,
    which allows obtaining a low-dimensional representation where the distances between
    samples are preserved.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个主题是介绍使用Isomap算法的流形学习，这是一个基于使用*k*最近邻算法（这是大多数这些算法中的常见步骤）构建的图的一个简单但强大的解决方案。原始的成对距离通过多维尺度技术进行处理，这使得可以获得一个低维表示，其中样本之间的距离得到保留。
- en: Two different approaches, based on local pieces of information, are locally
    linear embedding and Laplacian Spectral Embedding. The former tries to preserve
    the local linearity present in the original manifold, while the latter, which
    is based on the spectral decomposition of the normalized graph Laplacian, tries
    to preserve the nearness of original samples. Both methods are suitable for all
    those tasks where it's important not to consider the whole original distribution,
    but the similarity induced by small data *patches*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 基于局部信息片段的两种不同方法，局部线性嵌入和拉普拉斯谱嵌入。前者试图保留原始流形中存在的局部线性，而后者，基于归一化图拉普拉斯的谱分解，试图保留原始样本的邻近性。这两种方法都适用于所有那些任务，在这些任务中，重要的是不要考虑整个原始分布，而是考虑由小数据*补丁*引起的相似性。
- en: We closed this chapter by discussing t-SNE, which is a very powerful algorithm
    that tries to model a low-dimensional distribution that is as similar as possible
    to the original high-dimensional one. This task is achieved by minimizing the
    Kullback-Leibler divergence between the two distributions. t-SNE is a state-of-the-art
    algorithm, useful whenever it's important to consider the whole original distribution
    and the similarity between entire samples.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过讨论t-SNE来结束这一章节，这是一个非常强大的算法，试图模拟一个尽可能接近原始高维分布的低维分布。这个任务是通过最小化两个分布之间的Kullback-Leibler散度来实现的。t-SNE是一种最先进的算法，在需要考虑整个原始分布以及整个样本之间的相似性时非常有用。
- en: In the next chapter, [Chapter 4](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml),
    *Bayesian Networks and Hidden Markov Models *we're going to introduce Bayesian
    networks in both a static and dynamic context, and hidden Markov models, with
    practical prediction examples. These algorithms allow modeling complex probabilistic
    scenarios made up of observed and latent variables, and infer future states using
    optimized sampling methods based only on the observations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章（[第4章](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml)），*贝叶斯网络和隐马尔可夫模型*中，我们将介绍在静态和动态环境下使用贝叶斯网络，以及隐马尔可夫模型，并附带实际预测示例。这些算法允许对由观测和潜在变量组成的复杂概率场景进行建模，并使用仅基于观测的优化采样方法来推断未来状态。
