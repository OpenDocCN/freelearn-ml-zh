- en: Graph-Based Semi-Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we continue our discussion about semi-supervised learning,
    considering a family of algorithms that is based on the graph obtained from the
    dataset and the existing relationships among samples. The problems that we are
    going to discuss belong to two main categories: the propagation of class labels
    to unlabeled samples and the use of non-linear techniques based on the manifold
    assumption to reduce the dimensionality of the original dataset. In particular,
    this chapter covers the following propagation algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Label propagation based on the weight matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label propagation in Scikit-Learn (based on transition probabilities)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label spreading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propagation based on Markov random walks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the manifold learning section, we''re discussing:'
  prefs: []
  type: TYPE_NORMAL
- en: Isomap algorithm and multidimensional scaling approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locally linear embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laplacian Spectral Embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Label propagation** is a family of semi-supervised algorithms based on a
    graph representation of the dataset. In particular, if we have *N* labeled points
    (with bipolar labels +1 and -1) and *M* unlabeled points (denoted by *y=0*), it''s
    possible to build an undirected graph based on a measure of geometric affinity
    among samples. If *G = {V, E}* is the formal definition of the graph, the set
    of vertices is made up of sample labels *V = { -1, +1, 0 }*, while the edge set
    is based on an **affinity matrix** *W* (often called **adjacency matrix** when
    the graph is unweighted), which depends only on the *X* values, not on the labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following graph, there''s an example of such a structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a857d71f-f48b-4a65-92f3-112d69183aa5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of binary graph
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example graph, there are four labeled points (two with *y=+1* and
    two with *y=-1*), and two unlabeled points (*y=0*). The affinity matrix is normally
    symmetric and square with dimensions equal to *(N+M) x (N+M)*. It can be obtained
    with different approaches. The most common ones, also adopted by Scikit-Learn,
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '***k*-Nearest Neighbors** (we are going to discuss this algorithm with further
    details in [Chapter 8](59f765c2-2ad0-4605-826e-349080f85f1f.xhtml), *Clustering
    Algorithms*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3c0b235e-449d-4c59-8046-6130be025841.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Radial basis function kernel**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/34ccb44e-6279-4bcd-8b1a-49853b3cfdcf.png)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, in the radial basis function kernel, the parameter *γ* is represented
    as the reciprocal of *2σ²*; however, small *γ* values corresponding to a large
    variance increase the radius, including farther points and *smoothing* the class
    over a number of samples, while large *γ* values restrict the boundaries to a
    subset that tends to a single sample. Instead, in the k-nearest neighbors kernel,
    the parameter *k* controls the number of samples to consider as neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To describe the basic algorithm, we also need to introduce the **degree matrix**
    (*D*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7084e81a-bdeb-4eca-8e57-a56a7302bbb7.png)'
  prefs: []
  type: TYPE_IMG
- en: It is a diagonal matrix where each non-null element represents the *degree*
    of the corresponding vertex. This can be the number of incoming edges, or a measure
    proportional to it (as in the case of *W* based on the radial basis function).
    The general idea of label propagation is to let each node propagate its label
    to its neighbors and iterate the procedure until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, if we have a dataset containing both labeled and unlabeled samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d61fa3a2-6d63-46ed-a068-f89324c21af7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The complete steps of the **label propagation** algorithm (as proposed by Zhu
    and Ghahramani in *Learning from Labeled and Unlabeled Data with Label Propagation*,
    *Zhu X.*, *Ghahramani Z.*, *CMU-CALD-02-107*) are:'
  prefs: []
  type: TYPE_NORMAL
- en: Select an affinity matrix type (KNN or RBF) and compute *W*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the degree matrix *D*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *Y^((0)) = Y*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *Y[L] = {y[0], y[1], ..., y[N]}*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate until convergence of the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/892bd875-805e-429d-b1ab-f9a0b86176ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first update performs a propagation step with both labeled and unlabeled
    points. Each label is spread from a node through its outgoing edges, and the corresponding
    weight, normalized with the degree, increases or decreases the *effect* of each
    contribution. The second command instead resets all *y* values for the labeled
    samples. The final labels can be obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68bc4da1-a9a8-4ed4-b39e-bbd3bceb56eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The proof of convergence is very easy. If we partition the matrix *D^(-1)W*
    according to the relationship among labeled and unlabeled samples, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a787530c-3ecc-4854-9855-63b2401ae8bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we consider that only the first *N* components of *Y* are non-null and they
    are clamped at the end of each iteration, the matrix can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0f9ae2a-763c-4924-b8a5-99531e422ca7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are interested in proving the convergence for the part regarding the unlabeled
    samples (the labeled ones are fixed), so we can write the update rule as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b64dca0-af08-46d3-968f-bae377f18c79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Transforming the recursion into an iterative process, the previous formula
    becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/834692a0-7f1e-4423-aaa6-c62595755203.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, the second term is null, so we need to prove that
    the first term converges; however, it''s easy to recognize a truncated matrix
    geometrical series (Neumann series), and *A[UU]* is constructed to have all eigenvalues
    *|λ[i]| < 1*, therefore the series converges to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7abc27fc-c438-4f50-bba7-8bba0c25d66d.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of label propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can implement the algorithm in Python, using a test bidimensional dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the other examples, we set *y = 0* for all unlabeled samples (75 out
    of 100). The corresponding plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6daecfc9-002c-40ce-86d5-d33559e5fb02.png)'
  prefs: []
  type: TYPE_IMG
- en: Partially labeled dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The dots marked with a cross are unlabeled. At this point, we can define the
    affinity matrix. In this case, we compute it using both methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The KNN matrix is obtained using the Scikit-Learn function `kneighbors_graph()`
    with the parameters `n_neighbors=2` and `mode='connectivity'`; the alternative
    is `'distance'`, which returns the distances instead of 0 and 1 to indicate the
    absence/presence of an edge. The `include_self=True` parameter is useful, as we
    want to have *W[ii] = 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the RBF matrix, we need to define it manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The default value for *γ* is *10*, corresponding to a standard deviation *σ*
    equal to *0.22*. When using this method, it''s important to set a correct value
    for *γ*; otherwise, the propagation can degenerate in the predominance of a class
    (*γ* too small). Now, we can compute the degree matrices and its inverse. As the
    procedure is identical, from this point on we continue using the RBF affinity
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm is implemented using a variable threshold. The value adopted
    here is `0.01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The final result is shown in the following double plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d34f804-4d77-4999-bc98-8a0f5ca99ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left); dataset after a complete label propagation (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'As it''s possible to see, in the original dataset there''s a round dot surrounded
    by square ones (-0.9, -1). As this algorithm keeps the original labels, we find
    the same situation after the propagation of labels. This condition could be acceptable,
    even if both the smoothness and clustering assumptions are contradicted. Assuming
    that it''s reasonable, it''s possible to force a *correction* by relaxing the
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this way, we don''t reset the original labels, letting the propagation change
    all those values that disagree with the neighborhood. The result is shown in the
    following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38d6bb5b-425c-41d3-9bfa-94ccb560e004.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left); dataset after a complete label propagation with overwrite
    (right)
  prefs: []
  type: TYPE_NORMAL
- en: Label propagation in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements a slightly different algorithm proposed by Zhu and
    Ghahramani (in the aforementioned paper) where the affinity matrix *W* can be
    computed using both methods (KNN and RBF), but it is normalized to become a probability
    transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14a975e7-8317-4b6b-9476-6aab77891f64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm operates like a Markov random walk, with the following sequence
    (assuming that there are *Q* different labels):'
  prefs: []
  type: TYPE_NORMAL
- en: Define a matrix *Y^M[i] = [P(label=y[0]), P(label=y[1]), ..., and P(label=y[Q])]*,
    where *P(label=yi)* is the probability of the label *yi*, and each row is normalized
    so that all the elements sum up to *1*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *Y^((0)) = Y^M*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate until convergence of the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9595ccab-77ee-404e-8a4d-d631644d7591.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first update performs a label propagation step. As we''re working with
    probabilities, it''s necessary (second step) to renormalize the rows so that their
    element sums up to *1*. The last update resets the original labels for all labeled
    samples. In this case, it means imposing a *P(label=y[i]) = 1* to the corresponding
    label, and setting all the others to zero. The proof of convergence is very similar
    to the one for label propagation algorithms, and can be found in *Learning from
    Labeled and Unlabeled Data with Label Propagation*,*Zhu X.*, *Ghahramani Z.*,
    *CMU-CALD-02-107. *The most important result is that the solution can be obtained
    in closed form (without any iteration) through this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc8a69f4-f2cd-4e5c-b878-219bc03ef988.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term is the sum of a generalized geometric series, where *P[uu]* is
    the unlabeled-unlabeled part of the transition matrix *P*. *P[ul]*, instead, is
    the unlabeled-labeled part of the same matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our Python example, we need to build the dataset differently, because Scikit-Learn
    considers a sample unlabeled if *y=-1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train a `LabelPropagation` instance with an RBF kernel and `gamma=10.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following double plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3871338c-0515-4545-b288-00b1cf2319e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left). Dataset after a Scikit-Learn label propagation (right)
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the propagation converged to a solution that respects both the
    smoothness and the clustering assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Label spreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last algorithm (proposed by Zhou et al.) that we need to analyze is called
    **label spreading**, and it''s based on the normalized graph Laplacian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86147c4f-38c0-4a51-af9d-81278e3fe790.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This matrix has each a diagonal element *l[ii]* equal to *1*, if the degree
    *deg(l[ii]) > 0* (0 otherwise) and all the other elements equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30c5dc27-5c4b-4b06-92bc-3c4c24322f69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The behavior of this matrix is analogous to a discrete Laplacian operator,
    whose real-value version is the fundamental element of all diffusion equations.
    To better understand this concept, let''s consider the generic heat equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25203c70-650d-4ac9-8d41-f9b06964de5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation describes the behavior of the temperature of a room when a point
    is suddenly heated. From basic physics concepts, we know that heat will spread
    until the temperature reaches an equilibrium point and the speed of variation
    is proportional to the Laplacian of the distribution. If we consider a bidimensional
    grid at the equilibrium (the derivative with respect to when time becomes null)
    and we discretize the Laplacian operator (*∇² = ∇ · ∇*) considering the incremental
    ratios, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6c5dfc2-8f95-4cf8-883f-a5035e5aa388.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, at the equilibrium, each point has a value that is the mean of the
    direct neighbors. It's possible to prove the finite-difference equation has a
    single fixed point that can be found iteratively, starting from every initial
    condition. In addition to this idea, label spreading adopts a clamping factor *α*
    for the labeled samples. If *α=0*, the algorithm will always reset the labels
    to the original values (like for label propagation), while with a value in the
    interval *(0, 1]*, the percentage of clamped labels decreases progressively until *α=1*,
    when all the labels are overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete steps of the **label spreading** algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: Select an affinity matrix type (KNN or RBF) and compute *W*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the degree matrix *D*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the normalized graph Laplacian *L*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *Y^((0)) = Y*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *α* in the interval *[0, 1]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate until convergence of the following step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f1abf280-e4cf-4d61-9f5a-75043a721e23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s possible to show (as demonstrated in *Semi-Supervised Learning*, *Chapelle
    O.*,* Schölkopf B*., *Zien A.*, (edited by), *The MIT Press*) that this algorithm
    is equivalent to the minimization of a quadratic cost function with the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4b79495-24d9-4ce3-a660-bb9ba37f65c5.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term imposes consistency between original labels and estimated ones
    (for the labeled samples). The second term acts as a normalization factor, forcing
    the unlabeled terms to become zero, while the third term, which is probably the
    least intuitive, is needed to guarantee geometrical coherence in terms of smoothness.
    As we have seen in the previous paragraph, when a hard-clamping is adopted, the
    smoothness assumption could be violated. By minimizing this term (*μ* is proportional
    to *α*), it's possible to penalize the rapid changes inside the high-density regions.
    Also in this case, the proof of convergence is very similar to the one for label
    propagation algorithms, and will be omitted. The interested reader can find it
    in *Semi-Supervised Learning*,*Chapelle O.*,* Schölkopf B.*, *Zien A.*, (edited
    by), *The MIT Press.*
  prefs: []
  type: TYPE_NORMAL
- en: Example of label spreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can test this algorithm using the Scikit-Learn implementation. Let''s start
    by creating a very dense dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can train a `LabelSpreading` instance with a clamping factor `alpha=0.2`.
    We want to preserve 80% of the original labels but, at the same time, we need
    a smooth solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown, as usual, together with the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40b3c130-fc58-47bd-b456-230f36cbd7e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Original dataset (left). Dataset after a complete label spreading (right)
  prefs: []
  type: TYPE_NORMAL
- en: As it's possible to see in the first figure (left), in the central part of the
    cluster *(x [-1, 0])*, there's an area of circle dots. Using a hard-clamping,
    this *aisle *would remain unchanged, violating both the smoothness and clustering
    assumptions. Setting *α > 0*, it's possible to avoid this problem. Of course,
    the choice of *α* is strictly correlated with each single problem. If we know
    that the original labels are absolutely correct, allowing the algorithm to change
    them can be counterproductive. In this case, for example, it would be better to
    preprocess the dataset, filtering out all those samples that violate the semi-supervised
    assumptions. If, instead, we are not sure that all samples are drawn from the
    same *p[data]*, and it's possible to be in the presence of spurious elements,
    using a higher *α* value can smooth the dataset without any other operation.
  prefs: []
  type: TYPE_NORMAL
- en: Label propagation based on Markov random walks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this algorithm proposed by Zhu and Ghahramani is to find the probability
    distribution of target labels for unlabeled samples given a mixed dataset. This
    objective is achieved through the simulation of a stochastic process, where each
    unlabeled sample walks through the graph until it reaches a stationary absorbing
    state, a labeled sample where it stops acquiring the corresponding label. The
    main difference with other similar approaches is that in this case, we consider
    the probability of reaching a labeled sample. In this way, the problem acquires
    a closed form and can be easily solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to always build a k-nearest neighbors graph with all *N*
    samples, and define a weight matrix *W* based on an RBF kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19f63010-f444-401f-bcd3-b9e9172a32e9.png)'
  prefs: []
  type: TYPE_IMG
- en: '*W[ij] = 0* is *x*[*i*, ]and *x[j]* are not neighbors and *W[ii] = 1*. The
    transition probability matrix, similarly to the Scikit-Learn label propagation
    algorithm, is built as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f1890c2-e927-4afb-9fa3-27ef6492dbe3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a more compact way, it can be rewritten as *P = D^(-1)W*. If we now consider
    a *test sample*, starting from the state *x[i]* and randomly walking until an
    absorbing labeled state is found (we call this label *y^∞*), the probability (referred
    to as **binary classification**) can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b36e8bb-1b5c-4ec3-bebb-8512091ca2f7.png)'
  prefs: []
  type: TYPE_IMG
- en: When *x[i]* is labeled, the state is final, and it is represented by the indicator
    function based on the condition *y[i]=1*. When the sample is unlabeled, we need
    to consider the sum of all possible transitions starting from *x[i]* and ending
    in the closest absorbing state, with label *y=1* weighted by the relative transition
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite this expression in matrix form. If we create a vector *P^∞ =
    [ P[L](y^∞=1|X[L])*, *P[U](y^∞=1|X[U]) ]*, where the first component is based
    on labeled samples and the second on the unlabeled ones, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/693fff6a-7cf3-47ca-b449-706521663575.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we now expand the matrices, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81ec3959-f4ac-4ee5-ba07-e2b65dded4ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we are interested only in the unlabeled samples, we can consider only the
    second equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24164fb4-ff31-4d99-af72-d5d67c3fc335.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Simplifying the expression, we get the following linear system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dee84fb5-6771-46b1-a953-51816cfec912.png)'
  prefs: []
  type: TYPE_IMG
- en: The term *(D[uu] - W[uu])* is the unlabeled-unlabeled part of the unnormalized
    graph Laplacian *L = D - W*. By solving this system, we can get the probabilities
    for the class *y=1* for all unlabeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: Example of label propagation based on Markov random walks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this Python example of label propagation based on Markov random walks,
    we are going to use a bidimensional dataset containing 50 labeled samples belonging
    to two different classes, and 1,950 unlabeled samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the dataset is shown in the following diagram (the crosses represent
    the unlabeled samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4eb4f91d-25b3-4345-be04-e504f66fe33f.png)'
  prefs: []
  type: TYPE_IMG
- en: Partially labeled dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create the graph (using `n_neighbors=15`) and the weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to compute the unlabeled part of the unnormalized graph Laplacian
    and the unlabeled-labeled part of the matrix *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it''s possible to solve the linear system using the NumPy function
    `np.linalg.solve()`, which accepts as parameters the matrix *A* and the vector
    *b* of a generic system in the form *Ax=b*. Once we have the solution, we can
    merge the new labels with the original ones (where the unlabeled samples have
    been marked with *-1*). In this case, we don''t need to convert the probabilities,
    because we are using *0* and *1* as labels. In general, it''s necessary to use
    a threshold (0.5) to select the right label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Replotting the dataset, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7f28e6c-b0ab-4ec9-9f9c-ffbcf24992f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset after a complete Markov random walk label propagation
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, without any iteration, the labels have been successfully propagated
    to all samples in perfect compliance with the clustering assumption. Both this
    algorithm and label propagation can work using a closed-form solution, so they
    are very fast even when the number of samples is high; however, there''s a fundamental
    problem regarding the choice of *σ/γ* for the RBF kernel. As the same authors Zhu
    and Ghahramani remark, there is no standard solution, but it''s possible to consider
    when *σ → 0* and when *σ → ∞*. In the first case, only the nearest point has an
    influence, while in the second case, the influence is extended to the whole sample
    space, and the unlabeled points tend to acquire the same label. The authors suggest
    considering the entropy of all samples, trying to find the best σ value that minimizes
    it. This solution can be very effective, but sometimes the minimum entropy corresponds
    to a label configuration that isn''t impossible to achieve using these algorithms.
    The best approach is to try different values (at different scales) and select
    the one corresponding to a valid configuration with the lowest entropy. In our
    case, it''s possible to compute the entropy of the unlabeled samples as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14d06359-bf07-4221-8044-e51d200428de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Python code to perform this computation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The term `1e-6` has been added to avoid numerical problems when the probability
    is null. Repeating this process for different values allows us to find a set of
    candidates that can be restricted to a single value with a direct evaluation of
    the labeling accuracy (for example, when there is no precise information about
    the real distribution, it''s possible to consider the coherence of each cluster
    and the separation between them). Another approach is called **class rebalancing**,
    and it''s based on the idea of reweighting the probabilities of unlabeled samples
    to rebalance the number of points belonging to each class when the new unlabeled
    samples are added to the set. If we have *N* labeled points and *M* unlabeled
    ones, with *K* classes, the weight factor *w[j]* for the class *j* can be obtained
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ae97f0c-83f4-434d-bcf1-c0b4abf09c59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The numerator is the average computed over the labeled samples belonging to
    class *k*, while the denominator is the average over the unlabeled ones whose
    estimated class is *k*. The final decision about a class is no longer based only
    on the highest probability, but on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0c00d0b-2fd0-4c13-93d8-67f3c04f34fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Manifold learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 02](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction to
    Semi-Supervised Learning*, we discussed the manifold assumption, saying that high-dimensional
    data normally lies on low-dimensional manifolds. Of course, this is not a theorem,
    but in many real cases, the assumption is proven to be correct, and it allows
    us to work with non-linear dimensionality reduction algorithms that would be otherwise
    unacceptable. In this section, we're going to analyze some of these algorithms.
    They are all implemented in Scikit-Learn, therefore it's easy to try them with
    complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Isomap** is one of the simplest algorithms, and it''s based on the idea of
    reducing the dimensionality while trying to preserve the geodesic distances measured
    on the original manifold where the input data lies. The algorithm works in three
    steps. The first operation is a k-nearest neighbors clustering and the construction
    of the following graph. The vertices will be the samples, while the edges represent
    the connections among nearest neighbors, and their weight is proportional to the
    distance to the corresponding neighbor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step adopts the **Dijkstra algorithm** to compute the shortest pairwise
    distances on the graph of all couples of samples. In the following graph, there''s
    a portion of a graph, where some shortest distances are marked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/030d05e4-9518-4067-8ca2-8431a703a413.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a graph with marked shortest distances
  prefs: []
  type: TYPE_NORMAL
- en: For example, as *x[3]* is a neighbor of *x[5]* and *x[7]*, applying the Dijkstra
    algorithm, we could get the shortest paths *d(x[3], x[5]) = w[53]* and *d(x[3], x[7])
    = w[73]*. The computational complexity of this step is about *O(n²log n + n²k)*,
    which is lower than *O(n³)* when *k << n* (a condition normally met); however,
    for large graphs (with *n >> 1*), this is often the most expensive part of the
    whole algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third step is called **metric multidimensional scaling**, which is a technique
    for finding a low-dimensional representation while trying to preserve the inner
    product among samples. If we have a *P*-dimensional dataset *X*, the algorithm
    must find a *Q*-dimensional set *Φ* with *Q < P* minimizing the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a316c204-dade-4948-8693-15f2d01c96bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As proven in *Semi-Supervised Learning*  *Chapelle O.*,* Schölkopf B.*, *Zien
    A.*, (edited by), *The MIT Press*,the optimization is achieved by taking the top
    *Q* eigenvectors of the Gram matrix *G[ij] = x[i] · x*[*j* ](or in matrix form,
    *G=XX^T* if *X ∈ ℜ^(n × M)*); however, as the **Isomap** algorithm works with
    pairwise distances, we need to compute the matrix *D* of squared distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a0e1b64-7619-4716-b7c1-a03f465db905.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the *X* dataset is zero-centered, it''s possible to derive a simplified
    Gram matrix from *D*, as described by M. A. A. Cox and T. F. Cox:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10897a36-bcd6-411b-8743-f727a6a29cc8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Isomap** computes the top *Q* eigenvalues *λ[1], λ2, ..., λ[Q]* of *G*[*D* ]and
    the corresponding eigenvectors *ν[1], ν[2]**, ..., ν[Q]* and determines the *Q*-dimensional
    vectors as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2983ed5-32f1-4d17-af07-2367b64f4cdb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we''re going to discuss in [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml),
    *EM Algorithm and Applications* (and also as pointed out by Saul, Weinberger,
    Sha, Ham, and Lee in *Spectral Methods for Dimensionality Reduction*,*Saul L.
    K., Weinberger K. Q.*, *Sha F.*, *Ham J.*, *and Lee D. D.*), this kind of projection
    is also exploited by **Principal Component Analysis** (**PCA**), which finds out
    the direction with the highest variance, corresponding to the top k eigenvectors
    of the covariance matrix. In fact, when applying the SVD to the dataset *X*, we
    get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae41243f-3035-4652-93bf-969ff0301b9a.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagonal matrix *Λ* contains the eigenvalues of both *XX^T* and *X^TX*; therefore,
    the eigenvalues *λ[Gi]* of *G* are equal to *Mλ[^Σ][i]* where *λ[^Σ]*[*i* ]are
    the eigenvalues of the covariance matrix *Σ = M^(-1)X^TX*. Hence, Isomap achieves
    the dimensionality reduction, trying to preserve the pairwise distances, while
    projecting the dataset in the subspace determined by a group of eigenvectors,
    where the maximum explained variance is achieved. In terms of information theory,
    this condition guarantees the minimum loss with an effective reduction of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn also implements the Floyd-Warshall algorithm, which is slightly slower.
    For further information, please refer to *Introduction to Algorithms*, *Cormen
    T. H.*, *Leiserson C. E.*, *Rivest R. L.*, *The MIT Press*.
  prefs: []
  type: TYPE_NORMAL
- en: Example of Isomap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now test the Scikit-Learn **Isomap** implementation using the Olivetti
    faces dataset (provided by AT&T Laboratories, Cambridge), which is made up of
    400 64 × 64 grayscale portraits belonging to 40 different people. Examples of
    these images are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc559a7c-5399-4618-a868-bc6956689dc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Subset of the Olivetti faces dataset
  prefs: []
  type: TYPE_NORMAL
- en: The original dimensionality is 4096, but we want to visualize the dataset in
    two dimensions. It's important to understand that using the Euclidean distance
    for measuring the similarity of images might not the best choice, and it's surprising
    to see how well the samples are clustered by such a simple algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `faces` dictionary contains three main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`images`: Image array with shape 400 × 64 × 64'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data`: Flattened array with shape 400 × 4096'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target`: Array with shape 400 × 1 containing the labels (0, 39)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, we can instantiate the `Isomap` class provided by Scikit-Learn,
    setting `n_components=2` and `n_neighbors=5` (the reader can try different configurations),
    and then fitting the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As the resulting plot with 400 elements is very dense, I preferred to show
    in the following plot only the first 100 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb25fa4e-0e12-4f89-b6d3-8fd8fdc74b16.png)'
  prefs: []
  type: TYPE_IMG
- en: Isomap applied to 100 samples drawn from the Olivetti faces dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As it''s possible to see, samples belonging to the same class are grouped in
    rather dense agglomerates. The classes that seem better separated are 7 and 1\.
    Checking the corresponding faces, for class 7, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26a4a525-f57f-4fd0-9a2e-b2da1046cd20.png)'
  prefs: []
  type: TYPE_IMG
- en: Samples belonging to class 7
  prefs: []
  type: TYPE_NORMAL
- en: 'The set contains portraits of a young woman with a fair complexion, quite different
    from the majority of other people. Instead, for class 1, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0aedec99-9ac9-4df5-816a-2a68bdb52129.png)'
  prefs: []
  type: TYPE_IMG
- en: Samples belonging to class 1
  prefs: []
  type: TYPE_NORMAL
- en: In this case, it's a man with big glasses and a particular mouth expression.
    In the dataset, there are only a few people with glasses, and one of them has
    a dark beard. We can conclude that **Isomap** created a low-dimensional representation
    that is really coherent with the original geodesic distances. In some cases, there's
    a partial clustering overlap that can be mitigated by increasing the dimensionality
    or adopting a more complex strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Locally linear embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to Isomap, which works with the pairwise distances, this algorithm
    is based on the assumption that a high-dimensional dataset lying on a smooth manifold
    can have local linear structures that it tries to preserve during the dimensionality
    reduction process. **Locally Linear Embedding** (**LLE**), like Isomap, is based
    on three steps. The first one is applying the *k*-nearest neighbor algorithm to
    create a directed graph (in Isomap, it was undirected), where the vertices are
    the input samples and the edges represent a neighborhood relationship. As the
    graph is direct, a point *x[i]* can be a neighbor of *x[j]*, but the opposite
    could be false. It means that the weight matrix can be asymmetric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is based on the main assumption of local linearity. For example,
    consider the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17a66576-7e7d-4da4-a1bc-1bdf45b9dda7.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph where a neighborhood is marked with a shaded rectangle
  prefs: []
  type: TYPE_NORMAL
- en: 'The rectangle delimits a small neighboorhood. If we consider the point *x[5]*,
    the local linearity assumption allows us to think that *x[5] = w[56]x[6] + w[53]x*[*3*, ]without
    considering the cyclic relationship. This concept can be formalized for all *N*
    *P*-dimensional points through the minimization of the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec133037-488c-4333-a614-fab4cc445bfe.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to address the problem of low-rank neighborhood matrices (think about
    the previous example, with a number of neighbors equal to 20), Scikit-Learn also
    implements a regularizer that is based on a small arbitrary additive constant
    that is added to the local weights (according to a variant called **Modified LLE**
    or **MLLE***)*. At the end of this step, the matrix W that better matches the
    linear relationships among neighbors will be selected for the next phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third step, locally linear embedding tries to determine the low-dimensional
    (*Q < P*) representation that best reproduces the original relationship among
    nearest neighbors. This is achieved by minimizing the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/298f8db2-e6a9-419a-a88f-16eede731038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The solution for this problem is obtained through the adoption of the **Rayleigh-Ritz
    method**, an algorithm to extract a subset of eigenvectors and eigenvalues from
    a very large sparse matrix. For further details, read *A spectrum slicing method
    for the Kohn–Sham problem, Schofield G. Chelikowsky J. R.; Saad Y., Computer Physics
    Communications. 183*. The initial part of the final procedure consists of determining
    the matrix *D*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/451a151a-82cd-493e-b5b7-151bd5fbcbd7.png)'
  prefs: []
  type: TYPE_IMG
- en: It's possible to prove the last eigenvector (if the eigenvalues are sorted in
    descending order, it's the bottom one) has all components *v[1]^((N)), v[2]^((N))**,
    ..., v[N]^((N) )= v*, and the corresponding eigenvalue is null. As Saul and Roweis
    (*An introduction to locally linear embedding*,*Saul L. K.*, *Roweis S. T.*) pointed
    out, all the other *Q* eigenvectors (from the bottom) are orthogonal, and this
    allows them to have zero-centered embedding. Hence, the last eigenvector is discarded,
    while the remaining Q eigenvectors determine the embedding vectors *φ[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For further details about MLLE, please refer to *MLLE: Modified Locally Linear
    Embedding Using Multiple Weights*,*Zhang Z., Wang J.*,[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382).'
  prefs: []
  type: TYPE_NORMAL
- en: Example of locally linear embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now apply this algorithm to the Olivetti faces dataset, instantiating
    the Scikit-Learn class `LocallyLinearEmbedding` with `n_components=2` and `n_neighbors=15`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result (limited to the first 100 samples) is shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc32a388-61c7-4671-8b2b-3d5c2c123179.png)'
  prefs: []
  type: TYPE_IMG
- en: Locally linear embedding applied to 100 samples drawn from the Olivetti faces
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: Even if the strategy is different from Isomap, we can determine some coherent
    clusters. In this case, the similarity is obtained through the conjunction of
    small linear blocks; for the faces, they can represent particular micro-features,
    like the shape of the nose or the presence of glasses, that remain invariant in
    the different portraits of the same person. LLE is, in general, preferable when
    the original dataset is intrinsically locally linear, possibly lying on a smooth
    manifold. In other words, LLE is a reasonable choice when small parts of a sample
    are structured in a way that allows the reconstruction of a point given the neighbors
    and the weights. This is often true for images, but it can be difficult to determine
    for a generic dataset. When the result doesn't reproduce the original clustering,
    it's possible to employ the next algorithm or **t-SNE**, which is one the most
    advanced.
  prefs: []
  type: TYPE_NORMAL
- en: Laplacian Spectral Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This algorithm, based on the spectral decomposition of a graph Laplacian, has
    been proposed in order to perform a non-linear dimensionality reduction to try
    to preserve the nearness of points in the *P*-dimensional manifold when remapping
    on a *Q*-dimensional (with *Q < P*) subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure is very similar to the other algorithms. The first step is a
    *k*-nearest neighbor clustering to generate a graph where the vertices (we can
    assume to have *N* elements) are the samples, and the edges are weighted using
    an RBF kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b995de2-674b-45f0-8e21-137a7cedd0fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The resulting graph is undirected and symmetric. We can now define a pseudo-degree
    matrix *D*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/924b2158-8338-476f-9d88-854b753d8a0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The low-dimensional representation *Φ* is obtained by minimizing the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e635be3-4000-4606-853c-3d53eb27a48c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the two points *x[i]* and *x[j]* are near, the corresponding *W[ij]* is
    close to *1*, while it tends to 0 when the distance tends to *∞*. *D[ii]* is the
    sum of all weights originating from *x[i]* (and the same for *D[jj]*). Now, let''s
    suppose that *x[i]* is very close only to *x*[*j* ]so, to approximate *D[ii] =
    D[jj] ≈ W[ij]*. The resulting formula is a square loss based on the difference
    between the vectors *φ[i]* and *φ[j]*. When instead there are multiple *closeness*
    relationships to consider, the factor *W[ij]* divided by the square root of *D[ii]D[jj]* allows
    reweighting the new distances to find the best trade-off for the whole dataset.
    In practice, *L[Φ]* is not minimized directly. In fact, it''s possible to prove
    that the minimum can be obtained through the spectral decomposition of the symmetric
    normalized graph Laplacian (the name derives from this procedure):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d93c1574-7601-48d6-9ecc-9b79d84be4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like for the LLE algorithm, Laplacian Spectral Embedding also works with
    the bottom *Q + 1* eigenvectors. The mathematical theory behind the last step
    is always based on the application of the Rayleigh-Ritz method. The last one is
    discarded, and the remaining *Q* determines the low-dimensional representation *φ*[*i*.]
  prefs: []
  type: TYPE_NORMAL
- en: Example of Laplacian Spectral Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s apply this algorithm to the same dataset using the Scikit-Learn class
    `SpectralEmbedding`, with `n_components=2` and `n_neighbors=15`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot (zoomed in due to the presence of a high-density region)
    is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b627d15a-85ad-4aa7-a113-4f9468463063.png)'
  prefs: []
  type: TYPE_IMG
- en: Laplacian Spectral Embedding applied to the Olivetti faces dataset
  prefs: []
  type: TYPE_NORMAL
- en: Even in this case, we can see that some classes are grouped into small clusters,
    but at the same time, we observe many agglomerates where there are mixed samples.
    Both this and the previous method work with local pieces of information, trying
    to find low-dimensional representations that could preserve the geometrical structure
    of micro-features. This condition drives to a mapping where close points *share*
    local features (this is almost always true for images, but it's very difficult
    to prove for generic samples). Therefore, we can observe small clusters containing
    elements belonging to the same class, but also some *apparent* outliers, which,
    on the original manifold, can be globally different even if they share local *patches*.
    Instead, methods like Isomap or t-SNE work with the whole distribution, and try
    to determine a representation that is almost isometric with the original dataset
    considering its global properties.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This algorithm, proposed by Van der Mateen and Hinton and formally known as
    **t-Distributed Stochastic Neighbor Embedding** (**t-SNE**), is one of the most
    powerful manifold dimensionality reduction techniques. Contrary to the other methods,
    this algorithm starts with a fundamental assumption: the similarity between two
    *N*-dimensional points *x[i]* and *x[j]* can be represented as the conditional
    probability *p(x[j]|x[i])* where each point is represented by a Gaussian distribution
    centered in *x[i]* and with variance *σ[i]*. The variances are selected starting
    from the desired perplexity, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e19f867d-dc23-4af6-aaeb-85b043b4b402.png)'
  prefs: []
  type: TYPE_IMG
- en: Low-perplexity values indicate a low uncertainty, and are normally preferable.
    In common t-SNE tasks, values in the range *10÷50* are normally acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The assumption on the conditional probabilities can be interpreted thinking
    that if two samples are very similar, the probability associated with the first
    sample conditioned to the second one is high, while dissimilar points yield low
    conditional probabilities. For example, thinking about images, a point centered
    in the pupil can have as neighbors some points belonging to an eyelash. In terms
    of probabilities, we can think that *p(eyelash|pupil)* is quite high, while *p(nose|pupil)*
    is obviously lower. t-SNE models these conditional probabilities as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6b5d716-61c2-49a9-ab85-8b46e8267481.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probabilities *p(x[i]|x[i]**)* are set to zero, so the previous formula
    can be extended to the whole graph. In order to solve the problem in an easier
    way, the conditional probabilities are also symmetrized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1ab9e9c-a17a-49f4-8013-cc6fe8f9aefa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability distribution so obtained represents the high-dimensional input
    relationship. As our goal is to reduce the dimensionality to a value *M < N*,
    we can think about a similar probabilistic representation for the target points *φ[i]*,
    using a student-t distribution with one degree of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a1f8af3-c855-4f07-9696-b95dc7c9d696.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want the low-dimensional distribution *Q* to be as close as possible to
    the high-dimensional distribution *P*; therefore, the aim of the **t-SNE** algorithm
    is to minimize the Kullback-Leibler divergence between *P* and *Q*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f8a6945-e821-4be0-a118-8b6efc69d58d.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term is the entropy of the original distribution *P*, while the second
    one is the cross-entropy *H(P, Q)*, which has to be minimized to solve the problem.
    The best approach is based on a gradient-descent algorithm, but there are also
    some useful variations that can improve the performance discussed in *Visualizing
    High-Dimensional Data Using t-SNE*, *Van der Maaten L.J.P., Hinton G.E., Journal
    of Machine Learning Research 9 (Nov), 2008.*
  prefs: []
  type: TYPE_NORMAL
- en: Example of t-distributed stochastic neighbor embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can apply this powerful algorithm to the same Olivetti faces dataset, using
    the Scikit-Learn class `TSNE` with `n_components=2` and `perplexity=20`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result for all 400 samples is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b23c4bd-e002-43c1-be89-8fd83ab1a5e1.png)'
  prefs: []
  type: TYPE_IMG
- en: t-SNE applied to the Olivetti faces dataset
  prefs: []
  type: TYPE_NORMAL
- en: A visual inspection of the label distribution can confirm that t-SNE recreated
    the optimal clustering starting from the original high-dimensional distribution.
    This algorithm can be employed in several non-linear dimensionality reduction
    tasks, such as images, word embeddings, or complex feature vectors. Its main strength
    is hidden in the assumption to consider the similarities as probabilities, without
    the need to impose any constraint on the pairwise distances, either global or
    local. Under a certain viewpoint, it's possible to consider t-SNE as a reverse
    multiclass classification problem based on a cross-entropy cost function. Our
    goal is to find the labels (low-dimensional representation) given the original
    distribution and an assumption about the output distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we could try to answer a natural question: which algorithm must
    be employed? The obvious answer is it depends on the single problem. When it''s
    useful to reduce the dimensionality, preserving the global similarity among vectors
    (this is the case when the samples are long feature vectors without local properties,
    such as word embeddings or data encodings), t-SNE or Isomap are good choices.
    When instead it''s necessary to keep the local distances (for example, the structure
    of a visual patch that can be shared by different samples also belonging to different
    classes) as close as possible to the original representation, locally linear embedding
    or spectral embedding algorithms are preferable.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have introduced the most important label propagation techniques.
    In particular, we have seen how to build a dataset graph based on a weighting
    kernel, and how to use the geometric information provided by unlabeled samples
    to determine the most likely class. The basic approach works by iterating the
    multiplication of the label vector times the weight matrix until a stable point
    is reached and we have proven that, under simple assumptions, it is always possible.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach, implemented by Scikit-Learn, is based on the transition probability
    from a state (represented by a sample) to another one, until the convergence to
    a labeled point. The probability matrix is obtained using a normalized weight
    matrix to encourage transitions associated to close points and discourage all
    the *long jumps*. The main drawback of these two methods is the hard-clamping
    of labeled samples; this constraint can be useful if we *trust* our dataset, but
    it can be a limitation in the presence of outliers whose label has been wrongly
    assigned.
  prefs: []
  type: TYPE_NORMAL
- en: Label spreading solves this problem by introducing a clamping factor that determines
    the percentage of clamped labels. The algorithm is very similar to label propagation,
    but it's based on graph Laplacian and can be employed in all those problems where
    the data-generating distribution is not well-determined and the probability of
    noise is high.
  prefs: []
  type: TYPE_NORMAL
- en: The propagation based on Markov random walks is a very simple algorithm that
    can estimate the class distribution of unlabeled samples through a stochastic
    process. It's possible to imagine it as a *test sample* that walks through the
    graph until it reaches a final labeled state (acquiring the corresponding label).
    The algorithm is very fast and it has a closed-form solution that can be found
    by solving a linear system.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic was the introduction of manifold learning with the Isomap algorithm,
    which is a simple but powerful solution based on a graph built using a *k*-nearest
    neighbors algorithm (this is a common step in most of these algorithms). The original
    pairwise distances are processed using the multidimensional scaling technique,
    which allows obtaining a low-dimensional representation where the distances between
    samples are preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Two different approaches, based on local pieces of information, are locally
    linear embedding and Laplacian Spectral Embedding. The former tries to preserve
    the local linearity present in the original manifold, while the latter, which
    is based on the spectral decomposition of the normalized graph Laplacian, tries
    to preserve the nearness of original samples. Both methods are suitable for all
    those tasks where it's important not to consider the whole original distribution,
    but the similarity induced by small data *patches*.
  prefs: []
  type: TYPE_NORMAL
- en: We closed this chapter by discussing t-SNE, which is a very powerful algorithm
    that tries to model a low-dimensional distribution that is as similar as possible
    to the original high-dimensional one. This task is achieved by minimizing the
    Kullback-Leibler divergence between the two distributions. t-SNE is a state-of-the-art
    algorithm, useful whenever it's important to consider the whole original distribution
    and the similarity between entire samples.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 4](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml),
    *Bayesian Networks and Hidden Markov Models *we're going to introduce Bayesian
    networks in both a static and dynamic context, and hidden Markov models, with
    practical prediction examples. These algorithms allow modeling complex probabilistic
    scenarios made up of observed and latent variables, and infer future states using
    optimized sampling methods based only on the observations.
  prefs: []
  type: TYPE_NORMAL
