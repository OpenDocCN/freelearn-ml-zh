- en: Representing Data and Engineering Features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示数据和特征工程
- en: In the last chapter, we built our very first supervised learning models and
    applied them to some classic datasets, such as the **Iris** and the **Boston** datasets.
    However, in the real world, data rarely comes in a neat `<n_samples x n_features>` **feature
    matrix** that is part of a pre-packaged database. Instead, it is our responsibility
    to find a way to represent the data in a meaningful way. The process of finding
    the best way to represent our data is known as **feature engineering**, and it
    is one of the main tasks of data scientists and machine learning practitioners
    trying to solve real-world problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了我们第一个监督学习模型，并将其应用于一些经典数据集，如**Iris**和**Boston**数据集。然而，在现实世界中，数据很少以整洁的`<n_samples
    x n_features>`**特征矩阵**的形式出现，这是预包装数据库的一部分。相反，我们的责任是找到一种有意义的表示数据的方法。寻找最佳数据表示方式的过程被称为**特征工程**，这是数据科学家和试图解决现实世界问题的机器学习从业者的一项主要任务。
- en: I know you would rather jump right to the end and build the deepest neural network
    mankind has ever seen. ...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道您更愿意直接跳到结尾，构建人类有史以来最深的神经网络。 ...
- en: Technical requirements
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter04).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接中获取本章的代码：[https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter04)。
- en: 'Here is a summary of the software and hardware requirements:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是软件和硬件要求的一个总结：
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将需要OpenCV版本4.1.x（4.1.0或4.1.1都可以正常工作）。
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将需要Python版本3.6（任何Python 3.x版本都可以）。
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要Anaconda Python 3来安装Python和所需的模块。
- en: You can use any operating system—macOS, Windows, and Linux-based OSes, along
    with this book. We recommend you have at least 4 GB RAM in your system.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用任何操作系统——macOS、Windows以及基于Linux的操作系统，配合本书使用。我们建议您的系统至少有4GB RAM。
- en: You don't need to have a GPU to run the code provided along with this book.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不需要拥有GPU来运行本书提供的代码。
- en: Understanding feature engineering
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: 'Believe it or not, how well a machine learning system can learn is mainly determined
    by the quality of the training data. Although every learning algorithm has its
    strengths and weaknesses, differences in performance often come down to the way
    the data is prepared or represented. Feature engineering can hence be understood
    as a tool for **data representation**. Machine learning algorithms try to learn
    a solution to a problem from sample data, and feature engineering asks: what is
    the best representation of the sample data to use for learning a solution to the
    problem?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，一个机器学习系统能够学习的好坏主要取决于训练数据的质量。尽管每个学习算法都有其优势和劣势，但性能的差异通常归结于数据准备或表示的方式。因此，特征工程可以理解为一种**数据表示**的工具。机器学习算法试图从样本数据中学习解决问题的解决方案，而特征工程则问：使用什么最佳表示的样本数据来学习解决问题的解决方案？
- en: Remember, a couple of chapters ago, we talked about the whole machine learning
    pipeline. There, we already mentioned feature extraction, but we ...
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在前几章中，我们讨论了整个机器学习流程。在那里，我们已经提到了特征提取，但我们也 ...
- en: Preprocessing data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'The more disciplined we are in handling our data, the better results we are
    likely to achieve in the end. The first step in this procedure is known as **data
    preprocessing**, and it comes in (at least) three different flavors:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理数据方面越有纪律，最终可能获得的结果就越好。这个程序的第一步被称为**数据预处理**，它至少有三种不同的风味：
- en: '**Data formatting**: The data may not be in a format that is suitable for us
    to work with; for example, the data might be provided in a proprietary file format,
    which our favorite machine learning algorithm does not understand.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据格式化**：数据可能不是我们能够工作的格式；例如，数据可能以专有文件格式提供，而我们最喜欢的机器学习算法无法理解。'
- en: '**Data cleaning**: The data may contain invalid or missing entries, which need
    to be cleaned up or removed.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：数据可能包含无效或缺失条目，需要清理或删除。'
- en: '**Data sampling**: The data may be far too large for our specific purpose,
    forcing us to sample the data intelligently.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据采样**：数据可能远远超出了我们特定目的的需求，迫使我们智能地采样数据。'
- en: 'Once the data has been preprocessed, we are ready for the actual feature engineering:
    to transform the preprocessed data to fit our specific machine learning algorithm.
    This step usually involves one or more of three possible procedures:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据预处理完成，我们就可以准备进行实际的特征工程：将预处理后的数据转换以适应我们特定的机器学习算法。这一步通常涉及以下三种可能过程之一或多个：
- en: '**Scaling**: Certain machine learning algorithms often require the data to
    be within a common range, such as to have zero mean and unit variance. Scaling
    is the process of bringing all features (which might have different physical units)
    into a common range of values.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放**：某些机器学习算法通常要求数据在公共范围内，例如具有零均值和单位方差。缩放是将所有特征（可能具有不同的物理单位）带入一个共同值范围的过程。'
- en: '**Decomposition**: Datasets often have many more features than we could possibly
    process. Feature decomposition is the process of compressing data into a smaller
    number of highly informative data components.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分解**：数据集通常具有比我们能够处理更多的特征。特征分解是将数据压缩成更少但高度信息丰富的数据组件的过程。'
- en: '**Aggregation**: Sometimes, it is possible to group multiple features into
    a single, more meaningful one. For example, a database might contain the date
    and time for each user who logged into a web-based system. Depending on the task,
    this data might be better represented by simply counting the number of logins
    per user.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：有时，可以将多个特征组合成一个更有意义的单一特征。例如，数据库可能包含每个用户登录基于网络的系统的日期和时间。根据任务的不同，这些数据可能通过简单地计算每个用户的登录次数来更好地表示。'
- en: Let's look at some of these processes in more detail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些过程。
- en: Standardizing features
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化特征
- en: '**Standardization** refers to the process of scaling the data to have zero
    mean and unit variance. This is a common requirement for a wide range of machine learning algorithms,
    which might behave badly if individual features do not fulfill this requirement.
    We could manually standardize our data by subtracting from every data point the
    mean value (*μ*) of all of the data and dividing that by the variance (*σ*) of
    the data; that is, for every feature *x*, we would compute *(x - μ) / σ*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化**指的是将数据缩放到具有零均值和单位方差的过程。这对于广泛的机器学习算法来说是一个常见的要求，如果单个特征不满足这一要求，这些算法可能会表现不佳。我们可以通过从每个数据点中减去所有数据的平均值（*μ*）并除以数据的方差（*σ*）来手动标准化我们的数据；也就是说，对于每个特征*x*，我们会计算*(x
    - μ) / σ*。'
- en: Alternatively, scikit-learn offers a straightforward implementation of this
    process in its `preprocessing` module.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，scikit-learn在其`preprocessing`模块中提供了一个直接实现此过程的简单方法。
- en: 'Let''s consider a 3 x 3 data matrix, `X`, standing for three data points (rows)
    with three arbitrarily chosen feature values each (columns): ...'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个3 x 3的数据矩阵，`X`，代表三个数据点（行）以及每个数据点三个任意选择的特征值（列）：...
- en: Normalizing features
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化特征
- en: 'Similar to standardization, normalization is the process of scaling individual samples to
    have a unit norm. I''m sure you know that the norm stands for the length of a
    vector and can be defined in different ways. We discussed two of them in the previous
    chapter: the L1 norm (or Manhattan distance), and the L2 norm (or Euclidean distance).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准化类似，归一化是缩放单个样本以具有单位范数的过程。我确信你知道范数代表向量的长度，并且可以以不同的方式定义。我们在上一章讨论了其中的两种：L1范数（或曼哈顿距离）和L2范数（或欧几里得距离）。
- en: 'In scikit-learn, our data matrix, `X`, can be normalized using the `normalize` function,
    and the `l1` norm is specified by the `norm` keyword:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，我们可以使用`normalize`函数来对数据矩阵`X`进行归一化，并通过`norm`关键字指定`l1`范数：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Similarly, the L2 norm can be computed by specifying `norm=''l2''`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可以通过指定`norm='l2'`来计算L2范数：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Scaling features to a range
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放到一个范围
- en: 'An alternative to scaling features to zero mean and unit variance is to get features to
    lie between a given minimum and maximum value. Often, these values are zero and
    one, so that the maximum absolute value of each feature is scaled to unit size. In
    scikit-learn, this can be achieved using `MinMaxScaler`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放特征到零均值和单位方差的一个替代方法是让特征位于给定的最小值和最大值之间。通常，这些值是零和一，这样每个特征的绝对最大值就被缩放为单位大小。在scikit-learn中，这可以通过使用`MinMaxScaler`实现：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By default, the data will be scaled to fall within 0 and 1\. We can specify
    different ranges by passing a keyword argument, `feature_range`, to the `MinMaxScaler` constructor:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，数据将被缩放到0和1之间。我们可以通过向`MinMaxScaler`构造函数传递一个关键字参数`feature_range`来指定不同的范围：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Binarizing features
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二值化特征
- en: 'Finally, we might find ourselves not caring too much about the exact feature values of
    the data. Instead, we might just want to know whether a feature is present or
    absent. Binarizing the data can be achieved by thresholding the feature values.
    Let''s quickly remind ourselves of our feature matrix, `X`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可能不太关心数据的精确特征值。相反，我们可能只想知道一个特征是否存在。通过阈值处理特征值可以实现数据的二值化。让我们快速回顾一下我们的特征矩阵`X`：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s assume that these numbers represent the thousands of dollars in our
    bank accounts. If there are more than 0.5 thousand dollars in the account, we
    consider the person rich, which we represent with a 1\. Otherwise, we put a 0\.
    This is akin to thresholding the data with `threshold=0.5`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这些数字代表我们银行账户中的数千美元。如果账户中有超过0.5千美元，我们将其视为富人，用1表示。否则，我们用0表示。这类似于使用`threshold=0.5`对数据进行阈值处理：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result is a matrix made up entirely of ones and zeros.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个由全1和全0组成的矩阵。
- en: Handling the missing data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: 'Another common requirement in feature engineering is the handling of missing
    data. For example, we might have a dataset that looks like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程中，另一个常见的要求是处理缺失数据。例如，我们可能有一个看起来像这样的数据集：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Most machine learning algorithms cannot handle the **Not a Number** (**NAN**)
    values (`nan` in Python). Instead, we first have to replace all of the `nan` values
    with some appropriate fill values. This is known as the **imputation** of missing
    values.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法都无法处理**非数字**（Python中的`nan`）值。因此，我们首先必须用一些适当的填充值替换所有`nan`值。这被称为缺失值的**插补**。
- en: 'Three different strategies to impute missing values are offered by scikit-learn:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了三种不同的策略来插补缺失值：
- en: '`mean`: Replaces all of the `nan` values with a mean value along a specified
    axis of the matrix (default: *axis = 0*)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean`: 使用矩阵指定轴上的平均值替换所有`nan`值（默认：*axis = 0*）'
- en: '`median`: Replaces all ...'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`median`: 使用...'
- en: Understanding dimensionality reduction
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解降维
- en: Datasets often have many more features than we could possibly process. For example,
    let's say our job was to predict a country's poverty rate. We would probably start
    by matching a country's name with its poverty rate, but that would not help us
    to predict the poverty rate of a new country. So, we start thinking about the
    possible causes of poverty. But how many possible causes of poverty are there? Factors
    might include a country's economy, lack of education, high divorce rate, overpopulation,
    and so on. If each one of these causes was a feature used to help to predict the
    poverty rate, we would end up with a countless number of features. If you're a
    mathematician, you might think of these features as axes in a high-dimensional
    space, and every country's poverty rate is then a single point in this high-dimensional
    space.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集通常具有比我们可能处理的更多的特征。例如，让我们假设我们的工作是预测一个国家的贫困率。我们可能会首先将一个国家的名字与其贫困率相匹配，但这不会帮助我们预测新国家的贫困率。因此，我们开始思考贫困的可能原因。但有多少可能的贫困原因呢？可能包括一个国家的经济、缺乏教育、高离婚率、人口过剩等等。如果每个原因都是一个用于帮助预测贫困率的特征，我们最终会得到无数个特征。如果你是一位数学家，你可能会将这些特征视为高维空间中的**轴**，而每个国家的贫困率在这个高维空间中就是一个单独的点。
- en: 'If you''re not a mathematician, it might help to start small. Let''s say, we
    first look at only two features: a country''s **Gross Domestic Product** (**GDP**)
    and the number of citizens. We interpret the GDP as the *x axis*, and the number
    of citizens as the *y axis*, in a 2D space. Then, we look at the first country.
    It has a small GDP and an average number of citizens. We draw a point in the *x-y* plane
    that represents this country. We add a second, third, and fourth country. The
    fourth country just happens to have both a high GDP and a large number of citizens.
    Hence, our four data points might be spread across the *x-y* plane much like in
    the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是数学家，从小处着手可能会有所帮助。比如说，我们首先只看两个特征：一个国家的**国内生产总值**（**GDP**）和公民数量。我们将GDP解释为**x轴**，公民数量解释为**y轴**，在一个二维空间中。然后，我们来看第一个国家。它有一个较小的GDP和平均数量的公民。我们在**x-y**平面上画一个点来代表这个国家。我们再添加第二个、第三个和第四个国家。第四个国家恰好既有高GDP又有大量公民。因此，我们的四个数据点可能会像以下截图那样在**x-y**平面上分布得很开：
- en: '![](img/ecfc1cd0-8e6f-4658-8f61-ac5429753231.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecfc1cd0-8e6f-4658-8f61-ac5429753231.png)'
- en: However, what happens if we start adding a third feature, such as the country's
    divorce rate, to our analysis? This would add a third axis to our plot (*z axis*).
    Suddenly, we find that the data no longer spreads very nicely across the *x-y-z* cube,
    as most of the cube remains empty. While in two dimensions, it seemed like we
    had most of the *x-y* square covered, in three dimensions, we would need many
    more data points to fill the void between the data points 1 to 3 and the lonely
    data point 4 in the upper-right corner.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们开始添加第三个特征，比如国家的离婚率，到我们的分析中会发生什么？这将给我们的图表添加一个第三个轴（*z轴*）。突然，我们发现数据不再非常均匀地分布在**x-y-z**立方体上，因为立方体的大部分仍然是空的。在二维空间中，我们似乎已经覆盖了大部分的**x-y**正方形，但在三维空间中，我们需要更多的数据点来填补数据点1到3之间的空白，以及右上角孤独的数据点4。
- en: 'This problem is also known as the **curse of dimensionality**: the number of
    data points needed to fill the available space grows exponentially with the number
    of dimensions (or plot axes). If a classifier is not fed with data points that
    span the entire feature space (such as shown in the preceding cube example), the
    classifier will not know what to do once a new data point is presented that lies
    far away from all of the previously encountered data points.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题也被称为**维度灾难**：填充可用空间所需的数据点数量随着维度（或图表轴）数量的指数增长。如果一个分类器没有提供跨越整个特征空间的数据点（如前述立方体示例所示），那么当出现一个位于所有先前遇到的数据点都很远的新数据点时，分类器将不知道该怎么办。
- en: 'The curse of dimensionality means that, after a certain number of features
    (or dimensions), the performance of the classifier will start degrading. Let''s
    try to understand this. More features essentially mean that more variations in
    the dataset can be accounted for. But taking into account more than the required
    features will cause the classifier to even take into account any outliers or to
    overfit the dataset. Hence, the performance of the classifier will start degrading
    rather than improving:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难意味着，在一定的特征数量（或维度）之后，分类器的性能将开始下降。让我们来理解这一点。更多的特征本质上意味着可以解释数据集中的更多变化。但是，考虑超过所需特征的数量会导致分类器甚至考虑任何异常值或过度拟合数据集。因此，分类器的性能将开始下降而不是提高：
- en: '![](img/b1c75237-aa2d-4f6f-b2d4-f91c27700030.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b1c75237-aa2d-4f6f-b2d4-f91c27700030.png)'
- en: But, how do we find this seemingly optimal number of dimensions for our dataset?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们如何找到数据集的看似最优的维度数量？
- en: This is where dimensionality reduction comes in to play. These are a family
    of techniques that allow us to find a compact representation of our high-dimensional
    data, without losing too much information.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**降维**发挥作用的地方。这是一系列技术，允许我们找到高维数据的紧凑表示，而不会丢失太多信息。
- en: Implementing Principal Component Analysis (PCA) in OpenCV
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中实现主成分分析（PCA）
- en: One of the most common dimensionality reduction techniques is called **PCA**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的降维技术之一被称为**PCA**。
- en: Similar to the 2D and 3D examples shown earlier, we can think of an image as
    a point in a high-dimensional space. If we flatten a 2D grayscale image of height *m* and
    width *n* by stacking all of the columns, we get a (feature) vector of length *m
    x n x 1*. The value of the *i*^(th) element in this vector is the grayscale value
    of the *i*^(th) pixel in the image. Now, imagine we would like to represent every
    possible 2D grayscale image with these exact dimensions. How many images would
    that give?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面展示的2D和3D示例类似，我们可以将图像视为高维空间中的一个点。如果我们通过堆叠所有列将高度为*m*和宽度为*n*的2D灰度图像展平，我们得到一个长度为*m
    x n x 1*的（特征）向量。这个向量中第*i*个元素的值是图像中第*i*个像素的灰度值。现在，想象一下，如果我们想用这些精确的维度来表示每个可能的2D灰度图像，那会有多少个图像？
- en: Since grayscale pixels usually take values between 0 and 255, there are a total
    of 256 raised to the power of *m x* *n* images. Chances ...
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于灰度像素通常取值在0到255之间，总共有256的*m x* *n*次方个图像。机会 ...
- en: Implementing independent component analysis (ICA)
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现独立成分分析 (ICA)
- en: Other useful dimensionality reduction techniques that are closely related to
    PCA are provided by scikit-learn, but not OpenCV. We mention them here for the
    sake of completeness. ICA performs the same mathematical steps as PCA, but it
    chooses the components of the decomposition to be as independent as possible from
    each other, rather than per predictor as in PCA.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了与PCA密切相关但不是OpenCV的其他有用的降维技术。我们在这里提及它们是为了完整性。ICA执行与PCA相同的数学步骤，但它选择分解的组件尽可能相互独立，而不是像PCA那样按预测变量选择。
- en: 'In scikit-learn, ICA is available from the `decomposition` module:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，ICA可以从`decomposition`模块中获取：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Why do we use `tol=0.005`? Because we want the FastICA to converge to some particular
    value. There are two methods to do that—increase the number of iterations (the
    default value is `200`) or decrease the tolerance (the default value is `0.0001`).
    I tried to increase the iterations but, unfortunately, it didn't work, so I went
    ahead with the other option. Can you figure out why it didn't converge?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用`tol=0.005`？因为我们希望FastICA收敛到某个特定的值。有两种方法可以实现这一点——增加迭代次数（默认值为`200`）或减少容差（默认值为`0.0001`）。我尝试增加迭代次数，但不幸的是，它不起作用，所以我选择了另一种方法。你能想出为什么它没有收敛吗？
- en: 'As seen before, the data transformation happens in the `fit_transform` function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据转换发生在`fit_transform`函数中：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In our case, plotting the rotated data leads to a similar result to that achieved
    with PCA earlier, as can be verified in the diagram that follows this code block.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，绘制旋转后的数据会导致与之前使用PCA获得的结果相似，这可以在随后的代码块后的图中验证。
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This can be seen in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在以下图中看到：
- en: '![](img/1113f231-b274-4a4e-8f27-c47cf82851e9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1113f231-b274-4a4e-8f27-c47cf82851e9.png)'
- en: Implementing non-negative matrix factorization (NMF)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现非负矩阵分解 (NMF)
- en: Another useful dimensionality reduction technique is called **NMF**. It again
    implements the same basic mathematical operations as PCA and ICA, but it has the additional constraint
    that it **only operates on non-negative data**. In other words, we cannot have
    negative values in our feature matrix if we want to use NMF; the resulting components
    of the decomposition will all have non-negative values as well.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有用的降维技术称为**NMF**。它再次实现了与PCA和ICA相同的基本数学运算，但它有一个额外的约束，即它**仅对非负数据进行操作**。换句话说，如果我们想使用NMF，我们的特征矩阵中不能有负值；分解的结果也将全部具有非负值。
- en: 'In scikit-learn, NMF works exactly like ICA:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，NMF与ICA的工作方式完全相同：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Visualizing the dimensionality reduction using t-Distributed Stochastic Neighbor
    Embedding (t-SNE)
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用t-Distributed Stochastic Neighbor Embedding (t-SNE)可视化降维
- en: t-SNE is a technique for dimensionality reduction that is best suited to the
    visualization of high-dimensional data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种降维技术，非常适合高维数据的可视化。
- en: 'In this section, we will see an example of how to visualize high-dimensional
    datasets using t-SNE. Let''s use the digits dataset in this case, which has handwritten
    images of digits from 0 to 9\. It''s a publicly available dataset, commonly referred
    to as the MNIST dataset. We will see how we can visualize the dimensionality reduction
    on this dataset using t-SNE:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到一个如何使用t-SNE可视化高维数据集的例子。以数字数据集为例，其中包含从0到9的手写数字图像。这是一个公开可用的数据集，通常被称为MNIST数据集。我们将看到如何使用t-SNE在这个数据集上可视化降维：
- en: 'First, let''s load the dataset:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据集：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should first apply a dimensional reduction technique such as PCA to reduce
    the high number of dimensions to a lower number and then use a technique such
    as t-SNE to visualize the data. But, in this case, let''s use all of the dimensions
    and use t-SNE directly:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该首先应用PCA等降维技术将高维数减少到较低的维数，然后使用t-SNE等技术来可视化数据。但是，在这种情况下，让我们使用所有维度并直接使用t-SNE：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, let''s visualize the two dimensions that we have extracted using t-SNE
    with the help of a scatterplot:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用散点图帮助我们可视化使用t-SNE提取的两个维度：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And we get the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/61cf6482-31a5-407e-a21e-f5a514efb2ee.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61cf6482-31a5-407e-a21e-f5a514efb2ee.png)'
- en: Now, let's discuss how we can represent categorical variables in the next section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在下一节讨论如何表示分类变量。
- en: Representing categorical variables
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示分类变量
- en: One of the most common data types we might encounter while building a machine learning system
    is **categorical features** (also known as **discrete features**), such as the color of
    a fruit or the name of a company. The challenge with categorical features is that
    they don't change in a continuous way, which makes it hard to represent them with
    numbers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习系统时，我们可能会遇到的最常见的数据类型之一是**分类特征**（也称为**离散特征**），例如水果的颜色或公司的名称。分类特征的挑战在于它们不是以连续的方式变化的，这使得用数字表示它们变得困难。
- en: For example, a banana is either green or yellow, but not both. A product belongs
    either in the clothing department or in the books department, but rarely in both,
    and so on.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，香蕉要么是绿色的，要么是黄色的，但不会同时是两者。一个产品要么属于服装部门，要么属于书籍部门，很少同时属于两者，等等。
- en: How would you go about representing such features?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何表示这样的特征？
- en: 'For example, let''s assume we are trying to encode a dataset consisting of
    a list of forefathers of machine learning and artificial intelligence: ...'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在尝试编码一个包含机器学习和人工智能先驱名单的数据集：...
- en: Representing text features
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示文本特征
- en: Similar to categorical features, scikit-learn offers an easy way to encode another
    common feature type—text features. When working with text features, it is often
    convenient to encode individual words or phrases as numerical values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类特征类似，scikit-learn提供了一个简单的方法来编码另一种常见的特征类型——文本特征。当处理文本特征时，通常方便将单个单词或短语编码为数值。
- en: 'Let''s consider a dataset that contains a small corpus of text phrases:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个包含少量文本短语的语料库数据集：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One of the simplest methods of encoding such data is by word count; for each
    phrase, we simply count the occurrences of each word within it. In scikit-learn,
    this is easily done using `CountVectorizer`, which functions akin to `DictVectorizer`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 编码此类数据的最简单方法之一是通过词频统计；对于每个短语，我们只需统计其中每个单词的出现次数。在scikit-learn中，这可以通过使用`CountVectorizer`轻松完成，其功能类似于`DictVectorizer`：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'By default, this will store our feature matrix, `X`, as a sparse matrix. If
    we want to manually inspect it, we need to convert it into a regular array:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这将把我们的特征矩阵`X`存储为稀疏矩阵。如果我们想手动检查它，我们需要将其转换为常规数组：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To understand what these numbers mean, we have to look at the feature names:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这些数字的含义，我们必须查看特征名称：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, it becomes clear what the integers in `X` mean. If we look at the phrase
    that is represented in the top row of `X`, we see that it contains one occurrence
    of the word, `engineering`, and one occurrence of the word, `feature`. On the
    other hand, it does not contain the words `extraction` or `selection`. Does this
    make sense? A quick glance at our original data `sample` reveals that the phrase
    was indeed `feature engineering`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`X`中的整数含义变得清晰。如果我们查看`X`顶部行表示的短语，我们会看到它包含一次`engineering`这个词的出现，以及一次`feature`这个词的出现。另一方面，它不包含`extraction`或`selection`这两个词。这有意义吗？快速查看我们的原始数据`sample`揭示，这个短语确实是`feature
    engineering`。
- en: Looking only at the `X` array (no cheating!), can you guess what the last phrase
    in `sample` was?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 只看`X`数组（不要作弊！），你能猜出`sample`中的最后一个短语是什么吗？
- en: One possible shortcoming of this approach is that we might put too much weight
    on words that appear very frequently. One approach to fixing this is known as **Term
    Frequency-Inverse Document Frequency** (**TF-IDF**). What TF-IDF does might be
    easier to understand than its name, which is basically to weigh the word counts
    by a measure of how often they appear in the entire dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个可能的缺点是我们可能会过分重视出现频率非常高的单词。解决这个问题的一个方法被称为**词频-逆文档频率**（**TF-IDF**）。TF-IDF所做的事情可能比它的名字更容易理解，它基本上是通过衡量单词在整个数据集中出现的频率来权衡单词计数。
- en: 'The syntax for TF-IDF is pretty much similar to the previous command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF的语法与之前的命令非常相似：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We note that the numbers are now smaller than before, with the third column
    taking the biggest hit. This makes sense, as the third column corresponds to the
    most frequent word across all three phrases, `feature`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，现在的数字比以前小了，第三列的降幅最大。这是有道理的，因为第三列对应于所有三个短语中最频繁出现的单词“feature”：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you''re interested in the math behind TF-IDF, you can start with this paper:
    [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf).
    For more information about its specific implementation in scikit-learn, have a
    look at the API documentation at [http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)[.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对TF-IDF背后的数学感兴趣，可以从这篇论文开始：[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf)。有关scikit-learn中其具体实现的更多信息，请参阅API文档：[http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)[。](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf)
- en: Representing text features will become important in [Chapter 7](08148129-87ac-4042-944d-8e0a2bbbe0c5.xhtml), *Implementing
    a Spam Filter with Bayesian Learning*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们将讨论表示文本特征的重要性，该章节名为*使用贝叶斯学习实现垃圾邮件过滤器*。
- en: Representing images
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示图像
- en: One of the most common and important data types for computer vision is, of course,
    images. The most straightforward way to represent images is probably by using
    the grayscale value of each pixel in the image. Usually, grayscale values are
    not very indicative of the data they describe. For example, if we saw a single
    pixel with a grayscale value of 128, could we tell what object this pixel belonged
    to? Probably not. Therefore, grayscale values are not very effective image features.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉中最常见且重要的数据类型当然是图像。表示图像最直接的方式可能是使用图像中每个像素的灰度值。通常，灰度值并不能很好地表明它们所描述的数据。例如，如果我们看到一个灰度值为128的单个像素，我们能否判断这个像素属于哪个物体？可能不能。因此，灰度值并不是非常有效的图像特征。
- en: Using color spaces
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用颜色空间
- en: Alternatively, we might find that colors contain some information that raw grayscale
    values cannot capture. Most often, images come in the conventional RGB color space,
    where every pixel in the image gets an intensity value for its apparent **Redness** (**R**), **Greenness** (**G**),
    and **Blueness** (**B**). However, OpenCV offers a whole range of other color
    spaces, such as **Hue Saturation Value** (**HSV**), **Hue Saturation Lightness** (**HSL**),
    and the Lab color space. Let's have a quick look at them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可能会发现颜色包含一些原始灰度值无法捕捉到的信息。最常见的情况是，图像以传统的RGB颜色空间出现，其中图像中的每个像素都有一个表示其显色**红色**（**R**）、**绿色**（**G**）和**蓝色**（**B**）强度的值。然而，OpenCV提供了一系列其他颜色空间，例如**色调饱和度值**（**HSV**）、**色调饱和度亮度**（**HSL**）和Lab颜色空间。让我们快速了解一下它们。
- en: Encoding images in the RGB space
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在RGB空间中编码图像
- en: I am sure that you are already familiar with the RGB color space, which uses additive
    mixing of different shades of red, green, and blue to produce different composite
    colors. The RGB color space is useful in everyday life because it covers a large
    part of the color space that the human eye can see. This is why color television
    sets or color computer monitors only need to care about producing mixtures of
    red, green, and blue light.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你已经熟悉RGB颜色空间了，它通过不同色调的红、绿、蓝的加色混合来产生不同的合成颜色。RGB颜色空间在日常生活中非常有用，因为它覆盖了人眼可以看到的大部分颜色空间。这就是为什么彩色电视或彩色计算机显示器只需要关注产生红、绿、蓝光的混合。
- en: In OpenCV, RGB images are supported straight out of the box. All you need to
    know, or need to be reminded of, is that color images are actually stored as BGR
    images in OpenCV; that is, the order of color channels is blue-green-red instead
    of red-green-blue. The reasons for this ...
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，RGB图像直接支持。你需要知道或需要提醒的是，在OpenCV中，彩色图像实际上是存储为BGR图像；也就是说，颜色通道的顺序是蓝-绿-红，而不是红-绿-蓝。这样做的原因是...
- en: Encoding images in the HSV and HLS space
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在HSV和HLS空间中编码图像
- en: However, ever since the RGB color space was created, people have realized that
    it is actually quite a poor representation of human vision. Therefore, researchers
    have developed many alternative representations. One of them is called **HSV** (short
    for **Hue, Saturation, and Value**) and the other one is called **HLS** (**Hue,
    Lightness, and Saturation**). You might have seen these color spaces in color
    pickers and common image editing software. In these color spaces, the hue of the
    color is captured by a single hue channel, the colorfulness is captured by a saturation
    channel, and the lightness or brightness is captured by a lightness or value channel.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自从RGB颜色空间被创建以来，人们已经意识到它实际上是对人类视觉的一种相当差的表示。因此，研究人员已经开发了许多替代表示。其中之一被称为**HSV**（代表**色调、饱和度和亮度**）和另一个被称为**HLS**（**色调、亮度和饱和度**）。你可能已经在颜色选择器和常见的图像编辑软件中看到过这些颜色空间。在这些颜色空间中，颜色的色调由一个单独的色调通道捕获，颜色的饱和度由饱和度通道捕获，而亮度或亮度由亮度或值通道捕获。
- en: 'In OpenCV, an RGB image can easily be converted into the HSV color space using `cv2.cvtColor`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，可以使用`cv2.cvtColor`轻松将RGB图像转换为HSV颜色空间：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The same is true for the HLS color space. In fact, OpenCV provides a whole
    range of additional color spaces, which are available via `cv2.cvtColor`. All
    we need to do is to replace the color flag with one of the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于HLS颜色空间也是如此。实际上，OpenCV提供了一系列额外的颜色空间，这些颜色空间可以通过`cv2.cvtColor`访问。我们只需要将颜色标志替换为以下之一：
- en: HLS using `cv2.COLOR_BGR2HLS`
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`cv2.COLOR_BGR2HLS`的HLS
- en: LAB (lightness, green-red, and blue-yellow) using `cv2.COLOR_BGR2LAB`
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`cv2.COLOR_BGR2LAB`的LAB（亮度、绿色-红色和蓝色-黄色）
- en: YUV (overall luminance, blue luminance, and red luminance) using `cv2.COLOR_BGR2YUV`
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YUV（整体亮度、蓝色亮度和红色亮度）使用`cv2.COLOR_BGR2YUV`
- en: Detecting corners in images
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在图像中检测角点
- en: 'One of the most straightforward traditional features to find in an image are
    probably corners (locations where several edges meet). OpenCV provides at least
    two different algorithms to find corners in an image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中找到的最简单直观的传统特征可能是角点（几个边缘相交的位置）。OpenCV提供了至少两种不同的算法来在图像中找到角点：
- en: '**Harris Dorner Detection**: Knowing that edges are areas with high-intensity
    changes in all directions, Harris and Stephens came up with a fast way of finding
    such locations. This algorithm is implemented as `cv2.cornerHarris` in OpenCV.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Harris-Dorner检测**：知道边缘是所有方向上强度变化高的区域，Harris和Stephens提出了一种快速找到这种位置的方法。这个算法在OpenCV中实现为`cv2.cornerHarris`。'
- en: '**Shi-Tomasi Corner Detection**: Shi and Tomasi have their own idea of what
    constitute good features to track, and they usually do better than Harris corner
    detection by finding the *N* strongest corners. This algorithm is implemented
    as `cv2.goodFeaturesToTrack` in OpenCV.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shi-Tomasi角点检测**：Shi和Tomasi有他们自己对构成良好跟踪特征的想法，并且他们通常通过找到**N**个最强的角点，比Harris角点检测做得更好。这个算法在OpenCV中实现为`cv2.goodFeaturesToTrack`。'
- en: Harris ...
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Harris ...
- en: Using the star detector and BRIEF descriptor
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用星形检测器和BRIEF描述符
- en: However, corner detection is not sufficient when the scale of an image changes.
    Multiple papers have been published describing different algorithms for feature
    detection and description. We will look at a combination of the **Speeded Up Robust
    Features** (**SURF**) detector (for more information, see [https://en.wikipedia.org/wiki/Speeded_up_robust_features](https://en.wikipedia.org/wiki/Speeded_up_robust_features)) and
    the **Binary Robust Independent Elementary Features** (**BRIEF**) descriptor.
    The feature detector identifies the keypoints in the image and the feature descriptor
    calculates the actual feature value for all of the keypoints.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当图像的尺度发生变化时，角点检测是不够的。已经发表了多篇论文，描述了不同的特征检测和描述算法。我们将查看**Speeded Up Robust Features**（**SURF**）检测器（更多信息请参阅[https://en.wikipedia.org/wiki/Speeded_up_robust_features](https://en.wikipedia.org/wiki/Speeded_up_robust_features)）和**Binary
    Robust Independent Elementary Features**（**BRIEF**）描述符的组合。特征检测器识别图像中的关键点，特征描述符计算所有关键点的实际特征值。
- en: The details of these algorithms are beyond the scope of this book. Advanced
    users can refer to the papers describing these algorithms in detail.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的细节超出了本书的范围。高级用户可以参考详细描述这些算法的论文。
- en: 'For more details, you can refer to the following links:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节，您可以参考以下链接：
- en: 'For SURF: [https://www.vision.ee.ethz.ch/~surf/eccv06.pdf](https://www.vision.ee.ethz.ch/~surf/eccv06.pdf)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于SURF: [https://www.vision.ee.ethz.ch/~surf/eccv06.pdf](https://www.vision.ee.ethz.ch/~surf/eccv06.pdf)'
- en: For BRIEF: [https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf](https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于BRIEF: [https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf](https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf)'
- en: The entire process starts with reading the image, converting it into grayscale,
    using the star feature detector to find the interesting points, and finally, using
    the BRIEF descriptor to calculate the feature value.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程从读取图像开始，将其转换为灰度图，使用星形特征检测器找到有趣点，最后使用BRIEF描述符计算特征值。
- en: 'Let''s first read the image and convert it to grayscale:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先读取图像并将其转换为灰度图：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we will create the feature detector and descriptor:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建特征检测器和描述符：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, it''s time to use the star detector to get the keypoints and pass them
    to the BRIEF descriptor:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，是时候使用星形检测器获取关键点并将它们传递给BRIEF描述符了：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'There''s a catch here. At the time of writing this book, the OpenCV version
    4.0 didn''t have the resolved version of the `cv2.drawKeypoints` function. So,
    I have written a similar function that we can use to draw the keypoints. You don''t
    need to worry about the steps involved in the function—it''s just for your reference.
    If you have installed the OpenCV version specified in this book (OpenCV 4.1.0
    or OpenCV 4.1.1), you can use the `cv2.drawKeypoints` function directly:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个问题。在撰写这本书的时候，OpenCV 4.0版本还没有`cv2.drawKeypoints`函数的解决版本。因此，我编写了一个类似的功能，我们可以用它来绘制关键点。您不需要担心函数中涉及的步骤——它只是为了您的参考。如果您已经安装了本书中指定的OpenCV版本（OpenCV
    4.1.0或OpenCV 4.1.1），您可以直接使用`cv2.drawKeypoints`函数：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s now use this function to draw the detected keypoints:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用这个函数来绘制检测到的关键点：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And we get the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/ebc26310-d118-45b6-bb17-4c12686d3188.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebc26310-d118-45b6-bb17-4c12686d3188.png)'
- en: It's pretty awesome, right?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很棒，对吧？
- en: As easy and fast as BRIEF is, it doesn't work well with the rotation of an image.
    You can try it out by rotating the image (more information at [https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/](https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/)) and
    then running BRIEF. Let's see how ORB helps us to resolve this.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: BRIEF既简单又快速，但它不适用于图像的旋转。您可以尝试旋转图像（更多信息请参阅[https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/](https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/)），然后运行BRIEF。让我们看看ORB如何帮助我们解决这个问题。
- en: Using Oriented FAST and Rotated BRIEF (ORB)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Oriented FAST and Rotated BRIEF (ORB)
- en: 'Personally speaking, I am a huge fan of ORB. It''s free and a good alternative
    to SIFT and SURF, which are both protected by patent laws. ORB actually works
    better than SURF. It''s also interesting to note that Gary Bradski was one of
    the authors of the paper entitled *ORB: An Efficient Alternative to SIFT and SURF*.
    Can you figure out why that''s interesting? Google Gary Bradski and OpenCV and
    you will get your answer.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 说到个人观点，我非常喜爱ORB。它是免费的，是SIFT和SURF的良好替代品，这两者都受到专利法的保护。ORB实际上比SURF表现得更好。值得注意的是，Gary
    Bradski是论文《ORB：SIFT和SURF的有效替代品》的作者之一。你能想出这有什么有趣的地方吗？搜索Gary Bradski和OpenCV，你将找到答案。
- en: 'The entire process will more or less stay the same, so let''s quickly go through
    the code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程大致相同，所以让我们快速浏览一下代码：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went deep down the rabbit hole and looked at several common
    feature engineering techniques, focusing on both feature selection and feature
    extraction. We successfully formatted, cleaned, and transformed data so that it
    could be understood by common machine learning algorithms. We learned about the
    curse of dimensionality and dabbled a bit in dimensionality reduction by implementing
    PCA in OpenCV. Finally, we took a short tour of common feature extraction techniques
    that OpenCV provides for image data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了机器学习中的特征工程技术，重点关注特征选择和特征提取。我们成功地对数据进行格式化、清理和转换，使其能够被常见的机器学习算法理解。我们了解了维度诅咒，并在OpenCV中实现了PCA进行了一些维度约简的尝试。最后，我们简要游览了OpenCV为图像数据提供的常见特征提取技术。
- en: With these skills under our belt, we are now ready to take on any data, be it
    numerical, categorical, text, or image data. We know exactly what to do when we
    encounter missing data, and we know how to transfer our data to make it fit our
    preferred machine learning algorithm.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些技能在手，我们现在可以应对任何类型的数据，无论是数值型、分类型、文本型还是图像型数据。当我们遇到缺失数据时，我们清楚知道该怎么做，也知道如何转换我们的数据以适应我们偏好的机器学习算法。
- en: In the next chapter, we will take the next step and talk about a specific use
    case, which is how to use our newly acquired knowledge to make medical diagnoses
    using decision trees.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将迈出下一步，讨论一个具体的用例，即如何使用我们新获得的知识通过决策树进行医学诊断。
