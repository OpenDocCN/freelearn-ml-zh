- en: Representing Data and Engineering Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we built our very first supervised learning models and
    applied them to some classic datasets, such as the **Iris** and the **Boston** datasets.
    However, in the real world, data rarely comes in a neat `<n_samples x n_features>` **feature
    matrix** that is part of a pre-packaged database. Instead, it is our responsibility
    to find a way to represent the data in a meaningful way. The process of finding
    the best way to represent our data is known as **feature engineering**, and it
    is one of the main tasks of data scientists and machine learning practitioners
    trying to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: I know you would rather jump right to the end and build the deepest neural network
    mankind has ever seen. ...
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any operating system—macOS, Windows, and Linux-based OSes, along
    with this book. We recommend you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Believe it or not, how well a machine learning system can learn is mainly determined
    by the quality of the training data. Although every learning algorithm has its
    strengths and weaknesses, differences in performance often come down to the way
    the data is prepared or represented. Feature engineering can hence be understood
    as a tool for **data representation**. Machine learning algorithms try to learn
    a solution to a problem from sample data, and feature engineering asks: what is
    the best representation of the sample data to use for learning a solution to the
    problem?'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, a couple of chapters ago, we talked about the whole machine learning
    pipeline. There, we already mentioned feature extraction, but we ...
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The more disciplined we are in handling our data, the better results we are
    likely to achieve in the end. The first step in this procedure is known as **data
    preprocessing**, and it comes in (at least) three different flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data formatting**: The data may not be in a format that is suitable for us
    to work with; for example, the data might be provided in a proprietary file format,
    which our favorite machine learning algorithm does not understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleaning**: The data may contain invalid or missing entries, which need
    to be cleaned up or removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sampling**: The data may be far too large for our specific purpose,
    forcing us to sample the data intelligently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the data has been preprocessed, we are ready for the actual feature engineering:
    to transform the preprocessed data to fit our specific machine learning algorithm.
    This step usually involves one or more of three possible procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling**: Certain machine learning algorithms often require the data to
    be within a common range, such as to have zero mean and unit variance. Scaling
    is the process of bringing all features (which might have different physical units)
    into a common range of values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decomposition**: Datasets often have many more features than we could possibly
    process. Feature decomposition is the process of compressing data into a smaller
    number of highly informative data components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: Sometimes, it is possible to group multiple features into
    a single, more meaningful one. For example, a database might contain the date
    and time for each user who logged into a web-based system. Depending on the task,
    this data might be better represented by simply counting the number of logins
    per user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at some of these processes in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Standardization** refers to the process of scaling the data to have zero
    mean and unit variance. This is a common requirement for a wide range of machine learning algorithms,
    which might behave badly if individual features do not fulfill this requirement.
    We could manually standardize our data by subtracting from every data point the
    mean value (*μ*) of all of the data and dividing that by the variance (*σ*) of
    the data; that is, for every feature *x*, we would compute *(x - μ) / σ*.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, scikit-learn offers a straightforward implementation of this
    process in its `preprocessing` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a 3 x 3 data matrix, `X`, standing for three data points (rows)
    with three arbitrarily chosen feature values each (columns): ...'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to standardization, normalization is the process of scaling individual samples to
    have a unit norm. I''m sure you know that the norm stands for the length of a
    vector and can be defined in different ways. We discussed two of them in the previous
    chapter: the L1 norm (or Manhattan distance), and the L2 norm (or Euclidean distance).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, our data matrix, `X`, can be normalized using the `normalize` function,
    and the `l1` norm is specified by the `norm` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the L2 norm can be computed by specifying `norm=''l2''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Scaling features to a range
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An alternative to scaling features to zero mean and unit variance is to get features to
    lie between a given minimum and maximum value. Often, these values are zero and
    one, so that the maximum absolute value of each feature is scaled to unit size. In
    scikit-learn, this can be achieved using `MinMaxScaler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the data will be scaled to fall within 0 and 1\. We can specify
    different ranges by passing a keyword argument, `feature_range`, to the `MinMaxScaler` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Binarizing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we might find ourselves not caring too much about the exact feature values of
    the data. Instead, we might just want to know whether a feature is present or
    absent. Binarizing the data can be achieved by thresholding the feature values.
    Let''s quickly remind ourselves of our feature matrix, `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that these numbers represent the thousands of dollars in our
    bank accounts. If there are more than 0.5 thousand dollars in the account, we
    consider the person rich, which we represent with a 1\. Otherwise, we put a 0\.
    This is akin to thresholding the data with `threshold=0.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The result is a matrix made up entirely of ones and zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Handling the missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common requirement in feature engineering is the handling of missing
    data. For example, we might have a dataset that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Most machine learning algorithms cannot handle the **Not a Number** (**NAN**)
    values (`nan` in Python). Instead, we first have to replace all of the `nan` values
    with some appropriate fill values. This is known as the **imputation** of missing
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three different strategies to impute missing values are offered by scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mean`: Replaces all of the `nan` values with a mean value along a specified
    axis of the matrix (default: *axis = 0*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median`: Replaces all ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Datasets often have many more features than we could possibly process. For example,
    let's say our job was to predict a country's poverty rate. We would probably start
    by matching a country's name with its poverty rate, but that would not help us
    to predict the poverty rate of a new country. So, we start thinking about the
    possible causes of poverty. But how many possible causes of poverty are there? Factors
    might include a country's economy, lack of education, high divorce rate, overpopulation,
    and so on. If each one of these causes was a feature used to help to predict the
    poverty rate, we would end up with a countless number of features. If you're a
    mathematician, you might think of these features as axes in a high-dimensional
    space, and every country's poverty rate is then a single point in this high-dimensional
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re not a mathematician, it might help to start small. Let''s say, we
    first look at only two features: a country''s **Gross Domestic Product** (**GDP**)
    and the number of citizens. We interpret the GDP as the *x axis*, and the number
    of citizens as the *y axis*, in a 2D space. Then, we look at the first country.
    It has a small GDP and an average number of citizens. We draw a point in the *x-y* plane
    that represents this country. We add a second, third, and fourth country. The
    fourth country just happens to have both a high GDP and a large number of citizens.
    Hence, our four data points might be spread across the *x-y* plane much like in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecfc1cd0-8e6f-4658-8f61-ac5429753231.png)'
  prefs: []
  type: TYPE_IMG
- en: However, what happens if we start adding a third feature, such as the country's
    divorce rate, to our analysis? This would add a third axis to our plot (*z axis*).
    Suddenly, we find that the data no longer spreads very nicely across the *x-y-z* cube,
    as most of the cube remains empty. While in two dimensions, it seemed like we
    had most of the *x-y* square covered, in three dimensions, we would need many
    more data points to fill the void between the data points 1 to 3 and the lonely
    data point 4 in the upper-right corner.
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem is also known as the **curse of dimensionality**: the number of
    data points needed to fill the available space grows exponentially with the number
    of dimensions (or plot axes). If a classifier is not fed with data points that
    span the entire feature space (such as shown in the preceding cube example), the
    classifier will not know what to do once a new data point is presented that lies
    far away from all of the previously encountered data points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The curse of dimensionality means that, after a certain number of features
    (or dimensions), the performance of the classifier will start degrading. Let''s
    try to understand this. More features essentially mean that more variations in
    the dataset can be accounted for. But taking into account more than the required
    features will cause the classifier to even take into account any outliers or to
    overfit the dataset. Hence, the performance of the classifier will start degrading
    rather than improving:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1c75237-aa2d-4f6f-b2d4-f91c27700030.png)'
  prefs: []
  type: TYPE_IMG
- en: But, how do we find this seemingly optimal number of dimensions for our dataset?
  prefs: []
  type: TYPE_NORMAL
- en: This is where dimensionality reduction comes in to play. These are a family
    of techniques that allow us to find a compact representation of our high-dimensional
    data, without losing too much information.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Principal Component Analysis (PCA) in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common dimensionality reduction techniques is called **PCA**.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the 2D and 3D examples shown earlier, we can think of an image as
    a point in a high-dimensional space. If we flatten a 2D grayscale image of height *m* and
    width *n* by stacking all of the columns, we get a (feature) vector of length *m
    x n x 1*. The value of the *i*^(th) element in this vector is the grayscale value
    of the *i*^(th) pixel in the image. Now, imagine we would like to represent every
    possible 2D grayscale image with these exact dimensions. How many images would
    that give?
  prefs: []
  type: TYPE_NORMAL
- en: Since grayscale pixels usually take values between 0 and 255, there are a total
    of 256 raised to the power of *m x* *n* images. Chances ...
  prefs: []
  type: TYPE_NORMAL
- en: Implementing independent component analysis (ICA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other useful dimensionality reduction techniques that are closely related to
    PCA are provided by scikit-learn, but not OpenCV. We mention them here for the
    sake of completeness. ICA performs the same mathematical steps as PCA, but it
    chooses the components of the decomposition to be as independent as possible from
    each other, rather than per predictor as in PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, ICA is available from the `decomposition` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Why do we use `tol=0.005`? Because we want the FastICA to converge to some particular
    value. There are two methods to do that—increase the number of iterations (the
    default value is `200`) or decrease the tolerance (the default value is `0.0001`).
    I tried to increase the iterations but, unfortunately, it didn't work, so I went
    ahead with the other option. Can you figure out why it didn't converge?
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen before, the data transformation happens in the `fit_transform` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In our case, plotting the rotated data leads to a similar result to that achieved
    with PCA earlier, as can be verified in the diagram that follows this code block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1113f231-b274-4a4e-8f27-c47cf82851e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing non-negative matrix factorization (NMF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another useful dimensionality reduction technique is called **NMF**. It again
    implements the same basic mathematical operations as PCA and ICA, but it has the additional constraint
    that it **only operates on non-negative data**. In other words, we cannot have
    negative values in our feature matrix if we want to use NMF; the resulting components
    of the decomposition will all have non-negative values as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, NMF works exactly like ICA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing the dimensionality reduction using t-Distributed Stochastic Neighbor
    Embedding (t-SNE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: t-SNE is a technique for dimensionality reduction that is best suited to the
    visualization of high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will see an example of how to visualize high-dimensional
    datasets using t-SNE. Let''s use the digits dataset in this case, which has handwritten
    images of digits from 0 to 9\. It''s a publicly available dataset, commonly referred
    to as the MNIST dataset. We will see how we can visualize the dimensionality reduction
    on this dataset using t-SNE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You should first apply a dimensional reduction technique such as PCA to reduce
    the high number of dimensions to a lower number and then use a technique such
    as t-SNE to visualize the data. But, in this case, let''s use all of the dimensions
    and use t-SNE directly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s visualize the two dimensions that we have extracted using t-SNE
    with the help of a scatterplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61cf6482-31a5-407e-a21e-f5a514efb2ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's discuss how we can represent categorical variables in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Representing categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common data types we might encounter while building a machine learning system
    is **categorical features** (also known as **discrete features**), such as the color of
    a fruit or the name of a company. The challenge with categorical features is that
    they don't change in a continuous way, which makes it hard to represent them with
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a banana is either green or yellow, but not both. A product belongs
    either in the clothing department or in the books department, but rarely in both,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How would you go about representing such features?
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s assume we are trying to encode a dataset consisting of
    a list of forefathers of machine learning and artificial intelligence: ...'
  prefs: []
  type: TYPE_NORMAL
- en: Representing text features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to categorical features, scikit-learn offers an easy way to encode another
    common feature type—text features. When working with text features, it is often
    convenient to encode individual words or phrases as numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a dataset that contains a small corpus of text phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the simplest methods of encoding such data is by word count; for each
    phrase, we simply count the occurrences of each word within it. In scikit-learn,
    this is easily done using `CountVectorizer`, which functions akin to `DictVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, this will store our feature matrix, `X`, as a sparse matrix. If
    we want to manually inspect it, we need to convert it into a regular array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand what these numbers mean, we have to look at the feature names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, it becomes clear what the integers in `X` mean. If we look at the phrase
    that is represented in the top row of `X`, we see that it contains one occurrence
    of the word, `engineering`, and one occurrence of the word, `feature`. On the
    other hand, it does not contain the words `extraction` or `selection`. Does this
    make sense? A quick glance at our original data `sample` reveals that the phrase
    was indeed `feature engineering`.
  prefs: []
  type: TYPE_NORMAL
- en: Looking only at the `X` array (no cheating!), can you guess what the last phrase
    in `sample` was?
  prefs: []
  type: TYPE_NORMAL
- en: One possible shortcoming of this approach is that we might put too much weight
    on words that appear very frequently. One approach to fixing this is known as **Term
    Frequency-Inverse Document Frequency** (**TF-IDF**). What TF-IDF does might be
    easier to understand than its name, which is basically to weigh the word counts
    by a measure of how often they appear in the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax for TF-IDF is pretty much similar to the previous command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We note that the numbers are now smaller than before, with the third column
    taking the biggest hit. This makes sense, as the third column corresponds to the
    most frequent word across all three phrases, `feature`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re interested in the math behind TF-IDF, you can start with this paper:
    [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf).
    For more information about its specific implementation in scikit-learn, have a
    look at the API documentation at [http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)[.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Representing text features will become important in [Chapter 7](08148129-87ac-4042-944d-8e0a2bbbe0c5.xhtml), *Implementing
    a Spam Filter with Bayesian Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Representing images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common and important data types for computer vision is, of course,
    images. The most straightforward way to represent images is probably by using
    the grayscale value of each pixel in the image. Usually, grayscale values are
    not very indicative of the data they describe. For example, if we saw a single
    pixel with a grayscale value of 128, could we tell what object this pixel belonged
    to? Probably not. Therefore, grayscale values are not very effective image features.
  prefs: []
  type: TYPE_NORMAL
- en: Using color spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alternatively, we might find that colors contain some information that raw grayscale
    values cannot capture. Most often, images come in the conventional RGB color space,
    where every pixel in the image gets an intensity value for its apparent **Redness** (**R**), **Greenness** (**G**),
    and **Blueness** (**B**). However, OpenCV offers a whole range of other color
    spaces, such as **Hue Saturation Value** (**HSV**), **Hue Saturation Lightness** (**HSL**),
    and the Lab color space. Let's have a quick look at them.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding images in the RGB space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am sure that you are already familiar with the RGB color space, which uses additive
    mixing of different shades of red, green, and blue to produce different composite
    colors. The RGB color space is useful in everyday life because it covers a large
    part of the color space that the human eye can see. This is why color television
    sets or color computer monitors only need to care about producing mixtures of
    red, green, and blue light.
  prefs: []
  type: TYPE_NORMAL
- en: In OpenCV, RGB images are supported straight out of the box. All you need to
    know, or need to be reminded of, is that color images are actually stored as BGR
    images in OpenCV; that is, the order of color channels is blue-green-red instead
    of red-green-blue. The reasons for this ...
  prefs: []
  type: TYPE_NORMAL
- en: Encoding images in the HSV and HLS space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, ever since the RGB color space was created, people have realized that
    it is actually quite a poor representation of human vision. Therefore, researchers
    have developed many alternative representations. One of them is called **HSV** (short
    for **Hue, Saturation, and Value**) and the other one is called **HLS** (**Hue,
    Lightness, and Saturation**). You might have seen these color spaces in color
    pickers and common image editing software. In these color spaces, the hue of the
    color is captured by a single hue channel, the colorfulness is captured by a saturation
    channel, and the lightness or brightness is captured by a lightness or value channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In OpenCV, an RGB image can easily be converted into the HSV color space using `cv2.cvtColor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The same is true for the HLS color space. In fact, OpenCV provides a whole
    range of additional color spaces, which are available via `cv2.cvtColor`. All
    we need to do is to replace the color flag with one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: HLS using `cv2.COLOR_BGR2HLS`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LAB (lightness, green-red, and blue-yellow) using `cv2.COLOR_BGR2LAB`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YUV (overall luminance, blue luminance, and red luminance) using `cv2.COLOR_BGR2YUV`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting corners in images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most straightforward traditional features to find in an image are
    probably corners (locations where several edges meet). OpenCV provides at least
    two different algorithms to find corners in an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Harris Dorner Detection**: Knowing that edges are areas with high-intensity
    changes in all directions, Harris and Stephens came up with a fast way of finding
    such locations. This algorithm is implemented as `cv2.cornerHarris` in OpenCV.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shi-Tomasi Corner Detection**: Shi and Tomasi have their own idea of what
    constitute good features to track, and they usually do better than Harris corner
    detection by finding the *N* strongest corners. This algorithm is implemented
    as `cv2.goodFeaturesToTrack` in OpenCV.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harris ...
  prefs: []
  type: TYPE_NORMAL
- en: Using the star detector and BRIEF descriptor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, corner detection is not sufficient when the scale of an image changes.
    Multiple papers have been published describing different algorithms for feature
    detection and description. We will look at a combination of the **Speeded Up Robust
    Features** (**SURF**) detector (for more information, see [https://en.wikipedia.org/wiki/Speeded_up_robust_features](https://en.wikipedia.org/wiki/Speeded_up_robust_features)) and
    the **Binary Robust Independent Elementary Features** (**BRIEF**) descriptor.
    The feature detector identifies the keypoints in the image and the feature descriptor
    calculates the actual feature value for all of the keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: The details of these algorithms are beyond the scope of this book. Advanced
    users can refer to the papers describing these algorithms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details, you can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For SURF: [https://www.vision.ee.ethz.ch/~surf/eccv06.pdf](https://www.vision.ee.ethz.ch/~surf/eccv06.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For BRIEF: [https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf](https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The entire process starts with reading the image, converting it into grayscale,
    using the star feature detector to find the interesting points, and finally, using
    the BRIEF descriptor to calculate the feature value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first read the image and convert it to grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create the feature detector and descriptor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, it''s time to use the star detector to get the keypoints and pass them
    to the BRIEF descriptor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s a catch here. At the time of writing this book, the OpenCV version
    4.0 didn''t have the resolved version of the `cv2.drawKeypoints` function. So,
    I have written a similar function that we can use to draw the keypoints. You don''t
    need to worry about the steps involved in the function—it''s just for your reference.
    If you have installed the OpenCV version specified in this book (OpenCV 4.1.0
    or OpenCV 4.1.1), you can use the `cv2.drawKeypoints` function directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now use this function to draw the detected keypoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebc26310-d118-45b6-bb17-4c12686d3188.png)'
  prefs: []
  type: TYPE_IMG
- en: It's pretty awesome, right?
  prefs: []
  type: TYPE_NORMAL
- en: As easy and fast as BRIEF is, it doesn't work well with the rotation of an image.
    You can try it out by rotating the image (more information at [https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/](https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/)) and
    then running BRIEF. Let's see how ORB helps us to resolve this.
  prefs: []
  type: TYPE_NORMAL
- en: Using Oriented FAST and Rotated BRIEF (ORB)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Personally speaking, I am a huge fan of ORB. It''s free and a good alternative
    to SIFT and SURF, which are both protected by patent laws. ORB actually works
    better than SURF. It''s also interesting to note that Gary Bradski was one of
    the authors of the paper entitled *ORB: An Efficient Alternative to SIFT and SURF*.
    Can you figure out why that''s interesting? Google Gary Bradski and OpenCV and
    you will get your answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire process will more or less stay the same, so let''s quickly go through
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went deep down the rabbit hole and looked at several common
    feature engineering techniques, focusing on both feature selection and feature
    extraction. We successfully formatted, cleaned, and transformed data so that it
    could be understood by common machine learning algorithms. We learned about the
    curse of dimensionality and dabbled a bit in dimensionality reduction by implementing
    PCA in OpenCV. Finally, we took a short tour of common feature extraction techniques
    that OpenCV provides for image data.
  prefs: []
  type: TYPE_NORMAL
- en: With these skills under our belt, we are now ready to take on any data, be it
    numerical, categorical, text, or image data. We know exactly what to do when we
    encounter missing data, and we know how to transfer our data to make it fit our
    preferred machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take the next step and talk about a specific use
    case, which is how to use our newly acquired knowledge to make medical diagnoses
    using decision trees.
  prefs: []
  type: TYPE_NORMAL
