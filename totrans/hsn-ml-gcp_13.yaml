- en: Beyond Feedforward Networks – CNN and RNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越前馈网络——CNN和RNN
- en: '**Artificial Neural Networks** (**ANNs**) are now extremely widespread tools
    in various technologies. In the simplest application, ANNs provide a feedforward
    architecture for connections between neurons. The feedforward neural network is
    the first and simplest type of ANN devised. In the presence of basic hypotheses
    that interact with some problems, the intrinsic unidirectional structure of feedforward
    networks is strongly limiting. However, it is possible to start from it and create
    networks in which the results of computing one unit affect the computational process
    of another. It is evident that algorithms that manage the dynamics of these networks
    must meet new convergence criteria.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**）现在在各种技术中极为广泛地被用作工具。在最简单的应用中，ANNs为神经元之间的连接提供了一种前馈架构。前馈神经网络是设计出的第一种也是最简单的人工神经网络。在存在与某些问题相互作用的假设的情况下，前馈网络的内向结构具有强烈的限制性。然而，可以从它开始，创建一个计算一个单元的结果会影响另一个单元计算过程的网络。显然，管理这些网络动态的算法必须满足新的收敛标准。'
- en: In this chapter, we'll go over the main ANN architectures, such as convolutional
    NNs, recurrent NNs, and **long short-term memory** (**LSTM**). We'll explain the
    concepts behind each type of NN and tell you which problem they should be applied
    to. Each type of NN is implemented with TensorFlow on a realistic dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍主要的人工神经网络架构，例如卷积神经网络（CNN）、循环神经网络（RNN）和**长短期记忆**（**LSTM**）。我们将解释每种类型NN背后的概念，并告诉您它们应该应用于哪些问题。每种类型的NN都使用TensorFlow在真实数据集上实现。
- en: 'The topics covered are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖的主题包括：
- en: Convolutional networks and their applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络及其应用
- en: Recurrent networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环网络
- en: LSTM architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM架构
- en: At the end of the chapter, we will understand training, testing, and evaluating
    a **convolutional neural network** (**CNN**). We will learn how to train and test
    the CNN model in Google Cloud Platform. We will cover the concepts as CNN and
    RNN architecture. We will also be able to train an LSTM model. The reader will
    learn which type of neural network to apply to different problems and how to define
    and implement them on Google Cloud Platform.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将了解训练、测试和评估**卷积神经网络**（**CNN**）。我们将学习如何在Google Cloud Platform上训练和测试CNN模型。我们将涵盖CNN和RNN架构的概念。我们还将能够训练一个LSTM模型。读者将学习将哪种类型的神经网络应用于不同的问题，以及如何在Google
    Cloud Platform上定义和实现它们。
- en: Convolutional neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: ANN is a family of models inspired from biological neural networks (the human
    brain) that, starting from the mechanisms regulating natural neural networks,
    plan to simulate human thinking. They are used to estimate or approximate functions
    that may depend on a large number of inputs, many of which are often unknown.
    ANNs are generally presented as interconnected neuron systems among which an exchange
    of messages takes place. Each connection has a related weight; the value of the
    weight is adjustable based on experience, and this makes neural networks an instrument
    adaptable to the various types of input and having the ability to learn.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ANN是从生物神经网络（人脑）中受到启发的模型系列，它从调节自然神经网络的机制开始，计划模拟人类思维。它们用于估计或近似可能依赖于大量输入的函数，其中许多通常是未知的。ANN通常表示为相互连接的神经元系统，其中发生消息交换。每个连接都有一个相关的权重；权重的值可以根据经验进行调整，这使得神经网络成为适应各种类型输入并具有学习能力的工具。
- en: 'ANNs define the neuron as a central processing unit, which performs a mathematical
    operation to generate one output from a set of inputs. The output of a neuron
    is a function of the weighted sum of the inputs plus the bias. Each neuron performs
    a very simple operation that involves activation if the total amount of signal
    received exceeds an activation threshold. In the following figure, a simple ANN
    architecture is shown:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs将神经元定义为中央处理单元，它通过对一组输入执行数学运算来生成一个输出。神经元的输出是输入加权总和加上偏差的函数。每个神经元执行一个非常简单的操作，如果接收到的总信号量超过激活阈值，则涉及激活。在以下图中，显示了一个简单的ANN架构：
- en: '![](img/c87c20a5-de1e-4326-a726-6876f608cc91.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c87c20a5-de1e-4326-a726-6876f608cc91.png)'
- en: Essentially, CNN are ANNs. In fact, just like the latter, CNNs are made up of
    neurons connected to one another by weighted branches (weight); the training parameters
    of the nets are once again the weight and the bias.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，卷积神经网络（CNN）是人工神经网络。事实上，就像后者一样，CNN由通过加权分支（权重）相互连接的神经元组成；网络的训练参数再次是权重和偏差。
- en: In CNN, the connection pattern between neurons is inspired by the structure
    of the visual cortex in the animal world. The individual neurons present in this
    part of the brain (visual cortex) respond to certain stimuli in a narrow region
    of the observation, called the **receptive field**. The receptive fields of different
    neurons are partially overlapped in order to cover the entire field of vision.
    The response of a single neuron to stimuli taking place in its receptive field
    can be mathematically approximated by a convolution operation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，神经元之间的连接模式受到动物世界中视觉皮层结构的启发。大脑的这一部分（视觉皮层）中的单个神经元对观察到的狭窄区域内的某些刺激做出反应，该区域称为**感受野**。不同神经元的感受野部分重叠，以覆盖整个视野。单个神经元对其感受野内发生的刺激的反应可以通过卷积运算进行数学近似。
- en: Everything related to the training of a neural network, that is, forward/backward
    propagation and updating of the weight, also applies in this context; moreover,
    a whole CNN always uses a single function of differentiable cost. However, CNNs
    make a specific assumption that their input has a precise data structure, such
    as an image, and this allows them to take specific properties in their architectureto
    better process such data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络训练相关的所有内容，即前向/反向传播和权重的更新，也适用于此上下文；此外，整个CNN总是使用一个可微分的成本函数。然而，CNN假设它们的输入具有精确的数据结构，例如图像，这使得它们可以在其架构中采用特定的属性以更好地处理此类数据。
- en: The normal neural networks stratified with an FC architecture—where every neuron
    of each layer is connected to all the neurons of the previous layer (excluding
    bias neurons)—in general do not scale well with an increase in the size of input
    data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FC架构分层的一般神经网络——其中每一层的每个神经元都与前一层的所有神经元（除偏置神经元外）相连——在输入数据规模增加时通常无法很好地扩展。
- en: 'Let''s take a practical example: suppose we want to analyze an image to detect
    objects. To start, let''s see how the image is processed. As we know, in the coding
    of an image, it is divided into a grid of small squares, each of which represents
    a pixel. At this point, to encode the color images, it will be enough to identify
    for each square a certain number of shades and different color gradations. And
    then we code each one by means of an appropriate sequence of bits. Here is a simple
    image encoding:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个实际例子：假设我们想要分析一张图像以检测对象。首先，让我们看看图像是如何处理的。正如我们所知，在图像编码中，它被分成一个小方格的网格，每个小方格代表一个像素。在这个阶段，为了编码彩色图像，我们只需要为每个方格识别一定数量的色调和不同的颜色渐变。然后我们通过适当的位序列对每个方格进行编码。以下是一个简单的图像编码示例：
- en: '![](img/3d7e6060-4f75-44d2-b22a-3cb150720eeb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d7e6060-4f75-44d2-b22a-3cb150720eeb.png)'
- en: The number of squares in the grid defines the resolution of the image. For example,
    an image that is 1,600 pixels wide and 800 pixels high (1,600 x 800) contains
    (multiplied) 1,280,000 pixels, or 1.2 megapixels. To this, we must multiply the
    three color channels, finally obtaining 1,600 x 800 x 3 = 3,840,000\. So, each
    neuron completely connected in the first hidden layer would have 3,840,000 weights.
    This is only for a single neuron; considering the whole network, the thing would
    certainly become unmanageable!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网格中的方块数量定义了图像的分辨率。例如，宽度为1,600像素、高度为800像素（1,600 x 800）的图像包含（相乘）1,280,000个像素，或1.2兆像素。因此，我们必须乘以三个颜色通道，最终得到1,600
    x 800 x 3 = 3,840,000。这意味着第一个隐藏层中完全连接的每个神经元都会有3,840,000个权重。这只是一个神经元的例子；考虑到整个网络，事情肯定会变得难以管理！
- en: CNNs are designed to recognize visual patterns directly in images represented
    by pixels and require zero or very limited preprocessing. They are able to recognize
    extremely variable patterns, such as freehand writing and images representing
    the real world.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CNN被设计成直接在由像素表示的图像中识别视觉模式，并且需要零或非常有限的预处理。它们能够识别极其变化的模式，例如自由手写和代表现实世界的图像。
- en: 'Typically, a CNN consists of several alternate convolution and subsampling
    levels (pooling) followed by one or more FC final levels in the case of classification.
    The following figure shows a classic image-processing pipeline:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN由几个交替的卷积和子采样级别（池化）组成，在分类的情况下，后面跟着一个或多个FC最终级别。以下图显示了经典的图像处理流程：
- en: '![](img/cfbfe186-3a40-4f4a-a38c-fdecbf683450.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cfbfe186-3a40-4f4a-a38c-fdecbf683450.png)'
- en: To solve problems in the real world, these steps can be combined and stacked
    as often as necessary. For example, you can have two, three, or even more layers
    of **Convolution**. You can enter all the **Pooling** you want to reduce the size
    of the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决现实世界中的问题，这些步骤可以按需组合和堆叠。例如，你可以有两层、三层，甚至更多层的**卷积**。你可以输入所有你想要的**池化**来减少数据的大小。
- en: As already mentioned, different types of levels are typically used in a CNN.
    In the following sections, the main ones will be covered.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在CNN中通常使用不同类型的层。在以下章节中，将介绍主要的一些。
- en: Convolution layer
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: This is the main type of layer; the use of one or more of these layers in a
    CNN is essential. The parameters of a convolutional layer, in practice, relate
    to a set of workable filters. Each filter is spatially small, along the width
    and height dimensions, but it extends over the entire depth of the input volume
    to which it is applied.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是主要类型的层；在CNN中使用一个或多个这些层是必不可少的。实际上，卷积层的参数与一组可操作的滤波器相关。每个滤波器在宽度和高度维度上是空间上小的，但它扩展到它所应用的输入体积的整个深度。
- en: 'Unlike normal neural networks, convolutional layers have neurons organized
    in three dimensions: **width**, **height**, and **depth**. They are shown in the
    following figure:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通神经网络不同，卷积层中的神经元以三维组织：**宽度**、**高度**和**深度**。它们在以下图中展示：
- en: '![](img/98a4cb93-dbff-4d8e-867d-ba939b15ed8e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/98a4cb93-dbff-4d8e-867d-ba939b15ed8e.png)'
- en: During forward propagation, each filter is translated—or more precisely, convolved—along
    the width and height of the input volume, producing a two-dimensional activation
    map (or feature map) for that filter. As the filter is moved along the input area,
    a scalar product operation is performed between the values ​​of the filter and
    those of the input portion to which it is applied.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中，每个滤波器沿着输入体积的宽度和高度进行平移——或者更准确地说，进行卷积——产生该滤波器的二维激活图（或特征图）。当滤波器在输入区域移动时，滤波器的值与它所应用的输入部分的值之间执行标量积操作。
- en: Intuitively, the network will have as its objective the learning of filters
    that are activated in the presence of some specific type of feature in a given
    spatial region of the input. The queuing of all these feature maps (for all filters)
    along the depth dimension forms the output volume of a convolutional layer. Each
    element of this volume can be interpreted as the output of a neuron that observes
    only a small region of the input and which shares its parameters with the other
    neurons in the same feature map. This is because these values ​​all come from
    the application of the same filter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地讲，网络的目标是学习在输入的特定空间区域出现某种特定类型的特征时被激活的滤波器。所有这些特征图（对于所有滤波器）沿着深度维度的排队形成了卷积层的输出体积。这个体积的每个元素都可以解释为观察输入的小区域并与其他特征图中的神经元共享其参数的神经元的输出。这是因为这些值都来自应用相同的滤波器。
- en: 'In summary, let''s focus our attention on the following points:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，让我们关注以下要点：
- en: '**Local receptive field**: Each neuron of a layer is (completely) connected
    to a small region of the input (called a **local receptive field**); each connection
    learns a weight.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部感受野**：层的每个神经元都与输入的一个小区域（称为**局部感受野**）完全连接；每个连接学习一个权重。'
- en: '**Shared weights**: Since the interesting features (edge, blob, and so on)
    can be found anywhere in the image, the neurons of the same layer share the weights.
    This means that all the neurons of the same layer will recognize the same feature,
    placed at different points of the input.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享权重**：由于有趣的特征（边缘、块等）可以在图像的任何地方找到，同一层的神经元共享权重。这意味着同一层的所有神经元都将识别相同的位置在不同输入点处的特征。'
- en: '**Convolution**: The same weight map is applied to different positions. The
    convolution output is called a **feature map**.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积**：相同的权重图应用于不同的位置。卷积输出称为**特征图**。'
- en: Each filter captures a feature present in the previous layer. So to extract
    different features, we need to train multiple convolutional filters. Each filter
    returns a feature map that highlights different characteristics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个滤波器捕捉前一层中存在的特征。因此，为了提取不同的特征，我们需要训练多个卷积滤波器。每个滤波器返回一个突出不同特性的特征图。
- en: Rectified Linear Units
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩形线性单元
- en: '**Rectified Linear Units** (**ReLU**) play the role of neuronal activation
    function in neural networks. A ReLU level is composed of neurons that apply the
    function f*(x) = max (0, x)*. These levels increase the non-linearity of the network
    and at the same time do not modify the receiving fields of convolution levels.
    The function of the ReLUs is preferred over others, such as the hyperbolic tangent
    or the sigmoid, since, in comparison to these, it leads to a much faster training
    process without significantly affecting the generalization accuracy.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU（修正线性单元**）在神经网络中扮演神经元激活函数的角色。ReLU层由应用函数 f*(x) = max (0, x)* 的神经元组成。这些层增加了网络的非线性，同时不修改卷积层的接收域。与双曲正切或Sigmoid等其他函数相比，ReLU函数的功能更受青睐，因为它在比较中导致训练过程更快，同时不会显著影响泛化精度。'
- en: Pooling layers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: These layers are periodically inserted into a network to reduce the spatial
    size (width and height) of current representations, as well as volumes in a specific
    network stage; this serves to reduce the number of parameters and the computational
    time of the network. It also monitors overfitting. A pooling layer operates on
    each depth slice of the input volume independently to resize it spatially.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层定期插入到网络中，以减少当前表示的空间尺寸（宽度和高度）以及特定网络阶段中的体积；这有助于减少网络参数数量和计算时间。它还监控过拟合。池化层独立地对输入体积的每个深度切片进行操作，以在空间上调整其大小。
- en: For example, this technique partitions an input image into a set of squares,
    and for each of the resulting regions, it returns the maximum value as output.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这种技术将输入图像分割成一系列正方形，并对每个生成的区域，返回最大值作为输出。
- en: 'CNNs also use pooling layers located immediately after the convolutional layers.
    A pooling layer divides input into regions and selects a single representative
    value (max-pooling and average pooling). Using a pooling layer:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs还使用位于卷积层之后的池化层。池化层将输入划分为区域，并选择单个代表性值（最大池化和平均池化）。使用池化层：
- en: Reduces the calculations of subsequent layers
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了后续层的计算量
- en: Increases the robustness of the features with respect to spatial position
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强了特征对空间位置的抗干扰性
- en: It is based on the concept that, once a certain feature has been identified,
    its precise position in the input is not as important as its approximate position
    in relation to the other features. In the typical CNN architecture, convolution
    levels and pooling levels are repeatedly alternated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于这样的概念，一旦某个特征被识别，其在输入中的精确位置并不像其相对于其他特征的近似位置那样重要。在典型的CNN架构中，卷积层和池化层反复交替。
- en: Fully connected layer
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全连接层
- en: This type of layer is exactly the same as any of the layers of a classical ANN
    with **fully connected** (**FC**) architecture. Simply in an FC layer, each neuron
    is connected to all the neurons of the previous layer, specifically to their activations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的层与具有**完全连接**（**FC**）架构的任何经典ANN层完全相同。简单地说，在FC层中，每个神经元都连接到前一层中的所有神经元，特别是它们的激活。
- en: 'This type of layer, unlike what has been seen so far in CNNs, does not use
    the property of local connectivity. An FC layer is connected to the entire input
    volume, and, therefore, as you can imagine, there will be many connections. The
    only settable parameter of this type of layer is the number of K neurons that
    make it up. What basically defines an FC layer is as follows: connecting its K
    neurons with all the input volume and calculating the activation of each of its
    K neurons.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与CNN中迄今为止所看到的不同，这种类型的层不使用局部连接性属性。FC层连接到整个输入体积，因此，正如你可以想象的那样，会有很多连接。这种类型层的唯一可设置参数是构成它的K个神经元的数量。基本上定义FC层的是以下内容：将其K个神经元与所有输入体积连接，并计算其K个神经元中的每个神经元的激活。
- en: In fact, its output will be a single 1 x 1 x K vector, containing the calculated
    activations. The fact that after using a single FC layer you switch from an input
    volume (organized in three dimensions) to a single output vector (in a single
    dimension) suggests that after applying an FC layer, no more convoluted layers
    can be used. The main function of FC layers in the context of CNNs is to carry
    out a sort of grouping of the information obtained up to that moment, expressing
    it with a single number (the activation of one of its neurons), which will be
    used in subsequent calculations for the final classification.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，其输出将是一个1 x 1 x K的单个向量，包含计算出的激活值。使用单个全连接层后，从三维组织输入体积（在单维中组织单个输出向量）切换，这表明在应用全连接层之后，不能再使用卷积层。在CNN的上下文中，全连接层的主要功能是对到目前为止获得的信息进行某种分组，用一个单一的数字（其神经元之一的活动）来表示，这将在后续的计算中用于最终的分类。
- en: Structure of a CNN
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的结构
- en: 'After analyzing every component of a CNN in detail, it is time to see the general
    structure of a CNN as a whole. For example, starting from the images as input
    layers, there will be a certain series of convolutional layers interspersed with
    a ReLU layer and, when necessary, the standardization and pooling layers. Finally,
    there will be a series of FC layers before the output layer. Here is an example
    of a CNN architecture:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细分析了CNN的每个组件之后，现在是时候看看CNN的整体结构了。例如，从图像作为输入层开始，将会有一系列的卷积层，其中穿插着ReLU层，并在必要时使用标准化和池化层。最后，在输出层之前，将有一系列的全连接层。以下是一个CNN架构的示例：
- en: '![](img/b4fd6b63-f3f3-4956-8b4b-1b5dbc86088d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b4fd6b63-f3f3-4956-8b4b-1b5dbc86088d.png)'
- en: The basic idea is to start with a large image and continuously reduce the data
    step by step until you get a single result. The more the convolution passages
    you have, the more the neural network will be able to understand and process complex
    functions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是从一个大的图像开始，逐步减少数据，直到得到一个单一的结果。卷积通道越多，神经网络就越能够理解和处理复杂函数。
- en: TensorFlow overview
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow概述
- en: TensorFlow is an open source numerical computing library provided by Google
    for machine intelligence. It hides all of the programming required to build deep
    learning models and gives developers a black box interface to program.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是由谷歌提供的开源数值计算库，用于机器智能。它隐藏了构建深度学习模型所需的全部编程，并为开发者提供了一个黑盒接口来编程。
- en: In TensorFlow, nodes in the graph represent mathematical operations, while the
    graph edges represent the multidimensional data arrays (tensors) communicated
    between them. TensorFlow was originally developed by the Google brain team within
    Google's machine intelligence research for machine learning and deep neural networks
    research, but it is now available in the public domain. TensorFlow exploits GPU
    processing when configured appropriately.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，图中的节点代表数学运算，而图边代表它们之间传递的多维数据数组（张量）。TensorFlow最初是由谷歌大脑团队在谷歌机器智能研究内部为机器学习和深度神经网络研究开发的，但现在它已进入公共领域。当配置适当的时候，TensorFlow会利用GPU处理。
- en: 'The generic use cases for TensorFlow are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的通用用例如下：
- en: Image recognition
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别
- en: Computer vision
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Voice/sound recognition
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音/声音识别
- en: Time series analysis
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列分析
- en: Language detection
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言检测
- en: Language translation
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Text-based processing
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于文本的处理
- en: Handwriting Recognition
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写识别
- en: Many others
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多其他
- en: To use TensorFlow, we must first install Python. If you don't have a Python
    installation on your machine, it's time to get it. Python is a dynamic **object-oriented
    programming** (**OOP**) language that can be used for many types of software development.
    It offers strong support for integration with other languages and programs, is
    provided with a large standard library, and can be learned within a few days.
    Many Python programmers can confirm a substantial increase in productivity and
    feel that it encourages the development of higher quality code and maintainability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用TensorFlow，我们首先必须安装Python。如果你机器上没有Python安装，那么是时候安装它了。Python是一种动态的**面向对象编程**（**OOP**）语言，可用于许多类型的软件开发。它提供了与其他语言和程序集成的强大支持，附带一个大型标准库，可以在几天内学会。许多Python程序员可以证实生产力的显著提高，并觉得它鼓励开发更高质量的代码和可维护性。
- en: Python runs on Windows, Linux/Unix, macOS X, OS/2, Amiga, Palm handhelds, and
    Nokia phones. It also works on Java and .NET virtual machines. Python is licensed
    under the OSI-approved open source license; its use is free, including for commercial
    products.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Python运行在Windows、Linux/Unix、macOS X、OS/2、Amiga、Palm手持设备和诺基亚手机上。它还适用于Java和.NET虚拟机。Python遵循OSI批准的开源许可；其使用是免费的，包括商业产品。
- en: Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch
    Centrum in the Netherlands as a successor of a language called **ABC**. Guido
    remains Python's principal author, although it includes many contributions from
    others.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Python是在20世纪90年代初由荷兰Stichting Mathematisch Centrum的Guido van Rossum创建的，作为名为**ABC**的语言的继任者。尽管Python包括许多其他人的贡献，但Guido仍然是Python的主要作者。
- en: If you do not know which version to use, there is an English document that can
    help you choose. In principle, if you have to start from scratch, we recommend
    choosing Python 3.6\. All information about the available versions and how to
    install Python is given at [https://www.python.org/](https://www.python.org/).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道要使用哪个版本，有一份英文文档可以帮助你选择。原则上，如果你必须从头开始，我们建议选择Python 3.6。有关可用版本和如何安装Python的所有信息，请参阅[https://www.python.org/](https://www.python.org/)。
- en: 'After properly installing the Python version of our machine, we have to worry
    about installing TensorFlow. We can retrieve all library information and available
    versions of the operating system from the following link: [https://www.tensorflow.org/](https://www.tensorflow.org/).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确安装我们的机器的Python版本后，我们必须担心安装TensorFlow。我们可以从以下链接检索所有库信息和操作系统的可用版本：[https://www.tensorflow.org/](https://www.tensorflow.org/)。
- en: 'Also, in the install section, we can find a series of guides that explain how
    to install a version of TensorFlow that allows us to write applications in Python.
    Guides are available for the following operating systems:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在安装部分，我们可以找到一系列指南，解释如何安装允许我们用Python编写应用程序的TensorFlow版本。以下操作系统的指南可用：
- en: Ubuntu
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu
- en: macOS X
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: macOS X
- en: Windows
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows
- en: 'For example, to install TensorFlow on Windows, we must choose one of the following
    types:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要在Windows上安装TensorFlow，我们必须选择以下类型之一：
- en: TensorFlow with CPU support only
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅支持CPU的TensorFlow
- en: TensorFlow with GPU support
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持GPU的TensorFlow
- en: 'To install TensorFlow, start a terminal with privileges as administrator. Then
    issue the appropriate `pip3 install` command in that terminal. To install the
    CPU-only version, enter the following command:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装TensorFlow，以管理员权限启动终端。然后在终端中发出适当的`pip3 install`命令。要安装仅CPU版本，请输入以下命令：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A series of code lines will be displayed on the video to keep us informed of
    the execution of the installation procedure, as shown in the following figure:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列代码行将在视频中显示，以使我们了解安装过程的执行情况，如图所示：
- en: '![](img/0bfbadcf-f411-4353-a285-bb857b7d5c74.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0bfbadcf-f411-4353-a285-bb857b7d5c74.png)'
- en: 'At the end of the process, the following code is displayed:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程结束时，将显示以下代码：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To validate the installation, invoke `python` from a shell as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证安装，从shell中调用`python`，如下所示：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Enter the following short program inside the Python interactive shell:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python交互式外壳中输入以下简短程序：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If the system outputs the following, then you are ready to begin writing TensorFlow
    programs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统输出以下内容，那么你就可以开始编写TensorFlow程序了：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, you will have a confirmation of correct installation of the library
    on your computer. Now you just need to use it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你将确认库在你的计算机上正确安装。现在你只需要使用它。
- en: Handwriting Recognition using CNN and TensorFlow
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN和TensorFlow进行手写识别
- en: '**Handwriting Recognition** (**HWR**) is a very commonly used procedure in
    modern technology. An image of written text can be detected offline from a piece
    of paper by optical scanning (**optical character recognition** or **OCR**) or
    intelligent word recognition. Alternatively, pen tip movements can be detected
    online (for example, from a pen computer surface, a task that is generally easier
    since there are more clues available).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**手写识别**（**HWR**）是现代技术中非常常用的程序。通过光学扫描（**光学字符识别**或**OCR**）或智能文字识别，可以从一张纸上离线检测到书写文本的图像。或者，可以在线检测笔尖移动（例如，从笔计算机表面，由于有更多线索，这通常更容易）。'
- en: Technically, recognition of handwriting is the ability of a computer to receive
    and interpret a handwritten intelligible input from sources such as paper documents,
    photos, touchscreens, and other devices. HWR is performed through various techniques
    that generally require OCR. However, a complete script recognition system also
    manages formatting, carries out correct character segmentation, and finds the
    most plausible words.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上来说，手写识别是计算机接收和解释来自纸张文件、照片、触摸屏和其他设备等来源的手写可识别输入的能力。HWR通过需要OCR的各种技术来完成。然而，一个完整的脚本识别系统还管理格式，执行正确的字符分割，并找到最可能的单词。
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) is
    a large database of handwritten digits. It has a set of 70,000 examples of data.
    It is a subset of NIST''s larger dataset. The digits are of 28 x 28 pixel resolution
    and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each
    pixel value from the 28 x 28 matrix and one value is the actual digit. The digits
    have been size-normalized and centered in a fixed-size image.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改后的国家标准与技术研究院**（**MNIST**）是一个包含手写数字的大型数据库。它包含70,000个数据示例。它是NIST更大数据集的一个子集。这些数字的分辨率为28
    x 28像素，存储在一个70,000行和785列的矩阵中；784列形成28 x 28矩阵中的每个像素值，一个值是实际的数字。这些数字已经被尺寸归一化，并居中在一个固定大小的图像中。'
- en: The digit images in the MNIST set were originally selected and experimented
    with by Chris Burges and Corinna Cortes using bounding box normalization and centering.
    Yann LeCun's version uses centering by center of mass within in a larger window.
    The data is available on Yann LeCun's website at
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST集中的数字图像最初是由Chris Burges和Corinna Cortes使用边界框归一化和居中技术选定的。Yann LeCun的版本使用更大窗口内的质心居中。数据可在Yann
    LeCun的网站上找到：
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).'
- en: 'Each image is created as 28 x 28\. The following figure shows a sample of images
    of 0-8 from the MNIST dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都是28 x 28像素创建的。以下图显示了MNIST数据集中0-8的图像样本：
- en: '![](img/4c607089-743f-4f34-930b-5d205e1a866d.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c607089-743f-4f34-930b-5d205e1a866d.png)'
- en: 'MNIST has a sample of several handwritten digits. This dataset can be fed for
    our training to an Python program and our code can recognize any new handwritten
    digit that is presented as data for prediction. This is a case where the neural
    network architecture functions as a computer vision system for an AI application.
    The following table shows the distribution of the MNIST dataset available on LeCun''s
    website:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST包含几个手写数字的样本。这个数据集可以被输入到我们的Python程序中，我们的代码可以识别任何作为预测数据呈现的新手写数字。这是一个神经网络架构作为计算机视觉系统在AI应用中起作用的案例。以下表格显示了LeCun网站上可用的MNIST数据集的分布：
- en: '| **Digit** | **Count** |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **数字** | **数量** |'
- en: '| 0 | 5923 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 5923 |'
- en: '| 1 | 6742 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6742 |'
- en: '| 2 | 5958 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5958 |'
- en: '| 3 | 6131 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6131 |'
- en: '| 4 | 5842 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5842 |'
- en: '| 5 | 5421 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5421 |'
- en: '| 6 | 5918 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5918 |'
- en: '| 7 | 6265 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 6265 |'
- en: '| 8 | 5851 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 5851 |'
- en: '| 9 | 5949 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5949 |'
- en: We will use the TensorFlow library to train and test the MNIST dataset. We will
    split the dataset of 70,000 rows into 60,000 training rows and 10,000 test rows.
    Next, we'll find the accuracy of the model. The model can then be used to predict
    any incoming dataset of 28 x 28 pixel handwritten digits containing numbers between
    zero and nine. For our sample Python code, we use a 100-row training dataset and
    a 10-row test dataset. In this example, we will learn to use the TensorFlow layers
    module that provides a high-level API that makes it easy to construct a neural
    network. It provides methods that facilitate creating dense (FC) layers and convolutional
    layers, adding activation functions, and applying dropout regularization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow库来训练和测试MNIST数据集。我们将70,000行的数据集分为60,000个训练行和10,000个测试行。接下来，我们将找到模型的准确率。然后，该模型可以用来预测任何包含0到9数字的28
    x 28像素手写数字的输入数据集。在我们的示例Python代码中，我们使用一个100行的训练数据集和一个10行的测试数据集。在这个例子中，我们将学习如何使用TensorFlow层模块，它提供了一个高级API，使得构建神经网络变得容易。它提供了创建密集（FC）层和卷积层、添加激活函数和应用dropout正则化的方法。
- en: 'To start we will analyze the code line by line, then we will see how to process
    it with the tools made available by Google Cloud Platform. Now, let''s go through
    the code to learn how to apply a CNN to solve a HWR problem. Let''s start from
    the beginning of the code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将逐行分析代码，然后我们将了解如何使用Google Cloud Platform提供的工具来处理它。现在，让我们从头开始分析代码，学习如何应用卷积神经网络（CNN）来解决手写识别（HWR）问题：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These three lines are added to write a Python 2/3 compatible code base. So
    let''s move on to importing modules:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这三条线被添加以编写兼容Python 2/3的代码库。所以让我们继续导入模块：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this way, we have imported the `numpy` and `tensorflow` module. Let''s analyze
    the next line of code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经导入了`numpy`和`tensorflow`模块。让我们分析下一行代码：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code sets the threshold for what messages will be logged. After an initial
    phase, we pass to define the function that will allow us to build a CNN model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码设置了将记录的消息的阈值。在初始阶段之后，我们将定义一个函数，该函数将允许我们构建一个CNN模型：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We have thus defined the function. Now let''s move on:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经定义了函数。现在让我们继续：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this code line, we have passed the input tensors in the form (`batch_size`,
    `image_width`, `image_height`, `channels`) as expected from the methods in the
    layers module, for creating convolutional and pooling layers for two-dimensional
    image data. Let''s move on to the first convolutional layer:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行代码中，我们以`（batch_size，image_width，image_height，channels）`的形式传递了输入张量，这是从层模块中的方法期望得到的，用于创建二维图像数据的卷积和池化层。让我们继续到第一个卷积层：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This layer creates a convolution kernel that is convolved with the layer input
    to produce a tensor of outputs. The number of filters in the convolution is 32,
    the height and width of the 2D convolution window are `[5,5]`, and the activation
    function is a ReLU function. To do this, we used the `conv2d()` method in the
    layers module. Next, we connect our first pooling layer to the convolutional layer
    we just created:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层创建了一个卷积核，该核与层输入卷积以产生输出张量。卷积中的过滤器数量为32，2D卷积窗口的高度和宽度为`[5,5]`，激活函数是ReLU函数。为此，我们使用了层模块中的`conv2d()`方法。接下来，我们将我们的第一个池化层连接到我们刚刚创建的卷积层：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We used the `max_pooling2d()` method in layers to construct a layer that performs
    max pooling with a 2 x 2 filter and stride of `2`. Now we will connect a second
    convolutional layer to our CNN:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在层中使用了`max_pooling2d()`方法来构建一个执行2x2过滤器最大池化和步长为`2`的层。现在我们将第二个卷积层连接到我们的CNN：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we will connect a second pooling layer to our CNN:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将连接第二个池化层到我们的卷积神经网络（CNN）：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will add a dense layer:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个密集层：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With this code, we added a dense layer with 1,024 neurons and ReLU activation
    to our CNN to perform classification on the features extracted by the convolution/pooling
    layers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这段代码，我们在CNN中添加了一个包含1,024个神经元的密集层和ReLU激活函数，以对卷积/池化层提取的特征进行分类。
- en: Remember, a ReLU level is composed of neurons that apply the function *f(x)
    = max (0, x)*. These levels increase the non-linearity of the network, and at
    the same time, they do not modify the receiving fields of convolution levels.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，ReLU层由应用函数*f(x) = max (0, x)*的神经元组成。这些层增加了网络的非线性，同时它们不修改卷积层的接收域。
- en: 'To improve the results, we will apply dropout regularization to our dense layer:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高结果，我们将在密集层上应用dropout正则化：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To do this we used the dropout method in layers. Next, we will add the final
    layer to our neural network:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们在层中使用了dropout方法。接下来，我们将添加最终的层到我们的神经网络：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is the `logits` layer, which will return the raw values for our predictions.
    With the previous code, we created a dense layer with `10` neurons (one for each
    target class 0–9), with linear activation. We just have to generate the predictions:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`logits`层，它将返回我们的预测的原始值。通过前面的代码，我们创建了一个包含`10`个神经元的密集层（每个目标类0-9一个），具有线性激活。我们只需生成预测：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We converted the raw values generated from our predictions into two different
    formats that our model function can return: a digit from 0–9 and the probability
    that the example is a zero, is a one, is a two, and so on. We compile our predictions
    in a dict and return an `EstimatorSpec` object. Now, we will pass to define a
    `loss` function:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将预测生成的原始值转换为两种不同的格式，我们的模型函数可以返回：一个从0到9的数字以及示例是零、是一、是二等的概率。我们在字典中编译我们的预测并返回一个`EstimatorSpec`对象。现在，我们将传递定义一个`loss`函数：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A `loss` function measures how closely the model''s predictions match the target
    classes. This function is used for both training and evaluation. We will configure
    our model to optimize this loss value during training:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss`函数衡量模型的预测与目标类之间的匹配程度。此函数用于训练和评估。我们将配置我们的模型，在训练期间优化此损失值：'
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We used a learning rate of `0.001` and stochastic gradient descent as the optimization
    algorithm. Now, we will add an accuracy metric in our model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`0.001`的学习率和随机梯度下降作为优化算法。现在，我们将在模型中添加一个准确度指标：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To do this, we defined the `eval_metric_ops` dict in the `EVAL` mode. We have
    thus defined the architecture of our network; now it is necessary to define the
    code to train and test our network. To do this, we will add a `main()` function
    to our Python code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们在`EVAL`模式下定义了`eval_metric_ops`字典。因此，我们已经定义了网络的架构；现在有必要定义训练和测试网络的代码。为此，我们将在Python代码中添加一个`main()`函数：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we will load training and eval data:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将加载训练和评估数据：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In this piece of code, we stored the training feature data and training labels
    as `numpy` arrays in `train_data` and `train_labels`, respectively. Similarly,
    we stored the evaluation feature data and evaluation labels in `eval_data` and
    `eval_labels`, respectively. Next, we will create an `Estimator` for our model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们将训练特征数据和训练标签分别存储在`train_data`和`train_labels`中的`numpy`数组中。同样，我们将评估特征数据和评估标签分别存储在`eval_data`和`eval_labels`中。接下来，我们将为我们的模型创建一个`Estimator`：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'An `Estimator` is a TensorFlow class for performing high-level model training,
    evaluation, and inference. The following code sets up logging for predictions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`Estimator`是TensorFlow的一个类，用于执行高级模型训练、评估和推理。以下代码设置了预测的日志记录：'
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we''re ready to train our model:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练我们的模型：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To do this, we have created `train_input_fn` and called `train()`on `mnist_classifier`.
    In the previous code, we fixed `steps=15000`, which means the model will train
    for 15,000 steps in all.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们创建了`train_input_fn`并在`mnist_classifier`上调用`train()`。在前面的代码中，我们固定了`steps=15000`，这意味着模型将总共训练15,000步。
- en: The time required to perform this training varies depending on the processor
    installed on our machine, but in any case, it will probably be more than 1 hour.
    To perform such training in less time, you can reduce the number of steps passed
    to the `train()` function; it is clear that this change will have a negative effect
    on the accuracy of the algorithm.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此训练所需的时间取决于我们机器上安装的处理器，但无论如何，它可能将超过1小时。为了在更短的时间内进行此类训练，您可以减少传递给`train()`函数的步骤数；很明显，这种变化将对算法的准确性产生负面影响。
- en: 'Finally, we will evaluate the model and print the results:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将评估模型并打印结果：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We called the `evaluate` method, which evaluates the metrics we specified in
    the `eval_metriced_ops` argument in the `model_fn`. Our Python code ends with
    the following lines:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用了`evaluate`方法，该方法评估了在`model_fn`的`eval_metriced_ops`参数中指定的度量标准。我们的Python代码以以下行结束：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: These lines are just a very quick wrapper that handles flag parsing and then
    dispatches to your own main function. At this point, we just have to copy the
    entire code into a file with a `.py` extension and run it on a machine where Python
    and TensorFlow are installed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行只是一个非常快速的包装器，用于处理标志解析，然后转发到您自己的主函数。在这个阶段，我们只需将整个代码复制到一个具有`.py`扩展名的文件中，并在安装了Python和TensorFlow的机器上运行它。
- en: Run Python code on Google Cloud Shell
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Google Cloud Shell上运行Python代码
- en: Google Cloud Shell provides command-line access to cloud resources directly
    from your browser. You can easily manage projects and resources without having
    to install the Google Cloud SDK or other tools in your system. With Cloud Shell,
    the `gcloud` command-line tool from Cloud SDK and other necessary utilities are
    always available, updated and fully authenticated when you need them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Shell直接从您的浏览器提供对云资源的命令行访问。您可以轻松管理项目和服务，无需在系统中安装Google Cloud SDK或其他工具。使用Cloud
    Shell，当您需要时，Cloud SDK的`gcloud`命令行工具和其他必要的实用工具始终可用，更新并完全认证。
- en: 'The following are some of the features of the Google Cloud Shell:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些Google Cloud Shell的功能：
- en: It's a shell environment for managing resources hosted on Google Cloud Platform.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个用于管理托管在Google Cloud Platform上的资源的shell环境。
- en: We can manage our GCP resources with the flexibility of a Linux shell. Cloud
    Shell provides command-line access to an instance of the virtual machine in a
    terminal window that opens in the web console.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用Linux shell的灵活性来管理我们的GCP资源。Cloud Shell通过在网页控制台中打开的终端窗口，提供了对虚拟机实例的命令行访问。
- en: It offers integrated authorization for access to projects and resources hosted
    on Google Cloud Platform.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为访问托管在Google Cloud Platform上的项目和资源提供了集成授权。
- en: Many of your favorite command-line tools, from bash and sh to emacs and vim,
    are already preinstalled and updated. Administration tools such as the MySQL client,
    Kubernetes, and Docker are configured and ready. You no longer need to worry about
    installing the latest version and all of its dependencies. Simply connect to Cloud
    Shell.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您最喜欢的许多命令行工具，从 bash 和 sh 到 emacs 和 vim，都已经预先安装和更新。如 MySQL 客户端、Kubernetes 和 Docker
    等管理工具已配置并就绪。您不再需要担心安装最新版本及其所有依赖项。只需连接到 Cloud Shell。
- en: Developers will have access to all favorite preconfigured development tools.
    You will find development and implementation tools for Java, Go, Python, Node.js,
    PHP, and Ruby. Run your web applications within the Cloud Shell instance and preview
    them in the browser. Then commit to the repository again with the preconfigured
    Git and Mercurial clients.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者将能够访问所有喜欢的预配置开发工具。您将找到 Java、Go、Python、Node.js、PHP 和 Ruby 的开发和实现工具。在 Cloud
    Shell 实例中运行您的网络应用程序，并在浏览器中预览它们。然后使用预配置的 Git 和 Mercurial 客户端再次提交到存储库。
- en: Cloud Shell provisions 5 GB of permanent disk storage space, mounted as the
    `$ HOME` directory on the Cloud Shell instance. All files stored in the `$ HOME`
    directory, including user configuration scripts and files such as `bashrc` and
    `vimrc`, persist from one session to another.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Shell 为 Cloud Shell 实例分配了 5 GB 的永久磁盘存储空间，挂载为 `$ HOME` 目录。存储在 `$ HOME`
    目录中的所有文件，包括用户配置脚本和 `bashrc`、`vimrc` 等文件，从一个会话持续到另一个会话。
- en: 'To start Cloud Shell, just click on the Activate Google Cloud Shell button
    at the top of the console window, as shown in the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 Cloud Shell，只需在控制台窗口顶部点击激活 Google Cloud Shell 按钮，如下面的截图所示：
- en: '![](img/ff23e585-6877-4cbd-9c15-825aa579545f.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff23e585-6877-4cbd-9c15-825aa579545f.png)'
- en: 'A Cloud Shell session opens inside a new frame at the bottom of the console
    and displays a command-line prompt. It can take a few seconds for the shell session
    to be initialized. Now, our Cloud Shell session is ready to use, as shown in the
    following screenshot:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台底部的新的框架中打开一个 Cloud Shell 会话，并显示命令行提示符。初始化 shell 会话可能需要几秒钟。现在，我们的 Cloud Shell
    会话已准备好使用，如下面的截图所示：
- en: '![](img/e3fc2475-c7d1-4feb-bc98-9e05eade34ad.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3fc2475-c7d1-4feb-bc98-9e05eade34ad.png)'
- en: At this point, we need to transfer the `cnn_hwr.py` file containing the Python
    code in the Google Cloud Platform. We have seen that to do so, we can use the
    resources made available by Google Cloud Storage. Then we open the Google Cloud
    Storage browser and create a new bucket.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要将包含 Python 代码的 `cnn_hwr.py` 文件传输到 Google Cloud Platform。我们已看到，为此，我们可以使用
    Google Cloud Storage 提供的资源。然后我们打开 Google Cloud Storage 浏览器并创建一个新的存储桶。
- en: Remember that buckets are the basic containers that hold your data. Everything
    you store in Cloud Storage must be contained in a bucket. You can use buckets
    to organize your data and control access to your data, but unlike directories
    and folders, you cannot nest buckets.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，存储桶是基本容器，用于存储您的数据。您存储在 Cloud Storage 中的所有内容都必须包含在存储桶中。您可以使用存储桶来组织您的数据并控制对数据的访问，但与目录和文件夹不同，您不能嵌套存储桶。
- en: 'To transfer the `cnn_hwr.py` file to Google Storage, perform the following
    steps:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `cnn_hwr.py` 文件传输到 Google Storage，请执行以下步骤：
- en: Just click on the CREATE BUCKET icon
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需点击创建存储桶图标
- en: Type the name of the new bucket (`cnn-hwr`) in the create a bucket window
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建存储桶窗口中输入新存储桶的名称（`cnn-hwr`）
- en: After this, a new bucket is available in the buckets list
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，在存储桶列表中会出现一个新的存储桶
- en: Click on the `cnn-hwr` bucket
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `cnn-hwr` 存储桶
- en: Click on uploads files icon in the window opened
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打开的窗口中点击上传文件图标
- en: Select the file in the dialog window opened
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打开的对话框窗口中选择文件
- en: Click Open
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击打开
- en: 'At this point, our file will be available in the new bucket, as shown in the
    following figure:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们的文件将出现在新的存储桶中，如下面的图所示：
- en: '![](img/e2410819-05e9-449f-b5f4-236d1b4d423b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2410819-05e9-449f-b5f4-236d1b4d423b.png)'
- en: 'Now we can access the file from Cloud Shell. To do this, we create a new folder
    in the shell. Type the following command in the shell prompt:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从 Cloud Shell 访问文件。为此，我们在 shell 中创建一个新的文件夹。在 shell 提示符中输入以下命令：
- en: '[PRE28]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, to copy the file from the Google Storage bucket to the `CNN-HWR` folder,
    simply type this command in the shell prompt:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要将文件从 Google Storage 存储桶复制到 `CNN-HWR` 文件夹，只需在 shell 提示符中输入以下命令：
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following code is displayed:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将显示：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let''s move into the folder and verify the presence of the file:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入文件夹并验证文件的存在：
- en: '[PRE31]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We just have to run the file:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需运行该文件：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'A series of preliminary instructions is displayed:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 显示了一系列初步指令：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'They indicate that the data download was successful, as was the invocation
    of the TensorFlow library. From this point on, the training of the network begins,
    which, as we have anticipated, may be quite long. At the end of the algorithm
    execution, the following information will be returned:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 它们表明数据下载成功，TensorFlow 库的调用也成功了。从这一点开始，网络的训练开始，正如我们预料的，这可能相当长。算法执行结束后，将返回以下信息：
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this case, we've achieved an accuracy of `97.2` percent on our test dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们在测试数据集上达到了`97.2`的准确率。
- en: Recurrent neural network
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Feedforward neural networks are based on input data that is powered to the network
    and converted into output. If it is a supervised learning algorithm, the output
    is a label that can recognize the input. Basically, these algorithms connect raw
    data to specific categories by recognizing patterns. Recurrent networks, on the
    other hand, take as input not only the current input data that is powered to the
    network, but also what they have experienced over time.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络基于输入数据，这些数据被提供给网络并转换为输出。如果是一个监督学习算法，输出是一个可以识别输入的标签。基本上，这些算法通过识别模式将原始数据连接到特定的类别。另一方面，循环网络不仅接受被提供给网络的当前输入数据，还包括它们随时间经历过的内容。
- en: An **recurrent neural network** (**RNN**) is a neural model in which a bidirectional
    flow of information is present. In other words, while the propagation of signals
    in feedforward networks takes place only in a continuous manner in a direction
    from inputs to outputs, recurrent networks are different. In them, this propagation
    can also occur from a neural layer following a previous one, or between neurons
    belonging to the same layer, and even between a neuron and itself.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**循环神经网络**（**RNN**）是一种信息双向流动的神经网络模型。换句话说，在前馈网络中，信号的传播仅以连续的方式从输入到输出方向进行，而循环网络则不同。在这些网络中，这种传播也可以从跟随前一个神经层的神经层发生，或者在同一层的神经元之间发生，甚至可以发生在同一个神经元自身之间。
- en: The decision made by a recurrent network at a specific instant affects the decision
    it will reach immediately afterwards. So, recurrent networks have two input sources—the
    present and the recent past—that combine to determine how to respond to new data,
    just as people do in life everyday.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一个循环网络在特定时刻做出的决策会影响它接下来将做出的决策。因此，循环网络有两个输入来源——当前和最近过去——它们结合起来决定如何对新数据进行响应，就像人们在日常生活中所做的那样。
- en: 'Recurrent networks are distinguished from feedforward networks thanks to the
    feedback loop linked to their past decisions, thus accepting their output momentarily
    as inputs. This feature can be emphasized by saying that recurrent networks have
    memory. Adding memory to neural networks has a purpose: there is information in
    the sequence itself and recurrent networks use it to perform the tasks that feedforward
    networks cannot.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络之所以与前馈网络不同，是因为它们与过去决策相关的反馈回路，因此暂时将它们的输出作为输入接受。这一点可以通过说循环网络具有记忆来强调。将记忆添加到神经网络中有其目的：序列本身包含信息，循环网络使用这些信息执行前馈网络无法执行的任务。
- en: Access to memory occurs through the content rather than by address or location.
    One approach to this is that the memory content is the pattern of activations
    on the nodes of an RNN. The idea is to start the network with an activation scheme
    that is a partial or noisy representation of the requested memory content and
    that the network stabilizes on the required content.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 访问记忆是通过内容而不是通过地址或位置来实现的。一种方法是认为记忆内容是 RNN 节点上的激活模式。想法是使用一个激活方案来启动网络，这个方案是请求的记忆内容的部分或噪声表示，并且网络稳定在所需的内容上。
- en: 'RNN is a class of neural network where there is at least one feedback connection
    between neurons that form a directed cycle. A typical RNN with connections between
    output layer and hidden layer is represented in the following figure:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一类神经网络，其中至少存在一个神经元之间的反馈连接，形成一个有向循环。以下图示了一个典型的 RNN，其输出层和隐藏层之间存在连接：
- en: '![](img/cf599c2a-4e1e-4a61-923d-fa7a950ceda9.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf599c2a-4e1e-4a61-923d-fa7a950ceda9.png)'
- en: In the recurring network shown in the figure, both the input level and the output
    level are used to define the weights of the hidden level.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在图所示的循环网络中，既使用输入层又使用输出层来定义隐藏层的权重。
- en: 'Ultimately, we can think of RNNs as a variant of ANNs: these variants can be
    characterized on a different number of hidden levels and a different trend of
    the data flow. The RNN are characterized by a different trend of the data flow,
    in fact the connections between the neurons form a cycle. Unlike feedforward networks,
    RNNs can use internal memory for their processing. RNNs are a class of ANNs that
    feature connections between hidden layers that are propagated through time in
    order to learn sequences.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以将RNN视为ANN的一种变体：这些变体可以在不同的隐藏层数量和数据流的不同趋势上进行表征。RNN的特点是数据流的不同趋势，实际上神经元之间的连接形成了一个循环。与前馈网络不同，RNN可以使用内部记忆进行其处理。RNN是一类ANN，其特征是隐藏层之间的连接，这些连接通过时间传播以学习序列。
- en: 'The way the data is kept in memory and flows at different time periods makes
    RNNs powerful and successful. RNN use cases include the following fields:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在内存中保持的方式以及在不同时间周期中的流动使得RNN强大且成功。RNN的应用案例包括以下领域：
- en: Stock market predictions
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股市预测
- en: Image captioning
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像标题
- en: Weather forecast
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气预报
- en: Time-series-based forecasts
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于时间序列的预测
- en: Language translation
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Speech recognition
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: HWR
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写识别
- en: Audio or video processing
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频或视频处理
- en: Robotics action sequencing
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人动作序列
- en: Recurrent networks are designed to recognize patterns as a sequence of data
    and are helpful in prediction and forecasting. They can work on text, images,
    speech, and time series data. RNNs are among the powerful ANNs and represent the
    biological brain, including memory with processing power.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络被设计成通过数据序列识别模式，并在预测和预测方面非常有用。它们可以在文本、图像、语音和时间序列数据上工作。RNN是功能强大的ANN之一，代表了生物大脑，包括具有处理能力的记忆。
- en: 'Recurrent networks take inputs from the current input (like a feedforward network)
    and the output that was calculated previously. In the following figure, we compare
    a single neuron operating scheme for both a feedforward neural network and an
    RNN:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络从当前输入（类似于前馈网络）以及之前计算得出的输出中获取输入。在下面的图中，我们比较了前馈神经网络和循环神经网络的单个神经元工作原理：
- en: '![](img/95ac9383-d0c0-4058-b8d6-2684554946cb.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/95ac9383-d0c0-4058-b8d6-2684554946cb.png)'
- en: As we can see in the simple, just-proposed single neuron scheme, the feedback
    signal is added to the input signal in the RNN. Feedback is a considerable and
    significant feature. A feedback network is more likely to update and has more
    computing capacity than a simple network limited to one-way signals from input
    to output. Feedback networks show phenomena and processes not revealed by one-way
    networks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在简单、刚刚提出的单个神经元方案中看到的那样，反馈信号被添加到RNN的输入信号中。反馈是一个相当重要和显著的特征。与仅限于从输入到输出的单向信号的简单网络相比，反馈网络更有可能更新，并且具有更多的计算能力。反馈网络显示出单向网络未揭示的现象和过程。
- en: 'To understand the differences between ANN and RNN, we consider the RNN as a
    network of neural networks, and the cyclic nature is unfolded in the following
    manner: the state of a neuron is considered at different time periods (*t-1*,
    *t*, *t+1*, and so on) until convergence or until the total number of epochs is
    reached.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解ANN和RNN之间的差异，我们将RNN视为神经网络的网络，并且循环性质以下列方式展开：考虑神经元在不同时间周期（*t-1*，*t*，*t+1*，等等）的状态，直到收敛或达到总epoch数。
- en: The network learning phase can be performed using gradient descent procedures
    similar to those leading to the backpropagation algorithm for feedforward networks.
    At least this is valid in the case of simple architectures and deterministic activation
    functions. When activations are stochastic, simulated annealing approaches may
    be more appropriate.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 网络学习阶段可以使用类似于导致前馈网络反向传播算法的梯度下降过程进行。至少在简单架构和确定性激活函数的情况下是有效的。当激活是随机的，模拟退火方法可能更合适。
- en: 'RNN architectures can have many different forms. There are more variants in
    the way the data flows backwards:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: RNN架构可以有多种不同的形式。在数据向后流动的方式上有更多变体：
- en: Fully recurrent
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全循环
- en: Recursive
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归
- en: Hopfield
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳频
- en: Elman networks
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elman网络
- en: LSTM
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM
- en: Gated recurrent unit
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: Bidirectional
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向
- en: Recurrent MLP
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环多层感知器
- en: In the following pages, we will analyze the architecture of some of these networks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几页中，我们将分析这些网络中的一些架构。
- en: Fully recurrent neural networks
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全循环神经网络
- en: 'A fully RNN is a network of neurons, each with a directed (one-way) connection
    to every other neuron. Each neuron has a time-varying, real-valued activation.
    Each connection has a modifiable real-valued weight. Input neurons, output neurons,
    and hidden neurons are expected. This type of network is a multilayer perceptron
    with the previous set of hidden unit activations feeding back into the network
    along with the inputs, as shown in the following figure:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全递归神经网络是一个神经元网络，每个神经元都有一个指向其他每个神经元的定向（单向）连接。每个神经元都有一个时变的、实值的激活度。每个连接都有一个可修改的实值权重。期望有输入神经元、输出神经元和隐藏神经元。这种网络是一种多层感知器，其中前一组隐藏单元的激活度与输入一起反馈到网络中，如下面的图所示：
- en: '![](img/7a3453ca-581a-4f84-be6f-df21024492a6.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7a3453ca-581a-4f84-be6f-df21024492a6.png)'
- en: At each step, each non-input unit calculates its current activation as a nonlinear
    function of the weighted sum of activations of all units that connect to it.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步中，每个非输入单元将其当前激活度计算为连接到它的所有单元激活度的加权和的非线性函数。
- en: Recursive neural networks
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'A recursive network is just a generalization of a recurrent network. In a recurrent
    network, the weights are shared and dimensionality remains constant along the
    length of the sequence. In a recursive network, the weights are shared and dimensionality
    remains constant but at every node. The following figure shows what a recursive
    neural network looks like:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 递归网络是循环网络的推广。在循环网络中，权重是共享的，维度在序列长度上保持不变。在递归网络中，权重是共享的，维度在节点上保持不变。以下图显示了递归神经网络的外观：
- en: '![](img/dab54544-cb52-4a9c-a2ea-30990052aa8f.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dab54544-cb52-4a9c-a2ea-30990052aa8f.png)'
- en: Recursive neural networks can be used for learning tree-like structures. They
    are highly useful for parsing natural scenes and language.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络可用于学习树状结构。它们在解析自然场景和语言方面非常有用。
- en: Hopfield recurrent neural networks
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 霍普菲尔德循环神经网络
- en: In 1982, physicist John J. Hopfield published a fundamental article in which
    a mathematical model commonly known as the **Hopfield network** was introduced.
    This network highlighted new computational capabilities deriving from the collective
    behavior of a large number of simple processing elements. A Hopfield Network is
    a form of recurrent ANN.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 1982年，物理学家约翰·J·霍普菲尔德发表了一篇基础文章，在其中引入了一个数学模型，通常被称为**霍普菲尔德网络**。这个网络突出了从大量简单处理元素的集体行为中产生的新计算能力。霍普菲尔德网络是一种循环人工神经网络（ANN）的形式。
- en: According to Hopfield every physical system can be considered as a potential
    memory device if it has a certain number of stable states, which act as an attractor
    for the system itself. On the basis of this consideration, he formulated the thesis
    that the stability and placement of such attractors represented spontaneous properties
    of systems consisting of considerable quantities of mutually interacting neurons.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 根据霍普菲尔德的观点，每个物理系统都可以被认为是一个潜在的存储设备，如果它具有一定数量的稳定状态，这些状态作为系统本身的吸引子。基于这一考虑，他提出了这样的吸引子的稳定性和位置代表了由大量相互作用的神经元组成的系统的自发性属性的观点。
- en: 'Structurally, the Hopfield network constitutes a recurrent symmetrical neural
    network (therefore with a synaptic weights matrix that is symmetric), one that
    is completely connected and in which each neuron is connected to all the others,
    as shown in the following figure:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 结构上，霍普菲尔德网络构成一个循环对称神经网络（因此具有对称的突触权重矩阵），它是完全连接的，其中每个神经元都与所有其他神经元相连，如下面的图所示：
- en: '![](img/0e4650f6-14ac-4d42-b5c5-009679651721.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0e4650f6-14ac-4d42-b5c5-009679651721.png)'
- en: As already mentioned before, a recurrent network is a neural model in which
    a flow of bidirectional information is present; in other words, while in feedforward
    networks the propagation of the signals takes place only in a continuous manner
    in the direction that leads from the inputs to the outputs in the recurrent networks
    this propagation can also occur from a neural layer following a previous one or
    between neurons belonging to at the same layer (Hopfield network) and even between
    a neuron and itself.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，循环神经网络是一种包含双向信息流的神经网络；换句话说，在前馈网络中，信号的传播仅以连续的方式在从输入到输出的方向上进行，而在循环网络中，这种传播也可以从跟随前一个神经层的神经层或同一层中的神经元（霍普菲尔德网络）之间发生，甚至可以发生在神经元与其自身之间。
- en: 'The dynamics of a Hopfield network is described by a nonlinear system of differential
    equations and the neuron update mechanism can be:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 跳频网络的动力学由一个非线性微分方程组描述，神经元更新机制可以是：
- en: '**Asynchronous**: One neuron is updated at a time'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步**：一次更新一个神经元'
- en: '**Synchronous**: All neurons are updated at the same time'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步**：所有神经元同时更新'
- en: '**Continuous**: All the neurons are continually updated'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续**：所有神经元持续更新'
- en: Elman neural networks
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elman神经网络
- en: 'The Elman neural network is a feedforward network in which the hidden layer,
    besides being connected to the output layer, forks into another identical layer,
    called the **context layer**, to which it is connected with weights equal to one.
    At each moment of time (each time the data is passed to the neurons of the input
    layer), the neurons of the context layer maintain the previous values and pass
    them to the respective neurons of the hidden layer. The following figure shows
    an Elman network scheme:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Elman神经网络是一种前馈网络，其中除了与输出层相连外，隐藏层还分支到另一个相同的层，称为**上下文层**，它与权重等于一的上下文层相连。在每一个时刻（每次数据传递到输入层的神经元时），上下文层的神经元保持前一个值并将它们传递给相应的隐藏层神经元。以下图显示了Elman网络方案：
- en: '![](img/9ef7e2f5-d6cb-4ba1-a4b8-26a326797a9e.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9ef7e2f5-d6cb-4ba1-a4b8-26a326797a9e.png)'
- en: Like feedforward networks, Elman's RNNs can be trained with an algorithm called
    **Backpropagation Through Time** (**BPTT**), a variant of the backpropagation
    created specifically for the RNNs. Substantially, this algorithm unrolls the neural
    network transforming it into a feedforward network, with a number of layers equal
    to the length of the sequence to be learned; subsequently, the classic backpropagation
    algorithm is applied. Alternatively, it is possible to use global optimization
    methods, such as genetic algorithms, especially with RNN topologies on which it
    is not possible to apply BPTT.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络类似，Elman的RNN可以使用称为**时间反向传播**（**BPTT**）的算法进行训练，这是专门为RNN创建的反向传播的变体。实质上，该算法将神经网络展开，将其转换为前馈网络，层数等于要学习的序列长度；随后，应用经典的反向传播算法。或者，可以使用全局优化方法，如遗传算法，特别是在无法应用BPTT的RNN拓扑结构上。
- en: Long short-term memory networks
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: LSTM is a particular architecture of RNN, originally conceived by Hochreiter
    and Schmidhuber in 1997\. This type of neural network has been recently rediscovered
    in the context of deep learning because it is free from the problem of vanishing
    gradient, and in practice it offers excellent results and performance.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是RNN的一种特定架构，最初由Hochreiter和Schmidhuber在1997年构思。这种类型的神经网络最近在深度学习背景下被重新发现，因为它不受梯度消失问题的影响，并且在实践中提供了出色的结果和性能。
- en: The vanishing gradient problem affects the training of ANNs with gradient-based
    learning methods. In gradient-based methods such as backpropagation, weights are
    adjusted proportionally to the gradient of the error. Because of the way in which
    the aforementioned gradients are calculated, we obtain the effect that their module
    decreases exponentially, proceeding towards the deepest layers. The problem is
    that in some cases, the gradient will be vanishingly small, effectively preventing
    the weight from changing its value. In the worst case, this may completely stop
    the neural network from further training.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题影响了基于梯度的学习方法的ANN训练。在梯度方法，如反向传播中，权重按误差梯度的比例进行调整。由于上述梯度的计算方式，我们得到一个效果，即它们的模量呈指数下降，朝着最深的层前进。问题是，在某些情况下，梯度会变得极其小，实际上阻止了权重改变其值。在最坏的情况下，这可能会完全阻止神经网络进一步训练。
- en: LSTM-based networks are ideal for prediction and classification of time sequences,
    and they are supplanting many classic machine learning approaches. In fact, in
    2012, Google replaced its voice recognition models, passing from the Hidden Markov
    Models (which represented the standard for over 30 years) to deep learning neural
    networks. In 2015, it switched to the RNNs LSTM combined with **connectionist
    temporal classification** (**CTC**).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LSTM的网络非常适合预测和分类时间序列，它们正在取代许多经典机器学习方法。事实上，在2012年，谷歌替换了其语音识别模型，从表示了30多年标准的隐马尔可夫模型（HMM）过渡到深度学习神经网络。在2015年，它切换到结合**连接主义时间分类**（**CTC**）的RNN
    LSTM。
- en: CTC is a type of neural network output and associated scoring function for training
    RNNs.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: CTC是一种神经网络输出和相关评分函数，用于训练RNN。
- en: This is due to the fact that LSTM networks are able to consider long-term dependencies
    between data, and in the case of speech recognition, this means managing the context
    within a sentence to improve recognition capacity.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 LSTM 网络能够考虑数据之间的长期依赖关系，在语音识别的情况下，这意味着管理句子中的上下文以提高识别能力。
- en: 'An LSTM network consists of cells (LSTM blocks) linked together. Each cell
    is in turn composed of three types of ports: **input gate**, **output gate**,
    and **forget gate**. They respectively implement the write, read, and reset functions
    on the cell memory. The ports are not binary but analogical (generally managed
    by a sigmoid activation function mapped in a range (0, 1), where zero indicates
    total inhibition and 1 indicates total activation), and they are multiplicative.
    The presence of these ports allows the LSTM cells to remember information for
    an indefinite amount of time. In fact, if the input gate is below the activation
    threshold, the cell will maintain the previous state, while if it is enabled,
    the current state will be combined with the input value. As the name suggests,
    the forget gate resets the current state of the cell (when its value is brought
    to zero), and the output gate decides whether the value inside the cell must be
    taken out or not.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络由相互连接的细胞（LSTM 块）组成。每个细胞又由三种类型的端口组成：**输入门**、**输出门**和**遗忘门**。它们分别实现了对细胞内存的写入、读取和重置功能。端口不是二进制而是模拟的（通常由一个映射到范围（0,
    1）的 sigmoid 激活函数管理，其中零表示完全抑制，1 表示完全激活），并且是乘法的。这些端口的存 在使得 LSTM 细胞能够记住信息一段时间。实际上，如果输入门低于激活阈值，细胞将保持之前的状态，而如果它被激活，当前状态将与输入值相结合。正如其名所示，遗忘门将细胞当前状态重置（当其值被带到零时），输出门决定细胞内的值是否必须取出。
- en: 'The following figure shows an LSTM unit:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 LSTM 单元：
- en: '![](img/84226791-6851-4aa4-975d-682e1f237ec4.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84226791-6851-4aa4-975d-682e1f237ec4.png)'
- en: The approaches based on neural networks are very powerful, as they allow capture
    of the characteristics and relationships between the data. In particular, it has
    also been seen that LSTM networks, in practice, offer high performance and excellent
    recognition rates. One disadvantage is that the neural networks are black box
    models, so their behavior is not predictable, and it is not possible to trace
    the logic with which they process the data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的这些方法非常强大，因为它们能够捕捉数据之间的特征和关系。特别是，在实践中，也发现 LSTM 网络提供了高性能和优秀的识别率。一个缺点是神经网络是黑盒模型，因此它们的行为是不可预测的，并且无法追踪它们处理数据的逻辑。
- en: Handwriting Recognition using RNN and TensorFlow
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 和 TensorFlow 进行手写识别
- en: To practice RNNs, we will use the dataset previously used to construct the CNN.
    I refer to the MNIST dataset, a large database of handwritten digits. It has a
    set of 70,000 examples of data. It is a subset of NIST's larger dataset. Images
    of 28 x 28 pixel resolution are stored in a matrix of 70,000 rows and 785 columns;
    each pixel value from the 28 x 28 matrix and one value is the actual digit. In
    a fixed-size image, the digits have been size-normalized.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习 RNN，我们将使用之前用于构建 CNN 的数据集。我指的是 MNIST 数据集，这是一个包含大量手写数字的大型数据库。它包含 70,000 个数据示例。它是
    NIST 更大数据集的一个子集。28 x 28 像素分辨率的图像存储在一个 70,000 行和 785 列的矩阵中；28 x 28 矩阵中的每个像素值和一个值是实际的数字。在固定大小的图像中，数字已经被尺寸归一化。
- en: In this case, we will implement an RNN (LSTM) using the TensorFlow library to
    classify images. We will consider every image row as a sequence of pixels. Because
    the MNIST image shape is 28 x 28, we will handle 28 sequences of 28 time steps
    for every sample.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，我们将使用 TensorFlow 库实现一个 RNN（LSTM）来对图像进行分类。我们将把每行图像视为像素序列。由于 MNIST 图像的形状是
    28 x 28，我们将为每个样本处理 28 个时间步长的 28 个序列。
- en: 'To start, we will analyze the code line by line; then we will see how to process
    it with the tools made available by Google Cloud Platform. Now, let''s go through
    the code to learn how to apply an RNN (LSTM) to solve an HWR problem. Let''s start
    from the beginning of the code:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将逐行分析代码；然后我们将看到如何使用 Google Cloud Platform 提供的工具来处理它。现在，让我们通过代码学习如何将 RNN（LSTM）应用于解决
    HWR 问题。让我们从代码的开始部分开始：
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'These three lines are added to write a Python 2/3 compatible code base. So
    let''s move on to importing modules:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这三行代码是为了编写一个兼容 Python 2/3 的代码库而添加的。因此，让我们继续导入模块：
- en: '[PRE36]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In this way, we have imported the `tensorflow` module and, from `tensorflow.contrib`,
    the `rnn` module. The `tensorflow.contrib` contains volatile or experimental code.
    The `rnn` module is a module for constructing RNN Cells and additional RNN operations.
    Let''s analyze the next lines of code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经导入了`tensorflow`模块，以及来自`tensorflow.contrib`的`rnn`模块。《tensorflow.contrib》包含易变或实验性代码。《rnn`模块是一个用于构建RNN单元和额外的RNN操作的模块。让我们分析下一行代码：
- en: '[PRE37]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The first line is used to import the `mnist` dataset from the TensorFlow library;
    in fact, the `minist` dataset is already present in the library as an example.
    The second line reads the data from a local directory. Let''s move on to set the
    training parameters:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行用于从TensorFlow库导入`mnist`数据集；实际上，`minist`数据集已经作为示例存在于库中。第二行从本地目录读取数据。让我们继续设置训练参数：
- en: '[PRE38]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `learning_rate` is a value used by the learning algorithm to determine
    how quickly the weights are adjusted. It determines the acquisition time for neurons
    with weights that are trained using the algorithm. The `training_steps` sets the
    number of times the training process is performed. The `batch_size` is the number
    of samples you feed in your network. The `display_step` decides how many steps
    are shown the partial results of the training. Now let''s set the network parameters:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`学习率`是学习算法用来确定权重调整速度的值。它决定了使用该算法训练的权重神经元的获取时间。《训练步数》设置训练过程执行次数。《批量大小》是你输入网络的样本数量。《显示步数》决定显示训练部分结果的步数。现在让我们设置网络参数：'
- en: '[PRE39]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first parameter (`num_input`) sets the MNIST data input (image shape: 28
    x 28). The `timesteps` parameter is equivalent to the number of time steps you
    run your RNN. The `num_hidden` parameter sets the number of hidden layers of the
    neural network. Finally the `num_classes` parameter sets the MNIST total classes
    (0-9 digits). Let''s analyze the following lines of code:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数（`num_input`）设置MNIST数据输入（图像形状：28 x 28）。`timesteps`参数相当于你运行RNN的时间步数。《num_hidden`参数设置神经网络隐藏层的数量。最后，《num_classes`参数设置MNIST总类别（0-9数字）。让我们分析以下代码行：
- en: '[PRE40]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In these lines of code, we used a `tf.placeholder()` function. A placeholder
    is simply a variable that we will assign data to at a later date. It allows us
    to create our operations and build our computation graph without needing the data.
    In this way, we have set up the `tf.Graph` input. A `tf.Graph` contains two relevant
    kinds of information: graph structure and graph collections. TensorFlow uses a
    dataflow graph to represent your computation in terms of the dependencies between
    individual operations. This leads to a low-level programming model in which you
    first define the dataflow graph and then create a TensorFlow session to run parts
    of the graph across a set of local and remote devices. Let''s move on to define
    `weights`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些代码行中，我们使用了`tf.placeholder()`函数。占位符简单地说是一个我们将在以后日期分配数据的变量。它允许我们创建操作和构建计算图，而无需数据。这样，我们已经设置了`tf.Graph`输入。《tf.Graph》包含两种相关信息：图结构和图集合。TensorFlow使用数据流图来表示你的计算，以单个操作之间的依赖关系为依据。这导致了一种低级编程模型，其中你首先定义数据流图，然后创建TensorFlow会话，在一系列本地和远程设备上运行图的一部分。让我们继续定义`权重`：
- en: '[PRE41]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Weights in a network are the most important factor for converting an input
    to impact the output. This is similar to slope in linear regression, where a weight
    is multiplied to the input to add up to form the output. Weights are numerical
    parameters that determine how strongly each of the neurons affects the other.
    Bias is like the intercept added in a linear equation. It is an additional parameter
    used to adjust the output along with the weighted sum of the inputs to the neuron.
    Now we have to define the `RNN` by creating a new function:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的权重是将输入转换为影响输出的最重要的因素。这与线性回归中的斜率类似，其中权重乘以输入以形成输出。权重是决定每个神经元如何影响其他神经元的数值参数。偏差类似于线性方程中添加的截距。它是一个额外的参数，用于调整输出，以及神经元输入的加权总和。现在我们必须通过创建一个新的函数来定义`RNN`：
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `unstack()` function is used to get a list of `timesteps` tensors of shape
    (`batch_size`, `n_input`). Then we have defined an `lstm` cell with TensorFlow,
    and we''ve got an `lstm` cell output. Finally, we have placed a linear activation,
    using the `RNN` in the inner loop and last output. Let''s move on:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `unstack()` 函数来获取形状为 (`batch_size`, `n_input`) 的 `timesteps` 张量列表。然后我们使用
    TensorFlow 定义了一个 `lstm` 单元，并得到了 `lstm` 单元输出。最后，我们在内循环和最后一个输出处放置了一个线性激活，使用 `RNN`。接下来，让我们继续：
- en: '[PRE43]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The first line of code uses the newly defined `RNN` function to build the network,
    while the second line of code predicts using the function `tf.nn.softmax()`, which
    computes `softmax` activations. Next, we will define `loss` and `optimizer`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行代码使用新定义的 `RNN` 函数构建网络，而第二行代码使用 `tf.nn.softmax()` 函数进行预测，该函数计算 `softmax` 激活。接下来，我们将定义
    `loss` 和 `optimizer`：
- en: '[PRE44]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `loss` function maps an event or values of one or more variables onto a
    real number, intuitively representing some `cost` associated with the event. We
    have used the `tf.reduce_mean()` function, which computes the mean of elements
    across the dimensions of a tensor. The `optimizer` base class provides methods
    to compute gradients for a loss and apply gradients to variables. A collection
    of subclasses implement classic optimization algorithms such as gradient descent
    and AdaGrad. Let''s go ahead to evaluate model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss` 函数将事件或一个或多个变量的值映射到一个实数，直观地表示与事件相关的一些 `cost`。我们使用了 `tf.reduce_mean()`
    函数，该函数计算张量维度的元素均值。`optimizer` 基类提供了计算损失梯度并将梯度应用于变量的方法。一系列子类实现了经典的优化算法，如梯度下降和 AdaGrad。让我们继续评估模型：'
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then we will initialize the variables by assigning their default value:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过分配它们的默认值来初始化变量：
- en: '[PRE46]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now we can start training the network:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练网络：
- en: '[PRE47]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally we will calculate the accuracy for `128` mnist test images:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将计算 `128` 个 mnist 测试图像的准确率：
- en: '[PRE48]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: At this point, we just have to copy the entire code into a file with a `.py`
    extension and run it on a machine where Python and TensorFlow are installed.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只需将整个代码复制到一个以 `.py` 扩展名的文件中，并在安装了 Python 和 TensorFlow 的机器上运行它。
- en: LSTM on Google Cloud Shell
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Google Cloud Shell 上使用 LSTM
- en: After having thoroughly analyzed the Python code, it is time to run it around
    to classify the images contained in the dataset. To do this, we work in a similar
    way to what was done in the case of the example on CNN. So we will use the Google
    Cloud Shell. Google Cloud Shell provides command-line access to Cloud resources
    directly from your browser. You can easily manage projects and resources without
    having to install the Google Cloud SDK or other tools in your system. With Cloud
    Shell, the `gcloud` command-line tool from Cloud SDK and other necessary utilities
    are always available, updated and fully authenticated when you need them.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在彻底分析 Python 代码之后，是时候运行它来对数据集中的图像进行分类了。为此，我们以与 CNN 示例类似的方式进行操作。因此，我们将使用 Google
    Cloud Shell。Google Cloud Shell 直接从您的浏览器提供对云资源的命令行访问。您可以轻松管理项目和服务，而无需在系统中安装 Google
    Cloud SDK 或其他工具。使用 Cloud Shell，当您需要时，Cloud SDK 的 `gcloud` 命令行工具和其他必要的实用工具总是可用、更新并完全认证。
- en: 'To start Cloud Shell, just click the Activate Google Cloud Shell button at
    the top of the console window, as shown in the following screenshot:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 Cloud Shell，只需在控制窗口顶部单击“激活 Google Cloud Shell”按钮，如下面的截图所示：
- en: '![](img/ebe7c69a-a3d7-4867-aeeb-3f223daf727a.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebe7c69a-a3d7-4867-aeeb-3f223daf727a.png)'
- en: 'A Cloud Shell session opens inside a new frame at the bottom of the console
    and displays a command-line prompt. It can take a few seconds for the shell session
    to be initialized. Now, our Cloud Shell session is ready to use, as shown in the
    following screenshot:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Shell 会话在控制窗口底部的新的框架中打开，并显示一个命令行提示符。初始化 shell 会话可能需要几秒钟。现在，我们的 Cloud Shell
    会话已准备好使用，如下面的截图所示：
- en: '![](img/443a8156-ece7-4c3d-a847-f7686d3b2d3a.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/443a8156-ece7-4c3d-a847-f7686d3b2d3a.png)'
- en: At this point, we need to transfer the `rnn_hwr.py` file containing the Python
    code in the Google Cloud Platform. We have seen that to do so, we can use the
    resources made available by Google Cloud Storage. Then we open the Google Cloud
    Storage browser and create a new bucket.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们需要将包含 Python 代码的 `rnn_hwr.py` 文件传输到 Google Cloud Platform。我们已经看到，为此，我们可以使用
    Google Cloud Storage 提供的资源。然后我们打开 Google Cloud Storage 浏览器并创建一个新的存储桶。
- en: 'To transfer the `cnn_hwr.py` file on Google Storage, follow these steps:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `cnn_hwr.py` 文件传输到 Google 存储，请按照以下步骤操作：
- en: Just click on CREATE BUCKET icon
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需单击 CREATE BUCKET 图标
- en: Type the name of the new bucket (`rnn-hwr`) in the create a bucket window
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建存储桶窗口中输入新存储桶的名称（`rnn-hwr`）
- en: After this, a new bucket is available in the buckets list
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，在存储桶列表中会出现一个新的存储桶
- en: Click on the `rnn-hwr` bucket
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`rnn-hwr`存储桶
- en: Click on UPLOAD FILES icon in the window opened
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击打开的窗口中的“上传文件”图标
- en: Select the file in the dialog window opened
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出的对话框中选择文件
- en: Click Open
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“打开”
- en: 'At this point, our file will be available in the new bucket, as shown in the
    following screenshot:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们的文件将出现在新的存储桶中，如下面的截图所示：
- en: '![](img/ec3a8d23-345c-4434-8d2d-f3d2d63d6b62.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec3a8d23-345c-4434-8d2d-f3d2d63d6b62.png)'
- en: 'Now we can access the file from the Cloud Shell. To do this, we create a new
    folder in the shell. Type this command in the shell prompt:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从Cloud Shell访问该文件。为此，我们在shell中创建一个新的文件夹。在shell提示符中输入以下命令：
- en: '[PRE49]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, to copy the file from the Google Storage bucket to the `CNN-HWR` folder,
    simply type the following command in the shell prompt:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要将文件从Google Storage存储桶复制到`CNN-HWR`文件夹，只需在shell提示符中输入以下命令：
- en: '[PRE50]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following code is displayed:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 显示以下代码：
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s move into the folder and verify the presence of the file:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入文件夹并验证文件的存在：
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We just have to run the file:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需运行文件：
- en: '[PRE53]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'A series of preliminary instructions is displayed:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 显示一系列初步指令：
- en: '[PRE54]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'They indicate that the data download was successful, as was the invocation
    of the TensorFlow library. From this point on, the training of the network begins,
    which, as we have anticipated, may be quite long. At the end of the algorithm
    execution, the following information will be returned:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 它们表明数据下载成功，TensorFlow库的调用也成功。从这一点开始，网络的训练开始，正如我们预期的，可能相当长。算法执行结束后，将返回以下信息：
- en: '[PRE55]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In this case, we've achieved an accuracy of `97.6` percent on our test dataset.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们在测试数据集上达到了`97.6`的准确率。
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we tried to broaden the concepts underlying standard neural
    networks by adding features to solve more complex problems. To begin with, we
    discovered the architecture of CNNs. CNNs are ANNs in which the hidden layers
    are usually constituted by convolutional layers, pooling layers, FC layers, and
    normalization layers. The concepts underlying CNN were covered.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过添加功能来扩展标准神经网络的底层概念，以解决更复杂的问题。首先，我们发现了CNN的架构。CNN是一种ANN，其隐藏层通常由卷积层、池化层、全连接层和归一化层组成。CNN的底层概念得到了覆盖。
- en: We understood training, testing, and evaluating a CNN through the analysis of
    a real case. For this purpose, an HWR problem was addressed in Google Cloud Platform.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过分析一个真实案例来理解CNN的训练、测试和评估。为此，在Google Cloud Platform中解决了一个HWR问题。
- en: Then, we explored RNN. Recurrent networks take, as their input, not only current
    input data that is powered to the network but also what they have experienced
    over time. Several RNN architectures were analyzed. In particular, we focused
    on LSTM networks.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了RNN。循环神经网络不仅接受当前输入数据作为网络输入，还接受它们在一段时间内经历过的数据。分析了几个RNN架构。特别是，我们专注于LSTM网络。
