- en: 'Chapter 13: Governing the ML System for Continual Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will reflect on the need for continual learning in **machine
    learning** (**ML**) solutions. Adaptation is at the core of machine intelligence.
    The better the adaptation, the better the system. Continual learning focuses on
    the external environment and adapts to it. Enabling continual learning for an
    ML system can reap great benefits. We will look at what is needed to successfully
    govern an ML system as we explore continuous learning and study the governance
    component of the Explainable Monitoring Framework, which helps us control and
    govern ML systems to achieve maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: We will delve into the hands-on implementation of governance by enabling alert
    and action features. Next, we will look into ways of assuring quality for models
    and controlling deployments, and we'll learn the best practices to generate model
    audits and reports. Lastly, we will learn about methods to enable model retraining
    and maintain CI/CD pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by reflecting on the need for continual learning and go on to
    explore the following topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for continual learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Governing an ML system using Explainable Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling model retraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining the CI/CD pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for continual learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we got started in [*Chapter 1*](B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015),
    *Fundamentals of MLOps Workflow*, we learned about the reasons AI adoption is
    stunted in organizations. One of the reasons was the lack of continual learning
    in ML systems. Yes, continual learning! We will address this challenge in this
    chapter and make sure we learn how to enable this capability by the end of this
    chapter. Now, let's look into continual learning.
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Continual learning is built on the principle of continuously learning from
    data, human experts, and the external environment. Continual learning enables
    lifelong learning, with adaptation at its core. It enables ML systems to become
    intelligent over time to adapt to the task at hand. It does this by monitoring
    and learning from the environment and the human experts assisting the ML system.
    Continual learning can be a powerful add-on to an ML system. It can allow you
    to realize the maximum potential of an AI system over time. Continual learning
    is highly recommended. Let''s have a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – A loan issuing officer – a traditional system versus an ML
    system assisted by a human'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – A loan issuing scenario – a traditional system versus an ML system
    assisted by a human
  prefs: []
  type: TYPE_NORMAL
- en: There are several advantages to deploying a model (enabled by continual learning)
    compared to having a traditional process in an organization that is fully dependent
    on human employees. For example, in the preceding diagram, we can see the steps
    of a bank's loan approval process in two cases. The first scenario is driven by
    human experts (such as in a traditional bank setup) only. The second scenario
    is where the process is automated or augmented using an ML system to screen applications,
    negotiate, provide loan application finalization (where a human expert reviews
    the ML system's decision and approves or rejects it), and approve the loan. The
    processing time of the traditional setup is 1 week, while the processing time
    of the ML system (working together with human experts) is 6 hours.
  prefs: []
  type: TYPE_NORMAL
- en: The ML system is faster and more sustainable for the bank as it is continually
    learning and improving with a human assistant's help. Human employees have a fixed
    term of employment in a company or job. When they leave, their domain expertise
    is gone, and training a new employee or onboarding a new employee for the same
    task is costly. On the other hand, an ML model working together with or assisted
    by human experts that learns continually as time progresses manages to learn with
    time and retains that knowledge indefinitely (with regards to time). The continual
    learning that's acquired by the ML system (together with human experts) can be
    retained forever by the bank compared to the traditional approach, where human
    employees are constantly changing. Continual learning can unleash great value
    for an ML system and for the business in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: The need for continual learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram shows some of the reasons why continual learning is needed
    and how it will enhance your ML system to maximize your business value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Benefits of continual learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – Benefits of continual learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the benefits of continual learning in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptation**: In most straightforward applications, data drift might stay
    the same as data keeps coming in. However, many applications have dynamically
    changed data drifts, such as recommendation or anomaly detection systems, where
    data keeps flowing. In these cases, continually learning is important for adapting
    and being accurate with predictions. Hence, adapting to the changing nature of
    data and the environment is important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: A white paper published by IDC ([https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf](https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf))
    suggests that by 2025, the rate of data generation will grow to 160 ZB/year, and
    that we will not be able to store all of it. The paper predicts that we will only
    be able to store between 3% and 12%. Data needs to be processed on the fly; otherwise,
    it will be lost since the storage infrastructure cannot keep up with the data
    that is produced. The main trick here is to process incoming data once, store
    only the essential information, and then get rid of the rest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance**: Predictions from ML systems need to be relevant and need to
    adapt to changing contexts. Continual learning is needed to keep the ML systems
    highly relevant to and valuable in the changing contexts and environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Continual learning will enable high performance for the ML
    system, since it powers the ML system to be relevant by adapting to the changing
    data and environment. In other words, being more relevant will improve the performance
    of the ML system, for example, in terms of accuracy or other metrics, by providing
    more meaningful or valuable predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For these reasons, continual learning is needed in an ML system, so without
    continual learning, we cannot reach the maximum value an ML system has to offer.
    In other words, projects are doomed to fail. Continual learning is the key to
    succeeding in AI projects. An efficient governance strategy as part of Explainable
    Monitoring can enable continual learning. An important part of continual learning
    is model retraining, so that we can cope with evolving data and make relevant
    decisions. To do this, we can fuse Explainable Monitoring and model retraining
    to enable continual learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Explainable Monitoring + Model Retraining = Continual Learning**'
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead we will see continual learning in depth. Now, let's explore how
    we can bring efficient governance to ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable monitoring – governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will implement the governance mechanisms that we learned
    about previously in [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Key Principles of Monitoring Your ML System*, for the business use case we have
    been working on. We will delve into three of the components of governing an ML
    system, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Components of governing your ML system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – Components of governing your ML system
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of ML systems results from how they are governed to maximize
    business value. To have end-to-end trackability and comply with legislation, system
    governance requires quality assurance and monitoring, model auditing, and reporting.
    We can regulate and rule ML systems by monitoring and analyzing model outputs.
    Smart warnings and behavior guide governance to optimize business value. Let's
    look at how the ML system's governance is orchestrated by warnings and behavior,
    model quality assurance and control, model auditing, and reports.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts and actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alerts are generated by performing scheduled checks to detect conditions. Upon
    meeting a condition, an alert is generated. Based on the alert that's generated,
    we can perform actions. In this section, we will learn about these elements and
    how they are orchestrated to govern an ML system.
  prefs: []
  type: TYPE_NORMAL
- en: What is an alert?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An alert is a scheduled task running in the background to monitor an application
    to check if specific conditions are being detected. An alert is driven by three
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Schedule**: How often should we check for conditions?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditions**: What needs to be detected?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: What should we do when a condition is detected?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can create alerts based on application performance to monitor aspects such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Alerts for availability based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts for failed requests based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts for server response time based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts for server exceptions based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts based on a threshold for data drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts based on a threshold for model drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts based on errors or exceptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important area of governing ML systems is dealing with errors, so let's turn
    our attention to error handling.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Potential errors are always possible in an application. We can foresee them
    by addressing all the possible edge cases for our ML application. Using the framework
    shown in the following diagram, we can address these errors. The purpose of this
    framework is to identify edge cases and automated debugging methods to tackle
    possible errors. This will keep the ML service up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Framework for debugging and investigating errors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – Framework for debugging and investigating errors
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, we started by identifying resources where
    errors might be present and choosing a resource to address that error. Upon choosing
    a resource, we check for errors by checking for high utilization of resources
    and resource saturation (a resource is saturated when its capacity is fully utilized
    or its capacity is past a set threshold). In the case of either issue, we investigate
    the discovery by investigating logs and devising a solution to handle any errors.
    Eventually, we automate debugging by using premade scripts to handle any issues
    (blocking the system from functioning optimally), for example, by restarting the
    resource or reloading a function or file to get the resource up and running in
    a healthy state.
  prefs: []
  type: TYPE_NORMAL
- en: By addressing all the possible edge cases and devising automated error handling
    or debugging, we can make our applications failure-proof to serve our users. Having
    a failure-proof application enables robustness, making sure the users have a seamless
    experience and value from using the ML application. Once you have identified an
    error, address it by investigating or creating an automated debugging process
    and solving the error. After all, prevention is better than a cure. Hence, checking
    for all possible edge cases and addressing them beforehand can be rewarding.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can handle potential errors by using exception handling functionalities.
    Exception handling is a programming technique that is used for dealing with rare
    situations that necessitate special care. Exception handling for a wide range
    of error types is easy to implement in Python. We can use the `try`, `except`,
    `else`, and `finally` functionalities to handle errors and exceptions, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Handling exceptions and edge cases'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – Handling exceptions and edge cases
  prefs: []
  type: TYPE_NORMAL
- en: All the statements are executed before an exception is encountered in the `try`
    clause. The exception(s) that are found in the `try` clause are caught and treated
    with the `except` block. The `else` block allows you to write parts that can only
    run if there are no exceptions in the `try` clause. Using `finally`, with or without
    any previously experienced exceptions, you can run parts of code that should always
    run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of some possible common exceptions or errors to look out for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These edge cases or errors are common and can be addressed in the application
    by using `try` and `exception` techniques. The strategy is to mitigate situations
    where your ML system will look very basic or naive for the user; for example,
    a chatbot that sends error messages in the chats. In such cases, the cost of errors
    is high, and the users will lose trust in the ML system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will implement some custom exception and error handling for the business
    use case we have been implementing and implement actions based on the alerts that
    are generated. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your Azure DevOps project, go to our `13_Govenance_Continual_Learning`.
    From there, access the `score.py` file. We will begin by importing the required
    libraries. This time, we will use the `applicationinsights` library to track custom
    events or exceptions of Application Insights that are connected to the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As shown in the preceding code, we have imported the `TelemetryClient` function
    from the `applicationinsights` library. We will use the `TelemetryClient` function
    to access the Application Insights that are connected to our endpoint. Provide
    your instrumentation key from Application Insights to the `TelemetryClient` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This **Instrumentation Key** can be accessed from your Application Insights,
    which should be connected to the ML application, as shown in the following screenshot:![Figure
    13.6 – Fetching an instrumentation key from Application Insights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.6 – Fetching an instrumentation key from Application Insights
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After fetching your `TelemetryClient` function, as shown in the following code.
    Here, we create a `TelemetryClient` object in the `tc` variable, which is used
    to track custom events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Likewise, we will implement some other custom events – that is, `ValueNotFound`,
    `OutofBoundsException`, and `InferenceError` – in the `run` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'try:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '# scale incoming data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: data = scaler.transform(data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'except Exception as e:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'try and except can be handy in this case, since we are trying to scale the
    data using a scaler file that''s been loaded in the init function. If scaling
    the data is not successful, then an exception is raised. Here, we use the `track_event`
    function to track the exception on Application Insights. We generate a custom
    event named `ScalingError` in case an exception is generated. An exception message
    and an error code of `301` is logged on Application Insights. Likewise, the most
    important step of dealing with the scoring file – inferencing the model – needs
    to be done meticulously. Now, we will use `try` and `except` again to make sure
    the inference is successful without any exceptions. Let''s see how we can handle
    exceptions in this case. Note that we are accessing element number `2` for the
    `model.run` function. This causes an error in the model''s inference as we are
    referring to an incorrect or nonexistent element of the list:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's look at how to investigate these errors in Application Insights using
    the error logs and generate actions for it.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can set up alerts and actions based on the exception events that we created
    previously (in the *Dealing with errors* section). In this section, we will set
    up an action in the form of an email notification based on an alert that we've
    generated. Whenever an exception or alert is generated in Application Insights,
    we will be notified via an email. Then, we can investigate and solve it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up an action (email) upon receiving an alert by going to Application
    Insights, which should be connected to your ML system endpoint. You can access
    Application Insights via your Azure ML workspace. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to `Endpoints` and check for Application Insights. Once you've accessed the
    Application Insights dashboard, click on `Transaction search`, as shown in the
    following screenshot, to check for your custom event logs (for example, inference
    exception):![Figure 13.7 – Checking the custom event logs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.7 – Checking the custom event logs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can check for custom events that have been generated upon exceptions and
    errors occurs via the logs, and then set up alerts and actions for these custom
    events. To set up an alert and action, go to the **Monitoring** > **Alerts** section
    and click on **New alert rule**, as shown in the following screenshot:![Figure
    13.8 – Setting up a new alert rule
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.8 – Setting up a new alert rule
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, you can create conditions for actions based on alerting. To set up a condition,
    click on **Add condition**. You will be presented with a list of signals or log
    events you can use to make conditions. Select **InferenceError**, as shown in
    the following screenshot:![Figure 13.9 – Configuring a condition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.9 – Configuring a condition
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the signal or event of your choice, you will get to configure
    its condition logic, as shown in the following screenshot. Configure the condition
    by setting up a threshold for it. In this case, we will provide a threshold of
    `400` as the error raises a value of `401` (since we had provided a custom value
    of `401` for the `InferenceError` event). When an inference exception occurs,
    it raises an `InferenceError` with a value above `400` (`401`, to be precise):![Figure
    13.10 – Configuring the condition logic and threshold
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.10 – Configuring the condition logic and threshold
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After setting up the threshold, you will be asked to configure other actions,
    such as running an **Automation Runbook**, **Azure Function**, **Logic App,**
    or **Secure Webhook**, as shown in the following screenshot. For now, we will
    not prompt these actions, but it is good to know that we have them since we can
    run some scripts or applications as a backup mechanism to automate error debugging:![Figure
    13.11 – Actions to automate debugging (optional)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.11 – Actions to automate debugging (optional)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This way, we can automate debugging by having pre-configured scripts or applications
    set up in case an error occurs or to prevent errors. Prevention is better than
    a cure, after all!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, we will create a condition. Click **Review and create** to create the
    condition, as shown in the preceding screenshot. Once you have created this condition,
    you will see it in the **Create alert rule** panel, as shown in the following
    screenshot. Next, set up an action by clicking on **Add action groups** and then
    **Create action group**:![Figure 13.12 – Creating an action group
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.12 – Creating an action group
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Provide an email address so that you can receive notifications, as shown in
    the following screenshot. Here, you can name your notification (in the **Alert
    rule name** field) and provide the necessary information to set up an email alert
    action:![Figure 13.13 – Configuring email notifications
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_13_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.13 – Configuring email notifications
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After providing all the necessary information, including an email, click on
    the **Review + Create** button to configure the action (an email based on an error).
    Finally, provide alert rule details such as **Alert rule name**, **Description**,
    and **Severity**, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.14 – Configuring email notifications'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16572_13_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.14 – Configuring email notifications
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on `InferenceError`). With that, you have created an alert, so now it''s
    time to test it. Go to the `13_Govenance_Continual_Learning` folder and access
    the `test_inference.py` script (replace the URL with your endpoint link). Then,
    run the script by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the script will output an error. Stop the script after performing some
    inferences. Within 5-10 minutes of the error, you will be notified of the error
    via email, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.15 – Email notification of an error in production'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.15 – Email notification of an error in production
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – you have successfully set up an email action alert for an
    error! This way, you can investigate when an error has been discovered in order
    to resolve it and get the system up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at how to ensure we have quality assurance for models and can
    control them in order to maximize business value.
  prefs: []
  type: TYPE_NORMAL
- en: Model QA and control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evolving or dynamically changing data leads to increased prediction error rates.
    This may result from data drift as the business and external environment change,
    or it may be due to data poisoning attacks. This increase in prediction error
    rates results in having to re-evaluate ML models as they are retrained (manually
    or automatically), leading to the discovery of new algorithms that are more accurate
    than the previous ones. Here are some guidelines for testing ML models with new
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable continual learning by retraining your models and evaluating their performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the performance of all the models on a new dataset at periodic intervals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raise an alert when an alternative model starts giving better performance or
    greater accuracy than the existing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain a registry of models containing their latest performance details and
    reports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain end-to-end lineages of all the models to reproduce them or explain
    their performance to stakeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model auditing and reports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Establishing a periodic auditing and reporting system for MLOps is a healthy
    practice as it enables an organization to track its operations end to end, as
    well as comply with the law and enable them to explain its operations to stakeholders
    upon request. We can ensure that the ML system conforms to the conventions that
    have been established and deliberated at the societal and governmental levels.
    To audit and report MLOps, it is recommended for auditors to inspect the fundamentals
    of an audit, shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.16 – Fundamentals of an audit report for ML Operations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.16 – Fundamentals of an audit report for ML Operations
  prefs: []
  type: TYPE_NORMAL
- en: Data audit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data is what drives many of the decisions made by ML systems. Due to this, the
    auditors need to consider data for auditing and reporting, inspecting the training
    data, testing the data, inferring the data, and monitoring the data. This is essential
    and having end-to-end traceability to track the use of data (for example, which
    dataset was used to train which model) is needed for MLOps. Having a *Git for
    Data* type of mechanism that versions data can enable auditors to reference, examine,
    and document the data.
  prefs: []
  type: TYPE_NORMAL
- en: Model audit (fairness and performance)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Auditors of ML systems need to have a hacker's mindset to identify the different
    ways in which a model could fail and not give fair predictions. First, the training
    data is inspected and compared to the inference data using Explainable AI techniques.
    This can help auditors make fair judgments about each model and each of its predictions
    on an individual level. To make fairness and performance assessments for each
    model, we can use data slicing techniques, which can reveal valuable information
    for making an assessment. Due to this, it is valuable for auditors to request
    the results of data slicing for the required demographics and slices of data.
    To make a collective assessment, we can compare models and assess their performance.
    This can reveal another angle of information for making fairness and performance
    assessments.
  prefs: []
  type: TYPE_NORMAL
- en: If a model audit were to proceed, it would assess the model's inputs (training
    data), the model itself, and its outputs. Data consistency and possible biases
    in the training data will need to be assessed. For example, if a resume screening
    model had been trained on previous or historic decisions where candidates had
    received job offers and workers were promoted, we'd want to make sure that the
    training data hasn't been influenced by past recruiters' and managers' implicit
    biases. Benchmarking against competing models, performing statistical tests to
    ensure that the model generalizes from training to unknown results, and using
    state-of-the-art techniques to allow model interpretability are all parts of the
    model evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: Project and governance audit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is it necessary to have a deep understanding of AI models to audit algorithms?
    Certainly not. An audit of an AI system's progress is like a project management
    audit. Is there a clear target for the desired achievement? This is a good and
    straightforward question to ask if a government entity has implemented AI in a
    particular environment. Furthermore, is there a viable framework to manage the
    model after the developers leave, if external developers have been applied to
    the AI system? To reduce the need for specialist expertise, the company must have
    extensive documentation of concept creation and the staff who are familiar with
    the model. Hence, auditing the development and governance practices can be rewarding
    in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Auditing data considerations, model fairness and performance, and project management
    and governance of ML systems can provide a comprehensive view of MLOps. Using
    error alerts and actions, we can perform timely investigations into errors to
    get the system up and running and in some cases, we can even do automated debugging
    to automate error resolution and MLOps. Finally, by undertaking model quality
    assurance, control, and auditing, we can ensure efficient governance of our MLOps.
    Next, we will look at how to enable model retraining so that we have continual
    learning capabilities for our ML system.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling model retraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve talked about what model drift is and how to recognize it. So,
    the question is, what should we do about it? If a model''s predictive performance
    has deteriorated due to changes in the environment, the solution is to retrain
    the model using a new training set that represents the current situation. How
    much should your model be retrained by? And how can you choose your new workout
    routine? The following diagram shows the **Model retrain** function triggering
    the **Build** module based on the results of the **Monitor** module. There are
    two ways to trigger the model retrain function. One is manually and the other
    is by automating the model retraining function. Let''s see how we can enable both:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.17 – Model retraining enabled in an MLOps workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.17 – Model retraining enabled in an MLOps workflow
  prefs: []
  type: TYPE_NORMAL
- en: Manual model retraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The product owner or quality assurance manager has the responsibility of ensuring
    manual model retraining is successful. The manual model triggering step involves
    evaluating model drift and if it goes above a threshold (you need to determine
    a threshold for drift that will trigger model retraining), then they must trigger
    the model training process by training the model using a new dataset (this can
    be the previous training dataset and the latest inference data). This way, the
    product owner or quality assurance manager has full control over the process,
    and also knows when and how to trigger model retraining to deliver maximized value
    from the ML system.
  prefs: []
  type: TYPE_NORMAL
- en: Automated model retraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to fully automate the MLOps pipeline, automating model drift management
    can be an ideal approach to retraining the production model. Automating model
    drift management is done by configuring batch jobs that monitor application diagnostics
    and model performance. Then, you must activate model retraining. A key part of
    automating model drift management is setting the threshold that will automatically
    trigger the retraining model function. If the drift monitoring threshold is set
    too low, you run the risk of having to retrain too often, which will result in
    high compute costs. If the threshold is set too high, you risk not retraining
    often enough, resulting in suboptimal production models. Figuring out the right
    threshold is trickier than it seems because you have to figure out how much additional
    training data you'll need to reflect this new reality. Even if the environment
    has changed, replacing an existing model with one that has a very small training
    set is pointless. Once you have the threshold figured out, you could have jobs
    (for example, as part of the CI/CD pipeline) that compare the feature distributions
    of live datasets to those of training data on a regular basis (as we did in [*Chapter
    12*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222), *Model Serving and Monitoring*).
    When a large deviation is detected (or above the defined threshold), the system
    can schedule model retraining and deploy a new model automatically. This can be
    done using a work scheduler such as Jenkins or Kubernetes Jobs or CI/CD pipeline
    cron jobs. This way, you can fully automate the MLOps pipeline and the model retraining
    part.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it doesn't make sense to retrain the model in cases where there is
    low new data coming in or if you are doing batch inference once in a blue moon
    (for example, every 6 months). You can train the model before inference or periodically
    whenever you need to.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining the CI/CD pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may recall, in [*Chapter 10*](B16572_10_Final_JM_ePub.xhtml#_idTextAnchor189)*,
    Essentials of Production Release*, we mentioned that *a model is not the product;
    the pipeline is the product*. Hence, after setting up automated or semi-automated
    CI/CD pipelines, it is critical to monitor the performance of our pipeline. We
    can do that by inspecting the releases in Azure DevOps, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.18 – Maintaining CI/CD pipeline releases'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_13_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.18 – Maintaining CI/CD pipeline releases
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of an inspection is to keep the CI/CD pipeline in a healthy and robust
    state. Here are some guidelines for keeping the CI/CD pipeline healthy and robust:'
  prefs: []
  type: TYPE_NORMAL
- en: If a build is broken, a **fix it asap** policy from the team should be implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate automated acceptance tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Require pull requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peer code review each story or feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audit system logs and events periodically (recommended).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularly report metrics visibly to all the team members (for example, slackbot
    or email notifications).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these practices, we can avoid high failure rates and make the
    CI/CD pipeline robust, scalable, and transparent for all the team members.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the key principles of continual learning in
    ML solutions. We learned about Explainable Monitoring (the governance component)
    by implementing hands-on error handling and configuring actions to alert developers
    of ML systems using email notifications. Lastly, we looked at ways to enable model
    retraining and how to maintain the CI/CD pipeline. With this, you have been equipped
    with the critical skills to automate and govern MLOps for your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on finishing this book! The world of MLOps is constantly evolving
    for the better. You are now equipped to help your business thrive using MLOps.
    I hope you enjoyed reading and learning by completing the hands-on MLOps implementations.
    Go out there and be the change you wish to see. All the best with your MLOps endeavors!
  prefs: []
  type: TYPE_NORMAL
