- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conformal Prediction for Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) grapples with the complexities of
    human language, where uncertainty is an inherent challenge. As NLP models become
    integral to risk-sensitive and critical applications, ensuring their reliability
    is paramount. Conformal prediction emerges as a promising technique, offering
    a way to quantify the trustworthiness of these models’ predictions, particularly
    when faced with miscalibrated outputs from deep learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will navigate the NLP conformal prediction world, understand
    its significance, and learn how to harness its power for more reliable and confident
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why deep learning produces miscalibrated predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various approaches to quantify uncertainty in NLP problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conformal prediction for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building NLP classifiers using conformal prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source tools for conformal prediction in NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty quantification for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Uncertainty quantification in NLP is an essential yet often overlooked aspect
    of model development and deployment. As NLP models become increasingly integrated
    into critical applications—from healthcare diagnostics to financial predictions—the
    need to understand and convey the confidence level of their outputs becomes paramount.
    Uncertainty quantification provides a framework for assessing the reliability
    of predictions, allowing users and developers to gauge the model’s decisiveness
    and the potential risks of relying on its results. This section delves into the
    importance, methodologies, and practical considerations of uncertainty quantification
    in NLP, highlighting its pivotal role in building robust and trustworthy language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore uncertainty in NLP and the benefits and challenges of quantifying
    uncertainty in NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: What is uncertainty in NLP?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP, at its core, is about making sense of human language—a medium known for
    its richness, ambiguity, and diversity. The inherent variability in language usage,
    context-driven meanings, and the ever-evolving nature of linguistic constructs
    make NLP tasks inherently uncertain. For instance, the word “bank” could refer
    to a financial institution or the side of a river depending on the context.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of quantifying uncertainty in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quantifying uncertainty in NLP is not just a theoretical exercise; it has the
    following tangible benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trustworthiness**: Quantifying uncertainty either bolsters confidence in
    specific predictions or highlights areas of caution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance evaluation**: This assesses the efficacy of various models by
    examining the uncertainty in their metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancement opportunities**: It can recognize areas where a model can be
    refined, especially in contexts such as active learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk management**: By understanding the degree of uncertainty in predictions,
    stakeholders can make more informed decisions. For instance, an NLP model predicting
    sentiment might be 80% certain that a review is positive. Knowing this, a business
    might prioritize addressing reviews where the model’s certainty is lower.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model transparency**: A model that can express its uncertainty is perceived
    as more transparent and trustworthy. Users of the model can better understand
    when to trust the model’s output and when to approach it with caution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training**: During the training phase, understanding areas of high
    uncertainty can guide data collection efforts. If a model is uncertain about a
    particular data type, gathering more of that data can lead to more robust training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenges of uncertainty in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite its importance, managing uncertainty in NLP is challenging. Here are
    some reasons why:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data sparsity**: Many NLP tasks lack representative data for all possible
    linguistic variations, leading to models that are uncertain about less common
    data points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ambiguity in language**: As mentioned, words can have multiple meanings based
    on context, leading to inherent uncertainty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model complexity**: Advanced models such as deep learning networks can sometimes
    act as black boxes, making it challenging to discern areas of uncertainty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In building robust NLP systems, understanding and quantifying uncertainty becomes
    paramount. As we dive deeper into the chapter, we’ll explore techniques, particularly
    **conformal prediction**, that offer a structured approach to tackle these challenges
    head-on.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding why deep learning produces miscalibrated predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rapidly evolving field of NLP, deep learning played a pivotal role in
    enabling machines to process and generate language in ways that were once the
    exclusive domain of humans. The next section introduces the key concepts and milestones
    in deep learning that has significantly influenced NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to deep learning in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Deep learning**, a subset of machine learning, relies on neural networks
    with many layers (hence “deep”) to analyze various data factors. In the context
    of NLP, deep learning has been a game-changer, enabling machines to understand
    and generate human language with unprecedented accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evolution of architectures**: The journey began with simpler architectures
    such as feedforward neural networks and **recurrent neural networks** (**RNNs**).
    With its ability to remember past information, the latter was particularly influential
    in sequence-based tasks such as language translation. Later, more advanced architectures
    such as **long short-term memory** (**LSTM**) and the **Transformer model** further
    elevated performance standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT and transformers**: The introduction of **Bidirectional Encoder Representations
    from Transformers** (**BERT**) marked a significant milestone. BERT achieved state-of-the-art
    results in numerous NLP tasks by analyzing words concerning their entire context
    (both left and right of a word). The Transformer architecture, which BERT is based
    on, introduced attention mechanisms that allow models to focus on specific parts
    of the input text, much like how humans pay attention to particular words when
    comprehending language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language models and LLMs**: **Large language models** (**LLMs**) such as
    **generative pre-trained transformer** (**GPT**) and its iterations, such as ChatGPT,
    have set new standards in NLP. With billions of parameters, these models can generate
    human-like text, answer questions, and even assist in creative writing. ChatGPT,
    in particular, has been influential in creating conversational agents capable
    of more natural and coherent interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning and fine-tuning**: One of the revolutionary aspects of
    these developments is the idea of transfer learning. Models such as BERT and GPT
    are pre-trained on vast corpora and can be fine-tuned on specific tasks with smaller
    datasets. This approach has democratized deep learning in NLP, allowing teams
    with limited resources to achieve competitive results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these advancements, deep learning models have become the backbone of many
    modern NLP applications, from chatbots to search engines. However, as we’ll explore
    in the subsequent sections, their complexity and sheer scale introduce challenges,
    especially in calibration.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with deep learning predictions in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning has undeniably advanced the capabilities of NLP, but it also
    brings forth several challenges and pitfalls. As we navigate the landscape of
    deep learning in NLP, we must be aware of these issues. Some of the notable challenges
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model overconfidence**: Deep learning models, given their capacity to fit
    complex patterns, often become overconfident in their predictions. For instance,
    in sentiment analysis, a model might predict a text as positive with 60% confidence
    when, in reality, the actual confidence should be much lower due to ambiguous
    phrasing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data distribution shift**: NLP models are often trained on specific datasets
    and may not be exposed to the full linguistic diversity of real-world inputs.
    When faced with out-of-distribution data, these models can produce miscalibrated
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of explicit uncertainty modeling**: Traditional deep learning approaches
    don’t inherently model uncertainty. They optimize for accuracy, often at the cost
    of a reliable uncertainty estimate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity and non-linearity**: The intricate architectures of deep learning
    models, especially with multiple layers and non-linear activations, can sometimes
    lead to unpredictable behavior, especially when handling edge cases or rare linguistic
    constructs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through the implications of miscalibration next.
  prefs: []
  type: TYPE_NORMAL
- en: The implications of miscalibration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Miscalibration in NLP models is more than just a mere academic concern. In
    real-world applications, it can lead to misinformed decisions, misplaced trust,
    and even potentially harmful outcomes, especially in sensitive areas such as healthcare,
    finance, and legal systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision-making risks**: Overconfident models can lead stakeholders to make
    decisions based on misguided confidence, potentially causing miscommunications
    or flawed strategies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss of trust**: Users might lose faith in an NLP system if it frequently
    expresses high confidence in incorrect predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource misallocation**: In automated systems, a miscalibrated model might
    prioritize tasks inefficiently, wasting computational resources on tasks where
    human intervention would have been more appropriate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing these challenges is the first step. As we progress, we’ll delve
    into conformal prediction—a technique that presents a viable solution to the miscalibration
    issues plaguing deep learning models in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Various approaches to quantify uncertainty in NLP problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple methods to quantify uncertainty in NLP problems have been explored
    to address the challenges of miscalibration and language’s inherent unpredictability.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at Bayesian approaches to UQ.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian approaches to uncertainty quantification
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian methods provide a framework for modeling uncertainty. By treating model
    parameters as distributions rather than fixed values, Bayesian neural networks
    offer a measure of uncertainty associated with predictions. This probabilistic
    approach ensures that the model not only gives an estimate but also conveys the
    confidence or spread of that estimate.
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the examples of Bayesian approaches to UQ.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variational inference** is a technique to approximate the posterior distribution
    of the model parameters, enabling the network to output distributions for predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian neural networks** (**BNNs**) are neural networks with weights assigned
    to probability distributions. By sampling from these distributions, BNNs can produce
    a range of outputs, reflecting the uncertainty in predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo dropout** is a technique wherein dropout is applied during inference.
    We can gain insight into the model’s uncertainty by running the model multiple
    times and observing the variance in outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrap methods and ensemble techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bootstrapping involves creating multiple datasets from the original training
    data through resampling. By training separate models on these datasets, we can
    capture model uncertainty. This variance across different resamples allows for
    a more robust evaluation of how changes in the input data can impact predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at some of the examples of bootstrap methods and model ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: Short for bootstrap aggregating, this involves training multiple
    models on different bootstrap samples. The variance in predictions across models
    provides an estimate of uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model ensembles**: Combining predictions from multiple models can also capture
    uncertainty. If models trained on the same data but with different architectures
    disagree on a prediction, it indicates higher uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out-of-distribution (OOD) detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Identifying inputs that are significantly different from the training data
    can also help in uncertainty estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Likelihood-based methods**: These methods compare the likelihood of new data
    points to the training data. Lower likelihood indicates higher uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial training**: By training models to recognize adversarial examples,
    we can enhance their ability to identify uncertain inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and appropriately employing these techniques is crucial in NLP,
    given human language’s inherent ambiguities and nuances. Each approach has its
    strengths and suitable scenarios, so practitioners must choose wisely based on
    the specifics of their NLP task.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conformal prediction is a flexible and statistically robust approach to uncertainty
    quantification. It is a distribution-free framework that can estimate uncertainty
    for machine learning models without requiring model retraining or access to limited
    APIs. The central idea behind conformal prediction is to output a set of predictions
    containing the correct output with a user-specified probability. Conformal prediction
    can help quantify the uncertainty associated with the model’s predictions in language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction is a framework that delivers valid confidence intervals
    for predictions, irrespective of the underlying machine learning model. In the
    NLP landscape, with its inherent challenges of ambiguity, context sensitivity,
    and linguistic diversity, conformal prediction offers a structured way to quantify
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '**Validity and efficiency** are the two fundamental principles of conformal
    prediction. Validity ensures that the prediction regions (or sets) are correct
    with a predefined probability, while efficiency ensures these regions are as tight
    as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: How conformal prediction works in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mechanics of conformal prediction are rooted in ordering predictions based
    on their “strangeness” or non-conformity scores. The idea is to understand how
    different a new observation is compared to previous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-conformity score**: This score measures how different a new prediction
    is from previous predictions for any NLP task. For instance, the non-conformity
    might be based on the distance from the decision boundary in text classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P-values**: P-values are calculated based on the non-conformity scores, representing
    the confidence level of predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical applications of conformal prediction in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Conformal prediction isn’t just a theoretical construct; its practical applications
    in NLP are wide-ranging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: When determining the sentiment of a text snippet, conformal
    prediction can provide a range or set of possible sentiments, each with its confidence
    level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named entity recognition**: Conformal prediction can give a confidence score
    on each tagged entity instead of just tagging entities, helping in tasks where
    precision is critical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation**: Beyond translating text, conformal prediction can
    offer confidence intervals for different translation choices, aiding in tasks
    where mistranslations can have significant consequences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of using conformal prediction in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Conformal prediction, a relatively recent development in uncertainty quantification,
    brings a fresh perspective and many benefits to NLP. As we venture into an era
    where the demand for reliable and trustworthy models is ever-increasing, methods
    such as conformal prediction stand out, promising to address some innate challenges
    in NLP. Let’s delve into the distinct advantages of integrating conformal prediction
    in NLP tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model agnostic**: One of the strengths of conformal prediction is its compatibility
    with any machine learning model. Conformal prediction can be applied to any statistical,
    machine, or deep learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparent and interpretable**: Conformal prediction doesn’t operate as
    a black box. The non-conformity scores and resulting p-values offer interpretable
    metrics of uncertainty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive**: Conformal prediction is adaptive to the data it’s applied to.
    It doesn’t make strong distributional assumptions, making it robust despite diverse
    linguistic data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The introduction of conformal prediction into the NLP toolkit offers a promising
    avenue for practitioners to handle the inherent uncertainties of human language.
    Providing valid and reliable confidence measures helps build more robust and trustworthy
    NLP systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of applied conformal prediction on an NLP task, such as IMDB Movie
    reviews that have been pre-labeled with “positive” and “negative” sentiment class
    labels based on the review content, has been discussed here: [https://github.com/M-Soundouss/density_based_conformal_prediction/tree/master/imdb](https://github.com/M-Soundouss/density_based_conformal_prediction/tree/master/imdb).'
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction for NLP and LLMs is an emerging and crucial area of research.
  prefs: []
  type: TYPE_NORMAL
- en: A notable contribution to this field is a paper by Kumar et.al., titled *Conformal
    Prediction with Large Language Models for Multi-Choice Question* *Answering* ([https://arxiv.org/abs/2305.18404](https://arxiv.org/abs/2305.18404)).
  prefs: []
  type: TYPE_NORMAL
- en: This paper dives deep into how conformal prediction can be instrumental in quantifying
    uncertainty in language models, thereby paving the way for a more trustworthy
    and reliable deployment of large language models, especially in scenarios where
    safety is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: The paper’s primary focus is on multiple-choice question-answering tasks. Through
    a series of experiments, it showcases the efficacy of conformal prediction in
    deriving uncertainty estimates that are in strong correlation with prediction
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the experimental setup, the authors employed the LLaMA-13B model.
    This model, boasting 13 billion parameters and trained on a staggering 1 trillion
    tokens, generated predictions for MCQA questions sourced from the `MMLU benchmark`
    dataset ([https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)).
  prefs: []
  type: TYPE_NORMAL
- en: The experiments were structured around a calibration set that trained the conformal
    prediction model and an evaluation set that tested the model’s prowess. A cross-validation
    approach was adopted to ensure the experiment’s integrity, ensuring the calibration
    and evaluation sets were sampled from a consistent distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The performance metrics were multifaceted, encompassing accuracy, coverage,
    and efficiency. A pivotal observation was that the softmax outputs from the LLaMA-13B
    model, while reasonably calibrated on average, exhibited tendencies of underconfidence
    and overconfidence, particularly at the extremities of the probability distribution.
    This observation was particularly pronounced in subjects such as formal logic
    and college chemistry, which inherently possess more ambiguity and complexity,
    making them challenging for LLMs to navigate accurately.
  prefs: []
  type: TYPE_NORMAL
- en: One of the standout findings was the strong correlation between the uncertainty
    estimates provided by conformal prediction and prediction accuracy. Such a correlation
    implies that when the model exhibits higher uncertainty about its predictions,
    it’s more prone to errors. This insight is invaluable for downstream applications
    such as selective classification. By leveraging these uncertainty estimates, it’s
    feasible to filter out lower-quality predictions, thereby enhancing the overall
    user experience.
  prefs: []
  type: TYPE_NORMAL
- en: The paper underscores the potential of conformal prediction as a beacon for
    uncertainty quantification in LLMs. By integrating this approach, LLMs can be
    more reliable, especially in high-stakes environments, reinforcing their trustworthiness
    and broadening their applicability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second critical paper is *Robots That Ask For Help: Uncertainty Alignment
    for Large Language Model Planners* ([https://robot-help.github.io](https://robot-help.github.io)),
    published by a team of researchers from Princeton University and DeepMind.'
  prefs: []
  type: TYPE_NORMAL
- en: In robotics and artificial intelligence, the aspiration to equip robots with
    the capability to discern when uncertain is a pivotal challenge. The paper addresses
    this challenge, particularly regarding robots instructed via language. With its
    inherent flexibility, language offers a natural interface for humans to convey
    tasks, contextual information, and intentions. It also facilitates humans in providing
    clarifications to robots when they encounter uncertainties.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements have showcased the potential of LLMs in planning. These
    models can interpret and respond to unstructured language instructions, generating
    temporally extended plans. The strength of these LLMs lies in their ability to
    harness the vast knowledge and rich context they have been pre-trained with, leading
    to enhanced abstract reasoning capabilities. However, a significant impediment
    with current LLMs is their propensity to “hallucinate.” In other words, they tend
    to generate outputs with high confidence that, while plausible, might be incorrect
    and not anchored in reality.
  prefs: []
  type: TYPE_NORMAL
- en: Such unwarranted confidence in outputs can be detrimental, especially in LLM-based
    robotic planning. This is further exacerbated when natural language instructions,
    often riddled with inherent or unintentional ambiguities, are provided in real-world
    settings. Misinterpreting such instructions can lead to undesirable or, in extreme
    cases, unsafe actions.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, the paper presents an example where a robot tasked with heating
    food is instructed to place a bowl in the microwave. In scenarios where multiple
    bowls are present, such an instruction becomes ambiguous. Moreover, if one of
    the bowls is metallic, placing it in the microwave would be hazardous. Instead
    of acting on such vague instructions, the ideal robot should recognize its uncertainty
    and seek clarification. While previous works in language-based planning have either
    overlooked the need for such clarifications or relied heavily on extensive prompting,
    this paper introduces **KNOWNO**.
  prefs: []
  type: TYPE_NORMAL
- en: KNOWNO is a framework designed to measure and align the uncertainty of LLM-based
    planners. It ensures that these planners know their limitations and seek assistance
    when required. The foundation of KNOWNO is built on the theory of conformal prediction,
    which offers statistical guarantees on task completion while minimizing the need
    for human intervention in intricate multi-step planning settings. Experiments
    across various simulated and real robot setups demonstrate the framework’s efficacy.
    These experiments encompass tasks with diverse modes of ambiguity, ranging from
    spatial uncertainties to numeric ones and from human preferences to Winograd schemas.
  prefs: []
  type: TYPE_NORMAL
- en: The paper posits KNOWNO as a promising lightweight approach to model uncertainty.
    It can seamlessly complement and scale with the burgeoning capabilities of foundational
    models. LLMs can be more reliable by leveraging conformal prediction, especially
    when precision and safety are paramount.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the chapter, we have explored the inherent uncertainty challenges in the
    NLP domain. Recognizing the pivotal role of NLP models in today’s critical systems,
    the chapter emphasizes the importance of ensuring these models’ predictions are
    trustworthy and reliable. The chapter introduces conformal prediction as a solution
    to address the miscalibration seen in deep learning models’ outputs, offering
    a means to quantify the confidence of predictions robustly. Throughout this chapter,
    you gained insights into the intricacies of uncertainty quantification specific
    to NLP, the reasons why deep learning models often produce miscalibrated predictions,
    and various methods of quantifying uncertainty in NLP. Finally, we deeply studied
    the conformal prediction technique tailored for NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, you should have a holistic understanding of the
    challenges of uncertainty in NLP, the merits and mechanics of conformal prediction,
    and practical knowledge to apply this technique to NLP problems effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deep into the intriguing world of imbalanced
    data and show how conformal prediction can address existing challenges in handling
    such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Advanced Topics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will provide illustrations on how conformal prediction can be used
    to solve imbalanced data problems, introducing you to various conformal prediction
    methods that can be used for multi-class classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19925_11.xhtml#_idTextAnchor149), *Handling Imbalanced Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19925_12.xhtml#_idTextAnchor159), *Multi-Class Conformal Prediction*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
