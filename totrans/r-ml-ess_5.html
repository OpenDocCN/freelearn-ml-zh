<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Step 2 &#x2013; Applying Machine Learning Techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Step 2 – Applying Machine Learning Techniques</h1></div></div></div><p>This chapter focuses on applying the machine learning algorithm, and it is the core of developing the solution. There are different types of techniques that learn from the data. Depending on our target, we can use the data to identify similarities between objects or to estimate an attribute on new objects.</p><p>In order to show the machine learning techniques, we start from the flag data that we processed in the previous chapter. However, reading this chapter doesn't require you to know about the previous, although it is recommended to understand where the data came from.</p><p>In this chapter you will learn to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Identify homogeneous groups of items</li><li class="listitem" style="list-style-type: disc">Explore and visualize the item groups</li><li class="listitem" style="list-style-type: disc">Estimate a new country language</li><li class="listitem" style="list-style-type: disc">Set the configuration of a machine learning technique</li></ul></div><div class="section" title="Identifying a homogeneous group of items"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Identifying a homogeneous group of items</h1></div></div></div><p>Our data <a id="id241" class="indexterm"/>describes each country flag. Is there any way to identify groups of countries with similar flag attributes? We can use some clustering techniques that are machine learning algorithms that define homogeneous clusters using the data.</p><p>Starting from the flag attributes, in the previous chapter, we built a feature table and we stored it into the <code class="literal">dtFeatures.txt</code> file. In order to load the file into R, the first step is to define the directory containing the file using <code class="literal">setwd</code>. Then, we can load the file into the <code class="literal">dfFeatures</code> data frame using <code class="literal">read.table</code>, and we can convert it into the <code class="literal">dtFeatures</code> data table, as shown:</p><div class="informalexample"><pre class="programlisting"># load the flag features
setwd('&lt;INSER YOUR DIRECTORY/PATH&gt;")
dfFeatures &lt;- read.table(file = 'dtFeatures.txt')
library("data.table")
dtFeatures &lt;- data.table(dfFeatures)</pre></div><p>Let's take a look at the data using <code class="literal">str</code>, similar to the previous chapters:</p><div class="informalexample"><pre class="programlisting"># explore the features
str(dtFeatures)
<span class="strong"><strong>Classes 'data.table' and 'data.frame':	194 obs. of  38 variables:</strong></span>
<span class="strong"><strong> $ language     : Factor w/ 10 levels "Arabic","Chinese",..: 8 7 1 3 7 8 3 3 10 10 ...</strong></span>
<span class="strong"><strong> $ red          : Factor w/ 2 levels "no","yes": 2 2 2 2 2 2 1 2 1 1 ...</strong></span>
<span class="strong"><strong> $ green        : Factor w/ 2 levels "no","yes": 2 1 2 1 1 1 1 1 1 1 ...</strong></span>
<span class="strong"><strong> $ blue         : Factor w/ 2 levels "no","yes": 1 1 1 2 2 1 2 2 2 2 ...</strong></span>
</pre></div><p>The language column is a factor and there are 10 languages, called <code class="literal">levels</code> of the factor. All the other <a id="id242" class="indexterm"/>columns contain features describing the flags and they are factors with two levels: <code class="literal">yes</code> and <code class="literal">no</code>. The features are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">colors</code> feature (for example, <code class="literal">red</code>) has a <code class="literal">yes</code> level if the flag contains the color</li><li class="listitem" style="list-style-type: disc">The <code class="literal">patterns</code> feature (for example, <code class="literal">circle</code>) has a <code class="literal">yes</code> level if the flag contains the pattern</li><li class="listitem" style="list-style-type: disc">The <code class="literal">nBars</code>/<code class="literal">nStrp</code>/<code class="literal">nCol</code> features followed by a number (for example, <code class="literal">nBars3</code>) have a  <code class="literal">yes</code> level if the flag has 3 bars</li><li class="listitem" style="list-style-type: disc">The <code class="literal">topleft</code>/<code class="literal">botright</code>/<code class="literal">mainhue</code> features followed by a color (for example, <code class="literal">topleftblue</code>) have a <code class="literal">yes</code> level if the top-left part is blue</li></ul></div><div class="section" title="Identifying the groups using k-means"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec34"/>Identifying the groups using k-means</h2></div></div></div><p>Our target is to <a id="id243" class="indexterm"/>identify groups of similar flags. For this <a id="id244" class="indexterm"/>purpose, we can start using a basic clustering algorithm, that is, <span class="strong"><strong>k-means</strong></span>.</p><p>The <a id="id245" class="indexterm"/>k-means target is to identify <span class="emphasis"><em>k</em></span> (for example, eight) homogeneous clusters of flags. Imagine dividing all the flags in eight clusters. One of them includes 10 flags out of which seven contain the color red. Let's suppose that we have a <code class="literal">red</code> attribute that is <code class="literal">1</code> if the flag contains red and <code class="literal">0</code> otherwise. We can say that the <code class="literal">average flag</code> of this cluster contains <code class="literal">red</code> with a probability of 70 percent, so its <code class="literal">red</code> attribute is 0.7. Doing the same with every other attribute, we can define <code class="literal">average flag</code>, whose attributes are the average within the group. Each cluster has an average flag that we can determine using the same approach.</p><p>The k-means algorithm is based on an average object that is called the cluster center. At the beginning, the algorithm divides the flags into 8 random groups and determines their 8 centers. Then, k-means reassigns each flag to the group whose center is the most similar. In this way, the clusters are more homogeneous and the algorithm can recompute their centers. After a few iterations, we have 8 groups containing homogeneous flags.</p><p>The <a id="id246" class="indexterm"/>k-means algorithm is a very popular technique and R provides us with the <code class="literal">kmeans</code> function. In order to use it, we can take a look at its help:</p><div class="informalexample"><pre class="programlisting"># K-MEANS
# see the function documentation
help(kmeans)</pre></div><p>We need two inputs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">x</code>: A numeric data matrix</li><li class="listitem" style="list-style-type: disc"><code class="literal">centers</code>: The number of clusters (or the cluster centers to start with)</li></ul></div><p>Starting from <code class="literal">dtFeatures</code>, we need to build a numeric feature matrix <code class="literal">dtFeaturesKm</code>. First, we can put the feature names into <code class="literal">arrayFeatures</code> and generate the <code class="literal">dtFeaturesKm</code> data <a id="id247" class="indexterm"/>table containing all the features. Perform <a id="id248" class="indexterm"/>the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define <code class="literal">arrayFeatures</code> that is a vector containing the feature name. The <code class="literal">dtFeatures</code> method contains the attribute in the first column and the features in the others, so we extract all the column names apart from the first:<div class="informalexample"><pre class="programlisting">arrayFeatures &lt;- names(dtFeatures)[-1]</pre></div></li><li class="listitem">Define <code class="literal">dtFeaturesKm</code> containing the features:<div class="informalexample"><pre class="programlisting">dtFeaturesKm &lt;- dtFeatures[, arrayFeatures, with=F]</pre></div></li><li class="listitem">Convert a generic column (for example, <code class="literal">red</code>) into the numeric format. We can use <code class="literal">as.numeric</code> to convert the column format from factor into numeric:<div class="informalexample"><pre class="programlisting">dtFeaturesKm[, as.numeric(red)]</pre></div></li><li class="listitem">The new vector contains <code class="literal">1</code> if the value is <code class="literal">no</code> and <code class="literal">2</code> if the value is <code class="literal">yes</code>. In order to use the same standards as our k-means descriptions, we prefer to have <code class="literal">0</code> if the attribute is <code class="literal">no</code> and <code class="literal">1</code> if the attribute is <code class="literal">yes</code>. In this way, when we are computing the average attribute within a group, it will be a number between 0 and 1 that can be seen as a portion of flags whose attribute is <code class="literal">yes</code>. Then, in order to have 0 and 1, we can use <code class="literal">as.numeric(red) – 1</code>:<div class="informalexample"><pre class="programlisting">dtFeaturesKm[, as.numeric(red) - 1]</pre></div><p>Alternatively, we could have done the same using the ifelse function.</p></li><li class="listitem">We need to convert each column format into 0-1. The <code class="literal">arrayFeatures</code> data table contains names of all the features and we can process each of them using a <code class="literal">for</code> loop. If we want to transform a column whose name is contained in <code class="literal">nameCol</code>, we need to use the <code class="literal">eval</code>-<code class="literal">get</code> notation. With <code class="literal">eval(nameCol) :=</code> we redefine the column, and with <code class="literal">get(nameCol)</code> we use the current value of the column, as shown:<div class="informalexample"><pre class="programlisting">for(nameCol in arrayFeatures)
  dtFeaturesKm[
    , eval(nameCol) := as.numeric(get(nameCol)) - 1
    ]</pre></div></li><li class="listitem">Now convert all the features in the 0-1 format. Let's visualize it:<div class="informalexample"><pre class="programlisting">View(dtFeaturesKm)</pre></div></li><li class="listitem">The <code class="literal">kmeans</code> function requires the data to be in the matrix form. In order to convert <code class="literal">dtFeaturesKm</code> into a matrix, we can use <code class="literal">as.matrix</code>:<div class="informalexample"><pre class="programlisting">matrixFeatures &lt;- as.matrix(dtFeaturesKm)</pre></div></li></ol></div><p>The <code class="literal">matrixFeatures</code> data table contains data to build the k-means algorithm and the other <code class="literal">kmeans</code> inputs <a id="id249" class="indexterm"/>are the parameters. The k-means algorithm doesn't automatically detect the number of clusters, so we need to specify it through the <code class="literal">centers</code> input. Given the set of objects, we can identify any number of clusters out of them. Which is the number that reflects the data most? There are some techniques that allow us to define it, but they're out of the scope of this chapter. We can just <a id="id250" class="indexterm"/>define a reasonable number of <a id="id251" class="indexterm"/>centers, for example, 8:</p><div class="informalexample"><pre class="programlisting"># cluster the data using the k-means
nCenters &lt;- 8
modelKm &lt;- kmeans(
  x = matrixFeatures,
  centers = nCenters
  )</pre></div><p>The <code class="literal">modelKm</code> function <a id="id252" class="indexterm"/>is a list containing different <a id="id253" class="indexterm"/>model components. The help of <code class="literal">kmeans</code> provides us with a detailed description of the output and we can use <code class="literal">names</code> to get the element names. Let's see the components:</p><div class="informalexample"><pre class="programlisting">names(modelKm)
<span class="strong"><strong>[1] "cluster"      "centers"      "totss"        "withinss"    </strong></span>
<span class="strong"><strong>[5] "tot.withinss" "betweenss"    "size"         "iter"        </strong></span>
<span class="strong"><strong>[9] "ifault"      </strong></span>
</pre></div><p>We can visualize the cluster centers that are contained in <code class="literal">centers</code>, as shown:</p><div class="informalexample"><pre class="programlisting">View(modelKm$centers)</pre></div><p>Each row defines a center and each column shows an attribute. All the attributes are between 0 and 1, and they represent the percentage of flags in the cluster with an attribute equal to <code class="literal">1</code>. For instance, if <code class="literal">red</code> is <code class="literal">0.5</code>, it means that half of the flags contain the color red.</p><p>The <a id="id254" class="indexterm"/>element that we will use is <code class="literal">cluster</code> and it contains a label specifying the cluster of each flag. For <a id="id255" class="indexterm"/>instance, if the first element of a cluster is <code class="literal">3</code>, this <a id="id256" class="indexterm"/>means that the first flag in <code class="literal">matrixFeatures</code> (and also in <code class="literal">dtFeatures</code>) belongs to the third cluster.</p><div class="section" title="Exploring the clusters"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec01"/>Exploring the clusters</h3></div></div></div><p>We can take a <a id="id257" class="indexterm"/>look at each cluster in order to explore its flags. In order <a id="id258" class="indexterm"/>to do that, we can add the cluster to the initial table by defining the <code class="literal">clusterKm</code> column, as shown:</p><div class="informalexample"><pre class="programlisting"># add the cluster to the data table
dtFeatures[, clusterKm := modelKm$cluster]</pre></div><p>In order to explore a cluster, we can determine how many of its countries speak each language. Starting from <code class="literal">dtFeatures</code>, we can summarize the data about each cluster using data table aggregation. First, let's define the column that contains the cluster:</p><div class="informalexample"><pre class="programlisting"># aggregate the data by cluster
nameCluster &lt;- 'clusterKm'</pre></div><p>We want to determine how many rows we have in each cluster. The data table command that allows us to determine the number of rows is <code class="literal">.N</code>, shown as follows:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, list(.N), by=nameCluster]</pre></div><p>If we want to have a different column name for the cluster size, we can specify it within the list, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, list(nCountries=.N), by=nameCluster]</pre></div><p>In order to determine how many countries we have for each language, we can use <code class="literal">table</code>:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, table(language)]</pre></div><p>In order to use <code class="literal">table</code> within an aggregation, the output should be a list. For this purpose, we can convert the table using <code class="literal">as.list</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, as.list(table(language))]</pre></div><p>Now, we can apply this operation to each group using <code class="literal">by</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, as.list(table(language)), by=nameCluster]</pre></div><p>What if we want to visualize the percentage of countries speaking each language? We can divide each value of the table by the number of countries in the cluster, as follows:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, as.list(table(language) / .N), by=nameCluster]</pre></div><p>We want to generate <code class="literal">dtClusters</code> containing the number of countries in each group and the percentage of each language. In order to do this, we can generate two lists using the commands that we've just seen. In order to combine the two lists, we can just use <code class="literal">c(list1, list2)</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtClusters &lt;- dtFeatures[
  , c(list(nCountries=.N), as.list(table(language) / .N)),
  by=nameCluster
  ]</pre></div><p>Each row of <code class="literal">dtClusters</code> represents a cluster. The <code class="literal">nCountries</code> column displays the number of countries in the cluster and all the other columns show the percentage of each language. In order to visualize this data, we can build a histogram with a bar for each cluster. Each bar is divided into segments representing the number of countries speaking each language. The <code class="literal">barplot</code> function allows us to build the desired chart, if we give a matrix as the input. Each matrix column corresponds to a bar and each row defines the chunks in which the <a id="id259" class="indexterm"/>bar is divided.</p><p>We need to define a matrix containing the language percentages. This can be done by carrying <a id="id260" class="indexterm"/>out the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define <code class="literal">arrayLanguages</code> containing the <code class="literal">dtClusters</code> language column names:<div class="informalexample"><pre class="programlisting">arrayLanguages &lt;- dtFeatures[, unique(language)]</pre></div></li><li class="listitem">Build <code class="literal">dtBarplot</code> containing the language columns:<div class="informalexample"><pre class="programlisting">dtBarplot &lt;- dtClusters[, arrayLanguages, with=F]</pre></div></li><li class="listitem">Convert <code class="literal">dtBarplot</code> into a matrix using <code class="literal">as.matrix</code>. In order to build the chart, we need to transpose the matrix (invert the rows and columns) using the R function <code class="literal">t</code>:<div class="informalexample"><pre class="programlisting">matrixBarplot &lt;- t(as.matrix(dtBarplot))</pre></div></li><li class="listitem">Define a vector with the cluster sizes, that is, the number of countries. We will display the numbers under the columns:<div class="informalexample"><pre class="programlisting">nBarplot &lt;- dtClusters[, nCountries]</pre></div></li><li class="listitem">Define the legend names as the country names:<div class="informalexample"><pre class="programlisting">namesLegend &lt;- names(dtBarplot)</pre></div></li><li class="listitem">Reduce the legend names' length in order to avoid having a legend overlapping the chart. Using <code class="literal">substring</code>, we limit the names to 12 characters, as shown:<div class="informalexample"><pre class="programlisting">help(substring)
namesLegend &lt;- substring(namesLegend, 1, 12)</pre></div></li><li class="listitem">Define the colors using <code class="literal">rainbow</code>. We need to define a color for each element of <code class="literal">namesLegend</code>, so the number of colors is <code class="literal">length(namesLegend)</code>, as shown:<div class="informalexample"><pre class="programlisting">arrayColors &lt;- rainbow(length(namesLegend))</pre></div></li><li class="listitem">Define the chart title using <code class="literal">paste</code>:<div class="informalexample"><pre class="programlisting">plotTitle &lt;- paste('languages in each cluster of', nameCluster)</pre></div></li></ol></div><p>Now we <a id="id261" class="indexterm"/>have all the <code class="literal">barplot</code> inputs, so we <a id="id262" class="indexterm"/>can build the chart. In order to be sure that the legend dosen't overlap the bars, we include the <code class="literal">xlim</code> argument that specifies the plot boundaries, as shown:</p><div class="informalexample"><pre class="programlisting"># build the histogram
barplot(
  height = matrixBarplot,
  names.arg = nBarplot,
  col = arrayColors,
  legend.text = namesLegend,
  xlim = c(0, ncol(matrixBarplot) * 2),
  main = plotTitle,
  xlab = 'cluster'
)</pre></div><p>The chart obtained is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_05_01.jpg" alt="Exploring the clusters"/></div><p>The k-means algorithm performs a series of steps starting from the initial clusters that are defined by splitting the data randomly. The final output depends on the initial random split that is different every time we run the algorithm. So, if we run k-means more than once, we might obtain different results. However, this chart helps us identify some patterns within the language group. For instance, in the eighth cluster, almost all the countries speak English, so we can deduce that there are some English-speaking countries with a similar flag. In the fifth cluster, more than half of the countries speak French, so we can deduce the same. Some less relevant results are that Arabic has a high share in the first cluster and Spanish is <a id="id263" class="indexterm"/>quite relevant in the seventh cluster.</p><p>We are using other <a id="id264" class="indexterm"/>clustering algorithms and we will visualize the results in a similar way. In order to have clean and compact code, we can define the <code class="literal">plotCluster</code> function. The inputs are the <code class="literal">dtFeatures</code> feature data table and the <code class="literal">nameCluster</code> cluster column name. The code is almost the same as the preceding one, shown as follows:</p><div class="informalexample"><pre class="programlisting"># define a function to build the histogram
plotCluster &lt;- function(
  dtFeatures, # data table with the features
  nameCluster # name of the column defining the cluster
){
  # aggregate the data by cluster
  dtClusters &lt;- dtFeatures[
    , c(list(nCountries=.N), as.list(table(language) / .N)),
    by=nameCluster]
  
  # prepare the histogram inputs
  arrayLanguages &lt;- dtFeatures[, unique(language)]
  dtBarplot &lt;- dtClusters[, arrayLanguages, with=F]
  matrixBarplot &lt;- t(as.matrix(dtBarplot))
  nBarplot &lt;- dtClusters[, nCountries]
  namesLegend &lt;- names(dtBarplot)
  namesLegend &lt;- substring(namesLegend, 1, 12)
  arrayColors &lt;- rainbow(length(namesLegend))
  
  # build the histogram
  barplot(
    height = matrixBarplot,
    names.arg = nBarplot,
    col = arrayColors,
    legend.text = namesLegend,
    xlim=c(0, ncol(matrixBarplot) * 2),
    main = paste('languages in each cluster of', nameCluster),
    xlab = 'cluster'
  )
  
}</pre></div><p>This function should build the same histogram as the previous one. Let's check it using the following code:</p><div class="informalexample"><pre class="programlisting"># visualize the histogram using the functions
plotCluster(dtFeatures, nameCluster)</pre></div><p>Another way to visualize the clusters is to build a world map using a different color for each cluster. In <a id="id265" class="indexterm"/>addition, we can visualize a world map for the languages.</p><p>In order to build the map, we need to install and load the <code class="literal">rworldmap</code> package, as shown:</p><div class="informalexample"><pre class="programlisting"># define a function for visualizing the world map
install.packages('rworldmap')
library(rworldmap)</pre></div><p>This package builds a world map starting from the country names, that is, in our case the <code class="literal">dfFeatures</code> row <a id="id266" class="indexterm"/>names. We can add the <code class="literal">country</code> column to <code class="literal">dtFeatures</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, country := rownames(dfFeatures)]</pre></div><p>Our data is quite old so Germany is still divided in two parts. In order to visualize it on the map, we can convert <code class="literal">Germany-FRG</code> into <code class="literal">Germany</code>. Similarly, we can convert <code class="literal">USSR</code> into <code class="literal">Russia</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[country == 'Germany-FRG', country := 'Germany']
dtFeatures[country == 'USSR', country := 'Russia']</pre></div><p>Now, we can define a function to build a world map showing the clusters. The inputs are the <code class="literal">dtFeatures</code> data table and the <code class="literal">colPlot</code> column name of the feature to visualize (for example, <code class="literal">clusterKm</code>). The other argument is <code class="literal">colourPalette</code> and it determines the color to be used <a id="id267" class="indexterm"/>in the map. See <code class="literal">help(mapCountryData)</code> for more information, as shown:</p><div class="informalexample"><pre class="programlisting">plotMap &lt;- function(
  dtFeatures, # data table with the countries
  colPlot # feature to visualize
  colourPalette = 'negpos8' # colors
){
  # function for visualizing a feature on the world map</pre></div><p>We define the <code class="literal">colPlot</code> column containing the cluster to visualize. In the case of a string, we use just the first 12 characters, as shown:</p><div class="informalexample"><pre class="programlisting">  # define the column to plot
  dtFeatures[, colPlot := NULL]
  dtFeatures[, colPlot := substring(get(colPlot), 1, 12)]</pre></div><p>We build <code class="literal">mapFeatures</code> containing the data that we need to build the chart. See <code class="literal">help(joinCountryData2Map)</code> for more information. The <code class="literal">joinCode = 'NAME'</code> input specifies that the countries are defined by their names and not by an abbreviation. The <code class="literal">nameJoinColumn</code> specifies which column we have the country names in, shown as follows:</p><div class="informalexample"><pre class="programlisting">  # prepare the data to plot
  mapFeatures &lt;- joinCountryData2Map(
    dtFeatures[, c('country', 'colPlot'), with=F],
    joinCode = 'NAME',
    nameJoinColumn = 'country'
  )</pre></div><p>We can build the chart using <code class="literal">mapCountryData</code>. We specify that we are using the colors of the rainbow and that the country with the missing data will be gray, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">  # build the chart
  mapCountryData(
    mapFeatures,
    nameColumnToPlot='colPlot',
    catMethod = 'categorical',
    colourPalette = colourPalette,
    missingCountryCol = 'gray',
    mapTitle = colPlot
  )
  
}</pre></div><p>Now, we can use <a id="id268" class="indexterm"/>
<code class="literal">plotMap</code> to visualize the k-means clusters on the <a id="id269" class="indexterm"/>world map, as shown:</p><div class="informalexample"><pre class="programlisting">plotMap(dtFeatures, colPlot = 'clusterKm')</pre></div><div class="mediaobject"><img src="graphics/7740OS_05_02.jpg" alt="Exploring the clusters"/></div><p>We can see that many <a id="id270" class="indexterm"/>Asian countries belong to the fifth cluster. In addition, we can observe that Italy, France, and Ireland belong to the same cluster, since their flag is similar. Apart <a id="id271" class="indexterm"/>from that, it's hard to identify any other pattern.</p></div></div><div class="section" title="Identifying a cluster's hierarchy"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec35"/>Identifying a cluster's hierarchy</h2></div></div></div><p>Other techniques to <a id="id272" class="indexterm"/>identify homogeneous groups are the <a id="id273" class="indexterm"/>hierarchic clustering algorithms. These techniques build the clusters, merging the objects iteratively. At the beginning, we have a cluster for each country. We define a measure of how <span class="emphasis"><em>similar</em></span> two clusters are and, at each step, we identify the two clusters whose flag is the most <span class="emphasis"><em>similar</em></span> and merge them into a unique cluster. In the end, we have a cluster including all the countries.</p><p>The R function that performs hierarchic clustering is <code class="literal">hclust</code>. Let's take a look at its <code class="literal">help</code> function:</p><div class="informalexample"><pre class="programlisting"># HIERARCHIC CLUSTERING
# function for hierarchic clustering
help(hclust)</pre></div><p>The first input is <code class="literal">d</code> and the documentation explains that it's a dissimilarity structure, that is, a matrix containing all the distances between the objects. As suggested by the documentation, we can use the <code class="literal">dist</code> function to build the input, as shown:</p><div class="informalexample"><pre class="programlisting"># build the distance matrix
help(dist)</pre></div><p>The input of <code class="literal">dist</code> is a numeric matrix describing the flags. We already built <code class="literal">matrixDistances</code> for the k-means algorithm, so we can reuse it. The other relevant input is <code class="literal">method</code> and it specifies how <code class="literal">dist</code> measures the distance between two flags. Which method should we use? All the features are binary as they have two possible outcomes, that is, <code class="literal">0</code> and <code class="literal">1</code>. Then, the distance can be the number of attributes with a different value. The <code class="literal">method</code> object that determines the distance in this way is <code class="literal">manhattan</code>, as shown:</p><div class="informalexample"><pre class="programlisting">matrixDistances &lt;- dist(matrixFeatures, method = 'manhattan')</pre></div><p>The <code class="literal">matrixDistances</code> function contains the dissimilarity between any two flags. The other input is <code class="literal">method</code> and it specifies the agglomeration method. In our case, we set the method as <code class="literal">complete</code>. There <a id="id274" class="indexterm"/>are other options for <code class="literal">method</code> and <a id="id275" class="indexterm"/>they define the linkage, that is, the way of computing the distance between clusters, as shown:</p><div class="informalexample"><pre class="programlisting"># build the hierarchic clustering model
modelHc &lt;- hclust(d = matrixDistances, method = 'complete')</pre></div><p>The <code class="literal">modelHc</code> method contains the clustering model and we can visualize the cluster using <code class="literal">plot</code>. You can consult the help of <code class="literal">hclust</code> to understand the <code class="literal">plot</code> parameters, as shown:</p><div class="informalexample"><pre class="programlisting"># visualize the hierarchic clustering model
plot(modelHc, labels = FALSE, hang = -1)</pre></div><div class="mediaobject"><img src="graphics/7740OS_05_03.jpg" alt="Identifying a cluster's hierarchy"/></div><p>This chart shows the algorithm procedure. At the bottom, we have all the countries, and each <a id="id276" class="indexterm"/>flag belongs to a different cluster. Each line represents a cluster and the lines converge when the algorithm merges the clusters. On the left-hand side of the chart, you can see a scale representing the distance between the flags, and at each level the algorithm merges the clusters that are at a specific distance from each other. At the top, all the flags belong to the same <a id="id277" class="indexterm"/>cluster. This chart is called <span class="strong"><strong>dendrogram</strong></span>. Consider <a id="id278" class="indexterm"/>the following code:</p><div class="informalexample"><pre class="programlisting"># define the clusters
heightCut &lt;- 17.5
abline(h=heightCut, col='red')</pre></div><p>The clusters that we want to identify are the ones above the red line. The function that identifies the cluster starting from <code class="literal">modelHc</code> is <code class="literal">cutree</code>, and we can specify the horizontal line height in the <code class="literal">h</code> argument, as shown:</p><div class="informalexample"><pre class="programlisting">cutree(modelHc, h = heightCut)</pre></div><p>Now, we can add the cluster to <code class="literal">dtFeatures</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dcFeatures[, clusterHc := cutree(modelHc, h = heightCut)]</pre></div><p>As mentioned earlier, we can see which languages are spoken in each cluster. We can reuse <code class="literal">plotCluster</code> and <code class="literal">plotMap</code>:</p><div class="informalexample"><pre class="programlisting"># visualize the clusters
plotCluster(dtFeatures, nameCluster = 'clusterHc')</pre></div><div class="mediaobject"><img src="graphics/7740OS_05_04.jpg" alt="Identifying a cluster's hierarchy"/></div><p>In the eighth cluster, English is the predominant language. Apart from that, Arabic is relevant in the first cluster only, French and German are relevant in the second and third if taken together, and Spanish is <a id="id279" class="indexterm"/>relevant in the third.</p><p>We can also <a id="id280" class="indexterm"/>visualize the world map with the clusters, as shown:</p><div class="informalexample"><pre class="programlisting">plotMap(dtFeatures, colPlot = 'clusterHc')</pre></div><p>The chart obtained is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_05_05.jpg" alt="Identifying a cluster's hierarchy"/></div><p>Similar to k-means, the only continent with a predominant cluster is Asia.</p><p>This section described two <a id="id281" class="indexterm"/>popular clustering techniques that identify homogeneous flag clusters. They both allow us to understand the similarities between <a id="id282" class="indexterm"/>different flags, and we can use this information as support to solve some problems.</p></div></div></div>
<div class="section" title="Applying the k-nearest neighbor algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Applying the k-nearest neighbor algorithm</h1></div></div></div><p>This section <a id="id283" class="indexterm"/>shows you how to estimate a new country language starting from its flag, using a simple supervised learning technique that is the <span class="strong"><strong>k-nearest neighbor</strong></span> (<span class="strong"><strong>KNN</strong></span>). In this case, we estimate the language, which is a <code class="literal">categoric</code> attribute so we use a classification technique. If the attribute was numeric, we would have used a regression <a id="id284" class="indexterm"/>technique. The reason I chose KNN is that it's simple to explain, and there are some options to modify its parameters in order to improve the result's accuracy.</p><p>Let's see how the KNN works. We know the flag and the language of 150 countries and we want to determine the language of a new country starting from its flag. First, we identify the 10 countries whose flag is the most similar to the new one. Out of them, we have six Spanish-speaking countries, two English-speaking countries, one French-speaking country, and one Arabic-speaking country.</p><p>Out of these 10 countries, the most common language is Spanish, so we can expect that the new flag belongs to a Spanish-speaking country.</p><p>The KNN is based upon this approach. In order to estimate a new country language, we identify the <span class="emphasis"><em>K</em></span> countries whose flag is the most similar. Then, we estimate that the new country speaks the most common language among them.</p><p>We have a table describing 194 flags through 37 binary attributes whose value can be <code class="literal">Yes</code> or <code class="literal">No</code>. For instance, the <code class="literal">mainhuegreen</code> attribute is <code class="literal">yes</code>, if the predominant flag color is green and <code class="literal">no</code> otherwise. All the attributes describe the flag's colors and patterns.</p><p>Similar to the previous section, before modifying <code class="literal">dtFeatures</code>, we define <code class="literal">arrayFeatures</code> containing the feature names. As we added some columns to <code class="literal">dtFeatures</code>, we <a id="id285" class="indexterm"/>extract the feature names from <code class="literal">dfFeatures</code>. Then, we add the <code class="literal">country</code> column with the country names coming <a id="id286" class="indexterm"/>from <code class="literal">dfFeatures</code>, as shown:</p><div class="informalexample"><pre class="programlisting"># define the feature names
arrayFeatures &lt;- names(dfFeatures)[-1]
# add the country to dtFeatures
dtFeatures[, country := rownames(dfFeatures)]
dtFeatures[country == 'Germany-FRG', country := 'Germany']
dtFeatures[country == 'USSR', country := 'Russia']</pre></div><p>Starting from <code class="literal">dtFeatures</code>, we can apply KNN. Given a new flag, how do we determine which are the 10 most similar flags? Given any two flags, we can measure how <span class="emphasis"><em>similar</em></span> they are. The easiest way is to count how many features have the same value across the two flags. The more attributes they have in common, the more similar they are.</p><p>In the previous chapter, we already explored and transformed the features, so we don't need to process them. However, we haven't explored the language column yet. For each language, we can determine how many countries speak the language using <code class="literal">table</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, table(language)]</pre></div><p>The number of countries varies a lot from one language to another. The most popular language is <code class="literal">English</code>, with 43 countries, and there are some languages with just four countries. In order to have an overview of all the languages, we can visualize the table by building a chart. In the previous section, we defined <code class="literal">plotMap</code>, which shows the groups on the world map. We can use it to show the countries speaking each language, as shown:</p><div class="informalexample"><pre class="programlisting">plotMap(dtFeatures, colPlot = 'language', colourPalette = 'rainbow')</pre></div><p>The chart obtained is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_05_06.jpg" alt="Applying the k-nearest neighbor algorithm"/></div><p>It's nice to see a map showing countries that speak each language, but it's still a bit hard to understand how big the groups are. A better option is to generate a pie chart whose slices are <a id="id287" class="indexterm"/>proportional to the number of countries in each group. The R function is <code class="literal">pie</code>, as shown:</p><div class="informalexample"><pre class="programlisting"># visualize the languages
help(pie)</pre></div><p>The <code class="literal">pie</code> function <a id="id288" class="indexterm"/>requires an input, that is, a vector containing <a id="id289" class="indexterm"/>the number of countries speaking each language. If the input vector fields have a name, it'll be displayed in the chart. We can build the required vector using <code class="literal">table</code>, as shown:</p><div class="informalexample"><pre class="programlisting">arrayTable &lt;- dtFeatures[, table(language)]</pre></div><p>Fortunately, <code class="literal">pie</code> doesn't require any other argument:</p><div class="informalexample"><pre class="programlisting">pie(arrayTable)</pre></div><p>The chart obtained is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_05_07.jpg" alt="Applying the k-nearest neighbor algorithm"/></div><p>There are some languages that are spoken in just a few countries. For instance, there are just 4 Slavic countries. Given a new country, we want to determine its language starting from its flag. Let's pretend that we don't know which language is spoken in one of the 4 Slavic countries. If we take into account its 10 nearest neighbors, there cannot be more than 3 other Slavic countries. What if there are 4 English-speaking countries out of its 10 neighbors? Despite all the remaining Slavic countries that are in its neighborhood, there are more English countries just because the English group is bigger. Therefore, the algorithm will estimate that the country is English. Similarly, we have the same issue with any other small group. Like almost all the machine learning algorithm, the KNN won't be able to classify the countries that belong to any other smaller group.</p><p>While dealing <a id="id290" class="indexterm"/>with any classification problem, if some groups are small, we don't have enough related information. In this context, even a good technique won't be able to classify the new objects that belong to a small <a id="id291" class="indexterm"/>group. In addition, given a new country that belongs to a medium-sized group, it likely has a lot of neighbors that belong to the big groups. Therefore, a new country speaking one of these languages might be assigned to the big groups.</p><p>By knowing the model limitations, we can define a feasible machine learning problem. In order to avoid having small groups, we can merge some groups. The clustering techniques allowed us to identify which language groups are more well-defined, and accordingly, we can split the languages in these groups: <code class="literal">English</code>, <code class="literal">Spanish</code>, <code class="literal">French and German</code>, <code class="literal">Slavic and other Indo-European</code>, <code class="literal">Arabic</code>, and <code class="literal">Other</code>.</p><p>We can define the language groups to build <code class="literal">listGroups</code> whose elements contain the language spoken by the groups. For instance, we can define the <code class="literal">indoEu</code> group containing <code class="literal">Slavic</code> and <code class="literal">Other Indo-European</code> language, as shown:</p><div class="informalexample"><pre class="programlisting"># reduce the number of groups
listGroups &lt;- list(
  english = 'English',
  spanish = 'Spanish',
  frger = c('French', 'German'),
  indoEu = c('Slavic', 'Other Indo-European'),
  arabic = 'Arabic',
  other = c(
    'Japanese/Turkish/Finnish/Magyar', 'Chinese', 'Others'
    )
  )</pre></div><p>Now, we can redefine the <code class="literal">language</code> column containing the language groups. For each element of <code class="literal">listGroups</code>, we convert all the languages into the element name. For instance, we convert <code class="literal">Slavic</code> and <code class="literal">Other Indo-European</code> into <code class="literal">indoEu</code>.</p><p>We <a id="id292" class="indexterm"/>can perform this operation within a <code class="literal">for</code> loop. All the group names are contained in the list names, so we can iterate over the elements of <code class="literal">names(listGroups)</code>, as shown:</p><div class="informalexample"><pre class="programlisting">for(nameGroup in names(listGroups)){</pre></div><p>Here, <code class="literal">nameGroup</code> defines a group name and <code class="literal">listGroups[[nameGroup]]</code> contains its languages. We can <a id="id293" class="indexterm"/>extract the rows of <code class="literal">dtFeatures</code> speaking any of the group languages, using <code class="literal">language %in% listGroups[[nameGroup]]</code>. Then, we can reassign the language column to the <code class="literal">nameGroup</code> group name using the <code class="literal">:=</code> data table notation, as shown:</p><div class="informalexample"><pre class="programlisting">  dtFeatures[
    language %in% listGroups[[nameGroup]],
    language := nameGroup
    ]
}</pre></div><p>We redefined the <code class="literal">language</code> column grouping the languages. Let's take a look at it:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, language]</pre></div><p>Here, <code class="literal">language</code> is a factor and there are just six possible levels that are our language groups. However, you can see that R has printed <code class="literal">16 Levels: Arabic Chinese English French ... Other</code> in the console. The reason is that the <code class="literal">language</code> column format is <code class="literal">factor</code> and it keeps track of the 10 initial values. In order to display just the six language groups, we can redefine the <code class="literal">language</code> column using <code class="literal">factor</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtFeatures[, language := factor(language)]
dtFeatures[, language]</pre></div><p>Now we have just six levels. Just like we did earlier, we can visualize the group sizes data using <code class="literal">plotMap</code>, as shown:</p><div class="informalexample"><pre class="programlisting"># visualize the language groups
plotMap(dtFeatures, colPlot = 'language')</pre></div><p>The map <a id="id294" class="indexterm"/>obtained is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_05_08.jpg" alt="Applying the k-nearest neighbor algorithm"/></div><p>We can see that the countries of each category are geographically close to each other.</p><p>In order to visualize the new group sizes, we can use <code class="literal">pie</code>, as shown:</p><div class="informalexample"><pre class="programlisting">pie(dtFeatures[, table(language)])</pre></div><p>The chart obtained is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_05_09.jpg" alt="Applying the k-nearest neighbor algorithm"/></div><p>All the six groups contain enough countries. The <span class="strong"><strong>english</strong></span> and <span class="strong"><strong>other</strong></span> groups are a bit bigger than the others, but the sizes are comparable.</p><p>Now we can build the KNN model. R provides us with the <code class="literal">kknn</code> package containing the KNN algorithm. Let's install and load the package, as shown:</p><div class="informalexample"><pre class="programlisting"># install and load the package
install.packages("kknn")
library(kknn)</pre></div><p>The function that <a id="id295" class="indexterm"/>builds the KNN is called <code class="literal">kknn</code>, such as the <a id="id296" class="indexterm"/>package. Let's see its help function:</p><div class="informalexample"><pre class="programlisting">help(kknn)</pre></div><p>The first input is formula and it defines the features and the output. Then, we have to define a <a id="id297" class="indexterm"/>training set, containing the data to be used to build the model, and a test set, containing the data upon which we are applying the model. We use all the information about the training set and pretend not to know the language of the test set countries. There are other optional inputs defining some model parameters.</p><p>All the feature names are contained in <code class="literal">arrayFeatures</code>. In order to define how the output depends on the features, we need to build a string in the <code class="literal">output ~ feature1 + feature2 + …</code>. format. Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define the first part of the string: <code class="literal">output ~ </code>:<div class="informalexample"><pre class="programlisting">formulaKnn &lt;- 'language ~'</pre></div></li><li class="listitem">For each feature, add <code class="literal">+ feature</code> using <code class="literal">paste</code>:<div class="informalexample"><pre class="programlisting">for(nameFeature in arrayFeatures){
  formulaKnn &lt;- paste(formulaKnn, '+', nameFeature)
}</pre></div></li><li class="listitem">Convert the string into the <code class="literal">formula</code> format:<div class="informalexample"><pre class="programlisting">formulaKnn &lt;- formula(formulaKnn)</pre></div></li></ol></div><p>We built <code class="literal">formulaKnn</code> containing the relationship to put into <code class="literal">kknn</code>.</p><p>Now, we need to define the training set and the test set starting from <code class="literal">dtFeatures</code>. A fair split is putting 80 percent of the data in the training set, and for this purpose we can add each country to the training set with a probability of 80 percent and to the test set otherwise. We can define the <code class="literal">indexTrain</code> vector whose length is equal to the number of lines in <code class="literal">dtFeatures</code>. The R function is <code class="literal">sample</code>, as shown:</p><div class="informalexample"><pre class="programlisting">help(sample)</pre></div><p>The arguments are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">x</code>: The values to be <a id="id298" class="indexterm"/>put into the vector that are <code class="literal">TRUE</code> and <code class="literal">FALSE</code> in this case.</li><li class="listitem" style="list-style-type: disc"><code class="literal">size</code>: The <a id="id299" class="indexterm"/>length of the vector that is the number of rows in <code class="literal">dtFeatures</code> in our case.</li><li class="listitem" style="list-style-type: disc"><code class="literal">replace</code>: In order to <a id="id300" class="indexterm"/>sample the values more than once, it's <code class="literal">TRUE</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">prob</code>: The <a id="id301" class="indexterm"/>probability of choosing the elements of <code class="literal">x</code>. In our case, we pick <code class="literal">TRUE</code> with a probability of 80 percent and <code class="literal">FALSE</code> with a probability of 20 percent.</li></ul></div><p>Using our <a id="id302" class="indexterm"/>arguments, we can build <code class="literal">indexTrain</code>, shown as follows:</p><div class="informalexample"><pre class="programlisting"># split the dataset into training and test set
indexTrain &lt;- sample(
  x=c(TRUE, FALSE),
  size=nrow(dtFeatures),
  replace=TRUE,
  prob=c(0.8, 0.2)
)</pre></div><p>Now, we need to add the rows, for which <code class="literal">indexTrain</code> is <code class="literal">TRUE</code>, to the training set and the remaining rows to the testing set. We extract all the rows, for which <code class="literal">indexTrain</code> is <code class="literal">TRUE</code>, using a simple data table operation, as shown:</p><div class="informalexample"><pre class="programlisting">dtTrain &lt;- dtFeatures[indexTrain]</pre></div><p>In order to extract the test rows, we have to switch <code class="literal">TRUE</code> and <code class="literal">FALSE</code> using the <code class="literal">NOT</code> operator that in R is <code class="literal">!</code>, as shown:</p><div class="informalexample"><pre class="programlisting">dtTest &lt;- dtFeatures[!indexTrain]</pre></div><p>Now we have all the basic arguments for using <code class="literal">kknn</code>. The other parameters that we set are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">k</code>: The number of neighbors is <code class="literal">10</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">kernel</code>: KNN has the option of assigning a different relevance to the features, but we're not using this feature at the moment. Setting the <code class="literal">kernel</code> parameter as <code class="literal">rectangular</code>, we use the basic KNN.</li><li class="listitem" style="list-style-type: disc"><code class="literal">distance</code>: We want to compute the distance between two flags as the number of attributes that they don't have in common (similar to the previous chapter). In <a id="id303" class="indexterm"/>order to do this, we set the distance parameter equal to <code class="literal">1</code>. For more information, you can learn about <span class="strong"><strong>Minkowski distance</strong></span>.</li></ul></div><p>Let's build the <a id="id304" class="indexterm"/>KNN model:</p><div class="informalexample"><pre class="programlisting"># build the model
modelKnn &lt;- kknn(
  formula = formulaKnn,
  train = dtTrain,
  test = dtTest,
  k = 10,
  kernel = 'rectangular',
  distance = 1
)</pre></div><p>The model has learned from <code class="literal">dtTrain</code> and estimated the language of the countries in <code class="literal">dtTest</code>. As <a id="id305" class="indexterm"/>we can see in the <code class="literal">kknn</code> help, <code class="literal">modelKnn</code> is a list containing a description of the model. The component showing the predicted language is <code class="literal">fitted.valued</code>, as shown:</p><div class="informalexample"><pre class="programlisting"># extract the fitted values
modelKnn$fitted.values</pre></div><p>We can add the predicted language to <code class="literal">dtTest</code> in order to compare it with the real language:</p><div class="informalexample"><pre class="programlisting"># add the estimated language to dtTest
dtTest[, languagePred := modelKnn$fitted.values]</pre></div><p>For the countries in <code class="literal">dtTest</code>, we know the real and the predicted languages. We can count how many times they are the same using <code class="literal">sum(language == languagePred)</code>. We can measure the model accuracy by dividing the number of correct predictions by the total, that is, <code class="literal">.N</code> (the number of rows), as shown:</p><div class="informalexample"><pre class="programlisting"># evaluate the model
percCorrect &lt;- dtTest[, sum(language == languagePred) / .N]
percCorrect</pre></div><p>Here, <code class="literal">percCorrect</code> <a id="id306" class="indexterm"/>varies a lot depending <a id="id307" class="indexterm"/>on the training/test dataset split. As we have different language groups, <code class="literal">percCorrect</code> is not particularly high.</p></div>
<div class="section" title="Optimizing the k-nearest neighbor algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Optimizing the k-nearest neighbor algorithm</h1></div></div></div><p>We built our <a id="id308" class="indexterm"/>KNN model using 37 features that have a different relevance to the language. Given a new flag, its neighbors are the flags sharing a lot of attributes, regardless of their relevance. If a flag has different common <a id="id309" class="indexterm"/>attributes that are irrelevant to the language, we erroneously include it in the neighborhood. On the other hand, if a flag shares a few highly-relevant attributes, it won't be included.</p><p>KNN performs worse in the presence of irrelevant attributes. This fact is called the curse of dimensionality and it's quite common in machine learning algorithms. A solution to the curse of dimensionality is to rank the features on the basis of their relevance and to select the most relevant. Another option that we won't see in this chapter is using dimensionality reduction techniques.</p><p>In the previous chapter, in the <span class="emphasis"><em>Ranking the features using a filter or a dimensionality reduction </em></span>section, we measured the feature's relevance using the information gain ratio. Now, we can compute the <code class="literal">dtGains</code> table, similar to the previous chapter, starting from <code class="literal">dtTrain</code>. We cannot use the whole <code class="literal">dtFeatures</code> because we're pretending not to know the language of the test set countries. If you want to see how <code class="literal">information.gain</code> works, you can take a look at <a class="link" href="ch04.html" title="Chapter 4. Step 1 – Data Exploration and Feature Engineering">Chapter 4</a>, <span class="emphasis"><em>Step 1 – Data Exploration and Feature Engineering</em></span>. Consider the following example:</p><div class="informalexample"><pre class="programlisting"># compute the information gain ratio
library('FSelector')
formulaFeat &lt;- paste(arrayFeatures, collapse = ' + ')
formulaGain &lt;- formula(paste('language', formulaFeat, sep = ' ~ '))
dfGains &lt;- information.gain(language~., dtTrain)
dfGains$feature &lt;- row.names(dfGains)
dtGains &lt;- data.table(dfGains)
dtGains &lt;- dtGains[order(attr_importance, decreasing = T)]
View(dtGains)</pre></div><p>The <code class="literal">feature</code> column contains the feature names and the <code class="literal">attr_importance</code> column displays the feature gain, which expresses its relevance. In order to select the most relevant features, we can first rebuild <code class="literal">arrayFeatures</code> with the sorted features. Then, we'll be able to select the top, as shown:</p><div class="informalexample"><pre class="programlisting"># re-define the feature vector
arrayFeatures &lt;- dtGains[, feature]</pre></div><p>Starting from <code class="literal">arrayFeatures</code> and given a <code class="literal">nFeatures</code> number, we want to build the formula using the top <code class="literal">nFeatures</code> features. In order to be able to do this for any <code class="literal">nFeatures</code>, we can define a function to build the formula, as shown:</p><div class="informalexample"><pre class="programlisting"># define a function for building the formula
buildFormula &lt;- function(
  arrayFeatures, # feature vector
  nFeatures # number of features to include
){</pre></div><p>The steps are <a id="id310" class="indexterm"/>as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Extract the top <code class="literal">nFeatures</code> features and put them into <code class="literal">arrayFeaturesTop</code>:<div class="informalexample"><pre class="programlisting">arrayFeaturesTop &lt;- arrayFeatures[1:nFeatures]</pre></div></li><li class="listitem">Build the first part of the formula string:<div class="informalexample"><pre class="programlisting">formulaKnn &lt;- paste('language', '~')</pre></div></li><li class="listitem">Add the features to the formula:<div class="informalexample"><pre class="programlisting">for(nameFeature in arrayFeaturesTop){
  formulaKnn &lt;- paste(formulaKnn, '+', nameFeature)
}</pre></div></li><li class="listitem">Convert <code class="literal">formulaKnn</code> into a <code class="literal">formula</code> format:<div class="informalexample"><pre class="programlisting">formulaKnn &lt;- formula(formulaKnn)</pre></div></li><li class="listitem">Return the output:<div class="informalexample"><pre class="programlisting">  return(formulaKnn)
}</pre></div><div class="informalexample"><pre class="programlisting">formulaKnnTop &lt;- buildFormula(arrayFeatures, nFeatures = 10)
formulaKnnTop</pre></div></li></ol></div><p>Using our <a id="id311" class="indexterm"/>function, we can build <code class="literal">formulaKnnTop</code> using the top 10 features, as shown:</p><p>Now, we can build the model using the same inputs as before, with the exception of <code class="literal">formula input</code> that now contains <code class="literal">formulaKnnTop</code>, as shown:</p><div class="informalexample"><pre class="programlisting"># build the model
modelKnn &lt;- kknn(
  formula = formulaKnnTop,
  train = dtTrain,
  test = dtTest,
  k = 10,
  kernel = 'rectangular',
  distance = 1
)</pre></div><p>As mentioned earlier, we can add the predicted language to <code class="literal">dtTest</code> in a new column called <code class="literal">languagePred10</code>:</p><div class="informalexample"><pre class="programlisting"># add the output to dtTest
dtTest[, languagePredTop := modelKnn$fitted.values]</pre></div><p>We can compute the percentage of languages that we identified correctly:</p><div class="informalexample"><pre class="programlisting"># evaluate the model
percCorrectTop &lt;- dtTest[, sum(language == languagePredTop) / .N]
percCorrectTop</pre></div><p>Have we achieved any improvement by selecting the top features? In order to determine which model is the most accurate, we can compare <code class="literal">percCorrect10</code> with <code class="literal">percCorrect</code> and determine which is the highest. We randomly defined the split between <code class="literal">dtTrain</code> and <code class="literal">dtTest</code>, so the result changes every time we run the algorithm.</p><p>There is another option to avoid the curse of dimensionality. The flags are described by 37 features with different relevancies and we selected the 10 most relevant. In this way, the similarity depends on the number of features that are in common out of the top 10. What if we have two flags with just two out of the top 10 features and 20 out of the remaining features in common? Are <a id="id312" class="indexterm"/>they less similar than two flags with three out of the top 10 features in common? Instead of ignoring the other 27 features, we can use them giving them a lower relevance.</p><p>There is <a id="id313" class="indexterm"/>a KNN variation, called <a id="id314" class="indexterm"/>
<span class="strong"><strong>weighted KNN</strong></span>, which identifies the relevance of each feature and builds the KNN accordingly. There are different KNN versions and the <code class="literal">kknn</code> function allows us to use some of them, specifying the <code class="literal">kernel</code> argument. In our case, we can set <code class="literal">kernel = 'optimal'</code>, as shown:</p><div class="informalexample"><pre class="programlisting"># build the weighted knn model
modelKnn &lt;- kknn(
  formula = formulaKnn,
  train = dtTrain,
  test = dtTest,
  k = 10,
  kernel = 'optimal',
  distance = 1
)</pre></div><p>As mentioned earlier, we can measure the accuracy:</p><div class="informalexample"><pre class="programlisting"># add the estimated language to dtTest
dtTest[, languagePredWeighted := modelKnn$fitted.values]
percCorrectWeighted &lt;- dtTest[
  , sum(language == languagePredWeighted) / .N
  ]</pre></div><p>Depending on the training/test split, <code class="literal">percCorrectWeighted</code> can be higher or lower than <code class="literal">percCorrect</code>.</p><p>We saw different options to build a supervised machine learning model. In order to identify which performs <a id="id315" class="indexterm"/>best, we need to <a id="id316" class="indexterm"/>evaluate each option and optimize the parameters.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Summary</h1></div></div></div><p>In this chapter, you learned how to identify homogeneous clusters and visualize the clustering process and results. You defined a feasible supervised machine learning problem and solved it using KNN. You evaluated the model, accuracy and modified its parameters. You also ranked the features and selected the most relevant.</p><p>In the next chapter, you will see a better approach to evaluating the accuracy of a supervised learning model. You will see a structured approach to optimizing the model parameters and selecting the most relevant features.</p></div></body></html>