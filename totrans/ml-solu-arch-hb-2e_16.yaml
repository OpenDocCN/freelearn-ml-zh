- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing Generative AI Platforms and Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying generative AI at scale in an enterprise introduces new complexities
    around infrastructure, tooling, and operational processes required to harness
    its potential while managing risks. This chapter explores the essential components
    for building robust generative AI platforms and examines **Retrieval-Augmented
    Generation** (**RAG**), an effective architecture pattern for generative applications.
    Additionally, we highlight near-term generative AI solution opportunities ripe
    for business adoption across industries. With the right platform foundations and
    a pragmatic approach focused on delivering tangible value, enterprises can start
    realizing benefits from generative AI today while paving the way for increasing
    innovation as this technology matures. Readers will gain insight into the practical
    building blocks and strategies that accelerate generative AI adoption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter is going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Operational considerations for generative AI platforms and solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The retrieval-augmented generation pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a **Large Language Model** (**LLM**) adaptation method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end generative AI platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for deploying generative AI applications in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical generative AI business solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we close to artificial general intelligence?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational considerations for generative AI platforms and solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an enterprise, deploying generative AI solutions at scale requires robust
    infrastructure, tools, and operations. Organizations should contemplate establishing
    a dedicated generative AI platform to meet these evolving project demands.
  prefs: []
  type: TYPE_NORMAL
- en: Architecturally and operationally, a generative AI platform builds on top of
    an ML platform with additional new and enhanced technology infrastructure for
    large-scale model training, large model hosting, model evaluation, guardrails,
    and model monitoring. As such, the core operation and automation requirements
    for a generative AI platform are similar to those of a traditional MLOps practice.
    However, the unique aspects of generative AI projects such as model selection,
    model tuning, and integration with external data sources, require several new
    process workflows to be established, and as a result, new technology components
    need to be incorporated into the operation and automation of the AI/ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will delve into several new business and operational workflows
    involved in building and running generative AI initiatives, and their implication
    in new technology requirements and new functional roles.
  prefs: []
  type: TYPE_NORMAL
- en: New generative AI workflow and processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key steps in building generative AI solutions is to select the right
    foundation models for further evaluation and/or fine-tuning against the requirements.
    Before generative AI, data scientists were mainly concerned with selecting the
    right algorithms to train models from scratch. While there were techniques such
    as transfer learning available, the scope was small, and mainly limited to some
    computer vision tasks and tuning of language models such as BERT. With large foundation
    models, the process of model selection becomes a lot more involved, and the model-tuning
    process has evolved as well. While instruction fine-tuning remains similar to
    traditional supervised learning, **Reinforcement Learning Human Feedback** (**RLHF**)
    is a brand-new process to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: A generative AI project also requires a new process to manage and evaluate different
    prompt templates against different generative AI models for different use cases.
    These templates need to be version-tracked against various versions of different
    generative AI models as part of experimentation and testing. Metadata such as
    descriptions of the templates, their intended use, and limitations need to be
    documented and tracked. Testing results for the different combination of templates
    and models need to be stored and managed. New capabilities such as prompt template
    generation and tuning are also required.
  prefs: []
  type: TYPE_NORMAL
- en: Some generative AI use cases such as document search and retrieval also require
    a new workflow for generating embeddings for different data modalities. This requires
    a new architecture pattern for splitting the source data (e.g., documents) and
    running it through embedding models to generate embeddings. These processes often
    need to be tuned to support different embedding needs. Moreover, the knowledge
    encoded in pre-trained foundation models is frozen in time. To keep the knowledge
    up to date, continuous incremental pre-training is required with new, up-to-date
    datasets. This flow to source, process, and manage the new dataset to hydrate
    the knowledge of the foundation model requires new technology components to manage
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, as generative AI technology can generate factually incorrect and sometimes
    toxic information, a new monitoring capability is needed for detection. For example,
    a set of filters might be needed to detect potential adversarial prompting and
    monitor the output response for incorrect and toxic information.
  prefs: []
  type: TYPE_NORMAL
- en: New technology components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a technology component perspective, some new tools such as vector databases
    and prompt stores, FM evaluation tools, and RLHF workflow tools are now needed
    to support the additional requirements. From a model deployment perspective, in
    addition to the deployment of large generative models themselves, new models are
    now needed for inspecting the inputs from the users and outputs from the model
    to ensure responsible AI principles are followed and adversarial attacks are detected
    and mitigated.
  prefs: []
  type: TYPE_NORMAL
- en: New roles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the traditional roles and personas involved in ML projects and
    ML platforms, some new roles are now also required for building and using generative
    AI models. For example, to support RLHF, a new set of data annotators with domain
    expertise and language and linguistic proficiency is needed to help rate/rank
    responses. These annotators need knowledge to detect bias and harmful content,
    as well as being a good judge of the style and tone of texts. To support the development
    and testing of prompts, a new role called prompt engineer has also been established.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring generative AI platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A generative AI platform builds on top of an ML platform, with new technology
    components to support new workflows such as prompt management and FM benchmarking.
    It is a new, emerging concept yet to have commonly agreed-upon architecture patterns.
    The following diagram illustrates one example blueprint for building such a platform.
    Keep in mind that many of the proposed components will need to be custom-built
    as there is no managed or even open-source technology available.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Generative AI platform'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the preceding figure, the following new technology components
    will be needed to enhance the existing MLOps platform to support the needs of
    generative AI model development and deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt management**: As one of the most important factors in getting desired
    responses from FMs, prompts need to be properly managed, tracked, and versioned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FM benchmarking/playground workbench**: As the field continues to accelerate,
    we expect more FMs to become available. In addition, organizations will fine-tune
    existing FMs to create new models. To quickly determine if any new or fine-tuned
    FMs should be considered for different use cases, it is important to have an FM
    benchmark tool to quickly evaluate FMs against required criteria and use cases.
    Automation is critical as well as human-in-the-loop review and approval. Experiment
    tracking is also critical as part of the new capability to track and measure the
    performance of various prompts and response pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Central foundation model repository**: Unlike a regular model registry, where
    the main purpose is to keep an inventory of all models. There is a need to maintain
    a list of approved FMs for the rest of the organization to use. Both the process
    of introducing new models into the central repository and the process of adopting
    it for various downstream tasks such as fine-tuning and additional pre-training
    need to be properly managed. In addition, many of the foundation models will come
    from third-party providers via an API, so the new model repository will need to
    handle proper listing and access provisioning on behalf of the third-party providers.
    Centralizing FM models, especially those from third-party sources, in a shared
    repository raises additional cybersecurity concerns. These models may potentially
    harbor vulnerabilities that could be exploited. To mitigate these risks, it is
    crucial to implement robust cybersecurity measures, including thorough security
    scans, before adding the models to the repository. This helps ensure the integrity
    and security of the model repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised fine-tuning and RLHF**: This component provides support for frequent
    instruction fine-tuning and domain adaptation incremental pre-training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced model monitoring and filters**: The platform should provide a common
    set of capabilities to monitor in production both the input and output of models
    for bias, toxic content, and factually incorrect responses. These should be configurable
    capabilities that can meet the needs of different use cases and responsible AI
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FM gateway**: Many organizations will likely adopt FMs from different internal
    and external sources and providers. A governed and secured FM gateway layer for
    accessing different FMs is a new platform component that needs to be implemented.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s take a closer look at some of the key components of a generative
    AI platform.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt management component
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The prompt management component integrates with various generative AI platform
    components, enriching workflows such as model evaluation, generative AI chat applications,
    and generative AI agent interactions. At its core, the prompt management component
    consists of two main layers: the store and the management layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software application  Description automatically generated](img/B20836_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Prompt management component'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, the prompt management component illustrates the role
    of the management layer in overseeing a catalog of reusable, versioned prompt
    templates. This layer not only facilitates template authoring, tagging, and ingestion
    capabilities but also governs template access and consumption features, including
    the logging of usage history. It should also have workflow tools to automate the
    process for prompt approval and publishing. Other core functionalities could include
    a prompt recommendation engine based on context and automated prompt generation
    based on inputs and model targets.
  prefs: []
  type: TYPE_NORMAL
- en: For those opting to build this component on AWS, DynamoDB can serve as a viable
    choice for constructing the template store and managing usage history and metadata.
    The management layer and web UI can be implemented as a custom containerized software
    stack running on compute services like EKS. The API management layer can be easily
    implemented using AWS API Gateway.
  prefs: []
  type: TYPE_NORMAL
- en: An API Gateway-enabled API access layer will be used to support both prompt
    store user applications and integration with other downstream and consuming applications
    and systems.
  prefs: []
  type: TYPE_NORMAL
- en: FM benchmark workbench
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following diagram illustrates a high-level architecture for building an
    FM benchmarking solution using AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B20836_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: FM benchmark workbench'
  prefs: []
  type: TYPE_NORMAL
- en: Within this architecture, target models sourced from a model registry are loaded
    into SageMaker endpoints. The testing process involves the generation of testing
    prompts by inserting various testing data from databases or S3 into predefined
    prompt templates, all of which are hosted within DynamoDB. These generated prompts
    are subsequently sent to the API endpoint for response generation. The resulting
    responses are evaluated using a predefined set of metrics, and the evaluation
    results are stored in DynamoDB. This stored data is made accessible to a workbench
    front-end application for further reference and utilization by the users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker now also has built-in model evaluation capabilities with its
    Clarify component. SageMaker Clarify’s **Foundation Model Evaluations** (**FMEval**)
    provides a centralized solution to assess model quality, fairness, and reliability
    across LLMs. It has support for both automated model evaluation and workflows
    for human evaluation. Some of the main tasks supported by the automated evaluation
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open-ended text generation**: For this task, Clarify can automate evaluations
    for factual knowledge, semantic robustness, prompt stereotyping, and toxicity.
    Clarify provides default testing datasets including TREX, CrowS-Pairs, RealToxicityPrompt,
    and BOLD.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text summarization**: With this task, Clarify can assess accuracy, toxicity,
    and semantic robustness. For this evaluation, Clarify has built-in datasets including
    Government Report dataset, gigaword, and XSum.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question answering**: For question and answering tasks, Clarify also assesses
    accuracy, toxicity, and semantic robustness, with built-in datasets such as BoolQ,
    TriviaQA, and Natural Questions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Classification**: For this task, Clarify uses Women’s E-Commerce Clothing
    Reviews to assess accuracy and semantic robustness.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With automated evaluation, you can use built-in datasets or bring your own dataset
    for testing.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the human evaluation of a natural language processing model, you
    must first define the relevant metrics and metric types. A comparative rating
    can be used to evaluate multiple models side by side on those metrics. Individual
    rating is required for evaluating a single model. Both rating mechanisms are applicable
    to any text-related task.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning and RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As organizations seek to customize FMs for their specific use cases, they need
    tools for fine-tuning and aligning FMs with human preferences and use cases. The
    core components of FM fine-tuning and RLHF play a pivotal role in facilitating
    end-to-end adaptation workflows.
  prefs: []
  type: TYPE_NORMAL
- en: The core functionalities of these components should include automated supervised
    fine-tuning on an organization-specific dataset for target use cases while ensuring
    integration with the underlying training infrastructure. Additionally, it would
    also integrate with the FM evaluation service for both automated and human-assisted
    model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Another core functionality should be the support for a complete RLHF loop, allowing
    support from the collection and management of human preference data to interactive
    human evaluation/voting of FM outputs, automated reward model training, and finally,
    fine-tuning of models through reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: FM monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since FMs are trained on extensive public datasets, which can potentially contain
    harmful and biased content, these models may demonstrate similar problematic behavior.
    While many proprietary models have integrated safeguards and filters to screen
    out harmful responses or inappropriate prompts, organizations may have distinct
    requirements for filtering, especially when they utilize open-source models that
    may lack similar built-in safeguards. The following diagram illustrates an architectural
    approach for incorporating supplementary filtering components into the inference
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software system  Description automatically generated](img/B20836_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: Implementing model monitoring for generative AI models'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture framework, AWS Lambda functions are positioned within the
    inference workflow to inspect both the input fed into the generative models and
    the responses generated by these models. Specialized detectors and classifier
    models, including toxicity classifiers, bias detectors, and adversarial prompt
    detectors, can be deployed and hosted as distinct endpoints. These Lambda functions
    are responsible for triggering these models to execute screening procedures. Moreover,
    alongside employing ML models, the Lambda functions can also incorporate rule-based
    logic to apply supplementary filtering measures.
  prefs: []
  type: TYPE_NORMAL
- en: AWS has also implemented FM monitoring support, called guardrails, directly
    in its Bedrock service. It provides capabilities such as blocking undesired topics,
    filtering harmful content, and redacting PII data for the FMs hosted in Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI platform architecture and implementation are yet to mature at
    the time of writing. Organizations will need to evaluate the specific needs and
    invest in the building of these technology components to support their new generative
    AI workload at scale.
  prefs: []
  type: TYPE_NORMAL
- en: The retrieval-augmented generation pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundation models are frozen in time and limited to the knowledge they were
    trained on, lacking access to an organization’s private data or changing public
    domain information. To enhance the accuracy of responses, especially when using
    proprietary or up-to-date data, we require a mechanism to integrate external information
    into the model’s response generation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where **retrieval-augmented generation** (**RAG**) can step in. RAG
    is a new architecture pattern introduced to support generative AI-based solutions
    such as enterprise knowledge search and document question answering where external
    data sources are required. There are two main stages to RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: The indexing stage for preparing a knowledge base with data ingestion and indexes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The query stage for retrieving relevant context from the knowledge base and
    passing it to the LLM to generate a response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Architecturally, RAG architecture consists of the following key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge and document store**: This contains enterprise knowledge and documents
    used to provide context and facts for the LLM to generate responses based on real
    knowledge and facts. The store can be a knowledge graph, a database, a document
    store, or an object store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document chunking component**: Long documents need to be broken up into small
    chunks containing different knowledge and data, and they can be managed and retrieved
    based on specific user queries. This technology component splits the documents
    into small pieces based on predefined logic and rules (by number of words/characters,
    by paragraph).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document embedding component**: This component creates embeddings from the
    document chunks so these chunks can be effectively searched based on semantic
    similarity. This is a key concept for knowledge retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector DB**: This component stores the document chunks and associated embeddings
    and provides the capability for semantic searches using various techniques such
    as cosine similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retriever and re-ranker**: This component retrieves and re-ranks the top
    matches from the vector DB depending on specific requirements or context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query embedding component**: This component creates embeddings of user queries;
    these queries are then used to look up embeddings from the vector DB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow orchestration**: This component orchestrates the various steps in
    the chunking, embedding, and query/response flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt template store**: This store maintains prompt templates to construct
    appropriate queries against LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task prompt builder**: This component selects the appropriate prompts from
    the prompt template store and creates a task prompt using the original query and
    retrieved document/knowledge as context. The prompt is formatted to optimize the
    responses from the LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLMs**: These LLMs are responsible for constructing responses from templates
    that use the knowledge/document chunks as part of the context to provide more
    factual and fluent responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this architecture and flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a vector database  Description automatically generated](img/B20836_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Retrieval-augmented generation architecture'
  prefs: []
  type: TYPE_NORMAL
- en: During the indexing stage, source documents are broken up by a document-chunking
    component that breaks them up into small chunks. These chunks are further processed
    by an embedding component, whereby a vector representation (embedding) is created
    for each chunk, capturing the semantic meaning of the text in that chunk. The
    embeddings, along with their associated document chunks, are then stored in a
    vector database.
  prefs: []
  type: TYPE_NORMAL
- en: During the query stage, the user’s query (prompt) is first processed by an embedding
    component to generate a vector representation (embedding) for the query itself.
    A retriever component then uses this query embedding to retrieve matching vectors
    and their associated document chunks from the previously indexed vector database,
    typically by calculating similarity measures like cosine distance between the
    query and stored embeddings. The document chunks corresponding to the most similar
    embeddings are retrieved. These retrieved chunks are then incorporated into the
    original user query as additional context, forming a new, augmented prompt that
    combines the initial query with the relevant retrieved information.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this augmented prompt is sent to an LLM for synthesis, allowing the
    LLM to generate a response informed by both the original query and the contextual
    information retrieved from the database.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will first explore leading open-source frameworks
    for building RAG applications. We will then discuss the evaluation of the RAG
    pipeline, followed by advanced RAG patterns. Finally, we will understand how to
    build RAG architecture using AWS services. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Open-source frameworks for RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As RAG has emerged as an important architecture for many generative AI use cases,
    multiple technology frameworks have been developed by the open-source community
    to help streamline the implementation of RAG-based solutions and provide new capabilities.
    There are many RAG frameworks such as LangChain, LlamaIndex, REALM, and Haystack.
    Here, let’s briefly review LangChain and LlamaIndex, two of the more popular frameworks
    for RAG.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key challenges of building an LLM-based application is the coordination
    of various components, including vector DB, data retrieval, embedding LLM, and
    response generation LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is an open-source library that provides a capability that provides
    abstraction for these various RAG components and allows you to orchestrate them.
    LangChain supports the concept of a chain whereby a list of dependent tasks and
    components are connected to perform a function. For example, you can create a
    simple chain that takes an input query, reformat it using a predefined prompt
    template, and then invoke an LLM with the reformatted query. You can also have
    a complex chain that takes a document, splits it into small chunks, passes them
    to an embedding LLM, and then stores the embeddings in a vector DB. While these
    orchestrations can be hardcoded programmatically, a LangChain chain provides a
    dynamic way to define the orchestration and provides modular abstractions for
    many components, such as the LLMs and vector DBs, to simplify the implementation.
    LangChain comes with a list of built-in use-case-specific chains for quick implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simple code example of using LangChain for a question-answering
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code sample initializes an OpenSearch and Amazon Bedrock client, defines
    a prompt template, creates a built-in RetrievalQA chain instance, and executes
    the chain to generate an answer to the question.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain also supports the concept of a tool. A tool is a function that LangChain
    can invoke to perform an action such as math calculation or searching the web.
    This is a critical feature as this extends an LLM’s capability to perform more
    complex and precise tasks and be more factually correct.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, LangChain also has support for the concept of an agent. An agent
    helps link LLMs with different tools for dynamic execution, enabling LLMs to select
    the right tool to perform different tasks based on the query. For example, you
    might have a query that requires first searching the internet to get some facts
    and then performing a mathematical calculation on the result. In this case, an
    agent would allow an LLM to dynamically figure out which tool to use to complete
    the query. The following diagram illustrates how agents and tools can work together
    to support the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a tool  Description automatically generated](img/B20836_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: Agent and tool workflow'
  prefs: []
  type: TYPE_NORMAL
- en: Since its initial release in 2022, LangChain has seen rapid uptake across the
    AI community and companies. Numerous integrations have been developed to provide
    a range of capabilities, such as document loaders, vector stores, embedding models,
    LLMs, and tools.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LlamaIndex is a data framework for building LLM applications. LlamaIndex offers
    a comprehensive toolkit encompassing data connectors for diverse data sources
    and formats (e.g., APIs, PDFs, SQL), data indexes that structure information for
    efficient consumption by LLMs, and a query interface for sending queries and getting
    back responses from the underlying indexes.
  prefs: []
  type: TYPE_NORMAL
- en: After the data is ingested using LlamaIndex, the data is split up into data
    chunks and an embedding is created for each chunk. A data chunk and its embedding
    is called a node in LlamaIndex. The nodes are stored in the underlying vector
    databases. It also supports the concept of a list index where a list of related
    nodes are linked together; for example, a list of all chunks from a single document.
    This can be used for use cases where you want to summarize the entire document
    instead of returning a piece of relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: These engines include query engines for robust knowledge retrieval, chat engines
    for interactive conversations, and data agents that empower LLM-driven knowledge
    workers with a range of tools and integrations. Moreover, LlamaIndex seamlessly
    integrates with various applications, such as LangChain, Flask, Docker, ChatGPT,
    and others, ensuring cohesive integration within your broader ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex has the concept of data agents, which are knowledge workers that
    perform various tasks including searching and the retrieval of data, and calling
    external service APIs. This concept is similar to the agent concept in LangChain.
    Given an input query, the data agent uses a reasoning loop to determine which
    tool to use and in which sequence to invoke the tools.
  prefs: []
  type: TYPE_NORMAL
- en: There are some overlaps between LlamaIndex and LangChain in the area of external
    data connectors and indexing, query management for data retrieval, and interacting
    with LLMs. The key difference is that LlamaIndex focuses on building rich capabilities
    in those overlapping areas, while LangChain is more general-purpose with support
    for tools, agents, and chains.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a RAG pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating the performance of a RAG pipeline is a complex task as there are
    multiple processes involved, such as knowledge indexing, knowledge retrieval,
    and response synthesis. In addition, there is the unpredictable nature of text
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluating a RAG application involves multiple stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 1 – Faithfulness evaluation of response against the context**: This
    stage tests if the synthesized response faithfully captures the facts and context
    of the retrieved source documents. If it does not, then it is a sign of LLM hallucination.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stage 2 – Response relevancy evaluation against the query**: This stage checks
    if the response matches the retrieved source documents, and then this stage evaluates
    if the synthesized response answers the query. If it does not match, then it is
    an indication that the semantic search and/or embeddings do not function correctly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stage 3 – Question answering on the source document**: This stage uses an
    external tool to generate questions from the source document, and test if the
    LLM can answer questions using data. If the LLM cannot answer the question correctly,
    then the LLM is defective.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are several tools that can be used for RAG evaluation. For example, LlamaIndex
    provides a number of modules for evaluating hallucination and relevancy, as well
    as generating questions from source data. There are also other open-source tools,
    such as DeepEval for writing unit tests, and Ragas, which measures the RAG pipeline’s
    performance against different dimensions, including faithfulness and answer relevancy
    for response generation, and context precision and recall for the retrieved context
    against annotated answers.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced RAG patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While RAG has proven to be highly versatile and effective in solving many generative
    AI use cases, it also comes with its unique set of challenges that we need to
    be aware of and address when implementing a RAG solution.
  prefs: []
  type: TYPE_NORMAL
- en: A core challenge in developing effective RAG solutions is ensuring high-quality
    retrieval results. Poor retrieval can stem from various factors. For instance,
    retrieved text blocks may lack correlation with the original query, leading to
    hallucinated or fabricated responses. Additionally, failure to retrieve all relevant
    knowledge blocks prevents the model from synthesizing complete, high-quality answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to improve retrieval quality is to improve the indexing of the documents.
    There are several methods for improving indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-indexing data optimization**: Standardizing text, removing irrelevant
    content, eliminating redundancies, and validating factual accuracy before indexing.
    This reduces noise in the indexed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index structure optimization**: Tuning chunk size parameters to balance context
    preservation against quality. Strategies like sliding window chunking help retain
    contextual information across chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata enhancement**: Incorporating supplemental metadata like chapter
    descriptions and dates into chunks provides useful indexing context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding fine-tuning**: Fine-tuning embeddings adds critical in-domain context
    for specialized areas with uncommon terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic embedding**: Generating contextualized embeddings improves upon static
    embedding limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyond indexing, implementing smarter retrieval pipelines can further enhance
    RAG performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recursive retrieval**: This approach introduces a multi-stage querying approach,
    first retrieving smaller semantic blocks followed by larger contextual blocks.
    This balances efficiency with rich contextual grounding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subqueries**: Various query strategies can be employed in different scenarios,
    including using query engines provided by frameworks like LlamaIndex, employing
    tree queries, utilizing vector queries, or employing the most basic sequential
    querying of chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving relevant contextual documents is only the first step. Preparing the
    retrieved evidence for input into the LLM presents additional challenges. Inputting
    all documents at once risks exceeding the LLM’s context capacity. Meanwhile, concatenating
    documents creates lengthy, unfocused prompts. Advanced techniques are needed to
    optimize the retrieved evidence for response generation. With optimized evidence
    preparation, the LLM can focus on crucial information within its context window
    for producing high-quality responses grounded in the retrieved data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Re-ranking**: Re-ranking techniques can optimize document order to prioritize
    the most relevant information for synthesizing. There are different rankers available.
    For example, the Diversity Ranker reorders documents to increase diversity within
    the context window. The LostInTheMiddleRanker positions the best document at the
    start and end of the context window alternately. Through re-ranking, the most
    informative evidence is strategically placed for the language model to focus on.
    This prevents the most relevant details from getting lost in the middle of a lengthy
    context. Effective re-ranking is crucial for enabling models to produce high-quality
    responses grounded in the top retrieved evidence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt compression**: Post-retrieval processing techniques can remove irrelevant
    context from prompts to reduce noise and improve response quality in RAG systems.
    For example, some models calculate mutual information across prompt elements to
    estimate and filter out unimportant or distracting information. By stripping away
    non-essential text, the language model can focus on the truly salient evidence
    when generating responses. Careful prompt pruning is an impactful strategy to
    reduce hallucinations and keep RAG responses grounded in the most critical supporting
    context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, optimizing the indexing, implementing recursive retrieval pipelines,
    strategically reformatting evidence, and pruning prompts are all impactful strategies
    to address these challenges. When combined creatively, these advanced patterns
    enable models to retrieve the most relevant knowledge at each stage, hone in on
    the salient context, and synthesize this tailored evidence into high-quality,
    grounded responses.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a RAG architecture on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you use AWS, there are multiple ready-to-use AWS and third-party services
    you can use to build a RAG architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**SageMaker JumpStart**: SageMaker JumpStart provides pre-trained, open-source
    models for a wide range of problem types. You can incrementally train and tune
    these models for specific requirements and host these models using the SageMaker
    hosting service to support RAG architecture. Choose SageMaker JumpStart if you
    like to train and host open-source FMs yourself and have the choice of different
    compute options for model hosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Bedrock**: Amazon Bedrock is a fully managed service that makes FMs
    from Amazon and leading AI startups available through an API. At the time of writing,
    Amazon Bedrock provides the Titan FM from Amazon, and FMs from AI21, Stability
    AI, Cohere, Meta, Mistral, and Anthropic. For more details about Bedrock, you
    can visit the AWS public site. You want to explore Bedrock if you want access
    to the top proprietary FMs via an API instead of hosting your own. Also, for workloads
    with a low volume of transactions, Bedrock can be more cost-effective. Amazon
    Bedrock also has built-in support for document indexing and vector storage, known
    as the Bedrock knowledge base for RAG development. It also comes with support
    for agents and tools, called Agents for Amazon Bedrock, for building agent-based
    workflow applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon OpenSearch**: Amazon OpenSearch Service provides capabilities for
    interactive log analytics, real-time application monitoring, website search, and
    more. It can also be used as a vector database in the RAG architecture. It allows
    for the building of indexes for knowledge chunks and embeddings, and it provides
    proximity-based vector search. OpenSearch can integrate with LangChain and LlamaIndex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Kendra**: Amazon Kendra is an intelligent search service that can
    be used as a knowledge store for RAG. Amazon Kendra also provides a RAG retrieval
    API that can work with LangChain seamlessly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Q**: Amazon Q is a relatively new service from AWS. There are multiple
    independent services under the Amazon Q product suite, including Amazon Q for
    Business, Amazon Q for Builder, Amazon Q for QuickSight, and Amazon Q for Connect.
    Amazon Q for Business is a fully managed RAG-based assistant that can answer questions,
    provide summaries, and generate content based on enterprise data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other third-party and open-source vector database options available,
    such as Pinecone and FAISS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example RAG architecture designed using AWS services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of data lake  Description automatically generated](img/B20836_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: RAG architecture on AWS'
  prefs: []
  type: TYPE_NORMAL
- en: A RAG-based architecture can support many use cases, such as question answering
    over documents, querying data from databases and knowledge graphs using natural
    language, interactive chatbots, customer support assistants, medical information
    search and recommendations, and education and training.
  prefs: []
  type: TYPE_NORMAL
- en: As RAG architecture is becoming a common critical component of many LLM application
    architecture stacks, it is crucial to establish an operational capability and
    process that is similar to that of a GenAI/ML platform. However, since this is
    still a new area, there has not been much development of technical capabilities
    and tools around RAG management. Different organizations will need to evaluate
    needs and develop customer software and infrastructure to enable the operation
    of a common RAG platform. Organizations can consider RAG infrastructure as an
    independent platform or part of the overall generative AI platform.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an LLM adaptation method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered various LLM adaptation methods, including prompt engineering,
    domain adaptation pre-training, fine-tuning, and RAG. All these methods are intended
    to get better responses from the pre-trained LLMs. With all these options, it
    leaves one wondering: how do we choose which method to use?'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break down some of the considerations when choosing these different methods.
  prefs: []
  type: TYPE_NORMAL
- en: Response quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Response quality measures how accurately the LLM response is aligned with the
    intent of the user queries. The evaluation of response quality can be intricate
    for different use cases, as there are different considerations for evaluating
    response quality, such as knowledge domain affinity, task accuracy, up-to-date
    data, source data transparency, and hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: For knowledge domain affinity, domain adaptation pre-training can be used to
    effectively teach LLM domain-specific knowledge and terminology. RAG is efficient
    in retrieving relevant data, but the LLM used for the response synthetization
    may not capture domain-specific patterns, terminology, and nuance as well as fine-tuned
    or domain adaptation pre-training models. If you need strong domain-specific performance,
    you want to consider domain adaptation pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to maximize accuracy for specific tasks, then fine-tuning is the
    recommended approach. Prompt engineering can also help improve task accuracy through
    single-shot or few-shot prompting techniques, but it is prompt-specific and does
    not generalize across different prompts.
  prefs: []
  type: TYPE_NORMAL
- en: If information freshness in the response is the primary goal, then RAG is the
    ideal solution since it has access to dynamic external data sources. Prompt engineering
    can also help with data freshness when up-to-date knowledge is provided as part
    of the prompt. Fine-tuning and domain adaptation pre-training have knowledge cutoffs
    based on the latest training dataset used.
  prefs: []
  type: TYPE_NORMAL
- en: For some applications such as medical diagnosis or financial analysis, knowing
    how the decisions were made and what data sources were used in making the decision
    is crucial. If this is a critical requirement for the use case, then RAG is the
    clear choice here, as RAG can provide references to the knowledge it used for
    constructing the response. Fine-tuning and domain adaptation pre-training behave
    more like a “black box,” often obscuring what data sources are used for decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, LLMs sometimes generate inaccurate responses
    that are not grounded in their training data or user input when they encounter
    unfamiliar queries and hallucinate plausible but false information. Fine-tuning
    can reduce fabrication by focusing the model on domain-specific knowledge. However,
    the risk remains for unfamiliar inputs. RAG systems better address hallucination
    risks by anchoring responses to retrieved documents. The initial retrieval step
    acts as a fact check, finding relevant passages to ground the response in real
    data. Subsequent generation is confined within the context of the retrievals rather
    than being unconstrained. This mechanism minimizes fabricated responses not supported
    by data.
  prefs: []
  type: TYPE_NORMAL
- en: Cost of the adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When evaluating LLM adaptation approaches, it is important to consider both
    initial implementation costs as well as long-term maintenance costs. With this
    in mind, let’s compare the costs of the different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering has the lowest overhead, involving simply writing and testing
    prompts to yield good results from the pre-trained language model. Maintenance
    may require occasional prompt updates as the foundation model is updated over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: RAG systems have moderately high startup costs due to requiring multiple components
    – embeddings, vector stores, retrievers, and language models. However, these systems
    are relatively static over time.
  prefs: []
  type: TYPE_NORMAL
- en: Full fine-tuning and domain adaptation pre-training can be expensive, needing
    massive computational resources and time to completely update potentially all
    parameters of a large foundation model, as well as the cost of dataset preparation.
    **Parameter Efficient Fine-Tuning** (**PEFT**) can be cheaper than full fine-tuning
    and domain adaptation pre-training. However, it is still considered more expensive
    than RAG due to the requirement for high-quality dataset preparation and training
    resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation complexity varies significantly across different techniques,
    from straightforward to highly advanced configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering has relatively low complexity, requiring mainly language
    skills and few-shot learning familiarity to craft prompts that elicit good performance
    from the foundation model. There are minimal requirements for programming skills
    and science knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: RAG systems have moderate complexity, needing software engineering to build
    the pipeline components like retrievers and integrators. The complexity rises
    with advanced RAG configurations and infrastructure, such as complex workflows
    involving agents and tools, and infrastructure components for monitoring, observability,
    evaluation, and orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT and full model fine-tuning have the highest complexity. These require deep
    expertise in deep learning, NLP, and data science to select training data, write
    tuning scripts, choose hyperparameters like learning rates, loss functions, etc.,
    and ultimately update the model’s internal representations.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having delved into the various technical components separately within the generative
    AI technical stack, let’s now consolidate them into a unified perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Generative AI tech stack'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a generative AI platform is an extension of an ML platform by introducing
    additional capabilities such as prompt management, input/output filtering, and
    tools for FM evaluation and RLHF workflows. To accommodate these enhancements,
    the ML platform’s pipeline capability will need to include new generative AI workflows.
    The new RAG infrastructure will form the foundational backbone of RAG-based LLM
    applications and will be closely integrated with the underlying generative AI
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: The development of generative AI applications will continue to leverage other
    core application architecture components, including streaming, batch processing,
    message queuing, and workflow tools.
  prefs: []
  type: TYPE_NORMAL
- en: Although many of the core components will likely possess their unique set of
    security and governance capabilities, there will be an overarching need for comprehensive
    end-to-end observability, monitoring, security, and governance for generative
    AI application development and operation at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for deploying generative AI applications in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying generative AI applications in production environments introduces a
    new set of challenges that go beyond the considerations for traditional software
    and ML deployments. While aspects such as functional correctness, system/application
    security, security scan of artifacts such as model files and code, infrastructure
    scalability, documentation, and operational readiness (e.g., observability, change
    management, incident management, and audit) remain essential, there are additional
    factors to consider when deploying generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: The following are some of the key additional considerations when deciding on
    the production deployment of generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model readiness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deciding whether a generative AI model is ready for production deployment,
    the focus should be on its accuracy for the target use cases. These models can
    solve a wide range of problems, but attempting to test for all possible scenarios
    and use cases would be an endless endeavor, making it challenging to feel confident
    in the deployment. Instead, concentrate on designing the application layer to
    support only the targeted use cases, simplifying the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, when determining if a performance metric is satisfactory, it’s
    essential to establish a threshold using existing benchmarks as a baseline. For
    example, if the error rate for the current process is 20% and deemed acceptable
    for business operations, then a generative AI application that can achieve the
    same or lower error rate should be considered capable of delivering at least the
    same or better value. By adopting this approach, you can confidently proceed with
    production deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Decision-making workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying generative AI applications, it’s crucial to consider whether
    the system will make automated decisions or involve human oversight. Due to the
    potential for hallucinations or inaccuracies in these models, the level of testing
    and evaluation rigor needs to be determined based on the decision-making flow.
  prefs: []
  type: TYPE_NORMAL
- en: If the system is designed to make fully automated decisions, you must assess
    the risk of incorrect decisions being made. Is this risk tolerable for your use
    case? If so, then automated decision-making can proceed after thorough testing
    against the target use cases and scenarios. However, if the potential risk is
    not tolerable, it’s essential to incorporate human oversight into the decision-making
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where human oversight is required, ensure that the individuals involved
    are well qualified to make informed decisions with the support of the generative
    AI application. The system should be designed to provide recommendations or insights
    to human decision-makers, who can then apply their expertise and judgment to mitigate
    potential errors or biases from the AI model.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI assessment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to responsible AI considerations, such as bias and harmful content,
    it’s essential to evaluate them on a case-by-case basis for each specific use
    case. Different use cases may have varying tolerances for certain types of language
    or biases. For example, some use cases could be more tolerant of certain language
    patterns, while others may have stricter requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the degree of bias that is considered acceptable can vary depending
    on the use case and scenarios involved. What might be considered an acceptable
    level of bias for one application may be unacceptable for another. It’s crucial
    to assess the potential impact of biases within the context of each use case and
    determine the appropriate thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of applying a one-size-fits-all approach, it’s recommended to conduct
    a thorough evaluation of bias and harmful content considerations for each specific
    use case. This targeted assessment will ensure that the deployed generative AI
    application aligns with the unique requirements and constraints of that particular
    use case, minimizing potential risks and ensuring responsible and ethical deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails in production environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite thorough testing and evaluation during the development phase, generative
    AI models can still exhibit unexpected or undesirable behaviors when deployed
    in live production environments. To address this challenge, it is essential to
    establish a comprehensive set of guardrails within the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of these guardrails are robust input validation systems. These systems
    scrutinize the data and prompts fed into the generative AI models, ensuring that
    only appropriate, safe, and intended inputs are used. This protects the models
    from being exposed to potentially harmful or adversarial inputs, which could trigger
    unpredictable or undesirable outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Complementing the input validation, organizations must also develop sophisticated
    output filtering and moderation systems. These systems review the generated content
    before it is released or exposed to end-users, detecting and flagging any outputs
    that may be biased, offensive, sensitive (PII), or otherwise undesirable. This
    allows for timely review and intervention, ensuring that potentially problematic
    content is addressed before it reaches the public.
  prefs: []
  type: TYPE_NORMAL
- en: To enable rapid response and intervention, the monitoring and validation systems
    should be integrated with automated alerting mechanisms. These mechanisms quickly
    notify the appropriate teams of any concerning behaviors or outputs detected by
    the system. This allows organizations to act swiftly, addressing issues before
    they escalate or cause harm.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the human remains a crucial safeguard. Generative AI workflows must
    maintain the ability for experienced operators to override or intervene when necessary.
    This serves as a backstop, allowing knowledgeable personnel to make informed decisions
    when the models exhibit unpredictable or undesirable behaviors that require immediate
    attention.
  prefs: []
  type: TYPE_NORMAL
- en: If you use Amazon Bedrock, you can consider the built-in Guardrails feature
    which can detect and filter out undesired topics, harmful content, or PII data.
  prefs: []
  type: TYPE_NORMAL
- en: External knowledge change management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For generative AI applications that rely on external knowledge retrieval, such
    as those based on the RAG architecture, it is crucial to consider the dynamic
    nature of the underlying knowledge sources. External knowledge can change over
    time, and for the same query, there could be multiple relevant answers depending
    on the recency and timeline of the information.
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, it is essential to either design prompts that accurately
    capture the desired temporal context or ensure that the knowledge retriever component
    is aware of the knowledge lineage and timeline. This way, the system can retrieve
    and present the most relevant and up-to-date information in response to a given
    query.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a query pertains to a standard operating procedure, the procedure
    itself may evolve over time. Without considering the timeline, the system might
    retrieve outdated information, potentially leading to incorrect or irrelevant
    responses. By incorporating knowledge lineage or explicitly specifying the desired
    time frame in the prompt, the system can retrieve the appropriate version of the
    standard operating procedure that aligns with the intended temporal context.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the knowledge retriever component can be designed to maintain
    and leverage a timeline or versioning system for external knowledge sources. This
    would allow the system to automatically retrieve the most recent and relevant
    information based on the query’s context, without relying solely on prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing these considerations is crucial for ensuring the accuracy, relevance,
    and reliability of generative AI applications that depend on external knowledge
    sources, especially in domains where information evolves rapidly or where temporal
    context is critical.
  prefs: []
  type: TYPE_NORMAL
- en: The list provided earlier serves as a sample of additional considerations. Organizations
    should contemplate additional decision points tailored to their specific needs,
    industry, and regulatory environment. It is crucial to carefully assess the scope
    and use case of generative AI, implementing checks and controls within the defined
    scope to facilitate efficient deployment decision-making. As generative AI technology
    advances, evolving requirements will emerge, necessitating ongoing considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Practical generative AI business solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we talked about the business potential of generative
    AI and potential use cases in various industries. We then followed that with a
    detailed discussion of the lifecycle of a generative project from business use
    case identification to deployment. In this chapter, we have covered operational
    considerations, building enterprise generative AI platforms, and one of the most
    important architecture patterns for building generative AI applications, RAG.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will highlight some of the more practical generative AI
    solution opportunities ready for business adoption in the near term. While research
    continues on aspirational applications, prudent enterprises should evaluate proven
    pilot use cases to drive measurable impact from generative AI’s rapid advances.
    With these examples, we will present the recommended approach to identify generative
    AI opportunities by understanding challenges associated with the specific business
    workflow within several industries.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI-powered semantic search engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enterprise search solutions enable organizations to provide powerful search
    capabilities for their internal data and content. Companies in sectors like technology,
    healthcare, finance, and manufacturing have widely adopted enterprise search platforms
    to improve information discovery for employees. These tools index structured databases,
    intranets, document repositories, emails, and more within an organization. Users
    can then quickly find relevant content by searching instead of hunting across
    siloed systems. Advanced natural language processing, ML algorithms, and cognitive
    capabilities enable enterprise search solutions to deliver precise, relevant results.
  prefs: []
  type: TYPE_NORMAL
- en: With the arrival of generative AI technologies, especially LLMs, enterprise
    search can be enhanced to provide an improved user experience, more accurate information,
    and greater specificity. For example, instead of simply returning a list of search
    results using keywords, LLMs can take natural language queries and directly provide
    the answers or a summarized version of an answer in natural language. LLMs can
    help understand the user intent and context semantically in the user queries for
    more relevant information retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can also take query/response history as part of the context when constructing
    search queries against the underlying knowledge base. LLMs can also help rewrite
    the queries with synonyms, related terms, and rephrases to broaden the search
    scope. With LLMs, results can be matched based on meaning, relationship, and concepts
    rather than just keywords.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple technology options and architecture patterns available to
    build an enterprise search platform powered by generative AI. You can choose to
    build your own semantic search engine using a combination of open-source and commercial
    components. Building a semantic search engine largely follows the RAG architecture
    pattern we have discussed previously. The following architecture shows a semantic
    search engine using a combination of AWS-managed services and open-source components.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a chat interface  Description automatically generated](img/B20836_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: Semantic search engine on AWS'
  prefs: []
  type: TYPE_NORMAL
- en: With this architecture, Kendra takes care of document ingestion and indexing,
    semantic search, and ranking. Amazon OpenSearch can be used to build additional
    alternative knowledge indexes if needed. The LLMs model from Bedrock or hosted
    in SageMaker provide query understanding and response generation via the Amazon
    Lex chat interface.
  prefs: []
  type: TYPE_NORMAL
- en: Although generative AI significantly improves the enterprise search experience,
    it is essential to acknowledge its limitations. In contrast to traditional keyword
    or semantic search methods, generative AI occasionally yields results that are
    irrelevant or off-topic, lacking the precision characteristic of conventional
    approaches. Moreover, it presents consistency challenges, generating different
    outputs for the same or similar inputs due to variations in its interpretation
    of instructions over time and the accuracy of the index retrievers.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there is the potential for privacy concerns, as generative AI
    might inadvertently disclose sensitive information, raising issues related to
    privacy violations if sensitive datasets such as PII/PHI are not masked in the
    model training/tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Financial data analysis and research workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Financial analysts across banking, asset management, and other domains rely
    heavily on analyzing and synthesizing data to deliver insights. For example, an
    investment bank analyst might gather information from earnings reports, news,
    filings, and research coverage to extract details on financials, business outlooks,
    and corporate actions. The analyst then performs a comparative valuation analysis,
    forecasts growth and returns, and advises an investment strategy. This requires
    manually crunching numbers, modeling scenarios, and creating reports to communicate
    findings. With so much reading, data gathering, and analysis required, the process
    is often tedious and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI capabilities like natural language processing, data extraction,
    summarization, and text generation have shown promise in augmenting analysts’
    workflows. Generative AI-powered assistants that automate data aggregation, run
    comparative analytics, and draft reports could amplify analyst productivity multifold.
    The following are several areas in the workflow where generative AI can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data extraction**: Generative AI provides new capabilities to automatically
    extract structured data from unstructured documents and output it in usable formats.
    Traditional NLP techniques have enabled entity extraction and relation mapping
    to some extent. However, LLMs now achieve superior performance in accurately identifying
    key entities, relationships, and data points in texts. These models can parse
    details like financial figures, corporate actions, and business events from earnings
    reports, filings, news, and other sources. The extracted data can then be formatted
    for seamless loading into workflows like Excel financial models and PowerPoint
    presentations for further analysis. This alleviates tedious manual data entry
    and copying for analysts. With higher accuracy at directly generating structured
    outputs, generative AI can integrate deep unstructured data understanding into
    downstream systems. This fills a major gap in leveraging textual data like reports
    and articles in quantitative finance workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document QA**: Generative AI models enable users to ask freeform questions
    in natural language to extract additional insights from documents. For example,
    an analyst could query, “What were the key revenue drivers last quarter?” and
    the model would comprehend the underlying earnings transcript to summarize the
    major growth factors concisely in a generated response. Such ad hoc queries allow
    flexibly extracting only the most relevant points instead of processing the full
    document. The model would focus on areas related to the question and ignore superfluous
    text. By generating condensed, tailored answers to natural language questions,
    generative AI provides a powerful capability to slice and dice documents on demand.
    Analysts can dynamically explore and analyze long reports by conversing with the
    AI in plain language to uncover relevant facts, relationships, and conclusions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enterprise search and data query against internal data sources**: Conversational
    interfaces powered by generative AI can enable financial analysts to gather data
    through natural dialog. Instead of needing to navigate disparate systems and remember
    specific query languages, analysts could simply ask questions in plain language.
    For example, “Get me the 3-year sales growth by region from the EMEA market database.”
    The model would interpret the intent, translate the required queries for each
    data source, gather the results, and summarize them in a readable format for the
    analyst. This conversational ability to retrieve cross-system data on demand has
    the potential to unify access and accelerate insights. Analysts could explore
    connections across internal document stores, financial databases, knowledge bases,
    and search engines using everyday language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial analysis and report generation**: Generative AI enables financial
    analysts to directly request certain analytical tasks using natural language instructions.
    For example, an analyst could ask the model, “Compare the 5-year revenue growth
    and profitability margins for the top 5 companies in the industry.” The model
    would then extract relevant financial figures, compute required ratios, generate
    suitable visualizations, and summarize key takeaways in an output report. Where
    required, it could seamlessly leverage external tools and APIs to augment its
    analysis. Unlike rigid commands, natural instructions allow analysts to specify
    bespoke analysis on demand. By automating data gathering, financial modeling,
    and report generation, while coordinating external services, generative AI can
    dramatically amplify an analyst’s productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example prompt can help provide a financial analysis across a
    number of financial dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Architecturally, building generative AI-powered financial analysis and research
    solutions is mainly based on the RAG architecture and principles. The following
    diagram illustrates a conceptual application architecture for such an application.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of data lake  Description automatically generated](img/B20836_16_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.10: Generative AI-powered financial analysis application'
  prefs: []
  type: TYPE_NORMAL
- en: Finance is a highly precise science and business domain, and as such generative
    AI solutions for financial analysis require stringent accuracy and factual grounding.
    To limit hallucination risks in LLMs, techniques beyond prompt engineering and
    fine-tuning are necessary. Advanced information retrieval and embedding approaches
    can enhance output relevancy and correctness.
  prefs: []
  type: TYPE_NORMAL
- en: For example, instead of retrieving information just once and then generating
    the response for a query, the system can implement multi-hop information retrieval
    by predicting the related queries and retrieving other relevant information for
    a more complete context. From an embedding perspective, instead of simply chunking
    up documents and creating embeddings for the chunks, additional structural, semantic,
    and domain meta can be combined to create enriched embeddings. On the retrieval
    end, instead of returning the top matching fragments as is from a vector DB, a
    re-ranker can be implemented to post-process the outputs based on unique requirements.
    Comprehensive evaluation techniques and processes also need to be implemented
    to establish high confidence in the system. Where possible, implement a facts
    validator to validate responses against known facts.
  prefs: []
  type: TYPE_NORMAL
- en: Although advanced LLMs and techniques have showcased remarkable capabilities
    in automating or aiding various facets of financial analysis tasks, it remains
    premature to depend solely on LLMs for intricate financial analysis tasks. The
    precision demanded in financial decision-making necessitates accurate information,
    as any inaccuracies in LLM responses could result in substantial negative consequences.
  prefs: []
  type: TYPE_NORMAL
- en: The financial services industry as a whole has been actively embracing and implementing
    generative AI technology to achieve diverse business objectives. This widespread
    adoption has the potential to significantly impact various financial functions,
    ranging from financial analysis and combating financial crimes to the development
    of new business models, products, and enhanced customer experiences. However,
    this increasing trend also gives rise to concerns, particularly in areas such
    as risk management and transparency. For instance, in financial analysis, maintaining
    transparency is crucial for regulatory compliance and effective risk management.
    The inherent opacity of LLMs may pose challenges in meeting these regulatory requirements.
    Additionally, the demand for explanations in financial decision-making could be
    a potential hurdle, as comprehending the decision-making process of LLMs may prove
    challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Clinical trial recruiting workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clinical trials are lengthy research processes that test new medical treatments
    like drugs, devices, or interventions on human subjects. Clinical trials progress
    through different phases to evaluate a new medical treatment, starting with safety
    in smaller groups before expanding to measure efficacy and comparisons. In addition,
    patient recruiting is an important process in all phases of a clinical trial.
    Let’s look at these phases in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: In Phase 1 trials, the treatment is given to fewer than 100 people to assess
    safety and side effects. Since it focuses on a smaller, healthy group, recruiting
    patients at this stage is less complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phase 2 trials administer the treatment to several hundred participants with
    the target condition. Here, researchers continue collecting safety data while
    gathering preliminary efficacy information. Recruiting becomes more difficult
    as patients need to have the specific condition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Phase 3, the trial expands to 300-3000 participants to further understand
    safety, efficacy, dosages, and how it compares to existing treatments. The much
    larger sample size covering diverse demographics makes recruiting extremely challenging
    at this advanced stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phase 4 trials monitor the approved treatment in broad real-world populations
    to gather additional long-term safety and efficacy data. Recruiting for Phase
    4 can also be challenging as criteria tend to be more expansive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During recruitment, clinical research coordinators meticulously screen patient
    medical history across various systems like electronic health records to check
    if criteria like health status, past conditions, medications, demographics, etc.
    match the trial’s eligibility requirements. This manual and repetitive process
    of collating data from multiple sources to identify matches is a major bottleneck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI could automate and accelerate screening by intelligently querying
    patient EHR data against complex inclusion/exclusion logic specified in natural
    language. As EHRs contain comprehensive histories including diagnoses, medications,
    procedures, and test results, the models can parse criteria and rapidly filter
    candidates. For example, a researcher can directly ask the generative AI platform
    to compare a patient’s EHR records with the inclusion/exclusion criteria to determine
    if the patient is a match. Generative AI can also help synthesize and generate
    a summarized report about the overall selected cohort population and individual
    patients to help with human review. This can significantly speed up finding eligible
    patients for different trial phases.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, combining generative capabilities with predictive analytics can
    optimize trial performance. The combined capabilities can forecast enrollment
    rates, monitor site progress, and trigger recommended interventions when delays
    or issues occur by generating insights from patient and site data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a conceptual flow of applying generative AI and
    traditional predictive analytics to optimize the various tasks within clinical
    trials:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer  Description automatically generated](img/B20836_16_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.11: Clinical trial optimization'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example command for patient search against medical records
    using a list of inclusion and exclusion criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes of the command, an agent can facilitate the search of patient
    records against the semantic search engine of EHR records and generate a response
    with a list of matching patients.
  prefs: []
  type: TYPE_NORMAL
- en: While generative AI has the potential to automate and accelerate patient recruitment
    for clinical trials, it is crucial to keep qualified humans in the loop throughout
    the process. AI systems alone cannot fully validate that trial participants meet
    the complex eligibility criteria, which often involves interpreting medical histories,
    test results, prior conditions, and more. Humans with clinical expertise need
    to review participant profiles surfaced by AI to catch any inaccurate assessments
    of eligibility and prevent improper enrollment.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, human oversight is required to ensure AI-assisted recruiting adheres
    to ethical guidelines around transparency, fairness, and avoiding undue influence.
    Participants should comprehend why they were targeted and how their data is used.
    Automated processes could lead to opaque and biased recruiting without checks
    against discrimination. Experienced clinical research staff need to steward participant
    interactions to uphold understandability, equitability, and medical appropriateness.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, human involvement lends necessary nuance and discretion on a case-by-case
    basis. Factors like availability, transportation needs, and personal situations
    must be weighed. AI models alone lack the empathy and adaptability needed. The
    clinical trial process ultimately deals with human lives, so the compassion and
    experience of human recruiters remain indispensable when augmented by AI efficiency
    gains.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, generative AI’s ability to deeply understand criteria, reason
    across data sources, and generate matches and recommendations offers immense potential
    to transform the protracted patient recruitment process to help advance critical
    medical research. It is also important to acknowledge its limitations, such as
    hallucination, opaqueness, and privacy concerns such as the handling of PII/PHI
    data. Furthermore, the regulatory environment around the adoption of generative
    AI for clinical trials remains unclear, which poses challenges in the adoption
    of generative AI in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Media entertainment content creation workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The content creation process in the media and entertainment industry encompasses
    multiple stages, spanning from idea generation and scriptwriting to casting, production,
    post-production editing, sound design, graphics and animation, distribution, marketing,
    and monetization. However, this process is accompanied by various challenges that
    can impact the success of a project. These challenges include the need to generate
    unique and resonating ideas, crafting compelling and coherent storylines, ensuring
    that visual representations effectively convey intended emotions, the time-consuming
    nature of post-production editing, and the difficulty in selecting or creating
    suitable music and sounds.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, specialized skills are often required for animation and visual
    effects, and breaking through the competitive market with effective marketing
    strategies can be a daunting task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI presents a promising solution to address these challenges in
    practical ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Script generation**: Generative AI can be used to generate movie or TV show
    scripts based on human-provided ideas and inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storyboarding**: Generative AI can propose visual representations aligned
    with intended themes and emotions from the scripts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production**: Generative AI can be used to generate images and photos needed
    on film sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-production editing**: Generative AI can help automate post-production
    editing processes through techniques like text-guided editing. For example, you
    can use generative to create special effects, and for editing scenes and images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Media asset search**: Generative AI can help enhance media content search
    through advanced tagging and semantic matching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Marketing and promotion**: Generative AI can generate compelling marketing
    messages and visuals for promotional campaigns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Engagement**: Generative AI can enhance user engagement experience through
    personalization and recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows where generative AI can be applied throughout the
    media lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a marketing process  Description automatically generated](img/B20836_16_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.12: Media content development and distribution flow'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a prompt to generate a movie script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the preceding prompt against ChatGPT, you could get something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to create visual representations for some of the scenes from the
    script, we can employ a text-to-image model. For instance, we can provide the
    following input to a Stable Diffusion model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting image would resemble the image depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_16_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.13: Storyboarding using text-to-image model'
  prefs: []
  type: TYPE_NORMAL
- en: It’s still early days for generative AI-created entertainment, but it is already
    clear that the tremendous opportunities have attracted many companies to build
    generative AI tools for media use cases such as storyline generation tools. A
    generative AI video editing tool from Runway, a company that builds video editing
    tools, has been used for post-production editing such as in *Everything, Everywhere,
    All At Once*. However, this type of tool raises questions about intellectual property.
    If AI creates a new character influenced by a well-known figure, who owns the
    copywrite? There is also concern about the social impact that this technology
    will have on creative professionals such as animators, scriptwriters, and visual
    artists. Some generative AI technologies have taken a more cautious approach to
    creative content generation. For example, the Claude chatbot tool from Anthropic
    will block the generation of full-length movie scripts to avoid potential negative
    consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Car design workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Car design plays an essential role in shaping a high-quality automobile. It
    encompasses three key domains: exterior design, interior design, and color and
    trim design. The design team responsible for the exterior of the vehicle develops
    the proportions, shape, and surface details of the vehicle. The interior designer
    develops the proportions, shape, placement, and surfaces for the instrument panel,
    seats, door trim panels, headliner, pillar trims, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the emphasis is on ergonomics and the comfort of the passengers. Lastly,
    the trim designer is responsible for the research, design, and development of
    all interior and exterior colors and materials used on a vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: The design development process for exterior and interior design starts with
    manual sketches and digital drawings, which form the foundation of concept development.
    These sketches and drawings undergo rigorous review and approval processes within
    various layers of management. Subsequently, the concept is rendered into a digital
    format using a **computer-aided styling** (**CAS**) tool to further refine the
    style, and the design is transformed into vivid images. After that, industrial
    plasticine or clay modeling is developed from the images.
  prefs: []
  type: TYPE_NORMAL
- en: During the design process, it is imperative to maintain alignment with the designer’s
    stylistic vision while adhering to stringent criteria encompassing performance,
    manufacturability, and safety regulations. This requires product engineering to
    work concurrently with the designer to ensure the styling is grounded in various
    engineering constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'One area generative AI can play a role is concept development. Designers can
    use text-guided prompts, using keywords such as “pronounced spoiler,” “futuristic,”
    or “aerodynamics,” to swiftly generate car design concepts. General-purpose text-to-image
    models such as Stable Diffusion, Imagen, Amazon Titan Image Generator and DALLE-2
    models have demonstrated immense potential in text-guided concept design. For
    example, the following prompt example will generate a car exterior concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is one output variation when running the prompt using the Stable
    Diffusion model from Stability AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A blue sports car on a road  Description automatically generated](img/B20836_16_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.14: Sports car concept design using generative AI'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note, however, that these tools can only provide a source
    of style inspiration and do not address complex engineering and safety considerations
    integral to actual car design for production. These models need to be enhanced
    to support image generation while also optimizing specific engineering constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Progress has been made in this area, with examples like drag-guided diffusion
    models that can render creative car concepts while minimizing drag. Technically,
    these techniques aim to minimize an auxiliary loss function tied to a particular
    constraint, such as drag, during the model training and image generation process.
    As a result, when a car design image is generated, it also aligns with optimized
    constraints, such as minimizing the drag value.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a car  Description automatically generated](img/B20836_16_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.15: Generative AI-powered car design flow'
  prefs: []
  type: TYPE_NORMAL
- en: Such tools have the potential to greatly enhance the productivity of both car
    designers and product engineers, enabling them to avoid investing time in impractical
    car design concepts that cannot be feasibly produced for the market.
  prefs: []
  type: TYPE_NORMAL
- en: The adoption of generative AI in the automotive industry has significant implications
    across various aspects of the sector. Generative AI has the potential to revolutionize
    vehicle design, manufacturing processes, and overall operational efficiency. It
    can play a crucial role in the development of autonomous vehicles, enhancing their
    perception, decision-making, and response capabilities. However, the adoption
    of generative AI in the automotive industry also raises challenges and considerations.
    Safety and security concerns, ethical considerations related to decision-making
    algorithms, and regulatory compliance are critical aspects that need careful attention.
    Striking the right balance between innovation and responsible use of AI is crucial
    for the successful and sustainable integration of generative AI in the automotive
    sector.
  prefs: []
  type: TYPE_NORMAL
- en: Contact center customer service operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI has the potential to revolutionize the entire customer operations
    function, improving the customer experience and agent productivity through digital
    self-service and enhancing and augmenting agent skills. The technology has already
    gained traction in customer service because of its ability to automate interactions
    with customers using natural language. Contact centers are critical customer service
    operations across sectors like finance, telecom, and healthcare. Key responsibilities
    include workforce staffing, performance monitoring, agent training, and customer
    engagement.
  prefs: []
  type: TYPE_NORMAL
- en: However, several challenges plague contact center workflows – high call volumes,
    agent attrition, inconsistent service quality, meeting customer speed and personalization
    expectations, and agent burnout. Multilingual support and extracting insights
    from customer feedback add complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI can address many of these challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: For agent training, models can synthesize guided learning content from call
    transcripts and history. Generative AI can enhance quality assurance and coaching
    by gathering insights from customer conversations, determining what could be done
    better, and coaching agents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During customer interactions, AI assistants provide agents with real-time recommendations
    and answers to boost resolution rates. For example, generative AI can instantly
    retrieve data a company has on a specific customer, which can help a human customer
    service representative more successfully answer questions and resolve issues during
    an initial interaction. Generative AI can cut the time a human sales representative
    spends responding to a customer by providing assistance in real time and recommending
    the next steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For self-service, generative AI-fueled chatbots can give immediate and personalized
    responses to complex customer inquiries regardless of the language or location
    of the customer. By improving the quality and effectiveness of interactions via
    automated channels, generative AI could automate responses to a higher percentage
    of customer inquiries, enabling customer care teams to take on inquiries that
    can only be resolved by a human agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-call analysis by generative models identifies areas for improvement from
    conversation data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mining dialogs also reveals customer needs and intents to generate cross-sell
    opportunities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling call centers with generative AI capabilities requires the integration
    of LLMs, semantic search engines, and contact center applications. The following
    diagram shows an architecture for enabling Amazon Connect (an AWS call center
    service) with generative AI and chatbot capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a phone  Description automatically generated](img/B20836_16_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.16: Generative AI-powered contact center self-service'
  prefs: []
  type: TYPE_NORMAL
- en: With careful implementation, generative AI can drive significant operational
    efficiencies, service quality improvements, and customer experience breakthroughs
    across the contact center. With all the possibilities, it is also important to
    recognize the potential limitations of adopting generative AI for contact centers
    such as providing factually incorrect answers due to hallucination and misunderstanding
    of user queries, risk of exposing sensitive and private information, and introducing
    potentially harmful and biased responses.
  prefs: []
  type: TYPE_NORMAL
- en: Are we close to having artificial general intelligence?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial General Intelligence** (**AGI**) is a field within theoretical
    AI research working to create AI systems with cognitive functions comparable to
    human capabilities. AGI remains a theoretical concept that’s not well defined,
    and its definition and opinions on its eventual realization vary. Nevertheless,
    loosely speaking, AGI involves AI systems/agents equipped with a broad capacity
    to understand and learn across many diverse domains and address diverse problems
    in various contexts, not just narrow expertise in one field. These systems should
    have the ability to generalize the knowledge they gain, transfer learning from
    one domain, and apply knowledge and skills to novel situations and problems like
    humans do.'
  prefs: []
  type: TYPE_NORMAL
- en: The impressive capabilities displayed by LLMs and diffusion models have generated
    a lot of excitement about the potential to achieve AGI. Their ability to perform
    reasonably well across a wide variety of natural language processing and image
    generation tasks with minimal fine-tuning seems closer to flexible human-like
    intelligence than the previous narrow AI systems. As a result, there is increasingly
    optimistic speculation among some researchers and the media about whether we are
    on the brink of achieving true AGI through just the scaling of the data, model,
    and compute.
  prefs: []
  type: TYPE_NORMAL
- en: However, most AI experts caution that we still have a long way to go to realize
    fully general and human-level intelligence. While very broad in scope, FMs remain
    confined to language and visual domains. Moreover, their knowledge and reasoning
    abilities remain brittle and narrow compared to humans. Transferring learning
    across radically different tasks and knowledge domains remains difficult for current
    FMs. Moreover, multimodality is integral to human intelligence, and while significant
    progress has been made in multimodal AI, it is still early to have seamlessly
    integrated understanding and reasoning of different modalities like text, image,
    video, sound, touch, social cues, etc. Thus, while the capabilities of LLMs like
    GPT and Anthropic Claude represent notable progress, we are still far from replicating
    the robustness, flexibility, and multidimensional qualities of human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help measure the progress of AGI, Google’s DeepMind has published an AGI
    levels guide using performance and generality (narrow tasks vs. a range of general
    tasks) as the two dimensions. On the performance dimension, there are six levels
    in the guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 0 – No AI**: At this level, AI is not used for either narrow or general
    intelligent tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Level 1 – Emergent**: At this level, AI is equal to or better than unskilled
    humans in either some narrow tasks or a range of general tasks. The guide states
    that FMs such as GPT, Bard, Llama 2, Claude, and Gemini have achieved this level
    of maturity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Level 2 – Competent**: At this level, AI is better than at least 50% of skilled
    adults. According to the guide, some narrowly focused AI technology has achieved
    this level on some narrow tasks such as AI assistants like Siri and Alexa, essay
    writing, and coding. However, AGI has not achieved this level of progress.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Level 3 – Expert**: At this level, AI is better than at least 90% of skilled
    adults. Technologies such as AI grammar checkers and image generators have achieved
    this level on specific narrow tasks, however, no AGI has arrived at this level.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Level 4 – Virtuoso**: At this level, AI is better than 99% of skilled adults.
    Only a few narrow AI technologies such as AlphaGo have achieved this level of
    capability on narrow AI tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Level 5 – Superhuman**: This is where AI is better than 100% of humans. Again,
    only narrow AI such as AlphaFold (a protein folding model), AlphaZero (an AI model
    that plays the game of Go), and Stockfish (an open-source chess engine) has achieved
    this level of capability. So what is the path to achieving AGI? No one has claimed
    they know the definitive answer yet, but the pursuit continues through the exploration
    of various active research areas such as multi-modality understanding, memory
    management, cross-domain reasoning and planning, and continuous self-learning.
    Furthermore, leading AI experts have been proposing diverse theoretical approaches
    in these domains, ranging from the integration of symbolic reasoning and connectionist
    architectures to the study of emergent phenomena, and the whole organism approach.
    In the subsequent section, let’s explore the symbolic, connectionist, and neural-symbolic
    approaches.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The symbolic approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The symbolic approach relies on explicitly representing knowledge and reasoning
    using symbolic representations, such as logical statements, rules, and structured
    data formats. In this paradigm, the knowledge about a particular domain is manually
    encoded into the system using formal languages and logical formalisms. An example
    of symbolic representation could be “dogs have four legs.” This encoded knowledge
    forms a knowledge base, which acts as a repository of facts, concepts, relationships,
    and rules that define how the world works within that domain. The symbolic AI
    system then uses an inference engine, which is a component that applies logical
    operations and rules of reasoning to the knowledge base. This allows the system
    to derive new conclusions, make inferences, and solve problems by manipulating
    and combining the symbolic representations in a structured and logical manner.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a symbolic AI system designed for medical diagnosis, the knowledge
    base might contain rules that represent the relationships between symptoms, diseases,
    and treatments. The inference engine could then use these rules to analyze a patient’s
    symptoms and logically deduce the most likely diagnosis and appropriate treatment
    plan.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of symbolic AI is its ability to provide explainable
    and interpretable reasoning. Since the knowledge and reasoning processes are explicitly
    represented, the system’s decision-making process can be traced and understood
    by humans, which is particularly important in domains such as finance, law, and
    healthcare, where transparency and accountability are crucial.
  prefs: []
  type: TYPE_NORMAL
- en: However, symbolic AI systems also face challenges, such as the knowledge acquisition
    bottleneck, where manually encoding vast amounts of knowledge can be time-consuming
    and labor-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, these systems often struggle with handling ambiguity, uncertainty,
    and context-dependent knowledge, which are more easily tackled by other AI approaches,
    such as ML and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_16_17.png)Figure 16.17: Symbolic approach'
  prefs: []
  type: TYPE_NORMAL
- en: One prominent example of symbolic projects is the Cyc project, which stands
    as one of the longest-running symbolic AI initiatives. This ambitious endeavor
    aims to construct a comprehensive ontology and knowledge base that captures common
    sense rules about how the world operates. The Cyc project employs formal logic
    as its primary mechanism for reasoning and inference.
  prefs: []
  type: TYPE_NORMAL
- en: While groundbreaking in its scope and ambition, the Cyc project also highlights
    key challenges inherent to the symbolic AI approach. These challenges include
    the knowledge acquisition bottleneck, which refers to the arduous task of manually
    encoding vast amounts of knowledge in the system. Additionally, the project grapples
    with issues of brittleness, where slight deviations from the encoded rules or
    representations can lead to unexpected or erroneous behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability concerns also arise, as the complexity of symbolic systems can rapidly
    escalate as the knowledge base expands, potentially leading to computational intractability.
    Furthermore, the robust handling of nuance, uncertainty, and context-dependent
    interpretations remains a formidable challenge within the symbolic paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these obstacles, the Cyc project’s pioneering efforts have contributed
    significantly to the field of symbolic AI, pushing the boundaries of knowledge
    representation and reasoning capabilities. Its ongoing development continues to
    shed light on both the potential and limitations of the symbolic approach in the
    quest for artificial general intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: While symbolic AI laid the crucial groundwork for establishing formal methods
    for knowledge representation and reasoning, many researchers believe that purely
    symbolic systems alone are unlikely to be sufficient for realizing the flexibility,
    robustness, and open-ended generalization required for general intelligence comparable
    to humans.
  prefs: []
  type: TYPE_NORMAL
- en: The debate surrounding the extent to which symbolic approaches should be incorporated
    into AGI systems – whether as a core architecture or in a more complementary role
    – remains an active area of research with differing perspectives. Critics argue
    that the inherent limitations of symbolic systems, such as brittleness, scalability
    issues, and the knowledge acquisition bottleneck, pose significant challenges
    in capturing the nuanced, context-dependent, and continuously evolving nature
    of human intelligence. As a result, many advocate for a synergistic approach that
    combines the strengths of symbolic methods with other paradigms, leveraging the
    interpretability and strong generalization capabilities of symbolic reasoning
    while mitigating its weaknesses through complementary techniques, such as connectionist
    models or hybrid architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The connectionist/neural network approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach focuses on constructing systems that emulate the human brain.
    Neural network systems, like the brain, employ extensive parallel processing across
    interconnected nodes or “neurons,” distributing knowledge across connections.
    Unlike explicit symbolic encodings or rules such as “a dog is an animal,” neural
    models rely on sub-symbolic distributed representations for the same concept.
    In sub-symbolic representation, which is used in approaches like neural networks
    and connectionist models, knowledge or concepts are represented not through explicit
    symbols or rules, but through patterns of features and characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated neural network approaches aspire to develop comprehensive end-to-end
    cognitive architectures covering perception, reasoning, and action. The hypothesis
    is that scaling up neural networks in both architecture and training data may
    induce the emergence of general intelligence, resembling the neural connections
    in the brain. However, current neural nets face challenges, as they are often
    narrow, lack systematicity, and encounter difficulties with transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_16_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.18: Connectionist or neural network approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the remarkable capabilities demonstrated by LLMs, a compelling question
    arises: Can these models, arguably the most advanced neural networks to date,
    pave the way toward AGI? This question has sparked contrasting viewpoints within
    the AI community. Yann LeCun, the chief AI scientist at Meta, contends that the
    current autoregressive approach employed by LLMs is unlikely to lead to AGI, as
    these models are primarily trained to predict the next token rather than engage
    in genuine planning or reasoning processes. According to LeCun, current LLMs lack
    the ability to truly comprehend and reason about knowledge; instead, they retrieve
    and generate information in an approximate manner.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Ilya Sutskever, chief scientist at OpenAI, seems to lean
    toward the perspective that, with sufficient data and increasingly larger architectures,
    LLMs may indeed develop a profound understanding of semantic meanings, potentially
    leading to AGI. Some alternative viewpoints suggest that LLMs, especially multimodal
    variants capable of integrating diverse information sources, combined with their
    ability to leverage different tools, could achieve a certain level of general
    intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: As newer and more capable models are developed, and as more innovative systems
    are created around these models, only time will tell whether the current approach
    can ultimately lead to AGI. The ongoing advancements and debates within the field
    underscore the complexity and uncertainty surrounding this quest, while simultaneously
    fueling the relentless pursuit of more powerful AI.
  prefs: []
  type: TYPE_NORMAL
- en: The neural-symbolic approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural-symbolic approach aims to combine the best of both worlds by integrating
    neural networks with symbolic reasoning systems. The idea is to create AI systems
    that can learn patterns and representations from data using neural networks, while
    also leveraging the explicit knowledge and logical reasoning capabilities of symbolic
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple analogy: Imagine a young child is learning about the world.
    The child’s brain (the neural network component) can learn and recognize patterns,
    like shapes, colors, and objects, through experience and exposure. However, to
    truly understand and reason about the world, the child also needs to learn explicit
    rules, concepts, and knowledge from teachers, books, and other sources (the symbolic
    component).'
  prefs: []
  type: TYPE_NORMAL
- en: In a neural-symbolic AI system, the neural network component would be responsible
    for learning patterns and representations from data, just like the child’s brain.
    At the same time, the symbolic component would provide a structured knowledge
    base and logical reasoning capabilities, similar to the child learning from books
    and teachers.
  prefs: []
  type: TYPE_NORMAL
- en: By tightly integrating these two components, the AI system can leverage the
    strengths of both approaches. It can learn and adapt like neural networks, while
    also reasoning and making inferences using explicit knowledge and logical rules,
    much like humans do.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural-symbolic approach is seen by many researchers as a promising path
    toward achieving AGI, as it aims to capture the key aspects of human-like intelligence:
    the ability to learn from experience, reason with structured knowledge, and adapt
    to new situations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_16_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.19: Neural-symbolic architecture'
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGeometry, developed by DeepMind, is an innovative AI system that leverages
    the neural-symbolic approach to tackle complex geometric reasoning tasks. This
    system combines the powerful pattern recognition and learning capabilities of
    deep neural networks with the structured reasoning and symbolic manipulation of
    geometric knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of AlphaGeometry lies a neural network component that processes
    visual inputs and generates candidate symbolic expressions representing geometric
    concepts and relationships. These symbolic expressions are then passed to a symbolic
    reasoning engine, which employs logical rules and constraints to validate, refine,
    and manipulate the expressions. The symbolic output is then interpreted and used
    to guide the neural network’s predictions, enabling the system to iteratively
    improve its geometric understanding.
  prefs: []
  type: TYPE_NORMAL
- en: By tightly integrating neural and symbolic components, AlphaGeometry can effectively
    combine the strengths of both paradigms. The neural network excels at learning
    patterns and extracting geometric features from visual data, while the symbolic
    component provides a structured representation of geometric knowledge and enables
    logical reasoning over complex relationships.
  prefs: []
  type: TYPE_NORMAL
- en: This neural-symbolic approach allows AlphaGeometry to achieve impressive performance
    on challenging geometric tasks, outperforming previous methods and demonstrating
    an ability to generalize to novel problems. It showcases the potential of hybrid
    systems in advancing AI capabilities, particularly in domains that require both
    robust pattern recognition and structured reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: As the pursuit for AGI continues to captivate researchers and pioneers across
    multiple disciplines, the future holds both immense opportunities and formidable
    challenges. While connectionist approaches have demonstrated remarkable pattern
    recognition and learning capabilities, pushing the boundaries of these models
    to achieve the breadth and flexibility of human-level intelligence remains an
    ongoing endeavor. Symbolic approaches, with their explicit knowledge representation
    and reasoning prowess, offer a complementary pathway, yet struggles with the knowledge
    acquisition bottleneck and brittleness issues persist. The neural-symbolic paradigm,
    seamlessly fusing the strengths of these two worlds, emerges as a highly promising
    avenue.
  prefs: []
  type: TYPE_NORMAL
- en: Irrespective of the path, the realization of AGI hinges on our ability to synergize
    diverse approaches, harness the exponential growth of data and computing power,
    and deepen our understanding of intelligence itself. As researchers continue their
    pursuit in this uncharted territory, unprecedented breakthroughs and paradigm
    shifts await.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now coming to the end of this book spanning the breadth of machine learning
    – from foundational concepts to cutting-edge generative AI. We started the book
    by covering core ML techniques, algorithms, and industry applications to provide
    a strong base. We then progressed to data architectures, ML tools like TensorFlow
    and PyTorch, and engineering best practices to put skills into practice. Architecting
    robust ML infrastructure on AWS and optimization methods prepared you for real-world
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Securing and governing AI responsibly is critical, so we delved into risk management.
    To guide organizations on the ML journey, we discussed maturity models and evolutionary
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Closing the chapter by looking at generative AI and AGI, we explored the immense
    possibilities of the most disruptive new capability currently. Specifically, we
    delved into the intricacies of generative AI platforms, RAG architecture, and
    considerations for generative AI production deployment. Furthermore, we examined
    practical generative AI business applications across various industries, showcasing
    the transformative potential of this technology. Finally, the chapter concluded
    with an introduction to various theoretical approaches for achieving artificial
    general intelligence, providing a glimpse into the future of this rapidly evolving
    field.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this book enriching and that it provides a comprehensive foundation
    to propel your AI learning to new heights. The concepts and frameworks covered
    aim to equip you to build practical ML solutions. Keep learning, practicing, and
    developing your skills to maximize the value of AI. The future promises to be
    exponentially more exciting!
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoyed this book? Help readers like you by leaving an Amazon review. Scan the
    QR code below to get a free eBook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_Copy.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Limited Offer*'
  prefs: []
  type: TYPE_NORMAL
