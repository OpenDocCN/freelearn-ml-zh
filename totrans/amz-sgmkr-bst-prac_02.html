<html><head></head><body>
		<div id="_idContainer028">
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Chapter 1: Amazon SageMaker Overview</h1>
			<p>This chapter will provide a high-level overview of the Amazon SageMaker capabilities that map to the various phases of the <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) process. This will set a foundation for the best practices discussion of using SageMaker capabilities in order to handle various data science challenges. </p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Preparing, building, training and tuning, deploying, and managing ML models</li>
				<li>Discussion of data preparation capabilities</li>
				<li>Feature tour of model-building capabilities</li>
				<li>Feature tour of training and tuning capabilities</li>
				<li>Feature tour of model management and deployment capabilities </li>
			</ul>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Technical requirements</h1>
			<p>All notebooks with coding exercises will be available at the following GitHub link:</p>
			<p><a href="https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices">https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices</a></p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Preparing, building, training and tuning, deploying, and managing ML models</h1>
			<p>First, let's review the <a id="_idIndexMarker000"/>ML life cycle. By the end of this section, you should understand how SageMaker's capabilities map to the key phases of the ML life cycle. The following diagram shows you what the ML life cycle looks like:</p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B17249_01_001.jpg" alt="Figure 1.1 – Machine learning life cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – Machine learning life cycle</p>
			<p>As you can see, there are three phases<a id="_idIndexMarker001"/> of the ML life cycle at a high level:</p>
			<ul>
				<li>In the <strong class="bold">Data Preparation</strong> phase, you <a id="_idIndexMarker002"/>collect and explore data, label a ground truth dataset, and prepare your features. Feature engineering, in turn, has several steps, including data normalization, encoding, and calculating embeddings, depending on the ML algorithm you choose. </li>
				<li>In the <strong class="bold">Model Training</strong> phase, you <a id="_idIndexMarker003"/>build your model and tune it until you achieve a reasonable validation score that aligns with your business objective. </li>
				<li>In the <strong class="bold">Operations</strong> phase, you<a id="_idIndexMarker004"/> test how well your model performs against real-world data, deploy it, and monitor how well it performs. We will cover model monitoring in more detail in <a href="B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a>, <em class="italic">Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify</em>.</li>
			</ul>
			<p>This diagram is purposely simplified; in reality, each phase may have multiple smaller steps, and the whole life cycle is iterative. You're never really <em class="italic">done</em> with ML; as you gather data on how your model performs in production, you'll likely try to improve it by collecting more data, changing your features, or tuning the model.</p>
			<p>So how do SageMaker capabilities map to the ML life cycle? Before we answer that question, let's take a look at the <a id="_idIndexMarker005"/>SageMaker console (<em class="italic">Figure 1.2</em>):</p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B17249_01_002.jpg" alt="Figure 1.2 – Navigation pane in the SageMaker console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – Navigation pane in the SageMaker console</p>
			<p>The appearance of the console changes frequently and the preceding screenshot shows the current appearance of the console at the time of writing.</p>
			<p>These capability groups <a id="_idIndexMarker006"/>align to the ML life cycle, shown as<a id="_idIndexMarker007"/> follows:</p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B17249_01_003.jpg" alt="Figure 1.3 – Mapping of SageMaker capabilities to the ML life cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – Mapping of SageMaker capabilities to the ML life cycle</p>
			<p>SageMaker Studio is not shown here, as it is an integrated workbench that provides a user interface for many SageMaker capabilities. The marketplace provides both data and algorithms that can be used across the life cycle.</p>
			<p>Now that we have had a look at the console, let's dive deeper into the individual capabilities of SageMaker in each life cycle phase.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Discussion of data preparation capabilities</h1>
			<p>In this section, we'll <a id="_idIndexMarker008"/>dive into SageMaker's data preparation and feature engineering capabilities. By the end of this section, you should understand when to use SageMaker Ground Truth, Data Wrangler, Processing, Feature Store, and Clarify.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>SageMaker Ground Truth</h2>
			<p>Obtaining labeled data for classification, regression, and other tasks is often the biggest barrier to ML projects, as many companies have a lot of data but have not explicitly labeled it according to business properties such as <em class="italic">anomalous</em> and <em class="italic">high lifetime value</em>. <strong class="bold">SageMaker Ground Truth</strong> helps <a id="_idIndexMarker009"/>you systematically label data by defining a labeling workflow and assigning labeling tasks to a human workforce. </p>
			<p>Over time, Ground Truth can learn how to label data automatically, while still sending low-confidence results to humans for review. For advanced datasets such as 3D point clouds, which represent data points like shape coordinates, Ground Truth offers assistive labeling features, such as adding bounding boxes to the middle frames of a sequence once you label the start and end frames. The following diagram shows an example of labels applied to a dataset:</p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B17249_01_004.jpg" alt="Figure 1.4 – SageMaker Ground Truth showing the labels applied to sentiment reviews&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – SageMaker Ground Truth showing the labels applied to sentiment reviews</p>
			<p>The data is sourced from the <em class="italic">UCI Machine Learning Repository</em> (<a href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a>). To counteract individual worker bias or error, a data object can be sent to multiple workers. In this example, we only have one worker, so the confidence score is not used.</p>
			<p>Note that you can also use Ground Truth in other phases of the ML life cycle; for example, you may use it to check the labels generated by a production model.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>SageMaker Data Wrangler</h2>
			<p><strong class="bold">Data Wrangler</strong> helps you understand <a id="_idIndexMarker010"/>your data and perform feature engineering. Data Wrangler works with data stored in S3 (optionally accessed via Athena) and Redshift and performs typical visualization and transformations, such as <em class="italic">correlation plots</em> and <em class="italic">categorical encoding</em>. You can combine a series of transformations into a data flow and export that flow into an MLOps pipeline. The following screenshot shows an example of Data Wrangler information for a dataset:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B17249_01_005.jpg" alt="Figure 1.5 – Data Wrangler displaying summary table information regarding a dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – Data Wrangler displaying summary table information regarding a dataset</p>
			<p>You may also use Data Wrangler in the operations phase of the ML life cycle if you want to analyze the data coming into an ML model for production inference.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>SageMaker Processing</h2>
			<p><strong class="bold">SageMaker Processing</strong> jobs help you<a id="_idIndexMarker011"/> run data processing and feature engineering tasks on your datasets. By providing your own Docker image containing your code, or using a pre-built Spark or sklearn container, you can normalize and transform data to prepare your features. The following diagram shows the logical flow of a SageMaker Processing job:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B17249_01_006.jpg" alt="Figure 1.6 – Conceptual overview of a Spark processing job. Spark jobs are particularly handy for processing larger datasets"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Conceptual overview of a Spark processing job. Spark jobs are particularly handy for processing larger datasets</p>
			<p>You may also use <a id="_idIndexMarker012"/>processing jobs to evaluate the performance of ML models during the <strong class="bold">Model Training</strong> phase and to check data and model quality in the <strong class="bold">Model Operations</strong> phase.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>SageMaker Feature Store</h2>
			<p><strong class="bold">SageMaker Feature Store</strong> helps <a id="_idIndexMarker013"/>you organize and share your prepared features. Using a feature store improves quality and saves time by letting you reuse features rather than duplicate complex feature engineering code and computations that have already been done. Feature Store supports both batch and stream storage and retrieval. The following screenshot shows an example of feature group information:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B17249_01_007.jpg" alt="Figure 1.7 – Feature Store showing a feature group with a set of related features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – Feature Store showing a feature group with a set of related features</p>
			<p>Feature Store also helps during the <a id="_idIndexMarker014"/>Model Operations phase, as you can quickly look up complex feature vectors to help obtain real-time predictions.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>SageMaker Clarify</h2>
			<p><strong class="bold">SageMaker Clarify</strong> helps you <a id="_idIndexMarker015"/>understand model behavior and calculate bias metrics from your model. It checks for imbalance in the dataset, models that give different results based on certain attributes, and bias that appears due to data drift. It can also use leading explainability algorithms such as SHAP to help you explain individual predictions to get a sense of which features drive model behavior. The following figure shows an example of class imbalance scores for a dataset, where we have many more samples from the <em class="italic">Gift Card</em> category than the other categories:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B17249_01_008.jpg" alt="Figure 1.8 – Clarify showing class imbalance scores in a dataset. Class imbalance can lead to biased results in an ML model"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – Clarify showing class imbalance scores in a dataset. Class imbalance can lead to biased results in an ML model</p>
			<p>Clarify can be used <a id="_idIndexMarker016"/>throughout the entire ML life cycle, but consider using it early in the life cycle to detect imbalanced data (datasets that have many examples of one class but few of another).</p>
			<p>Now that we've introduced several SageMaker capabilities for data preparation, let's move on to model-building capabilities.</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Feature tour of model-building capabilities</h1>
			<p>In this section, we'll<a id="_idIndexMarker017"/> dive into SageMaker's model-building capabilities. By the end of this section, you should understand when to use SageMaker Studio or SageMaker notebook instances, and how to choose between SageMaker's built-in algorithms, frameworks, and<a id="_idIndexMarker018"/> libraries, versus a <strong class="bold">bring your own</strong> (<strong class="bold">BYO</strong>) approach.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>SageMaker Studio</h2>
			<p><strong class="bold">SageMaker Studio</strong> is an <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) for ML. It brings together Jupyter <a id="_idIndexMarker019"/>notebooks, experiment management, and other tools into a unified user interface. You can easily share notebooks and notebook snapshots with other team members using Git or a shared filesystem. The following screenshot shows an example of one of SageMaker Studio's built-in visualizations:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B17249_01_009.jpg" alt="Figure 1.9 – SageMaker Studio showing an experiment graph&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – SageMaker Studio showing an experiment graph</p>
			<p>SageMaker Studio can be used in all phases of the ML life cycle.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>SageMaker notebook instances</h2>
			<p>If you prefer a more <a id="_idIndexMarker020"/>traditional Jupyter or JupyterLab experience, and you don't need the additional integrations and collaboration tools that Studio provides, you can use a regular SageMaker notebook instance. You choose the notebook instance compute capacity (that is, whether you want GPUs and how much storage you need), and SageMaker provisions the environment with the Jupyter Notebook and JupyterLab and several of the common ML frameworks and libraries installed. </p>
			<p>The notebook instance also <a id="_idIndexMarker021"/>supports Docker in case you want to build and test containers with ML code locally. Best of all, the notebook instances come bundled with over 100 example notebooks. The following figure shows an example of the JupyterLab interface in a notebook:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B17249_01_010.jpg" alt="Figure 1.10 – JupyterLab interface in a SageMaker notebook, showing a list of example notebooks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – JupyterLab interface in a SageMaker notebook, showing a list of example notebooks</p>
			<p>Similar to SageMaker Studio, you can perform almost any part of the ML life cycle in a notebook instance.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>SageMaker algorithms</h2>
			<p>SageMaker bundles open source and <a id="_idIndexMarker022"/>proprietary algorithms for many common ML use cases. These algorithms are a good starting point as they are tuned for performance, often supporting distributed training. The following table lists the SageMaker algorithms provided for different types of ML problems:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B17249_01_011.jpg" alt="Figure 1.11 – SageMaker algorithms for various ML scenarios&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – SageMaker algorithms for various ML scenarios</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>BYO algorithms and scripts</h2>
			<p>If you prefer to write your own <a id="_idIndexMarker023"/>training and inference code, you can work with a <a id="_idIndexMarker024"/>supported ML, graph, or RL framework, or bundle your own code into a Docker image. The BYO approach works well if you already have a library of model code, or if you need to build a model for a use case where a pre-built algorithm doesn't work well. Data scientists who use R like to use this approach. SageMaker<a id="_idIndexMarker025"/> supports the following frameworks:</p>
			<ul>
				<li>Supported machine learning frameworks: XGBoost, sklearn</li>
				<li>Supported deep learning frameworks: TensorFlow, PyTorch, MXNet, Chainer</li>
				<li>Supported reinforcement learning frameworks: Ray RLLib, Coach</li>
				<li>Supporting graph frameworks: Deep Graph Library</li>
			</ul>
			<p>Now that we've introduced several SageMaker capabilities for model building, let's move on to training and tuning capabilities.</p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Feature tour of training and tuning capabilities</h1>
			<p>In this section, we'll dive into SageMaker's <a id="_idIndexMarker026"/>model training capabilities. By the end of this section, you should understand the basics of SageMaker training jobs, Autopilot and Hyperparameter Optimization (HPO), SageMaker Debugger, and SageMaker Experiments.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>SageMaker training jobs</h2>
			<p>When you launch a model <a id="_idIndexMarker027"/>training job, SageMaker manages a series of steps for you. It launches one or more training instances, transfers training data from S3 or other supported storage systems to the instances, gets your training code from a Docker image repository, and starts the job. It monitors job progress and collects model artifacts and metrics from the job. The following screenshot shows an example of the hyperparameters tracked in a training job:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B17249_01_012.jpg" alt="Figure 1.12 – SageMaker training jobs capture data such as input hyperparameter values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – SageMaker training jobs capture data such as input hyperparameter values</p>
			<p>For larger training datasets, SageMaker <a id="_idIndexMarker028"/>manages distributed training. It will distribute subsets of data from storage to different training instances and manage the inter-node communication during the training job. The specifics vary based on the ML framework you're using, but note that most of the supported frameworks and several of the SageMaker built-in algorithms support distributed training.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Autopilot</h2>
			<p>If you are working with <a id="_idIndexMarker029"/>tabular data and solving regression or classification problems, you may find that you're performing a lot of repetitive work. You may have settled on XGBoost as a high-performing algorithm, always one-hot encoding for low-cardinality categorical features, normalizing numeric features, and so on. Autopilot performs many of these routine steps for you. In the following diagram, you can see the logical steps for an Autopilot job:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B17249_01_013.jpg" alt="Figure 1.13 – Autopilot process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – Autopilot process</p>
			<p>Autopilot <a id="_idIndexMarker030"/>saves you time by automating a lot of that routine process. It will run normal feature preparation tasks, try the three supported algorithms (Linear Learner, XGBoost, and a multilayer perceptron), and run hyperparameter tuning. Autopilot is a great place to start even if you end up needing to refine the output, as it generates a notebook with the code used for the entire process. </p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>HPO</h2>
			<p>Some ML algorithms accept tens of hyperparameters as inputs. Tuning these by hand is time-consuming. <strong class="bold">Hyperparameter Optimization</strong> (<strong class="bold">HPO</strong>) simplifies<a id="_idIndexMarker031"/> that process by letting you define the hyperparameters you want to experiment with, the ranges to work over, and the metric you want to optimize. The following screenshot shows example output for an HPO job:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B17249_01_014.jpg" alt="Figure 1.14 – Hyperparameter tuning jobs showing the objective metric of interest&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – Hyperparameter tuning jobs showing the objective metric of interest</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>SageMaker Debugger</h2>
			<p><strong class="bold">SageMaker Debugger</strong> helps<a id="_idIndexMarker032"/> you debug and, depending on your ML framework, profile your training jobs. While making training jobs run faster is always helpful, debugging is particularly useful if you are writing your own deep learning code with neural networks. Problems such as exploding gradients or mysterious <strong class="source-inline">NaN</strong> in your tensors are quite tough to track down, particularly in distributed training jobs. Debugger can effectively help you set breakpoints to see where things are going wrong. The following figure shows an example of the training and validation loss captured by SageMaker Debugger:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B17249_01_015.jpg" alt="Figure 1.15 – Visualization of tensors captured by SageMaker Debugger&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.15 – Visualization of tensors captured by SageMaker Debugger</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>SageMaker Experiments</h2>
			<p>ML is an iterative process. When <a id="_idIndexMarker033"/>you're tuning a model, you may try several variations of hyperparameters, features, and even algorithms. It's important to track that work systematically so you can reproduce your results later on. That's where SageMaker Experiments comes into the picture. It helps you track, organize, and compare different trials. The following screenshot shows an example of SageMaker Experiments information:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B17249_01_016.jpg" alt="Figure 1.16 – Trial results in SageMaker Experiments&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.16 – Trial results in SageMaker Experiments</p>
			<p>Now that we've<a id="_idIndexMarker034"/> introduced several SageMaker capabilities for training and tuning, let's move on to model management and deployment capabilities.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Feature tour of model management and deployment capabilities</h1>
			<p>In this section, we'll <a id="_idIndexMarker035"/>dive into SageMaker's model hosting and monitoring capabilities. By the end of this section, you should understand the basics of SageMaker model endpoints along with the <a id="_idIndexMarker036"/>use of <strong class="bold">SageMaker Model Monitor</strong>. You'll also learn about deploying<a id="_idIndexMarker037"/> models on edge devices with <strong class="bold">SageMaker Edge Manager</strong>.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Model Monitor </h2>
			<p>In some <a id="_idIndexMarker038"/>organizations, the gap between the ML team and the operations team causes real problems. Operations teams may not understand how to monitor an ML system in production, and ML teams don't always have deep operational expertise. </p>
			<p>Model Monitor tries to solve that problem: it will instrument a model endpoint and collect data about the inputs to, and outputs from, an ML model used for inference. It can then analyze that data for data drift and other quality problems, as well as model accuracy or quality problems. The following diagram shows an example of model monitoring data captured for an inference endpoint:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B17249_01_017.jpg" alt="Figure 1.17 – Model Monitor checking data quality on inference inputs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.17 – Model Monitor checking data quality on inference inputs</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Model endpoints</h2>
			<p>In some cases, you need to <a id="_idIndexMarker039"/>get a large number of inferences at once, in which case SageMaker provides a batch inference capability. But if you need to get inferences closer to real time, you can host your model in a SageMaker managed endpoint. SageMaker handles the deployment and scaling of your endpoints. Just as important, SageMaker lets you host multiple models in a single endpoint. That's useful both for <strong class="bold">A/B testing</strong> (that is, you can direct some percentage of traffic to a newer model) and for hosting <a id="_idIndexMarker040"/>multiple models that are tuned for different traffic segments. </p>
			<p>You can also host an inference pipeline with<a id="_idIndexMarker041"/> multiple containers chained together, which is convenient if you need to preprocess inputs before performing inference. The following screenshot shows a model endpoint with two models serving different percentages of traffic:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B17249_01_018.jpg" alt="Figure 1.18 – Multiple models configured behind a single inference endpoint&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.18 – Multiple models configured behind a single inference endpoint</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Edge Manager</h2>
			<p>In some cases, you need to get <a id="_idIndexMarker042"/>model inferences on a device rather than from the cloud. You may need a lower response time that doesn't allow for an API call to the cloud, or you may have intermittent network connectivity. In video use cases, it's not always feasible to stream data to the cloud for inference. In such cases, <strong class="bold">Edge Manager</strong> and related tools such as <strong class="bold">SageMaker Neo</strong> help <a id="_idIndexMarker043"/>you compile models optimized to run on devices, deploy them, manage them, and get operational metrics back to the cloud. The following screenshot shows an example of a virtual device managed by Edge Manager:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B17249_01_019.jpg" alt="Figure 1.19 – A device registered to an Edge Manager device fleet&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.19 – A device registered to an Edge Manager device fleet</p>
			<p>Before we conclude with the summary, let's <a id="_idIndexMarker044"/>have a recap of the SageMaker capabilities provided for the <a id="_idIndexMarker045"/>following primary ML phases:</p>
			<ul>
				<li>For data preparation:</li>
			</ul>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B17249_01_20.jpg" alt="Figure 1.20 – SageMaker capabilities for data preparation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.20 – SageMaker capabilities for data preparation</p>
			<ul>
				<li>For <a id="_idIndexMarker046"/>operations:</li>
			</ul>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B17249_01_021.jpg" alt="Figure 1.21 – SageMaker capabilities for operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.21 – SageMaker capabilities for operations</p>
			<ul>
				<li>For <a id="_idIndexMarker047"/>model training:</li>
			</ul>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B17249_01_22.jpg" alt="Figure 1.22 – SageMaker capabilities for model training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.22 – SageMaker capabilities for model training</p>
			<p>With this, we have come to the end of this chapter.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Summary</h1>
			<p>In this chapter, you saw how to map SageMaker capabilities to different phases of the ML life cycle. You got a quick look at important SageMaker capabilities. In the next chapter, you will learn about the technical requirements and the use case that will be used throughout. You'll also learn about setting up managed data science environments for scaling model-building activities. </p>
		</div>
	</body></html>