- en: Supervised Learning Using Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行监督学习
- en: 'In this chapter, we will develop, test, and evaluate supervised machine learning
    models applied to a variety of real-world use cases using Python, Apache Spark,
    and its machine learning library, `MLlib`. Specifically, we will train, test,
    and interpret the following types of supervised machine learning models:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python、Apache Spark及其机器学习库`MLlib`开发、测试和评估应用于各种现实世界用例的监督机器学习模型。具体来说，我们将训练、测试和解释以下类型的监督机器学习模型：
- en: Univariate linear regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单变量线性回归
- en: Multivariate linear regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: Logistic regression
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Classification and regression trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: Random forests
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Linear regression
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: The first supervised learning model that we will study is that of linear regression.
    Formally, linear regression models the relationship between a *dependent* variable
    using a set of one or more *independent* variables. The resulting model can then
    be used to predict the numerical value of the *dependent* variable. But what does
    this mean in practice? Well, let's look at our first real-world use case to make
    sense of this.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的第一个监督学习模型是线性回归。形式上，线性回归使用一组一个或多个**独立**变量来建模**依赖**变量之间的关系。得到的模型可以用来预测**依赖**变量的数值。但这在实践中意味着什么呢？好吧，让我们看看我们的第一个实际应用案例来理解这一点。
- en: Case study – predicting bike sharing demand
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 预测自行车共享需求
- en: Bike sharing schemes have become very popular across the world over the last
    decade or so as people seek a convenient means to travel within busy cities while
    limiting their carbon footprint and helping to reduce road congestion. If you
    are unfamiliar with bike sharing systems, they are very simple; people rent a
    bike from certain locations in a city and thereafter return that bike to either
    the same or another location once they have finished their journey. In this example,
    we will be examining whether we can predict the daily demand for bike sharing
    systems given the weather on a particular day!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年左右的时间里，随着人们寻求在繁忙的城市中出行的同时减少碳足迹并帮助减少道路拥堵，自行车共享计划在全球范围内变得越来越受欢迎。如果你对自行车共享系统不熟悉，它们非常简单；人们可以从城市中的特定地点租用自行车，并在完成旅程后将其归还到同一地点或另一个地点。在本例中，我们将探讨是否可以根据特定一天的天气预测自行车共享系统的日需求量！
- en: 'The dataset that we will be using has been derived from the **University of
    California''s** (**UCI**) machine learning repository found at [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php).
    The specific bike sharing dataset that we will use, available from both the GitHub
    repository accompanying this book and from [https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset),
    has been cited by Fanaee-T, Hadi, and Gama, Joao, ''Event labeling combining ensemble
    detectors and background knowledge,'' Progress in Artificial Intelligence (2013):
    pp. 1-15, Springer Berlin Heidelberg.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的数据集是从位于[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)的加州大学（**UCI**）机器学习仓库中派生出来的。我们将使用的特定自行车共享数据集，可以从本书附带的GitHub仓库以及[https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)获取，已被Fanaee-T,
    Hadi, 和Gama, Joao在《Event labeling combining ensemble detectors and background
    knowledge》一文中引用，该文发表于2013年的《Progress in Artificial Intelligence》，Springer Berlin
    Heidelberg出版社，第1-15页。
- en: 'If you open `bike-sharing-data/day.csv`in any text editor, from either the
    GitHub repository accompanying this book or from UCI''s machine learning repository,
    you will find bike sharing data aggregated on a daily basis over 731 days using
    the following schema:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开`bike-sharing-data/day.csv`文件，无论是从本书附带的GitHub仓库还是从UCI的机器学习仓库，你将找到使用以下模式对731天内的自行车共享数据进行每日汇总的数据：
- en: '| **Column name** | **Data type** | **Description** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **列名** | **数据类型** | **描述** |'
- en: '| `instant` | `Integer` | Unique record identifier (primary key) |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `instant` | `整数` | 唯一记录标识符（主键） |'
- en: '| `dteday` | `Date` | Date |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `dteday` | `日期` | 日期 |'
- en: '| `season` | `Integer` | Season (1 – spring, 2 – summer, 3 – fall, 4 – winter)
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `season` | `整数` | 季节（1 – 春季，2 – 夏季，3 – 秋季，4 – 冬季） |'
- en: '| `yr` | `Integer` | Year |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `yr` | `整数` | 年份 |'
- en: '| `mnth` | `Integer` | Month |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `mnth` | `整数` | 月份 |'
- en: '| `holiday` | `Integer` | Day is a holiday or not |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `holiday` | `整数` | 该日是否为假日 |'
- en: '| `weekday` | `Integer` | Day of the week |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `weekday` | `整数` | 周几 |'
- en: '| `workingday` | `Integer` | 1 – neither a weekend nor a holiday, 0 – otherwise
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `workingday` | `整数` | 1 – 既不是周末也不是假日，0 – 否则 |'
- en: '| `weathersit` | `Integer` | 1 – clear, 2 – mist, 3 – light snow, 4 – heavy
    rain |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `weathersit` | `Integer` | 1 – 清晰，2 – 薄雾，3 – 小雪，4 – 大雨 |'
- en: '| `temp` | `Double` | Normalized temperature in Celsius |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `temp` | `Double` | 标准化温度（摄氏度） |'
- en: '| `atemp` | `Double` | Normalized feeling temperature in Celsius |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `atemp` | `Double` | 标准化的感觉温度（摄氏度） |'
- en: '| `hum` | `Double` | Normalized humidity |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `hum` | `Double` | 标准化湿度 |'
- en: '| `windspeed` | `Double` | Normalized wind speed |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `windspeed` | `Double` | 标准化风速 |'
- en: '| `casual` | `Integer` | Count of casual users for that day |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `casual` | `Integer` | 当日非注册用户数量 |'
- en: '| `registered` | `Integer` | Count of registered users for that day |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `registered` | `Integer` | 当日注册用户数量 |'
- en: '| `cnt` | `Integer` | Count of total bike renters that day |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `cnt` | `Integer` | 当日总自行车租赁者数量 |'
- en: Using this dataset, can we predict the total bike renters for a given day (*cnt*)
    given the weather patterns for that particular day? In this case, *cnt* is the
    *dependent* variable that we wish to predict based on a set of *independent* variables
    that we shall choose from.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据集，我们能否根据特定一天的天气模式预测该天的总自行车租赁者数量（*cnt*）？在这种情况下，*cnt* 是我们希望根据我们选择的 *自变量*
    集合预测的 *因变量*。
- en: Univariate linear regression
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单变量线性回归
- en: Univariate (or single-variable) linear regression refers to a linear regression
    model where we use only one independent variable *x* to learn a *linear* function
    that maps *x* to our dependent variable *y:*
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量（或单变量）线性回归是指我们只使用一个自变量 *x* 来学习一个将 *x* 映射到因变量 *y* 的 *线性* 函数的线性回归模型：
- en: '![](img/e16cc5f8-50c1-4e2f-b9c8-7cfcfb1aa96f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e16cc5f8-50c1-4e2f-b9c8-7cfcfb1aa96f.png)'
- en: 'In the preceding equation, we have the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下内容：
- en: '*y^i* represents the *dependent* variable (cnt) for the *i^(th)* observation'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y^i* 代表第 *i* 次观察的 *因变量*（cnt）'
- en: '*x^i* represents the single *independent* variable for the *i^(th)* observation'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x^i* 代表第 *i* 次观察的单个 *自变量*'
- en: ε^(*i*) represents the *error* term for the *i^(th)* observation
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ε^(*i*) 代表第 *i* 次观察的 *误差* 项
- en: '*β[0]* is the intercept coefficient'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*β[0]* 是截距系数'
- en: '*β[1]* is the regression coefficient for the single independent variable'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*β[1]* 是单个自变量的回归系数'
- en: 'Since, in general form, a univariate linear regression model is a linear function,
    we can easily plot this on a scatter graph where the x-axis represents the single
    independent variable, and the y-axis represents the dependent variable that we
    are trying to predict. *Figure 4.1* illustrates the scatter plot generated when
    we plot normalized feeling temperature (independent variable) against total daily
    bike renters (dependent variable):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于单变量线性回归模型在一般形式上是一个线性函数，我们可以在散点图上轻松地绘制它，其中x轴代表单个自变量，y轴代表我们试图预测的因变量。*图4.1*展示了当我们绘制标准化的感觉温度（自变量）与每日总自行车租赁者（因变量）的散点图时生成的散点图：
- en: '![](img/12c53f09-7f55-47e1-b8e7-210d12522132.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12c53f09-7f55-47e1-b8e7-210d12522132.png)'
- en: 'Figure 4.1: Normalized temperature against total daily bike renters'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：标准化温度与每日总自行车租赁者的关系
- en: By analyzing *Figure 4.1*, you will see that there seems to be a general positive
    linear trend between the normalized feeling temperature (**atemp**) and the total
    daily biker renters (**cnt**). However, you will also see that our blue trend
    line, which is the visual representation of our univariate linear regression function,
    is not perfect, in other words, not all of our data points fit exactly on this
    line. In the real world, it is extremely rare to have a perfect model; in other
    words, all predictive models will make some mistakes. The goal therefore is to
    minimize the number of mistakes our models make so that we may have confidence
    in the predictions that they provide.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析 *图4.1*，你会看到标准化的感觉温度（**atemp**）和每日总自行车租赁者（**cnt**）之间似乎存在一种普遍的正线性趋势。然而，你也会看到我们的蓝色趋势线，这是我们的单变量线性回归函数的视觉表示，并不完美，换句话说，并不是所有的数据点都完全位于这条线上。在现实世界中，几乎不可能有一个完美的模型；换句话说，所有预测模型都会犯一些错误。因此，目标是尽量减少模型犯的错误数量，以便我们对它们提供的预测有信心。
- en: Residuals
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差
- en: 'The errors (or mistakes) that our model makes are called error terms or *residuals*,
    and are denoted in our univariate linear regression equation by ε^(*i*). Our goal
    therefore is to choose regression coefficients for the independent variables (in
    our case *β[1]*) that minimize these residuals. To compute the *i^(th)* residual,
    we can simply subtract the predicted value from the actual value, as illustrated
    in *Figure 4.1*. To quantify the quality of our regression line, and hence our
    regression model, we can use a metric called the **Sum of Squared Errors** (**SSE**),
    which is simply the sum of all squared residuals, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型所犯的错误（或错误）被称为误差项或*残差*，在我们的单变量线性回归方程中用 ε^(*i*) 表示。因此，我们的目标是选择独立变量的回归系数（在我们的情况下
    *β[1]*）以最小化这些残差。为了计算第 *i* 个残差，我们可以简单地从实际值中减去预测值，如*图4.1*所示。为了量化回归线的质量，以及我们的回归模型，我们可以使用一个称为**均方误差总和**（**SSE**）的指标，它简单地是所有平方残差的和，如下所示：
- en: '![](img/18a90520-c676-47aa-8d27-6ca44ac2dbf9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/18a90520-c676-47aa-8d27-6ca44ac2dbf9.png)'
- en: A smaller SSE implies a better fit. However, SSE as a metric to quantify the
    quality of our regression model has its limitations. SSE scales with the number
    of data points *N*, which means that if we doubled the number of data points,
    the SSE may be twice as large, which may lead you to believe that the model is
    twice as bad, which is not the case! We therefore require other means to quantify
    the quality of our model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的 SSE 表示更好的拟合。然而，SSE 作为衡量我们回归模型质量的指标有其局限性。SSE 与数据点的数量 *N* 成比例，这意味着如果我们加倍数据点的数量，SSE
    可能会加倍，这可能会让你认为模型是两倍糟糕，但这并不是事实！因此，我们需要其他方法来衡量我们模型的质量。
- en: Root mean square error
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方根误差
- en: 'The **root mean square error** (**RMSE**) is the square root of the SSE divided
    by the total number of data points *N*, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）是 SSE 的平方根除以数据点的总数 *N*，如下所示：'
- en: '![](img/dff36623-b263-4ab0-8f39-9c455faf0dc6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dff36623-b263-4ab0-8f39-9c455faf0dc6.png)'
- en: The RMSE tends to be used more often as a means to quantify the quality of a
    linear regression model, since its units are the same as the dependent variable,
    and is normalized by N.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE（均方根误差）通常被用作量化线性回归模型质量的手段，因为它的单位与因变量相同，并且通过 N 进行归一化。
- en: R-squared
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R-squared
- en: 'Another metric that provides an error measure of a linear regression model
    is called the R^(*2*) (R-squared) metric. The R² metric represents the proportion
    of *variance* in the dependent variable explained by the independent variable(s).
    The equation for calculating R² is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个提供线性回归模型错误度量的指标称为 R^(*2*)（R-squared）指标。R² 指标表示因变量中由独立变量（或多个变量）解释的*方差*的比例。计算
    R² 的方程如下：
- en: '![](img/8baa308e-d8df-4565-bcc7-2b0b5bd2d866.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8baa308e-d8df-4565-bcc7-2b0b5bd2d866.png)'
- en: In this equation, SST refers to the **Total Sum of Squares**, which is just
    the SSE from the overall mean (as illustrated in *Figure 4.1* by the red horizontal
    line, which is often used as a **baseline** model). An R² value of 0 implies a
    linear regression model that provides no improvement over the baseline model (in
    other words, SSE = SST). An R² value of 1 implies a perfect predictive linear
    regression model (in other words, SSE = 0). The aim therefore is to get an R²
    value as close as possible to 1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，SST 指的是**总平方和**，它只是从整体均值（如*图4.1*中的红色水平线所示，常被用作**基准模型**）的 SSE。R² 值为 0
    表示线性回归模型没有比基准模型提供任何改进（换句话说，SSE = SST）。R² 值为 1 表示完美的预测线性回归模型（换句话说，SSE = 0）。因此，目标是使
    R² 值尽可能接近 1。
- en: Univariate linear regression in Apache Spark
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 中的单变量线性回归
- en: 'Returning to our case study, let''s develop a univariate linear regression
    model in Apache Spark using its machine learning library, `MLlib`, in order to
    predict the total daily bike renters using our bike sharing dataset:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的案例研究，让我们使用 Apache Spark 的机器学习库 `MLlib` 来开发一个单变量线性回归模型，以预测使用我们的共享单车数据集的每日总租车量：
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-01-univariate-linear-regression.ipynb`,
    and which may be found in the GitHub repository accompanying this book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下子部分描述了对应于本用例的 Jupyter Notebook 中相关的每个单元格，该笔记本的标题为 `chp04-01-univariate-linear-regression.ipynb`，并且可以在随本书附带的
    GitHub 仓库中找到。
- en: 'First, we import the required Python dependencies, including `pandas` (Python
    data analysis library), `matplotlib` (Python plotting library), and `pyspark` (Apache
    Spark Python API). By using the `%matplotlib` magic function, any plots that we
    generate will automatically be rendered within the Jupyter Notebook cell output:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的Python依赖项，包括`pandas`（Python数据分析库）、`matplotlib`（Python绘图库）和`pyspark`（Apache
    Spark Python API）。通过使用`%matplotlib`魔法函数，我们生成的任何图表将自动在Jupyter Notebook单元格输出中渲染：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Before we instantiate a Spark context, it is generally a good idea to load
    a sample of any pertinent dataset into `pandas` so that we may identify any trends
    or patterns before developing a predictive model. Here, we use the `pandas` library
    to load the entire CSV into a `pandas` DataFrame called `bike_sharing_raw_df` (since
    it is a very small dataset anyway):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实例化Spark上下文之前，通常将任何相关数据集的样本加载到`pandas`中是一个好主意，这样我们可以在开发预测模型之前识别出任何趋势或模式。在这里，我们使用`pandas`库将整个CSV加载到名为`bike_sharing_raw_df`的`pandas`
    DataFrame中（因为数据集本身非常小）：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In cells 3.1 to 3.4, we use the `matplotlib` library to plot various independent
    variables (`temp`, `atemp`, `hum`, and `windspeed`) against the dependent variable
    (`cnt`):'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在3.1到3.4单元格中，我们使用`matplotlib`库将各种独立变量（`temp`、`atemp`、`hum`和`windspeed`）与因变量（`cnt`）绘制在一起：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see in *Figure 4.2*, there is a general positive linear relationship
    between the normalized temperatures (`temp` and `atemp`) and the total daily bike
    renters (cnt). However, there is no such obvious trend when using humidity and
    wind speed as our independent variables. Therefore, we will proceed to develop
    a univariate linear regression model using normalized feeling temperature (`atemp`)
    as our single independent variable, with total daily bike renters (`cnt`) being
    our dependent variable:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.2*所示，标准化温度（`temp`和`atemp`）与每日总自行车租赁者（`cnt`）之间存在一般的正线性关系。然而，当使用湿度和风速作为独立变量时，没有这样的明显趋势。因此，我们将继续开发一个使用标准化感觉温度（`atemp`）作为单一独立变量的单变量线性回归模型，其中总每日自行车租赁者（`cnt`）作为因变量：
- en: '![](img/5415139f-e95c-4d00-a697-a67b1968a7df.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5415139f-e95c-4d00-a697-a67b1968a7df.png)'
- en: 'Figure 4.2: Bike sharing scatter plot'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：共享单车散点图
- en: 'In order to develop a Spark application, we need to first instantiate a Spark
    context (as described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*) to connect to our local Apache Spark cluster. We also
    instantiate a Spark `SQLContext` for the structured processing of our dataset:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了开发Spark应用程序，我们首先需要实例化一个Spark上下文（如[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，《大数据生态系统》）以连接到我们的本地Apache
    Spark集群。我们还实例化一个Spark `SQLContext`以对数据进行结构化处理：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now load our CSV dataset into a Spark DataFrame (see [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*) called `bike_sharing_df`. We use the `SQLContext` previously
    defined and we tell Spark to use the first row as the header row and to infer
    the schema data types:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的CSV数据集加载到名为`bike_sharing_df`的Spark DataFrame中（参见[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，《大数据生态系统》）。我们使用先前定义的`SQLContext`，并告诉Spark使用第一行作为标题行，并推断数据类型：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before developing a predictive model, it is also a good idea to generate standard
    statistical metrics for a dataset so as to gain additional insights. Here, we
    generate the row count for the DataFrame, as well as calculating the mean average,
    standard deviation, and the minimum and maximum for each column. We achieve this
    using the `describe()` method for a Spark DataFrame as follows:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开发预测模型之前，生成数据集的标准统计指标也是一个好主意，以便获得额外的见解。在这里，我们生成DataFrame的行数，以及计算每列的平均值、标准差、最小值和最大值。我们使用以下Spark
    DataFrame的`describe()`方法实现这一点：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now demonstrate how to plot a dataset using a Spark DataFrame as an input.
    In this case, we simply convert the Spark DataFrame into a `pandas` DataFrame
    before plotting as before (note that for very large datasets, it is recommended
    to use a representative sample of the dataset for plotting purposes):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们演示如何使用Spark DataFrame作为输入来绘制数据集。在这种情况下，我们在绘图之前简单地将Spark DataFrame转换为`pandas`
    DataFrame（注意，对于非常大的数据集，建议使用数据集的代表样本进行绘图）：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have finished our exploratory analysis, we can start developing
    our univariate linear regression model! First, we need to convert our independent
    variable (`atemp`) into a *numerical feature vector* (see [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml),
    *Artificial Intelligence and Machine Learning*). We can achieve this using MLlib''s `VectorAssembler`, which
    will take one or more feature columns, convert them into feature vectors, and
    store those feature vectors in an output column, which, in this example, is called
    `features`:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了我们的探索性分析，我们可以开始开发我们的单变量线性回归模型！首先，我们需要将我们的独立变量（`atemp`）转换为一个*数值特征向量*（见[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)，*人工智能与机器学习*）。我们可以通过使用MLlib的`VectorAssembler`来实现这一点，它将一个或多个特征列，将它们转换为特征向量，并将这些特征向量存储在一个输出列中，在这个例子中，它被称为`features`：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then apply the `VectorAssembler` *transformer* (see [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*) to the raw dataset and identify the column
    that contains our label (in this case, our dependent variable `cnt`). The output
    is a new Spark DataFrame called `bike_sharing_features_df` containing our independent
    numerical feature vectors (`atemp`) mapped to a known label (`cnt`):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`VectorAssembler` *转换器*（见[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)，*人工智能与机器学习*）应用于原始数据集，并识别包含我们的标签的列（在这种情况下，我们的因变量`cnt`）。输出是一个新的Spark
    DataFrame，称为`bike_sharing_features_df`，它包含我们的独立数值特征向量（`atemp`）映射到一个已知的标签（`cnt`）：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As per supervised learning models in general, we need a *training* dataset
    to train our model in order to learn the mapping function, and a *test* dataset
    in order to evaluate the performance of our model. We can randomly split our raw
    labeled feature vector DataFrame using the `randomSplit()` method and a seed,
    which is used to initialize the random generator, and which can be any number
    you like. Note that if you use a different seed, you will get a different random
    split between your training and test dataset, which means that you may get slightly
    different coefficients for your final linear regression model:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据监督学习模型的一般情况，我们需要一个*训练*数据集来训练我们的模型以学习映射函数，以及一个*测试*数据集来评估我们模型的性能。我们可以使用`randomSplit()`方法和一个种子随机分割我们的原始标记特征向量DataFrame，种子用于初始化随机生成器，可以是任何你喜欢的数字。请注意，如果你使用不同的种子，你将在训练和测试数据集之间得到不同的随机分割，这意味着你可能会得到最终线性回归模型的不同系数：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In our case, 75% of the original rows will form our training DataFrame called
    `train_df`, with the remaining 25% forming our test DataFrame called `test_df`,
    while using a `seed` of `12345`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，75%的原始行将形成我们的训练DataFrame，称为`train_df`，剩下的25%将形成我们的测试DataFrame，称为`test_df`，同时使用`seed`为`12345`。
- en: 'We are now ready to train our univariate linear regression model! We achieve
    this by using `MLlib`''s `LinearRegression` estimator (see [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml),
    *Artificial Intelligence and Machine Learning*) and passing it the name of the
    column containing our independent numerical feature vectors (in our case, called
    `features`) and the name of the column containing our labels (in our case, called
    `cnt`). We then apply the `fit()` method to train our model and output a linear
    regression *transformer* which, in our case, is called `linear_regression_model`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在已准备好训练我们的单变量线性回归模型！我们通过使用`MLlib`的`LinearRegression`估计器（见[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)，*人工智能与机器学习*）并将包含我们独立数值特征向量的列名（在我们的例子中，称为`features`）和包含我们标签的列名（在我们的例子中，称为`cnt`）传递给它来实现这一点。然后我们应用`fit()`方法来训练我们的模型并输出一个线性回归*转换器*，在我们的例子中，它被称为`linear_regression_model`：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Before we evaluate our trained univariate linear regression model on the test
    DataFrame, let''s generate some summary statistics for it. The transformer model
    exposes a series of statistics, including model coefficients (in other words, *β[1]* in
    our case), the intercept coefficient *β[0]*, the error metrics RMSE and R², and
    the set of residuals for each data point. In our case, we have the following:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们在测试DataFrame上评估我们的训练好的单变量线性回归模型之前，让我们为它生成一些摘要统计信息。转换器模型公开了一系列统计信息，包括模型系数（在我们的情况下，即*β[1]*），截距系数*β[0]*，错误度量RMSE和R²，以及每个数据点的残差集。在我们的情况下，我们有以下内容：
- en: β[0] = 829.62
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: β[0] = 829.62
- en: β[1] = 7733.75
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: β[1] = 7733.75
- en: RMSE = 1490.12
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE = 1490.12
- en: R² = 0.42
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: R² = 0.42
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Therefore, our trained univariate linear regression model has learned the following
    function in order to be able to predict our dependent variable *y* (total daily
    bike renters) using a single independent variable *x* (normalized feeling temperature):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的训练一元线性回归模型已经学习到了以下函数，以便能够使用单个自变量*x*（标准化感觉温度）来预测我们的因变量*y*（总日自行车租赁量）：
- en: '*y = 829.62 + 7733.75x*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = 829.62 + 7733.75x*'
- en: 'Let''s now apply our trained model to our test DataFrame in order to evaluate
    its performance on test data. Here, we apply our trained linear regression model
    transformer to the test DataFrame using the `transform()` method in order to make
    predictions. For example, our model predicts a total daily bike rental count of
    1742 given a normalized feeling temperature of 0.11793\. The actual total daily
    bike rental count was 1416 (an error of 326):'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将我们的训练模型应用于测试DataFrame，以评估其在测试数据上的性能。在这里，我们使用`transform()`方法将我们的训练线性回归模型转换器应用于测试DataFrame，以便进行预测。例如，我们的模型预测在标准化感觉温度为0.11793的情况下，总日自行车租赁量为1742。实际的总日自行车租赁量为1416（误差为326）：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now compute the same RMSE and R² error metrics, but based on the performance
    of our model on the *test *DataFrame. In our case, these are 1534.51 (RMSE) and
    0.34 (R²) respectively, calculated using `MLlib`''s `RegressionEvaluator`. So,
    in our case, our trained model actually performs more poorly on the test dataset:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在计算相同的RMSE和R²误差指标，但基于模型在*测试*DataFrame上的性能。在我们的案例中，这些是1534.51（RMSE）和0.34（R²），分别使用`MLlib`的`RegressionEvaluator`计算得出。因此，在我们的案例中，我们的训练模型在测试数据集上的表现实际上更差：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that we can generate the same metrics but using the `evaluate()` method
    of the linear regression model, as shown in the following code block:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们可以使用线性回归模型的`evaluate()`方法生成相同的指标，如下面的代码块所示：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we terminate our Spark application by stopping the Spark context:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过停止Spark上下文来终止我们的Spark应用程序：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Multivariate linear regression
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: Our univariate linear regression model actually performed relatively poorly
    on both the training and test datasets, with R² values of 0.42 on the training
    dataset and 0.34 on the test dataset respectively. Is there any way we can take
    advantage of the other independent variables available in our raw dataset to increase
    the predictive quality of our model?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的多元线性回归模型在训练集和测试集上的表现实际上相对较差，分别在训练集上具有0.42的R²值，在测试集上具有0.34的R²值。我们是否有办法利用原始数据集中可用的其他自变量来提高我们模型的预测质量？
- en: 'Multivariate (or multiple) linear regression extends univariate linear regression
    by allowing us to utilize more than one independent variable, in this case *K*
    independent variables, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 多元（或多个）线性回归通过允许我们利用一个以上的自变量来扩展一元线性回归，在这种情况下，*K*个自变量，如下所示：
- en: '![](img/92ac9e48-9e36-422a-b81b-5edbb280f7c0.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/92ac9e48-9e36-422a-b81b-5edbb280f7c0.png)'
- en: As before, we have our dependent variable *y^i* (for the *i^(th)* observation),
    an intercept coefficient *β[0],* and our residuals ε^(*i*). But we also now have
    *k* independent variables, each with their own regression coefficient, *β[k]*.
    The goal, as before, is to derive coefficients that minimize the amount of error
    that our model makes. The problem now though is how to choose which subset of
    independent variables to use in order to train our multivariate linear regression
    model. Adding more independent variables increases the complexity of models in
    general and, hence, the data storage and processing requirements of underlying
    processing platforms. Furthermore, models that are too complex tend to cause **overfitting**,
    whereby the model achieves better performance (in other words, a higher *R²* metric)
    on the training dataset used to train the model than on new data that it has not
    seen before.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们有因变量*y^i*（对于第*i*个观察值），截距系数*β[0]*，以及残差ε^(*i*)。但现在我们还有*k*个自变量，每个自变量都有自己的回归系数，*β[k]*。目标，如前所述，是推导出系数，以最小化模型产生的误差量。但问题现在是如何选择用于训练多元线性回归模型的独立变量子集。增加更多的自变量通常会增加模型的一般复杂性，从而增加底层处理平台的数据存储和处理需求。此外，过于复杂的模型往往会引起**过拟合**，即模型在用于训练模型的训练数据集上的性能（换句话说，更高的*R²*指标）优于它之前未见过的数据。
- en: Correlation
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关性
- en: 'Correlation is a metric that measures the linear relationship between two variables,
    and helps us to decide which independent variables to include in our model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性是一个衡量两个变量之间线性关系的指标，并帮助我们决定在模型中包含哪些自变量：
- en: +1 implies a perfect positive linear relationship
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +1表示完美的正线性关系
- en: 0 implies no linear relationship
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示没有线性关系
- en: -1 implies a perfect negative linear relationship
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -1表示完美的负线性关系
- en: When two variables have an *absolute* value of correlation close to 1, then
    these two variables are said to be "highly correlated".
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个变量的相关性的绝对值接近1时，这两个变量被认为是“高度相关”的。
- en: Multivariate linear regression in Apache Spark
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的多元线性回归
- en: 'Returning to our case study, let''s now develop a multivariate linear regression
    model in order to predict the total daily bike renters using our bike sharing
    dataset and a subset of independent variables:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的案例研究，现在让我们使用我们的共享单车数据集和独立变量子集来开发一个多元线性回归模型，以预测总日租车数量：
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-02-multivariate-linear-regression.ipynb`,
    and which may be found in the GitHub repository accompanying this book. Note that
    for the sake of brevity, we will skip those cells that perform the same functions
    as seen previously.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下子节描述了对应于本用例的Jupyter Notebook中每个相关的单元格，标题为`chp04-02-multivariate-linear-regression.ipynb`，并可在本书附带的GitHub仓库中找到。请注意，为了简洁起见，我们将跳过那些执行与之前相同功能的单元格。
- en: 'First, let''s demonstrate how we can use Spark to calculate the correlation
    value between our dependent variable, `cnt`, and each independent variable in
    our DataFrame. We achieve this by iterating over each column in our raw Spark
    DataFrame and using the `stat.corr()` method as follows:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们演示如何使用Spark计算我们的因变量`cnt`与DataFrame中的每个独立变量之间的相关值。我们通过遍历我们的原始Spark DataFrame中的每一列，并使用`stat.corr()`方法来实现这一点，如下所示：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The resultant correlation matrix shows that the independent variables—`season`, `yr`, `mnth`, `temp` ,
    and `atemp`, exhibit significant positive correlation with our dependent variable `cnt`.
    We will therefore proceed to train a multivariate linear regression model using
    this subset of independent variables.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的相关矩阵显示，独立变量——`season`、`yr`、`mnth`、`temp`和`atemp`与我们的因变量`cnt`表现出显著的积极相关性。因此，我们将继续使用这个独立变量子集来训练多元线性回归模型。
- en: 'As seen previously, we can apply a `VectorAssembler` in order to generate numerical
    feature vector representations of our collection of independent variables along
    with the `cnt` label. The syntax is identical to that seen previously, but this
    time we pass multiple columns to the `VectorAssembler` representing the columns
    containing our independent variables:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用`VectorAssembler`来生成我们独立变量集合的数值特征向量表示，以及`cnt`标签。语法与之前相同，但这次我们向`VectorAssembler`传递多个列，代表包含我们的独立变量的列：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We are now ready to generate our respective training and test datasets using
    the `randomSplit` method via the DataFrame API:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用DataFrame API通过`randomSplit`方法生成各自的训练和测试数据集：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can now train our multivariate linear regression model using the same `LinearRegression`
    estimator that we used in our univariate linear regression model:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用与我们的单变量线性回归模型中使用的相同的`LinearRegression`估计器来训练我们的多元线性回归模型：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After splitting our original DataFrame into a training and test DataFrame respectively,
    and applying the same *LinearRegression* estimator to the training DataFrame,
    we now have a trained multivariate linear regression model with the following
    summary training statistics (as can be seen in cell 8 of this Jupyter Notebook):'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将我们的原始DataFrame分别拆分为训练和测试DataFrame后，并将相同的*LinearRegression*估计器应用于训练DataFrame后，我们现在有一个训练好的多元线性回归模型，以下是其总结训练统计信息（如本Jupyter
    Notebook的第8个单元格所示）：
- en: β[0] = -389.94, β[1] = 526.05, β[2] = 2058.85, β[3] = -51.90, β[4] = 2408.66,
    β[5] = 3502.94
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: β[0] = -389.94, β[1] = 526.05, β[2] = 2058.85, β[3] = -51.90, β[4] = 2408.66,
    β[5] = 3502.94
- en: RMSE = 1008.50
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE = 1008.50
- en: R² = 0.73
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: R² = 0.73
- en: 'Therefore, our trained multivariate linear regression model has learned the
    following function in order to be able to predict our dependent variable *y* (total
    daily bike renters) using a set of independent variables *x[k]* (season, year,
    month, normalized temperature, and normalized feeling temperature):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的训练好的多元线性回归模型已经学习到了以下函数，以便能够使用一组独立变量*x[k]*（季节、年份、月份、标准化温度和标准化感觉温度）来预测我们的因变量*y*（总日租车数量）：
- en: '*y = -389.94 + 526.05x[1] + 2058.85x[2] - 51.90x[3] + 2408.66x[4] + 3502.94x[5]*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = -389.94 + 526.05x[1] + 2058.85x[2] - 51.90x[3] + 2408.66x[4] + 3502.94x[5]*'
- en: Furthermore, our trained multivariate linear regression model actually performs
    even better on the test dataset with a test RMSE of 964.60 and a test R² of 0.74.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的训练多元线性回归模型在测试数据集上的表现甚至更好，测试 RMSE 为 964.60，测试 R² 为 0.74。
- en: To finish our discussion of multivariate linear regression models, note that
    our training R² metric will always either increase or stay the same as more independent
    variables are added. However, a better training R² metric does not always imply
    a better test R² metric—in fact, a test R² metric can even be negative, meaning
    that it performs worse on the test dataset than the baseline model (which can
    never be the case for the training R² metric). The goal therefore is to be able
    to develop a model that works well for both the training and test datasets.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们对多元线性回归模型的讨论，请注意，随着更多独立变量的添加，我们的训练 R² 指标将始终增加或保持不变。然而，更好的训练 R² 指标并不总是意味着更好的测试
    R² 指标——事实上，测试 R² 指标甚至可以是负数，这意味着它在测试数据集上的表现比基线模型（训练 R² 指标永远不会是这样）更差。因此，目标是能够开发一个在训练和测试数据集上都表现良好的模型。
- en: Logistic regression
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: We have seen how linear regression models allows us to predict a numerical outcome.
    Logistic regression models, however, allow us to predict a *categorical* outcome
    by predicting the probability that an outcome is true.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到线性回归模型如何允许我们预测数值结果。然而，逻辑回归模型允许我们通过预测结果为真的概率来预测*分类*结果。
- en: 'As with linear regression, in logistic regression models, we also have a dependent
    variable *y* and a set of independent variables *x[1]*, *x[2]*, …, *x[k]*. In
    logistic regression however, we want to learn a function that provides the probability
    that *y = 1* (in other words, that the outcome variable is true) given this set
    of independent variables, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归一样，在逻辑回归模型中，我们也有一个因变量 *y* 和一组自变量 *x[1]*、*x[2]*、…、*x[k]*。然而，在逻辑回归中，我们想要学习一个函数，该函数提供在给定这组自变量的情况下
    *y = 1*（换句话说，结果变量为真）的概率，如下所示：
- en: '![](img/fcd811e4-d51a-4d40-b1b2-3c4669141f9c.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fcd811e4-d51a-4d40-b1b2-3c4669141f9c.png)'
- en: 'This function is called the **Logistic Response** function, and provides a
    number between 0 and 1, representing the probability that the outcome-dependent
    variable is true, as illustrated in *Figure 4.3*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数被称为**逻辑响应**函数，它提供一个介于 0 和 1 之间的数字，表示结果相关变量为真的概率，如图 *4.3* 所示：
- en: '![](img/6fcc1d21-faf7-44d6-9ed3-8776dc37c6fe.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6fcc1d21-faf7-44d6-9ed3-8776dc37c6fe.png)'
- en: 'Figure 4.3: Logistic response function'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：逻辑响应函数
- en: Positive coefficient values β[k] increase the probability that y = 1, and negative
    coefficient values decrease the probability that y = 1\. Our goal, therefore,
    when developing logistic regression models, is to choose coefficients that predict
    a high probability when y = 1, but predict a low probability when y = 0.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正系数值 β[k] 增加了 y = 1 的概率，而负系数值减少了 y = 1 的概率。因此，在开发逻辑回归模型时，我们的目标是选择那些在 y = 1 时预测高概率，但在
    y = 0 时预测低概率的系数。
- en: Threshold value
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阈值值
- en: 'We now know that logistic regression models provide us with the probability
    that the outcome variable is true, that is to say, y = 1\. However, in real-world
    use cases, we need to make *decisions*, not just deliver probabilities. Often,
    we make binary predictions, such as Yes/No, Good/Bad, and Go/Stop. A threshold
    value (*t*) allows us to make these decisions based on probabilities as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，逻辑回归模型为我们提供了结果变量为真的概率，也就是说，y = 1。然而，在实际应用中，我们需要做出*决策*，而不仅仅是提供概率。通常，我们会做出二元预测，例如是/否、好/坏、行/停。一个阈值值（*t*）允许我们根据概率做出以下决策：
- en: If P(y=1) >= t, then we predict y = 1
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 P(y=1) >= t，则我们预测 y = 1
- en: If P(y=1) < t, then we predict y = 0
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 P(y=1) < t，则我们预测 y = 0
- en: The challenge now is how to choose a suitable value of *t*. In fact, what does
    *suitable* mean in this context?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的挑战是如何选择一个合适的 *t* 值。实际上，在这个上下文中，“合适”是什么意思呢？
- en: In real-world use cases, some types of error are better than others. Imagine
    that you were a doctor and were testing a large group of patients for a particular
    disease using logistic regression. In this case, the outcome *y=1* would be a
    patient carrying the disease (therefore y=0 would be a patient not carrying the
    disease), and, hence, our model would provide P(y=1) for a given person. In this
    example, it is better to detect as many patients potentially carrying the disease
    as possible, even if it means misclassifying some patients as carrying the disease
    who subsequently turn out not to. In this case, we select a smaller threshold
    value. If we select a large threshold value, however, we would detect those patients
    that almost certainly have the disease, but we would misclassify a large number
    of patients as not carrying the disease when, in actual fact, they do, which would
    be a much worse scenario!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的用例中，某些类型的错误比其他类型的错误更好。想象一下，如果你是一位医生，正在使用逻辑回归对大量患者进行特定疾病的检测。在这种情况下，结果 *y=1*
    将是携带该疾病的患者（因此 y=0 将是不携带该疾病的患者），因此我们的模型将为特定个人提供 P(y=1)。在这个例子中，最好尽可能多地检测出可能携带疾病的患者，即使这意味着将一些患者错误地分类为携带疾病，但后来发现他们并不携带。在这种情况下，我们选择较小的阈值值。如果我们选择较大的阈值值，那么我们将检测出几乎肯定患有疾病的患者，但我们会错误地将大量患者分类为不携带疾病，而实际上他们确实携带，这将是一个更糟糕的情况！
- en: 'In general therefore, when using logistic regression models, we can make two
    types of error:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总的来说，当使用逻辑回归模型时，我们可以犯两种类型的错误：
- en: We predict y=1 (disease), but the actual outcome is y=0 (healthy)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测 y=1（疾病），但实际结果是 y=0（健康）
- en: We predict y=0 (healthy), but the actual outcome is y=1 (disease)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测 y=0（健康），但实际结果是 y=1（疾病）
- en: Confusion matrix
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'A confusion (or classification) matrix can help us qualify what threshold value
    to use by comparing the predicted outcomes against the actual outcomes as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一个混淆（或分类）矩阵可以帮助我们通过比较预测结果与实际结果来决定使用哪个阈值值，如下所示：
- en: '|  | **Predict y=0 (healthy)** | **Predict y=1 (disease)** |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测 y=0（健康）** | **预测 y=1（疾病）** |'
- en: '| **Actual y=0 (healthy)** | **True negatives** (**TN**) | **False positives**
    (**FP**) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **实际 y=0（健康）** | **真阴性**（**TN**） | **假阳性**（**FP**） |'
- en: '| **Actual y=1 (disease)** | **False negatives** (**FN**) | **True positives**
    (**TP**) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **实际 y=1（疾病）** | **假阴性**（**FN**） | **真阳性**（**TP**） |'
- en: 'By generating a confusion matrix, it allows us to quantify the accuracy of
    our model based on a given threshold value by using the following series of metrics:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成混淆矩阵，它允许我们使用以下一系列指标，根据给定的阈值值量化我们模型的准确性：
- en: N = number of observations
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N = 观察数
- en: Overall accuracy = (TN + TP) / N
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总准确率 = (TN + TP) / N
- en: Overall error rate = (FP + FN) / N
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总错误率 = (FP + FN) / N
- en: Sensitivity (True Positive Rate) = TP / (TP + FN)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 敏感性（真阳性率）= TP / (TP + FN)
- en: Specificity (True Negative Rate) = TN / (TN + FP)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特异性（真阴性率）= TN / (TN + FP)
- en: False positive error rate = FP / (TN + FP)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性错误率 = FP / (TN + FP)
- en: False negative error rate = FN / (TP + FN)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性错误率 = FN / (TP + FN)
- en: Logistic regression models with a higher threshold value will have a lower sensitivity
    and higher specificity. Models with a lower threshold value will have a higher
    sensitivity and lower specificity. The choice of threshold value therefore depends
    on the type of error that is "better" for your particular use case. In use cases
    where there is genuinely no preference, for example, political leaning of Conservative/Non-Conservative,
    then you should choose a threshold value of 0.5 that will predict the most likely
    outcome.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值值较高的逻辑回归模型将具有较低的敏感性和较高的特异性。阈值值较低的模型将具有较高的敏感性和较低的特异性。因此，阈值值的选择取决于对特定用例中哪种错误“更好”。在例如政治倾向为保守/非保守等没有真正偏好的用例中，你应该选择
    0.5 的阈值值，这将预测最可能的结果。
- en: Receiver operator characteristic curve
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收器操作特征曲线
- en: 'To further assist us in choosing a threshold value in a more visual way, we
    can generate a **receiver operator characteristic** (**ROC**) curve. An ROC curve
    plots the **false positive error rate** (**FPR**) against the **true positive
    rate** (**TPR**, or sensitivity) for every threshold value between 0 and 1, as
    illustrated in *Figure 4.4*:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以更直观的方式进一步帮助我们选择阈值值，我们可以生成一个**接收器操作特征**（**ROC**）曲线。ROC 曲线绘制了每个阈值值（0 到 1 之间）的**假阳性错误率**（**FPR**）与**真阳性率**（**TPR**，或敏感性）的关系，如图
    4.4 所示：
- en: '![](img/9366a343-9e08-481b-b1f5-b0b81bfb0c53.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9366a343-9e08-481b-b1f5-b0b81bfb0c53.png)'
- en: 'Figure 4.4: ROC curve'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：ROC 曲线
- en: As illustrated in *Figure 4.4*, using a threshold value of 0 means that you
    will catch ALL cases of y=1 (disease), but you will also incorrectly label all
    cases of y=0 (healthy) as y=1 (disease) as well, scaring a lot of healthy people!
    However using a threshold value of 1 means that you will NOT catch ANY cases of
    y=1 (disease), leaving a lot of people untreated, but you will correctly label
    all cases of y=0 (healthy). The benefit of plotting an ROC curve therefore is
    that it helps you to see the trade-off for *every* threshold value, and ultimately
    helps you to make a decision as to which threshold value to use for your given
    use case.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Area under the ROC curve
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a means of quantifying the quality of the predictions made by a logistic
    regression model, we can calculate the **Area under the ROC curve** (**AUC**),
    as illustrated in *Figure 4.5*. The AUC measures the proportion of time that the
    model predicts correctly, with an AUC value of 1 (maximum), implying a perfect
    model, in other words, our model predicts correctly 100% of the time, and an AUC
    value of 0.5 (minimum), implying our model predicts correctly 50% of the time,
    analogous to just guessing:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b08363c-6b44-4144-af01-3b21dc3b8799.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Area under the ROC curve'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Case study – predicting breast cancer
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now apply logistic regression to a very important real-world use case;
    predicting patients who may have breast cancer. Approximately 1 in 8 women are
    diagnosed with breast cancer during their lifetime (with the disease also affecting
    men), resulting in the premature deaths of hundreds of thousands of women annually
    across the world. In fact, it is projected that over 2 million new cases of breast
    cancer will have been reported worldwide by the end of 2018 alone. Various factors
    are known to increase the risk of breast cancer, including age, weight, family
    history, and previous diagnoses.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Using a dataset of quantitative predictors, along with a binary dependent variable
    indicating the presence or absence of breast cancer, we will train a logistic
    regression model to predict the probability of whether a given patient is healthy
    (y=1) or has the biomarkers of breast cancer (y=0).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we will use has again been derived from the University of California's
    (UCI) machine learning repository. The specific breast cancer dataset, available
    from both the GitHub repository accompanying this book and from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra),
    has been cited by [Patricio, 2018] Patrício, M., Pereira, J., Crisóstomo, J.,
    Matafome, P., Gomes, M., Seiça, R., and Caramelo, F. (2018). Using Resistin, glucose,
    age, and BMI to predict the presence of breast cancer. BMC Cancer, 18(1).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open `breast-cancer-data/dataR2.csv`in any text editor, from either
    the GitHub repository accompanying this book or from UCI''s machine learning repository,
    you will find breast cancer data that employs the following schema:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开`breast-cancer-data/dataR2.csv`在任何文本编辑器中，无论是从随本书附带的GitHub存储库还是从UCI的机器学习存储库，你将找到使用以下模式的乳腺癌数据：
- en: '| **Column name** | **Data type** | **Description** |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **列名** | **数据类型** | **描述** |'
- en: '| `Age` | `Integer` | Age of patient |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `Age` | `Integer` | 患者年龄 |'
- en: '| `BMI` | `Double` | Body mass index (kg/m²) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `BMI` | `Double` | 体质指数（kg/m²） |'
- en: '| `Glucose` | `Double` | Blood glucose level (mg/dL) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `Glucose` | `Double` | 血糖水平（mg/dL） |'
- en: '| `Insulin` | `Double` | Insulin level (µU/mL) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `Insulin` | `Double` | 胰岛素水平（µU/mL） |'
- en: '| `HOMA` | `Double` | Homeostatic Model Assessment (HOMA) – used to assess
    β-cell function and insulin sensitivity |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `HOMA` | `Double` | 代谢稳态模型评估（HOMA）- 用于评估β细胞功能和胰岛素敏感性 |'
- en: '| `Leptin` | `Double` | Hormone used to regulate energy expenditure (ng/mL)
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `Leptin` | `Double` | 用于调节能量消耗的激素（ng/mL） |'
- en: '| `Adiponectin` | `Double` | Protein hormone used to regulate glucose levels
    (µg/mL) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| `Adiponectin` | `Double` | 用于调节血糖水平的蛋白质激素（µg/mL） |'
- en: '| `Resistin` | `Double` | Hormone that causes insulin resistance (ng/mL) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| `Resistin` | `Double` | 导致胰岛素抵抗的激素（ng/mL） |'
- en: '| `MCP.1` | `Double` | Protein to aid recovery from injury and infection (pg/dL)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `MCP.1` | `Double` | 有助于从伤害和感染中恢复的蛋白质（pg/dL） |'
- en: '| `Classification` | `Integer` | 1 = Healthy patient as part of a control group,
    2 = patient with breast cancer |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `分类` | `Integer` | 1 = 作为对照组的健康患者，2 = 乳腺癌患者 |'
- en: Using this dataset, can we develop a logistic regression model that calculates
    the probability of a given patient being healthy (in other words, y=1) and thereafter
    apply a threshold value to make a predictive decision?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据集，我们能否开发一个逻辑回归模型，该模型计算给定患者健康的概率（换句话说，y=1），然后应用阈值值进行预测决策？
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-03-logistic-regression.ipynb`,
    and which may be found in the GitHub repository accompanying this book. Note that,
    for the sake of brevity, we will skip those cells that perform the same functions
    as seen previously.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下子部分描述了对应于本用例的Jupyter Notebook中相关的每个单元格，该笔记本的标题为`chp04-03-logistic-regression.ipynb`，并且可以在随本书附带的GitHub存储库中找到。请注意，为了简洁起见，我们将跳过那些执行与之前相同功能的单元格。
- en: 'After loading our breast cancer CSV file, we first identify the column that
    will act as our label, that is to say, `Classification`. Since the values in this
    column are either 1 (healthy) or 2 (breast cancer patient), we will apply a `StringIndexer` to
    this column to identify and index all the possible categories. The result is that
    a label of 1 corresponds to a healthy patient, and a label of 0 corresponds to
    a breast cancer patient:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在加载我们的乳腺癌CSV文件后，我们首先确定将作为标签的列，即`分类`。由于该列中的值要么是1（健康）要么是2（乳腺癌患者），我们将对此列应用`StringIndexer`以识别和索引所有可能的类别。结果是，标签1对应健康患者，标签0对应乳腺癌患者：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In our case, we will use all the raw quantitative columns [`Age`, `BMI`, `Glucose`, `Insulin`, `HOMA`, `Leptin`, `Adiponectin`, `Resistin`,
    and `MCP.1`] as independent variables in order to generate numerical feature vectors
    for our model. Again, we can use the `VectorAssembler` of `MLlib` to achieve this:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将使用所有原始定量列【`年龄`、`BMI`、`血糖`、`胰岛素`、`HOMA`、`Leptin`、`Adiponectin`、`Resistin`和`MCP.1`】作为独立变量，以生成模型的数值特征向量。同样，我们可以使用`MLlib`的`VectorAssembler`来实现这一点：
- en: '[PRE21]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After generating training and test DataFrames respectively, we apply the `LogisticRegression` estimator
    of `MLlib` to train a `LogisticRegression` model transformer:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成训练和测试DataFrame后，我们应用`MLlib`的`LogisticRegression`估计器来训练一个`LogisticRegression`模型转换器：
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We then use our trained logistic regression model to make predictions on the
    test DataFrame, using the `transform()` method of our logistic regression model
    transformer. This results in a new DataFrame with the columns `rawPrediction`,
    `prediction`, and `probability` appended to it. The probability of y=1, in other
    words, P(y=1), is contained within the `probability` column, and the overall predictive
    decision using a default threshold value of t=0.5 is contained within the `prediction` column:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用训练好的逻辑回归模型对测试DataFrame进行预测，使用我们的逻辑回归模型转换器的`transform()`方法。这导致一个新的DataFrame，其中附加了`rawPrediction`、`prediction`和`probability`列。y=1的概率，换句话说，P(y=1)，包含在`probability`列中，而使用默认阈值值t=0.5的整体预测决策包含在`prediction`列中：
- en: '[PRE24]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To quantify the quality of our trained logistic regression model, we can plot
    an ROC curve and calculate the AUC metric. The ROC curve is generated using the `matplotlib` library,
    given the **false positive rate** (**FPR**) and **true positive rate** (**TPR**),
    as exposed by evaluating our trained logistic regression model on the test DataFrame.
    We can then use `MLlib`''s `BinaryClassificationEvaluator` to calculate the AUC
    metric as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了量化我们训练好的逻辑回归模型的质量，我们可以绘制ROC曲线并计算AUC指标。ROC曲线使用`matplotlib`库生成，给定**假阳性率**（**FPR**）和**真阳性率**（**TPR**），这是通过在测试DataFrame上评估我们的训练好的逻辑回归模型来暴露的。然后，我们可以使用`MLlib`的`BinaryClassificationEvaluator`来计算AUC指标如下：
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The resultant ROC curve, generated using the `matplotlib` library, is illustrated
    in *Figure 4.6*:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`matplotlib`库生成的结果ROC曲线如图4.6所示：
- en: '![](img/d2252ec8-5665-4f13-b5ca-f39a89803285.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2252ec8-5665-4f13-b5ca-f39a89803285.png)'
- en: 'Figure 4.6: ROC curve rendered using `matplotlib`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：使用`matplotlib`渲染的ROC曲线
- en: 'One method of generating a confusion matrix based on the test dataset predictions
    is to simply filter the test predictions'' DataFrame based on cases where the
    predicted outcome equals, and does not equal, the actual outcome and thereafter
    count the number of records post-filter:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于测试数据集预测生成混淆矩阵的一种方法是根据预测结果等于和不等于实际结果的情况过滤测试预测的DataFrame，然后计算过滤后的记录数：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Alternatively, we can use MLlib''s RDD API (which is in maintenance mode as
    of Spark 2.0) to automatically generate the confusion matrix by converting the
    test predictions'' DataFrame into an RDD (see [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*), and thereafter passing it to the `MulticlassMetrics` evaluation
    abstraction:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，我们可以使用MLlib的RDD API（自Spark 2.0起处于维护模式）通过将测试预测的DataFrame转换为RDD自动生成混淆矩阵（见[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，*大数据生态系统*），然后传递给`MulticlassMetrics`评估抽象：
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The confusion matrix for our logistic regression model, using a default threshold
    value of 0.5, is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逻辑回归模型的混淆矩阵，使用默认阈值值为0.5，如下所示：
- en: '|  | **Predict y=0 (breast cancer)** | **Predict y=1 (healthy)** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测y=0（乳腺癌）** | **预测y=1（健康）** |'
- en: '| **Actual y=0****(breast cancer)** | 10 | 6 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **实际y=0****（乳腺癌）** | 10 | 6 |'
- en: '| **Actual y=1****(healthy)** | 4 | 8 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **实际y=1****（健康）** | 4 | 8 |'
- en: 'We can interpret this confusion matrix as follows. Out of a total of 28 observations,
    our model exhibits the following properties:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样解释这个混淆矩阵。在总共28个观察值中，我们的模型表现出以下特性：
- en: Correctly labeling 10 cases of breast cancer that actually are breast cancer
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确标记了10例实际为乳腺癌的乳腺癌病例
- en: Correctly labeling 8 healthy patients that actually are healthy patients
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确标记了8例实际为健康患者的健康患者
- en: Incorrectly labeling 6 patients as healthy when they actually have breast cancer
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误地将6名患者标记为健康，而他们实际上患有乳腺癌
- en: Incorrectly labeling 4 patients as having breast cancer when they are actually
    healthy patients
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误地将4名患者标记为患有乳腺癌，而他们实际上是健康的
- en: Overall accuracy = 64%
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体准确率 = 64%
- en: Overall error rate = 36%
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体错误率 = 36%
- en: Sensitivity = 67%
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵敏度 = 67%
- en: Specificity = 63%
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特异性 = 63%
- en: To improve our logistic regression model, we must, of course, include many more
    observations. Furthermore, the AUC metric for our model is 0.86, which is quite
    high. However, bear in mind that the AUC is a measure of accuracy, taking into
    account all possible threshold values, while the preceding confusion matrix only
    takes into account a single threshold value (in this case 0.5). As an extension
    exercise, generate confusion matrices for a range of threshold values to see how
    this affects our final classifications!
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进我们的逻辑回归模型，我们当然必须包括更多的观察结果。此外，我们模型的AUC指标为0.86，相当高。然而，请注意，AUC是一个考虑所有可能阈值值的准确度度量，而先前的混淆矩阵只考虑了一个阈值值（在这种情况下为0.5）。作为一个扩展练习，为一系列阈值值生成混淆矩阵，以查看这如何影响我们的最终分类！
- en: Classification and Regression Trees
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: 'We have seen how linear regression models allow us to predict a numerical outcome,
    and how logistic regression models allow us to predict a categorical outcome.
    However, both of these models assume a *linear* relationship between variables.
    **Classification and Regression Trees** (**CART**) overcome this problem by generating
    **Decision Trees**, which are also much easier to interpret compared to the supervised
    learning models we have seen so far. These decision trees can then be traversed
    to come to a final decision, where the outcome can either be numerical (regression
    trees) or categorical (classification trees). A simple classification tree used
    by a mortgage lender is illustrated in *Figure 4.7*:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到线性回归模型如何让我们预测数值结果，以及逻辑回归模型如何让我们预测分类结果。然而，这两个模型都假设变量之间存在*线性*关系。**分类和回归树**（**CART**）通过生成**决策树**来克服这个问题，与之前看到的监督学习模型相比，这些决策树也更容易解释。然后可以遍历这些决策树来做出最终决策，结果可以是数值（回归树）或分类（分类树）。一个简单的分类树，由抵押贷款提供者使用，如图*4.7*所示：
- en: '![](img/13ca1e7c-ba16-47ce-952c-472c29d1c620.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/13ca1e7c-ba16-47ce-952c-472c29d1c620.png)'
- en: 'Figure 4.7: Simple classification tree used by a mortgage lender'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：抵押贷款提供者使用的简单分类树
- en: When traversing decision trees, start at the top. Thereafter, traverse left
    for yes, or positive responses, and traverse right for no, or negative responses.
    Once you reach the end of a branch, the leaf nodes describe the final outcome.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在遍历决策树时，从顶部开始。之后，向左遍历表示是或正面响应，向右遍历表示否或负面响应。一旦到达分支的末端，叶子节点描述了最终结果。
- en: Case study – predicting political affiliation
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 - 预测政治派别
- en: For our next use case, we will use congressional voting records from the US
    House of Representatives to build a classification tree in order to predict whether
    a given congressman or woman is a Republican or a Democrat.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的下一个用例，我们将使用美国众议院的国会投票记录来构建一个分类树，以预测某个国会议员或女议员是共和党人还是民主党人。
- en: 'The specific congressional voting dataset that we will use is available from
    both the GitHub repository accompanying this book and UCI''s machine learning
    repository at [https://archive.ics.uci.edu/ml/datasets/congressional+voting+records](https://archive.ics.uci.edu/ml/datasets/congressional+voting+records).
    It has been cited by Dua, D., and Karra Taniskidou, E. (2017). UCI Machine Learning
    Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
    School of Information and Computer Science.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的特定国会投票数据集可以从本书附带的GitHub仓库和UCI机器学习仓库[https://archive.ics.uci.edu/ml/datasets/congressional+voting+records](https://archive.ics.uci.edu/ml/datasets/congressional+voting+records)获取。它已被Dua,
    D.和Karra Taniskidou, E.（2017）引用。UCI机器学习仓库[http://archive.ics.uci.edu/ml]。加州大学欧文分校，信息与计算机科学学院。
- en: 'If you open `congressional-voting-data/house-votes-84.data` in any text editor
    of your choosing, from either the GitHub repository accompanying this book or
    from UCI''s machine learning repository, you will find 435 congressional voting
    records, of which 267 belong to Democrats and 168 belong to Republicans. The first
    column contains the label string, in other words, Democrat or Republican, and
    the subsequent columns indicate how the congressman or woman in question voted
    on particular key issues at the time (y = for, n = against, ? = neither for nor
    against), such as an anti-satellite weapons test ban and a reduction in funding
    to a synthetic fuels corporation. Let''s now develop a classification tree in
    order to predict the political affiliation of a given congressman or woman based
    on their voting records:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用任何选择的文本编辑器打开 `congressional-voting-data/house-votes-84.data`，无论是来自本书附带的
    GitHub 仓库还是来自 UCI 的机器学习仓库，你将找到 435 项国会投票记录，其中 267 项属于民主党，168 项属于共和党。第一列包含标签字符串，换句话说，是民主党或共和党，后续列表明当时该议员或女议员在特定关键问题上的投票情况（y
    = 支持，n = 反对，? = 既不支持也不反对），例如反卫星武器测试禁令和对合成燃料公司的资金削减。现在，让我们开发一个分类树，以便根据议员的投票记录预测其政治派别：
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-04-classification-regression-trees.ipynb`,
    and which may be found in the GitHub repository accompanying this book. Note that
    for the sake of brevity, we will skip those cells that perform the same functions
    as seen previously.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下子部分描述了对应于本用例的 Jupyter Notebook 中每个相关的单元格，该笔记本命名为 `chp04-04-classification-regression-trees.ipynb`，并且可以在本书附带的
    GitHub 仓库中找到。请注意，为了简洁起见，我们将跳过那些执行与之前相同功能的单元格。
- en: 'Since our raw data file has no header row, we need to explicitly define its
    schema before we can load it into a Spark DataFrame, as follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的原始数据文件没有标题行，在将其加载到 Spark DataFrame 之前，我们需要明确定义其模式，如下所示：
- en: '[PRE28]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Since all of our columns, both the label and all the independent variables,
    are string-based data types, we need to apply a *StringIndexer* to them (as we
    did when developing our logistic regression model) in order to identify and index
    all possible categories for each column before generating numerical feature vectors.
    However, since we have multiple columns that we need to index, it is more efficient
    to build a *pipeline*. A pipeline is a list of data and/or machine learning transformation
    stages to be applied to a Spark DataFrame. In our case, each stage in our pipeline
    will be the indexing of a different column as follows:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们所有的列，包括标签和所有自变量，都是基于字符串的数据类型，我们需要对它们应用一个 *StringIndexer*（就像我们在开发我们的逻辑回归模型时做的那样），以便在生成数值特征向量之前识别和索引每个列的所有可能类别。然而，由于我们需要对多个列进行索引，构建一个
    *pipeline*（管道）会更有效率。管道是一系列应用于 Spark DataFrame 的数据和/或机器学习转换阶段。在我们的案例中，管道中的每个阶段将是对不同列的索引，如下所示：
- en: '[PRE29]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we instantiate our pipeline by passing to it the list of stages that
    we generated in the previous cell. We then execute our pipeline on the raw Spark
    DataFrame using the `fit()` method, before proceeding to generate our numerical
    feature vectors using `VectorAssembler` as before:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过传递之前单元格中生成的阶段列表来实例化我们的管道。然后，我们使用 `fit()` 方法在原始 Spark DataFrame 上执行我们的管道，然后再像之前一样使用
    `VectorAssembler` 生成我们的数值特征向量：
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We are now ready to train our classification tree! To achieve this, we can
    use MLlib''s `DecisionTreeClassifier` estimator to train a decision tree on our
    training dataset as follows:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好训练我们的分类树了！为了实现这一点，我们可以使用 MLlib 的 `DecisionTreeClassifier` 估计器在训练数据集上训练一个决策树，如下所示：
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After training our classification tree, we will evaluate its performance on
    the test DataFrame. As with logistic regression, we can use the AUC metric as
    a measure of the proportion of time that the model predicts correctly. In our
    case, our model has an AUC metric of 0.91, which is very high:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练我们的分类树之后，我们将评估它在测试 DataFrame 上的性能。与逻辑回归一样，我们可以使用 AUC 指标作为模型正确预测比例的衡量标准。在我们的案例中，我们的模型具有
    0.91 的 AUC 指标，这是一个非常高的数值：
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Ideally, we would like to visualize our classification tree. Unfortunately,
    there is not yet any direct method in which to render a Spark decision tree without
    using third-party tools such as [https://github.com/julioasotodv/spark-tree-plotting](https://github.com/julioasotodv/spark-tree-plotting).
    However, we can render a text-based decision tree by invoking the `toDebugString` method
    on our trained classification tree model, as follows:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望可视化我们的分类树。不幸的是，目前还没有直接的方法可以在不使用第三方工具（如[https://github.com/julioasotodv/spark-tree-plotting](https://github.com/julioasotodv/spark-tree-plotting)）的情况下渲染Spark决策树。然而，我们可以通过在训练好的分类树模型上调用`toDebugString`方法来渲染基于文本的决策树，如下所示：
- en: '[PRE33]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With an AUC value of 0.91, we can say that our classification tree model performs
    very well on the test data and is very good at predicting the political affiliation
    of congressmen and women based on their voting records. In fact, it classifies
    correctly 91% of the time across all threshold values!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当AUC值为0.91时，我们可以说我们的分类树模型在测试数据上表现非常好，并且非常擅长根据投票记录预测国会议员的政党归属。事实上，它在所有阈值值下正确分类的比率高达91%！
- en: Note that a CART model also generates probabilities, just like a logistic regression
    model. Therefore, we use a threshold value (default 0.5) in order to convert these
    probabilities into decisions, or classifications as in our example. There is,
    however, an added layer of complexity when it comes to training CART models—how
    do we control the number of splits in our decision tree? One method is to set
    a lower limit for the number of training data points to put into each subset or
    bucket. In `MLlib`, this value is tuneable, via the `minInstancesPerNode` parameter,
    which is accessible when training our `DecisionTreeClassifier`. The smaller this
    value, the more splits that will be generated.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，CART模型也生成概率，就像逻辑回归模型一样。因此，我们使用一个阈值值（默认为0.5）将这些概率转换为决策，或者在我们的例子中，转换为分类。然而，在训练CART模型时，有一个额外的复杂性层次——我们如何控制决策树中的分裂数量？一种方法是设置每个子集或桶中放入的训练数据点的下限。在`MLlib`中，这个值可以通过`minInstancesPerNode`参数进行调整，该参数在训练我们的`DecisionTreeClassifier`时可用。这个值越小，生成的分裂就越多。
- en: However, if it is too small, then overfitting will occur. Conversely, if it
    is too large, then our CART model will be too simple with a low level of accuracy.
    We will discuss how to select an appropriate value during our introduction to
    random forests next. Note that `MLlib` also exposes other configurable parameters,
    including `maxDepth` (the maximum depth of the tree) and `maxBins`, but note that
    the larger a tree becomes in terms of splits and depth, the more computationally
    expensive it is to compute and traverse. To learn more about the tuneable parameters
    available to a `DecisionTreeClassifier`, please visit [https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果它太小，那么就会发生过拟合。相反，如果它太大，那么我们的CART模型将过于简单，准确率低。我们将在介绍随机森林时讨论如何选择合适的值。请注意，`MLlib`还公开了其他可配置的参数，包括`maxDepth`（树的最大深度）和`maxBins`，但请注意，树在分裂和深度方面的规模越大，计算和遍历的成本就越高。要了解更多关于`DecisionTreeClassifier`可调参数的信息，请访问[https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html)。
- en: Random forests
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: One method of improving the accuracy of CART models is to build multiple decision
    trees, not just the one. In random forests, we do just that—a large number of
    CART trees are generated and thereafter, each tree in the forest votes on the
    outcome, with the majority outcome taken as the final prediction.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 提高CART模型准确性的一个方法是构建多个决策树，而不仅仅是单个决策树。在随机森林中，我们正是这样做的——生成大量CART树，然后森林中的每一棵树都对结果进行投票，以多数结果作为最终预测。
- en: To generate a random forest, a process known as bootstrapping is employed whereby
    the training data for each tree making up the forest is selected randomly with
    replacement. Therefore, each individual tree will be trained using a different
    subset of independent variables and, hence, different training data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成随机森林，采用了一种称为自助法的过程，即随机有放回地选择构成森林的每棵树的训练数据。因此，每棵单独的树将使用不同的独立变量子集或桶进行训练，因此训练数据也不同。
- en: K-Fold cross validation
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: 'Let''s now return to the challenge of choosing an appropriate lower-bound bucket
    size for an individual decision tree. This challenge is particularly pertinent
    when training a random forest since the computational complexity increases with
    the number of trees in the forest. To choose an appropriate minimum bucket size,
    we can employ a process known as K-Fold cross validation, the steps of which are
    as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到为单个决策树选择合适的下界桶大小的挑战。当训练随机森林时，这个挑战尤其相关，因为随着森林中树的数量增加，计算复杂度也会增加。为了选择合适的最小桶大小，我们可以采用一种称为K折交叉验证的过程，其步骤如下：
- en: Split a given training dataset into K subsets or "folds" of equal size.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将给定的训练数据集分成K个子集或“折”，大小相等。
- en: (K - 1) folds are then used to train the model, with the remaining fold, called
    the validation set, used to test the model and make predictions for each lower-bound
    bucket size value under consideration.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用(K - 1)个折来训练模型，剩余的折，称为验证集，用于测试模型并对考虑中的每个下界桶大小值进行预测。
- en: This process is then repeated for all possible training and test fold combinations,
    resulting in the generation of multiple trained models that have been tested on
    each fold for every lower-bound bucket size value under consideration.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后对所有可能的训练和测试折组合重复此过程，从而生成多个经过测试的模型，这些模型针对考虑中的每个下界桶大小值在每个折上进行了测试。
- en: For each lower-bound bucket size value under consideration, and for each fold,
    calculate the accuracy of the model on that combination pair.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于考虑中的每个下界桶大小值，以及每个折，计算模型在该组合对上的准确率。
- en: 'Finally, for each fold, plot the calculated accuracy of the model against each
    lower-bound bucket size value, as illustrated in *Figure 4.8*:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，对于每个折，将模型的计算准确率与每个下界桶大小值对比，如图4.8所示：
- en: '![](img/0c3d28a9-caf1-4562-993a-3d1749eaa725.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0c3d28a9-caf1-4562-993a-3d1749eaa725.png)'
- en: 'Figure 4.8: Typical K-Fold cross-validation output'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：典型的K折交叉验证输出
- en: As illustrated in *Figure 4.8*, choosing a small lower-bound bucket size value
    results in lower accuracy as a result of the model overfitting the training data.
    Conversely, choosing a large lower-bound bucket size value also results in lower
    accuracy as the model is too simple. Therefore, in our case, we would choose a
    lower-bound bucket size value of around 4 or 5, since the average accuracy of
    the model seems to be maximized in that region (as illustrated by the dashed circle
    in *Figure 4.8*).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.8*所示，选择较小的下界桶大小值会导致模型过度拟合训练数据，从而降低准确率。相反，选择较大的下界桶大小值也会导致准确率降低，因为模型过于简单。因此，在我们的情况下，我们会选择大约4或5的下界桶大小值，因为模型的平均准确率似乎在该区域达到最大（如*图4.8*中的虚线圆所示）。
- en: 'Returning to our Jupyter Notebook, `chp04-04-classification-regression-trees.ipynb`,
    let''s now train a random forest model using the same congressional voting dataset
    to see whether it results in a better performing model compared to our single
    classification tree that we developed previously:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的Jupyter Notebook，`chp04-04-classification-regression-trees.ipynb`，现在让我们使用相同的国会投票数据集来训练一个随机森林模型，看看它是否比我们之前开发的单个分类树有更好的性能：
- en: 'To build a random forest, we can use `MLlib`''s `RandomForestClassifier` estimator
    to train a random forest on our training dataset, specifying the minimum number
    of instances each child must have after a split via the `minInstancesPerNode`
    parameter, as follows:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要构建随机森林，我们可以使用`MLlib`的`RandomForestClassifier`估计器在我们的训练数据集上训练随机森林，通过`minInstancesPerNode`参数指定每个子节点在分裂后必须具有的最小实例数，如下所示：
- en: '[PRE34]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can now evaluate the performance of our trained random forest model on our
    test dataset by computing the AUC metric using the same `BinaryClassificationEvaluator` as
    follows:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以通过使用相同的`BinaryClassificationEvaluator`计算AUC指标来评估我们在测试数据集上训练的随机森林模型的性能，如下所示：
- en: '[PRE35]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Our trained random forest model has an AUC value of 0.97, meaning that it is
    more accurate in predicting political affiliation based on historical voting records
    than our single classification tree!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的随机森林模型具有0.97的AUC值，这意味着它在根据历史投票记录预测政治归属方面比我们之前开发的单个分类树更准确！
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have developed, tested, and evaluated various supervised
    machine learning models in Apache Spark using a wide variety of real-world use
    cases, from predicting breast cancer to predicting political affiliation based
    on historical voting records.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经在Apache Spark中使用了各种真实世界的用例，从预测乳腺癌到根据历史投票记录预测政治派别，开发了、测试了和评估了各种监督机器学习模型。
- en: In the next chapter, we will develop, test, and evaluate unsupervised machine
    learning models!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开发、测试和评估无监督机器学习模型！
