- en: Supervised Learning Using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will develop, test, and evaluate supervised machine learning
    models applied to a variety of real-world use cases using Python, Apache Spark,
    and its machine learning library, `MLlib`. Specifically, we will train, test,
    and interpret the following types of supervised machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: Univariate linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification and regression trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first supervised learning model that we will study is that of linear regression.
    Formally, linear regression models the relationship between a *dependent* variable
    using a set of one or more *independent* variables. The resulting model can then
    be used to predict the numerical value of the *dependent* variable. But what does
    this mean in practice? Well, let's look at our first real-world use case to make
    sense of this.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – predicting bike sharing demand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bike sharing schemes have become very popular across the world over the last
    decade or so as people seek a convenient means to travel within busy cities while
    limiting their carbon footprint and helping to reduce road congestion. If you
    are unfamiliar with bike sharing systems, they are very simple; people rent a
    bike from certain locations in a city and thereafter return that bike to either
    the same or another location once they have finished their journey. In this example,
    we will be examining whether we can predict the daily demand for bike sharing
    systems given the weather on a particular day!
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset that we will be using has been derived from the **University of
    California''s** (**UCI**) machine learning repository found at [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php).
    The specific bike sharing dataset that we will use, available from both the GitHub
    repository accompanying this book and from [https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset),
    has been cited by Fanaee-T, Hadi, and Gama, Joao, ''Event labeling combining ensemble
    detectors and background knowledge,'' Progress in Artificial Intelligence (2013):
    pp. 1-15, Springer Berlin Heidelberg.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open `bike-sharing-data/day.csv`in any text editor, from either the
    GitHub repository accompanying this book or from UCI''s machine learning repository,
    you will find bike sharing data aggregated on a daily basis over 731 days using
    the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column name** | **Data type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `instant` | `Integer` | Unique record identifier (primary key) |'
  prefs: []
  type: TYPE_TB
- en: '| `dteday` | `Date` | Date |'
  prefs: []
  type: TYPE_TB
- en: '| `season` | `Integer` | Season (1 – spring, 2 – summer, 3 – fall, 4 – winter)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `yr` | `Integer` | Year |'
  prefs: []
  type: TYPE_TB
- en: '| `mnth` | `Integer` | Month |'
  prefs: []
  type: TYPE_TB
- en: '| `holiday` | `Integer` | Day is a holiday or not |'
  prefs: []
  type: TYPE_TB
- en: '| `weekday` | `Integer` | Day of the week |'
  prefs: []
  type: TYPE_TB
- en: '| `workingday` | `Integer` | 1 – neither a weekend nor a holiday, 0 – otherwise
    |'
  prefs: []
  type: TYPE_TB
- en: '| `weathersit` | `Integer` | 1 – clear, 2 – mist, 3 – light snow, 4 – heavy
    rain |'
  prefs: []
  type: TYPE_TB
- en: '| `temp` | `Double` | Normalized temperature in Celsius |'
  prefs: []
  type: TYPE_TB
- en: '| `atemp` | `Double` | Normalized feeling temperature in Celsius |'
  prefs: []
  type: TYPE_TB
- en: '| `hum` | `Double` | Normalized humidity |'
  prefs: []
  type: TYPE_TB
- en: '| `windspeed` | `Double` | Normalized wind speed |'
  prefs: []
  type: TYPE_TB
- en: '| `casual` | `Integer` | Count of casual users for that day |'
  prefs: []
  type: TYPE_TB
- en: '| `registered` | `Integer` | Count of registered users for that day |'
  prefs: []
  type: TYPE_TB
- en: '| `cnt` | `Integer` | Count of total bike renters that day |'
  prefs: []
  type: TYPE_TB
- en: Using this dataset, can we predict the total bike renters for a given day (*cnt*)
    given the weather patterns for that particular day? In this case, *cnt* is the
    *dependent* variable that we wish to predict based on a set of *independent* variables
    that we shall choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Univariate (or single-variable) linear regression refers to a linear regression
    model where we use only one independent variable *x* to learn a *linear* function
    that maps *x* to our dependent variable *y:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e16cc5f8-50c1-4e2f-b9c8-7cfcfb1aa96f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y^i* represents the *dependent* variable (cnt) for the *i^(th)* observation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x^i* represents the single *independent* variable for the *i^(th)* observation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ε^(*i*) represents the *error* term for the *i^(th)* observation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*β[0]* is the intercept coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*β[1]* is the regression coefficient for the single independent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since, in general form, a univariate linear regression model is a linear function,
    we can easily plot this on a scatter graph where the x-axis represents the single
    independent variable, and the y-axis represents the dependent variable that we
    are trying to predict. *Figure 4.1* illustrates the scatter plot generated when
    we plot normalized feeling temperature (independent variable) against total daily
    bike renters (dependent variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12c53f09-7f55-47e1-b8e7-210d12522132.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Normalized temperature against total daily bike renters'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing *Figure 4.1*, you will see that there seems to be a general positive
    linear trend between the normalized feeling temperature (**atemp**) and the total
    daily biker renters (**cnt**). However, you will also see that our blue trend
    line, which is the visual representation of our univariate linear regression function,
    is not perfect, in other words, not all of our data points fit exactly on this
    line. In the real world, it is extremely rare to have a perfect model; in other
    words, all predictive models will make some mistakes. The goal therefore is to
    minimize the number of mistakes our models make so that we may have confidence
    in the predictions that they provide.
  prefs: []
  type: TYPE_NORMAL
- en: Residuals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The errors (or mistakes) that our model makes are called error terms or *residuals*,
    and are denoted in our univariate linear regression equation by ε^(*i*). Our goal
    therefore is to choose regression coefficients for the independent variables (in
    our case *β[1]*) that minimize these residuals. To compute the *i^(th)* residual,
    we can simply subtract the predicted value from the actual value, as illustrated
    in *Figure 4.1*. To quantify the quality of our regression line, and hence our
    regression model, we can use a metric called the **Sum of Squared Errors** (**SSE**),
    which is simply the sum of all squared residuals, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18a90520-c676-47aa-8d27-6ca44ac2dbf9.png)'
  prefs: []
  type: TYPE_IMG
- en: A smaller SSE implies a better fit. However, SSE as a metric to quantify the
    quality of our regression model has its limitations. SSE scales with the number
    of data points *N*, which means that if we doubled the number of data points,
    the SSE may be twice as large, which may lead you to believe that the model is
    twice as bad, which is not the case! We therefore require other means to quantify
    the quality of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean square error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **root mean square error** (**RMSE**) is the square root of the SSE divided
    by the total number of data points *N*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dff36623-b263-4ab0-8f39-9c455faf0dc6.png)'
  prefs: []
  type: TYPE_IMG
- en: The RMSE tends to be used more often as a means to quantify the quality of a
    linear regression model, since its units are the same as the dependent variable,
    and is normalized by N.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another metric that provides an error measure of a linear regression model
    is called the R^(*2*) (R-squared) metric. The R² metric represents the proportion
    of *variance* in the dependent variable explained by the independent variable(s).
    The equation for calculating R² is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8baa308e-d8df-4565-bcc7-2b0b5bd2d866.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, SST refers to the **Total Sum of Squares**, which is just
    the SSE from the overall mean (as illustrated in *Figure 4.1* by the red horizontal
    line, which is often used as a **baseline** model). An R² value of 0 implies a
    linear regression model that provides no improvement over the baseline model (in
    other words, SSE = SST). An R² value of 1 implies a perfect predictive linear
    regression model (in other words, SSE = 0). The aim therefore is to get an R²
    value as close as possible to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate linear regression in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Returning to our case study, let''s develop a univariate linear regression
    model in Apache Spark using its machine learning library, `MLlib`, in order to
    predict the total daily bike renters using our bike sharing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-01-univariate-linear-regression.ipynb`,
    and which may be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required Python dependencies, including `pandas` (Python
    data analysis library), `matplotlib` (Python plotting library), and `pyspark` (Apache
    Spark Python API). By using the `%matplotlib` magic function, any plots that we
    generate will automatically be rendered within the Jupyter Notebook cell output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we instantiate a Spark context, it is generally a good idea to load
    a sample of any pertinent dataset into `pandas` so that we may identify any trends
    or patterns before developing a predictive model. Here, we use the `pandas` library
    to load the entire CSV into a `pandas` DataFrame called `bike_sharing_raw_df` (since
    it is a very small dataset anyway):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In cells 3.1 to 3.4, we use the `matplotlib` library to plot various independent
    variables (`temp`, `atemp`, `hum`, and `windspeed`) against the dependent variable
    (`cnt`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in *Figure 4.2*, there is a general positive linear relationship
    between the normalized temperatures (`temp` and `atemp`) and the total daily bike
    renters (cnt). However, there is no such obvious trend when using humidity and
    wind speed as our independent variables. Therefore, we will proceed to develop
    a univariate linear regression model using normalized feeling temperature (`atemp`)
    as our single independent variable, with total daily bike renters (`cnt`) being
    our dependent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5415139f-e95c-4d00-a697-a67b1968a7df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Bike sharing scatter plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to develop a Spark application, we need to first instantiate a Spark
    context (as described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*) to connect to our local Apache Spark cluster. We also
    instantiate a Spark `SQLContext` for the structured processing of our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now load our CSV dataset into a Spark DataFrame (see [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*) called `bike_sharing_df`. We use the `SQLContext` previously
    defined and we tell Spark to use the first row as the header row and to infer
    the schema data types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Before developing a predictive model, it is also a good idea to generate standard
    statistical metrics for a dataset so as to gain additional insights. Here, we
    generate the row count for the DataFrame, as well as calculating the mean average,
    standard deviation, and the minimum and maximum for each column. We achieve this
    using the `describe()` method for a Spark DataFrame as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We now demonstrate how to plot a dataset using a Spark DataFrame as an input.
    In this case, we simply convert the Spark DataFrame into a `pandas` DataFrame
    before plotting as before (note that for very large datasets, it is recommended
    to use a representative sample of the dataset for plotting purposes):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have finished our exploratory analysis, we can start developing
    our univariate linear regression model! First, we need to convert our independent
    variable (`atemp`) into a *numerical feature vector* (see [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml),
    *Artificial Intelligence and Machine Learning*). We can achieve this using MLlib''s `VectorAssembler`, which
    will take one or more feature columns, convert them into feature vectors, and
    store those feature vectors in an output column, which, in this example, is called
    `features`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then apply the `VectorAssembler` *transformer* (see [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*) to the raw dataset and identify the column
    that contains our label (in this case, our dependent variable `cnt`). The output
    is a new Spark DataFrame called `bike_sharing_features_df` containing our independent
    numerical feature vectors (`atemp`) mapped to a known label (`cnt`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As per supervised learning models in general, we need a *training* dataset
    to train our model in order to learn the mapping function, and a *test* dataset
    in order to evaluate the performance of our model. We can randomly split our raw
    labeled feature vector DataFrame using the `randomSplit()` method and a seed,
    which is used to initialize the random generator, and which can be any number
    you like. Note that if you use a different seed, you will get a different random
    split between your training and test dataset, which means that you may get slightly
    different coefficients for your final linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In our case, 75% of the original rows will form our training DataFrame called
    `train_df`, with the remaining 25% forming our test DataFrame called `test_df`,
    while using a `seed` of `12345`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to train our univariate linear regression model! We achieve
    this by using `MLlib`''s `LinearRegression` estimator (see [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml),
    *Artificial Intelligence and Machine Learning*) and passing it the name of the
    column containing our independent numerical feature vectors (in our case, called
    `features`) and the name of the column containing our labels (in our case, called
    `cnt`). We then apply the `fit()` method to train our model and output a linear
    regression *transformer* which, in our case, is called `linear_regression_model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we evaluate our trained univariate linear regression model on the test
    DataFrame, let''s generate some summary statistics for it. The transformer model
    exposes a series of statistics, including model coefficients (in other words, *β[1]* in
    our case), the intercept coefficient *β[0]*, the error metrics RMSE and R², and
    the set of residuals for each data point. In our case, we have the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: β[0] = 829.62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: β[1] = 7733.75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSE = 1490.12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: R² = 0.42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, our trained univariate linear regression model has learned the following
    function in order to be able to predict our dependent variable *y* (total daily
    bike renters) using a single independent variable *x* (normalized feeling temperature):'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = 829.62 + 7733.75x*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now apply our trained model to our test DataFrame in order to evaluate
    its performance on test data. Here, we apply our trained linear regression model
    transformer to the test DataFrame using the `transform()` method in order to make
    predictions. For example, our model predicts a total daily bike rental count of
    1742 given a normalized feeling temperature of 0.11793\. The actual total daily
    bike rental count was 1416 (an error of 326):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now compute the same RMSE and R² error metrics, but based on the performance
    of our model on the *test *DataFrame. In our case, these are 1534.51 (RMSE) and
    0.34 (R²) respectively, calculated using `MLlib`''s `RegressionEvaluator`. So,
    in our case, our trained model actually performs more poorly on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we can generate the same metrics but using the `evaluate()` method
    of the linear regression model, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we terminate our Spark application by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Multivariate linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our univariate linear regression model actually performed relatively poorly
    on both the training and test datasets, with R² values of 0.42 on the training
    dataset and 0.34 on the test dataset respectively. Is there any way we can take
    advantage of the other independent variables available in our raw dataset to increase
    the predictive quality of our model?
  prefs: []
  type: TYPE_NORMAL
- en: 'Multivariate (or multiple) linear regression extends univariate linear regression
    by allowing us to utilize more than one independent variable, in this case *K*
    independent variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ac9e48-9e36-422a-b81b-5edbb280f7c0.png)'
  prefs: []
  type: TYPE_IMG
- en: As before, we have our dependent variable *y^i* (for the *i^(th)* observation),
    an intercept coefficient *β[0],* and our residuals ε^(*i*). But we also now have
    *k* independent variables, each with their own regression coefficient, *β[k]*.
    The goal, as before, is to derive coefficients that minimize the amount of error
    that our model makes. The problem now though is how to choose which subset of
    independent variables to use in order to train our multivariate linear regression
    model. Adding more independent variables increases the complexity of models in
    general and, hence, the data storage and processing requirements of underlying
    processing platforms. Furthermore, models that are too complex tend to cause **overfitting**,
    whereby the model achieves better performance (in other words, a higher *R²* metric)
    on the training dataset used to train the model than on new data that it has not
    seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Correlation is a metric that measures the linear relationship between two variables,
    and helps us to decide which independent variables to include in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: +1 implies a perfect positive linear relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 implies no linear relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -1 implies a perfect negative linear relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When two variables have an *absolute* value of correlation close to 1, then
    these two variables are said to be "highly correlated".
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate linear regression in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Returning to our case study, let''s now develop a multivariate linear regression
    model in order to predict the total daily bike renters using our bike sharing
    dataset and a subset of independent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-02-multivariate-linear-regression.ipynb`,
    and which may be found in the GitHub repository accompanying this book. Note that
    for the sake of brevity, we will skip those cells that perform the same functions
    as seen previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s demonstrate how we can use Spark to calculate the correlation
    value between our dependent variable, `cnt`, and each independent variable in
    our DataFrame. We achieve this by iterating over each column in our raw Spark
    DataFrame and using the `stat.corr()` method as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The resultant correlation matrix shows that the independent variables—`season`, `yr`, `mnth`, `temp` ,
    and `atemp`, exhibit significant positive correlation with our dependent variable `cnt`.
    We will therefore proceed to train a multivariate linear regression model using
    this subset of independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen previously, we can apply a `VectorAssembler` in order to generate numerical
    feature vector representations of our collection of independent variables along
    with the `cnt` label. The syntax is identical to that seen previously, but this
    time we pass multiple columns to the `VectorAssembler` representing the columns
    containing our independent variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to generate our respective training and test datasets using
    the `randomSplit` method via the DataFrame API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train our multivariate linear regression model using the same `LinearRegression`
    estimator that we used in our univariate linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After splitting our original DataFrame into a training and test DataFrame respectively,
    and applying the same *LinearRegression* estimator to the training DataFrame,
    we now have a trained multivariate linear regression model with the following
    summary training statistics (as can be seen in cell 8 of this Jupyter Notebook):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: β[0] = -389.94, β[1] = 526.05, β[2] = 2058.85, β[3] = -51.90, β[4] = 2408.66,
    β[5] = 3502.94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSE = 1008.50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: R² = 0.73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, our trained multivariate linear regression model has learned the
    following function in order to be able to predict our dependent variable *y* (total
    daily bike renters) using a set of independent variables *x[k]* (season, year,
    month, normalized temperature, and normalized feeling temperature):'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = -389.94 + 526.05x[1] + 2058.85x[2] - 51.90x[3] + 2408.66x[4] + 3502.94x[5]*'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, our trained multivariate linear regression model actually performs
    even better on the test dataset with a test RMSE of 964.60 and a test R² of 0.74.
  prefs: []
  type: TYPE_NORMAL
- en: To finish our discussion of multivariate linear regression models, note that
    our training R² metric will always either increase or stay the same as more independent
    variables are added. However, a better training R² metric does not always imply
    a better test R² metric—in fact, a test R² metric can even be negative, meaning
    that it performs worse on the test dataset than the baseline model (which can
    never be the case for the training R² metric). The goal therefore is to be able
    to develop a model that works well for both the training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how linear regression models allows us to predict a numerical outcome.
    Logistic regression models, however, allow us to predict a *categorical* outcome
    by predicting the probability that an outcome is true.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with linear regression, in logistic regression models, we also have a dependent
    variable *y* and a set of independent variables *x[1]*, *x[2]*, …, *x[k]*. In
    logistic regression however, we want to learn a function that provides the probability
    that *y = 1* (in other words, that the outcome variable is true) given this set
    of independent variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcd811e4-d51a-4d40-b1b2-3c4669141f9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This function is called the **Logistic Response** function, and provides a
    number between 0 and 1, representing the probability that the outcome-dependent
    variable is true, as illustrated in *Figure 4.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fcc1d21-faf7-44d6-9ed3-8776dc37c6fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Logistic response function'
  prefs: []
  type: TYPE_NORMAL
- en: Positive coefficient values β[k] increase the probability that y = 1, and negative
    coefficient values decrease the probability that y = 1\. Our goal, therefore,
    when developing logistic regression models, is to choose coefficients that predict
    a high probability when y = 1, but predict a low probability when y = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Threshold value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now know that logistic regression models provide us with the probability
    that the outcome variable is true, that is to say, y = 1\. However, in real-world
    use cases, we need to make *decisions*, not just deliver probabilities. Often,
    we make binary predictions, such as Yes/No, Good/Bad, and Go/Stop. A threshold
    value (*t*) allows us to make these decisions based on probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If P(y=1) >= t, then we predict y = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If P(y=1) < t, then we predict y = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenge now is how to choose a suitable value of *t*. In fact, what does
    *suitable* mean in this context?
  prefs: []
  type: TYPE_NORMAL
- en: In real-world use cases, some types of error are better than others. Imagine
    that you were a doctor and were testing a large group of patients for a particular
    disease using logistic regression. In this case, the outcome *y=1* would be a
    patient carrying the disease (therefore y=0 would be a patient not carrying the
    disease), and, hence, our model would provide P(y=1) for a given person. In this
    example, it is better to detect as many patients potentially carrying the disease
    as possible, even if it means misclassifying some patients as carrying the disease
    who subsequently turn out not to. In this case, we select a smaller threshold
    value. If we select a large threshold value, however, we would detect those patients
    that almost certainly have the disease, but we would misclassify a large number
    of patients as not carrying the disease when, in actual fact, they do, which would
    be a much worse scenario!
  prefs: []
  type: TYPE_NORMAL
- en: 'In general therefore, when using logistic regression models, we can make two
    types of error:'
  prefs: []
  type: TYPE_NORMAL
- en: We predict y=1 (disease), but the actual outcome is y=0 (healthy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We predict y=0 (healthy), but the actual outcome is y=1 (disease)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A confusion (or classification) matrix can help us qualify what threshold value
    to use by comparing the predicted outcomes against the actual outcomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predict y=0 (healthy)** | **Predict y=1 (disease)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual y=0 (healthy)** | **True negatives** (**TN**) | **False positives**
    (**FP**) |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual y=1 (disease)** | **False negatives** (**FN**) | **True positives**
    (**TP**) |'
  prefs: []
  type: TYPE_TB
- en: 'By generating a confusion matrix, it allows us to quantify the accuracy of
    our model based on a given threshold value by using the following series of metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: N = number of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall accuracy = (TN + TP) / N
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall error rate = (FP + FN) / N
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitivity (True Positive Rate) = TP / (TP + FN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity (True Negative Rate) = TN / (TN + FP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False positive error rate = FP / (TN + FP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False negative error rate = FN / (TP + FN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression models with a higher threshold value will have a lower sensitivity
    and higher specificity. Models with a lower threshold value will have a higher
    sensitivity and lower specificity. The choice of threshold value therefore depends
    on the type of error that is "better" for your particular use case. In use cases
    where there is genuinely no preference, for example, political leaning of Conservative/Non-Conservative,
    then you should choose a threshold value of 0.5 that will predict the most likely
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Receiver operator characteristic curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To further assist us in choosing a threshold value in a more visual way, we
    can generate a **receiver operator characteristic** (**ROC**) curve. An ROC curve
    plots the **false positive error rate** (**FPR**) against the **true positive
    rate** (**TPR**, or sensitivity) for every threshold value between 0 and 1, as
    illustrated in *Figure 4.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9366a343-9e08-481b-b1f5-b0b81bfb0c53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: ROC curve'
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in *Figure 4.4*, using a threshold value of 0 means that you
    will catch ALL cases of y=1 (disease), but you will also incorrectly label all
    cases of y=0 (healthy) as y=1 (disease) as well, scaring a lot of healthy people!
    However using a threshold value of 1 means that you will NOT catch ANY cases of
    y=1 (disease), leaving a lot of people untreated, but you will correctly label
    all cases of y=0 (healthy). The benefit of plotting an ROC curve therefore is
    that it helps you to see the trade-off for *every* threshold value, and ultimately
    helps you to make a decision as to which threshold value to use for your given
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a means of quantifying the quality of the predictions made by a logistic
    regression model, we can calculate the **Area under the ROC curve** (**AUC**),
    as illustrated in *Figure 4.5*. The AUC measures the proportion of time that the
    model predicts correctly, with an AUC value of 1 (maximum), implying a perfect
    model, in other words, our model predicts correctly 100% of the time, and an AUC
    value of 0.5 (minimum), implying our model predicts correctly 50% of the time,
    analogous to just guessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b08363c-6b44-4144-af01-3b21dc3b8799.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Area under the ROC curve'
  prefs: []
  type: TYPE_NORMAL
- en: Case study – predicting breast cancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now apply logistic regression to a very important real-world use case;
    predicting patients who may have breast cancer. Approximately 1 in 8 women are
    diagnosed with breast cancer during their lifetime (with the disease also affecting
    men), resulting in the premature deaths of hundreds of thousands of women annually
    across the world. In fact, it is projected that over 2 million new cases of breast
    cancer will have been reported worldwide by the end of 2018 alone. Various factors
    are known to increase the risk of breast cancer, including age, weight, family
    history, and previous diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: Using a dataset of quantitative predictors, along with a binary dependent variable
    indicating the presence or absence of breast cancer, we will train a logistic
    regression model to predict the probability of whether a given patient is healthy
    (y=1) or has the biomarkers of breast cancer (y=0).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we will use has again been derived from the University of California's
    (UCI) machine learning repository. The specific breast cancer dataset, available
    from both the GitHub repository accompanying this book and from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra),
    has been cited by [Patricio, 2018] Patrício, M., Pereira, J., Crisóstomo, J.,
    Matafome, P., Gomes, M., Seiça, R., and Caramelo, F. (2018). Using Resistin, glucose,
    age, and BMI to predict the presence of breast cancer. BMC Cancer, 18(1).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open `breast-cancer-data/dataR2.csv`in any text editor, from either
    the GitHub repository accompanying this book or from UCI''s machine learning repository,
    you will find breast cancer data that employs the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column name** | **Data type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `Age` | `Integer` | Age of patient |'
  prefs: []
  type: TYPE_TB
- en: '| `BMI` | `Double` | Body mass index (kg/m²) |'
  prefs: []
  type: TYPE_TB
- en: '| `Glucose` | `Double` | Blood glucose level (mg/dL) |'
  prefs: []
  type: TYPE_TB
- en: '| `Insulin` | `Double` | Insulin level (µU/mL) |'
  prefs: []
  type: TYPE_TB
- en: '| `HOMA` | `Double` | Homeostatic Model Assessment (HOMA) – used to assess
    β-cell function and insulin sensitivity |'
  prefs: []
  type: TYPE_TB
- en: '| `Leptin` | `Double` | Hormone used to regulate energy expenditure (ng/mL)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Adiponectin` | `Double` | Protein hormone used to regulate glucose levels
    (µg/mL) |'
  prefs: []
  type: TYPE_TB
- en: '| `Resistin` | `Double` | Hormone that causes insulin resistance (ng/mL) |'
  prefs: []
  type: TYPE_TB
- en: '| `MCP.1` | `Double` | Protein to aid recovery from injury and infection (pg/dL)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Classification` | `Integer` | 1 = Healthy patient as part of a control group,
    2 = patient with breast cancer |'
  prefs: []
  type: TYPE_TB
- en: Using this dataset, can we develop a logistic regression model that calculates
    the probability of a given patient being healthy (in other words, y=1) and thereafter
    apply a threshold value to make a predictive decision?
  prefs: []
  type: TYPE_NORMAL
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-03-logistic-regression.ipynb`,
    and which may be found in the GitHub repository accompanying this book. Note that,
    for the sake of brevity, we will skip those cells that perform the same functions
    as seen previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading our breast cancer CSV file, we first identify the column that
    will act as our label, that is to say, `Classification`. Since the values in this
    column are either 1 (healthy) or 2 (breast cancer patient), we will apply a `StringIndexer` to
    this column to identify and index all the possible categories. The result is that
    a label of 1 corresponds to a healthy patient, and a label of 0 corresponds to
    a breast cancer patient:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we will use all the raw quantitative columns [`Age`, `BMI`, `Glucose`, `Insulin`, `HOMA`, `Leptin`, `Adiponectin`, `Resistin`,
    and `MCP.1`] as independent variables in order to generate numerical feature vectors
    for our model. Again, we can use the `VectorAssembler` of `MLlib` to achieve this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After generating training and test DataFrames respectively, we apply the `LogisticRegression` estimator
    of `MLlib` to train a `LogisticRegression` model transformer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use our trained logistic regression model to make predictions on the
    test DataFrame, using the `transform()` method of our logistic regression model
    transformer. This results in a new DataFrame with the columns `rawPrediction`,
    `prediction`, and `probability` appended to it. The probability of y=1, in other
    words, P(y=1), is contained within the `probability` column, and the overall predictive
    decision using a default threshold value of t=0.5 is contained within the `prediction` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To quantify the quality of our trained logistic regression model, we can plot
    an ROC curve and calculate the AUC metric. The ROC curve is generated using the `matplotlib` library,
    given the **false positive rate** (**FPR**) and **true positive rate** (**TPR**),
    as exposed by evaluating our trained logistic regression model on the test DataFrame.
    We can then use `MLlib`''s `BinaryClassificationEvaluator` to calculate the AUC
    metric as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The resultant ROC curve, generated using the `matplotlib` library, is illustrated
    in *Figure 4.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2252ec8-5665-4f13-b5ca-f39a89803285.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: ROC curve rendered using `matplotlib`'
  prefs: []
  type: TYPE_NORMAL
- en: 'One method of generating a confusion matrix based on the test dataset predictions
    is to simply filter the test predictions'' DataFrame based on cases where the
    predicted outcome equals, and does not equal, the actual outcome and thereafter
    count the number of records post-filter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use MLlib''s RDD API (which is in maintenance mode as
    of Spark 2.0) to automatically generate the confusion matrix by converting the
    test predictions'' DataFrame into an RDD (see [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*), and thereafter passing it to the `MulticlassMetrics` evaluation
    abstraction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix for our logistic regression model, using a default threshold
    value of 0.5, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predict y=0 (breast cancer)** | **Predict y=1 (healthy)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual y=0****(breast cancer)** | 10 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual y=1****(healthy)** | 4 | 8 |'
  prefs: []
  type: TYPE_TB
- en: 'We can interpret this confusion matrix as follows. Out of a total of 28 observations,
    our model exhibits the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Correctly labeling 10 cases of breast cancer that actually are breast cancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correctly labeling 8 healthy patients that actually are healthy patients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrectly labeling 6 patients as healthy when they actually have breast cancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrectly labeling 4 patients as having breast cancer when they are actually
    healthy patients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall accuracy = 64%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall error rate = 36%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitivity = 67%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity = 63%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To improve our logistic regression model, we must, of course, include many more
    observations. Furthermore, the AUC metric for our model is 0.86, which is quite
    high. However, bear in mind that the AUC is a measure of accuracy, taking into
    account all possible threshold values, while the preceding confusion matrix only
    takes into account a single threshold value (in this case 0.5). As an extension
    exercise, generate confusion matrices for a range of threshold values to see how
    this affects our final classifications!
  prefs: []
  type: TYPE_NORMAL
- en: Classification and Regression Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen how linear regression models allow us to predict a numerical outcome,
    and how logistic regression models allow us to predict a categorical outcome.
    However, both of these models assume a *linear* relationship between variables.
    **Classification and Regression Trees** (**CART**) overcome this problem by generating
    **Decision Trees**, which are also much easier to interpret compared to the supervised
    learning models we have seen so far. These decision trees can then be traversed
    to come to a final decision, where the outcome can either be numerical (regression
    trees) or categorical (classification trees). A simple classification tree used
    by a mortgage lender is illustrated in *Figure 4.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13ca1e7c-ba16-47ce-952c-472c29d1c620.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Simple classification tree used by a mortgage lender'
  prefs: []
  type: TYPE_NORMAL
- en: When traversing decision trees, start at the top. Thereafter, traverse left
    for yes, or positive responses, and traverse right for no, or negative responses.
    Once you reach the end of a branch, the leaf nodes describe the final outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – predicting political affiliation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our next use case, we will use congressional voting records from the US
    House of Representatives to build a classification tree in order to predict whether
    a given congressman or woman is a Republican or a Democrat.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific congressional voting dataset that we will use is available from
    both the GitHub repository accompanying this book and UCI''s machine learning
    repository at [https://archive.ics.uci.edu/ml/datasets/congressional+voting+records](https://archive.ics.uci.edu/ml/datasets/congressional+voting+records).
    It has been cited by Dua, D., and Karra Taniskidou, E. (2017). UCI Machine Learning
    Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
    School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open `congressional-voting-data/house-votes-84.data` in any text editor
    of your choosing, from either the GitHub repository accompanying this book or
    from UCI''s machine learning repository, you will find 435 congressional voting
    records, of which 267 belong to Democrats and 168 belong to Republicans. The first
    column contains the label string, in other words, Democrat or Republican, and
    the subsequent columns indicate how the congressman or woman in question voted
    on particular key issues at the time (y = for, n = against, ? = neither for nor
    against), such as an anti-satellite weapons test ban and a reduction in funding
    to a synthetic fuels corporation. Let''s now develop a classification tree in
    order to predict the political affiliation of a given congressman or woman based
    on their voting records:'
  prefs: []
  type: TYPE_NORMAL
- en: The following sub-sections describe each of the pertinent cells in the corresponding
    Jupyter Notebook for this use case, entitled `chp04-04-classification-regression-trees.ipynb`,
    and which may be found in the GitHub repository accompanying this book. Note that
    for the sake of brevity, we will skip those cells that perform the same functions
    as seen previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our raw data file has no header row, we need to explicitly define its
    schema before we can load it into a Spark DataFrame, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Since all of our columns, both the label and all the independent variables,
    are string-based data types, we need to apply a *StringIndexer* to them (as we
    did when developing our logistic regression model) in order to identify and index
    all possible categories for each column before generating numerical feature vectors.
    However, since we have multiple columns that we need to index, it is more efficient
    to build a *pipeline*. A pipeline is a list of data and/or machine learning transformation
    stages to be applied to a Spark DataFrame. In our case, each stage in our pipeline
    will be the indexing of a different column as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate our pipeline by passing to it the list of stages that
    we generated in the previous cell. We then execute our pipeline on the raw Spark
    DataFrame using the `fit()` method, before proceeding to generate our numerical
    feature vectors using `VectorAssembler` as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train our classification tree! To achieve this, we can
    use MLlib''s `DecisionTreeClassifier` estimator to train a decision tree on our
    training dataset as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After training our classification tree, we will evaluate its performance on
    the test DataFrame. As with logistic regression, we can use the AUC metric as
    a measure of the proportion of time that the model predicts correctly. In our
    case, our model has an AUC metric of 0.91, which is very high:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Ideally, we would like to visualize our classification tree. Unfortunately,
    there is not yet any direct method in which to render a Spark decision tree without
    using third-party tools such as [https://github.com/julioasotodv/spark-tree-plotting](https://github.com/julioasotodv/spark-tree-plotting).
    However, we can render a text-based decision tree by invoking the `toDebugString` method
    on our trained classification tree model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With an AUC value of 0.91, we can say that our classification tree model performs
    very well on the test data and is very good at predicting the political affiliation
    of congressmen and women based on their voting records. In fact, it classifies
    correctly 91% of the time across all threshold values!
  prefs: []
  type: TYPE_NORMAL
- en: Note that a CART model also generates probabilities, just like a logistic regression
    model. Therefore, we use a threshold value (default 0.5) in order to convert these
    probabilities into decisions, or classifications as in our example. There is,
    however, an added layer of complexity when it comes to training CART models—how
    do we control the number of splits in our decision tree? One method is to set
    a lower limit for the number of training data points to put into each subset or
    bucket. In `MLlib`, this value is tuneable, via the `minInstancesPerNode` parameter,
    which is accessible when training our `DecisionTreeClassifier`. The smaller this
    value, the more splits that will be generated.
  prefs: []
  type: TYPE_NORMAL
- en: However, if it is too small, then overfitting will occur. Conversely, if it
    is too large, then our CART model will be too simple with a low level of accuracy.
    We will discuss how to select an appropriate value during our introduction to
    random forests next. Note that `MLlib` also exposes other configurable parameters,
    including `maxDepth` (the maximum depth of the tree) and `maxBins`, but note that
    the larger a tree becomes in terms of splits and depth, the more computationally
    expensive it is to compute and traverse. To learn more about the tuneable parameters
    available to a `DecisionTreeClassifier`, please visit [https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html).
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One method of improving the accuracy of CART models is to build multiple decision
    trees, not just the one. In random forests, we do just that—a large number of
    CART trees are generated and thereafter, each tree in the forest votes on the
    outcome, with the majority outcome taken as the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To generate a random forest, a process known as bootstrapping is employed whereby
    the training data for each tree making up the forest is selected randomly with
    replacement. Therefore, each individual tree will be trained using a different
    subset of independent variables and, hence, different training data.
  prefs: []
  type: TYPE_NORMAL
- en: K-Fold cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now return to the challenge of choosing an appropriate lower-bound bucket
    size for an individual decision tree. This challenge is particularly pertinent
    when training a random forest since the computational complexity increases with
    the number of trees in the forest. To choose an appropriate minimum bucket size,
    we can employ a process known as K-Fold cross validation, the steps of which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split a given training dataset into K subsets or "folds" of equal size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (K - 1) folds are then used to train the model, with the remaining fold, called
    the validation set, used to test the model and make predictions for each lower-bound
    bucket size value under consideration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is then repeated for all possible training and test fold combinations,
    resulting in the generation of multiple trained models that have been tested on
    each fold for every lower-bound bucket size value under consideration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each lower-bound bucket size value under consideration, and for each fold,
    calculate the accuracy of the model on that combination pair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, for each fold, plot the calculated accuracy of the model against each
    lower-bound bucket size value, as illustrated in *Figure 4.8*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0c3d28a9-caf1-4562-993a-3d1749eaa725.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Typical K-Fold cross-validation output'
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in *Figure 4.8*, choosing a small lower-bound bucket size value
    results in lower accuracy as a result of the model overfitting the training data.
    Conversely, choosing a large lower-bound bucket size value also results in lower
    accuracy as the model is too simple. Therefore, in our case, we would choose a
    lower-bound bucket size value of around 4 or 5, since the average accuracy of
    the model seems to be maximized in that region (as illustrated by the dashed circle
    in *Figure 4.8*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our Jupyter Notebook, `chp04-04-classification-regression-trees.ipynb`,
    let''s now train a random forest model using the same congressional voting dataset
    to see whether it results in a better performing model compared to our single
    classification tree that we developed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a random forest, we can use `MLlib`''s `RandomForestClassifier` estimator
    to train a random forest on our training dataset, specifying the minimum number
    of instances each child must have after a split via the `minInstancesPerNode`
    parameter, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now evaluate the performance of our trained random forest model on our
    test dataset by computing the AUC metric using the same `BinaryClassificationEvaluator` as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Our trained random forest model has an AUC value of 0.97, meaning that it is
    more accurate in predicting political affiliation based on historical voting records
    than our single classification tree!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have developed, tested, and evaluated various supervised
    machine learning models in Apache Spark using a wide variety of real-world use
    cases, from predicting breast cancer to predicting political affiliation based
    on historical voting records.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will develop, test, and evaluate unsupervised machine
    learning models!
  prefs: []
  type: TYPE_NORMAL
