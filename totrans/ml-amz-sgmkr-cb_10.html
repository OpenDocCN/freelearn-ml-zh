<html><head></head><body>
		<div id="_idContainer182">
			<h1 id="_idParaDest-115"><em class="italic"><a id="_idTextAnchor116"/>Chapter 8</em>: Model Scoring and Deployment</h1>
			<p>In the previous chapter, we learned how to use outputs generated by DataRobot to understand models and why a model provides a particular prediction. We will now learn how to use models to score input datasets and create predictions to be used in the intended applications. DataRobot<a id="_idIndexMarker369"/> automates many tasks that are required for scoring and generating row-level explanations. </p>
			<p>Creating predictions, however, is not where these tasks end. In most cases, these predictions need to be transformed into actions for consumption by people or applications. This mapping of predictions to actions requires an understanding of business and therefore needs a person to interpret the results (in most use cases). In this chapter, we will discuss how this is done. We're going to cover the following main topics:</p>
			<ul>
				<li>Scoring and prediction methods</li>
				<li>Generating prediction explanations</li>
				<li>Analyzing predictions and postprocessing</li>
				<li>Deploying DataRobot models</li>
				<li>Monitoring deployed models</li>
			</ul>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor117"/>Scoring and prediction methods</h1>
			<p>DataRobot provides multiple methods <a id="_idIndexMarker370"/>to score datasets using models that have been created. One of<a id="_idIndexMarker371"/> the easiest methods is batch scoring via the DataRobot <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>). For this, we need to follow these steps:</p>
			<ol>
				<li>Create a file with the dataset to be scored. Given that we are using a public dataset, we will simply use the same dataset to score. In a real project, you will have access to a <a id="_idIndexMarker372"/>new dataset for which you want to create predictions. For our purposes, we simply created a copy of our <strong class="source-inline">imports-85-data.xlsx</strong> dataset file and named it <strong class="source-inline">imports-85-data-score.xlsx</strong>. </li>
				<li>Now, let's select the <strong class="bold">Predict</strong> tab and then the <strong class="bold">Test Predictions</strong> tab for the <strong class="bold">XGBoost</strong> (<strong class="bold">XGB</strong>) models, as<a id="_idIndexMarker373"/> shown in the following screenshot: <div id="_idContainer157" class="IMG---Figure"><img src="image/Figure_8.1_B17159.jpg" alt="Figure 8.1 – Batch scoring&#13;&#10;"/></div><p class="figure-caption">Figure 8.1 – Batch scoring</p><p>In the preceding screenshot, you will see that you have an option to <strong class="bold">drag and drop a new dataset</strong> to add the scoring file to the model. </p></li>
				<li>Let's <a id="_idIndexMarker374"/>select our <strong class="source-inline">imports-85-data-score.xlsx</strong> scoring file and drop it into the <strong class="bold">Drag and drop a new dataset</strong> box. Once you drop the file, it will get uploaded and you can see it in the interface, as shown in the following screenshot: <div id="_idContainer158" class="IMG---Figure"><img src="image/Figure_8.2_B17159.jpg" alt="Figure 8.2 – Computing predictions&#13;&#10;"/></div><p class="figure-caption">Figure 8.2 – Computing predictions</p></li>
				<li>You can now click on the <strong class="bold">Compute predictions</strong> button to start the scoring process. Once this process is complete, you can click on the <strong class="bold">Download predictions</strong> button to download the predictions generated by the model. The download is in the form of a <strong class="source-inline">.csv</strong> file that you can view in Excel, as shown in the following screenshot: </li>
			</ol>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/Figure_8.3_B17159.jpg" alt="Figure 8.3 – Downloaded predictions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Downloaded predictions</p>
			<p>The downloaded <a id="_idIndexMarker375"/>predictions file can now be joined with the original dataset for further analysis.</p>
			<p>The second method for scoring a dataset is via the DataRobot batch prediction <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>), which<a id="_idIndexMarker376"/> will be discussed in the following section.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor118"/>Generating prediction explanations</h1>
			<p>In this section, we will focus<a id="_idIndexMarker377"/> on how to generate explanations along with predictions for the scoring dataset. After uploading the scoring dataset (as we discussed in the preceding section), you can now go to the <strong class="bold">Understand</strong> tab and then select the <strong class="bold">Prediction Explanations</strong> tab, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/Figure_8.4_B17159.jpg" alt="Figure 8.4 – Prediction explanations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Prediction explanations</p>
			<p>In the preceding screenshot, you can see that it now shows the scoring dataset that was uploaded. You<a id="_idIndexMarker378"/> can now click on the icon next to the dataset filename to compute the explanations. Once the computation is complete, you will see the download icon. You can use the download icon to download the generated explanations for the predictions made by the model. The explanations come in the form of a <strong class="source-inline">.csv</strong> file that can be opened using Excel, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/Figure_8.5_B17159.jpg" alt="Figure 8.5 – Prediction explanations file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Prediction explanations file</p>
			<p>In the preceding<a id="_idIndexMarker379"/> screenshot, we see that the file contains the predictions, as well as an explanation for each prediction. For example, if we look at row <strong class="bold">69</strong> that is highlighted in <em class="italic">Figure 8.5</em>, we see that the value of <strong class="source-inline">make</strong> explains 5.17% of the value difference from the base for this automobile. Similarly, you can see the relative contribution of each feature value. Notice that the features in the file are not sorted by the most important feature and also that the most important feature for a given row is not the same as some other row. The feature importance will change from row to row.</p>
			<p>Now that we have the predictions and their explanations, let's look at how to analyze these and determine how to use them to take actions or make decisions.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor119"/>Analyzing predictions and postprocessing</h1>
			<p>Before we <a id="_idIndexMarker380"/>charge off to deploy the model, it would be advisable to analyze the predictions and see if they make sense, whether there are some patterns in the errors, and also how to<a id="_idIndexMarker381"/> turn the predictions into something actionable. These are aspects where traditional data science tools and methods are not of much help, and you need to rely on judgment and methods from other disciplines to help formulate the next steps. For this, let's start by combining the scoring dataset file with the explanations file. This can be done in <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>), Python, or Excel. The combined file looks something<a id="_idIndexMarker382"/> like this: </p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/Figure_8.6_B17159.jpg" alt="Figure 8.6 – Combined scoring data and predictions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Combined scoring data and predictions</p>
			<p>We also <a id="_idIndexMarker383"/>created a new <strong class="bold">ERROR</strong> column that simply subtracts <strong class="bold">prediction</strong> from <strong class="bold">price</strong>. We can now use Excel to create a pivot table and look at the results from <a id="_idIndexMarker384"/>multiple perspectives. For example, let's create a pivot table and look at the <strong class="bold">Average of ERROR</strong> value by <strong class="bold">symboling</strong>, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/Figure_8.7_B17159.jpg" alt="Figure 8.7 – Average of ERROR value by symboling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Average of ERROR value by symboling</p>
			<p>The preceding screenshot shows that errors are much higher for the value <strong class="bold">-2</strong>. Looking at the dataset, we <a id="_idIndexMarker385"/>find that we have only three data points for <strong class="bold">-2</strong>, thus it is not a surprise that the model performs poorly. This tells us that we cannot trust the results when the <strong class="bold">symboling</strong> value is <strong class="bold">-2</strong> and that we should try to get more data for this value. Analysis such as this can point to areas of improvement and where to focus your efforts. We also realize that since this is an average error, we should use the average of the <a id="_idIndexMarker386"/>absolute percentage value of the error to prevent incorrect conclusions, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/Figure_8.8_B17159.jpg" alt="Figure 8.8 – Average of abs perc ERROR value by symboling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Average of abs perc ERROR value by symboling</p>
			<p>Now, we see that the absolute percent error decreases as the <strong class="bold">symboling</strong> value increases. At this point, there is no hard and fast way to find insights except exploring the output data and looking at it from different perspectives to see what you can find. Typically, it is a good idea to sort the errors and look at rows that have unusually large errors, and then see if you can determine why this is so.</p>
			<p>Now, on to one of the most important aspects of building a data science model—understanding which actions to take. Now that we have a reasonable model to predict price, a question arises: <em class="italic">What should we do with this information?</em> Hopefully, the answer was determined at the start of the project as to what was the goal of this exercise. Let's assume that the objective is to set the price of a new vehicle by looking at the prediction of the model and providing all the parameters such as <strong class="source-inline">engine_size</strong>, and so on. We could also imagine that a model such as this could be useful even during the design stage when designers are trying to determine trade-offs between different parameters such as <strong class="source-inline">bore</strong> or <strong class="source-inline">width</strong>. This goes on to say that a predictive model can many times be applied to use cases that were not considered while building the model. </p>
			<p>This, however, requires <a id="_idIndexMarker387"/>us to understand the broader context of the business <a id="_idIndexMarker388"/>problem. This is the primary reason we took time to discuss and understand the business context in <a href="B17159_03_Final_NM_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Understanding and Defining Business Problems</em>. It might be useful to revisit that chapter to refresh the concepts discussed there as we will use some of the techniques that were introduced there, such as causal modeling.</p>
			<p>To determine how we use price prediction, let's review what we know about how price relates to other parameters. In <a href="B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097"><em class="italic">Chapter 5</em></a>, <em class="italic">Exploratory Data Analysis with DataRobot</em>, we looked at association analysis information. Association strengths using mutual information were generated by DataRobot. We can use that information to draw a network graph between different features, as shown in the following screenshot. You can do this by drawing a circle for each feature, and then creating lines between features that have high association strengths:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/Figure_8.9_B17159.jpg" alt="Figure 8.9 – Network graph of associations between features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Network graph of associations between features</p>
			<p>In <a href="B17159_07_Final_NM_ePub.xhtml#_idTextAnchor110"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Understanding and Explainability</em>, we saw the feature importance <a id="_idIndexMarker389"/>for price in <a id="_idIndexMarker390"/>terms of <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) values <a id="_idIndexMarker391"/>is specific to the model we selected. The following might represent a causal diagram for this problem:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/Figure_8.10_B17159.jpg" alt="Figure 8.10 – Causal diagram for the XGB model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Causal diagram for the XGB model</p>
			<p>The left side <a id="_idIndexMarker392"/>of the diagram represents the most important features from the SHAP values. Let's imagine that the actual price charged is a bit different from the <a id="_idIndexMarker393"/>prediction. The <strong class="bold">Price Delta</strong> feature reflects a decision someone might make to charge a price different from the prediction. The <strong class="bold">Price</strong> feature impacts <strong class="bold">Units Sold</strong>, which ultimately affects the profitability. Note that this reflects just one possible way of using this model to help make pricing decisions.</p>
			<p>If, on the other hand, we imagine that we are trying to help the car design team come up with the best car configuration that will also be the most profitable one, then we might look at the diagram a bit differently. This is because different choices of car or engine design will also impact the cost of the car. Also, we know from <em class="italic">Figure 8.9</em> that the features are not independent. Changing the <strong class="bold">bore</strong> feature will change the <strong class="bold">Engine Size</strong> and the <strong class="bold">Horsepower</strong> features. Hence, when we are looking into making decisions, we have to think about the causal impacts as well. This is a very simplified view, and you can imagine that for a real problem, these diagrams will be a lot more complex. Imagine business leaders making those decisions by taking into account all of these relationships in their heads. This is one of the reasons that many times, models are not used by business users.</p>
			<p>In our example problem, the causal diagram shown in <em class="italic">Figure 8.10</em> is fairly simple. You can imagine real-world problems where this diagram will be a lot more complex. In such cases, it is very difficult to assess the impact the deployment of a model will have on the ecosystem. This <a id="_idIndexMarker394"/>includes users and other stakeholders. Complex problems tend to have many unanticipated consequences, especially when the affected parties are people. </p>
			<p>In such situations, if <a id="_idIndexMarker395"/>the potential impact may be large, it is advisable to test the new model in a synthetic or simulated environment. With the testing and impact analyses complete, we are now ready to deploy our model.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor120"/>Deploying DataRobot models</h1>
			<p>DataRobot makes it pretty easy to deploy the models you have developed. To prepare a model for <a id="_idIndexMarker396"/>deployment, here are the steps:</p>
			<ol>
				<li value="1">Let's unlock the project so that we can see the metrics for the holdout datasets, as shown in the following screenshot:<div id="_idContainer167" class="IMG---Figure"><img src="image/Figure_8.11_B17159.jpg" alt="Figure 8.11 – Unlocking DataRobot models&#13;&#10;"/></div><p class="figure-caption">Figure 8.11 – Unlocking DataRobot models</p><p>In the preceding screenshot, you can see the <strong class="bold">Unlock project Holdout for all models</strong> option on the right side of the interface. </p></li>
				<li>You should unlock the project only after you have selected the model that you are choosing for <a id="_idIndexMarker397"/>deployment. In our case, we have selected the XGB model that uses the <strong class="bold">FL1 top23</strong> feature list. Clicking <a id="_idIndexMarker398"/>on this option brings up a dialog box, as shown in the following screenshot:<div id="_idContainer168" class="IMG---Figure"><img src="image/Figure_8.12_B17159.jpg" alt="Figure 8.12 – Unlocking project holdout&#13;&#10;"/></div><p class="figure-caption">Figure 8.12 – Unlocking project holdout</p></li>
				<li>Unlocking the project is an irreversible process. Let's unlock the project and see the holdout metrics, as shown in the following screenshot:<div id="_idContainer169" class="IMG---Figure"><img src="image/Figure_8.13_B17159.jpg" alt="Figure 8.13 – Unlocked project view&#13;&#10;"/></div><p class="figure-caption">Figure 8.13 – Unlocked project view</p><p><em class="italic">Figure 8.13</em> shows that the holdout values are higher than the cross-validation values, as expected. The <a id="_idIndexMarker399"/>holdout values are a better representation of the kind of performance you should expect from a model after deployment. </p></li>
				<li>Now that the project is unlocked, let's retrain the selected model with 100% of the data to improve this model's performance. For that, click on the orange <strong class="bold">+</strong> sign for the model, as shown in <em class="italic">Figure 8.13</em>. This will bring up a dialog box for changing the sample size, as shown in the following screenshot: <div id="_idContainer170" class="IMG---Figure"><img src="image/Figure_8.14_B17159.jpg" alt="Figure 8.14 – Defining new sample size&#13;&#10;"/></div><p class="figure-caption">Figure 8.14 – Defining new sample size</p><p>In the <a id="_idIndexMarker400"/>preceding screenshot, you see options to change the sample size. </p></li>
				<li>Drag the slider bar all the way to <strong class="bold">100%</strong> to indicate that you want to train the model with 100% of the data, as shown in the following screenshot: <div id="_idContainer171" class="IMG---Figure"><img src="image/Figure_8.15_B17159.jpg" alt="Figure 8.15 – Setting new sample size&#13;&#10;"/></div><p class="figure-caption">Figure 8.15 – Setting new sample size</p></li>
				<li>You can <a id="_idIndexMarker401"/>now click the <strong class="bold">Run with new sample size</strong> button. DataRobot will now retrain the XGB model with 100% of the data. For the XGB model, you can now click on the <strong class="bold">Predict</strong> tab and then the <strong class="bold">Deploy</strong> tab, as shown in the following screenshot: <div id="_idContainer172" class="IMG---Figure"><img src="image/Figure_8.16_B17159.jpg" alt="Figure 8.16 – Deploying a model&#13;&#10;"/></div><p class="figure-caption">Figure 8.16 – Deploying a model</p></li>
				<li>Next, click<a id="_idIndexMarker402"/> on the <strong class="bold">Deploy model</strong> button. This will bring up a new page, as shown in the following screenshot: <div id="_idContainer173" class="IMG---Figure"><img src="image/Figure_8.17_B17159.jpg" alt="Figure 8.17 – Creating a deployment for the model&#13;&#10;"/></div><p class="figure-caption">Figure 8.17 – Creating a deployment for the model</p></li>
				<li>You can <a id="_idIndexMarker403"/>now give a name to your deployed model. You can also select your prediction environment where the deployed model is hosted, as set up by your administrator. Under the <strong class="bold">Data Drift</strong> section, you can specify if you want to track data drift or enable challenger models. You can also enable the storage of prediction rows, which allows DataRobot to analyze performance over time. Similarly, you can enable the tracking of attributes for segment-based analysis of model performance. </li>
				<li>You can now click the <strong class="bold">Create deployment</strong> button. DataRobot will now deploy your model and create a baseline for model drift. Once completed, you will see information about your deployed model, as shown in the following screenshot: <div id="_idContainer174" class="IMG---Figure"><img src="image/Figure_8.18_B17159.jpg" alt="Figure 8.18 – Deployed model overview&#13;&#10;"/></div><p class="figure-caption">Figure 8.18 – Deployed model overview</p><p>You can now see <a id="_idIndexMarker404"/>the endpoint for the <strong class="bold">REpresentational State Transfer</strong> (<strong class="bold">REST</strong>) API for<a id="_idIndexMarker405"/> your prediction model. For example, for the <strong class="bold">price Predictions</strong> model, the <strong class="bold">prediction environment</strong> is <strong class="source-inline">https://app2.datarobot.com</strong>.</p></li>
				<li>You can now<a id="_idIndexMarker406"/> invoke this API to generate predictions. You can also see other information about your deployment by clicking on different tabs. If you click on the <strong class="bold">Service Health</strong> tab, you will see a page like this:</li>
			</ol>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/Figure_8.19_B17159.jpg" alt="Figure 8.19 – Service health of deployments&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.19 – Service health of deployments</p>
			<p>The preceding <a id="_idIndexMarker407"/>screenshot shows the status of the price prediction model. It shows how many predictions have been done, the response time for a prediction, and the error rates. The screenshot does not show any values because we just deployed this model.</p>
			<p>We are now ready to start monitoring this deployed model.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor121"/>Monitoring deployed models</h1>
			<p>As you will <a id="_idIndexMarker408"/>have guessed by now, the job of the data science team does not end once a model is deployed. We now have to monitor this model to see how it is performing, whether it is working as intended, and if we need to intervene and make any changes. We'll proceed as follows:</p>
			<ol>
				<li value="1">To see how that works, let's click on the <strong class="bold">Predictions</strong> tab, as shown in the following screenshot:<div id="_idContainer176" class="IMG---Figure"><img src="image/Figure_8.20_B17159.jpg" alt="Figure 8.20 – Making predictions using the deployed model&#13;&#10;"/></div><p class="figure-caption">Figure 8.20 – Making predictions using the deployed model</p></li>
				<li>We can <a id="_idIndexMarker409"/>now upload a dataset to be scored, by dragging and dropping a file (here, we will use the same file that we used before during model training) into the <strong class="bold">Prediction source</strong> box. We can now see other options becoming available, as shown in the following screenshot:<div id="_idContainer177" class="IMG---Figure"><img src="image/Figure_8.21_B17159.jpg" alt="Figure 8.21 – Computing predictions for a dataset&#13;&#10;"/></div><p class="figure-caption">Figure 8.21 – Computing predictions for a dataset</p></li>
				<li>After selecting the <a id="_idIndexMarker410"/>options, we can click on the <strong class="bold">Compute and download predictions</strong> button. After DataRobot finishes the computations, we will see the output file becoming available, as shown in the following screenshot: <div id="_idContainer178" class="IMG---Figure"><img src="image/Figure_8.22_B17159.jpg" alt="Figure 8.22 – Downloading predictions&#13;&#10;"/></div><p class="figure-caption">Figure 8.22 – Downloading predictions</p><p>The output file<a id="_idIndexMarker411"/> can now be downloaded and analyzed. Since we are interested in monitoring the model, let's click on the <strong class="bold">Service Health</strong> tab, as shown in the following screenshot:</p><div id="_idContainer179" class="IMG---Figure"><img src="image/Figure_8.23_B17159.jpg" alt="Figure 8.23 – Service health of the model&#13;&#10;"/></div><p class="figure-caption">Figure 8.23 – Service health of the model</p><p>We can now see that the <a id="_idIndexMarker412"/>model has serviced 15 requests with a median response time of 325 <strong class="bold">milliseconds</strong> (<strong class="bold">ms</strong>) and an error rate of 0%. The overall service health looks good. </p></li>
				<li>We can now look at the data drift for the model by clicking on the <strong class="bold">Data Drift</strong> tab, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/Figure_8.24_B17159.jpg" alt="Figure 8.24 – Data drift for the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.24 – Data drift for the model</p>
			<p>In the<a id="_idIndexMarker413"/> preceding screenshot, at the top of the <strong class="bold">Data Drift</strong> page, we see the data drift between the scoring data and training data. The left graph shows drift by feature importance, and we can see that the amount of drift is very low. This is not surprising since we used the same dataset. For real datasets, the drift will be a bit higher. Similarly, the graph on the right shows the distribution of records grouped by price. Here again, we see that the distributions are very similar for the target feature, <strong class="source-inline">price</strong>. If you scroll down the page, you will see additional graphs, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/Figure_8.25_B17159.jpg" alt="Figure 8.25 – Data drift for the model: additional information&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.25 – Data drift for the model: additional information</p>
			<p>The preceding <a id="_idIndexMarker414"/>screenshot shows the average prediction values over time. This will indicate whether the predictions have been stable or if they have changed over time. You will have to rely on your understanding of the business problem to determine whether the amount of drift is acceptable or not. DataRobot will also give you an indication by showing a red, yellow, or green status. A red status would indicate that there is an issue that needs to be resolved; similarly, yellow means that you should be aware of potential issues, and green indicates that everything looks fine. In general, the issue could be errors in the data pipeline or a change in the business environment. A change in the business environment would be an indication that the model needs to be retrained.</p>
			<p>If the model needs to be retrained or if you need to rebuild the model, you can follow the steps that we have outlined in the preceding chapters. This completes a basic view of how you use DataRobot to build and deploy a model. </p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Summary</h1>
			<p>In this chapter, we learned how to use models after training. We discussed the methods that are used to score a dataset and also methods that are used for analyzing the resulting outputs. We also covered methods and considerations for turning predictions into actions or decisions. This is a critical step whereby you have to engage with your business stakeholders to make sure that introducing this model will not cause unforeseen problems. This is also the time to work on change management tasks such as communicating changes to people who are impacted by the change and ensure that users are trained in the new process and know how to use the new capabilities.</p>
			<p>We then discussed how to use DataRobot capabilities to rapidly deploy a model and then monitor the model performance. It is easy to underestimate the importance of this capability. Model deployment and monitoring are not easy, and many organizations spend a lot of time and effort trying to deploy a model. Hopefully, we have shown how easily this can be accomplished with DataRobot.</p>
			<p>We have now completed the basic steps needed to build and deploy a model and can now go over some advanced concepts and capabilities of DataRobot. You are now ready to dive into advanced topics based on your interest or based on the type of project you will be working on. For example, if you are working on a time series problem, then you can review <a href="B17159_09_Final_NM_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 9</em></a>, <em class="italic">Forecasting and Time Series Modeling</em>.</p>
		</div>
	</body></html>