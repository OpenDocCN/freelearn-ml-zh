<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Automated Classification of Lithofacies Formation Using ML</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will explore the idea of building an end-to-end cloud-based machine learning system to identify <strong>lithofacies</strong> based on well log measurements. This is a crucial step in all drilling applications. First, we will start by introducing the problem and the dataset. Next, we will explain the types of pre<span>processing</span> and post processing needed for such a use case. Finally, a complete solution will be built using machine learning services, Python, and IBM Watson Studio.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Understanding lithofacies</li>
<li>Exploring the data</li>
<li>Training the classifier</li>
<li>Evaluating the classifier</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding lithofacies</h1>
                </header>
            
            <article>
                
<p class="mce-root">Sedimentary rock that has been formed through the deposition and solidification of sediment transported by water, ice, and wind is usually deposited in layers. The geological properties of these layers depend upon a number of forces such as tectonics, sea level, sediment supply, physical and biological processes of sediment transport and deposition, and climate. The result of these forces and interactions yield what is known as a <strong>geometric arrangement</strong>, making up the stratigraphic architecture of an area. The arrangement or internal anatomy of the sediment bodies within the architecture is identified through lithofacies analysis and the interpretation of the depositional environments.</p>
<p>Gathering and interpreting this information is a critical component in the work of oil and gas, groundwater, and mineral and geothermal exploration (as well as being a significant share of environmental and geotechnical study).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Depositional environments</h1>
                </header>
            
            <article>
                
<p>The previously mentioned depositional environments are created by the various physical and biological processes of transporting and depositing sediments. These processes result in various distributions of grain size and<em> </em>biogenic sedimentary structures that characterize (or classify) the deposited sediment through a direct relationship to the depositional force that produced them.</p>
<p>Relating the features found in an environmental structure back to the forces that created them is the basic method used by geologists to interpret the depositional environment of the sedimentary sequence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lithofacies formation</h1>
                </header>
            
            <article>
                
<p>One of the very first steps in the progression of <strong>litho</strong><strong>facies analysis</strong> (and, therefore, lithofacies formation), is the description and interpretation of available and conventional core data.</p>
<p>An important outcome of core descripting is the subdivision of cores into lithofacies, defined as <strong>classifications of a sedimentary sequence</strong> based on lithology (the study of the characteristics of rock), grain size, physical and biogenic sedimentary structures, and the stratification that relates to the depositional processes that produced them.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Lithofacies and lithofacies associations (groups of related lithofacies) are the basic units for the interpretation of depositional environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Our use case</h1>
                </header>
            
            <article>
                
<p>I hope that after reading the preceding sections of this chapter, you have already formulated the idea that a critical component in evaluating opportunities for drilling applications is lithology and lithofacies formation.</p>
<p>Our goal, in this project, is to use machine learning to interpret core data and identify lithofacies (that is, classify bodies of rock or rock types into mappable units of a designated stratigraphic unit) based upon its physical characteristics, composition, formation, or various other attributes, obtained in well logging data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following sections, we will explore the well training data and plot the learning in various forms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Well logging</h1>
                </header>
            
            <article>
                
<p>Well logging, sometimes referred to as <strong>borehole logging</strong>, is the practice of making a detailed record (or a well log) of the geological formations penetrated by a borehole or a well. This log may be established either on a visual inspection of the samples brought to the surface (called <strong>geological logs</strong>) or on the physical measurements made by instruments lowered into the hole (called geophysical logs).</p>
<p>Geophysical well logs such as, <span>drilling, completing, producing, or abandoning can be done during any phase of a well's history.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Log ASCII Standard (LAS)</h1>
                </header>
            
            <article>
                
<p>Thankfully, there is a commonly acceptable format in which well logs are expected to be.</p>
<p>LAS is an industry-standard file format used in all oil-and-gas and water well industries to log and store well log information and data. A single LAS file can only contain data for one well. But in that one well, it can contain any number of datasets (called <strong>curves</strong>). Common curves found in an LAS file may include natural gamma, travel time, resistivity logs, and other possible information.</p>
<div class="packt_infobox">For more information on LAS files, you can refer to this paper: <a href="https://www.bcogc.ca/node/11400/download">https://www.bcogc.ca/node/11400/download</a>.</div>
<p>Wow! Although not rocket science, the data is not a simple relational table. Preliminary work for this exercise will be to better understand the specifics of the data provided.</p>
<p>In this chapter, our goal is to implement a machine learning algorithm in Python using <kbd>scikit-learn</kbd>, one of the most popular machine learning tools for Python, based upon a sample well drilling log dataset for the task of training a classifier to distinguish between different types of lithofacies.</p>
<p>Suppose that we are told that the training log dataset was created from sample logs, and based upon research defining eight different lithofacies, along with various log measurements, such as gamma-ray, neutron porosity, <strong>photoelectric factor</strong> (<strong>PeF</strong>), and resistivity.</p>
<p>We also know that in this file, we'll have six lithofacies data points (<kbd>GCR</kbd>, <kbd>NPHI</kbd>, <kbd>PE</kbd>, <kbd>PEF</kbd>, <kbd>ILD</kbd>, and <kbd>ILM</kbd>), along with an ID and the lithofacies type.</p>
<p>The following screenshot shows a portion of the top section of an actual well logging file:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b68a5bb4-aef2-4d70-b9eb-4c9fba0ae94a.png" style=""/></div>
<p>The following screenshot is a peek at a portion of our file, which we will use for training our machine learning model. This file excludes the top LAS-formatted section headers and is simply a continuous list of curves or well-logging measurements:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/752d0e9e-ecd2-4c1e-aab9-c49d7f42b6bc.png" style=""/></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data asset</h1>
                </header>
            
            <article>
                
<p>We will assume that you have created a new IBM Watson Studio project, and therefore, you can proceed to add a data asset (our well sample log file) to it so that we can work with the data. We have loaded data files in our previous chapters, but here is a quick refresher:</p>
<ol>
<li>From the new project's <span class="packt_screen">Assets</span> page, click on <span class="packt_screen">Add to project</span> | <span class="packt_screen">DATA</span></li>
<li>In the <span class="packt_screen">Load</span> pane that opens, browse to the file. Remember that you must stay on the page until the load is complete</li>
<li>IBM Watson then saves the files in the object storage that is associated with your project, and they are listed as data assets on the <span class="packt_screen">Assets</span> <span>page of your project</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data asset annotations</h1>
                </header>
            
            <article>
                
<p>As you build your assets within IBM Watson Studio, it is highly recommended that you take the time to annotate your data assets at the time that you load them. This will allow you to quickly search for and locate those assets for collaboration with others and to use within other projects. You can accomplish this by simply adding a description and one or more tags to your asset.</p>
<p>A tag is metadata that simplifies searching for your assets. A tag consists of one string containing spaces, letters, numbers, underscores, dashes, and the <span># and @ </span>symbols. You can create multiple tags on the same asset by using a comma to separate the individual tag values.</p>
<p>From the project page, under <span class="packt_screen">Data assets</span>, you can click on the asset name:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9e8b5376-5a96-4eb0-8dd4-1e9ed26ccde4.png"/></div>
<p>Next, click inside the <span class="packt_screen">Tags</span> box to add tags, and you can manually assign business terms and tags to the data, as well as a description, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><strong><img src="assets/7c3c91a5-584b-4920-a47e-31e7b1b1b8d8.png" style="width:22.92em;height:29.25em;"/><br/></strong></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Profiling the data</h1>
                </header>
            
            <article>
                
<p>Another method for developing a good understanding of what your data offers is to create a profile of the data, using the profile feature within IBM Watson Studio.</p>
<p class="mce-root"/>
<p>The profile of a data asset includes generated metadata and statistics about the textual content of the data so that you can <em>see</em> how it is made up. You can create a profile on the asset's <span class="packt_screen">Profile</span> page in the project, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/72826041-99d2-4e0b-b965-b78f7bd2d3ea.png"/></div>
<p>Once you click on <span class="packt_screen">Create Profile</span>, IBM Watson reviews the data and generates the visuals that you can scroll though and easily examine:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fd9d5d20-4461-4fa1-8ff0-7f8174f3c5a0.png"/></div>
<div class="packt_infobox">
<p>Depending upon the size and complexity of the data, generating the profile may take a few minutes. The good news is that after it is created, it is saved with the file so that it doesn't have to be generated again.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a notebook and Python instead</h1>
                </header>
            
            <article>
                
<p>Rather than using the profiler, you can use visualizations within an IBM Watson notebook to present data visually to identify patterns, gain insights, and make decisions based upon your project's objectives or assumptions. As we've seen in earlier chapters, many open source visualization libraries, such as <kbd>matplotlib</kbd>, are already pre-installed on IBM Watson Studio for you, and all you have to do is import them.</p>
<div class="packt_infobox">
<p>You can install other third-party and open source visualization libraries and packages in the same manner, or take advantage of other IBM visualization libraries and tools, such as <strong>Brunel</strong>, to create interactive graphs with simple code and SPSS models to create interactive tables and charts to help evaluate and improve a predictive analytics model.</p>
</div>
<p>In the next few sections, we will use a notebook and Python commands to show the various ways to analyze and condition data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>Again, since we assume that we already have a new IBM Watson Studio project created, we can go ahead and add a new notebook to the project (from your project, click on <span class="packt_screen">Add to Project</span> | <span class="packt_screen">Notebook</span>, just as we did in prior chapters, just be sure to specify the language as Python). Let's take a look at the following steps:</p>
<ol>
<li>Load and open the file, then print the first five records (from the file). Recall that to accomplish this, there is no coding required.</li>
<li>You simply click on <span class="packt_screen">Insert to code</span> <span>and then</span> <span class="packt_screen">Insert pandas DataFrame</span> <span>for our file in the <span class="packt_screen">Files</span> | <span class="packt_screen">Data Asset</span> pane:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/92e6b2c1-df3d-46a9-b1cd-b71b74030a96.png" style=""/></div>
<p style="padding-left: 60px">This automatically generates the following code in our notebook's first cell, which will load our data file into a pandas DataFrame object (<kbd>df_data_1</kbd>) and then print the first five records of the file:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1af1fbbd-c4f8-4b1f-8815-dc265de72dca.png"/></div>
<p style="padding-left: 60px">The preceding code generates the following output for us:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-500 image-border" src="assets/eda02023-ffa5-42dc-8dd5-5174db5f5310.png" style=""/></div>
<p style="padding-left: 60px">From this review, we can see that each row of the dataset represents one lithofacies, and they are each represented by several features that are in our table's columns (as shown in the preceding screenshot).</p>
<p style="padding-left: 60px">Using the <kbd>print</kbd> and <kbd>.shape</kbd> functions of Python, we see that we have <kbd>180</kbd> lithofacies (the number of records in the file) and <kbd>8</kbd> features in the dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/376a8c2b-3641-4b56-97ce-8310c444ed2f.png" style=""/></div>
<ol start="3">
<li>We can also use the <kbd>.unique()</kbd> function to demonstrate that we have eight different types of lithofacies in our dataset:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/476c4681-fd03-4a3d-82d8-c2f3c0f52aaf.png" style=""/></div>
<p class="mce-root"/>
<ol start="4">
<li>Next, we can use the <kbd>.size()</kbd> function to see how each lithofacies is represented within the file. The data seems pretty balanced between <kbd>22</kbd> and <kbd>25</kbd>, with the <kbd>Mdst/Mdst-Wkst</kbd> lithofacies being the most unbalanced with <kbd>16</kbd>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8c7e0eae-94a9-41af-b0ef-da03eef5df28.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizations</h1>
                </header>
            
            <article>
                
<p>You can use Python to create a bar chart based upon the lithofacies size; this makes it a bit easier to understand as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4f50fce8-81b5-4861-b250-b5ab91541912.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Box plotting</h1>
                </header>
            
            <article>
                
<p>Often used in performing explanatory data analysis, a box plot is a type of graph that is used to show and understand the shape of a distribution, its central value, and its variability. Seeing a box plot for each numeric variable in our well log data will give you a better idea of the distribution of the input variables as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8e5ec820-fac0-4fc1-83cb-515e7d0c3bbb.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Histogram</h1>
                </header>
            
            <article>
                
<p>A <strong>histogram</strong> is used to graphically recap and display the distribution of data points within a dataset.</p>
<p>Using the following Python code, we can now try a histogram for each numeric input value within our data (<kbd>GCR</kbd>, <kbd>ILD</kbd>, <kbd>ILM</kbd>, <kbd>NPHI</kbd>, <kbd>PE</kbd>, and <kbd>PEF</kbd>):</p>
<pre>import pylab as pl<br/>df_data_1.drop('lito_ID' ,axis=1).hist(bins=30, figsize=(10,10))<br/>pl.suptitle("Histogram for each numeric input variable")<br/><span>plt.savefig('lithofacies_hist')<br/></span><span>plt.show()</span></pre>
<p>This gives you the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f5a9c946-0b60-4950-9387-3220a8d696c3.png"/></div>
<p>Once a histogram is generated from the data, the first question that is usually asked is whether the shape of the histogram is normal. A characteristic of a <strong>normal distribution</strong> (of data), <em>s</em>, is that it is symmetrical. This means that if the distribution is cut in half, each side will be the mirror of the other, forming a bell-shaped curve, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eef7aa30-8519-4100-9dc4-e4eeb72d413e.png" style=""/></div>
<p>From our generated histograms, perhaps the <strong>NPHI</strong> data point comes the closest to showing a normal distribution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The scatter matrix</h1>
                </header>
            
            <article>
                
<p>A <strong>scatter matrix</strong> is another common analysis tool as it include several pairwise scatter plots of variables presented in a matrix format. It is also used to verify if variables are correlated and whether the correlation is positive or negative.</p>
<p>The following code can be used to experiment with this type of visualization:</p>
<pre>from pandas.tools.plotting import scatter_matrix<br/>from matplotlib import cm<br/><span>feature_names = [ 'GCR', 'NPHI', 'PE', 'ILD', 'ILM']<br/></span>X = df_data_1[feature_names]<br/>y = df_data_1['lito_ID']<br/>cmap = cm.get_cmap('gnuplot')<br/>scatter = pd.plotting.scatter_matrix(X, c = y, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap = cmap)<br/>plt.suptitle('Scatter-matrix for each input variable')<br/>plt.savefig('lithofacies_scatter_matrix')</pre>
<p class="mce-root"/>
<p>This gives you the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7e242789-03cf-443d-b1ab-3fce75e9faf4.png"/></div>
<p>A scatter plot attempts to reveal relationships or associations between variables (called a correlation<em>)</em>. Refer to the following link to learn more about scatter plots: </p>
<p><a href="https://mste.illinois.edu/courses/ci330ms/youtsey/scatterinfo.html">https://mste.illinois.edu/courses/ci330ms/youtsey/scatterinfo.html</a></p>
<p>Looking at the scatter plot generated from our log data (shown in the preceding screenshot), I really don't see any specific or direct correlations between the data.</p>
<p>At this point, you may continue performing a deep dive into the data, perform some reforming or aggregations, or even perhaps go back to the original source (of the data) and request additional or new data.</p>
<p>In the interest of time, for this exercise, we will assume that we will use what data we have and move on to creating and testing various modeling algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the classifier</h1>
                </header>
            
            <article>
                
<p><kbd>scikit-learn</kbd> library can be used to<span> code machine learning classifier</span> and is the only Python library which has four-step modeling pattern.</p>
<div class="packt_infobox">
<p>Refer to the following link for more information about <kbd>sckit-learn</kbd>: <a href="http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf">http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf</a>.</p>
</div>
<p>The coding process of implementing the <kbd>scikit-learn</kbd> model applies to various classifiers within <kbd>sklearn</kbd>, such as decision trees, <strong>k-nearest neighbors</strong> (<strong>KNN</strong>), and more. We will look at a few of these classifiers here, using our well logging data.</p>
<p>The first step in using Scikit to build a model is to create training and test datasets and apply scaling, using the following lines of Python code:</p>
<pre>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)<br/>from sklearn.preprocessing import MinMaxScaler<br/>scaler = MinMaxScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</pre>
<p>Now that we have created a training dataset, we can proceed with building our various types of machine learning models using that data. Typically, in a particular machine learning project, you will have some idea as to the type of machine learning algorithm that you'll want to use, but perhaps not. Either way, you want to verify the performance of your selected algorithm(s).</p>
<p>The following sections show the Python commands which with to create models based using the <kbd>scikit-learn</kbd> module.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a logistic regression model</h1>
                </header>
            
            <article>
                
<p><strong>Regression analysis</strong> is used to understand which of the independent variables (our features: <kbd>GCR</kbd>, <kbd>NPHI</kbd>, <kbd>PE</kbd>, <kbd>ILD</kbd>, and <kbd>ILM</kbd>) are related to the dependent variable; that is, the type of lithofacies.</p>
<p>The following lines of Python code create a logistic regression classifier model and print its accuracy statistics:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1b36a60b-1c77-4299-bfe9-c73918b2f3bd.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a KNN model</h1>
                </header>
            
            <article>
                
<p>The KNN algorithm is a simple, supervised machine learning algorithm that can also be used for classification and regression problems.</p>
<p>The following lines of Python code create a KNN classifier model and print its accuracy statistics:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/09803695-03fa-409a-8f59-43877547ab38.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Gaussian Naive Bayes model</h1>
                </header>
            
            <article>
                
<p><span>Given the class variable, a</span>ll Naive Bayes classifiers infer that the value of a particular feature in the data is independent of the value of any other feature. </p>
<p>The following lines of Python code create a <strong>Gaussian Naive Bayes</strong> (<strong>GaussianNB</strong>) classifier model and print its accuracy statistics:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6db00de6-f0d7-42d8-b1fa-7968c9ee3111.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a support vector machine model</h1>
                </header>
            
            <article>
                
<p>A <strong>support vector machine</strong> (<strong>SVM</strong>) is a supervised learning model with associated learning algorithms that analyze data used for classification and regression analysis.</p>
<p>The following lines of Python code create an SVM classifier model and print its accuracy statistics:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-512 image-border" src="assets/38e0acc3-c204-4d0f-be26-03f071d7d277.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a decision tree model</h1>
                </header>
            
            <article>
                
<p>A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. </p>
<p>The following lines of Python code create a decision tree classifier mode and print its accuracy statistics:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-513 image-border" src="assets/bbbae996-3329-4140-b918-69c59eade65b.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summing them up</h1>
                </header>
            
            <article>
                
<p>Again, if we were working a real project, this step or phase (of the project) would include a much deeper review of each model's performance results and, perhaps, would require the decision to even return back to the data exploration and transformation phase. However, for the sake of time, we'll move forward.</p>
<p>Since we've now seen some simple ways for creating and, at least, superficially judging each model's performance (as far as accuracy), we will move on to the last section of this chapter and look at an example of visualizing a selected model using Python commands.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the classifier</h1>
                </header>
            
            <article>
                
<p>Reviewing the outputs printed after each model build, we should notice that the decision tree model has one of the best results:</p>
<pre>Accuracy of Decision Tree classifier on training set: 0.99 <br/>Accuracy of Decision Tree classifier on test set: 0.00</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A disclaimer of sorts</h1>
                </header>
            
            <article>
                
<p>Typically, we would spend much more time evaluating and verifying the performance of a selected model (and continually training it), but again, you get the general idea (there is plenty of due diligence work to do!), and our goals are more around demonstrating the steps in building an end-to-end machine learning solution using IBM Watson Studio and its resources.</p>
<p>With that in mind, we will now use some Python code to create some visualizations of our models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding decision trees</h1>
                </header>
            
            <article>
                
<p>Decision tree algorithms are very commonly-used supervised learning algorithm models for classification and regression tasks. In this section, we will show how you can visualize decision tree classifiers to better understand their logic.</p>
<p>Decision tree classifiers build a sequence of simple if/else rulings on data through the use of which they can then predict the target value.</p>
<p>Decision trees are usually simpler to interpret because of their structure and the ability we have to visualize the modeled tree, using modules such as the <kbd>sklearn export_graphviz</kbd> function.</p>
<p>The following standard Python code can be used to visualize the decision tree model that we previously built in our notebook:</p>
<pre>!pip install graphviz<br/>from sklearn.tree import DecisionTreeClassifier, export_graphviz<br/>from sklearn import tree<br/>from sklearn.datasets import load_wine<br/>from IPython.display import SVG<br/>from graphviz import Source<br/>from IPython.display import display<br/><br/># feature matrix<br/>feature_names = [ 'GCR', 'NPHI', 'PE', 'ILD', 'ILM']<br/>X = df_data_1[feature_names]<br/><br/># target vector<br/>y = df_data_1['lithofacies']<br/><br/># print dataset description<br/>estimator = DecisionTreeClassifier()<br/>estimator.fit(X, y)<br/><br/>graph = Source(tree.export_graphviz(estimator, out_file=None<br/>   , feature_names=labels<br/>   , filled = True))<br/>display(SVG(graph.pipe(format='svg')))</pre>
<div class="packt_infobox"><span>The sample code can be found at <a href="https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084">https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084</a>.</span></div>
<p>The following screenshot shows the code executed in our notebook and the graphical output it creates (although this is difficult to fit into a single screenshot):</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-518 image-border" src="assets/6cedfd34-c220-42ff-8706-cd2c1f2b139c.png" style=""/></div>
<p class="mce-root">Very seldom does one allow.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced a real-world use case in the evaluation of well drilling logs to classify lithofacies.</p>
<p>We loaded a sample file and performed various profiling and visualization exercises comparing Watson profiles, as well as using Python commands within a notebook. Finally, we used specialized Python libraries to build various types of models and then presented a graph of a supervised machine learning algorithm.</p>
<p>In the next chapter, we will build a cloud-based, multibiometric identity authentication platform.</p>


            </article>

            
        </section>
    </body></html>