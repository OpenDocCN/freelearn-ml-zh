<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Market Basket Analysis, Recommendation Engines, and Sequential Analysis</h1>
            </header>

            <article>
                
<div class="packt_quote">It's much easier to double your business by doubling your conversion rate than by doubling your traffic.<br/>
                                                                          - Jeff Eisenberg, CEO of BuyerLegends.com</div>
<div class="packt_quote">I don't see smiles on the faces of people at Whole Foods.<br/>
                                                                           - Warren Buffett</div>
<p>One would have to live on the dark side of the moon in order to not observe each and every day the results of the techniques that we are about to discuss in this chapter. If you visit <a href="http://www.amazon.com"><span class="URLPACKT">www.amazon.com</span></a>, watch movies on <a href="http://www.netflix.com"><span class="URLPACKT">www.netflix.com</span></a>, or visit any retail website, you will be exposed to terms such as "related products", "because you watched...", "customers who bought <em>x</em> also bought <em>y</em>", or "recommended for you", at every twist and turn. With large volumes of historical real-time or near real-time information, retailers utilize the algorithms discussed here to attempt to increase both the buyer's quantity and value of their purchases. </p>
<p>The techniques to do this can be broken down into two categories: association rules and recommendation engines. Association rule analysis is commonly referred to as market basket analysis as one is trying to understand what items are purchased together. With recommendation engines, the goal is to provide a customer with other items that they will enjoy based on how they have rated previously viewed or purchased items.<br/>
Another technique a business can use is to understand the sequence in which you purchase or use their products and services. This is called sequential analysis. A very common implementation of this methodology is to understand how customers click through various webpages and/or links.</p>
<p>In the examples coming up, we will endeavor to explore how R can be used to develop such algorithms. We will not cover their implementation, as that is outside the scope of this book. We will begin with a market basket analysis of purchasing habits at a grocery store, then dig into building a recommendation engine on website reviews, and finally, analyze the sequence of web pages.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of a market basket analysis</h1>
            </header>

            <article>
                
<p>Market basket analysis is a data mining technique that has the purpose of finding the optimal combination of products or services and allows marketers to exploit this knowledge to provide recommendations, optimize product placement, or develop marketing programs that take advantage of cross-selling. In short, the idea is to identify which items go well together, and profit from it.</p>
<p>You can think of the results of the analysis as an <kbd>if...then</kbd> statement. If a customer buys an airplane ticket, then there is a 46 percent probability that they will buy a hotel room, and if they go on to buy a hotel room, then there is a 33 percent probability that they will rent a car. </p>
<p>However, it is not just for sales and marketing. It is also being used in fraud detection and healthcare; for example, if a patient undergoes treatment A, then there is a 26 percent probability that they might exhibit symptom X. Before going into the details, we should have a look at some terminology, as it will be used in the example:</p>
<ul>
<li><strong>Itemset</strong>: This is a collection of one or more items in the dataset.</li>
<li><strong>Support</strong>: This is the proportion of the transactions in the data that contain an itemset of interest.</li>
<li><strong>Confidence</strong>: This is the conditional probability that if a person purchases or does x, they will purchase or do y; the act of doing x is referred to as the <em>antecedent</em> or Left-Hand Side (LHS), and y is the <em>consequence</em> or Right-Hand Side (RHS).</li>
</ul>
<ul>
<li><strong>Lift</strong>: This is the ratio of the support of x occurring together with y divided by the probability that x and y occur if they are independent. It is the <strong>confidence</strong> divided by the probability of x times the probability of y; for example, say that we have the probability of x and y occurring together as 10 percent and the probability of x is 20 percent and y is 30 percent, then the lift would be 10 percent (20 percent times 30 percent) or 16.67 percent.</li>
</ul>
<p>The package in R that you can use to perform a market basket analysis is <strong>arules: Mining Association Rules and Frequent Itemsets</strong>. The package offers two different methods of finding rules. Why would one have different methods? Quite simply, if you have massive datasets, it can become computationally expensive to examine all the possible combinations of the products. The algorithms that the package supports are <strong>apriori</strong> and <strong>ECLAT</strong>. There are other algorithms to conduct a market basket analysis, but apriori is used most frequently, and so, that will be our focus.</p>
<p>With apriori, the principle is that, if an itemset is frequent, then all of its subsets must also be frequent. A minimum frequency (support) is determined by the analyst prior to executing the algorithm, and once established, the algorithm will run as follows:</p>
<ul>
<li>Let <em>k=1</em> (the number of items)</li>
<li>Generate itemsets of a length that are equal to or greater than the specified support</li>
<li>Iterate <em>k + (1...n)</em>, pruning those that are infrequent (less than the support)</li>
<li>Stop the iteration when no new frequent itemsets are identified</li>
</ul>
<p>Once you have an ordered summary of the most frequent itemsets, you can continue the analysis process by examining the confidence and lift in order to identify the associations of interest.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>For our business case, we will focus on identifying the association rules for a grocery store. The dataset will be from the <kbd>arules</kbd> package and is called <kbd>Groceries</kbd>. This dataset consists of actual transactions over a 30-day period from a real-world grocery store and consists of 9,835 different purchases. All the items purchased are put into one of 169 categories, for example, bread, wine, meat, and so on.<br/>
Let's say that we are a start-up microbrewery trying to make a headway in this grocery outlet and want to develop an understanding of what potential customers will purchase along with beer. This knowledge may just help us in identifying the right product placement within the store or support a cross-selling campaign.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>For this analysis, we will only need to load two packages, as well as the <kbd>Groceries</kbd> dataset:</p>
<pre>
    <strong>&gt; library(arules)</strong><br/>    <br/>    <strong>&gt; library(arulesViz)</strong><br/>    <br/>    <strong>&gt; data(Groceries)<br/></strong><br/>    <strong>&gt; head(Groceries)<br/></strong><br/>    <strong>transactions in sparse format with</strong><br/>    <strong> 9835 transactions (rows) and</strong><br/>    <strong> 169 items (columns)<br/><br/>    &gt; str(Groceries)<br/>    Formal class 'transactions' [package "arules"] with 3 slots<br/>      ..@ data :Formal class 'ngCMatrix' [package "Matrix"] with 5 <br/>        slots<br/>      .. .. ..@ i : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ...<br/>      .. .. ..@ p : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ...<br/>      .. .. ..@ Dim : int [1:2] 169 9835<br/>      .. .. ..@ Dimnames:List of 2<br/>      .. .. .. ..$ : NULL<br/>      .. .. .. ..$ : NULL<br/>      .. .. ..@ factors : list()<br/>      ..@ itemInfo :'data.frame': 169 obs. of 3 variables:<br/>      .. ..$ labels: chr [1:169] "frankfurter" "sausage" "liver loaf" <br/>        "ham" ...<br/>      .. ..$ level2: Factor w/ 55 levels "baby food","bags",..: 44 44 <br/>      44 44 44 44<br/>      44 42 42 41 ...<br/>      .. ..$ level1: Factor w/ 10 levels "canned food",..: 6 6 6 6 6 6 <br/>      6 6 6 6 <br/>      ...<br/>      ..@ itemsetInfo:'data.frame': 0 obs. of 0 variables</strong>
</pre>
<p>This dataset is structured as a sparse matrix object, known as the <kbd>transaction</kbd> class.</p>
<p>So, once the structure is that of the class transaction, our standard exploration techniques will not work, but the <kbd>arules</kbd> package offers us other techniques to explore the data. On a side note, if you have a data frame or matrix and want to convert it to the <kbd>transaction</kbd> class, you can do this with a simple syntax, using the <kbd>as()</kbd> function.</p>
<div class="packt_infobox">
<p>The following code is for illustrative purposes only, so do not run it:</p>
<p><kbd>&gt; # transaction.class.name &lt;- as(current.data.frame,"transactions")</kbd>.</p>
</div>
<p>The best way to explore this data is with an item frequency plot using the <kbd>itemFrequencyPlot()</kbd> function in the <kbd>arules</kbd> package. You will need to specify the transaction dataset, the number of items with the highest frequency to plot, and whether or not you want the relative or absolute frequency of the items. Let's first look at the absolute frequency and the top <kbd>10</kbd> items only:</p>
<pre>
    <strong>&gt; itemFrequencyPlot(Groceries, topN = 10, type = "absolute")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="257" width="366" class="image-border" src="assets/image_10_01.png"/></div>
<p>The top item purchased was <strong>whole milk</strong> with roughly <strong>2</strong>,<strong>500</strong> of the 9,836 transactions in the basket. For a relative distribution of the top 15 items, let's run the following code:</p>
<pre>
    <strong>&gt; itemFrequencyPlot(Groceries, topN = 15)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="267" width="381" class="image-border" src="assets/image_10_02.png"/></div>
<p>Alas, here we see that beer shows up as the 13th and 15th most purchased item at this store. Just under 10 percent of the transactions had purchases of <strong>bottled beer</strong> and/or <strong>canned beer</strong>.</p>
<p>For the purpose of this exercise, this is all we really need to do, therefore, we can move right on to the modeling and evaluation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>We will start by mining the data for the overall association rules before moving on to our rules for beer specifically. Throughout the modeling process, we will use the apriori algorithm, which is the appropriately named <kbd>apriori()</kbd> function in the <kbd>arules</kbd> package. The two main things that we will need to specify in the function is the dataset and parameters. As for the parameters, you will need to apply judgment when specifying the minimum support, confidence, and the minimum and/or maximum length of basket items in an itemset. Using the item frequency plots, along with trial and error, let's set the minimum support at 1 in 1,000 transactions and minimum confidence at 90 percent. Additionally, let's establish the maximum number of items to be associated as four. The following is the code to create the object that we will call <kbd>rules</kbd>:</p>
<pre>
    <strong>&gt; rules &lt;- apriori(Groceries, parameter = list(supp = 0.001, conf = <br/>      0.9, maxlen=4))</strong>
</pre>
<p>Calling the object shows how many rules the algorithm produced:</p>
<pre>
    <strong>&gt; rules</strong><br/>    <strong>set of 67 rules</strong>
</pre>
<p>There are a number of ways to examine the rules. The first thing that I recommend is to set the number of displayed digits to only two, with the <kbd>options()</kbd> function in base R. Then, sort and inspect the top five rules based on the lift that they provide, as follows:</p>
<pre>
    <strong>&gt; options(digits = 2)</strong><br/>    <br/>    <strong>&gt; rules &lt;- sort(rules, by = "lift", decreasing = TRUE)</strong><br/>    <br/>    <strong>&gt; inspect(rules[1:5])</strong><br/>    <strong>  lhs                 rhs                support confidence lift</strong><br/>    <strong>1 {liquor, r</strong><strong>ed/blush wine}     =&gt; {bottled beer}      0.0019       <br/>       0.90 11.2</strong><br/>    <strong>2 {root vegetables, </strong><strong>butter, </strong><strong>cream cheese }      =&gt; {yogurt}            <br/>       0.0010       0.91  6.5</strong><br/>    <strong>3 {citrus fruit, </strong><strong>root vegetables,</strong><strong> soft cheese}=&gt; {other vegetables}  <br/>       0.0010       1.00  5.2</strong><br/>    <strong>4 {pip fruit, w</strong><strong>hipped/sour cream, b</strong><strong>rown bread}=&gt; {other vegetables}  <br/>       0.0011       1.00  5.2</strong><br/>    <strong>5 {butter,wh</strong><strong>ipped/sour cream, </strong><strong>soda}    =&gt; {other vegetables}  <br/>       0.0013       0.93  4.8</strong>
</pre>
<p>Lo and behold, the rule that provides the best overall lift is the purchase of <kbd>liquor</kbd> and <kbd>red wine</kbd> on the probability of purchasing <kbd>bottled beer</kbd>. I have to admit that this is pure chance and not intended on my part. As I always say, it is better to be lucky than good. Although, it is still not a very common transaction with a support of only 1.9 per 1,000.</p>
<p>You can also sort by the support and confidence, so let's have a look at the first <kbd>5</kbd> <kbd>rules</kbd> <kbd>by="confidence"</kbd> in descending order, as follows:</p>
<pre>
    <strong>&gt; rules &lt;- sort(rules, by = "confidence", decreasing = TRUE)<br/></strong><br/>    <strong>&gt; inspect(rules[1:5])</strong><br/>    <strong>  lhs             rhs                support confidence lift</strong><br/>    <strong>1 {citrus fruit, ro</strong><strong>ot vegetables,</strong><strong> soft cheese}=&gt; {other vegetables}  <br/>      0.0010          1  5.2</strong><br/>    <strong>2 {pip fruit, wh</strong><strong>ipped/sour cream, </strong><strong>brown bread}=&gt; {other vegetables}  <br/>      0.0011          1  5.2</strong><br/>    <strong>3 {rice, s</strong><strong>ugar}  =&gt; {whole milk}        0.0012          1  3.9</strong><br/>    <strong>4 {canned fish, h</strong><strong>ygiene articles} =&gt; {whole milk} 0.0011   1  3.9</strong><br/>    <strong>5 {root vegetables,</strong><strong> butter,</strong><strong> rice} =&gt; {whole milk} 0.0010   1  3.9</strong>
</pre>
<p>You can see in the table that <kbd>confidence</kbd> for these transactions is 100 percent. Moving on to our specific study of beer, we can utilize a function in <kbd>arules</kbd> to develop cross tabulations--the <kbd>crossTable()</kbd> function--and then examine whatever suits our needs. The first step is to create a table with our dataset:</p>
<pre>
    <strong>&gt; tab &lt;- crossTable(Groceries)</strong>
</pre>
<p>With <kbd>tab</kbd> created, we can now examine the joint occurrences between the items. Here, we will look at just the first three rows and columns:</p>
<pre>
    <strong>&gt; tab[1:3, 1:3]</strong><br/>    <strong>            frankfurter sausage liver loaf</strong><br/>    <strong>frankfurter         580      99          7</strong><br/>    <strong>sausage              99     924         10</strong><br/>    <strong>liver loaf            7      10         50</strong>
</pre>
<p>As you might imagine, shoppers only selected liver loaf 50 times out of the 9,835 transactions. Additionally, of the <kbd>924</kbd> times, people gravitated toward <kbd>sausage</kbd>, <kbd>10</kbd> times they felt compelled to grab <kbd>liver loaf</kbd>. (Desperate times call for desperate measures!) If you want to look at a specific example, you can either specify the row and column number or just spell that item out:</p>
<pre>
    <strong>&gt; table["bottled beer","bottled beer"]</strong><br/>    <strong>[1] 792</strong>
</pre>
<p>This tells us that there were <kbd>792</kbd> transactions of <kbd>bottled beer</kbd>. Let's see what the joint occurrence between <kbd>bottled beer</kbd> and <kbd>canned beer</kbd> is:</p>
<pre>
    <strong>&gt; table["bottled beer","canned beer"]</strong><br/>    <strong>[1] 26</strong>
</pre>
<p>I would expect this to be low as it supports my idea that people lean toward drinking beer from either a bottle or a can. I strongly prefer a bottle. It also makes a handy weapon to protect oneself from all these ruffian protesters like Occupy Wallstreet and the like.</p>
<p>We can now move on and derive specific rules for <kbd>bottled beer</kbd>. We will again use the <kbd>apriori()</kbd> function, but this time, we will add a syntax around <kbd>appearance</kbd>. This means that we will specify in the syntax that we want the left-hand side to be items that increase the probability of a purchase of <kbd>bottled beer</kbd>, which will be on the right-hand side. In the following code, notice that I've adjusted the <kbd>support</kbd> and <kbd>confidence</kbd> numbers. Feel free to experiment with your own settings:</p>
<pre>
<strong>    &gt; beer.rules &lt;- apriori(data = Groceries, </strong><strong>parameter = list(support <br/>      = 0.0015, confidence = 0.3), </strong><strong>appearance = list(default = "lhs",<br/>        rhs = "bottled beer"))</strong><br/>    <br/>    <strong>&gt; beer.rules</strong><br/>    <strong>set of 4 rules</strong>
</pre>
<p>We find ourselves with only <kbd>4</kbd> association rules. We have seen one of them already; now let's bring in the other three rules in descending order by lift:</p>
<pre>
    <strong>&gt; beer.rules &lt;- sort(beer.rules, decreasing = TRUE, by = "lift")</strong><br/>    <br/>    <strong>&gt; inspect(beer.rules)</strong><br/>    <strong>  lhs                   rhs            support confidence lift</strong><br/>    <strong>1 {liquor, r</strong><strong>ed/blush wine} =&gt; {bottled beer}  0.0019  0.90 11.2</strong><br/>    <strong>2 {liquor}               =&gt; {bottled beer}    0.0047  0.42  5.2</strong><br/>    <strong>3 {soda, </strong><strong>red/blush wine} =&gt; {bottled beer}    0.0016  0.36  4.4</strong><br/>    <strong>4 {other vegetables, </strong><strong>red/blush wine} =&gt; {bottled beer}0.0015 0.31  <br/>      3.8</strong>
</pre>
<p>In all of the instances, the purchase of <kbd>bottled beer</kbd> is associated with booze, either <kbd>liquor</kbd> and/or <kbd>red wine</kbd> , which is no surprise to anyone. What is interesting is that <kbd>white wine</kbd> is not in the mix here. Let's take a closer look at this and compare the joint occurrences of <kbd>bottled beer</kbd> and types of wine:</p>
<pre>
    <strong>&gt; tab["bottled beer", "red/blush wine"]</strong><br/>    <strong>[1] 48</strong><br/>    <br/>    <strong>&gt; tab["red/blush wine", "red/blush wine"]</strong><br/>    <strong>[1] 189</strong><br/>    <br/>    <strong>&gt; 48/189</strong><br/>    <strong>[1] 0.25</strong><br/>    <br/>    <strong>&gt; tab["white wine", "white wine"]</strong><br/>    <strong>[1] 187</strong><br/>    <br/>    <strong>&gt; tab["bottled beer", "white wine"]</strong><br/>    <strong>[1] 22</strong><br/>    <br/>    <strong>&gt; 22/187</strong><br/>    <strong>[1] 0.12</strong>
</pre>
<p>It's interesting that 25 percent of the time, when someone purchased <kbd>red wine</kbd>, they also purchased <kbd>bottled beer</kbd>; but with <kbd>white wine</kbd>, a joint purchase only happened in 12 percent of the instances. We certainly don't know why in this analysis, but this could potentially help us to determine how we should position our product in this grocery store. Another thing before we move on is to look at a plot of the rules. This is done with the <kbd>plot()</kbd> function in the <kbd>arulesViz</kbd> package.<br/>
There are many graphic options available. For this example, let's specify that we want a <kbd>graph</kbd>, showing <kbd>lift</kbd>, and the rules provided and shaded by <kbd>confidence</kbd>. The following syntax will provide this accordingly:</p>
<pre>
    <strong>&gt; plot(beer.rules, method = "graph", measure = "lift", shading = <br/>    "confidence")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_10_03.png"/></div>
<p>This graph shows that <strong>liquor</strong>/<strong>red wine</strong> provides the best <strong>lift</strong> and the highest level of <strong>confidence</strong> with both the <strong>size</strong> of the circle and its shading.</p>
<p>What we've just done in this simple exercise is show how easy it is with R to conduct a market basket analysis. It doesn't take much imagination to figure out the analytical possibilities that one can include with this technique, for example, in corporate customer segmentation, longitudinal purchase history, and so on, as well as how to use it in ad displays, co-promotions, and so on. Now let's move on to a situation where customers rate items, and learn how to build and test recommendation engines.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of a recommendation engine</h1>
            </header>

            <article>
                
<p>We will now focus on situations where users have provided rankings or ratings on previously viewed or purchased items. There are two primary categories of designing recommendation systems: <em>collaborative filtering and content-based</em> (Ansari, Essegaier, and Kohli, 2000). The former category is what we will concentrate on, as this is the focus of the <kbd>recommenderlab</kbd> R package that we will be using.</p>
<p>For content-based approaches, the concept is to link user preferences with item attributes. These attributes may be things such as the genre, cast, or storyline for a movie or TV show recommendation. As such, recommendations are based entirely on what the user provides as ratings; there is no linkage to what anyone else recommends. This has the advantage over content-based approaches in that when a new item is added, it can be recommended to a user if it matches their profile, instead of relying on other users to rate it first (the so-called "first rater problem"). However, content-based methods can suffer when limited content is available, either because of the domain or when a new user enters the system. This can result in non-unique recommendations, that is, poor recommendations (Lops, Gemmis, and Semeraro, 2011).</p>
<p>In collaborative filtering, the recommendations are based on the many ratings provided by some or all of the individuals in the database. Essentially, it tries to capture the wisdom of the crowd.</p>
<p>For collaborative filtering, we will focus on the following four methods:</p>
<ul>
<li><strong>User-based collaborative filtering</strong> (<strong>UBCF</strong>)</li>
<li><strong>Item-based collaborative filtering</strong> (<strong>IBCF</strong>)</li>
<li><strong>Singular value decomposition</strong> (<strong>SVD</strong>)</li>
<li><strong>Principal components analysis</strong> (<strong>PCA</strong>)</li>
</ul>
<p>We will look at these methods briefly before moving on to the business case. It is also important to understand that <kbd>recommenderlab</kbd> was not designed to be used as a real-world implementation tool, but rather as a laboratory tool in order to research algorithms provided in the package as well as algorithms that you wish to experiment with on your own.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">User-based collaborative filtering</h1>
            </header>

            <article>
                
<p>In UBCF, the algorithm finds <em>missing ratings for a user by first finding a neighborhood of similar users and then aggregating the ratings of these users to form a prediction</em> (Hahsler, 2011). The neighborhood is determined by selecting either the KNN that is the most similar to the user we are making predictions for or by some similarity measure with a minimum threshold. The two similarity measures available in <kbd>recommenderlab</kbd> are <strong>pearson correlation coefficient</strong> and <strong>cosine similarity</strong>. I will skip the formulas for these measures as they are readily available in the package documentation.</p>
<p>Once the neighborhood method is decided on, the algorithm identifies the neighbors by calculating the similarity measure between the individual of interest and their neighbors on only those items that were rated by both. Through a scoring scheme, say, a simple average, the ratings are aggregated in order to make a predicted score for the individual and item of interest.</p>
<p>Let's look at a simple example. In the following matrix, there are six individuals with ratings on four movies, with the exception of my rating for <em>Mad Max</em>. Using <em>k=1</em>, the nearest neighbor is <strong>Homer</strong>, with <strong>Bart</strong> a close second; even though <strong>Flanders</strong> hated the <strong>Avengers</strong> as much as I did. So, using Homer's rating for <strong>Mad Max</strong>, which is <strong>4</strong>, the predicted rating for me would also be a <strong>4</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="118" width="336" class="image-border" src="assets/image_10_04.png"/></div>
<p>There are a number of ways to weigh the data and/or control the bias. For instance, <strong>Flanders</strong> is quite likely to have lower ratings than the other users, so normalizing the data where the new rating score is equal to the user rating for an item minus the average for that user for all the items is likely to improve the rating accuracy.</p>
<p>The weakness of UBCF is that, to calculate the similarity measure for all the possible users, the entire database must be kept in memory, which can be quite computationally expensive and time-consuming.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Item-based collaborative filtering</h1>
            </header>

            <article>
                
<p>As you might have guessed, IBCF uses the similarity between the items and not users to make a recommendation. <em>The assumption behind this approach is that users will prefer items that are similar to other items they like</em> (Hahsler, 2011). The model is built by calculating a pairwise similarity matrix of all the items. The popular similarity measures are Pearson correlation and cosine similarity. To reduce the size of the similarity matrix, one can specify to retain only the k-most similar items. However, limiting the size of the neighborhood may significantly reduce the accuracy, leading to poorer performance versus UCBF.</p>
<p>Continuing with our simplified example, if we examine the following matrix, with <em>k=1</em> the item most similar to <strong>Mad Max</strong> is <strong>American Sniper</strong>, and we can thus take that rating as the prediction for <strong>Mad Max</strong>, as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="127" width="368" class="image-border" src="assets/image_10_05.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Singular value decomposition and principal components analysis</h1>
            </header>

            <article>
                
<p>It is quite common to have a dataset where the number of users and items number in the millions. Even if the rating matrix is not that large, it may be beneficial to reduce the dimensionality by creating a smaller (lower-rank) matrix that captures most of the information in the higher-dimension matrix. This may potentially allow you to capture important latent factors and their corresponding weights in the data. Such factors could lead to important insights, such as the movie genre or book topics in the rating matrix. Even if you are unable to discern meaningful factors, the techniques may filter out the noise in the data.</p>
<p>One issue with large datasets is that you will likely end up with a sparse matrix that has many ratings missing. One weakness of these methods is that they will not work on a matrix with missing values, which must be imputed. As with any data imputation task, there are a number of techniques that one can try and experiment with, such as using the mean, median, or code as zeroes. The default for <kbd>recommenderlab</kbd> is to use the median. </p>
<p>So, what is SVD? It is simply a method for matrix factorization, and can help transform a set of correlated features to a set of uncorrelated features. Say that you have a matrix called <strong>A</strong>. This matrix will factor into three matrices: <strong>U</strong>, <strong>D</strong>, and <strong>V<sup>T</sup></strong>. U is an orthogonal matrix, D is a non-negative, diagonal matrix, and V<sup>T</sup> is a transpose of an orthogonal matrix. Now, let's look at our rating matrix and walk through an example using R.</p>
<p>The first thing that we will do is recreate the rating matrix (think of it as matrix <strong>A</strong>, as shown in the following code):</p>
<pre>
    <strong>&gt; ratings &lt;- c(3, 5, 5, 5, 1, 1, 5, 2, 5, 1, 1, 5, 3, 5, 1, 5, 4 <br/>      ,2, 4, 3, 4, 2, 1, 4)</strong><br/>    <br/>    <strong>&gt; ratingMat &lt;- matrix(ratings, nrow = 6)</strong><br/>    <br/>    <strong>&gt; rownames(ratingMat) &lt;- c("Homer", "Marge", "Bart", "Lisa", <br/>      "Flanders", "Me")</strong><br/>    <br/>    <strong>&gt; colnames(ratingMat) &lt;- c("Avengers", "American Sniper", "Les <br/>      Miserable", "Mad Max")</strong><br/>    <br/>    <strong>&gt; ratingMat</strong><br/>    <strong>Avengers  American Sniper  Les Miserable   Mad    Max</strong><br/>    <strong>Homer        3               5             3       4</strong><br/>    <strong>Marge        5               2             5       3</strong><br/>    <strong>Bart         5               5             1       4</strong><br/>    <strong>Lisa         5               1             5       2</strong><br/>    <strong>Flanders     1               1             4       1</strong><br/>    <strong>Me           1               5             2       4</strong>
</pre>
<p>Now, we will use the <kbd>svd()</kbd> function in base R to create the three matrices described above, which R calls <kbd>$d</kbd>, <kbd>$u</kbd>, and <kbd>$v</kbd>. You can think of the <kbd>$u</kbd> values as an individual's loadings on that factor and <kbd>$v</kbd> as a movie's loadings on that dimension. For example, <kbd>Mad Max</kbd> loads on dimension one at -0.116 (1st row, 4th column):</p>
<pre>
    <strong>&gt; svd &lt;- svd(ratingMat)</strong><br/>    <br/>    <strong>&gt; svd</strong><br/>    <strong>$d</strong><br/>    <strong>[1] 16.1204848  6.1300650  3.3664409  0.4683445</strong><br/>    <br/>    <strong>$u</strong><br/>    <strong>           [,1]       [,2]       [,3]        [,4]</strong><br/>    <strong>[1,] -0.4630576  0.2731330  0.2010738 -0.27437700</strong><br/>    <strong>[2,] -0.4678975 -0.3986762 -0.0789907  0.53908884</strong><br/>    <strong>[3,] -0.4697552  0.3760415 -0.6172940 -0.31895450</strong><br/>    <strong>[4,] -0.4075589 -0.5547074 -0.1547602 -0.04159102</strong><br/>    <strong>[5,] -0.2142482 -0.3017006  0.5619506 -0.57340176</strong><br/>    <strong>[6,] -0.3660235  0.4757362  0.4822227  0.44927622</strong><br/>    <br/>    <strong>$v</strong><br/>    <strong>           [,1]       [,2]        [,3]       [,4]</strong><br/>    <strong>[1,] -0.5394070 -0.3088509 -0.77465479 -0.1164526</strong><br/>    <strong>[2,] -0.4994752  0.6477571  0.17205756 -0.5489367</strong><br/>    <strong>[3,] -0.4854227 -0.6242687  0.60283871 -0.1060138</strong><br/>    <strong>[4,] -0.4732118  0.3087241  0.08301592  0.8208949</strong>
</pre>
<p>It is easy to explore how much variation is explained by reducing the dimensionality. Let's sum the diagonal numbers of <kbd>$d</kbd>, then look at how much of the variation we can explain with just two factors, as follows:</p>
<pre>
    <strong>&gt; sum(svd$d)</strong><br/>    <strong>[1] 26.08534</strong><br/>    <br/>    <strong>&gt; var &lt;- sum(svd$d[1:2])</strong><br/>    <br/>    <strong>&gt; var</strong><br/>    <strong>[1] 22.25055</strong><br/>    <br/>    <strong>&gt; var/sum(svd$d)</strong><br/>    <strong>[1] 0.8529908</strong>
</pre>
<p>With two of the four factors, we are able to capture just over 85 percent of the total variation in the full matrix. You can see the scores that the reduced dimensions would produce. To do this, we will create a function. (Many thanks to the <a href="http://www.stackoverflow.com"><span class="URLPACKT">www.stackoverflow.com</span></a> respondents who helped me put this function together.) This function will allow us to specify the number of factors that are to be included for a prediction. It calculates a rating value by multiplying the <kbd>$u</kbd> matrix times the <kbd>$v</kbd> matrix times the <kbd>$d</kbd> matrix:</p>
<pre>
    <strong>&gt; f1 &lt;- function(x) {</strong><br/>    <strong>score = 0</strong><br/>    <strong>for(i in 1:n )</strong><br/>       <strong>score &lt;- score + svd$u[,i] %*% t(svd$v[,i]) * svd$d[i]</strong><br/>    <strong>return(score)}</strong>
</pre>
<p>By specifying <kbd>n=4</kbd> and calling the function, we can recreate the original rating matrix:</p>
<pre>
    <strong>&gt; n = 4</strong><br/>    <br/>    <strong>&gt; f1(svd)</strong><br/>    <strong>     [,1] [,2] [,3] [,4]</strong><br/>    <strong>[1,]    3    5    3    4</strong><br/>    <strong>[2,]    5    2    5    3</strong><br/>    <strong>[3,]    5    5    1    4</strong><br/>    <strong>[4,]    5    1    5    2</strong><br/>    <strong>[5,]    1    1    4    1</strong><br/>    <strong>[6,]    1    5    2    4</strong>
</pre>
<p>Alternatively, we can specify <kbd>n=2</kbd> and examine the resulting matrix:</p>
<pre>
    <strong>&gt; n = 2</strong><br/>    <br/>    <strong>&gt; f1(svd)</strong><br/>    <strong>            [,1]      [,2]     [,3]     [,4]</strong><br/>    <strong>[1,] 3.509402 4.8129937 2.578313 4.049294</strong><br/>    <strong>[2,] 4.823408 2.1843483 5.187072 2.814816</strong><br/>    <strong>[3,] 3.372807 5.2755495 2.236913 4.295140</strong><br/>    <strong>[4,] 4.594143 1.0789477 5.312009 2.059241</strong><br/>    <strong>[5,] 2.434198 0.5270894 2.831096 1.063404</strong><br/>    <strong>[6,] 2.282058 4.8361913 1.043674 3.692505</strong>
</pre>
<p>So, with SVD, you can reduce the dimensionality and possibly identify the meaningful latent factors.</p>
<p>If you went through the prior chapter, you will see the similarities with PCA. In fact, the two are closely related and often used interchangeably as they both utilize matrix factorization. You may be asking what is the difference? In short, PCA is based on the covariance matrix, which is symmetric. This means that you start with the data, compute the covariance matrix of the centered data, diagonalize it, and create the components.</p>
<p>Let's apply a portion of the PCA code from the prior chapter to our data in order to see how the difference manifests itself:</p>
<pre>
    <strong>&gt; library(psych)</strong><br/>    <br/>    <strong>&gt; pca &lt;- principal(ratingMat, nfactors = 2, rotate = "none")</strong><br/>    <br/>    <strong>&gt; pca</strong><br/>    <strong>Principal Components Analysis</strong><br/>    <strong>Call: principal(r = ratingMat, nfactors = 2, rotate =</strong><br/>    <strong>"none")</strong><br/>    <strong>Standardized loadings (pattern matrix) based upon correlation <br/>      matrix</strong><br/>    <strong>                  PC1   PC2   h2    u2</strong><br/>    <strong>Avengers        -0.09  0.98 0.98 0.022</strong><br/>    <strong>American Sniper  0.99 -0.01 0.99 0.015</strong><br/>    <strong>Les Miserable   -0.90  0.18 0.85 0.150</strong><br/>    <strong>Mad Max          0.92  0.29 0.93 0.071</strong><br/>    <br/>    <strong>                            PC1  PC2</strong><br/>    <strong>SS loadings           2.65 1.09</strong><br/>    <strong>Proportion Var        0.66 0.27</strong><br/>    <strong>Cumulative Var        0.66 0.94</strong><br/>    <strong>Proportion Explained  0.71 0.29</strong><br/>    <strong>Cumulative Proportion 0.71 1.00</strong>
</pre>
<p>You can see that PCA is easier to interpret. Notice how <kbd>American Sniper</kbd> and <kbd>Mad Max</kbd> have high loadings on the first component, while only <kbd>Avengers</kbd> has a high loading on the second component. Additionally, these two components account for 94 percent of the total variance in the data.  It is noteworthy to include that, in the time between the first and second editions of this book, PCA has become unavailable. </p>
<p>Having applied a simplistic rating matrix to the techniques of collaborative filtering, let's move on to a more complex example using real-world data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding and recommendations</h1>
            </header>

            <article>
                
<p>This business case is a joke, literally. Maybe it is more appropriate to say a bunch of jokes, as we will use the <kbd>Jester5k</kbd> data from the <kbd>recommenderlab</kbd> package. This data consists of 5,000 ratings on 100 jokes sampled from the Jester Online Joke Recommender System. It was collected between April 1999 and May 2003, and all the users have rated at least 36 jokes (Goldberg, Roeder, Gupta, and Perkins, 2001). Our goal is to compare the recommendation algorithms and select the best one.</p>
<p>As such, I believe it is important to lead off with a statistical joke to put one in the proper frame of mind. I'm not sure of how to properly provide attribution for this one, but it is popular all over the Internet.</p>
<p>A statistician's wife had twins. He was delighted. He rang the minister who was also delighted. "Bring them to church on Sunday and we'll baptize them", said the minister. "No", replied the statistician. "Baptize one. We'll keep the other as a control."</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding, preparation, and recommendations</h1>
            </header>

            <article>
                
<p>The one library that we will need for this exercise is <kbd>recommenderlab</kbd>. The package was developed by the Southern Methodist University's Lyle Engineering Lab, and they have an excellent website with supporting documentation at <a href="https://lyle.smu.edu/IDA/recommenderlab/"><span class="URLPACKT">https://lyle.smu.edu/IDA/recommenderlab/</span></a>:</p>
<pre>
    <strong>&gt; library(recommenderlab)</strong><br/>    <br/>    <strong>&gt; data(Jester5k)</strong><br/>    <br/>    <strong>&gt; Jester5k</strong><br/>    <strong>5000 x 100 rating matrix of class 'realRatingMatrix' with</strong><br/>    <strong>362106 ratings.</strong>
</pre>
<p>The rating matrix contains <kbd>362106</kbd> total ratings. It is quite easy to get a list of a user's ratings. Let's look at user number <kbd>10</kbd>. The following output is abbreviated for the first five jokes:</p>
<pre>
    <strong>&gt; as(Jester5k[10,], "list")</strong><br/>    <strong>$u12843</strong><br/>    <strong>   j1    j2    j3    j4    j5 ...</strong><br/>    <strong>-1.99 -6.89  2.09 -4.42 -4.90 ...</strong>
</pre>
<p>You can also look at the mean rating for a user (user <kbd>10</kbd>) and/or the mean rating for a specific joke (joke <kbd>1</kbd>), as follows:</p>
<pre>
    <strong>&gt; rowMeans(Jester5k[10,])</strong><br/>    <strong>u12843 </strong><br/>    <strong>  -1.6</strong><br/>    <br/>    <strong>&gt; colMeans(Jester5k[,1])</strong><br/>    <strong>  j1 </strong><br/>    <strong>0.92</strong>
</pre>
<p>One method to get a better understanding of the data is to plot the ratings as a histogram, both the raw data and after normalization. We will do this with the <kbd>getRating()</kbd> function from <kbd>recommenderlab</kbd>:</p>
<pre>
    <strong>&gt; hist(getRatings(Jester5k), breaks=100)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="255" width="403" class="image-border" src="assets/image_10_06.png"/></div>
<p>The <kbd>normalize()</kbd> function in the package centers the data by subtracting the mean of the ratings of the joke from that joke's rating. As the preceding distribution is slightly biased towards the positive ratings, normalizing the data can account for this, thus yielding a more normal distribution but still showing a slight skew towards the positive ratings, as follows:</p>
<pre>
    <strong>&gt; hist(getRatings(normalize(Jester5k)), breaks = 100)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="230" width="363" class="image-border" src="assets/image_10_07.png"/></div>
<p>Before modeling and evaluation, it is quite easy to create the <kbd>train</kbd> and <kbd>test</kbd> datasets with the <kbd>recommenderlab</kbd> package with the <kbd>evaluationScheme()</kbd> function. Let's do an 80/20 split of the data for the <kbd>train</kbd> and <kbd>test</kbd> sets. You can also choose k-fold cross-validation and bootstrap resampling if you desire. We will also specify that for the <kbd>test</kbd> set, the algorithm will be given 15 ratings. This means that the other rating items will be used to compute the error. Additionally, we will specify what the threshold is for a good rating; in our case, greater than or equal to <kbd>5</kbd>:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <br/>    <strong>&gt; e &lt;- evaluationScheme(Jester5k, method="split", </strong><br/>    <strong>train=0.8, given=15, goodRating=5)</strong><br/>    <br/>    <strong>&gt;  e</strong><br/>    <strong>Evaluation scheme with 15 items given</strong><br/>    <strong>Method: 'split' with 1 run(s).</strong><br/>    <strong>Training set proportion: 0.800</strong><br/>    <strong>Good ratings: &gt;=5.000000</strong><br/>    <strong>Data set: 5000 x 100 rating matrix of class</strong><br/>    <strong>'realRatingMatrix' with 362106</strong><br/>    <strong> ratings.</strong>
</pre>
<p>With the <kbd>train</kbd> and <kbd>test</kbd> data established, we will now begin to model and evaluate the different recommenders: user-based, item-based, popular, SVD, PCA, and random.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling, evaluation, and recommendations</h1>
            </header>

            <article>
                
<p>In order to build and test our recommendation engines, we can use the same function, <kbd>Recommender()</kbd>, merely changing the specification for each technique. In order to see what the package can do and explore the parameters available for all six techniques, you can examine the registry. Looking at the following IBCF, we can see that the default is to find 30 neighbors using the cosine method with the centered data while the missing data is not coded as a zero:</p>
<pre>
    <strong>&gt; recommenderRegistry$get_entries(dataType =</strong><br/>    <strong>"realRatingMatrix")</strong><br/>    <br/>    <strong>$ALS_realRatingMatrix</strong><br/><strong>    Recommender method: ALS for realRatingMatrix</strong><br/><strong>    Description: Recommender for explicit ratings based on latent <br/>      factors, calculated by alternating least squares algorithm.</strong><br/><strong>    Reference: Yunhong Zhou, Dennis Wilkinson, Robert Schreiber, Rong <br/>      Pan (2008).<br/>    Large-Scale Parallel Collaborative Filtering for the Netflix Prize, <br/>      4th Int'l   <br/>    Conf. Algorithmic Aspects in Information and Management, LNCS 5034.</strong><br/><strong>    Parameters:</strong><br/><strong>    normalize lambda n_factors n_iterations min_item_nr seed</strong><br/><strong>    1 NULL 0.1 10 10 1 NULL</strong><br/><br/><strong>    $ALS_implicit_realRatingMatrix</strong><br/><strong>    Recommender method: ALS_implicit for realRatingMatrix</strong><br/><strong>    Description: Recommender for implicit data based on latent factors, <br/>    calculated by alternating least squares algorithm.</strong><br/><strong>    Reference: Yifan Hu, Yehuda Koren, Chris Volinsky (2008). <br/>      Collaborative<br/>    Filtering for Implicit Feedback Datasets, ICDM '08 Proceedings of <br/>      the 2008 <br/>    Eighth IEEE International Conference on Data Mining, pages 263-272.</strong><br/><strong>    Parameters:</strong><br/><strong>    lambda alpha n_factors n_iterations min_item_nr seed</strong><br/><strong>    1 0.1 10 10 10 1 NULL</strong><br/><br/><strong>    $IBCF_realRatingMatrix</strong><br/><strong>    Recommender method: IBCF for realRatingMatrix</strong><br/><strong>    Description: Recommender based on item-based collaborative <br/>      filtering.</strong><br/><strong>    Reference: NA</strong><br/><strong>    Parameters:</strong><br/><strong>    k method normalize normalize_sim_matrix alpha na_as_zero</strong><br/><strong>    1 30 "Cosine" "center" FALSE 0.5 FALSE</strong><br/><br/><strong>    $POPULAR_realRatingMatrix</strong><br/><strong>    Recommender method: POPULAR for realRatingMatrix</strong><br/><strong>    Description: Recommender based on item popularity.</strong><br/><strong>    Reference: NA</strong><br/><strong>    Parameters:</strong><br/><strong>     normalize aggregationRatings aggregationPopularity</strong><br/><strong>     1 "center" new("standardGeneric" new("standardGeneric"</strong><br/><br/><strong>   $RANDOM_realRatingMatrix</strong><br/><strong>   Recommender method: RANDOM for realRatingMatrix</strong><br/><strong>   Description: Produce random recommendations (real ratings).</strong><br/><strong>   Reference: NA</strong><br/><strong>   Parameters: None</strong><br/><br/><strong>   $RERECOMMEND_realRatingMatrix</strong><br/><strong>   Recommender method: RERECOMMEND for realRatingMatrix</strong><br/><strong>   Description: Re-recommends highly rated items (real ratings).</strong><br/><strong>   Reference: NA</strong><br/><strong>   Parameters:</strong><br/><strong>     randomize minRating</strong><br/><strong>   1 1 NA</strong><br/><br/><strong>   $SVD_realRatingMatrix</strong><br/><strong>   Recommender method: SVD for realRatingMatrix</strong><br/><strong>   Description: Recommender based on SVD approximation with column-mean <br/>   imputation.</strong><br/><strong>   Reference: NA</strong><br/><strong>   Parameters:</strong><br/><strong>    k maxiter normalize</strong><br/><strong>    1 10 100 "center"</strong><br/><br/><strong>   $SVDF_realRatingMatrix</strong><br/><strong>   Recommender method: SVDF for realRatingMatrix</strong><br/><strong>   Description: Recommender based on Funk SVD with gradient descend.</strong><br/><strong>   Reference: NA</strong><br/><strong>   Parameters:</strong><br/><strong>    k gamma lambda min_epochs max_epochs min_improvement normalize</strong><br/><strong>    1 10 0.015 0.001 50 200 1e-06 "center"</strong><br/><strong>   verbose</strong><br/><strong>   1 FALSE</strong><br/><br/><strong>   $UBCF_realRatingMatrix</strong><br/><strong>   Recommender method: UBCF for realRatingMatrix</strong><br/><strong>   Description: Recommender based on user-based collaborative <br/>     filtering.</strong><br/><strong>   Reference: NA</strong><br/><strong>   Parameters:</strong><br/><strong>    method nn sample normalize</strong><br/><strong>    1 "cosine" 25 FALSE "center"<br/></strong>
</pre>
<p>Here is how you can put together the algorithms based on the <kbd>train</kbd> data. For simplicity, let's use the default algorithm settings. You can adjust the parameter settings by simply including your changes in the function as a list:</p>
<pre>
    <strong>&gt; ubcf &lt;- Recommender(getData(e,"train"), "UBCF")</strong><br/>    <br/>    <strong>&gt; ibcf &lt;- Recommender(getData(e,"train"), "IBCF")</strong><br/>    <br/>    <strong>&gt; svd &lt;- Recommender(getData(e, "train"), "SVD")</strong><br/>    <br/>    <strong>&gt; popular &lt;- Recommender(getData(e, "train"), "POPULAR")</strong><br/>    <br/>    <strong>&gt; pca &lt;- Recommender(getData(e, "train"), "PCA")</strong><br/>    <br/>    <strong>&gt; random &lt;- Recommender(getData(e, "train"), "RANDOM")</strong>
</pre>
<p>Now, using the <kbd>predict()</kbd> and <kbd>getData()</kbd> functions, we will get the predicted ratings for the 15 items of the <kbd>test</kbd> data for each of the algorithms, as follows:</p>
<pre>
    <strong>&gt; user_pred &lt;- predict(ubcf, getData(e, "known"), type = "ratings")</strong><br/>    <br/>    <strong>&gt; item_pred &lt;- predict(ibcf, getData(e, "known"), type = "ratings")</strong><br/>    <br/>    <strong>&gt; svd_pred &lt;- predict(svd, getData(e, "known"), type = "ratings")</strong><br/>    <br/>    <strong>&gt; pop_pred &lt;- predict(popular, getData(e, "known"), type = <br/>       "ratings")</strong><br/>    <br/>    <strong>&gt; rand_pred &lt;- predict(random, getData(e, "known"), type = <br/>       "ratings")</strong>
</pre>
<p>We will examine the error between the predictions and unknown portion of the <kbd>test</kbd> data using the <kbd>calcPredictionAccuracy()</kbd> function. The output will consist of <kbd>RMSE</kbd>, <kbd>MSE</kbd>, and <kbd>MAE</kbd> for all the methods. We'll examine <kbd>UBCF</kbd> by itself. After creating the objects for all five methods, we can build a table by creating an object with the <kbd>rbind()</kbd> function and giving names to the rows with the <kbd>rownames()</kbd> function:</p>
<pre>
    <strong>&gt; P1 &lt;- calcPredictionAccuracy(user_pred, getData(e,</strong><br/>    <strong>"unknown"))</strong><br/>    <br/>    <strong>&gt; P1</strong><br/>    <strong>RMSE  MSE  MAE </strong><br/>    <strong>4.5 19.9  3.5</strong><br/>    <br/>    <strong>&gt; P2 &lt;- calcPredictionAccuracy(item_pred, getData(e, "unknown"))</strong><br/>    <br/>    <strong>&gt; P3 &lt;- calcPredictionAccuracy(svd_pred, getData(e, "unknown"))<br/></strong><br/>    <strong>&gt; P4 &lt;- calcPredictionAccuracy(pop_pred, getData(e, "unknown"))</strong><br/>    <br/>    <strong>&gt; P5 &lt;- calcPredictionAccuracy(rand_pred, getData(e, "unknown"))</strong><br/>    <br/>    <strong>&gt; error &lt;- rbind(P1, P2, P3, P4, P5)</strong><br/>    <br/>    <strong>&gt; rownames(error) &lt;- c("UBCF", "IBCF", "SVD", "Popular", "Random")</strong><br/>    <br/>    <strong>&gt; error</strong><br/>    <strong>        RMSE MSE  MAE</strong><br/><strong>    UBCF     4.5  20  3.5</strong><br/><strong>    IBCF     4.6  22  3.5</strong><br/><strong>    SVD      4.6  21  3.7</strong><br/><strong>    Popular  4.5  20  3.5</strong><br/><strong>    Random   6.3  40  4.9<br/></strong>
</pre>
<p>We can see in the output that the user-based and popular algorithms slightly outperform IBCF and SVD and all outperform random predictions.</p>
<p>There is another way to compare methods using the <kbd>evaluate()</kbd> function. Making comparisons with <kbd>evaluate()</kbd> allows one to examine additional performance metrics as well as performance graphs. As the UBCF and Popular algorithms performed the best, we will look at them along with IBCF.</p>
<p>The first task in this process is to create a list of the algorithms that we want to compare, as follows:</p>
<pre>
    <strong>&gt; algorithms &lt;- list(POPULAR = list(name = "POPULAR"),<br/>    UBCF =list(name = "UBCF"), IBCF = list(name = "IBCF"))</strong><br/>    <br/>    <strong>&gt; algorithms</strong><br/>    <strong>$POPULAR</strong><br/>    <strong>$POPULAR$name</strong><br/>    <strong>[1] "POPULAR"</strong><br/>    <br/>    <strong>$UBCF</strong><br/>    <strong>$UBCF$name</strong><br/>    <strong>[1] "UBCF"</strong><br/>    <br/>    <strong>$IBCF</strong><br/>    <strong>$IBCF$name</strong><br/>    <strong>[1] "IBCF"</strong>
</pre>
<p>For this example, let's compare the top <kbd>5</kbd>, <kbd>10</kbd>, and <kbd>15</kbd> joke recommendations:</p>
<pre>
    <strong>&gt; evlist &lt;- evaluate(e, algorithms, n = c(5, 10, 15))</strong><br/>    <strong>POPULAR run </strong><br/>    <strong> 1  [0.07sec/4.7sec] </strong><br/>    <strong>UBCF run </strong><br/>    <strong> 1  [0.04sec/8.9sec] </strong><br/>    <strong>IBCF run </strong><br/>    <strong> 1  [0.45sec/0.32sec]3</strong>
</pre>
<p>Note that by executing the command, you will receive an output on how long it took to run the algorithm. We can now examine the performance using the <kbd>avg()</kbd> function:</p>
<pre>
<strong>    &gt; set.seed(1)</strong>    <br/><br/><strong>    &gt; avg(evlist)</strong><br/>   <strong> $POPULAR</strong><br/><strong>         TP    FP    FN    TN   precision  recall   TPR    FPR</strong><br/><strong>    5  2.07  2.93  12.9  67.1       0.414   0.182 0.182 0.0398</strong><br/><strong>    10 3.92  6.08  11.1  63.9       0.393   0.331 0.331 0.0828</strong><br/><strong>    15 5.40  9.60   9.6  60.4       0.360   0.433 0.433 0.1314</strong><br/><br/><strong>    $UBCF</strong><br/><strong>          TP    FP    FN    TN   precision   recall   TPR    FPR</strong><br/><strong>    5   2.07  2.93  12.93  67.1      0.414    0.179 0.179 0.0398</strong><br/><strong>    10  3.88  6.12  11.11  63.9      0.389    0.326 0.326 0.0835</strong><br/><strong>    15  5.41  9.59   9.59  60.4      0.360    0.427 0.427 0.1312</strong><br/><br/><strong>    $IBCF</strong><br/><strong>          TP    FP    FN    TN    precision   recall    TPR   FPR</strong><br/><strong>    5   1.02  3.98  14.0  66.0        0.205   0.0674 0.0674 0.0558</strong><br/><strong>    10  2.35  7.65  12.6  62.4        0.235   0.1606 0.1606 0.1069</strong><br/><strong>    15  3.72 11.28  11.3  58.7        0.248   0.2617 0.2617 0.1575<br/></strong>
</pre>
<p>Note that the performance metrics for <kbd>POPULAR</kbd> and <kbd>UBCF</kbd> are nearly the same. One could say that the simpler-to-implement popular-based algorithm is probably the better choice for a model selection.  We can plot and compare the results as <strong>Receiver Operating Characteristic Curves</strong> (<strong>ROC</strong>), comparing <kbd>TPR</kbd> and <kbd>FPR</kbd> or precision/recall, as follows:</p>
<pre>
    <strong>&gt; plot(evlist, legend = "topleft", annotate = TRUE)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="245" width="387" class="image-border" src="assets/image_10_08.png"/></div>
<p>To get the precision/recall curve plot you only need to specify <kbd>"prec"</kbd> in the <kbd>plot</kbd> function:</p>
<pre>
    <strong>&gt; plot(evlist, "prec", legend = "bottomright", annotate = TRUE)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="255" width="402" class="image-border" src="assets/image_10_09.png"/></div>
<p>You can clearly see in the plots that the popular-based and user-based algorithms are almost identical and outperform the item-based one. The <kbd>annotate=TRUE</kbd> parameter provides numbers next to the point that corresponds to the number of recommendations that we called for in our evaluation.</p>
<p>This was simple, but what are the actual recommendations from a model for a specific individual? This is quite easy to code as well. First, let's build a <kbd>"popular"</kbd> recommendation engine on the full dataset. Then, we will find the top five recommendations for the first two raters. We will use the <kbd>Recommend()</kbd> function and apply it to the whole dataset, as follows:</p>
<pre>
    <strong>&gt; R1 &lt;- Recommender(Jester5k, method = "POPULAR")</strong><br/>    <br/>    <strong>&gt; R1</strong><br/>    <strong>Recommender of type 'POPULAR' for 'realRatingMatrix' </strong><br/>    <strong>learned using 5000 users.</strong>
</pre>
<p>Now, we just need to get the top five recommendations for the first two raters and produce them as a list:</p>
<pre>
    <strong>&gt; recommend &lt;- predict(R1, Jester5k[1:2], n = 5)</strong><br/>    <br/>    <strong>&gt; as(recommend, "list")</strong><br/>    <strong>$u2841</strong><br/><strong>    [1] "j89" "j72" "j76" "j88" "j83"</strong><br/><br/><strong>    $u15547</strong><br/><strong>    [1] "j89" "j93" "j76" "j88" "j91"<br/></strong>
</pre>
<p>It is also possible to see a rater's specific rating score for each of the jokes by specifying this in the <kbd>predict()</kbd> syntax and then putting it in a matrix for review. Let's do this for ten individuals (raters <kbd>300</kbd> through <kbd>309</kbd>) and three jokes (<kbd>71</kbd> through <kbd>73</kbd>):</p>
<pre>
    <strong>&gt; rating &lt;- predict(R1, Jester5k[300:309], type = "ratings")</strong><br/>    <br/>    <strong>&gt; rating</strong><br/>    <strong>10 x 100 rating matrix of class 'realRatingMatrix' with 322</strong><br/>    <strong>ratings.</strong><br/>    <br/>    <strong>&gt; as(rating, "matrix")[, 71:73]</strong><br/>   <strong>           j71  j72     j73</strong><br/><strong>    u7628  -2.042 1.50 -0.2911</strong><br/><strong>    u8714      NA   NA      NA</strong><br/><strong>    u24213 -2.935   NA -1.1837</strong><br/><strong>    u13301  2.391 5.93  4.1419</strong><br/><strong>    u10959     NA   NA      NA</strong><br/><strong>    u23430 -0.432 3.11      NA</strong><br/><strong>    u11167 -1.718 1.82  0.0333</strong><br/><strong>    u4705  -1.199 2.34  0.5519</strong><br/><strong>    u24469 -1.583 1.96  0.1686</strong><br/><strong>    u13534 -1.545 2.00      NA<br/></strong>
</pre>
<p>The numbers in the matrix indicate the predicted rating scores for the jokes that the individual rated, while the NAs indicate those that the user did not rate.</p>
<p>Our final effort on this data will show how to build recommendations for those situations where the ratings are binary, that is, good or bad or 1 or 0. We will need to turn the ratings into this binary format with 5 or greater as a 1 and less than 5 as 0. This is quite easy to do with <kbd>Recommenderlab</kbd> using the <kbd>binarize()</kbd> function and specifying <kbd>minRating=5</kbd>:</p>
<pre>
    <strong>&gt; Jester.bin &lt;- binarize(Jester5k, minRating = 5)</strong>
</pre>
<p>Now, we will need to have our data reflect the number of ratings equal to one in order to match what we need the algorithm to use for the training. For argument's sake, let's go with greater than 10. The code to create the subset of the necessary data is shown in the following lines:</p>
<pre>
    <strong>&gt; Jester.bin &lt;- Jester.bin[rowCounts(Jester.bin) &gt; 10]</strong><br/>    <br/>    <strong>&gt; Jester.bin</strong><br/>    <strong>3054 x 100 rating matrix of class 'binaryRatingMatrix' with 84722 <br/>      ratings.</strong>
</pre>
<p>You will need to create <kbd>evaluationScheme</kbd>. In this instance, we will go with <kbd>cross-validation</kbd>. The default k-fold in the function is <kbd>10</kbd>, but we can also safely go with <kbd>k=5</kbd>, which will reduce our computation time:</p>
<pre>
    <strong>&gt; set.seed(456)</strong><br/>    <br/>    <strong>&gt; e.bin &lt;- evaluationScheme(Jester.bin, method = "cross-<br/>      validation", k = 5, given = 10)</strong>
</pre>
<p>For comparison purposes, the algorithms under evaluation will include <kbd>random</kbd>, <kbd>popular</kbd>, and <kbd>UBCF</kbd>:</p>
<pre>
    <strong>&gt; algorithms.bin &lt;- list("random" = list(name = "RANDOM", param = <br/>      NULL), "popular" = list(name = "POPULAR", param = NULL), "UBCF" = <br/>        list(name = "UBCF"))</strong>
</pre>
<p>It is now time to build our model, as follows:</p>
<pre>
    <strong>&gt; results.bin &lt;- evaluate(e.bin, algorithms.bin, n = c(5, 10, 15))</strong><br/>    <strong>RANDOM run </strong><br/>    <strong> 1  [0sec/0.41sec] </strong><br/>    <strong> 2  [0.01sec/0.39sec] </strong><br/>    <strong> 3  [0sec/0.39sec] </strong><br/>    <strong> 4  [0sec/0.41sec] </strong><br/>    <strong> 5  [0sec/0.4sec] </strong><br/>    <strong>POPULAR run </strong><br/>    <strong> 1  [0.01sec/3.79sec] </strong><br/>    <strong> 2  [0sec/3.81sec] </strong><br/>    <strong> 3  [0sec/3.82sec] </strong><br/>    <strong> 4  [0sec/3.92sec] </strong><br/>    <strong> 5  [0.02sec/3.78sec] </strong><br/>    <strong>UBCF run </strong><br/>    <strong> 1  [0sec/5.94sec] </strong><br/>    <strong> 2  [0sec/5.92sec] </strong><br/>    <strong> 3  [0sec/6.05sec] </strong><br/>    <strong> 4  [0sec/5.86sec] </strong><br/>    <strong> 5  [0sec/6.09sec]</strong>
</pre>
<p>Forgoing the table of performance metrics, let's take a look at the plots:</p>
<pre>
    <strong>&gt; plot(results.bin, legend = "topleft")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="239" width="402" class="image-border" src="assets/image_10_10.png"/></div>
<pre>
    <strong>&gt; plot(results.bin, "prec", legend = "bottomright")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="254" width="425" class="image-border" src="assets/image_10_11.png"/></div>
<p>The user-based algorithm slightly outperforms the popular-based one, but you can clearly see that they are both superior to any random recommendation. In our business case, it will come down to the judgment of the decision-making team as to which algorithm to implement.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Sequential data analysis</h1>
            </header>

            <article>
                
<div class="packt_quote"><span>There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don't know. But there are also unknown unknowns. There are things we don't know we don't know.<br/>
                                                          - Donald Rumsfeld, Former Secretary of Defense</span></div>
<p>The very first business question I came across after the 1st edition was published revolved around product sequential analysis. The team worked on complicated Excel spreadsheets and pivot tables, along with a bunch of SAS code, to produce insights. After coming across this problem, I explored what could be done with R and was pleasantly surprised to stumble into the <kbd>TraMineR</kbd> package, specifically designed for just such a task. I believe the application of R to the problem would have greatly simplified the analysis.</p>
<p>The package was designed for the social sciences, but it can be used in just about every situation where you want to mine and learn how observation's states evolve over discrete periods or events (longitudinal data). A classic use would be as in the case mentioned above where you want to understand the order in which customers purchase products. This would facilitate a recommendation engine of sorts where you can create the probability of the next purchase, as I've heard it being referred to as a next logical product offer. Another example could be in healthcare, examining the order that a patient receives treatments and/or medications, or even physician prescribing habits. I've worked on such tasks, creating simple and complex Markov chains to build models and create forecasts. Indeed, <kbd>TraMineR</kbd> allows the creation of Markov chain transition matrices to support such models.  </p>
<p>The code we will examine does the hard work of creating, counting, and plotting the various combinations of transitions over time, also incorporating covariates. That will be our focus, but keep in mind that one can also build a dissimilarity matrix for clustering. The core features covered in the practical exercise will consist of the following:</p>
<ul>
<li>Transition rates</li>
<li>duration within each state</li>
<li>Sequence frequency</li>
</ul>
<p>Let's get started.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Sequential analysis applied</h1>
            </header>

            <article>
                
<p>For this exercise, I've created an artificial dataset; to follow along, you can download it from GitHub:</p>
<p><a href="https://github.com/datameister66/data/blob/master/sequential.csv">https://github.com/datameister66/data/blob/master/sequential.csv</a></p>
<p>There are also datasets available with the package and tutorials are available. My intent was to create something new that mirrored situations I have encountered. I developed it completely from random (with some supervision), so it does not match any real world data. It consists of 5,000 observations, with each observation, the history of a customer and nine variables:</p>
<ul>
<li>Cust_segment--a factor variable indicating the customer's assigned segment (see <a href="f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml" target="_blank">Chapter 8</a>, <em>Cluster Analysis</em>)</li>
</ul>
<ul>
<li>Eight discrete purchase events named <kbd>Purchase1</kbd> through <kbd>Purchase8</kbd>; remember, these are events and not time-based, which is to say that a customer could have purchased all eight products at the same time, but in a specific order</li>
</ul>
<p>Within each purchase variable are the generic names of the product, seven possible products to be exact. They are named <kbd>Product_A</kbd> through <kbd>Product_G</kbd>.  What are these products? Doesn't matter! Use your imagination or apply it to your own situation. If a customer only purchased one product, then <kbd>Purchase1</kbd> would contain the name of that product and the other variables would be NULL.  </p>
<p>Here we load the file as a dataframe. The structure output is abbreviated for clarity:</p>
<pre>
<strong>    &gt; df &lt;- read.csv("sequential.csv")<br/></strong><br/><strong>    &gt; str(df)</strong><br/><strong>    'data.frame': 5000 obs. of 9 variables:</strong><br/><strong>    $ Cust_Segment: Factor w/ 4 levels "Segment1","Segment2",..: 1 1 1 <br/>    1 1 1 1 1 1 1 ...</strong><br/><strong>    $ Purchase1 : Factor w/ 7 levels "Product_A","Product_B",..: 1 2 7 <br/>    3 1 4 1 4 4 4 ...</strong>
</pre>
<p>Time for some exploration of the data, starting with a table of the customer segment counts and a count of the first product purchased:</p>
<pre>
<strong>    &gt; table(df$Cust_Segment)</strong><br/><br/><strong>    Segment1 Segment2 Segment3 Segment4 </strong><br/><strong>        2900      572      554      974</strong> <br/><br/><strong>    &gt; table(df$Purchase1)</strong><br/><br/><strong>    Product_A Product_B Product_C Product_D Product_E Product_F <br/>    Product_G </strong><br/><strong>         1451       765       659      1060       364       372       <br/>    329</strong>
</pre>
<p><kbd>Segment1</kbd> is the largest segment, and the most purchased initial product is <kbd>Product A</kbd>. However, is it the most purchased product overall? This code will provide the answer:</p>
<pre>
<strong>    &gt; table(unlist(df[, -1]))</strong><br/><br/><strong>    Product_A Product_B Product_C Product_D Product_E Product_F <br/>    Product_G </strong><br/><strong>    3855      3193      3564      3122      1688      1273   915   <br/>    22390</strong>
</pre>
<p>Yes, <kbd>ProductA</kbd> is the most purchased. The count of NULL values is 22,390.</p>
<p>Now you may be wondering if we can just build some summaries without much trouble, and that is surely the case. Here, I put the <kbd>count()</kbd> and <kbd>arrange()</kbd> functions from the <kbd>dplyr</kbd> package to good use to examine the frequency of sequences between the first and second purchase:</p>
<pre>
<strong>    &gt; dfCount &lt;- count(df, Purchase1, Purchase2)</strong><br/><br/><strong>    &gt; dfCount &lt;- arrange(dfCount, desc(n))</strong><br/><br/><strong>    &gt; dim(dfCount)</strong><br/><strong>    [1] 56 3</strong><br/><strong><br/>    &gt; head(dfCount)</strong><br/><strong>    Source: local data frame [6 x 3]</strong><br/><strong>    Groups: Purchase1 [4]</strong><br/><br/><strong>      Purchase1 Purchase2     n</strong><br/><strong>         &lt;fctr&gt;    &lt;fctr&gt; &lt;int&gt;</strong><br/><strong>    1 Product_A Product_A   548</strong><br/><strong>    2 Product_D             548</strong><br/><strong>    3 Product_B             346</strong><br/><strong>    4 Product_C Product_C   345</strong><br/><strong>    5 Product_B Product_B   291</strong><br/><strong>    6 Product_D Product_D   281</strong>
</pre>
<p>We see that the most frequent sequences are the purchase of <kbd>ProductA</kbd> followed by another purchase of <kbd>ProductA</kbd>, along with the purchase of <kbd>ProductD</kbd> followed by no additional purchases. What is interesting is the frequency of similar product purchases.</p>
<p>We can now begin further examination using the <kbd>TraMineR</kbd> package. To begin, the data needs to be put into an object of class sequence with the <kbd>seqdef()</kbd> function. This should consist of only the sequences and not any covariates. Also, you can specify the distance of tick marks in plotting functions with <kbd>xstep = n</kbd>. In our case, we will have a tick mark for every event:</p>
<pre>
<strong>    &gt; seq &lt;- seqdef(df[, -1], xtstep = 1)</strong><br/><br/><strong>    &gt; head(seq)</strong><br/><strong>      Sequence </strong><br/><strong>    1 Product_A-Product_A------ </strong><br/><strong>    2 Product_B------- </strong><br/><strong>    3 Product_G-Product_B-Product_B-Product_C-Product_B-Product_B-<br/>    Product_B- <br/>      Product_G</strong><br/><strong>    4 Product_C------- </strong><br/><strong>    5 Product_A------- </strong><br/><strong>    6 Product_D-------</strong>
</pre>
<p>We can now explore the data further. Let's look at the index plot, which produces the sequences of the first 10 observations. You can use indices with the data to examine as many observations and event periods as you wish:</p>
<pre>
<strong>    &gt; seqiplot(seq)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="211" width="428" class="image-border" src="assets/image_10_12.png"/></div>
<p>One can plot all observations with <kbd>seqIplot()</kbd>, but given the size of the data, it doesn't produce anything meaningful. A plot of distribution by state is more meaningful:</p>
<pre>
<strong>   &gt; seqdplot(seq)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="222" width="416" class="image-border" src="assets/image_10_13.png"/></div>
<p>With this plot, it is easy to see the distribution of product purchases by state. We can also group this plot by segments and determine whether there are differences:</p>
<pre>
<strong>    &gt; seqdplot(seq, group = df$Cust_Segment)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="250" width="467" class="image-border" src="assets/image_10_14.png"/></div>
<p>Here, we clearly see that <kbd>Segment2</kbd> has a higher proportion of <kbd>ProductA</kbd> purchases than the other segments. Another way to see that insight is with the modal plot:</p>
<pre>
<strong>    &gt; seqmsplot(seq, group = df$Cust_Segment)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="518" class="image-border" src="assets/image_10_15.png"/></div>
<p>This is interesting. Around 50% of <kbd>Segment2</kbd> purchased <kbd>ProductA</kbd> first, while segment 4's most frequent initial purchase was <kbd>ProductD</kbd>. Another plot that may be of interest, but I believe not in this case, is the mean time plot. It plots the average "time" spent in each state. Since we are not time-based, it doesn't make sense, but I include for your consideration:</p>
<pre>
<strong>    &gt; seqmtplot(seq, group = df$Cust_Segment)</strong>
</pre>
<p>Let's supplement our preceding code and look further at the transition of sequences. This code creates an object of sequences, then narrows that down to those sequences with an occurrence of at least 5%, then plots the top 10 sequences:</p>
<pre>
<strong>    &gt; seqE &lt;- seqecreate(seq)</strong><br/><br/><strong>    &gt; subSeq &lt;- seqefsub(seqE, pMinSupport = 0.05)</strong><br/><br/><strong>    &gt; plot(subSeq[1:10], col = "dodgerblue")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="198" width="371" class="image-border" src="assets/image_10_17.png"/></div>
<p>Note that the plot shows the percentage frequency of the sequences through the eight transition states. If you want to narrow that down to, say, the first two transitions, you would do that in the <kbd>seqecreate()</kbd> function using indices.</p>
<p>Finally, let's see how you can use the data to create a transition matrix. This matrix shows the probability of transitioning from one state to the next. In our case, it provides the probability of purchasing the next product. As I  mentioned before, this can also be used in a Markov chain simulation to develop a forecast. That is outside the scope of this chapter, but if you are interested I recommend having a look at the <kbd>markovchain</kbd> package in R and its tutorial on how to implement the procedure. Two possible transition matrices are available. One that incorporates the overall probability through all states and another that develops a transition matrix from one state to the next, that is, time-varying matrices. This code shows how to develop the former. To produce the latter, just specify <kbd>"time.varying = TRUE"</kbd> in the function:</p>
<pre>
<strong>    &gt; seqMat &lt;- seqtrate(seq)</strong><br/><strong>     [&gt;] computing transition rates for states<br/>  <br/>     /Product_A/Product_B/Product_C/Product_D/<br/>       Product_E/Product_F/Product_G ...</strong><br/><br/><strong>    &gt; options(digits = 2) # make output easier to read</strong><br/><br/><strong>    &gt; seqMat[2:4, 1:3]</strong><br/><strong>                   [-&gt; ] [-&gt; Product_A] [-&gt; Product_B]</strong><br/><strong>    [Product_A -&gt;]  0.19          0.417          0.166</strong><br/><strong>    [Product_B -&gt;]  0.26          0.113          0.475</strong><br/><strong>    [Product_C -&gt;]  0.19          0.058          0.041</strong>
</pre>
<p>The output shows rows 2 through 4 and columns 1 through 3. The matrix shows us that the probability of having Product A and the next purchase being <kbd>ProductA</kbd> is almost 42%, while it is 19% to not purchase another product, and 17% to purchase <kbd>ProductB</kbd>. The final output we will examine is the probability of not purchasing another product for each prior purchase:</p>
<pre>
<strong>    &gt; seqMat[, 1]</strong><strong> [ -&gt;] [Product_A -&gt;] [Product_B -&gt;] [Product_C -&gt;] <br/>      [Product_D -&gt;] </strong><br/><strong>     1.00           0.19           0.26           0.19           0.33 </strong><br/><strong>    [Product_E -&gt;] [Product_F -&gt;] [Product_G -&gt;] </strong><br/><strong>              0.18           0.25           0.41</strong>
</pre>
<p>Of course, the matrix shows that the probability of not purchasing another product after not purchasing is 100%. Also notice that the probability of not purchasing after acquiring Product D is 33%. Implications for Segment4? Perhaps.</p>
<p>What is fascinating is that this analysis was done with only a few lines of code and didn't require the use of Excel or some expensive visualization software. Have longitudinal data? Give sequential analysis a try!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, the goal was to provide an introduction to how to use R in order to build and test association rule mining (market basket analysis) and recommendation engines. Market basket analysis is trying to understand what items are purchased together. With recommendation engines, the goal is to provide a customer with other items that they will enjoy based on how they have rated previously viewed or purchased items. It is important to understand the R package that we used (<kbd>recommenderlab</kbd>) for recommendation is not designed for implementation, but to develop and test algorithms. The other thing examined here was longitudinal data and mining it to learn valuable insights, in our case, the order in which customers purchased our products. Such an analysis has numerous applications, from marketing campaigns to healthcare.</p>
<p>We are now going to shift gears back to supervised learning. In the next chapter, we are going to cover some of the most exciting and important methods in practical machine learning, that is multi-class classification and creating ensemble models, something that is very easy to do in R with recent package releases.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>