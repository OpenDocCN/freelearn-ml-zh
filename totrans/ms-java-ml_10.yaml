- en: Appendix A. Linear Algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear algebra is of primary importance in machine learning and it gives us
    an array of tools that are especially handy for the purpose of manipulating data
    and extracting patterns from it. Moreover, when data must be processed in batches
    as in much machine learning, great runtime efficiencies are gained from using
    the "vectorized" form as an alternative to traditional looping constructs when
    implementing software solutions in optimization or data pre-processing or any
    number of operations in analytics.
  prefs: []
  type: TYPE_NORMAL
- en: We will consider only the domain of real numbers in what follows. Thus, a vector
    ![Linear Algebra](img/B05137_10_image001.jpg) represents an array of *n* real-valued
    numbers. A matrix ![Linear Algebra](img/B05137_10_image004.jpg) is a two-dimensional
    array of *m* rows and *n* columns of real-valued numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Some key concepts from the foundation of linear algebra are presented here.
  prefs: []
  type: TYPE_NORMAL
- en: Vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The vector **x** (lowercase, bold, by convention; equivalently, ![Vector](img/B05137_10_image131.jpg))
    can be thought of as a point in *n*-dimensional space. Conventionally, we mean
    column-vector when we say vector. The *transpose* of a column vector is a *row*
    vector with the same number of elements, arranged in a single row.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vector](img/B05137_10_image005.jpg)![Vector](img/B05137_10_image006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Scalar product of vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Also known as the dot product, the scalar product is defined for two vectors
    of equal length. The result of the operation is a scalar value and is obtained
    by summing over the products of the corresponding elements of the vectors. Thus,
    given vectors **x** and **y**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scalar product of vectors](img/B05137_10_image132.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The dot product **x**T**y** is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scalar product of vectors](img/B05137_10_image133.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A matrix is a two-dimensional array of numbers. Each element can be indexed
    by its row and column position. Thus, a 3 x 2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix](img/B05137_10_image008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transpose of a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Swapping columns for rows in a matrix produces the transpose. Thus, the transpose
    of **A** is a 2 x 3 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transpose of a matrix](img/B05137_10_image010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Matrix addition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Matrix addition is defined as element-wise summation of two matrices with the
    same shape. Let **A** and **B** be two *m* x *n* matrices. Their sum **C** can
    be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**C**i,j = **A**i,j + **B**i,j'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multiplication with a scalar produces a matrix where each element is scaled
    by the scalar value. Here **A** is multiplied by the scalar value *d*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scalar multiplication](img/B05137_10_image015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Two matrices **A** and **B** can be multiplied if the number of columns of
    **A** equals the number of rows of **B**. If **A** has dimensions *m* x *n* and
    **B** has dimensions *n* x *p*, then the product **AB** has dimensions *m* x *p*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix multiplication](img/B05137_10_image019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Properties of matrix product
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Distributivity over addition: A(B + C) = AB + AC'
  prefs: []
  type: TYPE_NORMAL
- en: 'Associativity: A(BC) = (AB)C'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-commutativity: AB ≠ BA'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector dot-product is commutative: **x**T**y** = **y**T**x**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transpose of product is product of transposes: (**AB**)T = **A**T**B**T'
  prefs: []
  type: TYPE_NORMAL
- en: Linear transformation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There is a special importance to the product of a matrix and a vector in linear
    algebra. Consider the product of a 3 x 2 matrix **A** and a 2 x 1 vector **x**
    producing a 3 x 1 vector *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear transformation](img/B05137_10_image025.jpg)![Linear transformation](img/B05137_10_image026.jpg)![Linear
    transformation](img/B05137_10_image027.jpg)![Linear transformation](img/B05137_10_image028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (C)
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear transformation](img/B05137_10_image029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (R)
  prefs: []
  type: TYPE_NORMAL
- en: It is useful to consider two views of the preceding matrix-vector product, namely,
    the column picture (**C**) and the row picture (**R**). In the column picture,
    the product can be seen as a linear combination of the column vectors of the matrix,
    whereas the row picture can be thought of as the dot products of the rows of the
    matrix with the vector ![Linear transformation](img/B05137_10_image030.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Matrix inverse
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The product of a matrix with its inverse is the Identity matrix. Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix inverse](img/B05137_10_image031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix inverse, if it exists, can be used to solve a system of simultaneous
    equations represented by the preceding vector-matrix product equation. Consider
    a system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*1 + 2*x*2 = 3'
  prefs: []
  type: TYPE_NORMAL
- en: 3*x*1 + 9*x*2 = 21
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be expressed as an equation involving the matrix-vector product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix inverse](img/B05137_10_image034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve for the variables *x*1 and *x*2 by multiplying both sides by the
    matrix inverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix inverse](img/B05137_10_image035.jpg)![Matrix inverse](img/B05137_10_image036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix inverse can be calculated by different methods. The reader is advised
    to view Prof. Strang''s MIT lecture: [bit.ly/10vmKcL](http://bit.ly/10vmKcL).'
  prefs: []
  type: TYPE_NORMAL
- en: Eigendecomposition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Matrices can be decomposed to factors that can give us valuable insight into
    the transformation that the matrix represents. Eigenvalues and eigenvectors are
    obtained as the result of eigendecomposition. For a given square matrix **A**,
    an eigenvector is a non-zero vector that is transformed into a scaled version
    of itself when multiplied by the matrix. The scalar multiplier is the eigenvalue.
    All scalar multiples of an eigenvector are also eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A** **v** = *λ* **v**'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, **v** is an eigenvector and λ is the eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eigenvalue equation of matrix **A** is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: (**A** — *λ* **I**)**v** = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'The non-zero solution for the eigenvalues is given by the roots of the characteristic
    polynomial equation of degree *n* represented by the determinant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigendecomposition](img/B05137_10_image041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The eigenvectors can then be found by solving for *v* in **Av** = *λ* **v**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some matrices, called diagonalizable matrices, can be built entirely from their
    eigenvectors and eigenvalues. If **Λ** is the diagonal matrix with the eigenvalues
    of matrix A on its principal diagonal, and **Q** is the matrix whose columns are
    the eigenvectors of **A**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigendecomposition](img/B05137_10_image043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then **A = Q Λ Q**-1.
  prefs: []
  type: TYPE_NORMAL
- en: Positive definite matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If a matrix has only positive eigenvalues, it is called a **positive definite
    matrix**. If the eigenvalues are positive or zero, the matrix is called a **positive
    semi-definite matrix**. With positive definite matrices, it is true that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x**T**Ax** *≥* 0'
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition (SVD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SVD is a decomposition of any rectangular matrix **A** of dimensions *n* x
    *p* and is written as the product of three matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition (SVD)](img/B05137_10_image140.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**U** is defined to be *n* x *n*, **S** is a diagonal *n* x *p* matrix, and
    **V** is *p* x *p*. **U** and **V** are orthogonal matrices; that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Singular value decomposition (SVD)](img/B05137_10_image141.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The diagonal values of **S** are called the singular values of **A**. Columns
    of **U** are called left singular vectors of **A** and those of **V** are called
    right singular vectors of **A**. The left singular vectors are orthonormal eigenvectors
    of **A**T**A** and the right singular vectors are orthonormal eigenvectors of
    **AA**T.
  prefs: []
  type: TYPE_NORMAL
- en: The SVD representation expands the original data into a coordinate system such
    that the covariance matrix is a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
