<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 5. Sentiment Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Sentiment Analysis</h1></div></div></div><p>So far, we have explored some really cool applications in the analytics domain. In this chapter, we will explore the famous Natural Language Processing (NLP) technique, which you may have already guessed because of the name of the chapter. Absolutely right; we will build a sentiment analysis-based application. In general, everyone is familiar with sentiment analysis-based applications. If you aren't, then don't worry. We will discuss and understand all the necessary details.</p><p>First of all, I want to give you a basic idea about sentiment analysis. I will provide an example so it will be easy for you to understand. Regardless of where we live, we all watch movies. Nowadays, we read reviews or others' opinions on various social media platforms. After that, if a majority of the opinions about the movie are good, then we watch that movie. If the opinions are not impressive, we might not watch the movie. So during this entire process, our mind analyzes these opinions and categorizes them into either positive opinions, negative opinions, or neutral opinions. In this chapter, we will be performing the same kind of analysis.</p><p>Let me introduce the formal definition of sentiment analysis. Sentiment analysis is a technique where we consider a sentence, paragraph, document, or any information that is in the form of a natural language and determine whether that text's emotional tone is positive, negative, or neutral. We will be applying machine learning and Deep Learning to build a sentiment analysis application.</p><p>We will be covering the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement </li><li class="listitem" style="list-style-type: disc">Understanding the dataset </li><li class="listitem" style="list-style-type: disc">Building training and testing datasets for the baseline model</li><li class="listitem" style="list-style-type: disc">Feature engineering for the baseline model</li><li class="listitem" style="list-style-type: disc">Selecting the Machine Learning (ML) algorithm   </li><li class="listitem" style="list-style-type: disc">Training the baseline model </li><li class="listitem" style="list-style-type: disc">Understanding the testing matrix </li><li class="listitem" style="list-style-type: disc">Testing the baseline model </li><li class="listitem" style="list-style-type: disc">Problems with the existing approach </li><li class="listitem" style="list-style-type: disc">How to optimize the existing approach <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding key concepts for optimizing the approach  </li></ul></div></li><li class="listitem" style="list-style-type: disc">Implementing the revised approach <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Testing the revised approach </li><li class="listitem" style="list-style-type: disc">Understanding problems with the revised approach </li></ul></div></li><li class="listitem" style="list-style-type: disc">Best approach <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the best approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><div class="section" title="Introducing problem statements"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec49"/>Introducing problem statements</h1></div></div></div><p>We are living in a competitive world. Before buying any product and investing our time or money in anything, we try to find out what others think about that product(s) or service(s). We try to analyze their reviews or opinions. If we find them positive and trustworthy, then we buy the product and invest our money or time in that particular service. On the other hand, if we find these opinions or reviews negative, then we might not buy the product and not invest our money or time in that particular service. In the current era of the internet, it is easy to find reviews on social media platforms, blogs, news sources, and so on. This activity of analyzing reviews will be useful for consumers as well as makers of products or service providers. This is because, based on the reviews of their customer, they can change their product effectively, providing more satisfaction to their customers and make a good profit from that product or service. I have already given you the formal definition of sentiment analysis, so I'm not going to bore you with it again. Let's try to understand what the main focus of this chapter will be.</p><p>We will be developing a sentiment analysis application for movie reviews. During training, we will consider labels associated with each of the movie reviews so that we can train our machine learning algorithm based on the given labels. After training, when we pass any unseen movie reviews, then our trained machine learning algorithm will predict the sentiment, which means whether the provided movie review indicates a positive sentiment or a negative sentiment.</p><p>We will be considering an IMDb (Internet Movie Database) movie review dataset to develop sentiment analysis for movie reviews. We will look at the details regarding the dataset in the next section.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec50"/>Understanding the dataset</h1></div></div></div><p>In this section, we will <a id="id569" class="indexterm"/>look into our dataset. We have considered an IMDb dataset, which you can download at: <a class="ulink" href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a>. After clicking on this link, you can <a id="id570" class="indexterm"/>see that there is a link provided on the page. This link is titled Large Movie Review Dataset v1.0; we need to click on it. This way, we can download the IMDb dataset. Once you have downloaded the dataset, you need to extract the .tar.gz file. Once you extract the <code class="literal">.tar.gz</code> file, you can see that there are two folders inside the extracted folder and some other files. Let's look at each of them in the following section.</p><div class="section" title="Understanding the content of the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec74"/>Understanding the content of the dataset</h2></div></div></div><p>After extracting <a id="id571" class="indexterm"/>the dataset file, we'll see that there are some folders and files inside it. We will be discussing all of the content's meaning and what we will be using for our training purposes. This dataset has two folders and three files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">train folder</li><li class="listitem" style="list-style-type: disc">test folder</li><li class="listitem" style="list-style-type: disc"><code class="literal">imdb.vocab</code> file</li><li class="listitem" style="list-style-type: disc"><code class="literal">imdbEr.txt</code></li><li class="listitem" style="list-style-type: disc">README</li></ul></div><div class="section" title="Train folder"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec57"/>Train folder</h3></div></div></div><p>This folder <a id="id572" class="indexterm"/>contains data for training. Inside this folder, there are two main folders. The <code class="literal">pos</code> folder contains positive movie reviews and the <code class="literal">neg</code> folder contains negative movie reviews. Inside the <code class="literal">pos</code> folder, there are 12,500 positive movie reviews. Inside the <code class="literal">neg</code> folder, there are 12,500 negative movie reviews. So in total, we have 25,000 movie reviews; by using them, we will train our Machine Learning  (ML) model. For testing purposes, we can use movie reviews provided inside the <code class="literal">unsup</code> folder. These movie reviews are unlabeled, so we can use them for  testing purposes or divide our labeled data into training and testing groups so it will be easy for us to find out how our trained ML model works.</p><p>There are other files inside the train folder but we are not going to use them. Those files carry data that already has tokenized bag-of-word (BOW) features. In order to get a clear picture of the folder structure, you can refer to the code snippet provided in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_01.jpg" alt="Train folder" width="582" height="300"/><div class="caption"><p>Figure 5.1: Folder structure of the train folder</p></div></div><p>If<a id="id573" class="indexterm"/> you want to explore the dataset in more detail, then you can refer to the documentation provided at: <a class="ulink" href="http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html">http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html</a>.</p></div><div class="section" title="Test folder"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec58"/>Test folder</h3></div></div></div><p>This folder <a id="id574" class="indexterm"/>contains data for testing. Inside this folder, there are <code class="literal">pos</code> and <code class="literal">neg</code> folders, which contain positive and negative movie reviews, respectively. Each of the folders contains 12,500 movie reviews, so in total, we have 25,000 movie reviews for testing. These movie reviews are labeled one so we can use this dataset to test our trained model. There are other <code class="literal">BOW</code> files and <code class="literal">url</code> files, which we will not be using. You can see the folder structure of the test folder in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_02.jpg" alt="Test folder" width="574" height="238"/><div class="caption"><p>Figure 5.2: Folder structure of the test folder</p></div></div></div><div class="section" title="imdb.vocab file"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec59"/>imdb.vocab file</h3></div></div></div><p>This file <a id="id575" class="indexterm"/>contains the unique words used in all movie reviews, so it is the vocabulary file for the <code class="literal">IMDb</code> dataset. If you open this file, then you can see words and observe that all of them are unique. You can see the contents of this file in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_03.jpg" alt="imdb.vocab file" width="219" height="416"/><div class="caption"><p>Figure 5.3: Contents of the imdb.vocab file</p></div></div></div><div class="section" title="imdbEr.txt file"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec60"/>imdbEr.txt file</h3></div></div></div><p>This file <a id="id576" class="indexterm"/>indicates the expected rating for each token in the <code class="literal">imdb.vocab</code> file. This means that all these numerical values indicate the score for each individual word provided in the <code class="literal">imdb.vocab</code> file. If the word is positive, then the numerical value is a positive float number. If the word is negative, then the numerical value is a negative float value. You can see the contents of the file in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_04.jpg" alt="imdbEr.txt file" width="227" height="417"/><div class="caption"><p>Figure 5.4: The imdbEr.txt file, which has a score for each of the words given in the imdb.vocab file</p></div></div></div><div class="section" title="README"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec61"/>README</h3></div></div></div><p>This file <a id="id577" class="indexterm"/>contains the documentation regarding the dataset. You can get a hold of the basic information using this file.</p><p>Note that for developing this sentiment analysis application, we will consider data from only the <code class="literal">train</code> folder because processing up to 50,000 movie reviews takes a lot of computation power, so instead of 50,000 movie reviews, we will be considering only 25,000 movie reviews from the <code class="literal">train</code> folder, and we will hold out some movie reviews for testing. Now let's try to understand how the content of the movie review files has been provided.</p></div></div><div class="section" title="Understanding the contents of the movie review files"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec75"/>Understanding the contents of the movie review files</h2></div></div></div><p>Inside the <code class="literal">pos</code> and <code class="literal">neg</code> folders, there are <code class="literal">.txt</code> files that contain the movie reviews. All the <code class="literal">.txt</code> files <a id="id578" class="indexterm"/>inside the <code class="literal">pos</code> folder are positive movie reviews. You can refer to the sample content provided in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_05.jpg" alt="Understanding the contents of the movie review files" width="1000" height="108"/><div class="caption"><p>Figure 5.5: Sample movie review from the pos folder; the filename is 0_9.txt</p></div></div><p>The movie reviews are provided in simple plain text. Here, we will be performing only a small preprocessing change, in which we rename the <code class="literal">pos</code> and <code class="literal">neg</code> folder names to <code class="literal">positiveReviews</code> and <code class="literal">negativeReviews</code>, respectively. This <code class="literal">IMDb</code> dataset had already been preprocessed, so we are not performing any extensive preprocessing. You can download <a id="id579" class="indexterm"/>the final training dataset by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz.</a>
</p><p>Now we need to start building the ML model for our sentiment analysis application. We will perform the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Building the training and testing datasets</li><li class="listitem" style="list-style-type: disc">Feature engineering for the baseline model</li><li class="listitem" style="list-style-type: disc">Selecting the Machine Learning algorithm</li><li class="listitem" style="list-style-type: disc">Training the baseline model</li><li class="listitem" style="list-style-type: disc">Understanding the testing matrix</li><li class="listitem" style="list-style-type: disc">Testing the baseline model</li></ul></div><p>So let's try to understand all these steps.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the training and testing datasets for the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec51"/>Building the training and testing datasets for the baseline model</h1></div></div></div><p>In this <a id="id580" class="indexterm"/>section, we will be generating the training dataset as well as the testing dataset. We will iterate over the files of our dataset and consider all files whose names start with the digit 12 as our test dataset. So, roughly 90% of our dataset is considered the training dataset and 10 % of our dataset is considered the testing dataset. You can refer to the code for this in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_06.jpg" alt="Building the training and testing datasets for the baseline model" width="567" height="611"/><div class="caption"><p>Figure 5.6: Code snippet for building the training and testing dataset</p></div></div><p>As you <a id="id581" class="indexterm"/>can see, if the filename starts with 12 then we consider the content of those files as the testing dataset. All files apart from these are considered the training dataset. You can find the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb</a>.
</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Feature engineering for the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec52"/>Feature engineering for the baseline model</h1></div></div></div><p>For this <a id="id582" class="indexterm"/>application, we will be using a basic statistical feature extraction concept in order to generate the features from raw text data. In the NLP domain, we need to convert raw text into a numerical format so that the ML <a id="id583" class="indexterm"/>algorithm can be applied to that numerical data. There are many techniques available, including indexing, count based vectorization, <span class="strong"><strong>Term Frequency - Inverse Document Frequency </strong></span>(<span class="strong"><strong>TF-IDF</strong></span> ), and so on. I have already discussed the concept of TF-IDF in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Generate features using TF-IDF</em></span>:</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>Indexing is basically used for fast data retrieval. In indexing, we provide a unique identification number. This unique identification number can be assigned in alphabetical order or based on frequency. You can refer to this link: <a class="ulink" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html</a></p><p>Count-based vectorization sorts the words in alphabetical order and if a particular word is present then its vector value becomes 1, otherwise 0. The size of the vector is the same as the vocabulary size of our training dataset. You can refer to the simple code by using this link: <a class="ulink" href="https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py">https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py</a>
</p></div></div><p>Here, we are using the TF-IDF vectorizer technique from scikit-learn. The <code class="literal">TfidfVectorizer</code> function converts a collection of raw documents into a matrix of TF-IDF features. If you are new to TF-IDF, then I would recommend that you refer to <a class="ulink" href="http://www.tfidf.com/">http://www.tfidf.com/</a> or <a class="ulink" href="https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5">https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5</a>.</p><p>You can refer to the code snippet provided in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_07.jpg" alt="Feature engineering for the baseline model" width="476" height="208"/><div class="caption"><p>Figure 5.7: Code snippet for generating feature vectors by using TF-IDF</p></div></div><p>As you <a id="id584" class="indexterm"/>see in the preceding code snippet for generating feature vectors by using TF-IDF, we have defined some parameters, which I want to explain properly:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">min_df</code>: This <a id="id585" class="indexterm"/>parameter provides a strict lower limit for document frequency. We have set this parameter to 5. So terms that appear fewer than 5 times in the dataset will not be considered for generating the TF-IDF vector.  </li><li class="listitem" style="list-style-type: disc"><code class="literal">max_df</code>: This <a id="id586" class="indexterm"/>parameter ignores terms that have a document frequency strictly higher than the given threshold. If the value of this parameter is float, then it represents a proportion of the document. We have set the parameter value to 0.8, which means we are considering 80% of the dataset.</li><li class="listitem" style="list-style-type: disc"><code class="literal">sublinear_tf</code>: This parameter is used to apply scaling. The value of this  parameter is False by default . If its value is True, then the value of tf will <a id="id587" class="indexterm"/>be replaced with the <span class="emphasis"><em>1+log(tf)</em></span> formula. This formula will help us perform scaling on our vocabulary.</li><li class="listitem" style="list-style-type: disc"><code class="literal">use_idf</code>: This parameter indicates whether the IDF reweighting mechanism is enabled <a id="id588" class="indexterm"/>or not. By default, IDF reweighting is enabled and hence the flag value for this parameter is True.</li></ul></div><p>Two methods are used here, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">fit_transform()</code>: By using <a id="id589" class="indexterm"/>this method, you have learned vocabulary and IDF, and this method returns the term-document matrix.</li><li class="listitem" style="list-style-type: disc"><code class="literal">transform()</code>: This <a id="id590" class="indexterm"/>method transforms documents into a document-term matrix. This method uses vocabulary and document frequency learned from the fit_transform method.</li></ul></div><p>You can <a id="id591" class="indexterm"/>find the preceding code at this GitHub link <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb"> https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb</a>.
</p><p>Now let's see which algorithm is best suited to building the baseline model.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Selecting the machine learning algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec53"/>Selecting the machine learning algorithm</h1></div></div></div><p>Sentiment analysis <a id="id592" class="indexterm"/>is a classification problem. There are some algorithms that can be really helpful for us. In movie reviews, you may discover that there are some phrases that appear quite frequently. If these frequently used phrases indicate some kind of sentiment, most likely, they are phrases that indicate a positive sentiment or a negative sentiment. We need to find phrases that indicate a sentiment. Once we find phrases that indicate sentiment, we just need to classify the sentiment either in a positive sentiment class or a negative sentiment class. In order <a id="id593" class="indexterm"/>to find out the actual sentiment class, we need to identify the probability of the most likely positive phrases and most likely negative phrases so that based on a higher probability value, we can identify that the given movie review belongs to a positive or a negative sentiment. The probabilities we will <a id="id594" class="indexterm"/>be taking into account are the prior and posterior probability values. This is the fundamental base of the naive Bayes algorithm. So, we will be using the multinomial naive Bayes algorithm. Apart from this, we will be using the <span class="strong"><strong>Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>) algorithm. We will be implementing it with different types of kernel tricks.</p><p>If you <a id="id595" class="indexterm"/>want to learn more about naive Bayes, then you can refer to <a class="ulink" href="http://www.saedsayad.com/naive_bayesian.html">http://www.saedsayad.com/naive_bayesian.html</a>, and if you want to learn more <a id="id596" class="indexterm"/>about SVM, then you can refer to:<a class="ulink" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/"> https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/</a> or <a class="ulink" href="https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts">https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts</a>.</p><p>In the next section, we will look at the code that helps us perform training.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Training the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec54"/>Training the baseline model</h1></div></div></div><p>In this <a id="id597" class="indexterm"/>section, we will look at the code that helps us perform actual training on the training dataset. We will look at the implementation first, and then I will explain the code step by step. Here, we will be implementing Naive Bayes and SVM algorithms. For implementation, we will be using the scikit-learn library. You can find the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb</a>.</p><div class="section" title="Implementing the baseline model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec76"/>Implementing the baseline model</h2></div></div></div><p>In order <a id="id598" class="indexterm"/>to understand the implementation of the baseline model, you can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_05_08.jpg" alt="Implementing the baseline model" width="583" height="405"/><div class="caption"><p>Figure 5.8: Code snippet for performing training using naive Bayes and SVM </p></div></div><p>We have implemented the following four algorithms here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Multinomial naive Bayes</li><li class="listitem" style="list-style-type: disc">C-support vector classification with kernel rbf</li><li class="listitem" style="list-style-type: disc">C-support vector classification with kernel linear</li><li class="listitem" style="list-style-type: disc">Linear support vector classification</li></ul></div><div class="section" title="Multinomial naive Bayes"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec62"/>Multinomial naive Bayes</h3></div></div></div><p>As you <a id="id599" class="indexterm"/>can see in the preceding code snippet, we have used Multinomial naive Bayes. The multinomial naive Bayes classifier is suitable for classification with discrete features, which means that if the features are word counts or TF-IDF vectors, then we can use this classifier. The multinomial distribution normally requires integer feature counts. However, fractional counts such as TF-IDF might work as well. So, we have to apply this algorithm to <code class="literal">train_vectors</code>. The <code class="literal">fit()</code> method is the step where the actual training is performed. Here, we have used all hyper parameters by default.</p></div><div class="section" title="C-support vector classification with kernel rbf"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec63"/>C-support vector classification with kernel rbf</h3></div></div></div><p>We have <a id="id600" class="indexterm"/>also implemented SVM with the <span class="emphasis"><em>rbf</em></span> kernel. Kernel is a function that will help train the algorithm. The equation for the <span class="emphasis"><em>rbf</em></span> kernel function is as follows:</p><div class="mediaobject"><img src="Images/B08394_05_32.jpg" alt="C-support vector classification with kernel rbf" width="507" height="74"/></div></div><div class="section" title="C-support vector classification with kernel linear"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec64"/>C-support vector classification with kernel linear</h3></div></div></div><p>We have <a id="id601" class="indexterm"/>also implemented SVM with the linear kernel. The equation for the linear kernel function is as follows:</p><div class="mediaobject"><img src="Images/B08394_05_33.jpg" alt="C-support vector classification with kernel linear" width="175" height="34"/></div></div><div class="section" title="Linear support vector classification"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec65"/>Linear support vector classification</h3></div></div></div><p>We have <a id="id602" class="indexterm"/>also used linear support vector classification. We will use the <code class="literal">LinearSVC()</code> method type for implementing this classification. This is similar to SVC with the parameter kernel=<span class="emphasis"><em>linear</em></span> but implemented in terms of liblinear rather than <code class="literal">libsvm</code>, so it has more flexibility in the choice <a id="id603" class="indexterm"/>of penalties and loss functions, and should scale better to a large number of samples.</p><p>For all the preceding ML algorithms, we have provided input, which are <code class="literal">train_vectors</code> and <code class="literal">train_labels</code>. We will test the ML model accuracy by using the test vectors and comparing predicted labels with actual <code class="literal">test_labels</code>. Before performing testing, we will decide which kind of testing parameters we will be using. So let's look at the testing matrix.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the testing matrix"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec55"/>Understanding the testing matrix </h1></div></div></div><p>In this <a id="id604" class="indexterm"/>section, we will look at the testing matrix that we should consider in order to evaluate the trained ML models. For the baseline approach, we will be using the following five testing matrices:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Precision</li><li class="listitem" style="list-style-type: disc">Recall</li><li class="listitem" style="list-style-type: disc">F1-score</li><li class="listitem" style="list-style-type: disc">Support</li><li class="listitem" style="list-style-type: disc">Training accuracy</li></ul></div><p>Before we understand these terms, let's cover some basic terms that will help us to understand the preceding terms.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Positive</strong></span> (<span class="strong"><strong>TP</strong></span>)—If the <a id="id605" class="indexterm"/>classifier predicts that the given movie review carries a positive sentiment and that movie review has a positive sentiment in an actual scenario, then these kinds of test cases are considered TP. So, you can define the TP as if the test result is one that detects the condition when the condition is actually present.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Negative</strong></span> (<span class="strong"><strong>TN</strong></span>)—If the classifier predicts that the given movie review carries a <a id="id606" class="indexterm"/>negative sentiment and that movie review has a negative sentiment in an actual scenario, then those kinds of test cases are considered True Negative(TN). So, you can define the TN as if the test result is one that does not detect the condition when the condition is actually absent.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Positive </strong></span>(<span class="strong"><strong>FP</strong></span>)—If the classifier predicts that the given movie review carries a <a id="id607" class="indexterm"/>positive sentiment and that movie review has a negative sentiment in an actual scenario, then those kinds of test cases are considered False Positive (FP). So, you can define the FP as if the test result is one that detects the condition when the condition is actually absent. This is like a flase alram.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Negative </strong></span>(<span class="strong"><strong>FN</strong></span>)—If the classifier predicts that the given movie review carries a <a id="id608" class="indexterm"/>negative sentiment and that movie review has a positive sentiment in an actual scenario, then those kinds of test cases are considered False Negative (FN). So, you can define the FN as if the test result is one that does not detect the condition when the condition is actually present. This is the situation where certain conditions have been overlooked.</li></ul></div><p>Now we will look at all five main testing matrices that use TP, TN, FP, and FN terms.</p><div class="section" title="Precision"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec77"/>Precision</h2></div></div></div><p>Precision is <a id="id609" class="indexterm"/>the ability of the classifier to assign a positive class label <a id="id610" class="indexterm"/>for samples that originally belong to a positive class label. Precision does not assign a positive class for a given sample that originally belongs to a negative class. The equation for generating the precision score is as follows:</p><div class="mediaobject"><img src="Images/B08394_05_34.jpg" alt="Precision" width="174" height="52"/></div></div><div class="section" title="Recall"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec78"/>Recall</h2></div></div></div><p>Recall is <a id="id611" class="indexterm"/>the ability of the classifier to find all the positive samples. The <a id="id612" class="indexterm"/>equation for generating recall score is as follows:</p><div class="mediaobject"><img src="Images/B08394_05_35.jpg" alt="Recall" width="152" height="52"/></div></div><div class="section" title="F1-Score"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec79"/>F1-Score</h2></div></div></div><p>F1-score <a id="id613" class="indexterm"/>is the harmonic means of precision and recall. So you <a id="id614" class="indexterm"/>can find the equation as follows:</p><div class="mediaobject"><img src="Images/B08394_05_36.jpg" alt="F1-Score" width="284" height="55"/></div></div><div class="section" title="Support"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec80"/>Support</h2></div></div></div><p>Support is <a id="id615" class="indexterm"/>the number of occurrences of each class in true target labels. The value <a id="id616" class="indexterm"/>of support helps when it comes to calculating the average value for precision, recall, and F1-score. You can see the equations for calculating the average value of precision, recall, and F1-Score as follows:</p><div class="mediaobject"><img src="Images/B08394_05_37.jpg" alt="Support" width="884" height="67"/></div><div class="mediaobject"><img src="Images/B08394_05_38.jpg" alt="Support" width="784" height="67"/></div><div class="mediaobject"><img src="Images/B08394_05_39.jpg" alt="Support" width="907" height="67"/></div><p>Actual calculation <a id="id617" class="indexterm"/>using the preceding formula will be provided <a id="id618" class="indexterm"/>in the <span class="emphasis"><em>Testing the baseline mode</em></span> section. So bear with me for a while and we will see the actual testing result.</p></div><div class="section" title="Training accuracy"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec81"/>Training accuracy</h2></div></div></div><p>Training accuracy <a id="id619" class="indexterm"/>guides us in order to obtain the correct direction for developing any ML application. We test the trained ML model on the testing dataset. When we perform this testing, we have actual labels of the sentiment class for each of the testing records, and we also have the predicted sentiment class for all testing records so we can compare the results. So, the set of labels predicted for a testing dataset must exactly match the corresponding set of labels in the actual testing dataset. We count the records where our predicted labels are the same as actual labels, and then we convert this count to a percentage.</p><p>We will see all the testing matrices for each of the implemented ML algorithms in the next section, and then we will decide which algorithm performs well. So let's look at the implementation of testing the baseline model.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Testing the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec56"/>Testing the baseline model</h1></div></div></div><p>Here, we will <a id="id620" class="indexterm"/>look at the code snippet that performs the actual testing. We will be obtaining all the testing matrices that have been explained so far. We are going to test all the different ML algorithms so that we can compare the accuracy score.</p><div class="section" title="Testing of Multinomial naive Bayes"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec82"/>Testing of Multinomial naive Bayes</h2></div></div></div><p>You can <a id="id621" class="indexterm"/>see the testing result for the multinomial naive Bayes algorithm in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_09.jpg" alt="Testing of Multinomial naive Bayes" width="1000" height="871"/><div class="caption"><p>Figure 5.9: Code snippet for testing multinomial naive Bayes algorithm</p></div></div><p>As you can see, using this algorithm we have achieved an accuracy score of 81.5%.</p></div><div class="section" title="Testing of SVM with rbf kernel"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec83"/>Testing of SVM with rbf kernel </h2></div></div></div><p>You can <a id="id622" class="indexterm"/>see the testing result for SVM with the rbf kernel algorithm in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_10.jpg" alt="Testing of SVM with rbf kernel" width="987" height="871"/><div class="caption"><p>Figure 5.10: Code snippet for testing SVM with rbf kernel</p></div></div><p>As you can see, we have performed a test on the testing dataset and obtained an accuracy of 65.4%.</p></div><div class="section" title="Testing SVM with the linear kernel"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec84"/>Testing SVM with the linear kernel</h2></div></div></div><p>You can <a id="id623" class="indexterm"/>see the testing result for SVM with the linear kernel algorithm in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_11.jpg" alt="Testing SVM with the linear kernel" width="994" height="883"/><div class="caption"><p>Figure 5.11: Code snippet for testing SVM with linear kernel</p></div></div><p>As you can see, we have performed a test on the testing dataset and obtained an accuracy of 83.6%.</p></div><div class="section" title="Testing SVM with linearSVC"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec85"/>Testing SVM with linearSVC</h2></div></div></div><p>You can <a id="id624" class="indexterm"/>see the testing result for SVM with the linearSVC kernel algorithm in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_12.jpg" alt="Testing SVM with linearSVC" width="1000" height="883"/><div class="caption"><p>Figure 5.12: Code snippet for testing SVM with linearSVC kernel</p></div></div><p>We have performed a test on the testing dataset here and obtained an accuracy of 83.6%.</p><p>So, after seeing the accuracy score of each of the implemented algorithms, we can say that SVM with linear kernel and linearSVC is performing really well. Now, you may wonder <a id="id625" class="indexterm"/>whether we can improve the accuracy. We can do that for sure. So let's discuss the problems that exist in this baseline approach.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Problem with the existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec57"/>Problem with the existing approach</h1></div></div></div><p>In the <a id="id626" class="indexterm"/>baseline approach, we got great accuracy. However, we ignored the following points, which we can be implemented in our revised approach:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We did not focus on word embedding-based techniques</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Deep learning</strong></span> (<span class="strong"><strong>DL</strong></span>) algorithms such as CNN can be helpful for us</li></ul></div><p>We need <a id="id627" class="indexterm"/>to focus on these two points because word embedding-based techniques really help retain the semantics of the text. So we should use these techniques as well as the DL-based-algorithm, which helps us provide more accuracy because DL algorithms perform well when a nested data structure is involved. What do I mean by a nested data structure? Well, that means any written sentence or spoken sentence made up of phrases, phrases made of words, and so on. So, natural language has a nested data structure. DL algorithms help us understand the nested structure of the sentences from our text dataset.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="How to optimize the existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec58"/>How to optimize the existing approach </h1></div></div></div><p>There are <a id="id628" class="indexterm"/>certain techniques that can help us improve this application. The key techniques that can help us improvise the baseline approach are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We can <a id="id629" class="indexterm"/>use word embedding-based techniques such as Word2Vec, glove, and so on</li><li class="listitem" style="list-style-type: disc">We should also implement <span class="strong"><strong>Convolution Neural Networks</strong></span> (<span class="strong"><strong>CNN</strong></span> ) to get an idea about how a deep learning algorithm can help us</li></ul></div><p>So in the revised approach, we will be focusing on word embedding techniques and the Deep Learning algorithm. We will be using Keras with the TensorFlow backend. Before implementation, let's <a id="id630" class="indexterm"/>understand the revised approach in detail.</p><div class="section" title="Understanding key concepts for optimizing the approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec86"/>Understanding key concepts for optimizing the approach  </h2></div></div></div><p>In this section, we will understand the revised approach in detail, so we know what steps we <a id="id631" class="indexterm"/>should implement. We are using Keras, a Deep Learning library that provides us with high-level APIs so we can implement CNN easily. The following steps are involved:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Importing the dependencies</strong></span>: In this step, we will be importing different dependencies such as Numpy and Keras with the TensorFlow backend. We will be using different APIs belonging to the Keras library.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Downloading and loading the IMDb dataset</strong></span>: In this step, we will be downloading the IMDb dataset and loading this dataset by using Keras APIs</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Choosing top words and maximum text length</strong></span>: In this stage, we will set the value of our vocabulary, which we can use during the word embedding stage. So, we have selected the top 10,000 words. After that, we have restricted the length of movie reviews to 1600.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Implementing word embedding</strong></span>: At this stage of the code, we will be using default embedding techniques from Keras and generate the word vector with a length of 300.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Building a CNN</strong></span>: In this stage, we will be making three-layer neural networks, where the first layer has 64 neurons, the second layer has 32 neurons, and the last layer has 16 neurons. Here, we are using sigmoid as an activation function. The Activation function introduces non-linearity to the neural network so that we can generate a probability score for each class using mathematical functions.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Training and obtaining the accuracy</strong></span>: Finally, we train the model and generate the accuracy score. We have set the epoch value to 3 and set adam as the optimization function and our loss function is <code class="literal">binary_crossentropy</code>.  Epoch basically indicates how many times we need to perform training on our whole dataset. Cross-entropy loss measures the performance of a classifier whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability differs from the actual label. After training, we will generate the accuracy of the model. This training stage may take time as well as a great amount of computation power.</li></ul></div><p>Now let's <a id="id632" class="indexterm"/>see the code in the next section.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Implementing the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec59"/>Implementing the revised approach</h1></div></div></div><p>In this <a id="id633" class="indexterm"/>section, we will see the implementation in the form of a code snippet. We will be following the same step that we saw in the previous section. So, without any delay, let's look at the code. You can refer to this code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb</a>.</p><div class="section" title="Importing the dependencies"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec87"/>Importing the dependencies</h2></div></div></div><p>You can <a id="id634" class="indexterm"/>refer to the code snippet in the following figure, where you can find the imported dependencies as well:</p><div class="mediaobject"><img src="Images/B08394_05_13.jpg" alt="Importing the dependencies" width="549" height="289"/><div class="caption"><p>Figure 5.13: Code snippet where we can see the imported dependencies </p></div></div><p>As you <a id="id635" class="indexterm"/>can see, we have used the TensorFlow backend with the Keras library.</p></div><div class="section" title="Downloading and loading the IMDb dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec88"/>Downloading and loading the IMDb dataset</h2></div></div></div><p>You can <a id="id636" class="indexterm"/>refer to the code snippet <a id="id637" class="indexterm"/>in the following figure, where you can also find the code for downloading and loading the IMDb dataset:</p><div class="mediaobject"><img src="Images/B08394_05_14.jpg" alt="Downloading and loading the IMDb dataset" width="1000" height="230"/><div class="caption"><p>Figure 5.14: Code snippet for downloading and loading the IMDb dataset</p></div></div><p>We have also set the value of the vocabulary.</p></div><div class="section" title="Choosing the top words and the maximum text length"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec89"/>Choosing the top words and the maximum text length</h2></div></div></div><p>At this <a id="id638" class="indexterm"/>stage, we have set the top word <a id="id639" class="indexterm"/>values as well as the maximum text length value. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_15.jpg" alt="Choosing the top words and the maximum text length" width="1000" height="405"/><div class="caption"><p>Figure 5.15: Code snippet to set the vocabulary value and the maximum text length value</p></div></div><p>In the first part of code, we have set the top_word parameter to 10,000, and in the second part of the code, we have set the length of the movie review to 1,600. The top_word indicates the vocabulary size. From our dataset, we have picked the top 10,000 unique words. Most of the movie reviews have words that are present in our word vocabulary. Here, we are not processing very long movie reviews, because of that reason we have set the length of the movie review.</p></div><div class="section" title="Implementing word embedding"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec90"/>Implementing word embedding</h2></div></div></div><p>At this <a id="id640" class="indexterm"/>stage, we have implemented the default Keras word embedding method in order to obtain a feature vector with a size of 300. You can refer to the following code snippet:</p><div class="mediaobject"><img src="Images/B08394_05_16.jpg" alt="Implementing word embedding" width="772" height="157"/><div class="caption"><p>Figure 5.16: Code snippet for obtaining word feature vector based on the word embedding technique</p></div></div></div><div class="section" title="Building a convolutional neural net (CNN)"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec91"/>Building a convolutional neural net (CNN)</h2></div></div></div><p>In this <a id="id641" class="indexterm"/>section you can refer to the code, which will help you understand the architecture of a neural net. Here, we have used CNN because it handles higher level features or a nested structure of the dataset really well. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_17.jpg" alt="Building a convolutional neural net (CNN)" width="999" height="394"/><div class="caption"><p>Figure 5.17: Code snippet for building CNN</p></div></div><p>Here, we have built the neural network with two dense layers in it.</p></div><div class="section" title="Training and obtaining the accuracy"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec92"/>Training and obtaining the accuracy</h2></div></div></div><p>At this <a id="id642" class="indexterm"/>stage, we have performed the training. You can <a id="id643" class="indexterm"/>refer to the code in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_18.jpg" alt="Training and obtaining the accuracy" width="998" height="384"/><div class="caption"><p>Figure 5.18: Code snippet for performing training on the training dataset</p></div></div><p>Here, we will be performing training three times as our epoch value is set to 3. As sentiment analysis is a binary problem, we have used <code class="literal">binary_crossentropy</code> as loss function. If you have a GPU-based computer, then the training time will be less; otherwise, this step is time consuming and the computation power consuming.</p><p>Once training is done, we can obtain training accuracy. For training accuracy, you can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_19.jpg" alt="Training and obtaining the accuracy" width="479" height="145"/><div class="caption"><p>Figure 5.19: Code snippet for obtaining training accuracy </p></div></div><p>Here, the accuracy <a id="id644" class="indexterm"/>is the training accuracy. Now we need to obtain testing accuracy because that will give us an actual idea about <a id="id645" class="indexterm"/>how well the train model is performing on unseen data. So let's see what the testing accuracy is.</p></div><div class="section" title="Testing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec93"/>Testing the revised approach</h2></div></div></div><p>In this <a id="id646" class="indexterm"/>section, we will obtain the accuracy of the testing dataset. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_20.jpg" alt="Testing the revised approach" width="447" height="137"/><div class="caption"><p>Figure 5.20: Code snippet for obtaining testing accuracy</p></div></div><p>After obtaining the testing accuracy, we got an accuracy value of 86.45%. This testing accuracy is better than our baseline approach. Now let's see what points we can improve in order to come up with the best approach.</p></div><div class="section" title="Understanding problems with the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec94"/>Understanding problems with the revised approach</h2></div></div></div><p>In this <a id="id647" class="indexterm"/>section, we will discuss what points of the revised approach we can improve. These are the points that we can implement in order to obtain the best possible approach:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We can use pretrained Word2Vec or glove models to generate the word vector</li><li class="listitem" style="list-style-type: disc">We should use a recurrent neural network with LSTM to get better output</li></ul></div><p>In this section, we will understand and implement the best approach, where we will load the pretrained glove (global word vector) model and use an RNN and LSTM network.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec60"/>The best approach</h1></div></div></div><p>There are <a id="id648" class="indexterm"/>some steps that we can follow in order to obtain the best possible approach. In this approach, we have used a glove pretrained model and have trained the model using the RNN and LSTM networks. The glove model has been pretrained on a large dataset so that it can generate more accurate vector values for words. That is the reason we are using glove here. In the next section, we will look at the implementation of the best approach. You can find all the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb</a>.</p><div class="section" title="Implementing the best approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec95"/>Implementing the best approach</h2></div></div></div><p>In order <a id="id649" class="indexterm"/>to implement the best approach, we will be performing the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Loading the glove model</li><li class="listitem" style="list-style-type: disc">Loading the dataset</li><li class="listitem" style="list-style-type: disc">Preprocessing</li><li class="listitem" style="list-style-type: disc">Loading the precomputed ID matrix</li><li class="listitem" style="list-style-type: disc">Splitting the train and test datasets</li><li class="listitem" style="list-style-type: disc">Building a neural network</li><li class="listitem" style="list-style-type: disc">Training the neural network</li><li class="listitem" style="list-style-type: disc">Loading the trained model</li><li class="listitem" style="list-style-type: disc">Testing the trained model</li></ul></div><div class="section" title="Loading the glove model"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec66"/>Loading the glove model</h3></div></div></div><p>In order <a id="id650" class="indexterm"/>to get the best performance, we will be <a id="id651" class="indexterm"/>using the pretrained glove model. You can download it at <a class="ulink" href="https://nlp.stanford.edu/projects/glove/.">https://nlp.stanford.edu/projects/glove/.</a> We have already generated the binary file and saved that file as an <code class="literal">.npy</code> extension. This file is with the <code class="literal">wordsList.npy</code> file. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_21.jpg" alt="Loading the glove model" width="769" height="870"/><div class="caption"><p>Figure 5.21: Code snippet for loading the glove pretrained model</p></div></div><p>Dimensionality <a id="id652" class="indexterm"/>of the word vector is 50 and this model contains word vectors for 400,000 words.</p></div><div class="section" title="Loading the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec67"/>Loading the dataset</h3></div></div></div><p>You can <a id="id653" class="indexterm"/>refer to the following code snippet: </p><div class="mediaobject"><img src="Images/B08394_05_22.jpg" alt="Loading the dataset" width="782" height="577"/><div class="caption"><p>Figure 5.22: Code snippet for loading the dataset</p></div></div><p>We have considered 25,000 movie reviews in total.</p></div><div class="section" title="Preprocessing"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec68"/>Preprocessing</h3></div></div></div><p>You can <a id="id654" class="indexterm"/>refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_23.jpg" alt="Preprocessing" width="861" height="151"/><div class="caption"><p>Figure 5.23: Code snippet for performing pre-processing</p></div></div></div><div class="section" title="Loading precomputed ID matrix"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec69"/>Loading precomputed ID matrix</h3></div></div></div><p>In this <a id="id655" class="indexterm"/>section, we are generating the index for each word. This process is computationally expensive, so I have already generated the index matrix and made it ready for loading. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_24.jpg" alt="Loading precomputed ID matrix" width="721" height="714"/><div class="caption"><p>Figure 5.24: Code snippet for generating matrix for word IDs</p></div></div></div><div class="section" title="Splitting the train and test datasets"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec70"/>Splitting the train and test datasets</h3></div></div></div><p>In this <a id="id656" class="indexterm"/>section, we will see the code snippet for generating the training and testing datasets. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_25.jpg" alt="Splitting the train and test datasets" width="409" height="511"/><div class="caption"><p>Figure 5.25: Code snippet for splitting dataset into training and testing dataset</p></div></div></div><div class="section" title="Building a neural network"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec71"/>Building a neural network</h3></div></div></div><p>We have <a id="id657" class="indexterm"/>used a <span class="strong"><strong>recurrent neural net</strong></span> (<span class="strong"><strong>RNN</strong></span>) with <span class="strong"><strong>Long-Short Term Memory Unit</strong></span> (<span class="strong"><strong>LSTMs</strong></span>) cells as a part of their hidden states. LSTM cells <a id="id658" class="indexterm"/>are used to store sequential information. If you <a id="id659" class="indexterm"/>have multiple sentences, then LSTM stores the context of the previous or previous to previous sentences, which helps us <a id="id660" class="indexterm"/>improve this application. If you want to understand LSTM in detail, then you can refer to <a class="ulink" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.</p><p>You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_26.jpg" alt="Building a neural network" width="837" height="827"/><div class="caption"><p>Figure 5.26: Code snippet for building RNN with LSTM</p></div></div><p>First, we define the hyper parameters. We set the batch size to 64, LSTM units to 64, the number <a id="id661" class="indexterm"/>of classes to 2, and then we perform 100,000 iterations.</p></div><div class="section" title="Training the neural network"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec72"/>Training the neural network</h3></div></div></div><p>In this <a id="id662" class="indexterm"/>section, you can refer to the code snippet in the following figure, which is used for performing training: </p><div class="mediaobject"><img src="Images/B08394_05_27.jpg" alt="Training the neural network" width="884" height="669"/><div class="caption"><p>Figure 5.27: Code snippet for performing training</p></div></div><p>After each 10,000 iterations, we save the model. During the training, you can monitor the progress by using TensorBoard. You can refer to the following figure, which shows the progress over the period of training. You can monitor the accuracy and loss percentage during training, so you find out how the DL model is converging during training:</p><div class="mediaobject"><img src="Images/B08394_05_28.jpg" alt="Training the neural network" width="1000" height="517"/><div class="caption"><p>Figure 5.28: Accuracy and loss graph generated during training on TensorBoard</p></div></div><p>Training is <a id="id663" class="indexterm"/>time consuming and computationally expensive, so with a GPU it may take 2- 3 hours to train the model. Therefore, you can use the pretrained model by downloading it from GitHub at: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz</a>.</p></div><div class="section" title="Loading the trained model"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec73"/>Loading the trained model</h3></div></div></div><p>Once training <a id="id664" class="indexterm"/>is done, we can save the trained model. After loading this model, we can check its accuracy as well. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_29.jpg" alt="Loading the trained model" width="999" height="476"/><div class="caption"><p>Figure 5.29: Code snippet for generating testing accuracy</p></div></div><p>Here we <a id="id665" class="indexterm"/>have generated a testing accuracy of 91.66%.</p></div><div class="section" title="Testing the trained model"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec74"/>Testing the trained model</h3></div></div></div><p>In this <a id="id666" class="indexterm"/>section, we will be passing new movie reviews, and by loading the trained model, we will generate the prediction of sentiment. You can refer to the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_05_30.jpg" alt="Testing the trained model" width="841" height="698"/><div class="caption"><p>Figure 5.30: Code snippet for loading trained model and generating sentiment for a given sentence of the movie review</p></div></div><p>In this snippet, we have passed one sentence as part of a movie review, and our trained model <a id="id667" class="indexterm"/>identifies it as a negative sentiment. You can also refer to the code snippet in the following figure, which generates a sentiment for the sentence carrying a negative word in it:</p><div class="mediaobject"><img src="Images/B08394_05_31.jpg" alt="Testing the trained model" width="670" height="428"/><div class="caption"><p>Figure 5.31: Code snippet for loading trained model and generating sentiment for given sentence of the movie review</p></div></div><p>This approach <a id="id668" class="indexterm"/>gives you great results.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec61"/>Summary</h1></div></div></div><p>In this chapter, we looked at how to build a sentiment analysis model that gives us state-of-the-art results. We used an IMDb dataset that had positive and negative movie reviews and understood the dataset. We applied the machine learning algorithm in order to get the baseline model. After that, in order to optimize the baseline model, we changed the algorithm and applied deep-learning-based algorithms. We used glove, RNN, and LSTM techniques to achieve the best results. We learned how to build sentiment analysis applications using Deep Learning. We used TensorBoard to monitor our model's training progress. We also touched upon modern machine learning algorithms as well as Deep Learning techniques for developing sentiment analysis, and the Deep Learning approach works best here.</p><p>We used a GPU to train the neural network, so if you discover that it needs more computation power from your end to train the model, then you can use the Google cloud or <span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) GPU-based instances. I have already uploaded the pretrained model, so you can directly use that as well. You can find the pretrained model at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz">https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz</a>.</p><p>In the next chapter, we will build a job recommendation system that will help people find jobs, especially related to the job profiles in which they are interested. For a job recommendation system, we will be using various resources for linking resumes, job search queries, and so on. Again, we will be developing this system using machine learning and Deep Learning systems.</p></div></div>



  </body></html>