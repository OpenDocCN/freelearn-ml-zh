- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we reviewed the recent developments in deep learning, we are now reaching
    the cutting-edge of machine learning, and we are now adding a very special dimension
    to our model (time, and hence sequences of inputs) through a recent series of
    algorithms called **recurrent neural networks (RNNs)**.
  prefs: []
  type: TYPE_NORMAL
- en: Solving problems with order — RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we have examined a number of models, from simple
    ones to more sophisticated ones, with some common properties:'
  prefs: []
  type: TYPE_NORMAL
- en: They accept unique and isolated input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have unique and fixed size output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs will depend exclusively on the current input characteristics, without
    dependency on past or previous input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real life, the pieces of information that the brain processes have an inherent
    structure and order, and the organization and sequence of every phenomenon we
    perceive has an influence on how we treat them. Examples of this include speech
    comprehension (the order of the words in a sentence), video sequence (the order
    of the frames in a video), and language translation. This prompted the creation
    of new models. The most important ones are grouped under the RNN umbrella.
  prefs: []
  type: TYPE_NORMAL
- en: RNN definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RNNs are **Artificial Neural Network** (**ANN**) models whose inputs and outputs
    are sequences. A more formal definition can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"An RNN represents a sequence with a high-dimensional vector (called the hidden
    state) of a fixed dimensionality that incorporates new observations using a complex
    non-linear function."'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are highly expressive and can implement an arbitrary memory-bounded computation,
    and as a result, they can be configured to achieve non-trivial performance on
    difficult sequence tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Types of sequence to be modeled
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RNNs work with sequences models, both in the input and the output realm. Based
    on this, we can have all the possible combinations to solve different kinds of
    problems. In the following diagram, we illustrate the main architectures used
    in this field, and then a reference of the recursive ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21c36793-0b24-4644-be7f-f1d24452af93.png)'
  prefs: []
  type: TYPE_IMG
- en: Type of sequences modes
  prefs: []
  type: TYPE_NORMAL
- en: Development of RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The origin of RNNs is surprisingly common to the other modern neural network
    architectures, dating back to Hopfield networks from the 1980s, but with counterparts
    in the 1970s.
  prefs: []
  type: TYPE_NORMAL
- en: 'The common structure for the first iterations of the recurrent architecture
    can be represented in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f28d6f32-e9b2-43f0-85d9-cbb5d2e8fa56.png)'
  prefs: []
  type: TYPE_IMG
- en: Recurrent cell unrolling
  prefs: []
  type: TYPE_NORMAL
- en: Classic RNN nodes have recurrent connections to themselves, and so they can
    evolve their weights as the input sequence progresses. Additionally, on the right
    of the diagram, you can see how the network can be *unrolled* to generate a set
    of outputs based on the stochastic model it has saved internally. It stores representations
    of recent input events in the form of activation (**short-term memory**, as opposed
    to **long-term memory**, embodied by slowly changing weights). This is potentially
    significant for many applications, including speech processing, music composition
    (such as *Mozer, 1992*), **Natural Language Processing** (**NLP**), and many other
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: Training method — backpropagation through time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the considerable number of model types we've been studying, it's possible
    that you can already see a pattern in the implementation of the training steps.
  prefs: []
  type: TYPE_NORMAL
- en: For recurrent neural networks, the most well-known error minimization technique
    is a variation of the well-known (for us) backpropagation methods, with a simple
    element – **backpropagation through time** (**BPTT**) works by unrolling all input
    timesteps. Each timestep has one input timestep, one copy of the whole network,
    and one output. Errors are calculated and accumulated for each timestep, and finally
    the network is rolled back up and the weights are updated.
  prefs: []
  type: TYPE_NORMAL
- en: Spatially, each timestep of the unrolled recurrent neural network can be seen
    as an additional layer, given the dependence from one timestep to another and
    how every timestep output is taken as an input for the subsequent timestep. This
    can lead to really complex training performance requirements, and thus, **truncated
    backpropagation through time** was born.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pseudocode represents the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main problems of the traditional RNNs — exploding and vanishing gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, RNNs have turned out to be difficult to train, especially on problems
    with complicated long-range temporal structures – precisely the setting where
    RNNs ought to be most useful. Since their potential has not been realized, methods
    that address the difficulty of training RNNs are of great importance.
  prefs: []
  type: TYPE_NORMAL
- en: The most widely used algorithms for learning what to put in short-term memory,
    however, take too much time or do not work well at all, especially when minimal
    time lags between inputs and corresponding teacher signals are long. Although
    theoretically fascinating, the existing methods did not provide clear practical
    advantages over traditional feedforward networks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main problems with RNNs happens in the backpropagation stage. Given
    its recurrent nature, the number of steps that the backpropagation of the errors
    has corresponds to a very deep network. This cascade of gradient calculations
    could lead to a very insignificant value in the last stages, or on the contrary,
    to ever-increasing and unbounded parameters. Those phenomena receive the names
    of vanishing and exploding gradients. This is one of the reasons for which LSTM
    was created.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with conventional BPTT is that error signals going backwards in
    time tend to either blow up or vanish – the temporal evolution of the backpropagated
    error exponentially depends on the size of the weights. It could lead to oscillating
    weights, or taking a prohibitive amount of time, or it could not work at all.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence of many different attempts to solve the problem with vanishing
    and exploding gradients, finally in 1997, *Schmidhuber* and *Sepp* published a
    fundamental paper on RNNs, and LSTM, which paved the way for all modern developments
    in the area.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LSTMs** are a fundamental step in RNNs, because they introduce long-term
    dependencies into the cells. The unrolled cells contain two different parameter
    lines: one long-term status, and the other representing short-term memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Between steps, the long-term forgets less important information, and adds filtered
    information from short-term events, incorporating them into the future.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are really versatile in their possible applications, and they are the
    most commonly employed recurrent models, along with GRUs, which we will explain
    later. Let's try to break down an LSTM into its components to get a better understanding
    of how they work.
  prefs: []
  type: TYPE_NORMAL
- en: The gate and multiplier operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LSTMs have two fundamental values: remembering important things from the present,
    and slowly forgetting unimportant things from the past. What kind of mechanism
    can we use to apply this kind of filtering? It''s called the **gate** operation.'
  prefs: []
  type: TYPE_NORMAL
- en: The gate operation basically has a multivariate vector input and a filter vector,
    which will be dot multiplied with the input, allowing or rejecting the elements
    from the inputs to be transferred. How do we adjust this gate's filters? This multivariate
    control vector (marked with an arrow on the diagram) is connected with a neural
    network layer with a sigmoid activation function. If we apply the control vector
    and pass through the sigmoid function, we will get a binary-like output vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, the gate will be represented by a series of switches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d40d6035-e0b8-420a-80ed-48b8095d5d64.png)'
  prefs: []
  type: TYPE_IMG
- en: LSTM gate
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important part of the process is the multiplications, which formalize
    the trained filters, effectively multiplying the inputs by an included gate. The
    arrow icon indicates the direction in which the filtered information will flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31203714-478a-44ff-9488-df48560da1d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Gate multiplication step
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to describe an LSTM cell in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'An LSTM has three gates to protect and control the cell state: one at the start
    of the data flow, another in the middle, and the last at the end of the cell''s
    informational boundaries. This operation will allow both discarding (hopefully
    not important) low-important state data and incorporating (hopefully important)
    new data to the state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows all the concepts in the operation of one LSTM cell.
    As inputs, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The cell state, which will store long-term information, because it carries the
    optimized weights dating from the origin of the cell training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The short-term state, *h(t)*, which will be directly combined with the current
    input on each iteration, and so it will have a much bigger influence on the latest
    values of the inputs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4874cf1c-76ae-4cb1-95e9-3af19d8f6a7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of an LSTM cell, with all its components
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore the data flow on this LSTM unit in order to better understand
    how the different gates and operations work together in a single cell.
  prefs: []
  type: TYPE_NORMAL
- en: Part 1 — set values to forget (input gate)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this stage, we take the values coming from the short-term memory combined
    with the input itself, and these values will then output a binary function, represented
    by a multivariable sigmoid. Depending on the input and short-term memory values,
    the sigmoid output will filter the long-term knowledge, represented by the weights
    of the cell''s state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/529af80b-efe4-4d92-9612-27619fdd7734.png)'
  prefs: []
  type: TYPE_IMG
- en: State forget parameters setup
  prefs: []
  type: TYPE_NORMAL
- en: Part 2 — set values to keep
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to set the filter that will allow or reject the incorporation
    of new and short-term memory to the cell's semi-permanent state. Then it is time
    to set the filter that will allow or reject the incorporation of new and short-term
    memory to the cell's semi-permanent state.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, at this stage, we will determine how much of the new and semi-new information
    will be incorporated in the cell''s new state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71b9d57f-6861-46bf-9ce0-1a318b602600.png)'
  prefs: []
  type: TYPE_IMG
- en: Short-term values selection step
  prefs: []
  type: TYPE_NORMAL
- en: Part 3 — apply changes to cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part of the sequence, we will finally pass through the information filter
    we have been configuring, and as a result, we will have an updated long-term state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to normalize the new and short-term information, we pass the new input
    and the short-term state via a neural network with **tanh** activation. This will
    allow us to feed the new information in a normalized *[-1,1]* range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b3418fd-359f-4ac4-b1bc-548766fb5c97.png)'
  prefs: []
  type: TYPE_IMG
- en: State changes persistence step
  prefs: []
  type: TYPE_NORMAL
- en: Part 4 — output filtered cell state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it''s the turn of the short-term state. It will also use the new and previous
    short-term state to set up the filter that will pass the long-term state, dot
    multiplied by a tanh function, again to normalize the information to incorporate
    a *(-1,1)* range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a048a2f-5c05-433d-8584-de049c86d8de.png)'
  prefs: []
  type: TYPE_IMG
- en: New short-term generation step
  prefs: []
  type: TYPE_NORMAL
- en: Univariate time series prediction with energy consumption data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will be solving a problem in the domain of regression. For
    this reason, we will build a multi-layer RNN with two LSTMs. The type of regression
    we will do is of the *many to one* type, because the network will receive a sequence
    of energy consumption values and will try to output the next value based on the
    previous four registers.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will be working on is a compendium of many measurements of the
    power consumption of one home over a period of time. As we might infer, this kind
    of behavior can easily follow patterns (it increases when the occupants use the
    microwave to prepare breakfast and use computers during the day, it decreases
    a bit in the afternoon, and then increases in the evening with all the lights,
    finally decreasing to zero when the occupants are asleep).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by setting the appropriate environment variables and loading the
    required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Dataset description and loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will be using the **Electricity Load Diagrams Data Sets**,
    from *Artur Trindade* ([https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)).
    This is the description of the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Data set has no missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: Values are in kW of each 15 min. To convert values in kWh values must be divided
    by 4.
  prefs: []
  type: TYPE_NORMAL
- en: Each column represent one client. Some clients were created after 2011\. In
    these cases consumption were considered zero.
  prefs: []
  type: TYPE_NORMAL
- en: All time labels report to Portuguese hour. However all days present 96 measures
    (24*15). Every year in March time change day (which has only 23 hours) the values
    between 1:00 am and 2:00 am are zero for all points. Every year in October time
    change day (which has 25 hours) the values between 1:00 am and 2:00 am aggregate
    the consumption of two hours."
  prefs: []
  type: TYPE_NORMAL
- en: In order to simplify our model description, we took just one client's complete
    measurements and converted its format to standard CSV. It is located in the `data`
    subfolder of this chapter's code folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we will load the first 1,500 values of the consumption of a sample home
    from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the subset of data that we need to model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70e61ce5-14e6-4465-a264-08bd834e1a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: If we take a look at this representation (we took the first 1,500 samples) we
    can see an initial transient state, probably when the measurements were put in
    place, and then we see a really clear cycle of high and low consumption levels.
    From simple observation, we can also see that the cycles are of more or less 100
    samples, pretty close to the 96 samples per day this dataset has.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to assure a better convergence of the backpropagation methods, we
    should try to normalize the input data. So, we will be applying the classic scale
    and centering technique, subtracting the mean value, and scaling by the `floor()`
    of the maximum value. To get the required values, we use the pandas `describe()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the graph of our normalized data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbb950bc-0fcc-49ad-aaa6-b31334d715b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this step, we will prepare our input dataset, because we need an input `x` (the
    previous 5 values) with a corresponding input `y` (the value after 5 timesteps).
    Then, we will assign the first 13,000 elements to the training set, and then we
    will assign the following 1,000 samples to the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will build the model, which will be a dual LSTM with a dropout layer
    at the end of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to run the model and adjust the weights. The model fitter will
    use 8% of the dataset values as the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After rescaling, it''s time to see how our model predicts the values compared
    with the actual test values, which didn''t participate in the training of the
    models, to understand how the model generalizes the behavior of the sample home:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/279595ac-742c-4790-a2de-d131878c6892.png)'
  prefs: []
  type: TYPE_IMG
- en: Final regressed data
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, our scope has expanded even more, adding the important dimension
    of time to the set of elements to be included in our generalization. Also, we
    learned how to solve a practical problem with RNNs, based on real data.
  prefs: []
  type: TYPE_NORMAL
- en: But if you think you have covered all the possible options, there are many more
    model types to see!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will talk about cutting edge architectures that can
    be trained to produce very clever elements, for example, transfer the style of
    famous painters to a picture, and even play video games! Keep reading for reinforcement
    learning and generative adversarial networks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hopfield, John J, *Neural networks and physical systems with emergent collective
    computational abilities.* Proceedings of the national academy of sciences 79.8
    (1982): 2554-2558.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio, Yoshua, Patrice Simard, and Paolo Frasconi, *Learning long-term dependencies
    with gradient descent is difficult.* IEEE transactions on neural networks 5.2
    (1994): 157-166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter, Sepp, and Jürgen Schmidhuber, *long short-term memory*. Neural
    Computation 9.8 (1997): 1735-1780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter, Sepp. *Recurrent neural net learning and vanishing gradient.* International
    Journal Of Uncertainity, Fuzziness and Knowledge-Based Systems 6.2 (1998): 107-116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever, Ilya, *Training recurrent neural networks.* University of Toronto,
    Toronto, Ont., Canada (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung, Junyoung, et al, *Empirical evaluation of gated recurrent neural networks
    on sequence modeling.* arXiv preprint arXiv:1412.3555 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
