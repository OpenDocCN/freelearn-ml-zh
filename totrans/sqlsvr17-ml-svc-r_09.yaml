- en: Machine Learning Services with R for DBAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R integration (along with Python integration in SQL Server 2017) offered a wide
    range of possibilities that one can use. And the targeted group of people has
    just increased in terms of people (job roles or departments) using R Services.
    DBAs (and also SysAdmins) will for sure gain a lot from this. Not only do R and
    statistics give them some additional impetus for discovering and gaining insights
    on their captured data, but also they might help them to find some hidden nuggets
    that they might have missed before. The mixture of different languages-and I am
    not solely talking about R, but also other languages-for sure bring new abilities
    to track, capture, and analyze captured data.
  prefs: []
  type: TYPE_NORMAL
- en: One thing is clear, if you have R (any Python) so close to the database, several
    people can switch from monitoring tasks to predicting tasks. This literally means
    that instead of taking actions when something has already happened, people can
    now diagnose and predict what might happen. I'm not saying this is an easy task,
    since we all know that the complexity, for example, of one query all of a sudden
    running slow, can have one or more hidden reasons, that might not be seen immediately,
    R in-database will for sure help find this hidden reason in near real time. Contrary
    to data-mining in SSAS, which in my personal opinion is still a very strong and
    good tool, there might be more  latency in comparison to sending and analyzing
    data through R Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will capture the important steps on how to help DBAs (or other
    roles to tackle similar issues) to get the advantages of R:'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data relevant for DBAs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and analyzing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating predictions with R Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving monitoring with predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering relevant data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gathering data - simple as it might be - is a task that needs to be well crafted.
    There are a few reasons for that. The first and most important is that we want
    to gather data in a way that will have minimum or zero impact on the production
    environment. This means that the process of collecting and storing data should
    not disturb any on-going process. The second important thing is storage. Where
    and how do you want to store the data and the retention policy of the stored data?
    At the beginning, this might seem a very trivial case, but over time, storage
    itself will play an important role. The third and also utterly important thing
    is which data you want to gather. Of course, we all want to have smart data present,
    that is, having all the data relevant for solving or improving our business processes.
    But in reality, gathering smart is neither that difficult nor that easy. First
    of all, one must understand the concept of the database feature and furthermore,
    how to capture the relevant indicators and how this particular feature will work.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see where and how one can see performance improvements, if you know where
    and how to look for them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, delayed durability is a feature that has been in SQL Server since
    SQL Server 2014, but can in some scenarios help improve performance for compromising
    the durability (durability is part of the ACID acronym-atomic, consistent, isolation,
    and durability-and prevents, in the case of a failure or system restart, committed
    data from not being saved or saved in an incorrect state). **Write-ahead log**
    (**WAL**) is a system that SQL Server uses, which means that all the changes are
    written to the log first, before they are allowed to be committed to the database
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this quick demo, we will create an empty database with `DELAYED_DURABILITY`
    set to allowed with `NO_WAIT`. An additional and important step to this test is
    to set the backup of the database to `NUL`, which is similar to the command `with
    truncate_only`. This statement discards any inactive logs (when the database is
    in full or bulk-logged recovery mode; for simple recovery mode, this does not
    hold water) and from the point when a full backup of the database is done, any
    inactive log records get discarded (deleted). This could be simulated. When a
    checkpoint runs, the attempt to back up the log will result in an error message.
    In other words, a database could be running in simple recovery mode. Essentially,
    the `NUL` command simply stores and discards the log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'I will create a sample table to do the inserts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Having created a table, we can now test two types of insert, with and without
    delayed durability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And the result is obvious: with the delayed durability set to on, one can gain
    in performance when doing so many inserts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00154.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'I could have also used the query stress to simulate several threads, each doing
    the same number of inserts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00155.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And with such heavy stress tool testing, the question is, How can we monitor
    and track the behavior of the delayed durability? One can test the performance
    with the performance monitor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, you can test the performance with the activity monitor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00157.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: But soon you will realize that either you need to store this information for
    later analysis or you need to get some additional know-how as to, which performance
    pointers or extended events are worth monitoring in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this case, you will need to check the wait resources on logging in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You will also need to either add some mechanism to capture those wait statistics
    in a table for later analysis or capture the performance monitor or use profiler,
    XE, and others. Querying data in runtime and capturing statistics is rather tedious
    job; imagine merging statistics from `sys.dm_os_wait_stats` and combining them
    with `sys.dm_io_virtual_file_stats`. All in all the, more data you try to gather,
    the more complicated querying these statistics might becomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Capturing with performance monitor of both queries from earlier, the picture
    looks as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The above screenshot shows, on the left-hand side (1), how delayed durability
    is working and how log flushing is happening in a sequential period of time. Comparing
    this to the right-hand side (2), we can see how delayed durability is turned off
    and log flushing is not active.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting raw data from the performance monitor may not be the right approach,
    but storing the same set of data through extended events will be much more lightweight
    on the system, as well as the users, for later analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up the extended event you will need for your analysis can be done fast
    and easily. But rather than choosing too many events, focus on the ones that are
    really needed, because, the log file might get big very quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After starting the event, read the content of the file by breaking down the
    XML structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, getting the information out of XML is another important task to tackle
    extended events correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Coming back to the starting point, gathering data for DBAs and further analysis
    is of the utmost importance. One thing can also be seen from this example: if
    we also add to log file growth, one of the logs needs to grow additionally by
    adding new VLF files. Adding delayed durability gives faster inserts as compared
    to transactions with delayed durability turned off. Sometimes adding new `XE`
    or measures can dramatically increase the logging file, where the data is being
    gathered. Using statistical analysis, we can optimize measure selection or later
    find that they give us additional insight information. Working on exploring and
    later analyzing data can give you a huge pay-off, in terms of workloads and in
    terms of different data gathered.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and analyzing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a similar way, gathering data using event features can give you a rich way
    to a lot of system information data. Deriving from the previous sample, with the
    following demo, we will see how measures of a server can be used for advanced
    statistical analyses and how to help reduce the amount of different information,
    and pin-point the relevant measures. A specific database and a stage table will
    be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, import the measure that can be found in the accompanying code file. There
    are 433 measuring points from 32 different extended events for the purpose of
    understanding the server and its environment settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the initial load, the table will be populated with measures of different
    extended events that have also been discretized and cleaned for further data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The `boxplot` function enables users to explore the distribution of each of
    the measures and find potential outliers. Use only R code to explore the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph gives a quick overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00161.gif)'
  prefs: []
  type: TYPE_IMG
- en: Boxplot shows that there are four events, where the value is vastly exceeding
    the average and third quartile. Cleaning these outliers will make the data easier
    to read and not result in abnormal distribution and skewed results. Note that
    there are particular analyses that deal with outliers and searching for such values.
    For this demo, we will recode these values into N/A.
  prefs: []
  type: TYPE_NORMAL
- en: 'After cleaning, adding summary statistics and a correlation is a relevant way
    to see how all of the events are correlating with one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A correlation matrix of server features is a nice way to represent which events
    are correlating and which are not, and how:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00162.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Going a step further, let's reduce these extended event measures, because it
    is obvious that not all are playing an important role and some might be just overhead.
    From the preceding heat map, it is hard to see which correlations for certain
    measures are not working out; therefore, we will use factor analysis. This observes
    correlations among the variables in such a way that it reflects the lower number
    of underlying variables. A factor is an underlying variable, that is, a latent
    variable that gets structured based on the observed and correlated variables.
    Structure is created by each of the factors being loaded with the response of
    correlated variable. This means that factor 1 can be, for example, loaded 25%
    with variable `A`, 65% with variable `B`, and 10% with variable `C`. So factor
    1 will take the majority (65%) of the features from variable `A` and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this manner, factor analysis will try to reduce the number of original correlated
    variables (our measures of extended events) and try to create new structured variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring data with R code, a simple exploratory factor analysis can reveal
    a number of factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screen plot reveals that there are seven factors available for
    extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following way, this will also reveal the loadings for the factors themselves;
    simply call the R function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows how loading construct each of the factor.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Storing loadings back into the database for further analysis and factor naming
    are a common practice and given the ability to incorporate factors into any further
    analysis (for example: classification or clustering methods) . Now that we know
    the number of factors, we can store the loadings into the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The results can be interpreted as: the higher the value (positive or negative),
    the more loaded is a particular measure is with the accompanying factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Factor 1, `PA1`, is mostly loaded with XE03 (`0.680`), XE13 (`0.640`), XE18
    (`-0.578`), and XE28 (`0.652`). All four are measuring the transactions of query
    and are as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the negative value is the free space in tempdb (KB) that is negatively
    loaded and it only means the relationship to the factor. But having the ability
    to reduce the number of extended events, and to have them combined through the
    advanced statistical analysis, is a very neat approach to a potentially complex
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'With loadings in the database, I can additionally represent how these factors
    are being dispersed on a scatterplot. I have exported the results of the preceding
    query to Power BI and used the clustering visual. Furthermore, you can see clusters
    of these factor loadings and the ones that are similar. The red group (to the
    left) is again something that DBAs and data scientists should look at together
    for further examination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating a baseline and workloads, and replaying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the ability to reduce and create new measures that are tailored and adapted
    to your particular server or environment, now we want to understand how the system
    is behaving with all the other parameters unchanged (in Latin, *ceteris paribus*).
    This is the baseline. And with the baseline, we establish what is normal, or in
    other words, what the performance is under normal conditions. A baseline is used
    for comparing what might be or seem abnormal or out of the ordinary. It can also
    serve as a control group for any future tests (this works well especially when
    new patches are rolled out an upgrade of a particular environment/server needs
    to be performed).
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical corporate baseline would be described as follows over a period of
    one day (24 hours) in the form of the number of database requests from users or
    machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00168.gif)'
  prefs: []
  type: TYPE_IMG
- en: When all requests are represented as a breakdown for each of the corporate processes,
    immediate patterns can be seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ERP system usually peak when people are at their workplace-on a normal
    day between 8.00 AM and 5.00 PM with two distinct peaks and a very obvious lunch
    break from 11.00 AM until 1.00 PM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00169.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Adding ETL jobs for daily ERP system maintenance, it is obvious where and when
    DBAs and SysAdmins usually try to squeeze these important jobs and how this is
    also limited and narrated by daily ERP workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00170.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'A warehouse has a completely different behavior pattern, which means that it
    is usually the highest request in the morning hours 4.00 AM and 5.00 AM and somehow
    steady until evening hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00171.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'A bakery, on the contrary, has a reverse request to the database, since the
    majority of their activities are done starting from 9.00 PM until 3.00 AM, so
    that customers get fresh bread in the morning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00172.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, websites can be understood as a constant database request resource
    with relatively slight daily changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'All these can be understood as a daily baseline and of course, things get even
    more complicated when this is projected on a monthly basis. Also, patterns emerge
    immediately. During the weekends (day 7, 14, and 21) the requests are diminished
    and toward the end of the month, the financial cycle needs to be closed; hence,
    there is additional load on the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.gif)'
  prefs: []
  type: TYPE_IMG
- en: Showing all these is crucial to understand how the baseline of the system (or
    environment) must be understood. This data can be collected using performance
    counters, many of the DMV, using the query store, and other tools. What we usually
    gather is what we will be later monitoring and predicting. So choosing wisely
    is the most important task, since through these measures, counters, and values
    you will be defining when your system is healthy and when it is not. But usually
    general information on the system and database is crucial. Also, SQL Server information,
    and many configurable parameters, query-related information, and database-I/O,
    and RAM-related information need to be stored as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'After having a baseline, we need to have the workload created. Usually, the
    workload is captured against the baseline in the production server, and a restore
    of capture statistics is replayed on the test server/environment. Database tuning
    or changes in configuration can be alternated by replaying the captured workload
    from production on the test environment, by alternatively changing the values
    of a particular parameter. The next demo is a representation of the workload expressed
    through two parameters that have been changed when the same workload has been
    replied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Querying the table `[dbo].[WLD]` is essentially just repeating the same workloads
    with changes on one or another parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00175.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we need to have the outlier analysis against the workload each time
    something has changed. T-SQL code with R can deliver a *Mahalanobis* graph, which
    clearly shows where the outliers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph was inserted into Power BI, where the workloads can be changed against
    both of the parameters. So DBAs cannot only change the workloads, they can also
    see which outliers have caused and needed extra attention when performing a replay
    on the restored workload:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'ANOVA or MANOVA can also be performed to see specific changes among the workloads.
    R code can do just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And the ANOVA statistics show the differences among the workloads and their
    changes in parameter settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00177.gif)'
  prefs: []
  type: TYPE_IMG
- en: Creating predictions with R - disk usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictions involve spotting any unplanned and unwanted activities or unusual
    system behavior, especially when compared it to the baseline. In this manner,
    raising a red flag would result in fewer false positive states.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we always come across disk-size problems. Based on this problem,
    we will demo database growth, store the data, and then run predictions against
    the collected data to be able at the end to predict when a DBA can expect disk
    space problems.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this scenario, I will create a small database of 8 MB and no possibility
    of growth. I will create two tables. One will serve as a baseline, `DataPack_Info_SMALL`,
    and the other will serve as a so-called everyday log, where everything will be
    stored for unexpected cases or undesired behavior. This will persist in the `DataPack_Info_LARGE`
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DataPack` table will serve as a storage place for all the generated inserts
    and later deletes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Populating the `DataPack` table will be done with the following simple `WHILE`
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Capturing disk space changes with the following query will be very important
    for the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Log` table will be filled along with the `DataPack` table in order to
    gather immediate changes in disk space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will serve as our baseline when comparing the results. When we query the
    `DataPack_Log_Small` table, the results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we get a strong and positive correlation between the `RowCounts`
    and `UsedSpaceKB` columns. This can easily be interpreted as: when the value for
    `RowCounts` goes up, the value for `UsedSpaceKB` also goes up. This is the only
    logical explanation. It would be somehow strange to have a negative correlation.
    Now, we will try to simulate random deletes and inserts and observe a similar
    behavior with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We have added a `DELETE` statement, as well as `RowCounts`, so that the demo
    will not be so straightforward. By calculating the correlation coefficient, it
    is obvious, that we again get a very strong and positive correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now compare our `LARGE` test with the baseline by running the same
    correlation coefficient on different datasets. The first is on our baseline (`DataPack_Info_SMALL`)
    and the second one is from our test table (`DataPack_Info_LARGE`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are very interesting. The baseline shows no correlation between
    `UsedSpaceKB` and `UnusedSpaceKB` (it is `-0.049`), whereas our test shows an
    almost 3x stronger negative correlation (it is `-0.109`). A couple of words on
    this correlation: this shows that `UsedSpaceKB` is negatively correlated with
    `UnUsedSpaceKB`; this which is still too small to draw any concrete conclusions,
    but it shows how a slight change can cause a difference in a simple correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: You can gather disk space usage information with T-SQL, by using PowerShell,
    by implementing .NET assembly, or creating a SQL Server job, or any other way.
    The important part and the biggest advantage is that, using R and the data collected,
    now you will not only be monitoring and reacting to the past data, but you will
    also be able to predict what will happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go a step further and assume the following query and dataset taken from
    our sample created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We will give a prediction on the size of the `usedSpaceKB` based on historical
    data. Our input will be `TableName`, `Operation`, and `NofRowsOperation` for a
    given number to predict on. I will be using a general linear model (the GLM algorithm)
    for predicting `usedDiskSpace`! Before you all start saying this is absurd, this
    cannot be done due to DBCC caching, page brakes, indexes, stall statistics, and
    many other parameters, I would like to point out that all this information can
    be added into the algorithm and would make the prediction even better. Since my
    queries are very simple `INSERT` and `DELETE` statements, you should also know
    what kinds of queries are you predicting. In addition, such an approach can be
    good for code testing, unit testing, and stress testing before deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following R code, we can start creating predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can predict the size of `UsedSpaceKB` based on the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a few things to clear out first. The following R code with the `xp_execute_external_script`
    would work much better as a stored procedure with input parameters for these columns:
    `TableName`, `Operation`, `NofRowsOperation`, and `UnusedSpaceKB`. Furthermore,
    to avoid unnecessary computational time for model building, it is usually the
    practice to store a serialized model in a SQL table and just deserialize it when
    running predictions. At last, since this was just a demo, make sure that the numbers
    used in predictions make sense, As we saw in our example, the `UsedSpaceKB` would
    be predicted much better if absolutely calculated, rather than using the cumulative
    values. Only later is the cumulative value calculated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up this rather long demo, let''s create a procedure and run some predictions
    to see how efficient this is. The stored procedure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to run the procedure two times in a row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Both predictions on used space disk are based on our demo data but can be used
    on a larger scale and for predictions as well. Of course, for even better predictions,
    some baseline statistics could also be included. With every model, we also need
    to test the predictions to see how good they are.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using SQL Server R for any kind of DBA task, as we have seen here, it is not
    always hardcore statistics or predictive analytics; we might also be some simple
    statistical understanding underlying the connection and relationships between
    the attribute's queries, gathered statistics, and indexes. Prognosing and predicting,
    for example, information from execution plans in order to prepare a better understanding
    of the query of cover missing index, is a crucial point. Parameter sniffing or
    a cardinality estimator would also be a great task to tackle along the usual statistics.
  prefs: []
  type: TYPE_NORMAL
- en: But we have seen that predicting events that are usually only monitored can
    be a huge advantage for a DBA and a very welcome feature for core systems.
  prefs: []
  type: TYPE_NORMAL
- en: With R integration into SQL Server, such daily, weekly, or monthly tasks can
    be automated to different, before not uses yet, extent. And as such, it can help
    give different insight to DBAs and also people responsible for system maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: In next chapter, we will be covering extending features beyond R external procedure
    and how to use them.
  prefs: []
  type: TYPE_NORMAL
