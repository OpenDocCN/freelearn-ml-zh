- en: Authorship Attribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Authorship analysis** is a text mining task that aims to identify certain
    aspects about an author, based only on the content of their writings. This could
    include characteristics such as age, gender, or background. In the specific **authorship
    attribution** task, we aim to identify which of a set of authors wrote a particular
    document. This is a classic classification task. In many ways, authorship analysis
    tasks are performed using standard data mining methodologies, such as cross-fold
    validation, feature extraction, and classification algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the problem of authorship attribution to piece
    together the parts of the data mining methodology we developed in the previous
    chapters. We identify the problem and discuss the background and knowledge of
    the problem. This lets us choose features to extract, which we will build a pipeline
    for achieving. We will test two different types of features: function words and
    character n -grams. Finally, we will perform an in-depth analysis of the results.
    We will work first with a dataset of books, and then a messy, real-world corpus
    of e-mails.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and how feature choice differs based on application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting the bag-of-words model with a specific goal in mind
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature types and the character n-grams model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning up a messy dataset for data mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attributing documents to authors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorship analysis has a background in **stylometry**, which is the study of
    an author's style of writing. The concept is based on the idea that everyone learns
    language slightly differently, and that measuring these nuances in people's writing
    will enable us to tell them apart using only the content of their writing.
  prefs: []
  type: TYPE_NORMAL
- en: Authorship analysis has historically (pre-1990) been performed using repeatable
    manual analysis and statistics, which is a good indication that it could be automated
    with data mining. Modern authorship analysis studies are almost entirely data
    mining-based, although quite a significant amount of work is still done with more
    manually driven analysis using linguistic styles and stylometrics. Many of the
    advances in feature engineering today are driven by advances in stylometrics.
    In other words, manual analysis discovers new features, which are then codified
    and used as part of the data mining process.
  prefs: []
  type: TYPE_NORMAL
- en: A key underlying feature of stylometry is that of **writer invariants**, which
    are features that a particular author has in all of their documents, but are not
    shared with other authors. In practice these writer invariants do not seem to
    exist, as authorship styles change over time, but the use of data mining can get
    us close to classifiers working off this principle.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a field, authorship analysis has many sub-problems, and the main ones are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authorship profiling:** This determines the age, gender, or other traits
    of the author based on the writing. For example, we can detect the first language
    of a person speaking English by looking for specific ways in which they speak
    the language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorship verification:** This checks whether the author of this document
    also wrote the other document. This problem is what you would normally think about
    in a legal court setting. For instance, the suspect''s writing style (content-wise)
    would be analyzed to see if it matched the ransom note.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorship clustering:** This is an extension of authorship verification,
    where we use cluster analysis to group documents from a big set into clusters,
    and each cluster is written by the same author.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the most common form of authorship analysis study is that of **authorship
    attribution**, a classification task where we attempt to predict which of a set
    of authors wrote a given document.
  prefs: []
  type: TYPE_NORMAL
- en: Applications and use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorship analysis has a number of **use cases**. Many use cases are concerned
    with problems such as verifying authorship, proving shared authorship/provenance,
    or linking social media profiles with real-world users.
  prefs: []
  type: TYPE_NORMAL
- en: In a historical sense, we can use authorship analysis to verify whether certain
    documents were indeed written by their supposed authors. Controversial authorship
    claims include some of Shakespeare's plays, the Federalist papers from the USA's
    foundation period, and other historical texts.
  prefs: []
  type: TYPE_NORMAL
- en: Authorship studies alone cannot prove authorship but can provide evidence for
    or against a given theory, such as whether a particular person wrote a given document.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can analyze Shakespeare's plays to determine his writing style,
    before testing whether a given sonnet actually does originate from him (some recent
    research indicates multiple authorship of some of his work).
  prefs: []
  type: TYPE_NORMAL
- en: A more modern use case is that of linking social network accounts. For example,
    a malicious online user could set up accounts on multiple online social networks.
    Being able to link them allows authorities to track down the user of a given account—for
    example if a person is harassing other online users.
  prefs: []
  type: TYPE_NORMAL
- en: Another example used in the past is to be a backbone to provide expert testimony
    in court to determine whether a given person wrote a document. For instance, the
    suspect could be accused of writing an e-mail harassing another person. The use
    of authorship analysis could determine whether it is likely that person did, in
    fact, write the document. Another court-based use is to settle claims of stolen
    authorship. For example, two authors may claim to have written a book, and authorship
    analysis could provide evidence on which is the more likely author.
  prefs: []
  type: TYPE_NORMAL
- en: Authorship analysis is not foolproof, though. A recent study found that attributing
    documents to authors can be made considerably harder by simply asking people,
    who are otherwise untrained, to hide their writing style. This study also looked
    at a framing exercise where people were asked to write in the style of another
    person. This framing of another person proved quite reliable, with the faked document
    commonly attributed to the person being framed.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these issues, authorship analysis is proving useful in a growing number
    of areas and is an interesting data mining problem to investigate.
  prefs: []
  type: TYPE_NORMAL
- en: Authorship attribution can be used in expert testimony, but by itself is hard
    to classify as hard evidence. Always check with a lawyer before using it for formal
    matters, such as authorship disputes.
  prefs: []
  type: TYPE_NORMAL
- en: Authorship attribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Authorship attribution** (as distinct from authorship *analysis*) is a classification
    task by which we have a set of candidate authors, a set of documents from each
    of those authors namely the **training set**, and a set of documents of unknown
    authorship otherwise known as the test set. If the documents of unknown authorship
    definitely belong to one of the candidates, we call this a closed problem, as
    per the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we cannot be sure of that the actual author is part of the training set,
    we call this an open problem. This distinction isn''t just specific to authorship
    attribution - any data mining application where the actual class may not be in
    the training set is considered an open problem, with the task being to find the
    candidate author or to select none of them. This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In authorship attribution, we typically have two restrictions on the tasks.
    They have been listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we only use content information from the documents - not metadata regarding
    the time of writing, delivery, handwriting style, and so on. There are ways to
    combine models from these different types of information, but that isn't generally
    considered authorship attribution and is more a **data fusion** application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second restriction is that we don't look at the topic of the documents;
    instead, we look for more salient features such as word usage, punctuation, and
    other text-based features. The reasoning here is that a person can write on many
    different topics, so worrying about the topic of their writing isn't going to
    model their actual authorship style. Looking at topic words can also lead to **overfitting**
    on the training data—our model may train on documents from the same author and
    also on the same topic. For instance, if you were to model my authorship style
    by looking at this book, you might conclude the words *data mining* are indicative
    of *my* writing style when, in fact, I write on other topics as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From here, the pipeline for performing authorship attribution looks a lot like
    the one we developed in [Chapter 6](lrn-dtmn-py-2e_ch06.html)*, Social Media Insight
    Using Naive Bayes*.
  prefs: []
  type: TYPE_NORMAL
- en: First, we extract features from our text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we perform some feature selection on those features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we train a classification algorithm to fit a model, which we can then
    use to predict the class (in this case, the author) of a document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are some differences between classifying content and classifying authorship,
    mostly having to do with which features are used, that we will cover in this chapter.
    It is critical to choose features based on the application.
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve into these issue, we will define the scope of the problem and
    collect some data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data we will use for the first part of this chapter is a set of books from
    **Project Gutenberg** at [www.gutenberg.org](http://www.gutenberg.org), which
    is a repository of public domain literature works. The books I used for these
    experiments come from a variety of authors:'
  prefs: []
  type: TYPE_NORMAL
- en: Booth Tarkington (22 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charles Dickens (44 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edith Nesbit (10 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arthur Conan Doyle (51 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mark Twain (29 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sir Richard Francis Burton (11 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emile Gaboriau (10 titles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, there are 177 documents from 7 authors, giving a significant amount
    of text to work with. A full list of the titles, along with download links and
    a script to automatically fetch them, is given in the code bundle called getdata.py.
    If running the code results in significantly fewer books than above, the mirror
    may be down. See this website for more mirror URLs to try in the script: [https://www.gutenberg.org/MIRRORS.ALL](https://www.gutenberg.org/MIRRORS.ALL)
  prefs: []
  type: TYPE_NORMAL
- en: To download these books, we use the requests library to download the files into
    our data directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in a new Jupyter Notebook, set up the data directory and ensure the
    following code links to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, download the data bundle from the code bundle supplied by Packt. Decompress
    the file into this directory. The books folder should then directly contain one
    folder for each author.
  prefs: []
  type: TYPE_NORMAL
- en: After taking a look at these files, you will see that many of them are quite
    messy—at least from a data analysis point of view. There is a large project Gutenberg
    disclaimer at the start of the files. This needs to be removed before we do our
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, most books begin with information such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The* *Project Gutenberg eBook of Mugby Junction, by Charles Dickens, et al, **Illustrated
    by Jules A.'
  prefs: []
  type: TYPE_NORMAL
- en: Goodman This eBook is for the use of anyone anywhere at no cost and with*
  prefs: []
  type: TYPE_NORMAL
- en: '*almost no restrictions whatsoever. You may copy it, give it away or*'
  prefs: []
  type: TYPE_NORMAL
- en: '*re-use it under the terms of the Project Gutenberg License included*'
  prefs: []
  type: TYPE_NORMAL
- en: '*with this eBook or online at www.gutenberg.org*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Title: Mugby Junction*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Author: Charles Dickens*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Release Date: January 28, 2009 [eBook #27924]Language: English*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Character set encoding: UTF-8'
  prefs: []
  type: TYPE_NORMAL
- en: '***START OF THE PROJECT GUTENBERG EBOOK MUGBY JUNCTION****'
  prefs: []
  type: TYPE_NORMAL
- en: After this point, the actual text of the book starts. The use of a line starting
    ***START OF THE PROJECT GUTENBERG is fairly consistent, and we will use that as
    a cue on when the text starts - anything before this line will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could alter the individual files on disk to remove this stuff. However,
    what happens if we were to lose our data? We would lose our changes and potentially
    be unable to replicate the study. For that reason, we will perform the preprocessing
    as we load the files—this allows us to be sure our results will be replicable
    (as long as the data source stays the same). The following code removes the main
    source of noise from the books, which is the prelude that Project Gutenberg adds
    to the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You may want to add to this function to remove other sources of noise, such
    as inconsistent formatting, footer information, and so on. Investigate the files
    to examine what issues they have.
  prefs: []
  type: TYPE_NORMAL
- en: We can now get our documents and classes using the following function, which
    loops through these folders, loads the text documents and records a number assigned
    to the author as the target class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call this function to actually load the books:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This dataset fits into memory quite easily, so we can load all of the text at
    once. In cases where the whole dataset doesn't fit, a better solution is to extract
    the features from each document one-at-a-time (or in batches) and save the resulting
    values to a file or in-memory matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a gauge on the properties of the data, one of the first things I usually
    do is create a simple histogram of the document lengths. If the lengths are relatively
    consistent, this is often easier to learn from than wildly different document
    lengths. In this case, there is quite a large variance in document lengths. To
    view this, first we extract the lengths into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we plot those. Matplotlib has a `hist` function that will do this, as
    does Seaborn, which produces nicer looking graphs by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting graph shows the variation in document lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Using function words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the earliest types of features, and one that still works quite well for
    authorship analysis, is to use function words in a bag-of-words model. Function
    words are words that have little meaning on their own, but are required for creating
    (English!) sentences. For example, the words *this* and *which* are words that
    are really only defined by what they do within a sentence, rather than their meaning
    in themselves. Contrast this with a content word such as *tiger*, which has an
    explicit meaning and invokes imagery of a large cat when used in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: The set of words that are considered function words is not always obvious. A
    good rule of thumb is to choose the most frequent words in usage (over all possible
    documents, not just ones from the same author).
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the more frequently a word is used, the better it is for authorship
    analysis. In contrast, the less frequently a word is used, the better it is for
    content-based text mining, such as in the next chapter, where we look at the topic
    of different documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph here gives a better idea between word and frequency relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: The use of function words is less defined by the content of the document and
    more by the decisions made by the author. This makes them good candidates for
    separating the authorship traits between different users. For instance, while
    many Americans are particular about the different in usage between *that* and
    *which* in a sentence, people from other countries, such as Australia, are less
    concerned with the distinction. This means that some Australians will lean towards
    almost exclusively using one word or the other,  while others may use *which*
    much more.
  prefs: []
  type: TYPE_NORMAL
- en: This difference, combined with thousands of other nuanced differences, makes
    a model of authorship.
  prefs: []
  type: TYPE_NORMAL
- en: Counting function words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can count function words using the CountVectorizer class we used in [Chapter
    6](lrn-dtmn-py-2e_ch06.html)*, Social Media Insight Using Naive Bayes*. This class
    can be passed a vocabulary, which is the set of words it will look for. If a vocabulary
    is not passed (we didn't pass one in the code of [Chapter 6](lrn-dtmn-py-2e_ch06.html)*,
    Social Media Insight Using Naive Bayes*), then it will learn this vocabulary from
    the training dataset. All the words are in the training set of documents (depending
    on the other parameters of course).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set up our vocabulary of function words, which is just a list containing
    each of them. Exactly which words are function words and which are not is up for
    debate. I''ve found the following list, from published research, to be quite good,
    obtained from my own research combining word lists from other researchers. Remember
    that the code bundle is available from Packt publishing (or the official github
    channel), and therefore you don''t need to type this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can set up an extractor to get the counts of these function words. Note
    the passing of the function words list as the `vocabulary` into the `CountVectorizer`
    initialiser.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For this set of function words, the frequency within these documents is very
    high - as you would expect. We can use the extractor instance to obtain these
    counts, by fitting it on the data, and then calling `transform` (or, the shortcut
    using `fit_transform`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Before plotting, we normalized these counts by dividing by the relevant document
    lengths. The following code does this, resulting in the percentage of words accounted
    for by each function word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then average these percentages across all documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Finally we plot them using Matplotlib (Seaborn lacks easy interfaces to basic
    plots like this).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Classifying with function words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The only new thing here is the use of **Support Vector Machines** (**SVM**),
    which we will cover in the next section (for now, just consider it a standard
    classification algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we import our classes. We import the SVC class, an SVM for classification,
    as well as the other standard workflow tools we have seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'SVMs take a number of parameters. As I said, we will use one blindly here,
    before going into detail in the next section. We then use a dictionary to set
    which parameters we are going to search. For the `kernel` parameter, we will try
    `linear` and `rbf`. For C, we will try values of 1 and 10 (descriptions of these
    parameters are covered in the next section). We then create a grid search to search
    these parameters for the best choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Gaussian kernels (such as RBF) only work for reasonably sized data sets, such
    as when the number of features is fewer than about 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set up a pipeline that takes the feature extraction step using the
    `CountVectorizer` (only using function words), along with our grid search using
    SVM. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, apply `cross_val_score` to get our cross-validated score for this pipeline.
    The result is 0.811, which means we approximately get 80 percent of the predictions
    correct.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVMs are classification algorithms based on a simple and intuitive idea, backed
    by some complex and innovative mathematics. SVMs perform classification between
    two classes (although we can extend it to more classes using various meta-algorithms),
    by simply drawing a separating line between the two (or a hyperplane in higher-dimensions).
    The intuitive idea is to choose the best line of separation, rather than just
    any specific line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that our two classes can be separated by a line such that any points
    above the line belong to one class and any below the line belong to the other
    class. SVMs find this line and use it for prediction, much the same way as linear
    regression works. SVMs, however, find the best line for separating the dataset.
    In the following figure, we have three lines that separate the dataset: blue,
    black, and green. Which would you say is the best option?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, a person would normally choose the blue line as the best option,
    as this separates the data in the cleanest way. More formally, it has the maximum
    distance from the line to any point in each class. Finding this line of maximum
    separation is an optimization problem, based on finding the lines of margin with
    the maximum distance between them. Solving this optimisation problem is the main
    task of the training phase of an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equations to solve SVMs is outside the scope of this book, but I recommend
    interested readers to go through the derivations at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikibooks.org/wiki/Support_Vector_Machines](http://en.wikibooks.org/wiki/Support_Vector_Machines)
    for the details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html](http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying with SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training the model, we have a line of maximum margin. The classification
    of new samples is then simply asking the question: does it fall above the line,
    or below it? If it falls above the line, it is predicted as one class. If it is
    below the line, it is predicted as the other class.'
  prefs: []
  type: TYPE_NORMAL
- en: For multiple classes, we create multiple SVMs—each a binary classifier. We then
    connect them using any one of a variety of strategies. A basic strategy is to
    create a one-versus-all classifier for each class, where we train using two classes—the
    given class and all other samples. We do this for each class and run each classifier
    on a new sample, choosing the best match from each of these. This process is performed
    automatically in most SVM implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw two parameters in our previous code: **C** and kernel. We will cover
    the kernel parameter in the next section, but the **C** parameter is an important
    parameter for fitting SVMs. The **C** parameter relates to how much the classifier
    should aim to predict all training samples correctly, at the risk of overfitting.
    Selecting a higher **C** value will find a line of separation with a smaller margin,
    aiming to classify all training samples correctly. Choosing a lower **C** value
    will result in a line of separation with a larger margin—even if that means that
    some training samples are incorrectly classified. In this case, a lower **C**
    value presents a lower chance of overfitting, at the risk of choosing a generally
    poorer line of separation'
  prefs: []
  type: TYPE_NORMAL
- en: One limitation with SVMs (in their basic form) is that they only separate data
    that is linearly separable. What happens if the data isn't? For that problem,
    we use kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the data cannot be separated linearly, the trick is to embed it on to a
    higher dimensional space. What this means, with a lot of hand-waving about the
    details, is to add new features to the dataset until the data is linearly separable.
    If you add the right kinds of features, this linear separation will always, eventually,
    happen.
  prefs: []
  type: TYPE_NORMAL
- en: The trick is that we often compute the inner-produce of the samples when finding
    the best line to separate the dataset. Given a function that uses the dot product,
    we effectively manufacture new features without having to actually define those
    new features. This is known as the kernel trick and is handy because we don't
    know what those features were going to be anyway. We now define a kernel as a
    function that itself is the dot product of the function of two samples from the
    dataset, rather than based on the samples (and the made-up features) themselves.
  prefs: []
  type: TYPE_NORMAL
- en: We can now compute what that dot product is (or approximate it) and then just
    use that.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of kernels in common use. The **linear kernel** is the most
    straightforward and is simply the dot product of the two sample feature vectors,
    the weight feature, and a bias value. There is also a **polynomial kernel**, which
    raises the dot product to a given degree (for instance, 2). Others include the
    **Gaussian** (**rbf**) and **Sigmoidal** functions. In our previous code sample,
    we tested between the **linear** kernel and the **rbf** kernel options.
  prefs: []
  type: TYPE_NORMAL
- en: The end result from all this derivation is that these kernels effectively define
    a distance between two samples that is used in the classification of new samples
    in SVMs. In theory, any distance could be used, although it may not share the
    same characteristics that enable easy optimization of the SVM training.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn's implementation of SVMs, we can define the kernel parameter
    to change which kernel function is used in computations, as we saw in the previous
    code sample.
  prefs: []
  type: TYPE_NORMAL
- en: Character n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw how function words can be used as features to predict the author of a
    document. Another feature type is character n-grams. An n-gram is a sequence of
    *n* tokens, where *n* is a value (for text, generally between 2 and 6). Word n-grams
    have been used in many studies, usually relating to the topic of the documents
    - as per the previous chapter. However, character n-grams have proven to be of
    high quality for authorship attribution.
  prefs: []
  type: TYPE_NORMAL
- en: Character n-grams are found in text documents by representing the document as
    a sequence of characters. These n-grams are then extracted from this sequence
    and a model is trained. There are a number of different models for this, but a
    standard one is very similar to the bag-of-words model we have used earlier.
  prefs: []
  type: TYPE_NORMAL
- en: For each distinct n-gram in the training corpus, we create a feature for it.
    An example of an n-gram is `<e t>`, which is the letter e, space, and then the
    letter t (the angle brackets are used to denote the start and end of the n-gram
    and are not part of the n-gram itself). We then train our model using the frequency
    of each n-gram in the training documents and train the classifier using the created
    feature matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Character n-grams are defined in many ways. For instance, some applications
    only choose within-word characters, ignoring whitespace and punctuation. Some
    use this information (like our implementation in this chapter) for classification.
    Ultimately, this is the purpose of the model, chosen by the data miner (you!).
  prefs: []
  type: TYPE_NORMAL
- en: A common theory for why character n-grams work is that people more typically
    write words they can easily say and character n-grams (at least when n is between
    2 and 6) are a good approximation for **phonemes**—the sounds we make when saying
    words. In this sense, using character n-grams approximates the sounds of words,
    which approximates your writing style. This is a common pattern when creating
    new features. First, we have a theory on what concepts will impact the end result
    (authorship style) and then create features to approximate or measure those concepts.
  prefs: []
  type: TYPE_NORMAL
- en: One key feature of a character n-gram matrix is that it is sparse and increases
    in sparsity with higher n-values quite quickly. For an n-value of 2, approximately
    75 percent of our feature matrix is zeros. For an n-value of 5, over 93 percent
    is zeros. This is typically less sparse than a word n-gram matrix of the same
    type though and shouldn't cause many issues using a classifier that is used for
    word-based classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting character n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to use our `CountVectorizer` class to extract character n-grams.
    To do that, we set the analyzer parameter and specify a value for n to extract
    n-grams with.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation in scikit-learn uses an n-gram range, allowing you to extract
    n-grams of multiple sizes at the same time. We won't delve into different n-values
    in this experiment, so we just set the values the same. To extract n-grams of
    size 3, you need to specify (3, 3) as the value for the n-gram range.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reuse the grid search from our previous code. All we need to do is specify
    the new feature extractor in a new pipeline and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot of implicit overlap between function words and character n-grams,
    as character sequences in function words are more likely to appear. However, the
    actual features are very different and character n-grams capture punctuation,
    a characteristic that function words do not capture. For example, a character
    n-gram includes the full stop at the end of a sentence, while a function word-based
    method would only use the preceding word itself.
  prefs: []
  type: TYPE_NORMAL
- en: The Enron dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enron was one of the largest energy companies in the world in the late 1990s,
    reporting revenue over $100 billion. It had over 20,000 staff and—as of the year
    2000—there seemed to be no indications that something was very wrong.
  prefs: []
  type: TYPE_NORMAL
- en: In 2001, the *Enron Scandal* occurred, where it was discovered that Enron was
    undertaking systematic, fraudulent accounting practices. This fraud was deliberate,
    wide-ranging across the company, and for significant amounts of money. After this
    was publicly discovered, its share price dropped from more than $90 in 2000 to
    less than $1 in 2001\. Enron shortly filed for bankruptcy in a mess that would
    take more than 5 years to finally be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: As part of the investigation into Enron, the Federal Energy Regulatory Commission
    in the United States made more than 600,000 e-mails publicly available. Since
    then, this dataset has been used for research into everything from social network
    analysis to fraud analysis. It is also a great dataset for authorship analysis,
    as we are able to extract e-mails from the sent folder of individual users. This
    allows us to create a dataset much larger than many previous datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Enron dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full set of Enron emails is available at [https://www.cs.cmu.edu/~./enron/](https://www.cs.cmu.edu/~./enron/)
  prefs: []
  type: TYPE_NORMAL
- en: The full dataset is quite large, and provided in a compression format called
    gzip. If you don't have a Linux-based machine to decompress (unzip) this file,
    get an alternative program, such as 7-zip ([http://www.7-zip.org/](http://www.7-zip.org/))
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the full corpus and decompress it into your data folder. By default,
    this will decompress into a folder called `enron_mail_20110402` which then contains
    a folder called `maildir`. In the Notebook, setup the data folder for the Enron
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Creating a dataset loader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we are looking for authorship information, we only want the e-mails we can
    attribute to a specific author. For that reason, we will look in each user''s
    sent folder—that is, emails they have sent. We can now create a function that
    will choose a couple of authors at random and return each of the emails in their
    sent folder. Specifically, we are looking for the payloads—that is, the content
    rather than the e-mails themselves. For that, we will need an e-mail parser. The
    code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We will be using this later to extract the payloads from the e-mail files that
    are in the data folder.
  prefs: []
  type: TYPE_NORMAL
- en: With our data loading function, we are going to have a lot of options. Most
    of these ensure that our dataset is relatively balanced. Some authors will have
    thousands of e-mails in their sent mail, while others will have only a few dozen.
    We limit our search to only authors with at least 10 e-mails using `min_docs_author`
    and take a maximum of 100 e-mails from each author using the `max_docs_author`
    parameter. We also specify how many authors we want to get—10 by default using
    the `num_authors` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The function is below. Its main purpose is to loop through the authors, retrieve
    a number of emails for that author, and store the **document** and **class** information
    in some lists. We also store the mapping between an author's name and their numerical
    class value, which lets us retrieve that information later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It may seem odd that we sort the e-mail addresses, only to shuffle them around.
    The `os.listdir` function doesn't always return the same results, so we sort it
    first to get some stability. We then shuffle using a random state, which means
    our shuffling can reproduce a past result if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outside of this function, we can now get a dataset by making the following
    function call. We are going to use a random state of 14 here (as always in this
    book), but you can try other values or set it to none to get a random set each
    time the function is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have a look at the dataset, there is still a further preprocessing set
    we need to undertake. Our e-mails are quite messy, but one of the worst bits (from
    an authorship analysis perspective) is that these e-mails contain writings from
    other authors, in the form of attached replies. Take the following email, which
    is `documents[100]`, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I would like to be on the panel but I have on a conflict on the conference*'
  prefs: []
  type: TYPE_NORMAL
- en: '*dates. Please keep me in mind for next year.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mark Haedicke*'
  prefs: []
  type: TYPE_NORMAL
- en: Email is a notoriously messy format. Reply quoting, for instance, is sometimes
    (but not always) prepended with a > character. Other times, the reply is embedded
    in the original message. If you are doing larger scale data mining with email,
    be sure to spend more time cleaning the data to get better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the books dataset, we can plot the histogram of document lengths to
    get a sense of the document length distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The result appears to show a strong grouping around shorter documents. While
    this is true, it also shows that some documents are very, very long. This may
    skew the results, particularly if some authors are prone to writing long documents.
    To compensate for this, one extension to this work may be to normalise document
    lengths to the first 500 characters before doing the training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use the existing parameter space and the existing classifier from our
    previous experiments—all we need to do is refit it on our new data. By default,
    training in scikit-learn is done from scratch—subsequent calls to `fit()` will
    discard any previous information.
  prefs: []
  type: TYPE_NORMAL
- en: There is a class of algorithms called *online learning* that update the training
    with new samples and don't restart their training each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we can compute our scores by using `cross_val_score` and print the
    results. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The result is 0.683, which is a reasonable result for such a messy dataset.
    Adding more data (such as increasing `max_docs_author` in the dataset loading)
    can improve these results, as will improving the quality of the data with extra
    cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is generally never a good idea to base an assessment on a single number.
    In the case of the f-score, it is usually more robust to *tricks* that give good
    scores despite not being useful. An example of this is accuracy. As we said in
    our previous chapter, a spam classifier could predict everything as being spam
    and get over 80 percent accuracy, although that solution is not useful at all.
    For that reason, it is usually worth going more in-depth on the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we will look at the confusion matrix, as we did in [Chapter
    8](lrn-dtmn-py-2e_ch08.html)*, Beating CAPTCHAs with Neural Networks*. Before
    we can do that, we need to predict a testing set. The previous code uses `cross_val_score`,
    which doesn''t actually give us a trained model we can use. So, we will need to
    refit one. To do that, we need training and testing subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we fit the pipeline to our training documents and create our predictions
    for the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you might be wondering what the best combination of parameters
    actually was. We can extract this quite easily from our grid search object (which
    is the classifier step of our pipeline):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The results give you all of the parameters for the classifier. However, most
    of the parameters are the defaults that we didn't touch. The ones we did search
    for were C and kernel, which were set to 1 and linear, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we get our author''s names, allowing us to that we can label the axis
    correctly. For this purpose, we use the authors dictionary that our Enron dataset
    loaded. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we show the confusion matrix using matplotlib. The only changes from
    the last chapter are highlighted below; just replace the letter labels with the
    authors from this chapter''s experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that authors are predicted correctly in most cases—there is a clear
    diagonal line with high values. There are some large sources of error though (darker
    values are larger): emails from user rapp-b are typically predicted as being from
    reitmeyer-j for instance.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at the text mining-based problem of authorship attribution.
    To perform this, we analyzed two types of features: function words and character
    n-grams. For function words, we were able to use the bag-of-words model—simply
    restricted to a set of words we chose beforehand. This gave us the frequencies
    of only those words. For character n-grams, we used a very similar workflow using
    the same class. However, we changed the analyzer to look at characters and not
    words. In addition, we used n-grams that are sequences of n tokens in a row—in
    our case characters. Word n-grams are also worth testing in some applications,
    as they can provide a cheap way to get the context of how a word is used.'
  prefs: []
  type: TYPE_NORMAL
- en: For classification, we used SVMs that optimize a line of separation between
    the classes based on the idea of finding the maximum margin. Anything above the
    line is one class and anything below the line is another class. As with the other
    classification tasks we have considered, we have a set of samples (in this case,
    our documents).
  prefs: []
  type: TYPE_NORMAL
- en: We then used a very messy dataset, the Enron e-mails. This dataset contains
    lots of artifacts and other issues. This resulted in a lower accuracy than the
    books dataset, which was much cleaner. However, we were able to choose the correct
    author more than half the time, out of 10 possible authors.
  prefs: []
  type: TYPE_NORMAL
- en: To take the concepts in this chapter further, look for new datasets containing
    authorship information. For instance, can you predict the author of a blog post?
    What about the author of a tweet (you may be able to reuse your data from [Chapter
    6](lrn-dtmn-py-2e_ch06.html), *Social Media Insight Using Naive Bayes*)?
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we consider what we can do if we don't have target classes.
    This is called unsupervised learning, an exploratory problem rather than a prediction
    problem. We also continue to deal with messy text-based datasets.
  prefs: []
  type: TYPE_NORMAL
