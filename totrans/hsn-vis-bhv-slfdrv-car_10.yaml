- en: '*Chapter 8*: Behavioral Cloning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to train a neural network to control the steering
    wheel of a car, effectively teaching it how to drive a car! Hopefully, you will
    be surprised by how simple the core of this task is, thanks to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve our goal, we will have to modify one of the examples of the CARLA
    simulator, first to save the images required to create the dataset, then to use
    our neural network to drive. Our neural network will be inspired by the architecture
    of Nvidia DAVE-2, and we will also see how to better visualize where the neural
    network focuses its attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Teaching a neural network how to drive with behavioral cloning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Nvidia DAVE-2 neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recording images and the steering wheel from Carla
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recording three video streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the saliency maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating with Carla for self-driving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training bigger datasets using generators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to use the code explained in this chapter, you need to have installed
    the following tools and modules:'
  prefs: []
  type: TYPE_NORMAL
- en: The Carla simulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `keras-vis` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV-Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GPU (recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Code in Action videos for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3kjIQLA](https://bit.ly/3kjIQLA)'
  prefs: []
  type: TYPE_NORMAL
- en: Teaching a neural network how to drive with behavioral cloning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A self-driving car is a complicated ensemble of hardware and software. The hardware
    of a normal car is already very complex, usually with thousands of mechanical
    pieces, and a self-driving car adds many sensors to that. The software is not
    any simpler, and in fact, rumor has it that already 15 years ago, a world-class
    carmaker had to take a step back, because the complexity of the software was getting
    out of control. To give you an idea, a sports car can have more than 50 CPUs!
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, making a self-driving car that is safe and reasonably fast is an incredible
    challenge, but despite this, we will see how powerful a dozen of lines of code
    can be. For me, it was an enlightening moment to realize that something so complex
    as driving could be coded in such a simple way. But I should not have been surprised
    because, with deep learning, data is more important than the code itself, at least
    to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: We don't have the luxury of testing on a real self-driving car, so we will use
    Carla, and we will train a neural network that can generate the steering angle
    after having been fed the video of the camera. We are not using other sensors,
    though, in principle, you could use all the sensors that you can imagine, just
    modifying the network to accept this additional data.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to teach Carla how to make a *lap*, using a part of the **Town04**
    track, one of the tracks included in Carla. We want our neural network to drive
    a bit straight, and then make some turns to the right until it reaches the initial
    point. In principle, to teach the neural network, we just need to drive Carla,
    recording images of the road and the corresponding steering angle that we applied,
    a process called **behavioral cloning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our task is divided into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Building the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing and training the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating the neural network in Carla
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to take inspiration from the DAVE-2 system, created by Nvidia.
    So, let's start describing it.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DAVE-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DAVE-2 is a system designed by Nvidia to train a neural network to drive a car,
    intended as a proof of concept to demonstrate that, in principle, a single neural
    network could be able to steer a car on a road. Putting it another way, our network
    could be trained to drive a real car on a real road, if enough data is provided.
    To give you an idea, Nvidia used around 72 hours of video, at 10 frames per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is very simple: we feed the neural network a video stream, and the
    neural network will simply generate the steering angle, or something equivalent.
    The training is created by a human driver, and the system collects data from the
    camera (training data) and from the steering wheel moved by the pilot (training
    labels). This is called *behavioral cloning* because the network is trying to
    clone the behavior of the human driver.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this would be a bit too simple, as most of the labels would
    simply be 0 (the driver going straight), so the network would have problems learning
    how to move to the middle of the lane. To alleviate this issue, Nvidia uses three
    cameras:'
  prefs: []
  type: TYPE_NORMAL
- en: One on the center of the car, which is the real human behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One on the left, simulating what to do if the car is too much on the left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One on the right, simulating what to do if the car is too much on the right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the left and right cameras to be useful, it is, of course, necessary to
    change the steering angle associated with their videos, to simulate a correction;
    so, the *left* camera needs to be associated with a turn *more to the right* and
    the *right* camera needs to be associated with a turn *more to the left*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Nvidia DAVE-2 system](img/Figure_8.1_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Nvidia DAVE-2 system
  prefs: []
  type: TYPE_NORMAL
- en: To make the system more robust, Nvidia adds random shift and rotation, adjusting
    the steering for it, but we will not do that. However, we are going to use three
    video streams, as suggested by them.
  prefs: []
  type: TYPE_NORMAL
- en: How do we get the three video streams and the steering angle? Of course, from
    Carla, which we will use quite a lot during this chapter. Before starting to write
    some code, let's get familiar with `manual_control.py`, a file that we will copy
    and modify.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know manual_control.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of writing a full client code to do what we need, we will change the
    `manual_control.py` file, from `PythonAPI/examples`.
  prefs: []
  type: TYPE_NORMAL
- en: I will usually say where the code to alter is located, but you really need to
    check GitHub to see it.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting, please consider that the code of this chapter might be stricter
    than usual on the version requirements, in particular the visualization part,
    as it uses a library that has not been updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'My recommendation is to use Python 3.7 and to install TensorFlow version 2.2,
    Keras 2.3, and `scipy` 1.2, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now look at `manual_control.py`, the first thing that you might notice
    is this block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It loads an `egg` file containing the code for Carla, which is located in the
    `PythonAPI/carla/dist/` folder. As an alternative, you can also install Carla
    using a command such as the following, of course with the name of your `egg` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, you will probably notice that the code is organized into the following
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`World`: The virtual world where our vehicle moves, which includes the map
    and all the actors (vehicles, pedestrians, and sensors).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KeyboardControl`: This reacts to the keys pressed by the user, and it has
    some logic to convert the binary on/off keys for steering, braking, and accelerating
    to a wider range of values, based on how long they are pressed for, making the
    car much easier to control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HUD`: This renders all the information related to the simulation, such as
    speed, steering, and throttle, and it manages the notifications that can show
    some information to the user, for a few seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FadingText`: This class is used by the HUD class to show notifications that
    disappear after a few seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HelpText`: This class displays some text using `pygame`, a gaming library
    used by Carla.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CollisionSensor`: This is a sensor that is able to detect collisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LaneInvasionSensor`: This is a sensor that is able to detect that you crossed
    a lane line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GnssSensor`: This is a GPS/GNSS sensor that provides the GNSS position inside
    the OpenDRIVE map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IMUSensor`: This is the inertial measurement unit, which uses a gyroscope
    to detect the accelerations applied to the car.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RadarSensor`: A radar, providing a two-dimensional map of the elements detected,
    including their speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CameraManager`: This is a class that manages the camera and prints it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also a couple of other notable methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main()`: This is mostly dedicated to parsing the arguments received by the
    OS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`game_loop()`: This mostly initializes pygame, the Carla client, and all the
    related objects, and it also implements the game loop, where, 60 times per second,
    the keys are analyzed and the most updated image is shown on the screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The visualization of the frame is triggered by `game_loop()`, with the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `world.render()` method calls `CameraManager.render()`, which displays the
    last frame available.
  prefs: []
  type: TYPE_NORMAL
- en: If you checked the code, you may have noticed that Carla uses a weak reference
    to avoid circular references. A **weak reference** is a reference that does not
    prevent an object from being garbage-collected, which is useful in some scenarios,
    such as a cache.
  prefs: []
  type: TYPE_NORMAL
- en: When you work with Carla, there is one important thing to consider. Some of
    your code runs on the server, while some of it runs on the client, and it might
    not be easy to draw a line between the two. This can have unintended consequences,
    such as your model running 10 to 30 times slower, probably because it is serialized
    to the server, though this is just my speculation after seeing this problem. For
    this reason, I run my inference in the `game_loop()` method, which surely runs
    on the client.
  prefs: []
  type: TYPE_NORMAL
- en: This also means that the frames are computed on the server and sent to the client.
  prefs: []
  type: TYPE_NORMAL
- en: An unfortunate additional thing to consider is that the API of Carla is not
    stable, and version 0.9.0 removed many functionalities that should be added back
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation is also not particularly updated with these missing APIs,
    so don't be surprised if things don't work as expected. Hopefully, this will be
    fixed soon. In the meantime, you can use an older version. We used Carla 0.9.9.2,
    and there are still some rough edges, but it is good enough for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know more about CARLA, let's see how we can record our dataset,
    starting with only one video stream.
  prefs: []
  type: TYPE_NORMAL
- en: Recording one video stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In principle, recording one video stream with Carla is very simple, because
    there is already an option to do so. If you run `manual_control.py`, from the
    `PythonAPI/examples` directory, when you press *R*, it starts to record.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that we also want the steering angle. Normally, you could save
    this data in a database of some type, a CSV file, or a pickle file. To keep things
    simpler and focused on the core task, we will just add the steering angle and
    some other data to the filename. This makes it a bit easier for you to build the
    dataset, as you might want to record multiple runs dedicated to fixing a specific
    problem, and you can just move the files to a new directory and easily preserve
    all the information without having to update the path on a database.
  prefs: []
  type: TYPE_NORMAL
- en: But if you don't like it, feel free to use a better system.
  prefs: []
  type: TYPE_NORMAL
- en: We could write a client from scratch that integrates with the Carla server and
    does what we need, but for simplicity and to better isolate the changes required,
    we will just copy `manual_control.py` to a file called `manual_control_recording.py`,
    and we will just add what we need.
  prefs: []
  type: TYPE_NORMAL
- en: Please remember that this file should run in the `PythonAPI/examples` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we want to do is change track to `Town04`, because it
    is more interesting than the default track:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The previous code needs to go in the `game_loop()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The variable client is clearly the client connecting to the Carla server.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to change the spawn point (the place where the simulation starts)
    to be fixed, because normally, this changes every time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to change the name of the file. While we are at it, we will not
    only save the steering angle, but also the throttle and the brake. We will not
    use them, but if you want to experiment, they will be there for you. The following
    method should be defined in the `CameraManager` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can save the file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `image.frame` variable contains the number of the current frame, and `camera_name`
    for now is not important, but it will have the `MAIN` value.
  prefs: []
  type: TYPE_NORMAL
- en: The `image` variable also contains the current image that we want to save.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should get names similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous file name, you can identify the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: The frame number (`00078843`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The camera (`MAIN`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steering angle (`0.000000`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The throttle (`0.500000`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The brake (`0.000000`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the image, in my case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – One frame from Carla, steering 0 degrees](img/Figure_8.2_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – One frame from Carla, steering 0 degrees
  prefs: []
  type: TYPE_NORMAL
- en: This frame is fine, but not great. I should have stayed in another lane, or
    the steering should have been slightly pointed toward the right. In the case of
    behavioral cloning, the car learns from you, so how you drive is important. Controlling
    Carla with a keyboard is not great, and when recording, it works worse because
    of the time spent saving the images.
  prefs: []
  type: TYPE_NORMAL
- en: The real problem is that we need to record three cameras, not just one. Let's
    see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Recording three video streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To record three video streams, the starting point is to have three cameras.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Carla has the following five cameras:'
  prefs: []
  type: TYPE_NORMAL
- en: A classical *third-person* view, from the back, above the car
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the front of the car, toward the road (looking forward)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the front of the car, toward the car (looking backward)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From far above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, you can see the first three cameras:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Cameras from above, toward the road, and toward the car](img/Figure_8.3_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Cameras from above, toward the road, and toward the car
  prefs: []
  type: TYPE_NORMAL
- en: The second camera looks very interesting to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are taken from the remaining two cameras:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Carla cameras from far above and from the left](img/Figure_8.4_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Carla cameras from far above and from the left
  prefs: []
  type: TYPE_NORMAL
- en: The last camera is also somehow interesting, though we do not want to record
    the car in our frames. We are missing the camera from the right because, for some
    reason, the authors of Carla haven't added it to the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, changing the cameras or adding a new one is quite simple. This is
    the definition of the original cameras, the `CameraManager` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As a first attempt, we can keep just the second and the fifth camera, but we
    want them at comparable positions. Carla has been written using a very famous
    engine for video-games: *Unreal Engine 4*. In *Unreal Engine*, the *z* axis is
    the vertical one (up and down), the *x* axis is for forward and backward, and
    the *y* axis is for lateral movements, left and right. So, we want the cameras
    to have the same *x* and *z* coordinates. We also want a third camera, from the
    right. For this, it is enough to change the sign of the *y* coordinates. This
    is the resulting code, only for the cameras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You could probably stop here. I ended up moving the lateral cameras more to
    the side, which can be done by changing `bound_y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the images that we get now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – New cameras: from the left, from the front (main camera), and
    from the right](img/Figure_8.5_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5 – New cameras: from the left, from the front (main camera), and
    from the right'
  prefs: []
  type: TYPE_NORMAL
- en: Now, it should be easier to understand that the left and right cameras can be
    used to teach the neural network how to correct the trajectory, if it is not in
    the correct location, compared to the main camera. This, of course, assumes that
    the stream recorded by the main camera is the intended position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if the correct cameras are now available, they are not in use. We need
    to add them, in `World. restart()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CameraManager.add_camera()` method is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'What this code does is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sets up a sensor, using the specified camera
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds the sensor to a list
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instructs the sensor to call a lambda function that invokes the `save_image()`
    method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following `get_camera_name()` method is used to get a meaningful name to
    the camera, based on its index, which is dependent on the cameras that we defined
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Before looking at the code of `save_image()`, let's discuss a small issue.
  prefs: []
  type: TYPE_NORMAL
- en: Recording three cameras for every frame is kind of slow, resulting in low **Frames
    Per Second** (**FPS**), which makes it difficult to drive the car. As a consequence,
    you would over-correct, recording a sub-optimal dataset where you basically teach
    the car how to zig-zag. To limit this problem, we will record only one camera
    view for each frame, then we rotate to the next camera view for the next frame,
    and we will cycle through all three camera views during recording. After all,
    consecutive frames are similar, so it is not a huge problem.
  prefs: []
  type: TYPE_NORMAL
- en: The camera used by Nvidia was recording at 30 FPS, but they decided to skip
    most of the frames, recording only at 10 FPS, because the frames were very similar,
    increasing the training time without adding much information. You would not record
    at the highest speed, but your dataset would be better, and if you want a bigger
    dataset, you can always just drive more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `save_image()` function needs to first check whether this is a frame that
    we want to record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The second step is to convert the image into a format suitable for OpenCV,
    as we are going to use it to save the image. We need to convert the raw buffer
    to NumPy, and we also need to drop one channel, because Carla produces images
    with BGRA, with four channels: blue, green, red, and alpha (transparency):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can resize the image, crop the part that we need, and save it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can see in the code repository in GitHub that I recorded a fair amount of
    frames, enough to drive for one or two turns, but if you want to drive along the
    whole track, you will need many more frames, and the better you drive, the better
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the cameras, we need to use them to build the dataset that
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: Recording the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To build the dataset, clearly, you need to record at least the turns that you
    expect your network to make. The more the better. But you should also record movements
    that help your car correct the trajectory. The left and right camera already help
    quite a lot, but you should also record a few stints where the car is close to
    the edge of the road, and the steering wheel is turning it toward the center.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Car close to the left, steering to the right](img/Figure_8.6_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Car close to the left, steering to the right
  prefs: []
  type: TYPE_NORMAL
- en: If there are turns that don't go the way you want, you can try to record them
    more than once, as I did.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might see the advantage of encoding the steering wheel in the name
    of the image. You can group these correction stints, or whatever you prefer, on
    dedicated directories, and take them in and out of the dataset as required.
  prefs: []
  type: TYPE_NORMAL
- en: If you want, you can even manually select a part of the pictures, to correct
    for wrong steering angles, though this might not be necessary if there is a limited
    number of frames with the wrong angle.
  prefs: []
  type: TYPE_NORMAL
- en: Despite saving only one camera per frame, you might still find it difficult
    to drive, above all in regard to the speed. I personally prefer to limit the throttle
    so that the car does not go too fast, but I can still slow down if I want.
  prefs: []
  type: TYPE_NORMAL
- en: 'The throttle can usually reach the value of `1`, so to limit it, it is enough
    to use a line of code similar to the following, in the `KeyboardControl ._parse_vehicle_keys()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To increase fluidity, you might run the client with a lower resolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also lower the resolution of the server, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have the raw dataset, it's time to create the real dataset, with
    the proper steering angles.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset that we recorded is raw, meaning that it needs some preprocessing
    before being ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing to do is to correct the steering angle for the left
    and right camera.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, this is done by an additional program so that you can eventually
    change it without having to record the frames again.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we need a method to extract the data from the name (we assume the
    file is a JPG or a PNG):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `to_float` method is just a convenience to convert -0 to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, changing the steering angle is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: I added a correction of 0.25\. If your camera is closer to the car, you might
    want to use a smaller number.
  prefs: []
  type: TYPE_NORMAL
- en: While we are at it, we can also add the frames mirrored, to increase the size
    of the dataset a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have converted the dataset, we are ready to train a neural network
    similar to DAVE-2 to learn how to drive.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create our neural network, we will take inspiration from DAVE-2, which is
    a surprisingly simple neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with a lambda layer, to confine the image pixels in the (-1, +1) range:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, there are three convolutional layers with kernel size `5` and strides
    `(2,2)`, which halves the output resolution, and three convolutional layers with
    kernel size `3`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we have the dense layers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: I am always amazed when I think that these few lines of code are enough to somehow
    allow a car to drive by itself on a real road!
  prefs: []
  type: TYPE_NORMAL
- en: While it looks more or less similar to other neural networks that we saw before,
    there is a very important difference—the last activation is not a softmax function,
    because this is not a classifier, but a neural network that needs to perform a
    *regression* task, predicting the correct steering angle given an image.
  prefs: []
  type: TYPE_NORMAL
- en: We say that a neural network is performing a regression when it is trying to
    predict a value in a potentially continuous interval—for example, between –1 and
    +1\. By comparison, in a classification task, the neural network is trying to
    predict which label is more likely correct and probably represents the content
    of the image. A neural network that can distinguish between cats and dogs is therefore
    a classifier, while a network that tries to predict the cost of an apartment based
    on the size and location is performing regression.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what we need to change to use a neural network for regression.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network for regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already seen, a difference is the lack of a softmax layer. In its
    place, we used Tanh (hyperbolic tangent), an activation useful to generate values
    in the range (-1, +1), which is the range that we need for the steering angle.
    However, in principle, you could not even have an activation and directly use
    the value of the last neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the Tanh function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The tanh function](img/Figure_8.7_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The tanh function
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Tanh will limit the range of the activation to the (-1, +1)
    range.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, when we train a classifier, as in the case of MNIST or CIFAR-10, we
    use `categorical_crossentropy` as a loss and `accuracy` as a metric. However,
    for regression, we need to use `mse` for the loss and we can optionally use `cosine_proximity`
    as a metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cosine proximity is an indication of similarity for vectors. So, 1 means
    that they are identical, 0 that they are perpendicular, and -1 that they are opposite.
    The loss and metric code snippet looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the code is as it is for classifiers, except that we don't need
    to use one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the graph for the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Behavioral cloning with DAVE-2, training](img/Figure_8.8_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Behavioral cloning with DAVE-2, training
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see slight overfitting. This is the value of the losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The loss, in this case, is the mean square error between the steering angle
    recorded for training and the angle computed by the network. We can see that the
    validation loss is quite good. If you have time, you can try to experiment with
    the model, adding dropouts or even changing the whole structure.
  prefs: []
  type: TYPE_NORMAL
- en: Soon, we will integrate our neural network with Carla and see how it drives,
    but before that, it could be legit to wonder whether the neural network is actually
    focusing its attention on the right parts of the road. The next section will show
    us how to do this, using a technique called **saliency maps**.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the saliency maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand what the neural network is focusing its attention on, we should
    use a practical example, so let''s choose an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Test image](img/Figure_8.9_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Test image
  prefs: []
  type: TYPE_NORMAL
- en: If we had to drive on this road, as humans, we would pay attention to the lanes
    and the wall, though admittedly, the wall is not as important as the last lane
    is before that.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already know how to get an idea of what a **CNN** (short for **convolutional
    neural network**) such as DAVE-2 is taking into consideration: as the output of
    a convolution layer is an image, we can visualize it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Part of the activations of the first convolutional layer](img/Figure_8.10_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Part of the activations of the first convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: This is a good starting point, but we would like something more. We would like
    to understand which pixels contribute the most to the prediction. For that, we
    need to get a **saliency map**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras does not directly support them, but we can use `keras-vis`. You can install
    it with `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step to get a saliency map is to create a model that starts with
    the input of our model but ends with the layer that we want to analyze. The resulting
    code is very similar to what we saw for the activations, except that for convenience,
    we also need the index of the layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'While not necessary in our case, you might want to change the activation to
    become linear, then reload the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is just a matter of calling `visualize_saliency()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We are interested in the saliency map of the last layer, the output, but as
    an exercise, we will go through all the convolutional layers to see what they
    understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the saliency map for the first convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Saliency map of the first convolutional layer](img/Figure_8.11_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Saliency map of the first convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: Not very impressive, as there is no saliency and we only see the original image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the map of the second layer looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Saliency map of the second convolutional layer](img/Figure_8.12_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Saliency map of the second convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an improvement, but even if we see some attention in the middle line,
    on the wall and the land after the right lane, it is not very clear. Let''s see
    the third layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Saliency map of the third convolutional layer](img/Figure_8.13_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Saliency map of the third convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are talking! We can see great attention on the central and left line,
    and some attention focused on the wall and the right line. The network seems to
    be trying to understand where the road ends. Let''s also see the fourth layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Saliency map of the fourth convolutional layer](img/Figure_8.14_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Saliency map of the fourth convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the attention is mostly focused on the central line, but
    there are also sparks of attention on the left line and on the wall, as well as
    a bit on the whole road.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check the fifth and last convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Saliency map of the fifth convolutional layer](img/Figure_8.15_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Saliency map of the fifth convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: The fifth layer is similar to the fourth layer, plus with some more attention
    on the left line and on the wall.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also visualize the saliency map for dense layers. Let''s see the result
    for the last layer, which is what we consider the real saliency map for this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Saliency map of the output layer](img/Figure_8.16_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Saliency map of the output layer
  prefs: []
  type: TYPE_NORMAL
- en: The last saliency map, the most important one, shows great attention to the
    central line and the right line, plus some attention on the upper-right corner,
    which could be an attempt to estimate the distance from the right lane. We can
    also see some attention on the wall and the left lane. So, all in all, it seems
    promising.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try with another image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Second test image](img/Figure_8.17_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Second test image
  prefs: []
  type: TYPE_NORMAL
- en: This is an interesting image, as it is taken from a part of the road where the
    network has not been trained, but it still behaved very well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the saliency map of the third convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Saliency map of the third convolutional layer](img/Figure_8.18_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Saliency map of the third convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: The neural network seems very concerned with the end of the road and it seems
    to have detected a couple of trees as well. If it was trained for braking, I bet
    it would do so!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the final map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Saliency map of the output layer](img/Figure_8.19_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Saliency map of the output layer
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty similar to the previous one, but there is some attention to the
    central line and the right line, and a tiny amount on the road in general. Looks
    good to me.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try with the last image, taken from the training to teach when to turn
    right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Third test image](img/Figure_8.20_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Third test image
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the final saliency map for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Saliency map of the output layer](img/Figure_8.21_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – Saliency map of the output layer
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the neural network is giving attention mostly to the right
    line, also keeping an eye on the whole road and with some spark of attention dedicated
    to the left line.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the saliency map can be a valid tool to understand the behavior
    of the network a bit more and do a kind of sanity check on its interpretation
    of the world.
  prefs: []
  type: TYPE_NORMAL
- en: Now it is finally time to integrate with Carla and see how we are performing
    in the real world. Fasten your seatbelt, because we are going to drive, and our
    neural network will be in the driver's seat!
  prefs: []
  type: TYPE_NORMAL
- en: Integrating the neural network with Carla
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now integrate our neural network with Carla, to achieve self-driving.
  prefs: []
  type: TYPE_NORMAL
- en: As before, we start by making a copy of `manual_control.py`, which we could
    call `manual_control_drive.py`. For simplicity, I will only write the code that
    you need to change or add, but you can find the full source code on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Please remember that this file should run in the `PythonAPI/examples` directory.
  prefs: []
  type: TYPE_NORMAL
- en: In principle, letting our neural network take control of the steering wheel
    is quite simple, as we just need to analyze the current frame and set the steering.
    However, we also need to apply some throttle, or the car will not move!
  prefs: []
  type: TYPE_NORMAL
- en: It's also very important that you run the inference phase in the game loop,
    or that you are really sure that it is running on the client, else the performance
    will drop substantially and your network will have a hard time driving due to
    the excess of latency between receiving the frame and sending the instruction
    to drive.
  prefs: []
  type: TYPE_NORMAL
- en: As the Carla client changes the car every time, the effect of the throttle will
    change, sometimes making your car too fast or too slow. You therefore need a way
    to change the throttle with a key, or you could always use the same car, which
    will be our solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get a list of the cars available in Carla with the following line of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'At the time of writing, this produces the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In `World.restart()`, you can select the car of your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Carla uses actors, which can represent vehicles, walkers, sensors, traffic
    lights, traffic signs, and so on; actors are created from templates called `try_spawn_actor()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the code now, you will see the car but with the wrong point of view.
    Pressing the *Tab* key will fix it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Left: default initial camera, right: camera for self-driving](img/Figure_8.22_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.22 – Left: default initial camera, right: camera for self-driving'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to start from the point where I was training the car, you should
    also set the starting point in the same method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If you don't do that, the car will be spawned in a random position, and it might
    have more problems driving.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `game_loop()`, we also need to select the proper track:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run it now, after pressing *Tab*, you should see something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Image from Carla, ready for self-driving](img/Figure_8.23_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – Image from Carla, ready for self-driving
  prefs: []
  type: TYPE_NORMAL
- en: If you press *F1*, you can remove the information on the left.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we want to be able to trigger the self-driving mode on and
    off, so we need a variable for that, such as the following, and one to hold the
    computed steering angle in the constructor of `KeyboardControl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in `KeyboardControl.parse_events()`, we will intercept the *D* key and
    switch the self-driving functionality on and off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is resizing and saving the last image received from the server,
    when it is still in BGR format, in `CameraManager._parse_image()`. This is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `array` variable originally contains the image in BGR format, and `::-1`
    in NumPy reverses the order, so the last line of code effectively converts the
    image from BGR into RGB, before visualizing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can load the model in `game_loop()`, outside of the main loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can run the model in `game_loop()`, inside the main loop, and save
    the steering, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing to do is just to use the steering that we computed, put a fix
    throttle, and limit the maximum speed, while we are at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This is all nice and good, except that it might not work because of a GPU error.
    Let's see what it is and how to overcome it.
  prefs: []
  type: TYPE_NORMAL
- en: Making your GPU work
  prefs: []
  type: TYPE_NORMAL
- en: 'You might get an error similar to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: My understanding of what's happening is that there is a conflict with some component
    of Carla, either the server or the client, which results in the GPU not having
    enough memory. In particular, it is TensorFlow creating the problem, as it tries
    to allocate all the memory in the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, this is easily fixable with a few lines of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The call to `set_memory_growth()` instructs TensorFlow to allocate only part
    of the GPU RAM, and eventually allocate more if required, solving our problem.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your car should be able to drive, so let's discuss a bit how
    it works.
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you could start running `manual_control_drive.py`, maybe instructing it
    to use a lower resolution, using the `--res 480x320` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: If you press the *D* key, the car should start to drive by itself. It's probably
    quite slow, but it should run, sometimes nicely, sometimes less nicely. It might
    not always take the turns that it is supposed to take. You can try to add images
    to the dataset or improve the architecture of the neural network – for example,
    by adding some dropout layers.
  prefs: []
  type: TYPE_NORMAL
- en: You could try to change the car or increase the speed. You might notice that
    at a higher speed, the car starts to move more erratically, as if the driver was
    drunk! This is due to the excessive latency between the car getting in the wrong
    position and the neural network reacting to it. I think this could be fixed partly
    with a computer fast enough to process many FPS. However, I think a real fix would
    be to also record higher speed runs, where the corrections would be stronger;
    this would require a better controller than the keyboard, and you should also
    insert the speed in the input, or have multiple neural networks and switch between
    them based on the speed.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, sometimes it can somehow also drive even if we are using the
    outside camera, with the result that our car is part of the image! Of course,
    the result is not good, and you get the *drunk drive* effect even at low speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just out of curiosity, let''s check the saliency map. This is the image that
    we are sending to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Image from the back](img/Figure_8.24_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – Image from the back
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can check the saliency map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Saliency maps: third convolution layer and output layer](img/Figure_8.25_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.25 – Saliency maps: third convolution layer and output layer'
  prefs: []
  type: TYPE_NORMAL
- en: The network is still able to recognize the lines and the road; however, it is
    very concerned about the car. My hypothesis is that the neural network *thinks*
    it is an obstacle and the road is ending.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to teach the car how to drive well with this camera, or with any
    other one, you will need to train it with that specific camera. If you want the
    car to drive properly on another track, you will need to train it on that specific
    track. Eventually, if you train it on many tracks and on many conditions, it should
    be able to drive everywhere. But this means building a huge dataset, with millions
    of images. Eventually, if your dataset is too big, you will run out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about generators, a technique that can help
    us overcome these problems.
  prefs: []
  type: TYPE_NORMAL
- en: Training bigger datasets using generators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training big datasets, memory consumption can be an issue. In Keras, one
    way to solve this problem is by using Python generators. A Python generator is
    a function that can lazily return a potentially infinite stream of values, with
    a very low memory footprint as you only need memory for one object, plus, of course,
    all the supporting data that you might need; a generator can be used as if it
    were a list. A typical generator has a loop, and for every object that needs to
    be part of the stream, it will use the `yield` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: In Keras, the generator needs to be aware of the batch size, because it needs
    to return a batch of samples and a batch of labels.
  prefs: []
  type: TYPE_NORMAL
- en: We will keep a list of the files to process, and we will write a generator that
    can use this list to return the image associated with it and the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will write a generic generator that, hopefully, you can reuse on other cases,
    and it will accept four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of IDs, which in our case are the filenames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function to retrieve the input (the image) from the ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function to retrieve the label (the steering wheel) from the ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To start, we need a function that can return an image given a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need a function that, given a filename, can return the label, which
    in our case is the steering angle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now write the generator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Every iteration in the `while` loop corresponds to an epoch, while the `for`
    loop generates all the batches required to complete each epoch; at the beginning
    of each epoch, we shuffle the IDs to improve the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, it used to be that you had to use the `fit_generator()` method, but
    nowadays, `fit()` is able to understand if the argument is a generator, but you
    still need to provide a couple of new parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`steps_per_epoch`: This gives how many batches there are in a single training
    epoch, which is the number of training samples divided by the batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_steps`: This gives how many batches there are in a single validation
    epoch, which is the number of validation samples divided by the batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the code that you need to use the `generator()` function that we just
    defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Thanks to this code, you can now leverage very big datasets. However, there
    is also another application of generators: custom on-demand data augmentation.
    Let''s say a few words about it.'
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting data the hard way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already saw an easy way to perform data augmentation, using `ImageDataGenerator`
    in [*Chapter 7*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158), *Detecting Pedestrians
    and Traffic Lights*. This could be appropriate for classifiers, because the transformations
    applied to the image do not alter its classification. However, in our case, some
    of these transformations would require a change in the prediction. In fact, Nvidia
    designed a custom data augmentation, where the image is randomly shifted and the
    steering wheel is updated accordingly. This could be done with a generator, where
    we take the original image, apply the transformation, and correct the steering
    wheel based on the amount of shifting.
  prefs: []
  type: TYPE_NORMAL
- en: But we are not limited to just replicating the same amount of images that we
    have in input, but we could create less (filtering) or more; for example, mirroring
    could be applied at runtime, and as a result, we duplicate the images in memory,
    without having to store double the amount of images and saving, as a consequence,
    half of the file access and the JPEG decompression; though of course, we would
    need some CPU to flip the image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through many interesting topics.
  prefs: []
  type: TYPE_NORMAL
- en: We started by describing DAVE-2, an experiment of Nvidia with the goal to demonstrate
    that a neural network can learn how to drive on a road, and we decided to replicate
    the same experiment but on a much smaller scale. First, we collected the image
    from Carla, taking care of recording not only the main camera but also two additional
    side cameras, to teach the network how to correct errors.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we created our neural network, copying the architecture of DAVE-2, and
    we trained it for regression, which requires some changes compared to the other
    training that we did so far. We learned how to generate saliency maps and get
    a better understanding of where the neural network is focusing its attention.
    Then, we integrated with Carla and used the network to *self-drive* the car!
  prefs: []
  type: TYPE_NORMAL
- en: At the end, we learned how to train a neural network using Python generators,
    and we discussed how this can be used to achieve more sophisticated data augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore a state-of-the-art technique that can be
    used to detect the road at a pixel level—semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After reading the chapter, you should be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the original name of the neural network that Nvidia trained for self-driving?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a classification and a regression task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the Python keyword that you can use to create a generator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a saliency map?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need to record three video streams?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are we running inference from the `game_loop()` method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nvidia DAVE-2: [https://devblogs.nvidia.com/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notes related to Carla 0.9.0 API changes: [https://carla.org/2018/07/30/release-0.9.0/](https://carla.org/2018/07/30/release-0.9.0/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carla: [https://carla.org](https://carla.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keras-vis`: [https://github.com/raghakot/keras-vis](https://github.com/raghakot/keras-vis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
