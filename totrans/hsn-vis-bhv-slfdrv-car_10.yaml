- en: '*Chapter 8*: Behavioral Cloning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*: 行为克隆'
- en: In this chapter, we are going to train a neural network to control the steering
    wheel of a car, effectively teaching it how to drive a car! Hopefully, you will
    be surprised by how simple the core of this task is, thanks to deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练一个神经网络来控制汽车的转向盘，有效地教会它如何驾驶汽车！希望你会对这项任务的核心如此简单感到惊讶，这要归功于深度学习。
- en: To achieve our goal, we will have to modify one of the examples of the CARLA
    simulator, first to save the images required to create the dataset, then to use
    our neural network to drive. Our neural network will be inspired by the architecture
    of Nvidia DAVE-2, and we will also see how to better visualize where the neural
    network focuses its attention.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的目标，我们不得不修改CARLA模拟器的一个示例，首先保存创建数据集所需的图像，然后使用我们的神经网络来驾驶。我们的神经网络将受到英伟达DAVE-2架构的启发，我们还将看到如何更好地可视化神经网络关注的区域。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Teaching a neural network how to drive with behavioral cloning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用行为克隆教神经网络如何驾驶
- en: The Nvidia DAVE-2 neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英伟达DAVE-2神经网络
- en: Recording images and the steering wheel from Carla
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Carla记录图像和转向盘
- en: Recording three video streams
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录三个视频流
- en: Creating the neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建神经网络
- en: Training a neural network for regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练用于回归的神经网络
- en: Visualizing the saliency maps
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化显著性图
- en: Integrating with Carla for self-driving
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Carla集成以实现自动驾驶
- en: Training bigger datasets using generators
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成器训练更大的数据集
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To be able to use the code explained in this chapter, you need to have installed
    the following tools and modules:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用本章中解释的代码，您需要安装以下工具和模块：
- en: The Carla simulator
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carla模拟器
- en: Python 3.7
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7
- en: The NumPy module
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy模块
- en: The TensorFlow module
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow模块
- en: The Keras module
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras模块
- en: The `keras-vis` module
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras-vis`模块'
- en: The OpenCV-Python module
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV-Python模块
- en: A GPU (recommended)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个GPU（推荐）
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下链接找到：[https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8)。
- en: 'The Code in Action videos for this chapter can be found here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的《代码实战》视频可以在以下链接找到：
- en: '[https://bit.ly/3kjIQLA](https://bit.ly/3kjIQLA)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3kjIQLA](https://bit.ly/3kjIQLA)'
- en: Teaching a neural network how to drive with behavioral cloning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用行为克隆教神经网络如何驾驶
- en: A self-driving car is a complicated ensemble of hardware and software. The hardware
    of a normal car is already very complex, usually with thousands of mechanical
    pieces, and a self-driving car adds many sensors to that. The software is not
    any simpler, and in fact, rumor has it that already 15 years ago, a world-class
    carmaker had to take a step back, because the complexity of the software was getting
    out of control. To give you an idea, a sports car can have more than 50 CPUs!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车是一个复杂的硬件和软件集合。普通汽车的硬件已经很复杂了，通常有成千上万的机械部件，而自动驾驶汽车又增加了许多传感器。软件并不简单，事实上，据说早在15年前，一家世界级的汽车制造商不得不退步，因为软件的复杂性已经失控。为了给您一个概念，一辆跑车可以有超过50个CPU！
- en: Clearly, making a self-driving car that is safe and reasonably fast is an incredible
    challenge, but despite this, we will see how powerful a dozen of lines of code
    can be. For me, it was an enlightening moment to realize that something so complex
    as driving could be coded in such a simple way. But I should not have been surprised
    because, with deep learning, data is more important than the code itself, at least
    to a certain extent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，制造一个既安全又相对快速的自动驾驶汽车是一个难以置信的挑战，但尽管如此，我们将看到一行代码可以有多强大。对我来说，意识到如此复杂的事情如驾驶可以用如此简单的方式编码，是一个启发性的时刻。但我并不应该感到惊讶，因为，在深度学习中，数据比代码本身更重要，至少在某种程度上。
- en: We don't have the luxury of testing on a real self-driving car, so we will use
    Carla, and we will train a neural network that can generate the steering angle
    after having been fed the video of the camera. We are not using other sensors,
    though, in principle, you could use all the sensors that you can imagine, just
    modifying the network to accept this additional data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有在真实自动驾驶汽车上测试的奢侈条件，所以我们将使用 Carla，并且我们将训练一个神经网络，在输入摄像头视频后能够生成转向角度。尽管如此，我们并没有使用其他传感器，原则上，你可以使用你想象中的所有传感器，只需修改网络以接受这些额外的数据。
- en: Our goal is to teach Carla how to make a *lap*, using a part of the **Town04**
    track, one of the tracks included in Carla. We want our neural network to drive
    a bit straight, and then make some turns to the right until it reaches the initial
    point. In principle, to teach the neural network, we just need to drive Carla,
    recording images of the road and the corresponding steering angle that we applied,
    a process called **behavioral cloning**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是教会 Carla 如何进行一次“绕圈”，使用 **Town04** 轨道的一部分，这是 Carla 包含的轨道之一。我们希望我们的神经网络能够稍微直行，然后进行一些右转，直到到达初始点。原则上，为了训练神经网络，我们只需要驾驶
    Carla，记录道路图像和我们应用的相应转向角度，这个过程被称为**行为克隆**。
- en: 'Our task is divided into three steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务分为三个步骤：
- en: Building the dataset
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建数据集
- en: Designing and training the neural network
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和训练神经网络
- en: Integrating the neural network in Carla
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Carla 中集成神经网络
- en: We are going to take inspiration from the DAVE-2 system, created by Nvidia.
    So, let's start describing it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将借鉴 Nvidia 创建的 DAVE-2 系统。那么，让我们开始描述它。
- en: Introducing DAVE-2
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 DAVE-2
- en: DAVE-2 is a system designed by Nvidia to train a neural network to drive a car,
    intended as a proof of concept to demonstrate that, in principle, a single neural
    network could be able to steer a car on a road. Putting it another way, our network
    could be trained to drive a real car on a real road, if enough data is provided.
    To give you an idea, Nvidia used around 72 hours of video, at 10 frames per second.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: DAVE-2 是 Nvidia 设计的一个系统，用于训练神经网络驾驶汽车，旨在作为一个概念验证，证明原则上一个单一的神经网络能够在一个道路上控制汽车。换句话说，如果提供足够的数据，我们的网络可以被训练来在真实的道路上驾驶真实的汽车。为了给你一个概念，Nvidia
    使用了大约 72 小时的视频，每秒 10 帧。
- en: 'The idea is very simple: we feed the neural network a video stream, and the
    neural network will simply generate the steering angle, or something equivalent.
    The training is created by a human driver, and the system collects data from the
    camera (training data) and from the steering wheel moved by the pilot (training
    labels). This is called *behavioral cloning* because the network is trying to
    clone the behavior of the human driver.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常简单：我们给神经网络提供视频流，神经网络将简单地生成转向角度，或者类似的东西。训练是由人类驾驶员创建的，系统从摄像头（训练数据）和驾驶员操作的转向盘（训练标签）收集数据。这被称为*行为克隆*，因为网络试图复制人类驾驶员的行为。
- en: 'Unfortunately, this would be a bit too simple, as most of the labels would
    simply be 0 (the driver going straight), so the network would have problems learning
    how to move to the middle of the lane. To alleviate this issue, Nvidia uses three
    cameras:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这会过于简单，因为大部分标签将简单地是 0（驾驶员直行），因此网络将难以学习如何移动到车道中间。为了缓解这个问题，Nvidia 使用了三个摄像头：
- en: One on the center of the car, which is the real human behavior
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆中央的一个，这是真实的人类行为
- en: One on the left, simulating what to do if the car is too much on the left
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧的一个，模拟如果汽车过于靠左时应该怎么做
- en: One on the right, simulating what to do if the car is too much on the right
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧的一个，模拟如果汽车过于靠右时应该怎么做
- en: For the left and right cameras to be useful, it is, of course, necessary to
    change the steering angle associated with their videos, to simulate a correction;
    so, the *left* camera needs to be associated with a turn *more to the right* and
    the *right* camera needs to be associated with a turn *more to the left*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使左右摄像头有用，当然有必要更改与它们视频相关的转向角度，以模拟一个校正；因此，*左*摄像头需要与一个*更向右*的转向相关联，而*右*摄像头需要与一个*更向左*的转向相关联。
- en: 'The following diagram shows the system:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示显示了系统：
- en: '![Figure 8.1 – Nvidia DAVE-2 system](img/Figure_8.1_B16322.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – Nvidia DAVE-2 系统](img/Figure_8.1_B16322.jpg)'
- en: Figure 8.1 – Nvidia DAVE-2 system
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – Nvidia DAVE-2 系统
- en: To make the system more robust, Nvidia adds random shift and rotation, adjusting
    the steering for it, but we will not do that. However, we are going to use three
    video streams, as suggested by them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使系统更健壮，Nvidia添加了随机的平移和旋转，并调整转向以适应，但我们将不会这样做。然而，我们将使用他们建议的三个视频流。
- en: How do we get the three video streams and the steering angle? Of course, from
    Carla, which we will use quite a lot during this chapter. Before starting to write
    some code, let's get familiar with `manual_control.py`, a file that we will copy
    and modify.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何获取三个视频流和转向角度？当然是从Carla获取的，我们将在本章中大量使用它。在开始编写代码之前，让我们熟悉一下`manual_control.py`文件，这是一个我们将复制并修改的文件。
- en: Getting to know manual_control.py
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解`manual_control.py`
- en: Instead of writing a full client code to do what we need, we will change the
    `manual_control.py` file, from `PythonAPI/examples`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会编写一个完整的客户端代码来完成我们需要的操作，而是会修改`manual_control.py`文件，从`PythonAPI/examples`。
- en: I will usually say where the code to alter is located, but you really need to
    check GitHub to see it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常会说明需要修改的代码位置，但您实际上需要检查GitHub以查看它。
- en: Before starting, please consider that the code of this chapter might be stricter
    than usual on the version requirements, in particular the visualization part,
    as it uses a library that has not been updated.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请考虑本章的代码可能比通常更严格地要求版本，特别是可视化部分，因为它使用了一个尚未更新的库。
- en: 'My recommendation is to use Python 3.7 and to install TensorFlow version 2.2,
    Keras 2.3, and `scipy` 1.2, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是使用Python 3.7，并安装TensorFlow版本2.2、Keras 2.3和`scipy` 1.2，如下所示：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you now look at `manual_control.py`, the first thing that you might notice
    is this block of code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在查看`manual_control.py`，您可能会注意到的第一件事是这个代码块：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It loads an `egg` file containing the code for Carla, which is located in the
    `PythonAPI/carla/dist/` folder. As an alternative, you can also install Carla
    using a command such as the following, of course with the name of your `egg` file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它加载一个包含Carla代码的`egg`文件，该文件位于`PythonAPI/carla/dist/`文件夹中。作为替代方案，您也可以使用以下命令安装Carla，当然需要使用您的`egg`文件名：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After this, you will probably notice that the code is organized into the following
    classes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，您可能会注意到代码被组织成以下类：
- en: '`World`: The virtual world where our vehicle moves, which includes the map
    and all the actors (vehicles, pedestrians, and sensors).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`World`: 我们车辆移动的虚拟世界，包括地图和所有演员（车辆、行人和传感器）。'
- en: '`KeyboardControl`: This reacts to the keys pressed by the user, and it has
    some logic to convert the binary on/off keys for steering, braking, and accelerating
    to a wider range of values, based on how long they are pressed for, making the
    car much easier to control.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KeyboardControl`: 这个类会响应用户按下的键，并且包含一些逻辑，将转向、制动和加速的二进制开/关键转换为更广泛的值范围，这取决于它们被按下的时间长短，从而使汽车更容易控制。'
- en: '`HUD`: This renders all the information related to the simulation, such as
    speed, steering, and throttle, and it manages the notifications that can show
    some information to the user, for a few seconds.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HUD`: 这个类渲染与模拟相关的所有信息，如速度、转向和油门，并管理可以显示一些信息给用户的提示，持续几秒钟。'
- en: '`FadingText`: This class is used by the HUD class to show notifications that
    disappear after a few seconds.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FadingText`: 这个类被HUD类使用，用于显示几秒钟后消失的通知。'
- en: '`HelpText`: This class displays some text using `pygame`, a gaming library
    used by Carla.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HelpText`: 这个类使用`pygame`（Carla使用的游戏库）显示一些文本。'
- en: '`CollisionSensor`: This is a sensor that is able to detect collisions.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CollisionSensor`: 这是一个能够检测碰撞的传感器。'
- en: '`LaneInvasionSensor`: This is a sensor that is able to detect that you crossed
    a lane line.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LaneInvasionSensor`: 这是一个能够检测您是否跨越了车道线的传感器。'
- en: '`GnssSensor`: This is a GPS/GNSS sensor that provides the GNSS position inside
    the OpenDRIVE map.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GnssSensor`: 这是一个GPS/GNSS传感器，它提供了OpenDRIVE地图内的GNSS位置。'
- en: '`IMUSensor`: This is the inertial measurement unit, which uses a gyroscope
    to detect the accelerations applied to the car.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IMUSensor`: 这是一个惯性测量单元，它使用陀螺仪来检测施加在汽车上的加速度。'
- en: '`RadarSensor`: A radar, providing a two-dimensional map of the elements detected,
    including their speed.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RadarSensor`: 一个雷达，提供检测到的元素（包括速度）的二维地图。'
- en: '`CameraManager`: This is a class that manages the camera and prints it.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CameraManager`: 这是一个管理相机并打印其信息的类。'
- en: 'There are also a couple of other notable methods:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些其他值得注意的方法：
- en: '`main()`: This is mostly dedicated to parsing the arguments received by the
    OS.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main()`: 这部分主要致力于解析操作系统接收到的参数。'
- en: '`game_loop()`: This mostly initializes pygame, the Carla client, and all the
    related objects, and it also implements the game loop, where, 60 times per second,
    the keys are analyzed and the most updated image is shown on the screen.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`game_loop()`：这个函数主要初始化pygame、Carla客户端以及所有相关对象，并且实现了游戏循环，其中每秒60次，分析按键并显示最新的图像在屏幕上。'
- en: 'The visualization of the frame is triggered by `game_loop()`, with the following
    line:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 帧的可视化是由`game_loop()`触发的，以下是一行：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `world.render()` method calls `CameraManager.render()`, which displays the
    last frame available.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`world.render()`方法调用`CameraManager.render()`，显示最后一帧可用的图像。'
- en: If you checked the code, you may have noticed that Carla uses a weak reference
    to avoid circular references. A **weak reference** is a reference that does not
    prevent an object from being garbage-collected, which is useful in some scenarios,
    such as a cache.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查了代码，你可能已经注意到Carla使用弱引用来避免循环引用。**弱引用**是一种不会阻止对象被垃圾回收的引用，这在某些场景中很有用，例如缓存。
- en: When you work with Carla, there is one important thing to consider. Some of
    your code runs on the server, while some of it runs on the client, and it might
    not be easy to draw a line between the two. This can have unintended consequences,
    such as your model running 10 to 30 times slower, probably because it is serialized
    to the server, though this is just my speculation after seeing this problem. For
    this reason, I run my inference in the `game_loop()` method, which surely runs
    on the client.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你与Carla一起工作时，有一件重要的事情需要考虑。你的一些代码在服务器上运行，而另一些代码在客户端上运行，可能不容易在这两者之间划清界限。这可能会导致意想不到的后果，例如你的模型运行速度慢10到30倍，这可能是由于它被序列化到服务器上，尽管这只是我在看到这个问题后的推测。因此，我在`game_loop()`方法中运行我的推理，这肯定是在客户端上运行的。
- en: This also means that the frames are computed on the server and sent to the client.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着帧是在服务器上计算并发送到客户端的。
- en: An unfortunate additional thing to consider is that the API of Carla is not
    stable, and version 0.9.0 removed many functionalities that should be added back
    soon.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不幸需要考虑的事情是，Carla的API并不稳定，版本0.9.0删除了许多应该很快就会恢复的功能。
- en: The documentation is also not particularly updated with these missing APIs,
    so don't be surprised if things don't work as expected. Hopefully, this will be
    fixed soon. In the meantime, you can use an older version. We used Carla 0.9.9.2,
    and there are still some rough edges, but it is good enough for our needs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 文档也没有特别更新这些缺失的API，所以如果你发现事情没有按预期工作，请不要感到惊讶。希望这很快就会得到修复。同时，你可以使用旧版本。我们使用了Carla
    0.9.9.2，还有一些粗糙的边缘，但对于我们的需求来说已经足够好了。
- en: Now that we know more about CARLA, let's see how we can record our dataset,
    starting with only one video stream.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对CARLA有了更多的了解，让我们看看我们如何录制我们的数据集，从只有一个视频流开始。
- en: Recording one video stream
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 录制一个视频流
- en: In principle, recording one video stream with Carla is very simple, because
    there is already an option to do so. If you run `manual_control.py`, from the
    `PythonAPI/examples` directory, when you press *R*, it starts to record.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，使用Carla录制一个视频流非常简单，因为已经有一个选项可以这样做。如果你从`PythonAPI/examples`目录运行`manual_control.py`，当你按下*R*键时，它就开始录制。
- en: The problem is that we also want the steering angle. Normally, you could save
    this data in a database of some type, a CSV file, or a pickle file. To keep things
    simpler and focused on the core task, we will just add the steering angle and
    some other data to the filename. This makes it a bit easier for you to build the
    dataset, as you might want to record multiple runs dedicated to fixing a specific
    problem, and you can just move the files to a new directory and easily preserve
    all the information without having to update the path on a database.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是我们还想要转向角度。通常，你可以将此类数据保存到某种类型的数据库中，CSV文件或pickle文件。为了使事情更简单并专注于核心任务，我们只需将转向角度和一些其他数据添加到文件名中。这使得你构建数据集变得更容易，因为你可能想要记录多个针对特定问题修复的运行，你只需将文件移动到新目录中，就可以轻松地保留所有信息，而无需在数据库中更新路径。
- en: But if you don't like it, feel free to use a better system.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你不喜欢，请随意使用更好的系统。
- en: We could write a client from scratch that integrates with the Carla server and
    does what we need, but for simplicity and to better isolate the changes required,
    we will just copy `manual_control.py` to a file called `manual_control_recording.py`,
    and we will just add what we need.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从头开始编写一个与Carla服务器集成的客户端，并完成我们所需的功能，但为了简单起见，并更好地隔离所需更改，我们只需将`manual_control.py`复制到一个名为`manual_control_recording.py`的文件中，然后我们只需添加所需的内容。
- en: Please remember that this file should run in the `PythonAPI/examples` directory.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个文件应该在 `PythonAPI/examples` 目录下运行。
- en: 'The first thing that we want to do is change track to `Town04`, because it
    is more interesting than the default track:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先想做的事情是将轨道改为 `Town04`，因为它比默认轨道更有趣：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The previous code needs to go in the `game_loop()` method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码需要放入 `game_loop()` 方法中。
- en: The variable client is clearly the client connecting to the Carla server.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 client 明显是连接到 Carla 服务器的客户端。
- en: 'We also need to change the spawn point (the place where the simulation starts)
    to be fixed, because normally, this changes every time:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将出生点（模拟开始的地方）改为固定，因为通常，它会每次都改变：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we need to change the name of the file. While we are at it, we will not
    only save the steering angle, but also the throttle and the brake. We will not
    use them, but if you want to experiment, they will be there for you. The following
    method should be defined in the `CameraManager` class:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要更改文件名。在此过程中，我们不仅会保存转向角度，还会保存油门和刹车。我们可能不会使用它们，但如果你想进行实验，它们将为你提供。以下方法应在
    `CameraManager` 类中定义：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can save the file as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按以下方式保存文件：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `image.frame` variable contains the number of the current frame, and `camera_name`
    for now is not important, but it will have the `MAIN` value.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`image.frame` 变量包含当前帧的编号，而 `camera_name` 目前并不重要，但它的值将是 `MAIN`。'
- en: The `image` variable also contains the current image that we want to save.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`image` 变量还包含我们想要保存的当前图像。'
- en: 'You should get names similar to the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似以下的名字：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the previous file name, you can identify the following components:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个文件名中，你可以识别以下组件：
- en: The frame number (`00078843`)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帧编号（`00078843`）
- en: The camera (`MAIN`)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相机（`MAIN`）
- en: The steering angle (`0.000000`)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转向角度（`0.000000`）
- en: The throttle (`0.500000`)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 油门（`0.500000`）
- en: The brake (`0.000000`)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刹车（`0.000000`）
- en: 'This is the image, in my case:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图像，以我的情况为例：
- en: '![Figure 8.2 – One frame from Carla, steering 0 degrees](img/Figure_8.2_B16322.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – Carla 的一帧，转向 0 度](img/Figure_8.2_B16322.jpg)'
- en: Figure 8.2 – One frame from Carla, steering 0 degrees
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – Carla 的一帧，转向 0 度
- en: This frame is fine, but not great. I should have stayed in another lane, or
    the steering should have been slightly pointed toward the right. In the case of
    behavioral cloning, the car learns from you, so how you drive is important. Controlling
    Carla with a keyboard is not great, and when recording, it works worse because
    of the time spent saving the images.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这帧还可以，但并不完美。我应该待在另一条车道上，或者转向应该稍微指向右边。在行为克隆的情况下，汽车会从你那里学习，所以你的驾驶方式很重要。用键盘控制 Carla
    并不好，而且在记录时，由于保存图像所花费的时间，效果更差。
- en: The real problem is that we need to record three cameras, not just one. Let's
    see how to do that.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的问题是，我们需要记录三个相机，而不仅仅是其中一个。让我们看看如何做到这一点。
- en: Recording three video streams
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记录三个视频流
- en: To record three video streams, the starting point is to have three cameras.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要记录三个视频流，起点是拥有三个相机。
- en: 'By default, Carla has the following five cameras:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Carla 有以下五个相机：
- en: A classical *third-person* view, from the back, above the car
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个经典的 *第三人称* 视角，从车后上方
- en: From the front of the car, toward the road (looking forward)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从车前朝向道路（向前看）
- en: From the front of the car, toward the car (looking backward)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从车前朝向汽车（向后看）
- en: From far above
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从高空
- en: From the left
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左侧
- en: 'Here, you can see the first three cameras:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到前三个相机：
- en: '![Figure 8.3 – Cameras from above, toward the road, and toward the car](img/Figure_8.3_B16322.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 从上方、朝向道路和朝向汽车的相机](img/Figure_8.3_B16322.jpg)'
- en: Figure 8.3 – Cameras from above, toward the road, and toward the car
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 从上方、朝向道路和朝向汽车的相机
- en: The second camera looks very interesting to us.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个相机对我们来说非常有趣。
- en: 'The following are taken from the remaining two cameras:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从剩余的两个相机中获取的：
- en: '![Figure 8.4 – Carla cameras from far above and from the left](img/Figure_8.4_B16322.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 从高空和左侧的 Carla 相机](img/Figure_8.4_B16322.jpg)'
- en: Figure 8.4 – Carla cameras from far above and from the left
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 从高空和左侧的 Carla 相机
- en: The last camera is also somehow interesting, though we do not want to record
    the car in our frames. We are missing the camera from the right because, for some
    reason, the authors of Carla haven't added it to the list.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个相机也有些有趣，尽管我们不想在我们的帧中记录汽车。由于某种原因，Carla 的作者没有将其添加到列表中，所以我们缺少右侧的相机。
- en: 'Luckily, changing the cameras or adding a new one is quite simple. This is
    the definition of the original cameras, the `CameraManager` constructor:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，更换相机或添加新相机相当简单。这是原始相机的定义，`CameraManager`构造函数：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a first attempt, we can keep just the second and the fifth camera, but we
    want them at comparable positions. Carla has been written using a very famous
    engine for video-games: *Unreal Engine 4*. In *Unreal Engine*, the *z* axis is
    the vertical one (up and down), the *x* axis is for forward and backward, and
    the *y* axis is for lateral movements, left and right. So, we want the cameras
    to have the same *x* and *z* coordinates. We also want a third camera, from the
    right. For this, it is enough to change the sign of the *y* coordinates. This
    is the resulting code, only for the cameras:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一次尝试，我们可以只保留第二和第五个相机，但我们希望它们处于可比较的位置。Carla是用一个非常著名的游戏引擎编写的：*Unreal Engine
    4*。在*Unreal Engine*中，*z*轴是垂直轴（上下），*x*轴用于前后，*y*轴用于横向移动，左右。因此，我们希望相机具有相同的*x*和*z*坐标。我们还想有一个第三个相机，从右侧。为此，只需改变*y*坐标的符号即可。这是仅针对相机的结果代码：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You could probably stop here. I ended up moving the lateral cameras more to
    the side, which can be done by changing `bound_y`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能可以在这里停止。我最终将侧向相机移动得更靠边，这可以通过更改`bound_y`来实现。
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'These are the images that we get now:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们现在得到的图像：
- en: '![Figure 8.5 – New cameras: from the left, from the front (main camera), and
    from the right](img/Figure_8.5_B16322.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 新相机：从左、从正面（主相机）和从右](img/Figure_8.5_B16322.jpg)'
- en: 'Figure 8.5 – New cameras: from the left, from the front (main camera), and
    from the right'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 新相机：从左、从正面（主相机）和从右
- en: Now, it should be easier to understand that the left and right cameras can be
    used to teach the neural network how to correct the trajectory, if it is not in
    the correct location, compared to the main camera. This, of course, assumes that
    the stream recorded by the main camera is the intended position.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，应该更容易理解，与主相机相比，左右相机可以用来教神经网络如何纠正轨迹，如果它不在正确的位置。当然，这假设主相机录制的流是预期的位置。
- en: 'Even if the correct cameras are now available, they are not in use. We need
    to add them, in `World. restart()`, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 即使现在有了正确的相机，它们也没有在使用。我们需要在`World.restart()`中添加它们，如下所示：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `CameraManager.add_camera()` method is defined as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`CameraManager.add_camera()`方法定义如下：'
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'What this code does is the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的作用如下：
- en: Sets up a sensor, using the specified camera
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的相机设置传感器
- en: Adds the sensor to a list
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将传感器添加到列表中
- en: Instructs the sensor to call a lambda function that invokes the `save_image()`
    method
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示传感器调用一个lambda函数，该函数调用`save_image()`方法
- en: 'The following `get_camera_name()` method is used to get a meaningful name to
    the camera, based on its index, which is dependent on the cameras that we defined
    earlier:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的`get_camera_name()`方法用于根据其索引为相机获取一个有意义的名称，该索引依赖于我们之前定义的相机：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Before looking at the code of `save_image()`, let's discuss a small issue.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看`save_image()`的代码之前，让我们讨论一个小问题。
- en: Recording three cameras for every frame is kind of slow, resulting in low **Frames
    Per Second** (**FPS**), which makes it difficult to drive the car. As a consequence,
    you would over-correct, recording a sub-optimal dataset where you basically teach
    the car how to zig-zag. To limit this problem, we will record only one camera
    view for each frame, then we rotate to the next camera view for the next frame,
    and we will cycle through all three camera views during recording. After all,
    consecutive frames are similar, so it is not a huge problem.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 每一帧录制三个相机有点慢，导致**每秒帧数**（**FPS**）低，这使得驾驶汽车变得困难。因此，你会过度纠正，录制一个次优的数据集，基本上是在教汽车如何蛇形行驶。为了限制这个问题，我们将为每一帧只录制一个相机视图，然后在下一帧旋转到下一个相机视图，我们将在录制过程中循环所有三个相机视图。毕竟，连续的帧是相似的，所以这不是一个大问题。
- en: The camera used by Nvidia was recording at 30 FPS, but they decided to skip
    most of the frames, recording only at 10 FPS, because the frames were very similar,
    increasing the training time without adding much information. You would not record
    at the highest speed, but your dataset would be better, and if you want a bigger
    dataset, you can always just drive more.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 英伟达使用的相机以30 FPS的速度录制，但他们决定跳过大多数帧，只以10 FPS的速度录制，因为帧非常相似，这样会增加训练时间而不会增加太多信息。你不会以最高速度录制，但你的数据集会更好，如果你想有一个更大的数据集，你总是可以多开一些车。
- en: 'The `save_image()` function needs to first check whether this is a frame that
    we want to record:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`save_image()` 函数需要首先检查这是否是我们想要记录的帧：'
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The second step is to convert the image into a format suitable for OpenCV,
    as we are going to use it to save the image. We need to convert the raw buffer
    to NumPy, and we also need to drop one channel, because Carla produces images
    with BGRA, with four channels: blue, green, red, and alpha (transparency):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是将图像转换为适合OpenCV的格式，因为我们将要使用它来保存图像。我们需要将原始缓冲区转换为NumPy，我们还需要删除一个通道，因为Carla产生的图像是BGRA，有四个通道：蓝色、绿色、红色和透明度（不透明度）：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can resize the image, crop the part that we need, and save it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以调整图像大小，裁剪我们需要的部分，并保存它：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can see in the code repository in GitHub that I recorded a fair amount of
    frames, enough to drive for one or two turns, but if you want to drive along the
    whole track, you will need many more frames, and the better you drive, the better
    it is.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub的代码仓库中看到，我记录了大量帧，足以驾驶一两个转弯，但如果你想沿着整个赛道驾驶，你需要更多的帧，而且你开得越好，效果越好。
- en: Now that we have the cameras, we need to use them to build the dataset that
    we need.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了摄像头，我们需要使用它们来构建我们所需的数据集。
- en: Recording the dataset
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记录数据集
- en: To build the dataset, clearly, you need to record at least the turns that you
    expect your network to make. The more the better. But you should also record movements
    that help your car correct the trajectory. The left and right camera already help
    quite a lot, but you should also record a few stints where the car is close to
    the edge of the road, and the steering wheel is turning it toward the center.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，为了构建数据集，你需要记录至少你期望你的网络做出的转弯。越多越好。但你也应该记录有助于你的汽车纠正轨迹的运动。左右摄像头已经帮助很多，但你应该也记录一些汽车靠近道路边缘，方向盘将其转向中心的段。
- en: 'For example, consider something like the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下内容：
- en: '![Figure 8.6 – Car close to the left, steering to the right](img/Figure_8.6_B16322.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 车辆靠近左侧，转向右侧](img/Figure_8.6_B16322.jpg)'
- en: Figure 8.6 – Car close to the left, steering to the right
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 车辆靠近左侧，转向右侧
- en: If there are turns that don't go the way you want, you can try to record them
    more than once, as I did.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有转弯没有按照你想要的方式进行，你可以尝试记录它们多次，就像我这样做。
- en: Now, you might see the advantage of encoding the steering wheel in the name
    of the image. You can group these correction stints, or whatever you prefer, on
    dedicated directories, and take them in and out of the dataset as required.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能看到了将方向盘编码到图像名称中的优势。你可以将这些纠正段，或者你喜欢的任何内容，放在专用目录中，根据需要将它们添加到数据集中或从中移除。
- en: If you want, you can even manually select a part of the pictures, to correct
    for wrong steering angles, though this might not be necessary if there is a limited
    number of frames with the wrong angle.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，甚至可以手动选择图片的一部分，以纠正错误的转向角度，尽管如果只有有限数量的帧有错误的角度，这可能不是必要的。
- en: Despite saving only one camera per frame, you might still find it difficult
    to drive, above all in regard to the speed. I personally prefer to limit the throttle
    so that the car does not go too fast, but I can still slow down if I want.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每帧只保存一个摄像头，但你可能仍然发现驾驶很困难，尤其是在速度方面。我个人更喜欢限制加速踏板，这样汽车不会开得太快，但当我想要减速时，我仍然可以减速。
- en: 'The throttle can usually reach the value of `1`, so to limit it, it is enough
    to use a line of code similar to the following, in the `KeyboardControl ._parse_vehicle_keys()`
    method:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 加速踏板通常可以达到`1`的值，所以要限制它，只需要使用类似以下的一行代码，在`KeyboardControl ._parse_vehicle_keys()`方法中：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To increase fluidity, you might run the client with a lower resolution:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加流畅性，你可能需要以较低的分辨率运行客户端：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can also lower the resolution of the server, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以降低服务器的分辨率，如下所示：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that you have the raw dataset, it's time to create the real dataset, with
    the proper steering angles.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了原始数据集，是时候创建真实数据集了，带有适当的转向角度。
- en: Preprocessing the dataset
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: The dataset that we recorded is raw, meaning that it needs some preprocessing
    before being ready to be used.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记录的数据集是原始的，这意味着在使用之前需要一些预处理。
- en: The most important thing to do is to correct the steering angle for the left
    and right camera.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是要纠正左右摄像头的转向角度。
- en: For convenience, this is done by an additional program so that you can eventually
    change it without having to record the frames again.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，这是通过一个额外的程序完成的，这样你最终可以更改它，而无需再次记录帧。
- en: 'To start, we need a method to extract the data from the name (we assume the
    file is a JPG or a PNG):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一种方法从名称中提取数据（我们假设文件是JPG或PNG格式）：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `to_float` method is just a convenience to convert -0 to 0.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_float`方法只是一个方便的转换，将-0转换为0。'
- en: 'Now, changing the steering angle is simple:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，改变转向角度很简单：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: I added a correction of 0.25\. If your camera is closer to the car, you might
    want to use a smaller number.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了0.25的校正。如果你的相机离车更近，你可能想使用更小的数字。
- en: While we are at it, we can also add the frames mirrored, to increase the size
    of the dataset a bit.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们还可以添加镜像帧，以稍微增加数据集的大小。
- en: Now that we have converted the dataset, we are ready to train a neural network
    similar to DAVE-2 to learn how to drive.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经转换了数据集，我们准备训练一个类似于DAVE-2的神经网络来学习如何驾驶。
- en: Modeling the neural network
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络建模
- en: 'To create our neural network, we will take inspiration from DAVE-2, which is
    a surprisingly simple neural network:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的神经网络，我们将从DAVE-2中汲取灵感，这是一个出奇简单的神经网络：
- en: 'We start with a lambda layer, to confine the image pixels in the (-1, +1) range:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从一个lambda层开始，将图像像素限制在(-1, +1)范围内：
- en: '[PRE23]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, there are three convolutional layers with kernel size `5` and strides
    `(2,2)`, which halves the output resolution, and three convolutional layers with
    kernel size `3`:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，有三个大小为`5`和步长`(2,2)`的卷积层，它们将输出分辨率减半，以及三个大小为`3`的卷积层：
- en: '[PRE24]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we have the dense layers:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们有密集层：
- en: '[PRE25]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: I am always amazed when I think that these few lines of code are enough to somehow
    allow a car to drive by itself on a real road!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当我想这些几行代码足以让汽车在真实道路上自动驾驶时，我总是感到惊讶！
- en: While it looks more or less similar to other neural networks that we saw before,
    there is a very important difference—the last activation is not a softmax function,
    because this is not a classifier, but a neural network that needs to perform a
    *regression* task, predicting the correct steering angle given an image.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它看起来与之前我们看到的其他神经网络或多或少相似，但有一个非常重要的区别——最后的激活函数不是softmax函数，因为这不是一个分类器，而是一个需要执行*回归*任务的神经网络，根据图像预测正确的转向角度。
- en: We say that a neural network is performing a regression when it is trying to
    predict a value in a potentially continuous interval—for example, between –1 and
    +1\. By comparison, in a classification task, the neural network is trying to
    predict which label is more likely correct and probably represents the content
    of the image. A neural network that can distinguish between cats and dogs is therefore
    a classifier, while a network that tries to predict the cost of an apartment based
    on the size and location is performing regression.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络试图在一个可能连续的区间内预测一个值时，我们称其为回归，例如在-1和+1之间。相比之下，在分类任务中，神经网络试图预测哪个标签更有可能是正确的，这很可能代表了图像的内容。因此，能够区分猫和狗的神经网络是一个分类器，而试图根据大小和位置预测公寓成本的神经网络则是在执行回归任务。
- en: Let's see what we need to change to use a neural network for regression.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们需要更改什么才能使用神经网络进行回归。
- en: Training a neural network for regression
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练回归神经网络
- en: As we have already seen, a difference is the lack of a softmax layer. In its
    place, we used Tanh (hyperbolic tangent), an activation useful to generate values
    in the range (-1, +1), which is the range that we need for the steering angle.
    However, in principle, you could not even have an activation and directly use
    the value of the last neuron.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，一个区别是缺少softmax层。取而代之的是，我们使用了Tanh（双曲正切），这是一个用于生成(-1, +1)范围内值的激活函数，这正是我们需要用于转向角度的范围。然而，原则上，你甚至可以没有激活函数，直接使用最后一个神经元的值。
- en: 'The following figure shows the Tanh function:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了Tanh函数：
- en: '![Figure 8.7 – The tanh function](img/Figure_8.7_B16322.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – tanh函数](img/Figure_8.7_B16322.jpg)'
- en: Figure 8.7 – The tanh function
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – tanh函数
- en: As you can see, Tanh will limit the range of the activation to the (-1, +1)
    range.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Tanh将激活函数的范围限制在(-1, +1)范围内。
- en: Usually, when we train a classifier, as in the case of MNIST or CIFAR-10, we
    use `categorical_crossentropy` as a loss and `accuracy` as a metric. However,
    for regression, we need to use `mse` for the loss and we can optionally use `cosine_proximity`
    as a metric.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当我们训练一个分类器，例如MNIST或CIFAR-10的情况，我们使用`categorical_crossentropy`作为损失函数，`accuracy`作为指标。然而，对于回归问题，我们需要使用`mse`作为损失函数，并且我们可以选择性地使用`cosine_proximity`作为指标。
- en: 'The cosine proximity is an indication of similarity for vectors. So, 1 means
    that they are identical, 0 that they are perpendicular, and -1 that they are opposite.
    The loss and metric code snippet looks as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度是向量的相似性指标。所以，1表示它们是相同的，0表示它们是垂直的，-1表示它们是相反的。损失和度量代码片段如下：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The rest of the code is as it is for classifiers, except that we don't need
    to use one-hot encoding.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码与分类器相同，只是我们不需要使用one-hot编码。
- en: 'Let''s see the graph for the training:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练的图表：
- en: '![Figure 8.8 – Behavioral cloning with DAVE-2, training](img/Figure_8.8_B16322.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 使用DAVE-2的行为克隆，训练](img/Figure_8.8_B16322.jpg)'
- en: Figure 8.8 – Behavioral cloning with DAVE-2, training
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 使用DAVE-2的行为克隆，训练
- en: 'You can see slight overfitting. This is the value of the losses:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到轻微的过拟合。这是损失值：
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The loss, in this case, is the mean square error between the steering angle
    recorded for training and the angle computed by the network. We can see that the
    validation loss is quite good. If you have time, you can try to experiment with
    the model, adding dropouts or even changing the whole structure.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，损失是训练中记录的转向角与网络计算的角度之间的均方误差。我们可以看到验证损失相当好。如果你有时间，你可以尝试对这个模型进行实验，添加dropout或甚至改变整个结构。
- en: Soon, we will integrate our neural network with Carla and see how it drives,
    but before that, it could be legit to wonder whether the neural network is actually
    focusing its attention on the right parts of the road. The next section will show
    us how to do this, using a technique called **saliency maps**.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，我们将把我们的神经网络与Carla集成，看看它是如何驾驶的，但在那之前，质疑神经网络是否真的专注于道路的正确部分可能是合理的。下一节将展示我们如何使用称为**显著性图**的技术来做这件事。
- en: Visualizing the saliency maps
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化显著性图
- en: 'To understand what the neural network is focusing its attention on, we should
    use a practical example, so let''s choose an image:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解神经网络关注的是什么，我们应该使用一个实际例子，所以让我们选择一张图片：
- en: '![Figure 8.9 – Test image](img/Figure_8.9_B16322.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – 测试图像](img/Figure_8.9_B16322.jpg)'
- en: Figure 8.9 – Test image
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 测试图像
- en: If we had to drive on this road, as humans, we would pay attention to the lanes
    and the wall, though admittedly, the wall is not as important as the last lane
    is before that.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们作为人类必须在这条路上驾驶，我们会注意车道和墙壁，尽管诚然，墙壁的重要性不如之前的最后一个车道。
- en: 'We already know how to get an idea of what a **CNN** (short for **convolutional
    neural network**) such as DAVE-2 is taking into consideration: as the output of
    a convolution layer is an image, we can visualize it as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何了解一个**CNN**（卷积神经网络的简称）如DAVE-2在考虑什么：因为卷积层的输出是一个图像，我们可以这样可视化：
- en: '![Figure 8.10 – Part of the activations of the first convolutional layer](img/Figure_8.10_B16322.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 第一卷积层的部分激活](img/Figure_8.10_B16322.jpg)'
- en: Figure 8.10 – Part of the activations of the first convolutional layer
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 第一卷积层的部分激活
- en: This is a good starting point, but we would like something more. We would like
    to understand which pixels contribute the most to the prediction. For that, we
    need to get a **saliency map**.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的起点，但我们希望得到更多。我们希望了解哪些像素对预测贡献最大。为此，我们需要获取一个**显著性图**。
- en: 'Keras does not directly support them, but we can use `keras-vis`. You can install
    it with `pip`, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Keras不支持它们，但我们可以使用`keras-vis`。你可以用`pip`安装它，如下所示：
- en: '[PRE28]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The first step to get a saliency map is to create a model that starts with
    the input of our model but ends with the layer that we want to analyze. The resulting
    code is very similar to what we saw for the activations, except that for convenience,
    we also need the index of the layer:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 获取显著性图的第一步是创建一个从我们模型的输入开始但以我们想要分析的层结束的模型。生成的代码与我们所看到的激活非常相似，但为了方便，我们还需要层的索引：
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'While not necessary in our case, you might want to change the activation to
    become linear, then reload the model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在我们的情况下不是必需的，但你可能想将激活变为线性，然后重新加载模型：
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, it is just a matter of calling `visualize_saliency()`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需要调用`visualize_saliency()`：
- en: '[PRE31]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We are interested in the saliency map of the last layer, the output, but as
    an exercise, we will go through all the convolutional layers to see what they
    understand.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对最后一层，即输出的显著性图感兴趣，但作为一个练习，我们将遍历所有卷积层，看看它们理解了什么。
- en: 'Let''s see the saliency map for the first convolutional layer:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第一卷积层的显著性图：
- en: '![Figure 8.11 – Saliency map of the first convolutional layer](img/Figure_8.11_B16322.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 第一卷积层的显著性图](img/Figure_8.11_B16322.jpg)'
- en: Figure 8.11 – Saliency map of the first convolutional layer
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 第一卷积层的显著性图
- en: Not very impressive, as there is no saliency and we only see the original image.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是很令人印象深刻，因为没有显著性，我们只能看到原始图像。
- en: 'Let''s see how the map of the second layer looks:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第二层的地图：
- en: '![Figure 8.12 – Saliency map of the second convolutional layer](img/Figure_8.12_B16322.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 第二卷积层的显著性图](img/Figure_8.12_B16322.jpg)'
- en: Figure 8.12 – Saliency map of the second convolutional layer
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 第二卷积层的显著性图
- en: 'This is an improvement, but even if we see some attention in the middle line,
    on the wall and the land after the right lane, it is not very clear. Let''s see
    the third layer:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种改进，但即使我们在中间线、墙上和右车道后的土地上看到了一些注意力，也不是很清晰。让我们看看第三层：
- en: '![Figure 8.13 – Saliency map of the third convolutional layer](img/Figure_8.13_B16322.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13 – 第三卷积层的显著性图](img/Figure_8.13_B16322.jpg)'
- en: Figure 8.13 – Saliency map of the third convolutional layer
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 第三卷积层的显著性图
- en: 'Now we are talking! We can see great attention on the central and left line,
    and some attention focused on the wall and the right line. The network seems to
    be trying to understand where the road ends. Let''s also see the fourth layer:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在才是重点！我们可以看到对中央线和左线的极大关注，以及一些注意力集中在墙上和右线上。网络似乎在试图理解道路的尽头在哪里。让我们也看看第四层：
- en: '![Figure 8.14 – Saliency map of the fourth convolutional layer](img/Figure_8.14_B16322.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – 第四卷积层的显著性图](img/Figure_8.14_B16322.jpg)'
- en: Figure 8.14 – Saliency map of the fourth convolutional layer
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – 第四卷积层的显著性图
- en: Here, we can see that the attention is mostly focused on the central line, but
    there are also sparks of attention on the left line and on the wall, as well as
    a bit on the whole road.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到注意力主要集中在中央线上，但左线和墙上也有注意力的火花，以及在整个道路上的少许注意力。
- en: 'We can also check the fifth and last convolutional layer:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查第五个也是最后一个卷积层：
- en: '![Figure 8.15 – Saliency map of the fifth convolutional layer](img/Figure_8.15_B16322.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.15 – 第五卷积层的显著性图](img/Figure_8.15_B16322.jpg)'
- en: Figure 8.15 – Saliency map of the fifth convolutional layer
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – 第五卷积层的显著性图
- en: The fifth layer is similar to the fourth layer, plus with some more attention
    on the left line and on the wall.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第五层与第四层相似，并且对左线和墙上的注意力有所增加。
- en: 'We can also visualize the saliency map for dense layers. Let''s see the result
    for the last layer, which is what we consider the real saliency map for this image:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以可视化密集层的显著性图。让我们看看最后一层的成果，这是我们认为是该图像真实显著性图的地方：
- en: '![Figure 8.16 – Saliency map of the output layer](img/Figure_8.16_B16322.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16 – 输出层的显著性图](img/Figure_8.16_B16322.jpg)'
- en: Figure 8.16 – Saliency map of the output layer
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 输出层的显著性图
- en: The last saliency map, the most important one, shows great attention to the
    central line and the right line, plus some attention on the upper-right corner,
    which could be an attempt to estimate the distance from the right lane. We can
    also see some attention on the wall and the left lane. So, all in all, it seems
    promising.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个显著性图，最重要的一个，对中央线和右线给予了极大的关注，加上对右上角的少许关注，这可能是一次尝试估计右车道的距离。我们还可以看到一些注意力在墙上和左车道上。所以，总的来说，这似乎很有希望。
- en: 'Let''s try with another image:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用另一张图片试试：
- en: '![Figure 8.17 – Second test image](img/Figure_8.17_B16322.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17 – 第二测试图像](img/Figure_8.17_B16322.jpg)'
- en: Figure 8.17 – Second test image
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 – 第二测试图像
- en: This is an interesting image, as it is taken from a part of the road where the
    network has not been trained, but it still behaved very well.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张很有趣的图片，因为它是从网络尚未训练的道路部分拍摄的，但它仍然表现得很出色。
- en: 'Let''s see the saliency map of the third convolutional layer:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第三卷积层的显著性图：
- en: '![Figure 8.18 – Saliency map of the third convolutional layer](img/Figure_8.18_B16322.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.18 – 第三卷积层的显著性图](img/Figure_8.18_B16322.jpg)'
- en: Figure 8.18 – Saliency map of the third convolutional layer
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18 – 第三卷积层的显著性图
- en: The neural network seems very concerned with the end of the road and it seems
    to have detected a couple of trees as well. If it was trained for braking, I bet
    it would do so!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络似乎非常关注道路的尽头，并且似乎还检测到了几棵树。如果它被训练用于制动，我敢打赌它会这么做！
- en: 'Let''s see the final map:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最终的地图：
- en: '![Figure 8.19 – Saliency map of the output layer](img/Figure_8.19_B16322.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19 – 输出层的显著性图](img/Figure_8.19_B16322.jpg)'
- en: Figure 8.19 – Saliency map of the output layer
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 输出层的显著性图
- en: This is pretty similar to the previous one, but there is some attention to the
    central line and the right line, and a tiny amount on the road in general. Looks
    good to me.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的一个非常相似，但更加关注中央线和右侧线，以及一般道路上的一小部分。对我来说看起来不错。
- en: 'Let''s try with the last image, taken from the training to teach when to turn
    right:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用最后一张图像，这张图像是从训练中取出的，用于教授何时向右转：
- en: '![Figure 8.20 – Third test image](img/Figure_8.20_B16322.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图8.20 – 第三测试图像](img/Figure_8.20_B16322.jpg)'
- en: Figure 8.20 – Third test image
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 – 第三测试图像
- en: 'This is the final saliency map for it:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的最终显著性图：
- en: '![Figure 8.21 – Saliency map of the output layer](img/Figure_8.21_B16322.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图8.21 – 输出层的显著性图](img/Figure_8.21_B16322.jpg)'
- en: Figure 8.21 – Saliency map of the output layer
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21 – 输出层的显著性图
- en: You can see that the neural network is giving attention mostly to the right
    line, also keeping an eye on the whole road and with some spark of attention dedicated
    to the left line.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到神经网络主要关注右侧线，同时关注整个道路，并对左侧线投入了一些注意力的火花。
- en: As you can see, the saliency map can be a valid tool to understand the behavior
    of the network a bit more and do a kind of sanity check on its interpretation
    of the world.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，显著性图可以是一个有效的工具，帮助我们更好地理解网络的行为，并对其对世界的解释进行一种合理性检查。
- en: Now it is finally time to integrate with Carla and see how we are performing
    in the real world. Fasten your seatbelt, because we are going to drive, and our
    neural network will be in the driver's seat!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候与Carla集成，看看我们在现实世界中的表现如何了。系好安全带，因为我们将要驾驶，我们的神经网络将坐在驾驶座上！
- en: Integrating the neural network with Carla
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将神经网络与Carla集成
- en: We will now integrate our neural network with Carla, to achieve self-driving.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将我们的神经网络与Carla集成，以实现自动驾驶。
- en: As before, we start by making a copy of `manual_control.py`, which we could
    call `manual_control_drive.py`. For simplicity, I will only write the code that
    you need to change or add, but you can find the full source code on GitHub.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先复制`manual_control.py`文件，可以将其命名为`manual_control_drive.py`。为了简化，我将只编写你需要更改或添加的代码，但你可以在GitHub上找到完整的源代码。
- en: Please remember that this file should run in the `PythonAPI/examples` directory.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个文件应该在`PythonAPI/examples`目录下运行。
- en: In principle, letting our neural network take control of the steering wheel
    is quite simple, as we just need to analyze the current frame and set the steering.
    However, we also need to apply some throttle, or the car will not move!
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，让我们的神经网络控制方向盘相当简单，因为我们只需要分析当前帧并设置转向。然而，我们还需要施加一些油门，否则汽车不会移动！
- en: It's also very important that you run the inference phase in the game loop,
    or that you are really sure that it is running on the client, else the performance
    will drop substantially and your network will have a hard time driving due to
    the excess of latency between receiving the frame and sending the instruction
    to drive.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，你需要在游戏循环中运行推理阶段，或者你确实确信它在客户端上运行，否则性能将大幅下降，你的网络将难以驾驶，因为接收帧和发送驾驶指令之间的延迟过多。
- en: As the Carla client changes the car every time, the effect of the throttle will
    change, sometimes making your car too fast or too slow. You therefore need a way
    to change the throttle with a key, or you could always use the same car, which
    will be our solution.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Carla客户端每次都会更换汽车，油门的效果也会改变，有时会使你的汽车速度过快或过慢。因此，你需要一种方法通过按键来改变油门，或者你可以始终使用同一辆汽车，这将是我们的解决方案。
- en: 'You can get a list of the cars available in Carla with the following line of
    code:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码行获取Carla中可用的汽车列表：
- en: '[PRE32]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'At the time of writing, this produces the following list:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，这会产生以下列表：
- en: '[PRE33]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In `World.restart()`, you can select the car of your choice:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在`World.restart()`中，你可以选择你喜欢的汽车：
- en: '[PRE34]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Carla uses actors, which can represent vehicles, walkers, sensors, traffic
    lights, traffic signs, and so on; actors are created from templates called `try_spawn_actor()`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Carla使用演员，可以代表车辆、行人、传感器、交通灯、交通标志等；演员是通过名为`try_spawn_actor()`的模板创建的：
- en: '[PRE35]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If you run the code now, you will see the car but with the wrong point of view.
    Pressing the *Tab* key will fix it:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在运行代码，你会看到汽车，但视角是错误的。按下*Tab*键可以修复它：
- en: '![Figure 8.22 – Left: default initial camera, right: camera for self-driving](img/Figure_8.22_B16322.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图8.22 – 左：默认初始相机，右：自动驾驶相机](img/Figure_8.22_B16322.jpg)'
- en: 'Figure 8.22 – Left: default initial camera, right: camera for self-driving'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22 – 左：默认初始相机，右：自动驾驶相机
- en: 'If you want to start from the point where I was training the car, you should
    also set the starting point in the same method:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要从我在训练汽车的地方开始，你也需要在相同的方法中设置起始点：
- en: '[PRE36]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you don't do that, the car will be spawned in a random position, and it might
    have more problems driving.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你这样做，汽车将在随机位置生成，并且可能驾驶时会有更多问题。
- en: 'In `game_loop()`, we also need to select the proper track:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在`game_loop()`中，我们还需要选择合适的赛道：
- en: '[PRE37]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If you run it now, after pressing *Tab*, you should see something like the
    following:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在运行它，在按下*Tab*之后，你应该看到以下类似的内容：
- en: '![Figure 8.23 – Image from Carla, ready for self-driving](img/Figure_8.23_B16322.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图8.23 – Carla的图像，准备自动驾驶](img/Figure_8.23_B16322.jpg)'
- en: Figure 8.23 – Image from Carla, ready for self-driving
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 – Carla的图像，准备自动驾驶
- en: If you press *F1*, you can remove the information on the left.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按下*F1*，你可以移除左侧的信息。
- en: 'For convenience, we want to be able to trigger the self-driving mode on and
    off, so we need a variable for that, such as the following, and one to hold the
    computed steering angle in the constructor of `KeyboardControl`:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们希望能够触发自动驾驶模式的开和关，因此我们需要一个变量来处理，如下所示，以及一个在`KeyboardControl`构造函数中保存计算出的转向角度的变量：
- en: '[PRE38]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, in `KeyboardControl.parse_events()`, we will intercept the *D* key and
    switch the self-driving functionality on and off:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在`KeyboardControl.parse_events()`中，我们将拦截*D*键，并切换自动驾驶功能的开和关：
- en: '[PRE39]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The next step is resizing and saving the last image received from the server,
    when it is still in BGR format, in `CameraManager._parse_image()`. This is shown
    here:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将从服务器接收到的最后一张图像进行缩放并保存，当它仍然是BGR格式时，在`CameraManager._parse_image()`中。这在这里展示：
- en: '[PRE40]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `array` variable originally contains the image in BGR format, and `::-1`
    in NumPy reverses the order, so the last line of code effectively converts the
    image from BGR into RGB, before visualizing it.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`array`变量最初包含BGR格式的图像，而NumPy中的`::-1`反转了顺序，所以最后一行代码实际上在可视化之前将图像从BGR转换为RGB。'
- en: 'Now, we can load the model in `game_loop()`, outside of the main loop:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在`game_loop()`函数外部，主循环之外加载模型：
- en: '[PRE41]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we can run the model in `game_loop()`, inside the main loop, and save
    the steering, as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在`game_loop()`主循环内部运行模型，并保存转向，如下所示：
- en: '[PRE42]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The last thing to do is just to use the steering that we computed, put a fix
    throttle, and limit the maximum speed, while we are at it:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要做的事情就是使用我们计算出的转向，设置一个固定的油门，并限制最大速度，同时进行：
- en: '[PRE43]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This is all nice and good, except that it might not work because of a GPU error.
    Let's see what it is and how to overcome it.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很好，但是它可能因为GPU错误而无法工作。让我们看看是什么问题以及如何克服它。
- en: Making your GPU work
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让你的GPU工作
- en: 'You might get an error similar to this one:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会得到类似于以下错误的错误：
- en: '[PRE44]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: My understanding of what's happening is that there is a conflict with some component
    of Carla, either the server or the client, which results in the GPU not having
    enough memory. In particular, it is TensorFlow creating the problem, as it tries
    to allocate all the memory in the GPU.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我对发生的事情的理解是，与Carla的某些组件存在冲突，无论是服务器还是客户端，这导致GPU内存不足。特别是，TensorFlow在尝试在GPU中分配所有内存时造成了问题。
- en: 'Luckily, this is easily fixable with a few lines of the following code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这可以通过以下几行代码轻松修复：
- en: '[PRE45]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The call to `set_memory_growth()` instructs TensorFlow to allocate only part
    of the GPU RAM, and eventually allocate more if required, solving our problem.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_memory_growth()`的调用指示TensorFlow只分配GPU RAM的一部分，并在需要时最终分配更多，从而解决问题。'
- en: At this point, your car should be able to drive, so let's discuss a bit how
    it works.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你的汽车应该能够驾驶，让我们讨论一下它是如何工作的。
- en: Self-driving!
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶！
- en: Now, you could start running `manual_control_drive.py`, maybe instructing it
    to use a lower resolution, using the `--res 480x320` parameter.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以开始运行`manual_control_drive.py`，可能需要使用`--res 480x320`参数来降低分辨率。
- en: If you press the *D* key, the car should start to drive by itself. It's probably
    quite slow, but it should run, sometimes nicely, sometimes less nicely. It might
    not always take the turns that it is supposed to take. You can try to add images
    to the dataset or improve the architecture of the neural network – for example,
    by adding some dropout layers.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按下 *D* 键，汽车应该会开始自动行驶。它可能相当慢，但应该会运行，有时运行得很好，有时则不那么好。它可能不会总是按照它应该走的路线行驶。你可以尝试向数据集中添加图像或改进神经网络的架构——例如，通过添加一些dropout层。
- en: You could try to change the car or increase the speed. You might notice that
    at a higher speed, the car starts to move more erratically, as if the driver was
    drunk! This is due to the excessive latency between the car getting in the wrong
    position and the neural network reacting to it. I think this could be fixed partly
    with a computer fast enough to process many FPS. However, I think a real fix would
    be to also record higher speed runs, where the corrections would be stronger;
    this would require a better controller than the keyboard, and you should also
    insert the speed in the input, or have multiple neural networks and switch between
    them based on the speed.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试更换汽车或提高速度。你可能会注意到，在更高的速度下，汽车开始更加无规律地移动，就像司机喝醉了一样！这是由于汽车进入错误位置和神经网络对其做出反应之间的过度延迟造成的。我认为这可以通过一个足够快的计算机来部分解决，以便处理许多FPS。然而，我认为真正的解决方案还需要记录更高速度的运行，其中校正会更强烈；这需要一个比键盘更好的控制器，你还需要在输入中插入速度，或者拥有多个神经网络，并根据速度在它们之间切换。
- en: Interestingly, sometimes it can somehow also drive even if we are using the
    outside camera, with the result that our car is part of the image! Of course,
    the result is not good, and you get the *drunk drive* effect even at low speed.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，有时即使我们使用外部摄像头，它也能以某种方式驾驶，结果是我们的汽车成为了图像的一部分！当然，结果并不好，即使速度很低，你也会得到 *醉酒驾驶*
    的效果。
- en: 'Just out of curiosity, let''s check the saliency map. This is the image that
    we are sending to the network:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，让我们检查一下显著性图。这是我们发送给网络的图像：
- en: '![Figure 8.24 – Image from the back](img/Figure_8.24_B16322.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图8.24 – 后视图](img/Figure_8.24_B16322.jpg)'
- en: Figure 8.24 – Image from the back
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24 – 后视图
- en: 'Now, we can check the saliency map:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查显著性图：
- en: '![Figure 8.25 – Saliency maps: third convolution layer and output layer](img/Figure_8.25_B16322.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图8.25 – 显著性图：第三卷积层和输出层](img/Figure_8.25_B16322.jpg)'
- en: 'Figure 8.25 – Saliency maps: third convolution layer and output layer'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25 – 显著性图：第三卷积层和输出层
- en: The network is still able to recognize the lines and the road; however, it is
    very concerned about the car. My hypothesis is that the neural network *thinks*
    it is an obstacle and the road is ending.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 网络仍然能够识别线条和道路；然而，它非常关注汽车。我的假设是神经网络 *认为* 它是一个障碍物，道路在结束。
- en: If you want to teach the car how to drive well with this camera, or with any
    other one, you will need to train it with that specific camera. If you want the
    car to drive properly on another track, you will need to train it on that specific
    track. Eventually, if you train it on many tracks and on many conditions, it should
    be able to drive everywhere. But this means building a huge dataset, with millions
    of images. Eventually, if your dataset is too big, you will run out of memory.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要教汽车如何使用这个摄像头或任何其他摄像头来驾驶，你需要用那个特定的摄像头来训练它。如果你想汽车在另一条赛道上正确驾驶，你需要在那条特定的赛道上训练它。最终，如果你在许多赛道和许多条件下训练它，它应该能够去任何地方驾驶。但这意味着构建一个包含数百万图像的巨大数据集。最终，如果你的数据集太大，你会耗尽内存。
- en: In the next section, we will talk about generators, a technique that can help
    us overcome these problems.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论生成器，这是一种可以帮助我们克服这些问题的技术。
- en: Training bigger datasets using generators
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用生成器训练更大的数据集
- en: When training big datasets, memory consumption can be an issue. In Keras, one
    way to solve this problem is by using Python generators. A Python generator is
    a function that can lazily return a potentially infinite stream of values, with
    a very low memory footprint as you only need memory for one object, plus, of course,
    all the supporting data that you might need; a generator can be used as if it
    were a list. A typical generator has a loop, and for every object that needs to
    be part of the stream, it will use the `yield` keyword.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练大数据集时，内存消耗可能成为一个问题。在Keras中，解决这个问题的方法之一是使用Python生成器。Python生成器是一个可以懒加载地返回可能无限值流的函数，具有非常低的内存占用，因为你只需要一个对象的内存，当然，还需要所有可能需要的支持数据；生成器可以用作列表。典型的生成器有一个循环，并且对于需要成为流一部分的每个对象，它将使用`yield`关键字。
- en: In Keras, the generator needs to be aware of the batch size, because it needs
    to return a batch of samples and a batch of labels.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，生成器需要知道批次大小，因为它需要返回一个样本批次和一个标签批次。
- en: We will keep a list of the files to process, and we will write a generator that
    can use this list to return the image associated with it and the label.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留一个要处理的文件列表，我们将编写一个生成器，可以使用这个列表返回与之关联的图像和标签。
- en: 'We will write a generic generator that, hopefully, you can reuse on other cases,
    and it will accept four parameters:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写一个通用的生成器，希望你在其他情况下也能重用，它将接受四个参数：
- en: A list of IDs, which in our case are the filenames
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个ID列表，在我们的例子中是文件名
- en: A function to retrieve the input (the image) from the ID
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个从ID检索输入（图像）的函数
- en: A function to retrieve the label (the steering wheel) from the ID
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个从ID检索标签（方向盘）的函数
- en: The batch size
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次大小
- en: 'To start, we need a function that can return an image given a file:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个函数，可以返回给定文件的图像：
- en: '[PRE46]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We also need a function that, given a filename, can return the label, which
    in our case is the steering angle:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个函数，给定一个文件名，可以返回标签，在我们的例子中是转向角度：
- en: '[PRE47]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now write the generator, as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写生成器，如下所示：
- en: '[PRE48]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Every iteration in the `while` loop corresponds to an epoch, while the `for`
    loop generates all the batches required to complete each epoch; at the beginning
    of each epoch, we shuffle the IDs to improve the training.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环中的每次迭代对应一个周期，而`for`循环生成完成每个周期所需的全部批次；在每个周期的开始，我们随机打乱ID以改善训练。'
- en: 'In Keras, it used to be that you had to use the `fit_generator()` method, but
    nowadays, `fit()` is able to understand if the argument is a generator, but you
    still need to provide a couple of new parameters:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，过去你必须使用`fit_generator()`方法，但现在`fit()`能够理解如果参数是一个生成器，但你仍然需要提供一些新的参数：
- en: '`steps_per_epoch`: This gives how many batches there are in a single training
    epoch, which is the number of training samples divided by the batch size.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps_per_epoch`：这表示单个训练周期中批次的数量，即训练样本数除以批次大小。'
- en: '`validation_steps`: This gives how many batches there are in a single validation
    epoch, which is the number of validation samples divided by the batch size.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_steps`：这表示单个验证周期中批次的数量，即验证样本数除以批次大小。'
- en: 'This is the code that you need to use the `generator()` function that we just
    defined:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你需要使用我们刚刚定义的`generator()`函数的代码：
- en: '[PRE49]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Thanks to this code, you can now leverage very big datasets. However, there
    is also another application of generators: custom on-demand data augmentation.
    Let''s say a few words about it.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了这段代码，你现在可以利用非常大的数据集了。然而，生成器还有一个应用：自定义按需数据增强。让我们简单谈谈这个话题。
- en: Augmenting data the hard way
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件方式增强数据
- en: We already saw an easy way to perform data augmentation, using `ImageDataGenerator`
    in [*Chapter 7*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158), *Detecting Pedestrians
    and Traffic Lights*. This could be appropriate for classifiers, because the transformations
    applied to the image do not alter its classification. However, in our case, some
    of these transformations would require a change in the prediction. In fact, Nvidia
    designed a custom data augmentation, where the image is randomly shifted and the
    steering wheel is updated accordingly. This could be done with a generator, where
    we take the original image, apply the transformation, and correct the steering
    wheel based on the amount of shifting.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一种简单的方法来进行数据增强，使用[*第7章*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158)中的`ImageDataGenerator`，*检测行人和交通灯*。这可能适用于分类器，因为应用于图像的变换不会改变其分类。然而，在我们的情况下，这些变换中的一些会需要改变预测。实际上，英伟达设计了一种自定义数据增强，其中图像被随机移动，方向盘根据移动量相应更新。这可以通过生成器来完成，其中我们取原始图像，应用变换，并根据移动量调整方向盘。
- en: But we are not limited to just replicating the same amount of images that we
    have in input, but we could create less (filtering) or more; for example, mirroring
    could be applied at runtime, and as a result, we duplicate the images in memory,
    without having to store double the amount of images and saving, as a consequence,
    half of the file access and the JPEG decompression; though of course, we would
    need some CPU to flip the image.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不仅限于复制输入中相同数量的图像，我们还可以创建更少（过滤）或更多；例如，镜像可以在运行时应用，结果是在内存中重复图像，而不必存储双倍数量的图像和保存，因此节省了文件访问和JPEG解压缩的一半；当然，我们还需要一些CPU来翻转图像。
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went through many interesting topics.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了众多有趣的主题。
- en: We started by describing DAVE-2, an experiment of Nvidia with the goal to demonstrate
    that a neural network can learn how to drive on a road, and we decided to replicate
    the same experiment but on a much smaller scale. First, we collected the image
    from Carla, taking care of recording not only the main camera but also two additional
    side cameras, to teach the network how to correct errors.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先描述了DAVE-2，这是英伟达的一个实验，旨在证明神经网络可以学会在道路上驾驶，我们决定在更小的规模上复制相同的实验。首先，我们从Carla收集图像，注意不仅要记录主摄像头，还要记录两个额外的侧摄像头，以教会网络如何纠正错误。
- en: Then, we created our neural network, copying the architecture of DAVE-2, and
    we trained it for regression, which requires some changes compared to the other
    training that we did so far. We learned how to generate saliency maps and get
    a better understanding of where the neural network is focusing its attention.
    Then, we integrated with Carla and used the network to *self-drive* the car!
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建了我们的神经网络，复制了DAVE-2的架构，并对其进行回归训练，这与其他我们迄今为止所做的训练相比需要一些改变。我们学习了如何生成显著性图，并更好地理解神经网络关注的地方。然后，我们与Carla集成，并使用该网络来自动驾驶汽车！
- en: At the end, we learned how to train a neural network using Python generators,
    and we discussed how this can be used to achieve more sophisticated data augmentations.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何使用Python生成器训练神经网络，并讨论了如何利用这种方法实现更复杂的数据增强。
- en: In the next chapter, we will explore a state-of-the-art technique that can be
    used to detect the road at a pixel level—semantic segmentation.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索一种用于在像素级别检测道路的尖端技术——语义分割。
- en: Questions
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'After reading the chapter, you should be able to answer the following questions:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该能够回答以下问题：
- en: What is the original name of the neural network that Nvidia trained for self-driving?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 英伟达为自动驾驶训练的神经网络的原始名称是什么？
- en: What is the difference between a classification and a regression task?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类和回归任务之间的区别是什么？
- en: What is the Python keyword that you can use to create a generator?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用哪个Python关键字来创建生成器？
- en: What is a saliency map?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是显著性图？
- en: Why do we need to record three video streams?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要记录三个视频流？
- en: Why are we running inference from the `game_loop()` method?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们从`game_loop()`方法中进行推理？
- en: Further reading
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Nvidia DAVE-2: [https://devblogs.nvidia.com/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nvidia DAVE-2: [https://devblogs.nvidia.com/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)'
- en: 'Notes related to Carla 0.9.0 API changes: [https://carla.org/2018/07/30/release-0.9.0/](https://carla.org/2018/07/30/release-0.9.0/)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '与Carla 0.9.0 API变更相关的笔记: [https://carla.org/2018/07/30/release-0.9.0/](https://carla.org/2018/07/30/release-0.9.0/)'
- en: 'Carla: [https://carla.org](https://carla.org)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carla: [https://carla.org](https://carla.org)'
- en: '`keras-vis`: [https://github.com/raghakot/keras-vis](https://github.com/raghakot/keras-vis)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras-vis`: [https://github.com/raghakot/keras-vis](https://github.com/raghakot/keras-vis)'
