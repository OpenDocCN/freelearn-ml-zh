- en: Parallel Programming using CUDA C
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CUDA C 进行并行编程
- en: In the last chapter, we saw how easy it is to install CUDA and write a program
    using it. Though the example was not impressive, it was shown to convince you
    that it is very easy to get started with CUDA. In this chapter, we will build
    upon this concept. It teaches you to write advance programs using CUDA for GPUs
    in detail. It starts with a variable addition program and then incrementally builds
    towards complex vector manipulation examples in CUDA C. It also covers how the
    kernel works and how to use device properties in CUDA programs. The chapter discusses
    how vectors are operated upon in CUDA programs and how CUDA can accelerate vector
    operations compared to CPU processing. It also discusses terminologies associated
    with CUDA programming.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了安装 CUDA 和使用它的程序是多么容易。尽管示例并不令人印象深刻，但它被用来向您证明开始使用 CUDA 非常容易。在本章中，我们将在此基础上进行构建。它详细介绍了如何使用
    CUDA 为 GPU 编写高级程序。它从一个变量加法程序开始，然后逐步构建到 CUDA C 中的复杂向量操作示例。它还涵盖了内核的工作原理以及如何在 CUDA
    程序中使用设备属性。本章讨论了在 CUDA 程序中如何操作向量，以及与 CUDA 编程相关的术语。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The concept of the kernel call
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核调用的概念
- en: Creating kernel functions and passing parameters to it in CUDA
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CUDA 中创建内核函数并将参数传递给它
- en: Configuring kernel parameters and memory allocation for CUDA programs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 CUDA 程序的内核参数和内存分配
- en: Thread execution in CUDA programs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA 程序中的线程执行
- en: Accessing GPU device properties from CUDA programs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 CUDA 程序中访问 GPU 设备属性
- en: Working with vectors in CUDA programs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CUDA 程序中处理向量
- en: Parallel communication patterns
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行通信模式
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires familiarity with the basic C or C++ programming language,
    particularly dynamic memory allocation functions. All the code used in this chapter
    can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).
    The code can be executed on any operating system, though it is only tested on
    Windows 10 and Ubuntu 16.04\.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要熟悉基本的 C 或 C++ 编程语言，特别是动态内存分配函数。本章中使用的所有代码都可以从以下 GitHub 链接下载：[https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA)。代码可以在任何操作系统上执行，尽管它只在
    Windows 10 和 Ubuntu 16.04 上进行了测试。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际运行情况：
- en: '[http://bit.ly/2PQmu4O](http://bit.ly/2PQmu4O)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2PQmu4O](http://bit.ly/2PQmu4O)'
- en: CUDA program structure
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA 程序结构
- en: We have seen a very simple `Hello, CUDA!` program earlier, that showcased some
    important concepts related to CUDA programs. A CUDA program is a combination of
    functions that are executed either on the host or on the GPU device. The functions
    that do not exhibit parallelism are executed on the CPU, and the functions that
    exhibit data parallelism are executed on the GPU. The GPU compiler segregates
    these functions during compilation. As seen in the previous chapter, functions
    meant for execution on the device are defined using the `__global__` keyword and
    compiled by the NVCC compiler, while normal C host code is compiled by the C compiler.
    A CUDA code is basically the same ANSI C code with the addition of some keywords
    needed for exploiting data parallelism.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经看到了一个非常简单的 `Hello, CUDA!` 程序，它展示了与 CUDA 程序相关的一些重要概念。CUDA 程序是由在主机或 GPU
    设备上执行的功能组合而成。不显示并行性的功能在 CPU 上执行，而显示数据并行的功能在 GPU 上执行。GPU 编译器在编译过程中将这些功能分开。正如前一章所看到的，用于在设备上执行的功能使用
    `__global__` 关键字定义，并由 NVCC 编译器编译，而正常的 C 主机代码由 C 编译器编译。CUDA 代码基本上是相同的 ANSI C 代码，增加了用于利用数据并行的某些关键字。
- en: So, in this section, a simple two-variable addition program is taken to explain
    important concepts related to CUDA programming, such as kernel calls, passing
    parameters to kernel functions from host to device, the configuration of kernel
    parameters, CUDA APIs needed to exploit data parallelism, and how memory allocation
    takes place on the host and the device.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们通过一个简单的双变量加法程序来解释与 CUDA 编程相关的重要概念，例如内核调用、从主机到设备传递内核函数的参数、内核参数的配置、用于利用数据并行的
    CUDA API，以及主机和设备上的内存分配是如何进行的。
- en: Two-variable addition program in CUDA C
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA C 中的双变量加法程序
- en: In the simple `Hello, CUDA!` code seen in [Chapter 1](26a373fe-8ec0-40bf-afb7-7db6a1d414c9.xhtml),
    *Introducing Cuda and Getting Started with Cuda,* the device function was empty.
    It had nothing to do. This section explains a simple addition program that performs
    addition of two variables on the device. Though it is not exploiting any data
    parallelism of the device, it is very useful for demonstrating important programming
    concepts of CUDA C. First, we will see how to write a kernel function for adding
    two variables.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](26a373fe-8ec0-40bf-afb7-7db6a1d414c9.xhtml)中看到的简单 `Hello, CUDA!` 代码，*介绍
    CUDA 和开始使用 CUDA*，设备函数是空的。它没有任何作用。本节解释了一个简单的加法程序，该程序在设备上执行两个变量的加法。尽管它没有利用设备的任何数据并行性，但它对于展示
    CUDA C 的重要编程概念非常有用。首先，我们将看到如何编写一个用于加法两个变量的内核函数。
- en: 'The code for the kernel function is shown here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 内核函数的代码如下所示：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `gpuAdd` function looks very similar to a normal `add` function implemented
    in ANSI C. It takes two integer variables `d_a` and `d_b` as inputs and stores
    the addition at the memory location indicated by the third integer pointer `d_c`.
    The return value for the device function is void because it is storing the answer
    in the memory location pointed to by the device pointer and not explicitly returning
    any value. Now we will see how to write the main function for this code. The code
    for the main function is shown here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpuAdd` 函数看起来与在 ANSI C 中实现的正常 `add` 函数非常相似。它接受两个整数变量 `d_a` 和 `d_b` 作为输入，并将加法存储在由第三个整数指针
    `d_c` 指示的内存位置。设备函数的返回值是 void，因为它将答案存储在设备指针指向的内存位置，而不是显式返回任何值。现在我们将看到如何编写此代码的主函数。主函数的代码如下所示：'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the `main` function, the first two lines define variables for host and device.
    The third line allocates memory of the `d_c` variable on the device using the
    `cudaMalloc` function. The `cudaMalloc` function is similar to the `malloc` function
    in C. In the fourth line of the main function, `gpuAdd` is called with `1` and
    `4` as two input variables and `d_c`, which is a device memory pointer as an output
    pointer variable. The weird syntax of the `gpuAdd` function, which is also called
    a kernel call, is explained in the next section. If the answer of `gpuAdd` needs
    to be used on the host, then it must be copied from the device's memory to the
    host's memory, which is done by the `cudaMemcpy` function. Then, this answer is
    printed using the `printf` function. The penultimate line frees the memory used
    on the device by using the `cudafree` function. It is very important to free up
    all the memory used on the device explicitly from the program; otherwise, you
    might run out of memory at some point. The lines that start with `//` are comments
    for more code readability, and these lines are ignored by compilers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `main` 函数中，前两行定义了主机和设备的变量。第三行使用 `cudaMalloc` 函数在设备上为 `d_c` 变量分配内存。`cudaMalloc`
    函数与 C 中的 `malloc` 函数类似。在主函数的第四行中，`gpuAdd` 被调用，使用 `1` 和 `4` 作为两个输入变量，以及 `d_c`，这是一个设备内存指针，作为输出指针变量。`gpuAdd`
    函数的奇怪语法，也称为内核调用，将在下一节中解释。如果需要将 `gpuAdd` 的答案用于主机，则必须通过 `cudaMemcpy` 函数将其从设备的内存复制到主机的内存。然后，使用
    `printf` 函数打印此答案。最后一行使用 `cudafree` 函数释放设备上使用的内存。显式释放程序中使用的所有设备内存非常重要；否则，你可能会在某个时刻耗尽内存。以
    `//` 开头的行是用于提高代码可读性的注释，这些行被编译器忽略。
- en: 'The two-variable addition program has two functions, `main` and `gpuAdd`. As
    you can see, `gpuAdd` is defined by using the `__global__` keyword, and hence
    it is meant for execution on the device, while the main function will be executed
    on the host. The program adds two variables on the device and prints the output
    on the command line, as shown here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 双变量加法程序有两个函数，`main` 和 `gpuAdd`。正如你所见，`gpuAdd` 是通过使用 `__global__` 关键字定义的，因此它旨在在设备上执行，而主函数将在主机上执行。该程序在设备上添加两个变量，并在命令行上打印输出，如下所示：
- en: '![](img/9869b1ed-27f0-432f-ab68-0b41b03af8e1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9869b1ed-27f0-432f-ab68-0b41b03af8e1.png)'
- en: We will use a convention in this book that host variables will be prefixed with
    `h_` and device variables will be prefixed with `d_`. This is not compulsory;
    it is just done so that readers can understand the concepts easily without any
    confusion between host and device.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用一个约定，即主机变量将以前缀 `h_` 开头，设备变量将以前缀 `d_` 开头。这不是强制性的；这只是为了让读者能够轻松理解概念，而不会在主机和设备之间产生混淆。
- en: All CUDA APIs such as `cudaMalloc`, `cudaMemcpy`, and `cudaFree`, along with
    other important CUDA programming concepts such as kernel call, passing parameters
    to kernels, and memory allocation issues are discussed in upcoming sections.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 CUDA API，如 `cudaMalloc`、`cudaMemcpy` 和 `cudaFree`，以及其他重要的 CUDA 编程概念，如内核调用、向内核传递参数和内存分配问题，将在接下来的章节中讨论。
- en: A kernel call
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核调用
- en: 'The device code that is written using ANSI C keywords along with CUDA extension
    keywords is called a **kernel**. It is launched from the host code by a method
    called **kernel call**. Basically, the meaning of kernel call is that we are launching
    device code from the host code. A kernel call typically generates a large number
    of blocks and threads to exploit data parallelism on the GPU. Kernel code is very
    similar to normal C functions; it is just that this code is executed by several
    threads in parallel. It has a very weird syntax, which is as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ANSI C 关键字以及 CUDA 扩展关键字编写的设备代码被称为 **内核**。它通过一种称为 **内核调用** 的方法从主机代码中启动。基本上，内核调用的意义是我们从主机代码中启动设备代码。内核调用通常会产生大量的块和线程，以在
    GPU 上利用数据并行性。内核代码与普通 C 函数非常相似；只是这些代码是由多个线程并行执行的。它有一个非常奇怪的语法，如下所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It starts with the name of the kernel that we want to launch. You should make
    sure that this kernel is defined using the `__global__` keyword. Then, it has
    the `<< < > >>` kernel launch operator that contains configuration parameters
    for kernel. It can include three parameters separated by a comma. The first parameter
    indicates the number of blocks you want to execute, and the second parameter indicates
    the number of threads each block will have. So, the total number of threads started
    by a kernel launch will be the product of these two numbers. The third parameter,
    which specifies the size of shared memory used by the kernel, is optional. In
    the program for variable addition, the kernel launch syntax is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它以我们想要启动的内核的名称开始。你应该确保这个内核是用 `__global__` 关键字定义的。然后，它有 `<< < > >>` 内核启动运算符，其中包含内核的配置参数。它可以包含三个用逗号分隔的参数。第一个参数表示你想要执行的块的数量，第二个参数表示每个块将有多少线程。因此，内核启动启动的总线程数将是这两个数字的乘积。第三个参数，指定内核使用的共享内存的大小，是可选的。在变量加法程序中，内核启动的语法如下：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, `gpuAdd` is the name of a kernel that we want to launch, and `<<<1,1>>>`
    indicates we want to start one block with one thread per block, which means that
    we are starting only one thread. Three arguments in round brackets are the parameters
    that are passed to the kernel. Here, we are passing two constants, `1` and `4`.
    The third parameter is a pointer to device memory `d_c`. It points at the location
    on device memory where the kernel will store its answer after addition. One thing
    that the programmer has to keep in mind is that pointers passed as parameters
    to kernel should only point to device memory. If it is pointing to host memory,
    it can crash your program. After kernel execution is completed, the result pointed
    by the device pointer can be copied back to host memory for further use. Starting
    only one thread for execution on the device is not the optimal use of device resources.
    Suppose you want to start multiple threads in parallel; what is the modification
    that you have to make in the syntax of the kernel call? This is addressed in the
    next section and is termed "configuring kernel parameters".
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`gpuAdd` 是我们想要启动的内核的名称，而 `<<<1,1>>>` 表示我们想要每个块有一个线程，这意味着我们只启动了一个线程。圆括号中的三个参数是传递给内核的参数。在这里，我们传递了两个常量，`1`
    和 `4`。第三个参数是指向设备内存 `d_c` 的指针。它指向内核在加法操作后将在设备内存中存储答案的位置。程序员必须注意的一点是，传递给内核的指针应该只指向设备内存。如果它指向主机内存，可能会使你的程序崩溃。内核执行完成后，设备指针指向的结果可以被复制回主机内存以供进一步使用。在设备上仅启动一个线程进行执行并不是设备资源的最佳使用方式。假设你想要并行启动多个线程；你需要在内核调用的语法中进行哪些修改？这将在下一节中讨论，并被称为“配置内核参数”。
- en: Configuring kernel parameters
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置内核参数
- en: For starting multiple threads on the device in parallel, we have to configure
    parameters in the kernel call, which are written inside the kernel launch operator.
    They specify the number of blocks and the number of threads per block. We can
    launch many blocks in parallel with many threads in each block. Normally, there
    is a limit of 512 or 1,024 threads per block. Each block runs on the streaming
    multiprocessor, and threads in one block can communicate with one another via
    shared memory. The programmer can't choose which multiprocessor will execute a
    particular block and in which order blocks or threads will execute.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在设备上并行启动多个线程，我们不得不在内核调用中配置参数，这些参数被写入内核启动操作符内部。它们指定了每个块中的线程数和块的数量。我们可以通过每个块中的多个线程并行启动多个块。通常，每个块中的线程数限制为512或1,024。每个块在流式多处理器上运行，一个块中的线程可以通过共享内存相互通信。程序员无法选择哪个多处理器将执行特定的块，以及块或线程执行的顺序。
- en: 'Suppose you want to start 500 threads in parallel; what is the modification
    that you can make to the kernel launch syntax that was shown previously? One option
    is to start one block of 500 threads via the following syntax:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想并行启动500个线程；你可以对之前显示的内核启动语法进行哪些修改？一个选项是通过以下语法启动一个包含500个线程的块：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can also start 500 blocks of one thread each or two blocks of 250 threads
    each. Accordingly, you have to modify values in the kernel launch operator. The
    programmer has to be careful that the number of threads per block does not go
    beyond the maximum supported limit of your GPU device. In this book, we are targeting
    computer vision applications where we need to work on two-and three-dimensional
    images. Here, it would be great if blocks and threads are not one-dimensional
    but more than that for better processing and visualization.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以启动500个每个线程的块或250个线程的2个块。相应地，你必须修改内核启动操作符中的值。程序员必须小心，确保每个块中的线程数不超过GPU设备的最大支持限制。在这本书中，我们针对计算机视觉应用，需要处理二维和三维图像。在这里，如果块和线程不是一维的，而是更多维的，将有助于更好的处理和可视化。
- en: 'GPU supports a three-dimensional grids of blocks and three-dimensional blocks
    of threads. It has the following syntax:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPU支持三维块网格和三维线程块。它具有以下语法：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here `N[bx]`, `N[by]`, and `N[bz]`indicate the number of blocks in a grid in
    the direction of the *x*, *y*, and *z* axes, respectively. Similarly, `N[t][x]`,
    `N[ty]`, and `N[tz]` indicate the number of threads in a block in the direction
    of the *x*, *y*, and z axes. If the *y* and z dimensions are not specified, they
    are taken as `1` by default. So, for example, to process an image, you can start
    a 16 x 16 grid of blocks, all containing 16 x 16 threads. The syntax will be as
    follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `N[bx]`、`N[by]` 和 `N[bz]` 分别表示在 *x*、*y* 和 *z* 轴方向上的网格中的块数。同样，`N[t][x]`、`N[ty]`
    和 `N[tz]` 表示在 *x*、*y* 和 *z* 轴方向上的块中的线程数。如果 *y* 和 *z* 维度未指定，则默认为 `1`。例如，为了处理图像，你可以启动一个16
    x 16的块网格，所有块都包含16 x 16的线程。语法如下：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To summarize, the configuration of the number of blocks and the number of threads
    is very important while launching the kernel. It should be chosen with proper
    care depending on the application that we are working on and the GPU resources.
    The next section will explain some important CUDA functions added over regular
    ANSI C functions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在启动内核时配置块和线程的数量非常重要。应根据我们正在处理的应用程序和GPU资源进行适当的考虑。下一节将解释一些在常规ANSI C函数之上添加的重要CUDA函数。
- en: CUDA API functions
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA API函数
- en: 'In the variable addition program, we have encountered some functions or keywords
    that are not familiar to regular C or C++ programmers. These keywords and functions
    include `__global__` , `cudaMalloc`, `cudaMemcpy`, and `cudaFree`. So, in this
    section, these functions are explained in detail one by one:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在变量加法程序中，我们遇到了一些对常规C或C++程序员来说不熟悉的函数或关键字。这些关键字和函数包括 `__global__`、`cudaMalloc`、`cudaMemcpy`
    和 `cudaFree`。因此，在本节中，我们将逐一详细解释这些函数：
- en: '**__global__** :It is one of three qualifier keywords, along with `__device__`
    and `__host__` . This keyword indicates that a function is declared as a device
    function and will execute on the device when called from the host. It should be
    kept in mind that this function can only be called from the host. If you want
    your function to execute on the device and called from the device function, then
    you have to use the `__device__` keyword. The `__host__` keyword is used to define
    host functions that can only be called from other host functions. This is similar
    to normal C functions. By default, all functions in a program are host functions.
    Both `__host__` and `__device__` can be simultaneously used to define any function.
    It generates two copies of the same function. One will execute on the host, and
    the other will execute on the device.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**__global__**：这是三个限定符关键字之一，与 `__device__` 和 `__host__` 一起。此关键字表示一个函数被声明为设备函数，当从主机调用时将在设备上执行。请注意，此函数只能从主机调用。如果您想使您的函数在设备上执行并从设备函数调用，那么您必须使用
    `__device__` 关键字。`__host__` 关键字用于定义只能从其他主机函数调用的主机函数。这类似于正常的C函数。默认情况下，程序中的所有函数都是主机函数。`__host__`
    和 `__device__` 可以同时使用来定义任何函数。它将生成相同函数的两个副本。一个将在主机上执行，另一个将在设备上执行。'
- en: '**cudaMalloc**:It is similar to the `Malloc` function used in C for dynamic
    memory allocation. This function is used to allocate a memory block of a specific
    size on the device. The syntax of `cudaMalloc` with an example is as follows:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cudaMalloc**：它类似于C中用于动态内存分配的 `Malloc` 函数。此函数用于在设备上分配特定大小的内存块。以下是一个 `cudaMalloc`
    的语法示例：'
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As shown in the preceding example code, it allocates a memory block of size
    equal to the size of one integer variable and returns the pointer `d_c`, which
    points to this memory location.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个示例代码所示，它分配了一个与一个整型变量大小相等的内存块，并返回指向该内存位置的指针 `d_c`。
- en: '**cudaMemcpy**:This function is similar to the `Memcpy` function in C. It is
    used to copy one block of memory to other blocks on a host or a device. It has
    the following syntax:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cudaMemcpy**：此函数类似于C中的 `Memcpy` 函数。它用于将一块内存从一个主机或设备上的其他块复制。它具有以下语法：'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This function has four arguments. The first and second arguments are the destination
    pointer and the source pointer, which point to the host or device memory location.
    The third argument indicates the size of the copy and the last argument indicates
    the direction of the copy. It can be from host to device, device to device, host
    to host, or device to host. But be careful, as you have to match this direction
    with the appropriate pointers as the first two arguments. As shown in the example,
    we are copying a block of one integer variable from the device to the host by
    specifying the device pointer `d_c` as the source, and the host pointer `h_c`
    as a destination.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数有四个参数。前两个参数是目标指针和源指针，它们指向主机或设备内存位置。第三个参数指示复制的尺寸，最后一个参数指示复制的方向。可以是主机到设备、设备到设备、主机到主机或设备到主机。但请注意，您必须将此方向与前两个参数中的适当指针匹配。如示例所示，我们通过指定设备指针
    `d_c` 为源，主机指针 `h_c` 为目标，将一个整型变量的块从设备复制到主机。
- en: '**cudaFree**: It is similar to the free function available in C. The syntax
    of `cudaFree` is as follows:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cudaFree**：它类似于C中可用的 `free` 函数。`cudaFree` 的语法如下：'
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It frees the memory space pointed to by `d_ptr`. In the example code, it frees
    the memory location pointed to by `d_c`. Please make sure that `d_c` is allocated
    memory, using `cudaMalloc` to free it using `cudaFree`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它释放由 `d_ptr` 指向的内存空间。在示例代码中，它释放了由 `d_c` 指向的内存位置。请确保使用 `cudaMalloc` 分配了 `d_c`
    的内存，然后使用 `cudaFree` 释放它。
- en: There are many other keywords and functions available in CUDA over and above
    existing ANSI C functions. We will be frequently using only these three functions,
    and hence they are discussed in this section. For more details, you can always
    visit the CUDA programming guide.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，除了现有的ANSI C函数之外，还有很多其他关键字和函数可用。我们将经常使用这三个函数，因此它们在本节中进行了讨论。更多详情，您可以随时访问CUDA编程指南。
- en: Passing parameters to CUDA functions
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向CUDA函数传递参数
- en: The `gpuAdd` kernel function of the variable addition program is very similar
    to the normal C function. So, like normal C functions, the kernel functions can
    also be passed parameters by value or by reference. Hence, in this section, we
    will see both the methods to pass parameters for CUDA kernels.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 变量加法程序的 `gpuAdd` 内核函数与正常的 C 函数非常相似。因此，像正常的 C 函数一样，内核函数也可以按值或按引用传递参数。因此，在本节中，我们将看到传递
    CUDA 内核参数的两种方法。
- en: Passing parameters by value
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按值传递参数
- en: 'If you recall, in the `gpuAdd` program, the syntax for calling the kernel was
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在 `gpuAdd` 程序中，调用内核的语法如下：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'On the other hand, the signature of the `gpuAdd` function in definition was
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`gpuAdd` 函数在定义中的签名如下：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, you can see that we are passing values of `d_a` and `d_b` while calling
    the kernel. First, parameter `1` will be copied to `d_a` and then parameter `4`
    will be copied to `d_b` while calling the kernal. The answer after addition will
    be stored at the address pointed by `d_c` on device memory. Instead of directly
    passing values `1` and `4` as inputs to the kernel, we can also write the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以看到我们在调用内核时传递了 `d_a` 和 `d_b` 的值。首先，参数 `1` 将被复制到 `d_a`，然后参数 `4` 将在调用内核时复制到
    `d_b`。加法操作后的答案将存储在设备内存中由 `d_c` 指向的地址。我们也可以直接将值 `1` 和 `4` 作为内核的输入，如下所示：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, `a` and `b` are integer variables that can contain any integer values.
    Passing parameters by values is not recommended, as it creates unnecessary confusion
    and complications in programs. It is better to pass parameters by reference.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`a` 和 `b` 是可以包含任何整数值的整型变量。按值传递参数不建议使用，因为它会在程序中造成不必要的混淆和复杂化。最好是通过引用传递参数。
- en: Passing parameters by reference
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按引用传递参数
- en: 'Now we will see how to write the same program by passing parameters by reference.
    For that, we have to first modify the kernel function for addition of two variables.
    The modified kernel for passing parameters by reference is shown here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何通过引用传递参数来编写相同的程序。为此，我们首先需要修改用于两个变量加法的内核函数。按引用传递参数的修改后的内核如下所示：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Instead of using integer variables `d_a` and `d_b` as inputs to the kernel,
    the pointers to these variables on device `*d_a` and `*d_b` are taken as inputs.
    The answer which will be obtained after the addition is stored at the memory location
    pointed by third integer pointer `d_c`. The pointers passed as a reference to
    this device function should be allocated memory with the `cudaMalloc` function.
    The main function for this code is shown here:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在将整数变量 `d_a` 和 `d_b` 作为内核的输入时，我们取这些变量在设备上的指针 `*d_a` 和 `*d_b` 作为输入。加法操作后的答案将存储在由第三个整数指针
    `d_c` 指向的内存位置。传递给这个设备函数的指针应该使用 `cudaMalloc` 函数分配内存。此代码的主函数如下所示：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`h_a`, `h_b`, and `h_c` are variables in the host memory. They are defined
    like normal C code. On the other hand, `d_a`, `d_b`, and `d_c` are pointers residing
    on host memory, and they point to the device memory. They are allocated memory
    from the host by using the `cudaMalloc` function. The values of `h_a` and `h_b`
    are copied to the device memory pointed to by `d_a` and `d_b` by using the `cudaMemcpy`
    function, and the direction of data transfer is from the host to the device. Then,
    in kernel call, these three device pointers are passed to the kernel as parameters.
    The kernel computes addition and stores the result at the memory location pointed
    by `d_c`. The result is copied back to the host memory by using `cudaMemcpy` again,
    but this time with the direction of data transfer as the device to host. The output
    of the program is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`h_a`、`h_b` 和 `h_c` 是主机内存中的变量。它们像正常的 C 代码一样定义。另一方面，`d_a`、`d_b` 和 `d_c` 是位于主机内存中的指针，它们指向设备内存。它们通过使用
    `cudaMalloc` 函数从主机分配内存。`h_a` 和 `h_b` 的值通过使用 `cudaMemcpy` 函数复制到由 `d_a` 和 `d_b`
    指向的设备内存中，数据传输方向是从主机到设备。然后，在内核调用中，这三个设备指针作为参数传递给内核。内核执行加法操作并将结果存储在由 `d_c` 指向的内存位置。结果再次通过
    `cudaMemcpy` 复制回主机内存，但这次数据传输方向是从设备到主机。程序输出如下：'
- en: '![](img/9097c292-8aee-4dbc-964a-31ec2a203559.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9097c292-8aee-4dbc-964a-31ec2a203559.png)'
- en: 'The memory used by three device pointers is freed by using the `cudaFree` at
    the end of the program. The sample memory map on the host and the device will
    look similar to the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 程序结束时使用 `cudaFree` 释放三个设备指针使用的内存。主机和设备上的示例内存映射将类似于以下内容：
- en: '| **Host Memory (CPU)** | **Device Memory (GPU)** |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **主机内存（CPU）** | **设备内存（GPU）** |'
- en: '| Address | Value | Address | Value |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 地址 | 值 | 地址 | 值 |'
- en: '| #01 | h_a=1 | #01 | 1 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| #01 | h_a=1 | #01 | 1 |'
- en: '| #02 | h_b=4 | #02 | 4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| #02 | h_b=4 | #02 | 4 |'
- en: '| #03 | h_c=5 | #03 | 5 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| #03 | h_c=5 | #03 | 5 |'
- en: '| #04 | d_a=#01 | #04 |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| #04 | d_a=#01 | #04 |  |'
- en: '| #05 | d_b=#02 | #05 |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| #05 | d_b=#02 | #05 |  |'
- en: '| #06 | d_c=#03 | #06 |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| #06 | d_c=#03 | #06 |  |'
- en: As you can see from the table, `d_a`, `d_b`, and `d_c` are residing on the host
    and pointing to values on the device memory. While passing parameters by reference
    to kernels, you should take care that all pointers are pointing to the device
    memory only. If it is not the case, then the program may crash.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，`d_a`、`d_b`和`d_c`位于主机上，并指向设备内存中的值。在通过引用传递参数给内核时，你应该注意所有指针都只指向设备内存。如果不是这样，程序可能会崩溃。
- en: While using device pointers and passing them to kernels, there are some restrictions
    that have to be followed by the programmer. The device pointers that are allocated
    memory using `cudaMalloc` can only be used to read or write from the device memory.
    They can be passed as parameters to the device function, and they should not be
    used to read and write memory from the host functions. To simplify, device pointers
    should be used to read and write device memory from the device function, and host
    pointers should be used to read and write host memory from host functions. So,
    in this book, you will always find the device pointer prefixed by `d_` in kernel
    functions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用设备指针并将它们传递给内核时，程序员必须遵循一些限制。使用`cudaMalloc`分配内存的设备指针只能用于从设备内存中读取或写入。它们可以作为参数传递给设备函数，但不应用于从主机函数中读取和写入内存。为了简化，设备指针应用于从设备函数中读取和写入设备内存，而主机指针应用于从主机函数中读取和写入主机内存。因此，在本书中，你将始终在内核函数中看到以`d_`为前缀的设备指针。
- en: To summarize, in this section, concepts related to CUDA programming were explained
    in detail by taking two-variable additional programs as an example. After this
    section, you should be familiar with basic CUDA programming concepts and the terminology
    associated with CUDA programs. In the next section, you will learn how threads
    are executed on the device.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，通过以两个变量的附加程序为例，详细解释了与CUDA编程相关的概念。在本节之后，你应该熟悉基本的CUDA编程概念以及与CUDA程序相关的术语。在下一节中，你将学习如何在设备上执行线程。
- en: Executing threads on a device
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设备上执行线程
- en: 'We have seen that, while configuring kernel parameters, we can start multiple
    blocks and multiple threads in parallel. So, in which order do these blocks and
    threads start and finish their execution? It is important to know this if we want
    to use the output of one thread in other threads. To understand this, we have
    modified the kernel in the `hello,CUDA!` program we saw in the first chapter,
    by including a print statement in the kernel call, which prints the block number.
    The modified code is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在配置内核参数时，我们可以并行启动多个块和多个线程。那么，这些块和线程的启动和完成执行的顺序是怎样的呢？如果我们想在其他线程中使用一个线程的输出，了解这一点很重要。为了理解这一点，我们修改了第一章节中看到的`hello,CUDA!`程序中的内核，通过在内核调用中包含一个打印语句来打印块号。修改后的代码如下：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As can be seen from the code, we are launching a kernel with 16 blocks in parallel
    with each block having a single thread. In the kernel code, we are printing the
    block ID of the kernel execution. We can think that 16 copies of the same `myfirstkernel`
    start execution in parallel. Each of these copies will have a unique block ID,
    which can be accessed by the `blockIdx.x CUDA` directive, and a unique thread
    ID, which can be accessed by `threadIdx.x`. These IDs will tell us which block
    and thread are executing the kernel. When you run the program many times, you
    will find that, each time, blocks execute in a different order. One sample output
    can be shown as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中可以看出，我们正在并行启动一个内核，有16个块，每个块有一个线程。在内核代码中，我们正在打印内核执行的块ID。我们可以认为16个相同的`myfirstkernel`副本并行开始执行。每个副本都将有一个唯一的块ID，可以通过`blockIdx.x
    CUDA`指令访问，以及一个唯一的线程ID，可以通过`threadIdx.x`访问。这些ID将告诉我们哪个块和线程正在执行内核。当你多次运行程序时，你会发现，每次块执行的顺序都不同。一个示例输出如下所示：
- en: '![](img/50a8685b-96c7-4229-8ac2-2562c57a9a02.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50a8685b-96c7-4229-8ac2-2562c57a9a02.png)'
- en: One question you should ask is how many different output patterns will the previous
    program produce? The correct answer is 16! It will produce *n* factorial number
    of outputs, where *n* indicates the number of blocks started in parallel. So,
    whenever you are writing the program in CUDA, you should be careful that the blocks
    execute in random order.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该问的一个问题是，前面的程序将产生多少种不同的输出模式？正确的答案是16！它将产生*n*阶乘数量的输出，其中*n*表示并行启动的块的数量。因此，每次在CUDA中编写程序时，您都应该小心，确保块以随机顺序执行。
- en: 'This program also contains one more CUDA directive: `cudaDeviceSynchronize()`.
    Why is it used? It is used because a kernel launch is an asynchronous process,
    which means it returns control to the CPU thread immediately after starting up
    the GPU process before the kernel has finished executing. In the previous code,
    the next line in CPU thread is `print` and application exit will terminate console
    before the kernel has finished execution. So, if we do not include this directive,
    you will not see any print statements of the kernel execution. The output that
    is generated later by the kernel has nowhere to go, and you won''t see it. To
    see the outputs generated by the kernel, we will include this directive, which
    ensures that the kernel finishes before the application is allowed to exit, and
    the output from the kernel will find a **waiting standard output queue**.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序还包含一个额外的CUDA指令：`cudaDeviceSynchronize()`。为什么使用它？这是因为内核启动是一个异步过程，这意味着在内核完成执行之前，它会立即将控制权返回给启动GPU进程之前的CPU线程。在前面的代码中，CPU线程的下一行是`print`，应用程序退出将在内核完成执行之前终止控制台。因此，如果我们不包括此指令，您将看不到任何内核执行的打印语句。内核随后生成的输出将无处可去，您将看不到它。为了看到内核生成的输出，我们将包括此指令，这确保了内核在应用程序被允许退出之前完成，并且内核的输出将找到**等待的标准输出队列**。
- en: Accessing GPU device properties from CUDA programs
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从CUDA程序中访问GPU设备属性
- en: 'CUDA provides a simple interface to find the information such as determining
    which CUDA-enabled GPU devices (if any) are present and what capabilities each
    device supports. First, it is important to get a count of how many CUDA-enabled
    devices are present on the system, as a system may contain more than one GPU-enabled
    device. This count can be determined by the CUDA API `cudaGetDeviceCount()`. The
    program for getting a number of CUDA enabled devices on the system is shown here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA提供了一个简单的接口来查找信息，例如确定哪些CUDA启用型GPU设备（如果有）存在以及每个设备支持哪些功能。首先，重要的是要获取系统上CUDA启用型设备数量的统计，因为一个系统可能包含多个启用GPU的设备。这个数量可以通过CUDA
    API `cudaGetDeviceCount()` 来确定。获取系统上CUDA启用型设备数量的程序如下所示：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The relevant information about each device can be found by querying the `cudaDeviceProp`
    structure, which returns all the device properties. If you have more than one
    CUDA-capable device, then you can start a for loop to iterate over all device
    properties. The following section contains the list of device properties divided
    into different sets and small code snippets used to access them from CUDA programs.
    These properties are provided by the `cudaDeviceProp` structure in CUDA 9 runtime.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查询`cudaDeviceProp`结构可以找到每个设备的相关信息，该结构返回所有设备属性。如果您有多个CUDA能力型设备，则可以启动一个for循环来遍历所有设备属性。以下部分包含设备属性列表，分为不同的集合以及用于从CUDA程序中访问它们的简短代码片段。这些属性由CUDA
    9运行时中的`cudaDeviceProp`结构提供。
- en: For more details about properties in the different versions of CUDA, you can
    check the programming guide for a particular version.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解CUDA不同版本中属性的相关详细信息，您可以查看特定版本的编程指南。
- en: General device properties
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用设备属性
- en: '`cudaDeviceProp` provides several properties that can be used to identify the
    device and the versions being used. It provides the `name` property that returns
    the name of the device as a string. We can also get a version of the driver and
    the runtime engine the device is using by querying `cudaDriverGetVersion` and
    `cudaRuntimeGetVersion` properties. Sometimes, if you have more than one device,
    you want to use the device that has more multiprocessors. The `multiProcessorCount`
    property returns the count of the number of multiprocessors on the device. The
    speed of the GPU in terms of clock rate can be fetched by using the `clockRate`
    property. It returns clock rate in Khz. The following code snippet shows how to
    use these properties from the CUDA program:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaDeviceProp`提供了几个属性，可用于识别设备和正在使用的版本。它提供了一个`name`属性，该属性以字符串形式返回设备名称。我们还可以通过查询`cudaDriverGetVersion`和`cudaRuntimeGetVersion`属性来获取设备使用的驱动程序和运行时引擎的版本。有时，如果您有多个设备，您可能希望使用具有更多多处理器的设备。`multiProcessorCount`属性返回设备上多处理器的数量。通过使用`clockRate`属性可以获取GPU的时钟速度，它以千赫兹为单位返回时钟频率。以下代码片段展示了如何从CUDA程序中使用这些属性：'
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Memory-related properties
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与内存相关的属性
- en: 'Memory on the GPU has a hierarchical architecture. It can be divided in terms
    of L1 cache, L2 cache, global memory, texture memory, and shared memory. The `cudaDeviceProp`
    provides many properties that help in identifying memory available with the device.
    `memoryClockRate` and `memoryBusWidth` provide clock rate and bus width of the
    memory respectively. The speed of the memory is very important. It affects the
    overall speed of your program. `totalGlobalMem` returns the size of global memory
    available with the device. `totalConstMem` returns the total constant memory available
    with the device. `sharedMemPerBlock` returns the total shared memory that can
    be used in tne device. The total number of registers available per block can be
    identified by using `regsPerBlock`. Size of L2 cache can be identified using the
    `l2CacheSize` property. The following code snippet shows how to use memory-related
    properties from the CUDA program:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: GPU上的内存具有分层架构。它可以按L1缓存、L2缓存、全局内存、纹理内存和共享内存来划分。`cudaDeviceProp`提供了许多属性，有助于识别设备上可用的内存。`memoryClockRate`和`memoryBusWidth`分别提供内存的时钟频率和总线宽度。内存的速度非常重要，它会影响您程序的整体速度。`totalGlobalMem`返回设备上可用的全局内存大小。`totalConstMem`返回设备上可用的总常量内存。`sharedMemPerBlock`返回设备中可以使用的总共享内存。每个块可用的寄存器总数可以通过使用`regsPerBlock`来识别。L2缓存的大小可以通过`l2CacheSize`属性来识别。以下代码片段展示了如何从CUDA程序中使用内存相关的属性：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Thread-related properties
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与线程相关的属性
- en: 'As seen in earlier sections, blocks and threads can be multidimensional. So,
    it would be nice to know how many threads and blocks can be launched in parallel
    in each dimension. There is also a limit on the number of threads per multiprocessor
    and the number of threads per block. This number can be found by using the `maxThreadsPerMultiProcessor`
    and the `maxThreadsPerBlock`. It is very important in the configuration of kernel
    parameters. If you launch more threads per block than the maximum threads possible
    per block, your program can crash. The maximum threads per block in each dimension
    can be identified by the `maxThreadsDim`. In the same way, the maximum blocks
    per grid in each dimension can be identified by using the `maxGridSize`. Both
    of them return an array with three values, which shows the maximum value in the
    *x*, *y,* and *z* dimensions respectively. The following code snippet shows how
    to use thread-related properties from the CUDA code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所示，块和线程可以是多维的。因此，了解每个维度中可以并行启动多少线程和块将很有帮助。每个多处理器和每个块的线程数量也有上限。这个数量可以通过使用`maxThreadsPerMultiProcessor`和`maxThreadsPerBlock`来找到。这在内核参数配置中非常重要。如果您在每个块中启动的线程数超过了每个块可能的最大线程数，则您的程序可能会崩溃。每个维度中每个块的最大线程数可以通过`maxThreadsDim`来识别。同样，每个维度中每个网格的最大块数可以通过使用`maxGridSize`来识别。这两个属性都返回一个包含三个值的数组，分别表示*x*、*y*和*z*维度上的最大值。以下代码片段展示了如何从CUDA代码中使用线程相关的属性：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'There are many other properties available in the `cudaDeviceProp` structure.
    You can check the CUDA programming guide for details of other properties. The
    output from all preceding code sections combined and executed on the NVIDIA Geforce
    940MX GPU and CUDA 9.0 is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cudaDeviceProp`结构体中还有许多其他属性可用。您可以查阅CUDA编程指南以获取其他属性的详细信息。以下是在NVIDIA Geforce
    940MX GPU和CUDA 9.0上执行并组合所有先前代码段输出的结果：
- en: '![](img/71c144c7-d4f6-46e1-bd68-3558fd3778f1.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/71c144c7-d4f6-46e1-bd68-3558fd3778f1.png)'
- en: 'One question you might ask is why you should be interested in knowing the device
    properties. The answer is that this will help you in choosing a GPU device with
    more multiprocessors, if multiple GPU devices are present. If in your application
    the kernel needs close interaction with the CPU, then you might want your kernel
    to run on an integrated GPU that shares system memory with the CPU. These properties
    will also help you in finding the number of blocks and number of threads per block
    available on your device. This will help you with the configuration of kernel
    parameters. To show you one use of device properties, suppose you have an application
    that requires double precision for floating-point operation. Not all GPU devices
    support this operation. To know whether your device supports double precision
    floating-point operation and set that device for your application, the following
    code can be used:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问的一个问题是，为什么你应该对了解设备属性感兴趣。答案是，这将帮助你在存在多个GPU设备的情况下选择具有更多多处理器的GPU设备。如果您的应用程序中的内核需要与CPU进行紧密交互，那么您可能希望内核在共享系统内存的集成GPU上运行。这些属性还将帮助您找到设备上可用的块数和每个块中的线程数。这将帮助您配置内核参数。为了向您展示设备属性的一个用途，假设您有一个需要双精度浮点运算的应用程序。并非所有GPU设备都支持此操作。为了知道您的设备是否支持双精度浮点运算，并将该设备设置为您的应用程序，可以使用以下代码：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This code uses two properties available in the `cudaDeviceprop` structure that
    help in identifying whether the device supports double precision operations. These
    two properties are major and minor. CUDA documentation says us that if major is
    greater than 1 and minor is greater than 3, then that device will support double
    precision operations. So, the program's `device_property` structure is filled
    with these two values. CUDA also provides the `cudaChooseDevice` API that helps
    in choosing a device with particular properties. This API is used on the current
    device to identify whether it contains these two properties. If it contains properties,
    then that device is selected for your application using the `cudaSetDevice` API.
    If more than one device is present in the system, this code should be written
    inside a for a loop to iterate over all devices.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用了`cudaDeviceprop`结构中可用的两个属性，这些属性有助于确定设备是否支持双精度运算。这两个属性是主版本号和次版本号。CUDA文档告诉我们，如果主版本号大于1且次版本号大于3，则该设备将支持双精度运算。因此，程序中的`device_property`结构被填充了这两个值。CUDA还提供了`cudaChooseDevice`
    API，该API有助于选择具有特定属性的设备。此API用于当前设备，以确定它是否包含这两个属性。如果包含属性，则使用`cudaSetDevice` API选择该设备用于您的应用程序。如果系统中存在多个设备，则此代码应编写在一个循环中，以便遍历所有设备。
- en: Though trivial, this section is very important for you in finding out which
    applications can be supported by your GPU device and which cannot.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很简单，但这一部分对于您了解哪些应用程序可以由您的GPU设备支持以及哪些不支持非常重要。
- en: Vector operations in CUDA
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的向量运算
- en: Until now, the programs that we have seen were not leveraging any advantages
    of the parallel-processing capabilities of GPU devices. They were just written
    to get you familiar with the programming concepts in CUDA. From this section,
    we will start utilizing the parallel-processing capabilities of the GPU by performing
    vector or array operations on it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的程序都没有利用GPU设备的并行处理能力。它们只是编写来让您熟悉CUDA中的编程概念。从本节开始，我们将通过在GPU上执行向量或数组运算来利用GPU的并行处理能力。
- en: Two-vector addition program
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两个向量加法程序
- en: 'To understand vector operation on the GPU, we will start by writing a vector
    addition program on the CPU and then modify it to utilize the parallel structure
    of GPU. We will take two arrays of some numbers and store the answer of element-wise
    addition in the third array. The vector addition function on CPU is shown here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解GPU上的向量运算，我们首先将在CPU上编写一个向量加法程序，然后修改它以利用GPU的并行结构。我们将取两个数字数组，并将逐元素加法的结果存储在第三个数组中。CPU上的向量加法函数如下所示：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `cpuAdd` should be very simple to understand. One thing you might find
    difficult to understand is the use of `tid`. It is included to make the program
    similar to the GPU program, in which `tid` indicated a particular thread ID. Here,
    also, if you have a multicore CPU, then you can initialize `tid` equal to 0 and
    1 for each of them and then add 2 to it in the loop so that one CPU will perform
    a sum on even elements and one CPU will perform addition on odd elements. The
    main function for the code is shown here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpuAdd` 应该非常容易理解。你可能觉得难以理解的是 `tid` 的使用。它被包含进来是为了使程序与 GPU 程序相似，其中 `tid` 表示特定的线程
    ID。在这里，如果你有一个多核 CPU，那么你可以为每个核心初始化 `tid` 为 0 和 1，然后在循环中将其加 2，这样其中一个 CPU 将对偶数元素进行求和，而另一个
    CPU 将对奇数元素进行加法。代码的 `main` 函数如下所示：'
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There are two functions in the program: `main` and `cpuAdd`. In the main function,
    we start by defining two arrays to hold inputs and initialize it to some random
    numbers. Then, we pass these two arrays as input to the `cpuAdd` function. The
    `cpuAdd` function stores the answer in the third array. Then, we print this answer
    on the console, which is shown here:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 程序中有两个函数：`main` 和 `cpuAdd`。在 `main` 函数中，我们首先定义了两个数组来存储输入，并将其初始化为一些随机数。然后，我们将这两个数组作为输入传递给
    `cpuAdd` 函数。`cpuAdd` 函数将答案存储在第三个数组中。然后，我们在控制台上打印这个答案，如下所示：
- en: '![](img/783fe03c-7410-4177-9649-d6aa378983ea.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/783fe03c-7410-4177-9649-d6aa378983ea.png)'
- en: 'This explanation of using the `tid in cpuadd` function may give you an idea
    of how to write the same function for the GPU execution, which can have many cores
    in parallel. If we initialize this add function with the ID of that core, then
    we can do the addition of all the elements in parallel. So, the modified kernel
    function for addition on the GPU is shown here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tid in cpuadd` 函数的解释可能给你一些如何为 GPU 执行编写相同函数的思路，因为 GPU 可以并行处理多个核心。如果我们用那个核心的
    ID 来初始化这个加法函数，那么我们就可以并行地对所有元素进行加法运算。因此，GPU 上加法操作的修改后的内核函数如下所示：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the `gpuAdd` kernel function, `tid` is initialized with the block ID of
    the current block in which the kernel is executing. All kernels will add an array
    element indexed by this block ID. If the number of blocks are equal to the number
    of elements in an array, then all the addition operations will be done in parallel.
    How this kernel is called from the main function is explained next. The code for
    the main function is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `gpuAdd` 内核函数中，`tid` 被初始化为当前内核执行的块的块 ID。所有内核都将添加由该块 ID 索引的数组元素。如果块的数量等于数组中的元素数量，那么所有加法操作都将并行执行。接下来将解释如何从
    `main` 函数中调用这个内核。`main` 函数的代码如下：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The GPU main function has the known structure as explained in the first section
    of this chapter:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 的 `main` 函数具有本章第一部分所述的已知结构：
- en: It starts with defining arrays and pointers for host and device. The device
    pointers are allocated memory using the `cudaMalloc` function.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从定义主机和设备的数组和指针开始。使用 `cudaMalloc` 函数为设备指针分配内存。
- en: The arrays, which are to be passed to the kernel, are copied from the host memory
    to the device memory by using the `cudaMemcpy` function.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要传递给内核的数组通过使用 `cudaMemcpy` 函数从主机内存复制到设备内存。
- en: The kernel is launched by passing the device pointers as parameters to it. If
    you see the values inside the kernel launch operator, they are *N* and *1*, which
    indicate we are launching *N* blocks with one thread per each block.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核是通过将设备指针作为参数传递给它来启动的。如果你看到内核启动操作符内的值，它们是 *N* 和 *1*，这表示我们正在启动 *N* 个块，每个块有一个线程。
- en: The answer stored by the kernel on the device memory is copied back to the host
    memory by again using the `cudaMemcpy`, but this time with the direction of data
    transfer from the device to the host.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核在设备内存中存储的答案通过再次使用 `cudaMemcpy` 被复制回主机内存，但这次数据传输的方向是从设备到主机。
- en: 'And, finally, memory allocated to three device pointers is freed up by using
    the `cudaFree` function. The output of the program is as follows:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用 `cudaFree` 函数释放分配给三个设备指针的内存。程序的输出如下：
- en: '![](img/2e6cda1b-587e-4e29-b716-4459ad2ddfe2.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e6cda1b-587e-4e29-b716-4459ad2ddfe2.png)'
- en: 'All CUDA programs follow the same pattern as shown before. We are launching
    N blocks in parallel. The meaning of this is that we are launching N copies of
    the same kernel simultaneously. You can understand this by taking a real-life
    example: Suppose you want to transfer five big boxes from one place to another.
    In the first method, you can perform this task by hiring one person who takes
    one block from one place to the other and repeat this five times. This option
    will take time, and it is similar to how vectors are added to the CPU. Now, suppose
    you hire five people and each of them carries one box. Each of them also knows
    the ID of the box they are carrying. This option will be much faster than the
    previous one. Each one of them just needs to be told that they have to carry one
    box with a particular ID from one place to the other.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所有CUDA程序都遵循之前显示的相同模式。我们在并行启动N个块。这意味着我们同时启动了N个相同的内核副本。你可以通过一个现实生活中的例子来理解这一点：假设你想要将五个大箱子从一个地方运到另一个地方。在第一种方法中，你可以通过雇佣一个人来完成这个任务，这个人从一处运到另一处，然后重复五次。这个选项会花费时间，这类似于向量在CPU上是如何加的。现在，假设你雇佣了五个人，每个人携带一个箱子。他们每个人也知道他们携带的箱子的ID。这个选项将比之前的选项快得多。他们每个人只需要被告知他们必须携带一个特定ID的箱子从一处运到另一处。
- en: This is exactly how kernels are defined and executed on the device. Each kernel
    copy knows the ID of it. This can be known by the `blockIdx.x` command. Each copy
    works on an array element indexed by its ID. All copies add all elements in parallel,
    which significantly reduces the processing time for the entire array. So, in a
    way, we are improving the throughput by doing operations in parallel over CPU
    sequential execution. The comparison of throughput between the CPU code and the
    GPU code is explained in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是内核在设备上定义和执行的方式。每个内核副本都知道自己的ID。这可以通过`blockIdx.x`命令来知道。每个副本在其ID索引的数组元素上工作。所有副本并行地添加所有元素，这显著减少了整个数组的处理时间。所以，从某种意义上说，我们通过在CPU的顺序执行上并行执行操作来提高吞吐量。CPU代码和GPU代码之间的吞吐量比较将在下一节中解释。
- en: Comparing latency between the CPU and the GPU code
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较CPU和GPU代码之间的延迟
- en: 'The programs for CPU and the GPU addition are written in a modular way so you
    can play around with the value of N. If N is small, then you will not notice any
    significant time difference between the CPU and the GPU code. But if you N is
    sufficiently large, then you will notice the significant difference in the CPU
    execution time and the GPU execution time for the same-vector addition. The time
    taken for the execution of a particular block can be measured by adding the following
    lines to the existing code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和GPU的加法程序以模块化的方式编写，这样你可以玩转N的值。如果N很小，那么你不会注意到CPU和GPU代码之间有显著的时间差异。但是，如果你N足够大，那么你将注意到相同向量加法中CPU执行时间和GPU执行时间的显著差异。可以通过在现有代码中添加以下行来测量特定块的执行时间：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Time is measured by calculating the total number of clock cycles taken to perform
    a particular operation. This can be done by taking the difference of starting
    and ending the clock tick count, measured using the clock() function. This is
    divided by the number of clock cycles per second, to get the execution time. When
    N is taken as 10,000,000 in the previous vector addition programs of the CPU and
    the GPU and executed simultaneously, the output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 时间是通过计算执行特定操作所花费的总时钟周期数来衡量的。这可以通过使用`clock()`函数测量的开始和结束的时钟滴答计数之差来完成。这个差值除以每秒的时钟周期数，以得到执行时间。当在CPU和GPU之前的向量加法程序中将N设置为10,000,000并同时执行时，输出如下：
- en: '![](img/085e74b0-e73e-485d-bf4d-1a3dc7f3ffc9.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/085e74b0-e73e-485d-bf4d-1a3dc7f3ffc9.png)'
- en: As can be seen from the output, the execution time or throughput is improved
    from 25 milliseconds to almost 1 millisecond when the same function is implemented
    on GPU. This proves what we have seen in theory earlier that executing code in
    parallel on GPU helps in the improvement of throughput. CUDA provides an efficient
    and accurate method for measuring the performance of CUDA programs, using CUDA
    events, which will be explained in the later chapters.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，当相同的函数在GPU上实现时，执行时间或吞吐量从25毫秒提高到了几乎1毫秒。这证明了我们在理论中之前看到的事实，即在GPU上并行执行代码有助于提高吞吐量。CUDA提供了一个高效且准确的方法来测量CUDA程序的性能，使用CUDA事件，这将在后面的章节中解释。
- en: Elementwise squaring of vectors in CUDA
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中向量的逐元素平方
- en: 'Now, one question you can ask is, now that we are launching N blocks in parallel
    with one thread in each block, can we work in a reverse way? The answer is *yes*.
    We can launch only one block with N threads in parallel. To show that and make
    you more familiar with working around vectors in CUDA, we take the second example
    of the element-wise squaring of numbers in an array. We take one array of numbers
    and return an array that contains the square of these numbers. The kernel function
    to find the element-wise square is shown here:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以问的一个问题是，既然我们正在每个块中用一个线程并行启动 N 个块，我们能否以相反的方式工作？答案是 *是的*。我们可以并行地只启动一个包含
    N 个线程的块。为了展示这一点并让你更熟悉在 CUDA 中围绕向量工作，我们以数组中数字逐元素平方的第二个例子为例。我们取一个数字数组，并返回一个包含这些数字平方的数组。用于找到逐元素平方的内核函数如下所示：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `gpuSquare` kernel function has pointers to two arrays as arguments. The
    first pointer `d_in` points to the memory location where the input array is stored,
    while the second pointer `d_out` points to the memory location where output will
    be stored. In this program, instead of launching multiple blocks in parallel,
    we want to launch multiple threads in parallel, so `tid` is initialized with a
    particular thread ID using `threadIdx.x`. The main function for this program is
    as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpuSquare` 内核函数有两个数组的指针作为参数。第一个指针 `d_in` 指向存储输入数组的内存位置，而第二个指针 `d_out` 指向存储输出的内存位置。在这个程序中，我们不想并行启动多个块，而是想并行启动多个线程，因此使用
    `threadIdx.x` 初始化 `tid` 为特定的线程 ID。这个程序的主函数如下所示：'
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This main function follows a similar structure to the vector addition program.
    One difference that you will see here from the vector addition program is that
    we are launching a single block with N threads in parallel. The output of the
    program is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主函数遵循与向量加法程序相似的结构。你在这里会看到的一个区别是，我们正在并行地启动一个包含 N 个线程的单个块。程序输出如下：
- en: '![](img/f31562a9-874e-4188-9432-a6b379875f16.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f31562a9-874e-4188-9432-a6b379875f16.png)'
- en: Whenever you are using this way of launching N threads in parallel, you should
    take care that the maximum threads per block are limited to 512 or 1,024\. So,
    the value of N should be less than this value. If N is 2,000 and the maximum number
    of threads per block for your device is 512, then you can't write `<< <1,2000
    > >>`. Instead, you should use something such as `<< <4,500> >>`. The choice of
    a number of blocks and the number of threads per block should be made judiciously.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你使用这种方式并行启动 N 个线程时，你应该注意每个块的最大线程数限制为 512 或 1,024。因此，N 的值应该小于这个值。如果你的设备每个块的最大线程数是
    512，而 N 是 2,000，那么你不能写 `<< <1,2000 > >>`。相反，你应该使用类似 `<< <4,500> >>` 这样的东西。应该明智地选择块的数量和每个块中线程的数量。
- en: To summarize, we have learned how to work with vectors and how we can launch
    multiple blocks and multiple threads in parallel. We have also seen that by doing
    vector operations on GPU, it improves throughput, compared to the same operation
    on the CPU. In the last section of this chapter, we discuss the various parallel
    communication patterns that are followed by threads executing in parallel.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们学习了如何处理向量，以及我们如何并行地启动多个块和多个线程。我们还看到，通过在 GPU 上执行向量操作，与在 CPU 上执行相同的操作相比，它提高了吞吐量。在本章的最后部分，我们将讨论线程并行执行时遵循的各种并行通信模式。
- en: Parallel communication patterns
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行通信模式
- en: When several thread is executed in parallel, they follow a certain communication
    pattern that indicates where it is taking inputs and where it is writing its output
    in memory. We will discuss each communication pattern one by one. It will help
    you to identify communication patterns related to your application and how to
    write code for that.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个线程并行执行时，它们遵循一定的通信模式，这表明它们在哪里获取输入以及在内存中写入输出。我们将逐一讨论每种通信模式。这将帮助你识别与你的应用程序相关的通信模式以及如何编写相应的代码。
- en: Map
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射
- en: 'In this communication pattern, each thread or task takes a single input and
    produces a single output. Basically, it is a one-to-one operation. The vector
    addition program and element-wise squaring program, seen in the previous sections,
    are examples of the map pattern. The code of the map pattern will look as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种通信模式中，每个线程或任务取单个输入并产生单个输出。基本上，它是一个一对一的操作。前面章节中看到的向量加法程序和逐元素平方程序是映射模式的例子。映射模式的代码如下所示：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Gather
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gather
- en: 'In this pattern, each thread or task has multiple inputs, and it produces a
    single output to be written at a single location in memory. Suppose you want to
    write a program that finds a moving average of three numbers; this is an example
    of a gather operation. It takes three inputs from memory and writes single output
    to memory. So, there is data reuse on the input side. It is basically a many-to-one
    operation. The code for gather pattern will look as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式中，每个线程或任务有多个输入，并且它产生一个输出，该输出将被写入内存中的单个位置。假设你想编写一个程序来找到三个数的移动平均值；这是一个收集操作的例子。它从内存中获取三个输入，并将单个输出写入内存。因此，在输入端有数据重用。这基本上是一个多对一的操作。收集模式的代码如下所示：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Scatter
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 散列
- en: 'In a scatter pattern, a thread or a task takes a single input and computes
    where in the memory it should write the output. Array sorting is an example of
    a scatter operation. It can also be one-to-many operations. The code for scatter
    pattern will look as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在散列模式中，一个线程或任务取单个输入并计算在内存中应该写入输出的位置。数组排序是一个散列操作的例子。它也可以是一对多操作。散列模式的代码如下所示：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Stencil
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模板
- en: When threads or tasks read input from a fixed set of a neighborhood in an array,
    then this is called a **stencil** **communication pattern**. It is very useful
    in image-processing examples where we work on 3x3 or 5x5 neighborhood windows.
    It is a special form of a gather operation, so code syntax is similar to it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当线程或任务从一个数组的固定邻域集合中读取输入时，这被称为**模板****通信模式**。它在图像处理示例中非常有用，我们在3x3或5x5邻域窗口上工作。它是一种特殊的收集操作形式，因此代码语法与之相似。
- en: Transpose
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转置
- en: 'When the input is in the form of a row-major matrix, and we want the output
    to be in column-major form, we have to use this transpose communication pattern.
    It is particularly useful if you have a structure of arrays and you want to convert
    it in the form of an array of structures. It is also a one-to-one operation. The
    code for the transpose pattern will look as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入以行主序矩阵的形式存在，而我们希望输出以列主序形式时，我们必须使用这种转置通信模式。如果你有一个数组结构并且想要将其转换为结构数组的形式，这尤其有用。它也是一种一对一操作。转置模式的代码如下所示：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this section, various communication patterns that CUDA programming follows
    is discussed. It is useful to find a communication pattern related to your application
    and use the code syntax of that pattern shown as an example.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，讨论了CUDA编程遵循的各种通信模式。找到与你的应用程序相关的通信模式并使用该模式的代码语法（如示例所示）是有用的。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: To summarize, in this chapter, you were introduced to programming concepts in
    CUDA C and how parallel computing can be done using CUDA. It was shown that CUDA
    programs can run on any NVIDIA GPU hardware efficiently and in parallel. So, CUDA
    is both efficient and scalable. The CUDA API functions over and above existing
    ANSI C functions needed for parallel data computations were discussed in detail.
    How to call device code from the host code via a kernel call, configuring of kernel
    parameters, and a passing of parameters to the kernel were also discussed by taking
    a simple two-variable addition example. It was also shown that CUDA does not guarantee
    the order in which the blocks or thread will run and which block is assigned to
    which multi-processor in hardware. Moreover, vector operations, which take advantage
    of parallel-processing capabilities of GPU and CUDA, were discussed. It can be
    seen that, by performing vector operations on the GPU, it can improve the throughput
    drastically, compared to the CPU. In the last section, various common communication
    patterns followed in parallel programming were discussed in detail. Still, we
    have not discussed memory architecture and how threads can communicate with one
    another in CUDA. If one thread needs data of the other thread, then what can be
    done is also not discussed. So, in the next chapter, we will discuss memory architecture
    and thread synchronization in detail.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，在本章中，你被介绍了CUDA C的编程概念以及如何使用CUDA进行并行计算。展示了CUDA程序可以高效且并行地运行在任何NVIDIA GPU硬件上。因此，CUDA既高效又可扩展。详细讨论了在并行数据计算中需要的超出现有ANSI
    C函数的CUDA API函数。还通过一个简单的两个变量加法示例讨论了如何通过内核调用从主机代码调用设备代码、配置内核参数以及向内核传递参数。还展示了CUDA不保证块或线程的运行顺序以及哪个块被分配到哪个多处理器。此外，还讨论了利用GPU和CUDA的并行处理能力进行的向量操作。可以看出，通过在GPU上执行向量操作，与CPU相比，可以显著提高吞吐量。在最后一节中，详细讨论了并行编程中遵循的各种常见通信模式。然而，我们还没有讨论内存架构以及线程如何在CUDA中相互通信。如果一个线程需要其他线程的数据，那么可以做什么也没有讨论。因此，在下一章中，我们将详细讨论内存架构和线程同步。 '
- en: Questions
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Write a CUDA program to subtract two numbers. Pass parameters by value in the
    kernel function.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个CUDA程序来减去两个数字。在内核函数中通过值传递参数。
- en: Write a CUDA program to multiply two numbers. Pass parameters by reference in
    the kernel function.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个CUDA程序来乘以两个数字。在内核函数中通过引用传递参数。
- en: Suppose you want to launch 5,000 threads in parallel. Configure kernel parameters
    in three different ways to accomplish this. Maximum 512 threads are possible per
    block.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想要并行启动5,000个线程。以三种不同的方式配置内核参数来完成此操作。每个块最多可以有512个线程。
- en: 'True or false: The programmer can decide in which order blocks will execute
    on the device, and blocks will be assigned to which streaming multiprocessor?'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对或错：程序员可以决定在设备上块将按何种顺序执行，以及块将被分配到哪个流多处理器？
- en: Write a CUDA program to find out that your system contains a GPU device that
    has a major-minor version of 5.0 or greater.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个CUDA程序以找出你的系统包含一个主次版本为5.0或更高版本的GPU设备。
- en: Write a CUDA program to find a cube of a vector that contains numbers from 0
    to 49.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个CUDA程序来找到一个包含从0到49的数字的向量的立方。
- en: For the following applications, which communication pattern is useful?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于以下应用，哪种通信模式是有用的？
- en: Image processing
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Moving average
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动平均
- en: Sorting array in ascending order
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按升序排序数组
- en: Finding cube of numbers in array
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数组中查找数字的立方
