- en: Parallel Programming using CUDA C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw how easy it is to install CUDA and write a program
    using it. Though the example was not impressive, it was shown to convince you
    that it is very easy to get started with CUDA. In this chapter, we will build
    upon this concept. It teaches you to write advance programs using CUDA for GPUs
    in detail. It starts with a variable addition program and then incrementally builds
    towards complex vector manipulation examples in CUDA C. It also covers how the
    kernel works and how to use device properties in CUDA programs. The chapter discusses
    how vectors are operated upon in CUDA programs and how CUDA can accelerate vector
    operations compared to CPU processing. It also discusses terminologies associated
    with CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of the kernel call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating kernel functions and passing parameters to it in CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring kernel parameters and memory allocation for CUDA programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread execution in CUDA programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing GPU device properties from CUDA programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with vectors in CUDA programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel communication patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires familiarity with the basic C or C++ programming language,
    particularly dynamic memory allocation functions. All the code used in this chapter
    can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).
    The code can be executed on any operating system, though it is only tested on
    Windows 10 and Ubuntu 16.04\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2PQmu4O](http://bit.ly/2PQmu4O)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA program structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen a very simple `Hello, CUDA!` program earlier, that showcased some
    important concepts related to CUDA programs. A CUDA program is a combination of
    functions that are executed either on the host or on the GPU device. The functions
    that do not exhibit parallelism are executed on the CPU, and the functions that
    exhibit data parallelism are executed on the GPU. The GPU compiler segregates
    these functions during compilation. As seen in the previous chapter, functions
    meant for execution on the device are defined using the `__global__` keyword and
    compiled by the NVCC compiler, while normal C host code is compiled by the C compiler.
    A CUDA code is basically the same ANSI C code with the addition of some keywords
    needed for exploiting data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: So, in this section, a simple two-variable addition program is taken to explain
    important concepts related to CUDA programming, such as kernel calls, passing
    parameters to kernel functions from host to device, the configuration of kernel
    parameters, CUDA APIs needed to exploit data parallelism, and how memory allocation
    takes place on the host and the device.
  prefs: []
  type: TYPE_NORMAL
- en: Two-variable addition program in CUDA C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the simple `Hello, CUDA!` code seen in [Chapter 1](26a373fe-8ec0-40bf-afb7-7db6a1d414c9.xhtml),
    *Introducing Cuda and Getting Started with Cuda,* the device function was empty.
    It had nothing to do. This section explains a simple addition program that performs
    addition of two variables on the device. Though it is not exploiting any data
    parallelism of the device, it is very useful for demonstrating important programming
    concepts of CUDA C. First, we will see how to write a kernel function for adding
    two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the kernel function is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gpuAdd` function looks very similar to a normal `add` function implemented
    in ANSI C. It takes two integer variables `d_a` and `d_b` as inputs and stores
    the addition at the memory location indicated by the third integer pointer `d_c`.
    The return value for the device function is void because it is storing the answer
    in the memory location pointed to by the device pointer and not explicitly returning
    any value. Now we will see how to write the main function for this code. The code
    for the main function is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the `main` function, the first two lines define variables for host and device.
    The third line allocates memory of the `d_c` variable on the device using the
    `cudaMalloc` function. The `cudaMalloc` function is similar to the `malloc` function
    in C. In the fourth line of the main function, `gpuAdd` is called with `1` and
    `4` as two input variables and `d_c`, which is a device memory pointer as an output
    pointer variable. The weird syntax of the `gpuAdd` function, which is also called
    a kernel call, is explained in the next section. If the answer of `gpuAdd` needs
    to be used on the host, then it must be copied from the device's memory to the
    host's memory, which is done by the `cudaMemcpy` function. Then, this answer is
    printed using the `printf` function. The penultimate line frees the memory used
    on the device by using the `cudafree` function. It is very important to free up
    all the memory used on the device explicitly from the program; otherwise, you
    might run out of memory at some point. The lines that start with `//` are comments
    for more code readability, and these lines are ignored by compilers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two-variable addition program has two functions, `main` and `gpuAdd`. As
    you can see, `gpuAdd` is defined by using the `__global__` keyword, and hence
    it is meant for execution on the device, while the main function will be executed
    on the host. The program adds two variables on the device and prints the output
    on the command line, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9869b1ed-27f0-432f-ab68-0b41b03af8e1.png)'
  prefs: []
  type: TYPE_IMG
- en: We will use a convention in this book that host variables will be prefixed with
    `h_` and device variables will be prefixed with `d_`. This is not compulsory;
    it is just done so that readers can understand the concepts easily without any
    confusion between host and device.
  prefs: []
  type: TYPE_NORMAL
- en: All CUDA APIs such as `cudaMalloc`, `cudaMemcpy`, and `cudaFree`, along with
    other important CUDA programming concepts such as kernel call, passing parameters
    to kernels, and memory allocation issues are discussed in upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel call
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The device code that is written using ANSI C keywords along with CUDA extension
    keywords is called a **kernel**. It is launched from the host code by a method
    called **kernel call**. Basically, the meaning of kernel call is that we are launching
    device code from the host code. A kernel call typically generates a large number
    of blocks and threads to exploit data parallelism on the GPU. Kernel code is very
    similar to normal C functions; it is just that this code is executed by several
    threads in parallel. It has a very weird syntax, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It starts with the name of the kernel that we want to launch. You should make
    sure that this kernel is defined using the `__global__` keyword. Then, it has
    the `<< < > >>` kernel launch operator that contains configuration parameters
    for kernel. It can include three parameters separated by a comma. The first parameter
    indicates the number of blocks you want to execute, and the second parameter indicates
    the number of threads each block will have. So, the total number of threads started
    by a kernel launch will be the product of these two numbers. The third parameter,
    which specifies the size of shared memory used by the kernel, is optional. In
    the program for variable addition, the kernel launch syntax is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, `gpuAdd` is the name of a kernel that we want to launch, and `<<<1,1>>>`
    indicates we want to start one block with one thread per block, which means that
    we are starting only one thread. Three arguments in round brackets are the parameters
    that are passed to the kernel. Here, we are passing two constants, `1` and `4`.
    The third parameter is a pointer to device memory `d_c`. It points at the location
    on device memory where the kernel will store its answer after addition. One thing
    that the programmer has to keep in mind is that pointers passed as parameters
    to kernel should only point to device memory. If it is pointing to host memory,
    it can crash your program. After kernel execution is completed, the result pointed
    by the device pointer can be copied back to host memory for further use. Starting
    only one thread for execution on the device is not the optimal use of device resources.
    Suppose you want to start multiple threads in parallel; what is the modification
    that you have to make in the syntax of the kernel call? This is addressed in the
    next section and is termed "configuring kernel parameters".
  prefs: []
  type: TYPE_NORMAL
- en: Configuring kernel parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For starting multiple threads on the device in parallel, we have to configure
    parameters in the kernel call, which are written inside the kernel launch operator.
    They specify the number of blocks and the number of threads per block. We can
    launch many blocks in parallel with many threads in each block. Normally, there
    is a limit of 512 or 1,024 threads per block. Each block runs on the streaming
    multiprocessor, and threads in one block can communicate with one another via
    shared memory. The programmer can't choose which multiprocessor will execute a
    particular block and in which order blocks or threads will execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you want to start 500 threads in parallel; what is the modification
    that you can make to the kernel launch syntax that was shown previously? One option
    is to start one block of 500 threads via the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can also start 500 blocks of one thread each or two blocks of 250 threads
    each. Accordingly, you have to modify values in the kernel launch operator. The
    programmer has to be careful that the number of threads per block does not go
    beyond the maximum supported limit of your GPU device. In this book, we are targeting
    computer vision applications where we need to work on two-and three-dimensional
    images. Here, it would be great if blocks and threads are not one-dimensional
    but more than that for better processing and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPU supports a three-dimensional grids of blocks and three-dimensional blocks
    of threads. It has the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here `N[bx]`, `N[by]`, and `N[bz]`indicate the number of blocks in a grid in
    the direction of the *x*, *y*, and *z* axes, respectively. Similarly, `N[t][x]`,
    `N[ty]`, and `N[tz]` indicate the number of threads in a block in the direction
    of the *x*, *y*, and z axes. If the *y* and z dimensions are not specified, they
    are taken as `1` by default. So, for example, to process an image, you can start
    a 16 x 16 grid of blocks, all containing 16 x 16 threads. The syntax will be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To summarize, the configuration of the number of blocks and the number of threads
    is very important while launching the kernel. It should be chosen with proper
    care depending on the application that we are working on and the GPU resources.
    The next section will explain some important CUDA functions added over regular
    ANSI C functions.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA API functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the variable addition program, we have encountered some functions or keywords
    that are not familiar to regular C or C++ programmers. These keywords and functions
    include `__global__` , `cudaMalloc`, `cudaMemcpy`, and `cudaFree`. So, in this
    section, these functions are explained in detail one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**__global__** :It is one of three qualifier keywords, along with `__device__`
    and `__host__` . This keyword indicates that a function is declared as a device
    function and will execute on the device when called from the host. It should be
    kept in mind that this function can only be called from the host. If you want
    your function to execute on the device and called from the device function, then
    you have to use the `__device__` keyword. The `__host__` keyword is used to define
    host functions that can only be called from other host functions. This is similar
    to normal C functions. By default, all functions in a program are host functions.
    Both `__host__` and `__device__` can be simultaneously used to define any function.
    It generates two copies of the same function. One will execute on the host, and
    the other will execute on the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cudaMalloc**:It is similar to the `Malloc` function used in C for dynamic
    memory allocation. This function is used to allocate a memory block of a specific
    size on the device. The syntax of `cudaMalloc` with an example is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding example code, it allocates a memory block of size
    equal to the size of one integer variable and returns the pointer `d_c`, which
    points to this memory location.
  prefs: []
  type: TYPE_NORMAL
- en: '**cudaMemcpy**:This function is similar to the `Memcpy` function in C. It is
    used to copy one block of memory to other blocks on a host or a device. It has
    the following syntax:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This function has four arguments. The first and second arguments are the destination
    pointer and the source pointer, which point to the host or device memory location.
    The third argument indicates the size of the copy and the last argument indicates
    the direction of the copy. It can be from host to device, device to device, host
    to host, or device to host. But be careful, as you have to match this direction
    with the appropriate pointers as the first two arguments. As shown in the example,
    we are copying a block of one integer variable from the device to the host by
    specifying the device pointer `d_c` as the source, and the host pointer `h_c`
    as a destination.
  prefs: []
  type: TYPE_NORMAL
- en: '**cudaFree**: It is similar to the free function available in C. The syntax
    of `cudaFree` is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It frees the memory space pointed to by `d_ptr`. In the example code, it frees
    the memory location pointed to by `d_c`. Please make sure that `d_c` is allocated
    memory, using `cudaMalloc` to free it using `cudaFree`.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other keywords and functions available in CUDA over and above
    existing ANSI C functions. We will be frequently using only these three functions,
    and hence they are discussed in this section. For more details, you can always
    visit the CUDA programming guide.
  prefs: []
  type: TYPE_NORMAL
- en: Passing parameters to CUDA functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `gpuAdd` kernel function of the variable addition program is very similar
    to the normal C function. So, like normal C functions, the kernel functions can
    also be passed parameters by value or by reference. Hence, in this section, we
    will see both the methods to pass parameters for CUDA kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Passing parameters by value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall, in the `gpuAdd` program, the syntax for calling the kernel was
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the signature of the `gpuAdd` function in definition was
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you can see that we are passing values of `d_a` and `d_b` while calling
    the kernel. First, parameter `1` will be copied to `d_a` and then parameter `4`
    will be copied to `d_b` while calling the kernal. The answer after addition will
    be stored at the address pointed by `d_c` on device memory. Instead of directly
    passing values `1` and `4` as inputs to the kernel, we can also write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, `a` and `b` are integer variables that can contain any integer values.
    Passing parameters by values is not recommended, as it creates unnecessary confusion
    and complications in programs. It is better to pass parameters by reference.
  prefs: []
  type: TYPE_NORMAL
- en: Passing parameters by reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will see how to write the same program by passing parameters by reference.
    For that, we have to first modify the kernel function for addition of two variables.
    The modified kernel for passing parameters by reference is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of using integer variables `d_a` and `d_b` as inputs to the kernel,
    the pointers to these variables on device `*d_a` and `*d_b` are taken as inputs.
    The answer which will be obtained after the addition is stored at the memory location
    pointed by third integer pointer `d_c`. The pointers passed as a reference to
    this device function should be allocated memory with the `cudaMalloc` function.
    The main function for this code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`h_a`, `h_b`, and `h_c` are variables in the host memory. They are defined
    like normal C code. On the other hand, `d_a`, `d_b`, and `d_c` are pointers residing
    on host memory, and they point to the device memory. They are allocated memory
    from the host by using the `cudaMalloc` function. The values of `h_a` and `h_b`
    are copied to the device memory pointed to by `d_a` and `d_b` by using the `cudaMemcpy`
    function, and the direction of data transfer is from the host to the device. Then,
    in kernel call, these three device pointers are passed to the kernel as parameters.
    The kernel computes addition and stores the result at the memory location pointed
    by `d_c`. The result is copied back to the host memory by using `cudaMemcpy` again,
    but this time with the direction of data transfer as the device to host. The output
    of the program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9097c292-8aee-4dbc-964a-31ec2a203559.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The memory used by three device pointers is freed by using the `cudaFree` at
    the end of the program. The sample memory map on the host and the device will
    look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Host Memory (CPU)** | **Device Memory (GPU)** |'
  prefs: []
  type: TYPE_TB
- en: '| Address | Value | Address | Value |'
  prefs: []
  type: TYPE_TB
- en: '| #01 | h_a=1 | #01 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| #02 | h_b=4 | #02 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| #03 | h_c=5 | #03 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| #04 | d_a=#01 | #04 |  |'
  prefs: []
  type: TYPE_TB
- en: '| #05 | d_b=#02 | #05 |  |'
  prefs: []
  type: TYPE_TB
- en: '| #06 | d_c=#03 | #06 |  |'
  prefs: []
  type: TYPE_TB
- en: As you can see from the table, `d_a`, `d_b`, and `d_c` are residing on the host
    and pointing to values on the device memory. While passing parameters by reference
    to kernels, you should take care that all pointers are pointing to the device
    memory only. If it is not the case, then the program may crash.
  prefs: []
  type: TYPE_NORMAL
- en: While using device pointers and passing them to kernels, there are some restrictions
    that have to be followed by the programmer. The device pointers that are allocated
    memory using `cudaMalloc` can only be used to read or write from the device memory.
    They can be passed as parameters to the device function, and they should not be
    used to read and write memory from the host functions. To simplify, device pointers
    should be used to read and write device memory from the device function, and host
    pointers should be used to read and write host memory from host functions. So,
    in this book, you will always find the device pointer prefixed by `d_` in kernel
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in this section, concepts related to CUDA programming were explained
    in detail by taking two-variable additional programs as an example. After this
    section, you should be familiar with basic CUDA programming concepts and the terminology
    associated with CUDA programs. In the next section, you will learn how threads
    are executed on the device.
  prefs: []
  type: TYPE_NORMAL
- en: Executing threads on a device
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen that, while configuring kernel parameters, we can start multiple
    blocks and multiple threads in parallel. So, in which order do these blocks and
    threads start and finish their execution? It is important to know this if we want
    to use the output of one thread in other threads. To understand this, we have
    modified the kernel in the `hello,CUDA!` program we saw in the first chapter,
    by including a print statement in the kernel call, which prints the block number.
    The modified code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen from the code, we are launching a kernel with 16 blocks in parallel
    with each block having a single thread. In the kernel code, we are printing the
    block ID of the kernel execution. We can think that 16 copies of the same `myfirstkernel`
    start execution in parallel. Each of these copies will have a unique block ID,
    which can be accessed by the `blockIdx.x CUDA` directive, and a unique thread
    ID, which can be accessed by `threadIdx.x`. These IDs will tell us which block
    and thread are executing the kernel. When you run the program many times, you
    will find that, each time, blocks execute in a different order. One sample output
    can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50a8685b-96c7-4229-8ac2-2562c57a9a02.png)'
  prefs: []
  type: TYPE_IMG
- en: One question you should ask is how many different output patterns will the previous
    program produce? The correct answer is 16! It will produce *n* factorial number
    of outputs, where *n* indicates the number of blocks started in parallel. So,
    whenever you are writing the program in CUDA, you should be careful that the blocks
    execute in random order.
  prefs: []
  type: TYPE_NORMAL
- en: 'This program also contains one more CUDA directive: `cudaDeviceSynchronize()`.
    Why is it used? It is used because a kernel launch is an asynchronous process,
    which means it returns control to the CPU thread immediately after starting up
    the GPU process before the kernel has finished executing. In the previous code,
    the next line in CPU thread is `print` and application exit will terminate console
    before the kernel has finished execution. So, if we do not include this directive,
    you will not see any print statements of the kernel execution. The output that
    is generated later by the kernel has nowhere to go, and you won''t see it. To
    see the outputs generated by the kernel, we will include this directive, which
    ensures that the kernel finishes before the application is allowed to exit, and
    the output from the kernel will find a **waiting standard output queue**.'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing GPU device properties from CUDA programs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CUDA provides a simple interface to find the information such as determining
    which CUDA-enabled GPU devices (if any) are present and what capabilities each
    device supports. First, it is important to get a count of how many CUDA-enabled
    devices are present on the system, as a system may contain more than one GPU-enabled
    device. This count can be determined by the CUDA API `cudaGetDeviceCount()`. The
    program for getting a number of CUDA enabled devices on the system is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The relevant information about each device can be found by querying the `cudaDeviceProp`
    structure, which returns all the device properties. If you have more than one
    CUDA-capable device, then you can start a for loop to iterate over all device
    properties. The following section contains the list of device properties divided
    into different sets and small code snippets used to access them from CUDA programs.
    These properties are provided by the `cudaDeviceProp` structure in CUDA 9 runtime.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about properties in the different versions of CUDA, you can
    check the programming guide for a particular version.
  prefs: []
  type: TYPE_NORMAL
- en: General device properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`cudaDeviceProp` provides several properties that can be used to identify the
    device and the versions being used. It provides the `name` property that returns
    the name of the device as a string. We can also get a version of the driver and
    the runtime engine the device is using by querying `cudaDriverGetVersion` and
    `cudaRuntimeGetVersion` properties. Sometimes, if you have more than one device,
    you want to use the device that has more multiprocessors. The `multiProcessorCount`
    property returns the count of the number of multiprocessors on the device. The
    speed of the GPU in terms of clock rate can be fetched by using the `clockRate`
    property. It returns clock rate in Khz. The following code snippet shows how to
    use these properties from the CUDA program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Memory-related properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory on the GPU has a hierarchical architecture. It can be divided in terms
    of L1 cache, L2 cache, global memory, texture memory, and shared memory. The `cudaDeviceProp`
    provides many properties that help in identifying memory available with the device.
    `memoryClockRate` and `memoryBusWidth` provide clock rate and bus width of the
    memory respectively. The speed of the memory is very important. It affects the
    overall speed of your program. `totalGlobalMem` returns the size of global memory
    available with the device. `totalConstMem` returns the total constant memory available
    with the device. `sharedMemPerBlock` returns the total shared memory that can
    be used in tne device. The total number of registers available per block can be
    identified by using `regsPerBlock`. Size of L2 cache can be identified using the
    `l2CacheSize` property. The following code snippet shows how to use memory-related
    properties from the CUDA program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Thread-related properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As seen in earlier sections, blocks and threads can be multidimensional. So,
    it would be nice to know how many threads and blocks can be launched in parallel
    in each dimension. There is also a limit on the number of threads per multiprocessor
    and the number of threads per block. This number can be found by using the `maxThreadsPerMultiProcessor`
    and the `maxThreadsPerBlock`. It is very important in the configuration of kernel
    parameters. If you launch more threads per block than the maximum threads possible
    per block, your program can crash. The maximum threads per block in each dimension
    can be identified by the `maxThreadsDim`. In the same way, the maximum blocks
    per grid in each dimension can be identified by using the `maxGridSize`. Both
    of them return an array with three values, which shows the maximum value in the
    *x*, *y,* and *z* dimensions respectively. The following code snippet shows how
    to use thread-related properties from the CUDA code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many other properties available in the `cudaDeviceProp` structure.
    You can check the CUDA programming guide for details of other properties. The
    output from all preceding code sections combined and executed on the NVIDIA Geforce
    940MX GPU and CUDA 9.0 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71c144c7-d4f6-46e1-bd68-3558fd3778f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One question you might ask is why you should be interested in knowing the device
    properties. The answer is that this will help you in choosing a GPU device with
    more multiprocessors, if multiple GPU devices are present. If in your application
    the kernel needs close interaction with the CPU, then you might want your kernel
    to run on an integrated GPU that shares system memory with the CPU. These properties
    will also help you in finding the number of blocks and number of threads per block
    available on your device. This will help you with the configuration of kernel
    parameters. To show you one use of device properties, suppose you have an application
    that requires double precision for floating-point operation. Not all GPU devices
    support this operation. To know whether your device supports double precision
    floating-point operation and set that device for your application, the following
    code can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This code uses two properties available in the `cudaDeviceprop` structure that
    help in identifying whether the device supports double precision operations. These
    two properties are major and minor. CUDA documentation says us that if major is
    greater than 1 and minor is greater than 3, then that device will support double
    precision operations. So, the program's `device_property` structure is filled
    with these two values. CUDA also provides the `cudaChooseDevice` API that helps
    in choosing a device with particular properties. This API is used on the current
    device to identify whether it contains these two properties. If it contains properties,
    then that device is selected for your application using the `cudaSetDevice` API.
    If more than one device is present in the system, this code should be written
    inside a for a loop to iterate over all devices.
  prefs: []
  type: TYPE_NORMAL
- en: Though trivial, this section is very important for you in finding out which
    applications can be supported by your GPU device and which cannot.
  prefs: []
  type: TYPE_NORMAL
- en: Vector operations in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, the programs that we have seen were not leveraging any advantages
    of the parallel-processing capabilities of GPU devices. They were just written
    to get you familiar with the programming concepts in CUDA. From this section,
    we will start utilizing the parallel-processing capabilities of the GPU by performing
    vector or array operations on it.
  prefs: []
  type: TYPE_NORMAL
- en: Two-vector addition program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand vector operation on the GPU, we will start by writing a vector
    addition program on the CPU and then modify it to utilize the parallel structure
    of GPU. We will take two arrays of some numbers and store the answer of element-wise
    addition in the third array. The vector addition function on CPU is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cpuAdd` should be very simple to understand. One thing you might find
    difficult to understand is the use of `tid`. It is included to make the program
    similar to the GPU program, in which `tid` indicated a particular thread ID. Here,
    also, if you have a multicore CPU, then you can initialize `tid` equal to 0 and
    1 for each of them and then add 2 to it in the loop so that one CPU will perform
    a sum on even elements and one CPU will perform addition on odd elements. The
    main function for the code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two functions in the program: `main` and `cpuAdd`. In the main function,
    we start by defining two arrays to hold inputs and initialize it to some random
    numbers. Then, we pass these two arrays as input to the `cpuAdd` function. The
    `cpuAdd` function stores the answer in the third array. Then, we print this answer
    on the console, which is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/783fe03c-7410-4177-9649-d6aa378983ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This explanation of using the `tid in cpuadd` function may give you an idea
    of how to write the same function for the GPU execution, which can have many cores
    in parallel. If we initialize this add function with the ID of that core, then
    we can do the addition of all the elements in parallel. So, the modified kernel
    function for addition on the GPU is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `gpuAdd` kernel function, `tid` is initialized with the block ID of
    the current block in which the kernel is executing. All kernels will add an array
    element indexed by this block ID. If the number of blocks are equal to the number
    of elements in an array, then all the addition operations will be done in parallel.
    How this kernel is called from the main function is explained next. The code for
    the main function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The GPU main function has the known structure as explained in the first section
    of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: It starts with defining arrays and pointers for host and device. The device
    pointers are allocated memory using the `cudaMalloc` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The arrays, which are to be passed to the kernel, are copied from the host memory
    to the device memory by using the `cudaMemcpy` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel is launched by passing the device pointers as parameters to it. If
    you see the values inside the kernel launch operator, they are *N* and *1*, which
    indicate we are launching *N* blocks with one thread per each block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer stored by the kernel on the device memory is copied back to the host
    memory by again using the `cudaMemcpy`, but this time with the direction of data
    transfer from the device to the host.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And, finally, memory allocated to three device pointers is freed up by using
    the `cudaFree` function. The output of the program is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2e6cda1b-587e-4e29-b716-4459ad2ddfe2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All CUDA programs follow the same pattern as shown before. We are launching
    N blocks in parallel. The meaning of this is that we are launching N copies of
    the same kernel simultaneously. You can understand this by taking a real-life
    example: Suppose you want to transfer five big boxes from one place to another.
    In the first method, you can perform this task by hiring one person who takes
    one block from one place to the other and repeat this five times. This option
    will take time, and it is similar to how vectors are added to the CPU. Now, suppose
    you hire five people and each of them carries one box. Each of them also knows
    the ID of the box they are carrying. This option will be much faster than the
    previous one. Each one of them just needs to be told that they have to carry one
    box with a particular ID from one place to the other.'
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly how kernels are defined and executed on the device. Each kernel
    copy knows the ID of it. This can be known by the `blockIdx.x` command. Each copy
    works on an array element indexed by its ID. All copies add all elements in parallel,
    which significantly reduces the processing time for the entire array. So, in a
    way, we are improving the throughput by doing operations in parallel over CPU
    sequential execution. The comparison of throughput between the CPU code and the
    GPU code is explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing latency between the CPU and the GPU code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The programs for CPU and the GPU addition are written in a modular way so you
    can play around with the value of N. If N is small, then you will not notice any
    significant time difference between the CPU and the GPU code. But if you N is
    sufficiently large, then you will notice the significant difference in the CPU
    execution time and the GPU execution time for the same-vector addition. The time
    taken for the execution of a particular block can be measured by adding the following
    lines to the existing code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Time is measured by calculating the total number of clock cycles taken to perform
    a particular operation. This can be done by taking the difference of starting
    and ending the clock tick count, measured using the clock() function. This is
    divided by the number of clock cycles per second, to get the execution time. When
    N is taken as 10,000,000 in the previous vector addition programs of the CPU and
    the GPU and executed simultaneously, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/085e74b0-e73e-485d-bf4d-1a3dc7f3ffc9.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the output, the execution time or throughput is improved
    from 25 milliseconds to almost 1 millisecond when the same function is implemented
    on GPU. This proves what we have seen in theory earlier that executing code in
    parallel on GPU helps in the improvement of throughput. CUDA provides an efficient
    and accurate method for measuring the performance of CUDA programs, using CUDA
    events, which will be explained in the later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Elementwise squaring of vectors in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, one question you can ask is, now that we are launching N blocks in parallel
    with one thread in each block, can we work in a reverse way? The answer is *yes*.
    We can launch only one block with N threads in parallel. To show that and make
    you more familiar with working around vectors in CUDA, we take the second example
    of the element-wise squaring of numbers in an array. We take one array of numbers
    and return an array that contains the square of these numbers. The kernel function
    to find the element-wise square is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gpuSquare` kernel function has pointers to two arrays as arguments. The
    first pointer `d_in` points to the memory location where the input array is stored,
    while the second pointer `d_out` points to the memory location where output will
    be stored. In this program, instead of launching multiple blocks in parallel,
    we want to launch multiple threads in parallel, so `tid` is initialized with a
    particular thread ID using `threadIdx.x`. The main function for this program is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This main function follows a similar structure to the vector addition program.
    One difference that you will see here from the vector addition program is that
    we are launching a single block with N threads in parallel. The output of the
    program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f31562a9-874e-4188-9432-a6b379875f16.png)'
  prefs: []
  type: TYPE_IMG
- en: Whenever you are using this way of launching N threads in parallel, you should
    take care that the maximum threads per block are limited to 512 or 1,024\. So,
    the value of N should be less than this value. If N is 2,000 and the maximum number
    of threads per block for your device is 512, then you can't write `<< <1,2000
    > >>`. Instead, you should use something such as `<< <4,500> >>`. The choice of
    a number of blocks and the number of threads per block should be made judiciously.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have learned how to work with vectors and how we can launch
    multiple blocks and multiple threads in parallel. We have also seen that by doing
    vector operations on GPU, it improves throughput, compared to the same operation
    on the CPU. In the last section of this chapter, we discuss the various parallel
    communication patterns that are followed by threads executing in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel communication patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When several thread is executed in parallel, they follow a certain communication
    pattern that indicates where it is taking inputs and where it is writing its output
    in memory. We will discuss each communication pattern one by one. It will help
    you to identify communication patterns related to your application and how to
    write code for that.
  prefs: []
  type: TYPE_NORMAL
- en: Map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this communication pattern, each thread or task takes a single input and
    produces a single output. Basically, it is a one-to-one operation. The vector
    addition program and element-wise squaring program, seen in the previous sections,
    are examples of the map pattern. The code of the map pattern will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Gather
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this pattern, each thread or task has multiple inputs, and it produces a
    single output to be written at a single location in memory. Suppose you want to
    write a program that finds a moving average of three numbers; this is an example
    of a gather operation. It takes three inputs from memory and writes single output
    to memory. So, there is data reuse on the input side. It is basically a many-to-one
    operation. The code for gather pattern will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Scatter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a scatter pattern, a thread or a task takes a single input and computes
    where in the memory it should write the output. Array sorting is an example of
    a scatter operation. It can also be one-to-many operations. The code for scatter
    pattern will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Stencil
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When threads or tasks read input from a fixed set of a neighborhood in an array,
    then this is called a **stencil** **communication pattern**. It is very useful
    in image-processing examples where we work on 3x3 or 5x5 neighborhood windows.
    It is a special form of a gather operation, so code syntax is similar to it.
  prefs: []
  type: TYPE_NORMAL
- en: Transpose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the input is in the form of a row-major matrix, and we want the output
    to be in column-major form, we have to use this transpose communication pattern.
    It is particularly useful if you have a structure of arrays and you want to convert
    it in the form of an array of structures. It is also a one-to-one operation. The
    code for the transpose pattern will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In this section, various communication patterns that CUDA programming follows
    is discussed. It is useful to find a communication pattern related to your application
    and use the code syntax of that pattern shown as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, in this chapter, you were introduced to programming concepts in
    CUDA C and how parallel computing can be done using CUDA. It was shown that CUDA
    programs can run on any NVIDIA GPU hardware efficiently and in parallel. So, CUDA
    is both efficient and scalable. The CUDA API functions over and above existing
    ANSI C functions needed for parallel data computations were discussed in detail.
    How to call device code from the host code via a kernel call, configuring of kernel
    parameters, and a passing of parameters to the kernel were also discussed by taking
    a simple two-variable addition example. It was also shown that CUDA does not guarantee
    the order in which the blocks or thread will run and which block is assigned to
    which multi-processor in hardware. Moreover, vector operations, which take advantage
    of parallel-processing capabilities of GPU and CUDA, were discussed. It can be
    seen that, by performing vector operations on the GPU, it can improve the throughput
    drastically, compared to the CPU. In the last section, various common communication
    patterns followed in parallel programming were discussed in detail. Still, we
    have not discussed memory architecture and how threads can communicate with one
    another in CUDA. If one thread needs data of the other thread, then what can be
    done is also not discussed. So, in the next chapter, we will discuss memory architecture
    and thread synchronization in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Write a CUDA program to subtract two numbers. Pass parameters by value in the
    kernel function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a CUDA program to multiply two numbers. Pass parameters by reference in
    the kernel function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you want to launch 5,000 threads in parallel. Configure kernel parameters
    in three different ways to accomplish this. Maximum 512 threads are possible per
    block.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: The programmer can decide in which order blocks will execute
    on the device, and blocks will be assigned to which streaming multiprocessor?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a CUDA program to find out that your system contains a GPU device that
    has a major-minor version of 5.0 or greater.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a CUDA program to find a cube of a vector that contains numbers from 0
    to 49.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the following applications, which communication pattern is useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving average
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sorting array in ascending order
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finding cube of numbers in array
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
