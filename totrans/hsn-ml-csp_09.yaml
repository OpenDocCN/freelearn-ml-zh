- en: Should I Take the Job â€“ Decision Trees in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will focus on:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create decision trees for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding truth tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual intuition regarding false negatives and false positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we dive right in, let's gain some background information which will be
    helpful to us.
  prefs: []
  type: TYPE_NORMAL
- en: For a decision tree to be complete and effective, it must contain all possibilities,
    meaning every pathway in and out. Event sequences must also be supplied and be
    mutually exclusive, meaning if one event happens, the other one cannot.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are a form of **supervised machine learning**, in that we have
    to explain what the input and output should be. There are decision nodes and leaves.
    The leaves are the decisions, final or not, and the nodes are where decision splits
    occur.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many algorithms available for our use, we are going to use
    the **Iterative Dichotomizer 3** (**I****D3**) algorithm. During each recursive
    step, the attribute that best classifies the set of inputs we are working with
    is selected according to a criterion **(InfoGain**, **GainRatio**, and so on).
    It must be pointed out that regardless of the algorithm that we use, none are
    guaranteed to produce the smallest tree possible. This has a direct implication
    on the performance of our algorithm. Keep in mind that with decision trees, learning
    is based solely on heuristics, not true optimized criteria. Let's use an example
    to explain this further.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example is from [http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf](http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf),
    and it illustrates the XOR learning concept, which all of us developers are (or
    should be) familiar with. You will see this happen in a later example as well,
    but for now, **a[3]** and **a[4]** are completely irrelevant to the problem we
    are trying to solve. They have zero influence on our answer. That being said,
    the ID3 algorithm will select one of them to belong to the tree, and in fact,
    it will use **a[4]** as the root! Remember that this is heuristic learning of
    the algorithm and not optimized findings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecc3b212-9667-450d-a320-6daea4b9b330.png)'
  prefs: []
  type: TYPE_IMG
- en: Hopefully this visual will make it easier to understand what we mean. The goal
    here isn't to get too deep into decision tree mechanics and theory. After all
    of that, you might be asking why we are even talking about decision trees. Despite
    any issues they may have, decision trees work as the base for many algorithms,
    especially those that need a human description of the results. They are also the
    basis for the Viola & Jones (2001) real-time facial detection algorithm we used
    in an earlier chapter. As perhaps a better example, the **Kinect of Microsoft
    Xbox 360** uses decision trees as well.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we will turn to the Accord.NET open source framework to illustrate
    our concept. In our sample, we'll be dealing with the following decision tree
    objects, so it's best that we discuss them upfront.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is our main class.
  prefs: []
  type: TYPE_NORMAL
- en: Decision node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a node for our decision tree. Each node may or may not have children
    associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: Decision variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This object defines the nature of each decision variable that the tree and nodes
    can process. The values can be ranges, continuous, or discrete.
  prefs: []
  type: TYPE_NORMAL
- en: Decision branch node collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This collection contains decision nodes grouped together with additional information
    about their decision variables for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a decision tree for determining financial risks. It is
    very easy to follow simply by navigating the nodes, making a decision as to which
    way to go, until you get to the final answer. In this case, someone is applying
    for credit and we need to make a decision on their credit worthiness. A decision
    tree is a great approach to solving this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6259d852-b213-4ae2-b21d-8336f635d200.png)'
  prefs: []
  type: TYPE_IMG
- en: With this simplistic visual diagram behind us, let's visit the problem we are
    trying to solve. It's one that all of us developers face (hopefully) from time
    to time.
  prefs: []
  type: TYPE_NORMAL
- en: Should I take the job?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have just been offered a new job, and you need to decide whether or not
    you are going to take it. There are a few things that are important for you to
    consider, so we will use those as input variables, or features, for our decision
    tree. Here is what is important to you: pay, benefits, company culture, and of
    course, *Can I work from home?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than load data from disk storage, we are going to create an in-memory
    database and add our features this way. We will create `DataTable` and create
    `Columns` as features as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we will load several rows of data, each with a different set of
    features, and our last column, `ShouldITakeJob` being a `Yes` or `No`, as our
    final decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all of the data is created and in our table, we need to put our previous
    features into a form of representation that our computer can understand. Since
    all our features are categories, it doesn''t really matter how we represent them,
    if we are consistent. Since numbers are easier, we will convert our features (categories)
    to a code book through a process known as `Codification`. This `codebook` effectively
    transforms every value into an integer. Notice that we will pass in our `data`
    categories as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to create decision variables for our decision tree to use. The
    tree will be trying to help us determine if we should take our new job offer or
    not. For this decision, there will be several categories of inputs, which we will
    specify in our decision variable array, and two possible decisions, `Yes` or `No`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DecisionVariable` array will hold the name of each category as well as
    the total count of possible attributes for this category. For example, the `Pay`
    category has three possible values, `Good`, `Average`, or `Poor`. So, we specify
    the category name and the number `3`. We then repeat this for all our other categories
    except the last one, which is our decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our decision tree created, we have to teach it the problem
    we are trying to solve. At this point, it really knows nothing. In order to do
    that, we will have to create a learning algorithm for the tree to use. In our
    case, that would be the ID3 algorithm we discussed earlier. Since we have only
    categorical values for this sample, the ID3 algorithm is the simplest choice.
    Please feel free to replace it with C4.5, C5.0, or whatever you want to try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the learning algorithm has been run, it is trained and ready to use. We
    simply provide a sample dataset to the algorithm so that it can give us back an
    answer. In this case, the pay is good, the company culture is good, the benefits
    are good, and I can work from home. If the decision tree is trained properly,
    the answer is a resounding `Yes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will turn our attention to using the `numl` open source machine learning
    package to show you another example of training and using a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: numl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`numl` is a very famous open source machine learning toolkit. As with most
    machine learning frameworks, it too uses the `Iris` dataset for many of its examples,
    including the one we will use for decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of our `numl` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98c83b86-cbb0-45fe-9484-ca73bde79182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the code behind that example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Definitely not the most complex of functions, is it? This is the beauty of using
    `numl` in your applications; it's incredibly easy to use and to integrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code creates a descriptor and `DecisionTreeGenerator`, loads
    the `Iris` dataset, and then generates a model. Here is just a sample of the data
    being loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And so on...
  prefs: []
  type: TYPE_NORMAL
- en: Accord.NET decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Accord.NET framework has its own decision tree example that we should point
    out as well. It takes a different, more graphical approach to decision trees,
    but the call is yours to make as to which you like and feel most comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the data is loaded you can create your decision tree and get it ready
    for learning. You will see a data plot similar to what you see here, using two
    categories, X and Y:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82332643-96dd-4752-b9b2-d7b2ee02fec0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next tab will let you see the tree nodes, leafs, and decisions. There is
    also a top-down graphical view of the tree on the right. The most useful information
    is located within the Tree View on the left, where you can see the nodes, their
    values, and the decisions that were made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d2c9930-b95d-4b85-a279-73a823e9521a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the last tab will let you perform Model Testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53650f92-6a31-4922-939c-400023ae0da3.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following is the learning code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This value is then fed to a `ConfusionMatrix`. For those of you that are not
    familiar with this, let me briefly explain.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **confusion matrix** is a table used to describe the performance of a classification
    model. It is run on a test dataset for which the truth values are known. This
    is how we arrive at things such as the following.
  prefs: []
  type: TYPE_NORMAL
- en: True positives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a case where we predicted yes, and this was the truth.
  prefs: []
  type: TYPE_NORMAL
- en: True negatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a case where we predicted no, and this was the truth.
  prefs: []
  type: TYPE_NORMAL
- en: False positives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a case where we predicted yes but the truth was no. You might sometimes
    see this referred to as a **type 1** error.
  prefs: []
  type: TYPE_NORMAL
- en: False negatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a case where we predicted no but the truth was yes. You might sometimes
    see this referred to as **type II** error.
  prefs: []
  type: TYPE_NORMAL
- en: Now, with all that being said, we need to talk about two other important terms,
    **precision** and **recall**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's describe them this way. For the past week, it has rained every day. That's
    7 days out of 7\. Simple enough. A week later, you are asked *How often did it
    rain last week?*
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is the ratio of the number of days you correctly recalled the rain relative
    to the overall number of correct events. If you said it rained 7 days, that's
    100%. If you said it rained 4 days, then that's 57% recall. In this case, it means
    your recall was not so precise, so we have precision to recognize.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is the ratio of times you correctly recalled it was going to rain, relative
    to the total number of days in that week.
  prefs: []
  type: TYPE_NORMAL
- en: For us, if our machine learning algorithm is good at recall, it doesn't necessarily
    mean it's good at precision. Makes sense? That gets us into other things such
    as F1 scores, which we'll leave for another day.
  prefs: []
  type: TYPE_NORMAL
- en: Error type visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some visualizations that may be of help:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea7d0942-166e-4cef-befb-29221e671d56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Identification of true positives versus false negatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1a2f8e6-bd4d-4696-ab9e-27bf8a3ca144.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After using the confusion matrix to compute the statistics, the scatter plot
    is created and everything is identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d96075af-68b0-44bf-919b-1505b414d992.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we devoted a lot of time towards decision trees; what they
    are, how we can use them, and how they can benefit us in our applications. In
    our next chapter we are going to enter the world of **Deep Belief Networks** (**DBNs**),
    what they are, and how we can use them. We'll even talk a little bit about what
    a computer dreams, when it dreams!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bishop, C. M., 2007\. *Pattern Recognition and Machine Learning (Information
    Science and Statistics).* 1st ed. 2006\. Corr. 2nd printing ed. s.l.: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fayyad, U. M. & Irani, K. B., 1992\. [http://deepblue.lib.umich.edu/bitstream/2027.42/46964/1/10994_2004_Article_422458.pdf](http://deepblue.lib.umich.edu/bitstream/2027.42/46964/1/10994_2004_Article_422458.pdf).
    *Machine Learning,* January, 8(1), pp. 87-102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quinlan, J. R., 1986\. [http://www.dmi.unict.it/~apulvirenti/agd/Qui86.pdf](http://www.dmi.unict.it/~apulvirenti/agd/Qui86.pdf).
    *Machine Learning,* 1(1), pp. 81-106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quinlan, J. R., 1993\. *C4.5: Programs for Machine Learning (Morgan Kaufmann
    Series in Machine Learning).* 1 ed. s.l.: Morgan Kaufmann.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shotton, J. et al., 2011\. [http://research.microsoft.com/apps/pubs/default.aspx?id=145347](http://research.microsoft.com/apps/pubs/default.aspx?id=145347)*.*
    s.l., s.n.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola, P. & Jones, M., 2001\. *Robust Real-time Object Detection.* s.l., s.n.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mitchell, T. M., 1997\. Decision Tree Learning. In:: *Machine Learning (McGraw-Hill
    Series in Computer Science).* s.l.: McGraw Hill.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mitchell, T. M., 1997\. *Machine Learning (McGraw-Hill Series in Computer Science).*
    Boston(MA): WCB/McGraw-Hill.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esmeir, S. & Markovitch, S., 2007\. [http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf](http://jmlr.csail.mit.edu/papers/volume8/esmeir07a/esmeir07a.pdf).
    *J. Mach. Learn. Res.,* May, Volume 8, pp. 891-933.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyafil, L. & Rivest, R. L., 1976\. Constructing Optimal Binary Decision Trees
    is NP-complete. *Information Processing Letters,* 5(1), pp. 15-17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Ross_Quinlan](https://en.wikipedia.org/wiki/Ross_Quinlan).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
