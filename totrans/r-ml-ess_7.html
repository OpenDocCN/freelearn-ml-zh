<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Overview of Machine Learning Techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Overview of Machine Learning Techniques</h1></div></div></div><p>There are different machine learning techniques and this chapter gives an overview of the most relevant ones. Some of them have already been introduced in the previous chapters and some are new.</p><p>In this chapter, you will learn the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The most relevant branches of techniques: supervised and unsupervised learning</li><li class="listitem" style="list-style-type: disc">Making predictions with supervised learning</li><li class="listitem" style="list-style-type: disc">Identifying hidden patterns and structures with unsupervised learning</li><li class="listitem" style="list-style-type: disc">Pros and cons of these techniques</li></ul></div><div class="section" title="Overview"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Overview</h1></div></div></div><p>There are <a id="id364" class="indexterm"/>different categories of machine learning techniques and in this chapter we will see the two most relevant branches—supervised and unsupervised learning, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/7740OS_07_01.jpg" alt="Overview"/></div><p>The supervised and unsupervised learning techniques deal with objects described by features. An example of supervised learning techniques is decision tree learning, and an example of <a id="id365" class="indexterm"/>unsupervised technique is k-means. In both cases, the algorithms learn from a set of objects and the difference is their target: supervised <a id="id366" class="indexterm"/>techniques predict attributes whose nature is already known and unsupervised techniques identify new patterns.</p><p>The supervised learning techniques predict an attribute of the objects. The algorithms learn from a training set of objects whose attribute is known and they predict the attribute of other objects. There are two categories of supervised learning techniques: classification and regression. We talk about classification if the predicted attribute is categoric and about regression if the attribute is numeric.</p><p>The unsupervised learning techniques identify patterns and structures of a set of objects. The two main branches of unsupervised learning are clustering and dimensionality reduction. The clustering <a id="id367" class="indexterm"/>techniques identify homogeneous groups of objects on the basis of their attributes and an example is k-means. Dimensionality reduction techniques identify a small set of significant features describing the objects and an example is the principal component analysis. The difference between clustering and dimensionality reduction depends on the identified attribute that is categoric or numeric respectively, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/7740OS_07_02.jpg" alt="Overview"/></div><p>This chapter will show you some popular techniques for each branch. In order to illustrate the techniques, we will <a id="id368" class="indexterm"/>reuse the flag dataset of <a class="link" href="ch04.html" title="Chapter 4. Step 1 – Data Exploration and Feature Engineering">Chapters 4</a>, <span class="emphasis"><em>Step 1 – Data Exploration and Feature Engineering</em></span>; <a class="link" href="ch05.html" title="Chapter 5. Step 2 – Applying Machine Learning Techniques">Chapter 5</a>, <span class="emphasis"><em>Step 2 – Applying Machine Learning Techniques</em></span>; and <a class="link" href="ch06.html" title="Chapter 6. Step 3 – Validating the Results">Chapter 6</a>, <span class="emphasis"><em>Step 3 – Validating the Results</em></span> that can be found in the supporting code bundle with this book.</p></div></div>
<div class="section" title="Supervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Supervised learning</h1></div></div></div><p>This chapter will show you <a id="id369" class="indexterm"/>some examples of popular supervised learning algorithms. These techniques are very useful for facing business problems because they make predictions about future attributes and outcomes. In addition, it is <a id="id370" class="indexterm"/>possible to measure the accuracy of each technique and/or parameter in order to choose the most suitable one and set it up in the best way.</p><p>As anticipated, there are two categories of techniques: classification and regression. However, most of the techniques can be used in both the contexts. Each of the following subsections introduces a different algorithm.</p><div class="section" title="The k-nearest neighbors algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec39"/>The k-nearest neighbors algorithm</h2></div></div></div><p>KNN is a <a id="id371" class="indexterm"/>supervised learning algorithm that performs classification or regression. Given a new object, the algorithm predicts its attribute starting from its <span class="emphasis"><em>k</em></span> neighbors that are its most similar objects. KNN is a lazy learning algorithm in the sense that it directly queries the training data to make a prediction.</p><p>In the <a id="id372" class="indexterm"/>case of a categoric attribute, the algorithm estimates it as the most common among the similar objects. In the case of a numeric attribute, it computes the median or average between them. In order to state which are the <span class="emphasis"><em>k</em></span> most <span class="emphasis"><em>similar</em></span> objects, KNN uses a similarity function that evaluates how similar two objects are. In order to measure similarity, the starting point is often a distance matrix expressing the dissimilarity. Then, the algorithm computes the similarity between the new object and each other and picks the <span class="emphasis"><em>k</em></span> most similar objects.</p><p>In our example, we will use the flag dataset and the features are the number of stripes and the number of colors in the flags. The attribute that we want to predict starting from its flag attributes is the language of a new country.</p><p>The training set is composed of some countries in such a way that there are no two countries with the same flag features. First, let's visualize the data. We can show the countries in a chart whose dimensions are the two features and whose color is the language, as follows:</p><div class="mediaobject"><img src="graphics/7740OS_07_03.jpg" alt="The k-nearest neighbors algorithm"/></div><p>We have <a id="id373" class="indexterm"/>two new countries with:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">7 stripes and 4 colors</li><li class="listitem" style="list-style-type: disc">3 stripes and 7 colors</li></ul></div><p>We <a id="id374" class="indexterm"/>want to determine the language of two new countries using a 4-nearest-neighbor algorithm. We can add the two countries to the chart and determine the 4 closest points for each of them, as shown here:</p><div class="mediaobject"><img src="graphics/7740OS_07_04.jpg" alt="The k-nearest neighbors algorithm"/></div><p>With regard to the country on the right-hand side of the chart, all its 4 closest neighbors belong to <span class="strong"><strong>Others</strong></span>, so we estimate that its language is <span class="strong"><strong>Others</strong></span>. The other country has a mixed neighborhood: 1 English, 1 Other Indo-European, and 2 Spanish countries. The most common language is Spanish, so we estimate that it is a Spanish-speaking country.</p><p>The KNN is a simple and scalable algorithm that achieves good results in many contexts. However, in the presence of many features, the similarity function takes account of all of them, including the less relevant, making it difficult to use the distance. In that context, the KNN is not able <a id="id375" class="indexterm"/>to identify the meaningful nearest neighbors and this issue is called the curse of dimensionality. A <a id="id376" class="indexterm"/>solution is to reduce the dimensionality by selecting the most relevant features or using a dimensionality reduction technique (this is the topic of the next section).</p></div><div class="section" title="Decision tree learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec40"/>Decision tree learning</h2></div></div></div><p>Decision tree <a id="id377" class="indexterm"/>learning is a supervised learning <a id="id378" class="indexterm"/>algorithm that builds a classification or regression tree. Each leaf of the tree represents the attribute estimation and each node splits the data accordingly with a condition of the features.</p><p>The decision tree learning is an eager method in the sense that it uses a training set to build a model that doesn't require you to query the data. All the other supervised learning techniques are eager as well.</p><p>The target of the algorithm is to define the most relevant feature and split the set in two groups accordingly with it. Then, for each group, the algorithm identifies its most relevant feature and divides the objects of the groups into two parts. This procedure goes on until we identify the leaves as small groups of objects. For each leaf, the algorithm estimates the feature as a mode, if it is categoric, or average, if it is numeric. After building the tree, if we have too many leaves, we can define a level in which we stop splitting the tree. In this way, each leaf will contain a reasonably big group. This procedure of stopping splitting is called pruning. In this way, we find a less complex and more accurate prediction.</p><p>In our example, we <a id="id379" class="indexterm"/>want to determine the <a id="id380" class="indexterm"/>language of a new country starting from different flag attributes, such as colors and patterns. The algorithm builds the tree learning from a training set. Let's visualize it:</p><div class="mediaobject"><img src="graphics/7740OS_07_05.jpg" alt="Decision tree learning"/></div><p>In any node, if the answer is <span class="strong"><strong>true</strong></span>, we go to the left, and if the answer is <span class="strong"><strong>false</strong></span>, we go to the right. First, the model identifies the most relevant attribute that is <span class="strong"><strong>saltires</strong></span>. If a flag contains a saltire, we go to the left and we determine that the related country is English. Otherwise, we go to the right and we check if the flag contains the blue color. Then, we go on checking the conditions until we reach a leaf.</p><p>Let's suppose that we built the tree without taking account of the Spanish flag. How do we estimate the language of Spain? Starting from the top, we check the conditions on each node we encounter.</p><p>These <a id="id381" class="indexterm"/>are the steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The flag doesn't contain a saltire, so we go to the left.</li><li class="listitem">The flag contains the blue color, so we go to the right.</li><li class="listitem">The flag doesn't contain a cross, so <code class="literal">crosses = no</code> is <code class="literal">true</code> and we go to the left.</li><li class="listitem">The flag doesn't contain an animated image, so we go to the right.</li><li class="listitem">The flag has two main colors, so <code class="literal">number of colors not equal to 4 or 5</code> is <code class="literal">true</code> and we go to the left.</li><li class="listitem">The flag doesn't contain any bars, so we go to the left.</li></ol></div><p>The flag doesn't have <a id="id382" class="indexterm"/>any vertical stripes, so <code class="literal">nStrp0 = no</code> is <code class="literal">true</code> and we go to the left, as shown here:</p><div class="mediaobject"><img src="graphics/7740OS_07_06.jpg" alt="Decision tree learning"/></div><p>In the end, the estimated language is <code class="literal">Spanish</code>.</p><p>The decision tree learning can deal with numeric and/or categoric features and attributes, so it can be applied in different contexts with just a little data preparation. In addition, it is applicable when there are many features, different from other algorithms. A disadvantage is that the algorithm can overfit in the sense that the model is too close to the data and is more <a id="id383" class="indexterm"/>complicated than the <a id="id384" class="indexterm"/>reality, although pruning can help with this.</p></div></div>
<div class="section" title="Linear regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Linear regression</h1></div></div></div><p>Linear regression is a <a id="id385" class="indexterm"/>statistical model identifying a relationship between numeric variables. Given a set of objects described by the <span class="emphasis"><em>y</em></span> attribute and the <code class="literal">x1, …,</code> and <code class="literal">xn</code> features, the model defines a relationship between the features and the attribute. The relationship is described by the linear function <span class="emphasis"><em>y = a0 + a1 * x1 + … + an * xn</em></span>, and <code class="literal">a0, …,</code> and <code class="literal">an</code> are parameters defined by the method in such a way that the relationship is as close as possible to the data.</p><p>In the case of machine learning, linear regression can be used to predict a numeric attribute. The algorithm learns from the training dataset to determine the parameters. Then, given a new object, the model inserts its features into the linear function to estimate the attribute.</p><p>In our example, we want to estimate the population of a country starting from its area. First, let's visualize the data about the area (in thousand km2) and the population (in millions), as shown in the following figure:</p><div class="mediaobject"><img src="graphics/7740OS_07_07.jpg" alt="Linear regression"/></div><p>Most of the countries have an area below 3000 thousand km2 and a population below 200 million and just a few countries have a much higher area and/or population. For this reason, most of the <a id="id386" class="indexterm"/>points are concentrated in the bottom-left area of the chart. In order to spread the points, we can transform the features using the logarithmic area and population, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/7740OS_07_08.jpg" alt="Linear regression"/></div><p>The target of linear regression is to identify a linear relationship that is as close to the data as possible. In <a id="id387" class="indexterm"/>our case, we have two dimensions, so we can visualize the relationship using a line. Given the area, the linear regression estimates that the population is on the line. Let's see it in the chart with the logarithmic features:</p><div class="mediaobject"><img src="graphics/7740OS_07_09.jpg" alt="Linear regression"/></div><p>Given a new country about which we know the area, we can estimate its population using the regression line. In the chart, there is a new country of which we know the area. The linear regression estimates that its point is on the red line.</p><p>Linear regression is a very simple and basic technique. The disadvantage is that it requires numeric features and attributes, so there are many contexts in which it is not applicable. However, it is possible to convert the categoric features into a numeric format using dummy variables or other techniques.</p><p>Another disadvantage is that the model makes strong assumptions on how the features and the attributes are <a id="id388" class="indexterm"/>related. The function estimating the output is linear, so in some contexts it might be far from the real relationship. In addition, if in reality the features interact with each other, the model is not able to keep track of the interaction. It's possible to solve this problem using a transformation that makes the relationship linear. It is also possible to define new features expressing non-linear interactions.</p><p>The linear regression is very basic and it is the starting point of some other techniques. For instance, the logistic regression predicts an attribute whose value is in the 0 to 1 range.</p></div>
<div class="section" title="Perceptron"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec40"/>Perceptron</h1></div></div></div><p>
<span class="strong"><strong>Artificial Neural Networks</strong></span> (<span class="strong"><strong>ANN</strong></span>) are the supervised learning techniques whose logic is similar to <a id="id389" class="indexterm"/>biological neural systems. A simple ANN technique is the single-layer perceptron and it is a classification technique estimating a binary attribute whose value can be 0 or 1. The perceptron works like a neuron in the sense that it sums the impact of all the inputs and outputs to 1 if the sum is above a <a id="id390" class="indexterm"/>defined threshold. The model is based on the following parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A weight for each feature, defining its impact</li><li class="listitem" style="list-style-type: disc">A threshold above which the estimated output is 1</li></ul></div><p>Starting from the features, the model estimates the attribute through these steps</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Compute the output through a linear regression: multiply each feature by its weight and sum all of them</li><li class="listitem" style="list-style-type: disc">Estimate the attribute to 1 if the output is above the threshold and to 0 otherwise</li></ul></div><p>The models are as shown in the following figure:</p><div class="mediaobject"><img src="graphics/7740OS_07_10.jpg" alt="Perceptron"/></div><p>In the beginning, the algorithm builds the perceptron with a defined set of coefficients and with a threshold. Then, the algorithm iteratively improves the coefficients using the training set. At each step, the algorithm estimates the attribute of each object. Then, the algorithm computes the difference between the real and the estimated attribute and uses the difference to modify the coefficients. In many situations, the algorithm does not reach a stable set of coefficients that are not modified, so we need to define at which point we stop. In the end, we have a perception defined by a set of coefficients and we can use it to estimate the attribute of new objects.</p><p>The perceptron is a simple example of a neural network and it allows us to easily understand the impact of the <a id="id391" class="indexterm"/>variables. However, the perceptron depends on a linear regression, so it is limited in the same way: the feature impact is linear <a id="id392" class="indexterm"/>and the features can't interact with each other.</p><div class="section" title="Ensembles"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec41"/>Ensembles</h2></div></div></div><p>Each algorithm <a id="id393" class="indexterm"/>has some weaknesses leading to incorrect results. What if we were able to solve the same problem using different algorithms and to pick the best result? If just a few algorithms commit the same mistake, we can just ignore them. It is not possible to determine which result is correct and which is not, but there is another option. By performing supervised learning on a new object, we can apply different algorithms and pick the most common or average result among them. In this way, if most of the algorithms identify the correct estimation, we will take it into account. The ensemble methods are based on this principle: they combine different classification or regression algorithms to increase their accuracy.</p><p>An ensemble method requires variability between the results coming from different algorithms <a id="id394" class="indexterm"/>and/or training datasets. Some options are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Changing the algorithm configuration</strong></span>: The algorithm is the same and its parameters vary within a range.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Changing the algorithm</strong></span>: We predict the attribute using different techniques. In addition, for each technique, we can use different configurations.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Using different data subsets</strong></span>: The algorithm is the same and every time it learns from a different random subset of the training data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Using different data samples (bagging)</strong></span>: The algorithm is the same and it learns from a bootstrap sample, that is, a set of objects picked randomly from the training dataset. The same object can be picked more than once.</li></ul></div><p>The final result combines the output of all the algorithms. In the case of classification, we use the mode, and in the case of regression, we use the average or median.</p><p>We can build an ensemble algorithm using any combination of supervised learning techniques, so there are several options. An example is a random forest that combines decision tree learning algorithms using bagging (the technique explained in the last bullet point in the previous list).</p><p>The ensemble methods often perform much better than the single algorithms. In the case of classification, the ensemble removes the biases affecting just a small part of the algorithms. However, the logic of different algorithms is often related and the same bias might be common. In this case, the ensemble keeps the bias.</p><p>The ensemble methods don't always work in the case of regression problems since the biases affect the final result. For instance, if there is just an algorithm computing a very biased result, the <a id="id395" class="indexterm"/>average will be highly affected by that. In this context, the median works better since it is much more stable and it is not affected by outliers.</p></div></div>
<div class="section" title="Unsupervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Unsupervised learning</h1></div></div></div><p>This chapter shows <a id="id396" class="indexterm"/>some unsupervised learning techniques. When facing a business problem, these techniques allow us to identify hidden structures and patterns and perform exploratory data analysis. In addition, unsupervised learning can <a id="id397" class="indexterm"/>simplify the problem, allowing us to build more accurate and less elaborated solutions. These techniques can also be used in the solution of the problem itself.</p><p>The two branches of techniques are clustering and dimensionality reduction and most of them are not applicable in both the contexts. This chapter shows some popular techniques.</p><div class="section" title="k-means"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec42"/>k-means</h2></div></div></div><p>k-means is a centroid-based clustering technique. Given a set of objects, the algorithm identifies <span class="emphasis"><em>k</em></span> <a id="id398" class="indexterm"/>homogeneous clusters. k-means is centroid-based in the sense that each cluster is defined by its centroid representing its average object.</p><p>The target <a id="id399" class="indexterm"/>of the algorithm is to identify <span class="emphasis"><em>k</em></span> centroids. Then, the k-means associates each object to the closest centroid, defining <span class="emphasis"><em>k</em></span> clusters. The algorithm starts with a random set of centroids and it iteratively changes them, improving the clustering.</p><p>In our example, the data is about the country flags and the two features are the number of stripes and the number of colors. We select a subset of the countries in such a way that there are no two flags with the same value of the attributes. Our target is to identify two homogeneous groups of countries. The first step of the k-means is identifying two random centroids. Let's visualize the data and the centroids in a chart:</p><div class="mediaobject"><img src="graphics/7740OS_07_11.jpg" alt="k-means"/></div><p>The <span class="strong"><strong>o</strong></span> represents the country flags and the <span class="strong"><strong>x</strong></span> represents the centroids. Before running k-means, we need to define a distance that is a way of determining dissimilarity between objects. For instance, in the preceding chart, we can use the Euclidean distance that expresses the length of the line connecting two points. The algorithm is iterative and each step consists of the <a id="id400" class="indexterm"/>following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For each point, determine the centroid whose distance is the minimum. Then, assign the point to the cluster related to the closest centroid.</li><li class="listitem">Recompute the centroid of each cluster in such a way that it is the average between its objects.</li></ol></div><p>In the end, we <a id="id401" class="indexterm"/>have two clusters with the related centroids representing average objects. Let's visualize them, as shown here:</p><div class="mediaobject"><img src="graphics/7740OS_07_12.jpg" alt="k-means"/></div><p>The colors represent the clusters and the black <span class="strong"><strong>x</strong></span> represents the final centroids.</p><p>k-means is one of the most popular clustering techniques because it is easy to understand and it doesn't require a lot of computational power. However, the algorithm has some limitations. It contains a random component, so if we run it twice on the same set of data it will probably <a id="id402" class="indexterm"/>identify different clusters. Another disadvantage is that it is not able to identify the clusters in some specific contexts, for <a id="id403" class="indexterm"/>instance, when the clusters have different sizes or elaborated shapes. k-means is a very simple and basic algorithm and it is the starting point to some more elaborate techniques.</p></div><div class="section" title="Hierarchical clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec43"/>Hierarchical clustering</h2></div></div></div><p>Hierarchical clustering <a id="id404" class="indexterm"/>is a branch of clustering techniques. Starting from a set of objects, the target builds a hierarchy of clusters. In <a id="id405" class="indexterm"/>agglomerative hierarchical clustering, each object belongs to a different cluster in the beginning. Then, the algorithm merges the clusters until there is one cluster containing all the objects. After having identified the hierarchy, we can define the clusters and stop their merging at any point.</p><p>During each agglomeration step, the algorithm merges the two most similar clusters and there are some parameters defining the similarity. First, we need to define a way to measure how similar two objects are. There are several options, depending on the situation. Then, we need to define the similarity between clusters; the methods are called <span class="strong"><strong>linkage</strong></span>. In <a id="id406" class="indexterm"/>order to measure the similarity, we start defining a distance function that is the opposite. To determine the distance between cluster1 and cluster2, we measure the distance between every possible object of cluster1 and every object of cluster2. Some options to measure the distance between the two clusters are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Single linkage</strong></span>: This <a id="id407" class="indexterm"/>is the minimum distance</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Complete</strong></span><span class="strong"><strong> linkage</strong></span>: This <a id="id408" class="indexterm"/>is the maximum distance</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Average</strong></span><span class="strong"><strong> linkage</strong></span>: This <a id="id409" class="indexterm"/>is the average distance</li></ul></div><p>Depending on the linkage, the results of the algorithms will be different.</p><p>The example uses the same data as k-means. The country flags are represented by the number of stripes and colors and we want to identify homogeneous groups. The distance that we use is the <a id="id410" class="indexterm"/>Euclidean (just the distance between two points) and the linkage is complete. First, let's identify the clusters from <a id="id411" class="indexterm"/>their hierarchy, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/7740OS_07_13.jpg" alt="Hierarchical clustering"/></div><p>The chart is called a <span class="strong"><strong>dendrogram</strong></span> <a id="id412" class="indexterm"/>and at the bottom of the chart every object belongs to a different cluster. Then, going up, we merge the clusters until all the objects belong to the same cluster. The height is the distance at which the algorithm merges the clusters. For instance, at a height of 3, all the clusters whose distance is below 3 are already merged.</p><p>The red line is at a height of 6 and it defines when we stop merging and below it the objects are divided in <a id="id413" class="indexterm"/>4 clusters. Now we can <a id="id414" class="indexterm"/>visualize the clusters in a chart as follows:</p><div class="mediaobject"><img src="graphics/7740OS_07_14.jpg" alt="Hierarchical clustering"/></div><p>The colors of the points represent the clusters. The algorithm has correctly identified the group on the right and has split the group on the left in three parts in a good way.</p><p>There are different options for the hierarchic cluster and some of them produce very good results in some contexts. Different from the k-means, the algorithm is deterministic, so it always leads to the same result.</p><p>A big disadvantage of hierarchic cluster is the computational time (<code class="literal">O(n3)</code>) that makes it impossible to apply it on large datasets. Another lack is the manual component to choose the algorithm configuration and the dendrogram cut. In order to identify a good solution, we usually <a id="id415" class="indexterm"/>need to run the algorithm with different configurations and to visualize the dendrogram to define its cut.</p></div><div class="section" title="PCA"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec44"/>PCA</h2></div></div></div><p>
<span class="strong"><strong>Principal Components Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) is a statistical procedure transforming the features. The PCA logic is based on the concepts of linear correlation and variance. In a machine learning context, the PCA is a dimensionality reduction technique.</p><p>Starting <a id="id416" class="indexterm"/>with the features describing a set of objects, the target defines other variables that are linearly uncorrelated with each other. The output is a new set of variables defined as linear combinations <a id="id417" class="indexterm"/>of the initial features. In addition, the new variables are ranked on the basis of their relevance. The number of the new variables is less than or equal to the initial number of features and it is possible to select the most relevant features. Then, we are able to define a smaller set of features, reducing the problem dimension.</p><p>The algorithm starts defining the feature combination with the highest variance. Then, at each step, it iteratively defines another feature combination maximizing the variance, under the condition that the new combination is not linearly correlated with the others.</p><p>In the example of <a class="link" href="ch04.html" title="Chapter 4. Step 1 – Data Exploration and Feature Engineering">Chapter 4</a>, <span class="emphasis"><em>Step 1 – Data Exploration and Feature Engineering</em></span>, <a class="link" href="ch05.html" title="Chapter 5. Step 2 – Applying Machine Learning Techniques">Chapter 5</a>, <span class="emphasis"><em>Step 2 – Applying Machine Learning Techniques</em></span>, and <a class="link" href="ch06.html" title="Chapter 6. Step 3 – Validating the Results">Chapter 6</a>, <span class="emphasis"><em>Step 3 – Validating the Results</em></span>, we have defined 37 attributes describing each country flag. Applying the PCA, we can define 37 new attributes defined as linear combination of the variables. The attributes are ranked by relevance, so we can select the top six and in this way have a small table describing the flag. In this way, we are able to build a supervised learning model estimating the language on the basis of six relevant features.</p><p>In the presence of a lot of features, the PCA allows us to define a smaller set of relevant variables. However, this technique is not applicable in all the contexts. A lack is that the result depends on how the features are scaled, so it is necessary to standardize the variables first.</p><p>Dealing with a supervised learning problem, we can use the PCA to reduce its dimensionality. However, the PCA only takes into account the features, ignoring how they are related with the attribute to predict, so it might select feature combinations that are not very relevant to the problem.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec42"/>Summary</h1></div></div></div><p>In this chapter, we learned about the main branches of machine learning techniques: supervised and unsupervised learning. We saw how to estimate a numeric or categoric attribute using supervised learning techniques such as KNN, decision tree, linear regression, and neural networks. We saw that it is possible to increase performance using ensembles that are techniques combining different supervised learning algorithms. We learned how to identify homogeneous groups using clustering techniques such as k-means and hierarchic clustering. We have also understood the importance of dimensionality reduction techniques such as the PCA to transform the features defining a smaller set of variables.</p><p>The next chapter shows an example of a business problem that can be faced using machine learning techniques. We will also see examples of both supervised and unsupervised learning techniques.</p></div></body></html>