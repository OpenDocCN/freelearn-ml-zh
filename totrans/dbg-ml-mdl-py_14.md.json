["```py\ntransform = transforms.Compose([    transforms.Resize((32, 32)),\n    transforms.ToTensor()\n])\ntrain_dataset = datasets.Flowers102(root='./data',\n    download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32,\n    shuffle=True)\n```", "```py\nclass VAE(nn.Module):    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(32 * 32 * 3, 512),\n            nn.ReLU(),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n        )self.fc_mean = nn.Linear(128, 32)\n         self.fc_var = nn.Linear(128, 32)\n         self.decoder = nn.Sequential(\n             nn.Linear(32, 128),\n             nn.ReLU(),\n             nn.Linear(128, 512),\n             nn.ReLU(),\n             nn.Linear(512, 32 * 32 * 3),\n             nn.Sigmoid(),\n         )\n    def forward(self, x):\n        h = self.encoder(x.view(-1, 32 * 32 * 3))\n        mean, logvar = self.fc_mean(h), self.fc_var(h)\n        std = torch.exp(0.5*logvar)\n        q = torch.distributions.Normal(mean, std)\n        z = q.rsample()\n        return self.decoder(z), mean, logvar\n```", "```py\nmodel = VAE()optimizer = optim.Adam(model.parameters(), lr=2e-3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n```", "```py\ndef loss_function(recon_x, x, mu, logvar):    BCE = nn.functional.binary_cross_entropy(recon_x,\n        x.view(-1, 32 * 32 * 3), reduction='sum')\n    KLD = -0.5 * torch.sum(\n        1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD\n```", "```py\nn_epoch = 400for epoch in range(n_epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mean, logvar = model(data)\n        loss = loss_function(recon_batch, data, mean,\n            logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n    print(f'Epoch: {epoch} Average loss: {\n        train_loss / len(train_loader.dataset):.4f}')\n```", "```py\nimport torchfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n# Pretrain a GPT-2 language model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n```", "```py\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n```", "```py\nclass Reward_Model(torch.nn.Module):    def __init__(self, input_size, hidden_size, output_size):\n        super(RewardModel, self).__init__()\n        self.fc_layer1 = torch.nn.Linear(input_size,\n            hidden_size)\n        self.fc_layer2 = torch.nn.Linear(hidden_size,\n            output_size)\n    def forward(self, x):\n        x = torch.relu(self.fc_layer1(x))\n        x = self.fc_layer2(x)\n        return x\n```", "```py\nreward_model = Reward_Model(input_size, hidden_size, output_size)\n```", "```py\nfor epoch in range(n_epochs):    for batch in dataloader:\n        input_ids = tokenizer.encode(batch['input'],\n            return_tensors='pt')\n        output_ids = tokenizer.encode(batch['output'],\n            return_tensors='pt')\n        reward = reward_model(batch['input'])\n        loss = model(input_ids, labels=output_ids).loss * reward\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n```", "```py\nclass Conv_AE(nn.Module):    def __init__(self):\n        super(Conv_AE, self).__init__()\n        # Encoding data\n        self.encoding_conv1 = nn.Conv2d(3, 8, 3, padding=1)\n        self.encoding_conv2 = nn.Conv2d(8, 32, 3,padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        # Decoding data\n        self.decoding_conv1 = nn.ConvTranspose2d(32, 8, 2,\n            stride=2)\n        self.decoding_conv2 = nn.ConvTranspose2d(8, 3, 2,\n            stride=2)\n    def forward(self, x):\n        # Encoding data\n        x = torch.relu(self.encoding_conv1(x))\n        x = self.pool(x)\n        x = torch.relu(self.encoding_conv2(x))\n        x = self.pool(x)\n        # Decoding data\n        x = torch.relu(self.decoding_conv1(x))\n        x = self.decoding_conv2(x)\n        x = torch.sigmoid(x)\n        return x\n```", "```py\nmodel = Conv_AE().to(device)criterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```", "```py\ndef create_mask(size=(32, 32), mask_size=8):    mask = np.ones((3, size[0], size[1]), dtype=np.float32)\n    height, width = size\n    m_height, m_width = mask_size, mask_size\n    top = np.random.randint(0, height - m_height)\n    left = np.random.randint(0, width - m_width)\n    mask[:, top:top+m_height, left:left+m_width] = 0\n    return torch.from_numpy(mask)\n```", "```py\nn_epoch = 200for epoch in range(n_epoch):\n    for data in train_loader:\n        img, _ = data\n        # Creating mask for small part in training images\n        mask = create_mask().to(device)\n        img_masked = img * mask\n        img = img.to(device)\n        img_masked = img_masked.to(device)\n        optimizer.zero_grad()\n        outputs = model(img_masked)\n        loss = criterion(outputs, img)\n        loss.backward()\n        optimizer.step()\n```"]