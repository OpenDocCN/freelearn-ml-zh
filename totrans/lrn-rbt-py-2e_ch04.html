<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Simulating a Differential Drive Robot Using ROS</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Simulating a Differential Drive Robot Using ROS</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we looked at how to model Chefbot. In this chapter, we are going to learn how to simulate the robot using the Gazebo simulator in ROS. We will learn how to create a simulation model of Chefbot, and we will create a hotel-like environment in Gazebo to test our application, which is programmed to automatically deliver food to customers. We will look at a detailed explanation of each of the steps to test out our application. The following are the important topics that we are going to cover in this chapter:</p>
<ul>
<li>Getting started with the Gazebo simulator</li>
<li>Working with the TurtleBot 2 simulation</li>
<li>Creating a simulation of Chefbot</li>
<li>URDF tags and plugins for simulations</li>
<li>Getting started with simultaneous localization and mapping</li>
<li>Implementing SLAM in a Gazebo environment</li>
<li>Creating a map using SLAM</li>
<li>Getting started with adaptive Monte Carlo localization</li>
<li>Implementing AMCL in a Gazebo environment</li>
<li>Autonomous navigation of Chefbot in a hotel using Gazebo</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To test the application and codes in this chapter, you need an Ubuntu 16.04 LTS PC/laptop with ROS Kinetic installed.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with the Gazebo simulator</h1>
                </header>
            
            <article>
                
<p>In the first chapter, we looked at the basic concepts of the Gazebo simulator and its installation procedures. In this chapter, we will learn more about the usage of Gazebo and how to simulate a differential drive robot in the Gazebo simulator. The first step is to understand the GUI interfaces and its various controls. As we have discussed in the first chapter, Gazebo has two main sections. The first is the Gazebo server and the second is the Gazebo client. The simulation is done on the Gazebo server, which acts as a backend. The GUI is the frontend, which acts as the Gazebo client. We will also look at Rviz (ROS Visualizer), which is a GUI tool in ROS that is used to visualize different kinds of robot sensor data from robot hardware or a simulator, such as Gazebo.</p>
<p>We can use Gazebo as an independent simulator to simulate the robot, or we can use interfaces with ROS and Python that can be used to program robots in the Gazebo simulator. If we are using Gazebo as an independent simulator, the default option to simulate the robot is by writing C++-based plugins (<a href="http://gazebosim.org/tutorials/?tut=plugins_hello_world"><span class="URLPACKT">http://gazebosim.org/tutorials/?tut=plugins_hello_world</span></a>). We can write C++ plugins for simulating a robot's behavior, creating new sensors, creating a new world, and so on. By default, the modeling of robots and environments in Gazebo is done using the SDF (<a href="http://sdformat.org/">http://sdformat.org/</a>) file. If we are using an ROS interface for Gazebo, we have to create a URDF file that contains all the parameters of the robot and has Gazebo-specific tags to mention the simulation properties of the robot. When we start the simulation using URDF, it will convert to an SDF file using some tools and display the robot in Gazebo. The ROS interface of Gazebo is called gazebo-ros-pkgs. It is a set of wrappers and plugins that have the ability to model a sensor, robot controller, and other simulations in Gazebo and communicate over ROS topics. In this chapter, we will be mainly focusing on the ROS-Gazebo interface for simulating Chefbot. The advantage of the ROS-Gazebo interface is that we can program the robot by making use of the ROS framework. We can program the robot using popular programming languages such as C++ and Python using ROS.</p>
<p>If you are not interested in using ROS and want to program the robot using Python, you should check out an interface called pygazebo (<a href="https://github.com/jpieper/pygazebo"><span class="URLPACKT">https://github.com/jpieper/pygazebo</span></a>). It is a Python binding of Gazebo. In the next section, we will see the GUI of Gazebo, along with some of its important controls.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Gazebo's graphical user interface</h1>
                </header>
            
            <article>
                
<p>We can start Gazebo in several ways. You have already seen this in <em>Chapter 1, Getting Started with Robot Operating System</em>. In this chapter, we are using the following command to start an empty world, meaning that there is no robot and no environment:</p>
<pre>$ roslaunch gazebo_ros empty_world.launch </pre>
<p>The preceding command will start the Gazebo server and client and load an empty world into Gazebo. Here is the view of the empty world in Gazebo:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/a8b1fed3-5755-414e-9712-211259906122.png" style="width:57.67em;height:29.17em;"/></div>
<div class="packt_figure">
<div class="CDPAlignCenter CDPAlign packt_figref">Gazebo user interface</div>
</div>
<p>The Gazebo user interface can be divided into three sections: <strong>Scene</strong>, <strong>Left Panel</strong>, and the <strong>Right Panel</strong>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Scene</h1>
                </header>
            
            <article>
                
<p>The Scene is the place where the simulation of the robot takes place. We can add various objects to the scene, and we can interact with the robot in the scene using the mouse and keyboard.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Left Panel</h1>
                </header>
            
            <article>
                
<p>You can see the left&#160;Panel when we launch Gazebo. There are three main tabs in the Left Panel:</p>
<ul>
<li><strong>World</strong>: The <strong>World</strong> tab contains a list of models in the current Gazebo Scene. Here, we can modify model parameters, such as the pose, and can also change the camera's pose.</li>
<li><strong>Insert</strong>: The <strong>Insert</strong> tab allows you to add a new simulation model to the Scene. The models are available in the local system, as well as the remote server. The <kbd>/home/&lt;user_name&gt;/.gazebo/model</kbd> folder will keep the local model files and models in the remote server in <a href="http://gazebosim.org/models">http://gazebosim.org/models</a>, as shown in the following screenshot:</li>
</ul>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="images/122b4b47-772b-4175-b19b-87c54b5cc0aa.png" style="width:15.17em;height:26.33em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The Insert tab in the left panel of Gazebo</div>
<p>You can see both the local files and remote files in the <strong>Insert</strong> tab that is shown in the preceding screenshot.</p>
<div class="packt_infobox">When you start Gazebo for the first time, or when you start a world that has models from the remote server, you may see a black screen on Gazebo or a warning on the terminal. This is because the model in the remote server is being downloaded and Gazebo has to wait a while. The waiting time can vary according to the speed of your internet connection. Once the model is downloaded, it will be kept in the local model folder, so there will not be any delay the next time.</div>
<ul>
<li><strong>Layers</strong>: Most of the time, we will not use this tab. This tab is for organizing the different visualizations available in the simulation. We can hide/unhide the models in the simulation by toggling each layer. Most of the time in the simulation, this tab will be empty.</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Right Panel</h1>
                </header>
            
            <article>
                
<p>The Right panel is hidden by default. We have to drag it in order to view it. This panel enables us to interact with the mobile parts of the model. We can see the joints of the model if we select the model in the scene.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gazebo toolbars</h1>
                </header>
            
            <article>
                
<p>The Gazebo has two toolbars. One is above the Scene and one is below it.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Upper toolbar</h1>
                </header>
            
            <article>
                
<p>he upper toolbar is very useful for interacting with the Gazebo Scene. This toolbar is mainly for manipulating the Gazebo Scene. It has functions to select the model, scale it, translate and rotate it, and add new shapes to the scene:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/47124ce9-d9b2-49fa-a1aa-44d43dc35a2c.png" style="width:54.92em;height:10.08em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Upper toolbar of Gazebo</div>
<p>The following list shows you detailed descriptions of each option:</p>
<ul>
<li><strong>Select Mode</strong>: If we are in Select Mode, we can select the models in the Scene and set their properties, as well as navigate inside the Scene.</li>
<li><strong>Translate Mode</strong>: In Translate Mode, we can select a model and translate it model by clicking the Left button.</li>
<li><strong>Rotate Mode</strong>: In Rotate Mode, we can select the model and change its orientation.</li>
<li><strong>Scale Mode</strong>: In Scale Mode, we can select the model and scale it.</li>
<li><strong>Undo/Redo</strong>: This enables us to undo or redo actions in the Scene.</li>
<li><strong>Simple Shapes</strong>: With this option, we can insert primitive shapes into the scene, such as a cylinder, cube, or sphere.</li>
<li><strong>Lights</strong>: The Lights option enables us to add different kinds of light sources into the Scene.</li>
<li><strong>Copy/Paste</strong>: The Copy and Paste options enable us to copy and paste different models and parts of the Scene.</li>
<li><strong>Align</strong>: This enables us to align models to one another.</li>
<li><strong>Snap</strong>: This snaps one model and moves it inside the Scene.</li>
<li><strong>Change view</strong>: This changes the view of the Scene. It mainly uses the perspective view and orthogonal view.</li>
<li><strong>Screenshot:</strong> This takes a screenshot of the current Scene.</li>
<li><strong>Record Log:</strong> This saves Gazebo's logs.</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bottom toolbar</h1>
                </header>
            
            <article>
                
<p>The bottom toolbar mainly gives us an idea about the simulation. It displays the Simulation Time, which refers to the time that is passing within the simulator. The simulation can sped up or slowed down. This depends on the computation required for the current simulation.</p>
<p>The Real Time display refers to the actual time passing in real life when the simulator is running. The <strong>real time factor</strong> (<strong>RTF</strong>) is the ratio between simulation time and the speed of real time. If the RTF is one, it means that the simulation is happening at a rate identical to the speed of time in reality.</p>
<p>The state of the world in Gazebo can change with each iteration. Each iteration can make changes in Gazebo for a fixed amount of time. That fixed time is called the step size. By default, the step size is 1 millisecond. The step size and iteration are shown in the tool bar, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/e829777b-5166-47cc-8597-0b4bae78a4b3.png" style="width:61.25em;height:10.33em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Lower toolbar of Gazebo</div>
<p>We can pause the simulation and see each step using the <strong>Step</strong> button.</p>
<div class="packt_infobox">You can get more information about the Gazebo GUI from <a href="http://gazebosim.org/tutorials?cat=guided_b&amp;tut=guided_b2"><span class="URLPACKT">http://gazebosim.org/tutorials?cat=guided_b&amp;amp;tut=guided_b2</span></a>.</div>
<p>Before going to the next section, you can play with Gazebo and learn more about how it works.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with a TurtleBot 2 simulation</h1>
                </header>
            
            <article>
                
<p>After working with Gazebo, now it's time to run a simulation on it and work with some robots. One of the popular robots available for education and research is TurtleBot. The TurtleBot software was developed within the ROS framework, and there is a good simulation of its operations available in Gazebo. The popular versions of TurtleBot are TurtleBot 2 and 3. We will learn about TurtleBot 2 in this section because our development of Chefbot was inspired by its design.</p>
<p>Installing TurtleBot 2 simulation packages in Ubuntu 16.04 is straightforward. You can use the following command to install TurtleBot 2 simulation packages for Gazebo:</p>
<pre>    <strong>$ sudo apt-get install ros-kinetic-turtlebot-gazebo</strong>  </pre>
<p>After installing the packages, we can start running the simulation. There are several launch files inside the turtlebot-gazebo packages that have different world files. A Gazebo world file (<kbd>*.world</kbd>) is an SDF file consisting of the properties of the models in the environment. When the world file changes, Gazebo will load with a different environment.</p>
<p>The following command will start a world that has a certain set of components:</p>
<pre>    <strong>$ roslaunch turtlebot_gazebo turtlebot_world.launch</strong>  </pre>
<p>The simulation will take some time to load, and when it loads, you will see the following models in the Gazebo Scene:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/92bc12f5-0ac0-42ad-8efd-3fb132a0e954.png" style="width:58.67em;height:28.75em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">TurtleBot 2 simulation in Gazebo</div>
<p>When we load the simulation in Gazebo, it will also load the necessary plugins to interact with ROS. TurtleBot 2 has the following important components:</p>
<ul>
<li>A mobile base with a differential drive</li>
<li>A depth sensor for creating a map</li>
<li>A bumper switch to detect collision</li>
</ul>
<p>When the simulation loads, it will load the ROS-Gazebo plugins to simulate a differential drive mobile base, depth sensor (Kinect or Astra), and plugins for bumper switches. So, after loading the simulation, if we enter a <kbd>$ rostopic list</kbd> command in the terminal, a selection of topics will appear as shown in the following screenshot.</p>
<p>As we saw earlier, we can see the topics from the differential drive plugin, depth sensor, and bumper switches. In addition to these, we can see the topics from the ROS-Gazebo plugins that mainly contain the current state of the robot and other models in the simulation.</p>
<p>The Kinect/Astra sensors can give an RGB image and depth image. The differential drive plugin can send the odometry data of the robot in the <kbd>/odom</kbd> (<kbd>nav_msgs/Odometry</kbd>) topic and can publish the robot's transformation in the <kbd>/tf</kbd> (<kbd>tf2_msgs/TFMessage</kbd>) topics, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/eac279c0-935f-46f8-b44c-d60a905d751c.png" style="width:42.50em;height:47.00em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">ROS topics from the TurtleBot 2 simulation</div>
<p>We can visualize the robot model and sensor data in Rviz. There is a TurtleBot package dedicated for visualization. You can install the following package to visualize the robot data:</p>
<pre>    <strong>$ sudo apt-get install ros-kinetic-turtlebot-rviz-launchers</strong>  </pre>
<p>After installing this package, we can use the following launch file to visualize the robot and its sensor data:</p>
<pre>    <strong>$ roslaunch turtlebot-rviz-launchers view_robot.launch</strong>  </pre>
<p>We will get the following Rviz window with the robot model displayed in it. We can then enable the sensor displays to visualize this particular data, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/4c288263-0b1a-45b7-b09c-593ed6df1ead.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">TurtleBot 2 visualization in Rviz</div>
<p>In the next section, we will learn how to move this robot.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Moving the robot</h1>
                </header>
            
            <article>
                
<p>The differential drive plugin of the robot is capable of receiving ROS Twist messages (<kbd>geometry_msgs/Twist</kbd>), which consist of the current linear and angular velocities of the robot. The teleoperation of the robot means moving the robot manually using a joy stick or keyboard by using ROS Twist messages. We will now look at how to move the Turtlebot 2 robot using keyboard teleoperation.</p>
<p>We have to install a package to teleoperate the TurtleBot 2 robot. The following command will install the TurtleBot teleoperation package:</p>
<pre>    <strong>$ sudo apt-get install ros-kinetic-turtlebot-teleop</strong>  </pre>
<p>To start teleoperation, we have to start the Gazebo simulator first, and then start the teleoperation node using the following command:</p>
<pre>    <strong>$ roslaunch turtlebot_teleop keyboard_teleop.launch </strong>
  </pre>
<p>In the terminal, we can see the key combination for moving the robot. You can move it using those keys, and you will see the robot moving in Gazebo and Rviz, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/45b86691-5f61-415d-aff2-573de750e345.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">TurtleBot 2 keyboard teleoperation</div>
<p>When we press the buttons on the keyboard, it will send a Twist message to the differential drive controller, and the controller will move the robot in the simulation. The teleop node sends a topic called <kbd>/cmd_vel_mux/input/teleop</kbd> (<kbd>geometry_msgs/Twist</kbd>), which is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/8412f9ba-3fa4-4742-8035-cbcab5be31bb.png" style="width:31.67em;height:5.75em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The TurtleBot keyboard teleoperation node</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a simulation of Chefbot</h1>
                </header>
            
            <article>
                
<p>We have seen how the turtlebot simulation works. In this section, we will be looking at how to create our own robot simulation using Gazebo.</p>
<p>Before we start discussing this subject, you should copy the <kbd>chefbot_gazebo</kbd> package to your catkin workspace and enter <kbd>catkin_make</kbd> to build the package. Make sure you have two packages in your workspace, one called <kbd>chefbot_description</kbd> and the other called <kbd>chefbot_gazebo</kbd>. The <kbd>chefbot_gazebo</kbd> package contains a simulation-related launch file and parameters, and <kbd>chefbot_description</kbd> has the robot's URDF model, along with its simulation parameters, and the launch file that is used to view the robot in Rviz and Gazebo.</p>
<p>Let's begin creating our Chefbot model in Gazebo so that you can familiarize yourself with the procedure. After that, we will dig deep into the xacro file and can look at the simulation parameters.</p>
<p>The following launch file will show the robot model in Gazebo with an empty world and start all the Gazebo plugins for the robot:</p>
<pre>    <strong>$ roslaunch chefbot_description view_robot_gazebo.launch</strong>  </pre>
<p>The following figure&#160;shows a screenshot of the Chefbot in Gazebo:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/4b2a0759-4163-4c93-8715-ff1030f0b0de.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The Chefbot in Gazebo</div>
<p>Let's see how we can add a URDF robot model in Gazebo. You can find the definition of the URDF robot model at <kbd>chefbot_description/launch/view_robot_gazebo.launch</kbd>.</p>
<p>The first section of the code calls the <kbd>upload_model.launch</kbd> file for creating the <kbd>robot_description</kbd> parameter. If it is successful, then it will start an empty world in Gazebo:</p>
<pre>&lt;launch&gt; 
  &lt;include file="$(find chefbot_description)/launch/upload_model.launch" /&gt; 
 
  &lt;include file="$(find gazebo_ros)/launch/empty_world.launch"&gt; 
    &lt;arg name="paused" value="false"/&gt; 
    &lt;arg name="use_sim_time" value="true"/&gt; 
    &lt;arg name="gui" value="true"/&gt; 
    &lt;arg name="recording" value="false"/&gt; 
    &lt;arg name="debug" value="false"/&gt; 
  &lt;/include&gt; </pre>
<p>So how does the robot model in the <kbd>robot_description</kbd> parameter show in Gazebo? The following code snippet in the launch file does that job:</p>
<pre>  &lt;node name="spawn_urdf" pkg="gazebo_ros" type="spawn_model" args="-param robot_description -urdf -z 0.1 -model chefbot" /&gt; </pre>
<p>The node called <kbd>spawn_model</kbd> inside the <kbd>gazebo_ros</kbd> package will read the <kbd>robot_description</kbd> and spawn the model in Gazebo. The <kbd>-z 0.1</kbd> argument indicates the height of the model to be placed in Gazebo. If the height is 0.1, the model will be spawned at a height of 0.1. If gravity is enabled, then the model will fall to the ground. We can change this parameter according to our requirement. The <kbd>-model</kbd> argument is the name of the robot model in Gazebo. This node will parse all the Gazebo parameters from the <kbd>robot_description</kbd> and start the simulation in Gazebo.</p>
<p>After spawning the model, we can publish the robot transformation (tf) using the following lines of code:</p>
<pre>  &lt;node pkg="robot_state_publisher" type="robot_state_publisher" name="robot_state_publisher"&gt; 
    &lt;param name="publish_frequency" type="double" value="30.0" /&gt; 
  &lt;/node&gt; </pre>
<p>We are publishing the ROS tf at 30 Hz.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Depth image to laser scan conversion</h1>
                </header>
            
            <article>
                
<p>The depth sensor on the robot provides the 3D coordinates of the environment. To achieve autonomous navigation, we can use this data to create a 3D map. There are different techniques for creating a map of the environment. One of the algorithms that we are using for this robot is called gmapping (<a href="http://wiki.ros.org/gmapping">http://wiki.ros.org/gmapping</a>). The gmapping algorithm mainly use a laser scan for creating the map, but in our case, we get an entire 3D point cloud from the sensor. We can convert the 3D depth data from a laser scan by taking a slice of the depth data. The following nodelet (<a href="http://wiki.ros.org/nodelet"><span class="URLPACKT">http://wiki.ros.org/nodelet</span></a>) in this launch file is able to receive the depth data and convert it to laser scan data:</p>
<pre>  &lt;node pkg="nodelet" type="nodelet" name="laserscan_nodelet_manager" args="manager"/&gt; 
  &lt;node pkg="nodelet" type="nodelet" name="depthimage_to_laserscan" 
        args="load depthimage_to_laserscan/DepthImageToLaserScanNodelet laserscan_nodelet_manager"&gt; 
    &lt;param name="scan_height" value="10"/&gt; 
    &lt;param name="output_frame_id" value="/camera_depth_frame"/&gt; 
    &lt;param name="range_min" value="0.45"/&gt; 
    &lt;remap from="image" to="/camera/depth/image_raw"/&gt; 
    &lt;remap from="scan" to="/scan"/&gt; 
  &lt;/node&gt; 
&lt;/launch&gt; </pre>
<p>The nodelet is a special kind of ROS node that has a property called zero copy transport, meaning that it doesn't take network bandwidth to subscribe to a topic. This will make the conversion from the depth image (<kbd>sensor_msgs/Image</kbd>) to the laser scan (<kbd>sensor_msgs/LaserScan</kbd>) faster and more efficient. One of the other properties of the nodelet is that it can be dynamically loaded as plugins. We can set various properties of this nodelet, such as the <kbd>range_min</kbd>, name of the image topic, and the output laser topic.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">URDF tags and plugins for Gazebo simulation</h1>
                </header>
            
            <article>
                
<p>We have seen the simulated robot in Gazebo. Now, we will look in more detail at the simulation-related tags in URDF and the various plugins we have included in the URDF model.</p>
<p>Most of the Gazebo-specific tags are in the <kbd>chefbot_description/gazebo/chefbot.gazebo.xacro</kbd> file. Also, some of the tags in <kbd>chefbot_description/urdf/chefbot.xacro</kbd> are used in the simulation. Defining the <kbd>&lt;collision&gt;</kbd> and <kbd>&lt;inertial&gt;</kbd> tags in chefbot.xacro is very important for our simulation. The <kbd>&lt;collision&gt;</kbd> tag in URDF defines a boundary around the robot link, which is mainly used to detect the collision of that particular link, whereas the <kbd>&lt;inertial&gt;</kbd> tag encompasses the mass of the link and the moment of inertia. Here is an example of the <kbd>&lt;inertial&gt;</kbd> tag definition:</p>
<pre>      &lt;inertial&gt; 
        &lt;mass value="0.564" /&gt; 
        &lt;origin xyz="0 0 0" /&gt; 
        &lt;inertia ixx="0.003881243" ixy="0.0" ixz="0.0" 
                 iyy="0.000498940" iyz="0.0" 
                 izz="0.003879257" /&gt; 
      &lt;/inertial&gt; </pre>
<p>These parameters are part of the robot's dynamics, so in the simulation these values will have an effect on the robot model. Also, in the simulation, it will process all the links and joints, as well as its properties.</p>
<p>Next, we will look at the tags inside the <kbd>gazebo/chefbot.gazebo.xacro</kbd> file. The important Gazebo-specific tag we are using is <kbd>&lt;gazebo&gt;</kbd>, which is used to define the simulation properties of an element in the robot. We can either define a property that is applicable to all the links or one that is specific to a link. Here is a code snippet inside the xacro file that defines the coefficient of the friction of a link:</p>
<pre>     &lt;gazebo reference="chefbot_wheel_left_link"&gt; 
       &lt;mu1&gt;1.0&lt;/mu1&gt; 
       &lt;mu2&gt;1.0&lt;/mu2&gt; 
       &lt;kp&gt;1000000.0&lt;/kp&gt; 
       &lt;kd&gt;100.0&lt;/kd&gt; 
       &lt;minDepth&gt;0.001&lt;/minDepth&gt; 
       &lt;maxVel&gt;1.0&lt;/maxVel&gt; 
 
     &lt;/gazebo&gt;  </pre>
<p>The <kbd>reference</kbd> property is used to specify a link in the robot. So, the preceding properties will only be applicable to the <kbd>chefbot_wheel_left_link</kbd>.</p>
<p>The following code snippet shows you how to set the color of a robot link. We can create custom colors, define the custom colors, or use the default colors in Gazebo. You can see that for the <kbd>base_link</kbd>, we are using the <kbd>Gazebo/White</kbd> color from Gazebo's default property:</p>
<pre>   &lt;material name="blue"&gt; 
       &lt;color rgba="0 0 0.8 1"/&gt; 
   &lt;/material&gt; 
 
   &lt;gazebo reference="base_link"&gt; 
     &lt;material&gt;Gazebo/White&lt;/material&gt; 
   &lt;/gazebo&gt; </pre>
<div class="packt_infobox">Refer to <a href="http://gazebosim.org/tutorials/?tut=ros_urdf">http://gazebosim.org/tutorials/?tut=ros_urdf</a> to see all the tags that are used in the simulation.</div>
<p>That covers the main tags of the simulation. Now we will look at the Gazebo-ROS plugins that we have used in this simulation.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cliff sensor plugin</h1>
                </header>
            
            <article>
                
<p>The cliff sensor is a set of IR sensors that detect cliffs, which helps to avoid steps and prevents the robot from falling. This is one of the sensors in the mobile base of Turtlebot 2, called Kobuki (<a href="http://kobuki.yujinrobot.com/">http://kobuki.yujinrobot.com/</a>). We're using this plugin in the Turtlebot 2 simulation.</p>
<p>We can set the parameters of the sensors, such as the minimum and maximum angle of the IR beams, the resolution, and the number of samples per second. We can also limit the detection range of the sensor. There are three cliff sensors in our simulation model, as shown in the following code:</p>
<pre>     &lt;gazebo reference="cliff_sensor_front_link"&gt; 
       &lt;sensor type="ray" name="cliff_sensor_front"&gt; 
         &lt;always_on&gt;true&lt;/always_on&gt; 
         &lt;update_rate&gt;50&lt;/update_rate&gt; 
         &lt;visualize&gt;true&lt;/visualize&gt; 
         &lt;ray&gt; 
           &lt;scan&gt; 
             &lt;horizontal&gt; 
               &lt;samples&gt;50&lt;/samples&gt; 
               &lt;resolution&gt;1.0&lt;/resolution&gt; 
               &lt;min_angle&gt;-0.0436&lt;/min_angle&gt;  &lt;!-- -2.5 degree --&gt; 
               &lt;max_angle&gt;0.0436&lt;/max_angle&gt; &lt;!-- 2.5 degree --&gt; 
             &lt;/horizontal&gt; 
 
           &lt;/scan&gt; 
           &lt;range&gt; 
             &lt;min&gt;0.01&lt;/min&gt; 
             &lt;max&gt;0.15&lt;/max&gt; 
             &lt;resolution&gt;1.0&lt;/resolution&gt; 
           &lt;/range&gt; 
         &lt;/ray&gt; 
       &lt;/sensor&gt; 
     &lt;/gazebo&gt; </pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Contact sensor plugin</h1>
                </header>
            
            <article>
                
<p>Here is the code snippet for the contact sensor on our robot. If the base of the robot collides with any objects, this plugin will trigger. It is commonly attached to the <kbd>base_link</kbd> of the robot, so whenever the bumper hits any object, this sensor will be triggered:</p>
<pre>     &lt;gazebo reference="base_link"&gt; 
       &lt;mu1&gt;0.3&lt;/mu1&gt; 
       &lt;mu2&gt;0.3&lt;/mu2&gt; 
       &lt;sensor type="contact" name="bumpers"&gt; 
         &lt;always_on&gt;1&lt;/always_on&gt; 
         &lt;update_rate&gt;50.0&lt;/update_rate&gt; 
         &lt;visualize&gt;true&lt;/visualize&gt; 
         &lt;contact&gt; 
           &lt;collision&gt;base_footprint_collision_base_link&lt;/collision&gt; 
         &lt;/contact&gt; 
       &lt;/sensor&gt; 
     &lt;/gazebo&gt; </pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gyroscope plugin</h1>
                </header>
            
            <article>
                
<p>The gyroscope plugin is used to measure the angular velocity of the robot. Using the angular velocity, we can compute the orientation of the robot. The orientation of the robot is used in the robot drive controller for computing the robot's pose, as shown in the following code:</p>
<pre>     &lt;gazebo reference="gyro_link"&gt; 
      &lt;sensor type="imu" name="imu"&gt; 
        &lt;always_on&gt;true&lt;/always_on&gt; 
        &lt;update_rate&gt;50&lt;/update_rate&gt; 
        &lt;visualize&gt;false&lt;/visualize&gt; 
        &lt;imu&gt; 
          &lt;noise&gt; 
            &lt;type&gt;gaussian&lt;/type&gt; 
             &lt;rate&gt; 
               &lt;mean&gt;0.0&lt;/mean&gt; 
               &lt;stddev&gt;${0.0014*0.0014}&lt;/stddev&gt; &lt;!-- 0.25 x 0.25 (deg/s) --&gt; 
               &lt;bias_mean&gt;0.0&lt;/bias_mean&gt; 
               &lt;bias_stddev&gt;0.0&lt;/bias_stddev&gt; 
             &lt;/rate&gt; 
                  &lt;accel&gt; &lt;!-- not used in the plugin and real robot, hence using tutorial values --&gt; 
                         &lt;mean&gt;0.0&lt;/mean&gt; 
                         &lt;stddev&gt;1.7e-2&lt;/stddev&gt; 
                         &lt;bias_mean&gt;0.1&lt;/bias_mean&gt; 
                         &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt; 
                  &lt;/accel&gt; 
          &lt;/noise&gt; 
         &lt;/imu&gt; 
                 &lt;/sensor&gt; 
     &lt;/gazebo&gt; </pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Differential drive plugin</h1>
                </header>
            
            <article>
                
<p>The differential drive plugin is the most important plugin of the simulation. This plugin simulates the differential drive behavior in the robot. It will move the robot model when it receives the command velocity (the linear and angular velocity) in the form of ROS Twist messages (<kbd>geometry_msgs/Twist</kbd>). This plugin also computes the odometry of the robot, which gives the local position of the robot, as shown in the following code:</p>
<pre>  &lt;gazebo&gt; 
       &lt;plugin name="kobuki_controller" filename="libgazebo_ros_kobuki.so"&gt; 
         &lt;publish_tf&gt;1&lt;/publish_tf&gt; 
 
         &lt;left_wheel_joint_name&gt;wheel_left_joint&lt;/left_wheel_joint_name&gt; 
         &lt;right_wheel_joint_name&gt;wheel_right_joint&lt;/right_wheel_joint_name&gt; 
         &lt;wheel_separation&gt;.30&lt;/wheel_separation&gt; 
         &lt;wheel_diameter&gt;0.09&lt;/wheel_diameter&gt; 
         &lt;torque&gt;18.0&lt;/torque&gt; 
         &lt;velocity_command_timeout&gt;0.6&lt;/velocity_command_timeout&gt; 
         &lt;cliff_detection_threshold&gt;0.04&lt;/cliff_detection_threshold&gt; 
         &lt;cliff_sensor_left_name&gt;cliff_sensor_left&lt;/cliff_sensor_left_name&gt; 
         &lt;cliff_sensor_center_name&gt;cliff_sensor_front&lt;/cliff_sensor_center_name&gt; 
         &lt;cliff_sensor_right_name&gt;cliff_sensor_right&lt;/cliff_sensor_right_name&gt; 
         &lt;cliff_detection_threshold&gt;0.04&lt;/cliff_detection_threshold&gt; 
         &lt;bumper_name&gt;bumpers&lt;/bumper_name&gt; 
 
          &lt;imu_name&gt;imu&lt;/imu_name&gt; 
       &lt;/plugin&gt; 
     &lt;/gazebo&gt; </pre>
<p>To compute the robot's odometry, we have to provide the robot's parameters, such as the distance between the wheels, wheel diameter, and the torque of the motors. According to our design, the wheel separation is 30 cm, the wheel diameter is 9 cm, and the torque is 18 N. If we want to publish the transformation of the robot, we can set the <kbd>publish_tf</kbd> as 1. Each tag inside the plugin is the parameter of the corresponding plugin. As you can see, it takes all the inputs from the contact sensor, imu, and cliff sensor.</p>
<p>The <kbd>libgazebo_ros_kobuki</kbd>.so plugin is installed along with Turtlebot 2 simulation packages. We are using the same plugin in our robot. We have to make sure that, the Turtlebot 2 simulation is installed on your system, prior to running this simulation.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Depth camera plugin</h1>
                </header>
            
            <article>
                
<p>The depth camera plugin simulates the characteristics of a depth camera, such as Kinect or Astra. The plugin name is <kbd>libgazebo_ros_openni_kinect.so</kbd>, and it helps us to simulate different kinds of depth sensors that have different characteristics. The plugin is shown in the following code:</p>
<pre>     &lt;plugin name="kinect_camera_controller" filename="libgazebo_ros_openni_kinect.so"&gt; 
          &lt;cameraName&gt;camera&lt;/cameraName&gt; 
          &lt;alwaysOn&gt;true&lt;/alwaysOn&gt; 
          &lt;updateRate&gt;10&lt;/updateRate&gt; 
          &lt;imageTopicName&gt;rgb/image_raw&lt;/imageTopicName&gt; 
          &lt;depthImageTopicName&gt;depth/image_raw&lt;/depthImageTopicName&gt; 
          &lt;pointCloudTopicName&gt;depth/points&lt;/pointCloudTopicName&gt; 
          &lt;cameraInfoTopicName&gt;rgb/camera_info&lt;/cameraInfoTopicName&gt; 
          &lt;depthImageCameraInfoTopicName&gt;depth/camera_info&lt;/depthImageCameraInfoTopicName&gt; 
          &lt;frameName&gt;camera_depth_optical_frame&lt;/frameName&gt; 
          &lt;baseline&gt;0.1&lt;/baseline&gt; 
          &lt;distortion_k1&gt;0.0&lt;/distortion_k1&gt; 
          &lt;distortion_k2&gt;0.0&lt;/distortion_k2&gt; 
          &lt;distortion_k3&gt;0.0&lt;/distortion_k3&gt; 
          &lt;distortion_t1&gt;0.0&lt;/distortion_t1&gt; 
          &lt;distortion_t2&gt;0.0&lt;/distortion_t2&gt; 
          &lt;pointCloudCutoff&gt;0.4&lt;/pointCloudCutoff&gt; 
        &lt;/plugin&gt; </pre>
<p>The plugin's publishers, the RGB image, depth image, and the point cloud data. We can set the camera matrix in the plugin, as well as customize other parameters.</p>
<div class="packt_infobox">You can refer to <a href="http://gazebosim.org/tutorials?tut=ros_depth_camera&amp;cat=connect_ros">http://gazebosim.org/tutorials?tut=ros_depth_camera&amp;amp;cat=connect_ros</a> to learn more about the depth camera plugin in Gazebo.</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visualizing the robot sensor data</h1>
                </header>
            
            <article>
                
<p>In this section, we learn how to visualize the sensor data from the simulated robot. In the <kbd>chefbot_gazebo</kbd> package, there are launch files to start the robot in an empty world or in a hotel-like environment. The custom environment can be built using Gazebo itself. Just create the environment using primitive meshes and save as a <kbd>*. world</kbd> file, which can be the input of the <kbd>gazebo_ros</kbd> node in the launch file. For starting the hotel environment in Gazebo, you can use the following command:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo chefbot_hotel_world.launch</strong>      </pre>
<div class="CDPAlignCenter CDPAlign"><img src="images/17b03ab3-7436-498a-b5a4-4665fe13f3e1.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The Chefbot in Gazebo in the hotel environment</div>
<p>The nine cubes inside the space represent nine tables. The robot can navigate to any of the tables to deliver food. We will learn how to do this, but before that, we will learn how to visualize the different kinds of sensor data from the robot model.</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/676fd283-5240-4564-a88f-ab5e878c1829.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The Chefbot in Gazebo in the hotel environment</div>
<p>The following command will launch the Rviz, which displays the sensor data from the robot:</p>
<pre>    <strong>$ roslaunch chefbot_description view_robot.launch</strong>  </pre>
<p>This generates a visualization of the sensor data, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/7f66a613-dc3a-4f86-b6ee-3fb46735ed1c.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The sensor visualization of Chefbot in Rviz</div>
<p>We can enable the Rviz display types to view different kinds of sensor data. In the preceding figure, you can see the depth cloud, laser scan, TF, robot model, and RGB camera images.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with Simultaneous Localization and Mapping</h1>
                </header>
            
            <article>
                
<p>One of the requirements of the Chefbot was that it should be able to navigate the environment autonomously and deliver food. To achieve this requirement, we have to use several algorithms, such as SLAM (Simultaneous Localization and Mapping) and AMCL (Adaptive Monte Carlo Localization). There are different approaches to solving the autonomous navigation problem. In this book, we are mainly sticking with these algorithms. The SLAM algorithms are used for mapping an environment at the same time as localizing the robot on the same map. It's seems like a chicken-and-egg problem, but now there are different algorithms to solve it. The AMCL algorithm is used to localize the robot in an existing map. The algorithm that we use in this book is called Gmapping (<a href="http://www.openslam.org/gmapping.html">http://www.openslam.org/gmapping.html</a>), which implements Fast SLAM 2.0 (<a href="http://robots.stanford.edu/papers/Montemerlo03a.html">http://robots.stanford.edu/papers/Montemerlo03a.html</a>). The standard gmapping library is wrapped in an ROS package called ROS Gmapping (<a href="http://wiki.ros.org/gmapping">http://wiki.ros.org/gmapping</a>), which can be used in our application.</p>
<p>The idea of the SLAM node is that as we move the robot around the environment, it will create a map of the environment using the laser scan data and the odometry data.</p>
<div class="packt_infobox">Refer to the ROS Gmapping wiki page at <a href="http://wiki.ros.org/gmapping">http://wiki.ros.org/gmapping</a>&#160;for more details.&#160;<a href="http://wiki.ros.org/gmapping"></a></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing SLAM in the Gazebo environment</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to implement SLAM and apply it to the simulation that we built. You can check the code at <kbd>chefbot_gazebo/launch/gmapping_demo.launch</kbd> and <kbd>launch/includes/ gmapping.launch.xml</kbd>. Basically, we are using a node from the gmapping package and configuring it with the proper parameters. The <kbd>gmapping.launch.xml</kbd> code fragment has the complete definition of this node. The following is the code snippet of this node:</p>
<pre>&lt;launch&gt;  
 &lt;arg name="scan_topic" default="scan" /&gt; 
 
  &lt;node pkg="gmapping" type="slam_gmapping" name="slam_gmapping" output="screen"&gt; 
    &lt;param name="base_frame" value="base_footprint"/&gt; 
    &lt;param name="odom_frame" value="odom"/&gt; 
    &lt;param name="map_update_interval" value="5.0"/&gt; 
    &lt;param name="maxUrange" value="6.0"/&gt; 
    &lt;param name="maxRange" value="8.0"/&gt; </pre>
<p>The name of the node that we are using is <kbd>slam_gmapping</kbd> and the package is <kbd>gmapping</kbd>. We have to provide a few parameters to this node, which can be found in the Gmapping wiki page.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a map using SLAM</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to create a map of our environment using SLAM. First, however, there are several commands that we have to use to start mapping. You should execute each command in each Linux terminal.</p>
<p>First, we have to start our simulation using the following command:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo chefbot_hotel_world.launch</strong>  </pre>
<p>Next, we have to start the keyboard teleoperation node in a new terminal. This will help us move the robot manually using the keyboard:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo keyboard_teleop.launch</strong>  </pre>
<p>The next command starts the SLAM in a new terminal:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo gmapping_demo.launch</strong>  </pre>
<p>Now the mapping will begin. To visualize the mapping process, we can start Rviz with the help of <strong>Navigation</strong> settings:</p>
<pre>    <strong>$ roslaunch chefbot_description view_navigation.launch</strong>  </pre>
<p>Now we can see the map created in Rviz, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/69957574-a44c-4995-8cdb-d6be0651b919.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Creating a map in Rviz using Gmapping.</div>
<p>Now we can use the teleop node to move the robot, and you can see that a map is being created in Rviz. To create a good map of the environment, you have to move the robot slowly, and often you have to rotate the robot. When we move the robot in the environment and build the map, you can save the current map using the following command:</p>
<pre>    <strong>$ rosrun map_server map_saver -f ~/Desktop/hotel</strong>  </pre>
<p>The map will be saved as <kbd>*.pgm</kbd> and <kbd>*.yaml</kbd>, where the <kbd>pgm</kbd> file is the map and the <kbd>yaml</kbd> file is the configuration of the map. You can see the saved map in your desktop.</p>
<p>After moving the robot around the environment, you may get a complete map, such as the one shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/38006876-9c8f-4f46-9196-0b3680f204e8.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Final map using Gmapping.</div>
<p>The map can be saved at any time, but make sure that the robot covers the entire area of the environment and has mapped all of its space, as shown in the preceding screenshot<em>.</em> Once we are sure that the map is completely built, enter the <kbd>map_saver</kbd> command again and close the terminals. If you aren't able to map the environment, you can check the existing map from <kbd>chefbot_gazebo/maps/hotel</kbd>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with Adaptive Monte Carlo Localization</h1>
                </header>
            
            <article>
                
<p>We have successfully built the map of the environment. Now we have to navigate autonomously from the current robot position to target position. The first step before starting autonomous navigation is localizing the robot in the current map. The algorithm we are using to localize on the map is called AMCL. The AMCL uses a particle filter to track the robot's position with respect to the map. We are using an ROS package to implement AMCL in our robot (<a href="http://wiki.ros.org/amcl">http://wiki.ros.org/amcl</a>). Similar to Gmapping, there are a lot of parameters to configure for the <kbd>amcl</kbd> node, which is inside the <kbd>amcl</kbd> package. You can find all the parameters of amcl on the ROS wiki page itself.</p>
<p>So how we can start AMCL for our robot? There is a launch file for doing that, which is placed in <kbd>chefbot_gazebo/amcl_demo.launch</kbd> and <kbd>chefbot_gazebo/includes/amcl.launch.xml</kbd>.</p>
<p>We can see the definition of <kbd>amcl_demo.launch</kbd>. The following code shows the definition of this launch file:</p>
<pre>&lt;launch&gt; 
  &lt;!-- Map server --&gt; 
  &lt;arg name="map_file" default="$(find chefbot_gazebo)/maps/hotel.yaml"/&gt; 
 
  &lt;node name="map_server" pkg="map_server" type="map_server" args="$(arg map_file)" /&gt; </pre>
<p>The first node in this launch file starts <kbd>map_server</kbd> from the <kbd>map_server</kbd> package. The <kbd>map_server</kbd> node loads a static map that we have already saved and publishes it into a topic called <kbd>map</kbd> (<kbd>nav_msgs/OccupancyGrid</kbd>). We can mention the map file as an argument of the <kbd>amcl_demo.launch</kbd> file, and if there is a map file, the <kbd>map_server</kbd> node will load that; otherwise it will load the default map, which is located in the <kbd>chefbot_gazeob/maps/hotel.yaml</kbd> file.</p>
<p>After loading the map, we start the <kbd>amcl</kbd> node and move the base node. The AMCL node helps to localize the robot on the current <kbd>map</kbd> and the <kbd>move_base</kbd> node inside the ROS navigation stack, which helps in navigating the robot from the start to the target position. We will learn more about the <kbd>move_base</kbd> node in the upcoming chapters. The <kbd>move_base</kbd> node also needs to be configured with parameters. The parameter files are kept inside the <kbd>chefbot_gazebo/param</kbd> folder, as shown in the following code:</p>
<pre>  &lt;!-- Localization --&gt; 
  &lt;arg name="initial_pose_x" default="0.0"/&gt; 
  &lt;arg name="initial_pose_y" default="0.0"/&gt; 
  &lt;arg name="initial_pose_a" default="0.0"/&gt; 
  &lt;include file="$(find chefbot_gazebo)/launch/includes/amcl.launch.xml"&gt; 
    &lt;arg name="initial_pose_x" value="$(arg initial_pose_x)"/&gt; 
    &lt;arg name="initial_pose_y" value="$(arg initial_pose_y)"/&gt; 
    &lt;arg name="initial_pose_a" value="$(arg initial_pose_a)"/&gt; 
  &lt;/include&gt; 
 
  &lt;!-- Move base --&gt; 
  &lt;include file="$(find chefbot_gazebo)/launch/includes/move_base.launch.xml"/&gt; 
&lt;/launch&gt; 
 </pre>
<div class="packt_infobox">You can refer more about ROS navigation stack from following link<br/>
<a href="http://wiki.ros.org/navigation/Tutorials/RobotSetup">http://wiki.ros.org/navigation/Tutorials/RobotSetup</a></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing AMCL in the Gazebo environment</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to implement AMCL in our Chefbot. We will use the following procedures to incorporate AMCL within the simulation. Each command should be executed in each terminal.</p>
<p>The first command starts the Gazebo simulator:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo chefbot_hotel_world.launch</strong>  </pre>
<p>Now we can start the AMCL launch file, with or without the map file as an argument. If you want to use a custom map that you have built, then use the following command:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo amcl_demo.launch map_file:=/home/&lt;your_user_name&gt;/Desktop/hotel</strong>  </pre>
<p>If you want to use the default map, you can use the following command:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo amcl_demo.launch</strong>  </pre>
<p>After starting AMCL, we can start Rviz to visualize the map and robot. We will see a view in Rviz as shown in the following screenshot. You can see a map and a robot surrounded by green particles. The green particles are called <kbd>amcl</kbd> particles. They indicate the uncertainty of the robot's position. If there are more particles around the robot, then this means that the uncertainty of the robot's position is higher. When it starts moving, the particle count will reduce and its position will be more certain. If the robot isn't able to localize the position of the map, we can use the <em>2D Pose Estimate</em> button in Rviz (on the toolbar) to manually set the initial position of the robot on the map. You can see the button in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/6521ef37-a6a5-4f66-9b4e-b3c36db0db91.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Starting AMCL on the hotel map.</div>
<p>If you zoom into the robot's position in Rviz, you can see the particles, as shown in the preceding screenshot. We can also see the obstacles around the robot in different colors:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/2a7f6680-0766-4bb0-b0cf-9a17ec0fa8e0.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">AMCL cloud around the robot.</div>
<p>In the next section, we will learn how to program the Chefbot to autonomously navigate this map. You don't need to close the current terminals; we can navigate the robot autonomously in the Rviz itself.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Autonomous navigation of Chefbot in the hotel using Gazebo</h1>
                </header>
            
            <article>
                
<p>To start the robot's autonomous navigation, we just need to command the target robot position on the map. There is a button in Rviz called <span class="packt_screen">2D Nav Goal</span>. We can click that button and click on a point on the map. You can now see an arrow indicating the position of the robot. When you give the target position in the map, you can see that the robot is planning a path from its current position to the target position. It will slowly move from its current position to the target position, avoiding all obstacles. The following screenshot shows the path planning and navigation of the robot to the target position. The color grid around the robot shows the local cost map of the robot, as well as the local planner path and the obstacles around the robot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/cadf1a03-9f08-4649-91d6-23ec2e051d5c.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Autonomous navigation of the robot.</div>
<p>In this way, if we command a position inside the map that is nearer to a table, the robot can go to that table and serve the food and then return back to its home position. Instead of commanding it from Rviz, we can write an ROS node to do the same. This will be explained in the last few chapters of this book.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to simulate our own robot, called Chefbot. We looked at the design of the Chefbot in the previous chapter. We started the chapter by learning about the Gazebo simulator and its different features and capabilities. After that, we looked at how the ROS framework and Gazebo simulator are used to perform a robot simulation. We installed the TurtleBot 2 package and tested the Turtlebot 2 simulation in Gazebo. After that, we created the Chefbot simulation and used Gmapping, AMCL, and autonomous navigation in a hotel environment. We learned that the accuracy of the simulation depends on the map, and that the robot will work better in a simulation if the generated map is perfect.</p>
<p>In the next chapter, we will learn how to design the robot's hardware and electronic circuit.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>How we can model a sensor in Gazebo?</li>
<li>How is ROS interfaced with Gazebo?</li>
<li>What are the important URDF tags for simulation?</li>
<li>What is Gmapping, and how we can implement it in ROS?</li>
<li>What is the function of the <kbd>move_base</kbd> node in ROS?</li>
<li>What is AMCL, and how we can implement it in ROS?</li>
</ol>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>To learn more about URDF, Xacro, and Gazebo, you can refer to the book <em>Mastering ROS for Robotics Programming - Second Edition</em> (<a href="https://www.packtpub.com/hardware-and-creative/mastering-ros-robotics-programming-second-edition">https://www.packtpub.com/hardware-and-creative/mastering-ros-robotics-programming-second-edition</a>).</p>


            </article>

            
        </section>
    </div>
</body>
</html>