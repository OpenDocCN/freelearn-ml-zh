- en: Unsupervised Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using denoising autoencoders to detect fraudulent transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating word embeddings using CBOW or skipgram representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the MNIST dataset using PCA and t-SNE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using word vectors for Twitter sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing LDA with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LDA to classify text documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for LDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you need the following files (available
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`CreditCardFraud.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`creditcard.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WordEmbeddings.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MnistTSNE.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TweetEmbeddings.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tweets.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LDA.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TopicModellingLDA.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PrepDataLDA.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](74eaa4cc-b9da-4cf4-9041-3ce42f998499.xhtml), *Clustering with
    Unsupervised Learning*, we have already addressed unsupervised learning. We said
    that unsupervised learning is a paradigm in machine learning where we build models
    without relying on labeled training data. Why return to this topic? In this case,
    we will discuss the problem of learning representations for data such as images,
    video, and the corpus of natural language, in an unsupervised way.
  prefs: []
  type: TYPE_NORMAL
- en: Using denoising autoencoders to detect fraudulent transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](74eaa4cc-b9da-4cf4-9041-3ce42f998499.xhtml), *Clustering with
    Unsupervised Learning*, we dealt with the topic of **autoencoders**. In the *Autoencoders
    to reconstruct handwritten digit images* recipe, there is a neural network whose
    purpose is to code its input into small dimensions, and the result obtained, to
    be able to reconstruct the input itself. The purpose of autoencoders is not simply
    to perform a sort of compression of the input or look for an approximation of
    the identity function; there are also techniques that allow us to direct the model
    (starting from a hidden layer of reduced dimensions) to give greater importance
    to some data properties.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will train an autoencoder in unsupervised mode to detect
    anomalies in credit card transaction data. To do this, the credit card fraud detection
    dataset will be used. This is a dataset containing the anonymized credit card
    transactions labeled as fraudulent or genuine. Transactions made by credit cards
    in September 2013 by European cardholders are listed. This dataset presents 492
    transactions labeled as frauds out of 284,807 transactions. The dataset is highly
    unbalanced, as the positive class (frauds) accounts for 0.172% of all transactions.
    The dataset is available on Kabble at the following URL: [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to use denoising autoencoders to detect fraudulent transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `CreditCardFraud.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the experiment reproducible, in the sense that it provides the same
    results with each reproduction, it is necessary to set the seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As already said, we will use the credit card fraud detection dataset (`creditcard.csv`)
    that is already provided to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s count the occurrences of the two classes (`fraud`= `1`; `normal`=`0`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As anticipated, the dataset is highly unbalanced—the positive class (`frauds`)
    is `492` out of `284315`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the available variables, the amount of the transactions (`Amount`) is
    the most interesting one. Let''s calculate some statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the values are very different with a high standard deviation.
    It is advisable to perform a scaling of the data. Remember, it is a good practice
    to rescale the data before training a machine learning algorithm. With rescaling,
    data units are eliminated, allowing you to compare data from different locations
    easily. To do this, we will use the `sklearn` `StandardScaler()` function. This
    function removes the mean and scales the values to unit variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We have thus confirmed that now the data has `mean=0` and unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we split the starting data into two sets: the training set (70%) and test
    set (30%). The training set will be used to train a classification model, and
    the test set will be used to test the model''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can build the Keras model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/655679f4-bb24-407d-b351-19b51b67eae3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have to configure the model for training. To do this, we will use the
    `compile()` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can plot the loss history to evaluate the model convergence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we use the model to reconstruct the result of the transactions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate the quality of the prediction, we used the **mean squared error**
    (**MSE**) loss function. MSE measures the average of the squares of the errors—that
    is, the average squared difference between the estimated values and what is estimated.
    MSE is a measure of the quality of an estimator—it is always non-negative, and
    has values close to zero. So, we calculated some statistics related to the error
    and the real values. The following results were obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c90abb89-fcb9-4bbb-b4e0-b67aee64c4d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can compare the results of the classification to the actual values.
    The best way to do this is to use a **confusion matrix**. In a confusion matrix,
    we compare our results to real data. What''s good about a confusion matrix is
    that it identifies the nature of the classification errors, as well as their quantities.
    In this matrix, the diagonal cells show the number of cases that were correctly
    classified; all the other cells show the misclassified cases. To calculate the
    confusion matrix, we can use the `confusion_matrix()` function that''s contained
    in the `sklearn.metrics` package as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following confusion matrix is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will calculate the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following accuracy is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The result looks great, but unfortunately the input dataset is highly unbalanced.
    If we evaluate the accuracy of fraudulent transactions only, this data is significantly
    reduced.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are different types of autoencoders available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanilla autoencoder**: It is the simplest form, characterized by a three-layer
    network, that is, a neural network with only one hidden layer. Input and output
    are the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilayer autoencoder**: If only one hidden layer is not enough, we can
    extend the autoencoder along the depth dimension. For example, three hidden layers
    are used, for better generalization, but we will also have to make the symmetric
    network using the intermediate layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional autoencoder**: Three-dimensional vectors are used instead of
    one-dimensional vectors. The input image is sampled to obtain a latent representation,
    that is, a dimensional reduction, thus forcing the autoencoder to learn from a
    compressed version of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularized autoencoder**: Rather than limiting the model''s capacity by
    maintaining a shallow encoder and decoder architecture, as well as a forced reduction,
    regularized autoencoders use a loss function to encourage the model to assume
    properties that go beyond the simple ability to copy the input to the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practice, we find two different types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse autoencoder**: This is usually used for classification. By training
    an autoencoder, the hidden units in the middle layer are activated too frequently.
    To avoid this, we need to lower their activation rate by limiting it to a fraction
    of the training data. This constraint is called a **sparsity constraint**, as
    each unit is activated only by a pre-defined type of input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Denoising autoencoder**: Rather than adding a penalty to the `loss` function,
    we can make the object change, adding noise to the input image and making the
    autoencoder learn to remove it autonomously. This means that the network will
    extract only the most relevant information, and learn from a robust representation
    of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the Keras library: [https://keras.io/](https://keras.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Autoencoders* (from Stanford University): [http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating word embeddings using CBOW and skipgram representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing Text
    Data,* we already dealt with this topic. In the *Word2Vec using gensim* recipe,
    we used the `gensim` library to build a word2vec model. Now, we will deepen the
    topic. **Word embedding** allows the computer to memorize both semantic and syntactic
    information of words starting from an unknown corpus and constructs a vector space
    in which the vectors of words are closer if the words occur in the same linguistic
    contexts, that is, if they are recognized as semantically more similar. **Word2vec** is
    a set of templates that are used to produce word embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will use the `gensim` library to generate word embeddings.
    We will also analyze two techniques to do this: CBOW and skipgram representations.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to generate word embeddings using CBOW and skipgram representations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `WordEmbeddings.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can train the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Three arguments are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sentences`: The training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_count=1`: The minimum count of words to consider when training the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sg=0`: The training algorithm, CBOW (0) or skip gram (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s print a summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s list and then print a summary of the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will access the vector for one word (`book`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a70d17e4-d07f-41ec-beac-86b4b5208671.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To use the `skipgram` algorithm, we have to perform a similar procedure except
    we set the argument `sg=1`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec uses **continuous bag-of-words** (**CBOW**) and skipgram to word embeddings.
    In the CBOW algorithm, the model predicts the current word from a window of surrounding
    context words. Context word order does not influence prediction. In the skipgram
    algorithm, the model uses the current word to predict the surrounding window of
    context words.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to authors, CBOW is and skip-gram is but does a better job for infrequent
    words.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `gensim` library: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Efficient Estimation of Word Representations in Vector Space* (by
    Tomas Mikolov and others): [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the MNIST dataset using PCA and t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of datasets of important dimensions, the data is previously transformed
    into a reduced series of representation functions. This process of transforming
    the input data into a set of functionalities is called **features extraction**.
    This is because the extraction of the characteristics proceeds from an initial
    series of measured data and produces derived values that can keep the information
    contained in the original dataset, but discharged from the redundant data.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the subsequent learning and generalization phases will be facilitated
    and, in some cases, this will lead to better interpretations. It is a process
    of extracting new features from the original features, thereby reducing the cost
    of feature measurement, which boosts classifier efficiency. If the features are
    carefully chosen, it is assumed that the features set will run the desired task
    with the reduced representation, instead of the full-sized input.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use **principal component analysis** (**PCA**) and **t-distributed
    Stochastic Neighbor Embedding methods** (**t-SNE**) to perform a feature extraction
    procedure. In this way, we will be able to visualize how the different elements
    of a very large dataset, such as MNIST, are grouped together.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to visualize the MNIST dataset using PCA and t-SNE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `MnistTSNE.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the `mnist` dataset, the following code must be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following tuples are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XTrain`, `XTest`: A `uint8` array of grayscale image data with the (`num_samples`,
    28, 28) shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YTrain`, `YTest`: A `uint8` array of digit labels (integers in the range 0-9)
    with the (`num_samples`) shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To reduce the dimensionality, we will flatten the 28 x 28 images into vectors
    of size 784:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract only a part of the data from this large dataset to obtain a better
    visualization (only 1,000 records):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s perform a `pca` analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We display the data available in the new plan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf01e21e-ec24-414e-83b7-51ec58641e64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we will repeat the procedure using the t-SNE method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We display the data available in the new plan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fff99ad2-2180-4e93-bf40-c924903a3664.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing the two results obtained, it is clear that the second method allows
    us to identify the groups representing the different digits in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA creates a new set of variables that are principal components. Each main
    component is a linear combination of the original variables. All principal components
    are orthogonal to one another, so there is no redundant information. The principal
    components as a whole constitute an orthogonal basis for the data space. The goal
    of PCA is to explain the maximum amount of variance with the least number of principal
    components. PCA is a form of multidimensional scaling. It transforms variables
    into a lower-dimensional space that retains the maximum details regarding the
    variables. A principal component is therefore a combination of the original variables
    after a linear transformation.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE is a dimensionality reduction algorithm developed by Geoffrey Hinton and
    Laurens van der Maaten, widely used as an automatic learning tool in many research
    fields. It is a non-linear dimension reduction technique that lends itself particularly
    to the embedding of high-dimensional datasets in a two- or three-dimensional space,
    in which they can be visualized by means of a dispersion plot. The algorithm models
    the points so that nearby objects in the original space are close together with
    reduced dimensionality, and distant objects are far away, trying to preserve the
    local structure.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A t-SNE algorithm is divided into two main phases. In the first phase, a probability
    distribution is constructed so that each pair of points in the original high-dimensional
    space associates a high probability value if the two points are similar, and low
    if they are dissimilar. Then, a second analogous probability distribution is defined
    in the small-sized space. The algorithm then minimizes the divergence of Kullback–Leibler
    of the two distributions by descending the gradient, reorganizing the points in
    the small-sized space.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.decomposition.PCA` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Official documentation of the `sklearn.manifold.TSNE` function: [https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.htm](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using word embedding for Twitter sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing Text
    Data*, we have already dealt with sentiment analysis. In *Analyzing the sentiment
    of a sentence* recipe, we have analyzed the sentiment of a sentence using a Naive
    Bayes classifier starting from the data contained in the `movie_reviews` corpus.
    On that occasion, we said that sentiment analysis is one of the most popular applications
    of NLP. *Sentiment analysis* refers to the process of determining whether a given
    piece of text is positive or negative. In some variations, we consider "neutral"
    as a third option.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the word embedding method to analyze the sentiment
    of Twitter posts by customers of some US airlines. The Twitter data was classified
    based on the opinions of some contributors. They were asked to first classify
    positive, negative, and neutral tweets, followed by categorizing negative ones.
    The dataset is available at the following link: [https://www.kaggle.com/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to use word embedding for Twitter sentiment analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `TweetEmbeddings.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the `Tweets` dataset (the `Tweets.csv` file that''s already provided
    to you), the following code must be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Only two columns are extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text`: Twitter posts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`airline_sentiment`: Positive, neutral, or negative classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we split the starting data into two sets: the training set (70%) and test
    set (30%). The training set will be used to train a classification model, and
    the test set will be used to test the model''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will convert words to numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To do this and tokenize the `XTrain` dataset `Tokenizer`, the `fit_on_texts` and
    `texts_to_sequences` methods were used.
  prefs: []
  type: TYPE_NORMAL
- en: 'To transform the input data into a format compatible with Keras, `pad_sequences`
    models will be used. This method transforms a list of sequences (lists of scalars)
    into a 2D NumPy array, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we will convert the target classes to numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will build the Keras model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The embedding layer takes as input a 2D tensor with shape (`batch_size`, `sequence_length`),
    where each entry is a sequence of integers. A 3D tensor with the shape (`batch_size`,
    `sequence_length`, and `output_dim`) is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will compile and fit the model created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate the model''s performance, let''s print the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will plot the model history:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd725421-9e17-46be-b82d-68c280a9048e.png)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing the progress of the validation loss, we realize that the model is
    overfitting. To handle orverfitting, as we learned in the *Building a ridge regressor*
    recipe in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml), *The Realm
    of Supervised Learning*, we need to use a regularization method.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term **sentiment analysis** refers to the use of NLP techniques, text analysis,
    and computational linguistics to identify and extract subjective information in
    written or spoken text sources. Sentiment analysis can be tackled through different
    approaches. The most commonly used can be grouped into four macro-categories (*A
    Study and Comparison of* *Sentiment Analysis Methods for Reputation Evaluation*
    by Collomb A, Costea C, Joyeux D, Hasan O, and Brunie L, 2014):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lexicon-based methods**: These detect emotional keywords, and assign arbitrary
    words affinity likely to represent particular emotions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rule-based methods**: These classify texts using emotional categories, based
    on the presence of unambiguous emotional words, such as *happy*, *sad*, and *bored*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical methods**: Here, we try to identify the owner of a sentiment,
    that is, who the subject is, and the objective, or the object to which the sentiment
    is felt. To measure opinion in context and find the characteristic that was judged,
    we check the grammatical relations between the words in the text. This is obtained
    through a thorough scan of the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning methods**: These use different learning algorithms to determine
    sentiment by having a dataset classified (supervised methods). The learning process
    is not immediate; in fact, models have to be built that associate a polarity to
    different types of comments and, if necessary, a topic for analysis purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `regularization` methods involve modifying the performance function, normally
    selected as the sum of the squares of regression errors on the training set. When
    a large number of variables are available, the least square estimates of a linear
    model often have a low bias but a high variance with respect to models with fewer
    variables. Under these conditions, there is an overfitting problem. To improve
    precision prediction by allowing greater bias but a small variance, we can use
    variable selection methods and dimensionality reduction, but these methods may
    be unattractive for computational burdens in the first case, or provide a difficult
    interpretation in the other case.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the Keras library: [https://keras.io/](https://keras.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to* Sentiment Analysis* (from Stanford University): [https://web.stanford.edu/class/cs124/lec/sentiment.pdf](https://web.stanford.edu/class/cs124/lec/sentiment.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing LDA with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** (**LDA**) is a generative model, used in the
    study of natural language, which allows you to extract arguments from a set of
    source documents and provide a logical explanation on the similarity of individual
    parts of documents. Each document is considered as a set of words that, when combined,
    form one or more subsets of latent topics. Each topic is characterized by a particular
    distribution of terms.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `sklearn.decomposition.LatentDirichletAllocation`
    function to produce a feature matrix of token counts, similar to what the `CountVectorizer`
    function (just used in the *Building a bag-of-words model* recipe of [Chapter
    7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing Text Data*) would produce
    on the text.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to implement LDA with scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `LDA.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To produce the input data, we will use the `sklearn.datasets.make_multilabel_classification`
    function. This function generates a random multilabel classification problem,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The following data is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X`: The generated samples that is an array of shape [`n_samples`, `n_features`]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Y`: The label sets that is an array or sparse CSR matrix of shape [`n_samples`,
    `n_classes`]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, the `Y` variable will not serve us, as we will use an unsupervised
    method that, as we know, does not require prior knowledge of the data label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can build the `LatentDirichletAllocation()` model (with an online variational
    Bayes algorithm):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Only two parameters are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=5`: This is the number of topics, `5`, because we used an input
    dataset built on the basis of five groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state=1`: This is the seed used by the random number generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will train the model for the data, `X`, with a variational Bayes method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will get topics for the last 10 samples of the `X` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0753c1cd-e12b-4272-a2c0-d2c1d700c891.png)'
  prefs: []
  type: TYPE_IMG
- en: For each example provided as input, a sequence of five values is returned representing
    the probabilities that the topic belongs to that group. Obviously, the value closest
    to 1 represents the best probability.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generative process of the LDA algorithm is based on the analysis of the
    data contained in the text. Word combinations are considered random variables.
    The LDA algorithm can be executed in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: A word distribution is associated with each topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each document is found in a topic distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each word in the document, verify its attribution to a document topic and
    to a word distribution of the topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the type of inference, the LDA algorithm allows us to reach a certain
    level of effectiveness and cost (efficiency) in terms of temporal and spatial
    complexity. The LDA model was presented for the first time in 2003 in a paper
    published by David Blei, Andrew Ng, and Michael Jordan.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.decomposition.LatentDirichletAllocation`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Latent Dirichlet Allocation* (by David Blei, Andrew Ng, and Michael
    Jordan): [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing
    Text Data*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LDA to classify text documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is a natural language analysis model that allows to understand the semantic
    meaning of the text by analyzing the similarity between the distribution of the
    terms of the document with that of a specific topic (topic) or of an entity. More
    recently, LDA has gained notoriety even in semantic SEO as a possible ranking
    factor for the Google search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `sklearn.decomposition.LatentDirichletAllocation` function
    to perform a topic modeling analysis.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to use LDA to classify text documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `TopicModellingLDA.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the data, we will use the `fetch_20newsgroups` dataset from the `sklearn`
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This is a collection of about 20,000 newsgroup documents, divided into 20 different
    newsgroups. The dataset is particularly useful for dealing with text classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will print the name of the newsgroup available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 11,314 samples in the data. We will extract only 2,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract a document term matrix. This is basically a matrix that
    counts the number of occurrences of each word in the document. So, we will define
    the object, and extract the document term matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the LDA model (with an online variational Bayes algorithm):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We will train the model for the data `NGDataVectModel` with a variational Bayes
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will print the topic extracted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/302f85f8-f61e-4d2c-9594-863a60d0c01b.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Topic modeling** refers to the process of identifying hidden patterns in
    text data. The goal is to uncover some hidden thematic structure in a collection
    of documents. This will help us organize our documents in a better way so that
    we can use them for analysis. This is an active area of research in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: The LDA analysis automatically allows to go back to the topic of the phrases
    by the association of co-occurrences with a reference **knowledge base** (**KB**),
    without having to interpret the meaning of the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, **de Finetti's theorem** establishes that any collection of changeable
    random variables is represented as a mixture of distributions, so if you want
    to have an exchangeable representation of words and documents, it is necessary
    to consider mixtures that capture the exchangeability of both. At the base of
    this methodology of thought lie the roots of the LDA model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `sklearn.decomposition.LatentDirichletAllocation` function: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing
    Text Data*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Exchangeability and de Finetti’s Theorem* (from the University of
    Oxford): [http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf](http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for LDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, *Using LDA to classify text documents*, we have seen
    how to use the LDA algorithm for topic modeling. We have seen that, before constructing
    the algorithm, the dataset must be appropriately processed so as to prepare the
    data in a format compatible with the input provided by the LDA model. In this
    recipe, we will analyze in detail these procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will analyze the procedures necessary to transform the data
    contained in a specific dataset. This data will then be used as input for an algorithm
    based on the LDA method.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to prepare data for LDA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `PrepDataLDA.py` file that''s already provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a series of sentences from which we want to extract the topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In the sentences we have just defined, there are topics that are repeated with
    different meanings. It is not easy to derive a link between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We insert these sentences into a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the elements that we will use in the transformation procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform the transformation on all the phrases, it is necessary to set a
    loop that just goes through the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start preparing the data. **Tokenization** is the process of dividing
    text into a set of meaningful pieces. These pieces are called **tokens**. For
    example, we can divide a chunk of text into words, or we can divide it into sentences.
    Let''s start with sentence tokenization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move on to the removal of meaningless words. There are some words in
    typical English sentences that do not take on significant significance for the
    construction of a topic model. For example, conjunctions and articles do not help
    identify topics. These terms are called **stop words** and must be removed from
    our token list. These terms (stop words) change according to the context in which
    we operate. Let''s remove the stop words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The last phase of data preparation concerns the stemming. The goal of stemming
    is to reduce these different forms into a common base form. This uses a heuristic
    process to cut off the ends of words to extract the base form. Let''s make the
    stemming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to add the element obtained to the text list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have to turn our token list into a dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s build a document term matrix using tokenized documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build an LDA model and print the topics extracted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data preparation is essential for the creation of a topic model. The preparation
    of the data goes through the following procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization**: The conversion of a document into its atomic elements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop words**: Removes meaningless words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming**: The fusion of equivalent words in meaning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preparation depends on the type of text we are processing. In some cases,
    it is necessary to carry out further operations before submitting the data to
    the LDA algorithm. For example, punctuation removal can be one of them, as well
    as the removal of special characters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to *Tokenization* (from the NLP group at Stanford University): [https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Stemming and lemmatization* (from Stanford University): [https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Dropping common terms: stop words* (from Stanford University): [https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
