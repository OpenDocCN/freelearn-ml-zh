- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Introducing Artificial Neural Network Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍人工神经网络建模
- en: '**Artificial neural networks** (**ANNs**) include data structures and algorithms
    for learning and classifying data. Through neural network techniques, a program
    can learn through examples and create an internal structure of rules to classify
    different inputs. MATLAB provides algorithms, pre-trained models, and apps to
    create, train, visualize, and simulate ANNs. In this chapter, we will see how
    to use MATLAB to build an ANN-based model to predict values and classify data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**）包括用于学习和分类数据的数据结构和算法。通过神经网络技术，程序可以通过示例学习并创建一个内部规则结构来分类不同的输入。MATLAB提供了算法、预训练模型和应用程序来创建、训练、可视化和模拟ANNs。在本章中，我们将了解如何使用MATLAB构建基于ANNs的模型来预测值和分类数据。'
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Getting started with ANNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始学习ANNs
- en: Training and testing an ANN model in MATLAB
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MATLAB中训练和测试ANNs模型
- en: Understanding data fitting with ANNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ANNs理解数据拟合
- en: Discovering pattern recognition using ANNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ANNs发现模式识别
- en: Building a clustering application with an ANN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ANNs构建聚类应用程序
- en: Exploring advanced optimization techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索高级优化技术
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will introduce basic concepts relating to machine learning.
    To understand these topics, a basic knowledge of algebra and mathematical modeling
    is needed. You will also need a working knowledge of the MATLAB environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍与机器学习相关的基本概念。为了理解这些主题，需要具备代数和数学建模的基本知识。您还需要熟悉MATLAB环境。
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中的MATLAB代码，您需要以下文件（可在GitHub上找到：[https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)）：
- en: '`ANNFitting.m`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ANNFitting.m`'
- en: '`ANNPatReg.m`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ANNPatReg.m`'
- en: Getting started with ANNs
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习ANNs
- en: Serial computers and their programs exhibit remarkable capabilities in executing
    tasks that involve repetitive, well-defined operations, prioritizing accuracy,
    reliability, and speed. While these information-processing systems are highly
    valuable, they lack true intelligence. The sole element of intelligence lies with
    the programmer who comprehends the task and formulates the program. To achieve
    true **artificial intelligence** (**AI**), a system must possess the ability to
    solve problems that humans consider simple, trivial, and intuitive.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 串行计算机及其程序在执行涉及重复、定义明确的操作的任务时表现出非凡的能力，优先考虑准确性、可靠性和速度。虽然这些信息处理系统非常有价值，但它们缺乏真正的智能。唯一的智能元素在于理解任务并制定程序的程序员。为了实现真正的**人工智能**（**AI**），系统必须具备解决人类认为简单、琐碎和直观的问题的能力。
- en: Basic concepts relating to ANNs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与ANNs相关的基本概念
- en: ANNs are designed to emulate the intricate workings of biological nervous systems,
    comprising a vast array of nerve cells or neurons interconnected in a complex
    network. Typically, each neuron establishes connections with tens of thousands
    of other neurons, resulting in hundreds of billions of connections. The emergence
    of intelligent behavior stems from the myriad interactions among these interconnected
    units.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs（人工神经网络）旨在模拟生物神经系统的复杂运作，由大量相互连接的神经元组成，这些神经元在复杂的网络中交织在一起。通常，每个神经元与其他成千上万的神经元建立连接，从而产生数百亿个连接。智能行为的出现源于这些相互连接单元之间无数的交互。
- en: In this network, specific units serve distinct functions. Some units act as
    receivers of information from the environment, while others respond to stimuli
    in the environment. Certain units, known as hidden units, solely communicate within
    the network, concealed from direct interaction with the external environment.
    Overall, the neural network’s structure involves input units, output units, and
    hidden units working in concert to process information and exhibit intelligent
    behavior.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络中，特定的单元执行不同的功能。一些单元作为从环境中接收信息的接收器，而其他单元则对环境中的刺激做出反应。某些单元被称为隐藏单元，它们仅在网络内部进行通信，不与外部环境直接交互。总体而言，神经网络的结构涉及输入单元、输出单元和隐藏单元协同工作以处理信息和展示智能行为。
- en: 'Each unit in the neural network executes a simple operation: it activates when
    the cumulative received signal surpasses an activation threshold. Once active,
    the unit transmits a signal to other connected units through communication channels.
    These connections act as filters, transforming the messages into excitatory or
    inhibitory signals, and adjusting their intensity based on individual characteristics.
    Remarkably, the network’s input-output link, or transfer function, is not explicitly
    programmed but rather acquired through a learning process using empirical data,
    which can be supervised, unsupervised, or reinforcement learning.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每个单元执行一个简单的操作：当累积接收到的信号超过激活阈值时，它会被激活。一旦激活，该单元将通过通信通道向其他连接的单元发送信号。这些连接充当过滤器，将消息转换为兴奋或抑制信号，并根据个体特征调整其强度。值得注意的是，网络的输入-输出链接，或传递函数，不是通过显式编程获得的，而是通过使用经验数据的学习过程获得的，这个过程可以是监督学习、无监督学习或强化学习。
- en: ANNs are a type of AI that can learn from experience. This learning is possible
    thanks to their structure, which is similar to that of the human brain. Like the
    human brain, ANNs are composed of a large number of nodes, which are connected
    to each other by connections. Nodes process information, and connections determine
    how information is transmitted from one node to another. ANNs learn from experience
    through a process called supervised learning. In this process, ANNs are provided
    with a dataset of desired input and output examples. ANNs then use this data to
    learn to map inputs to desired outputs. For example, an ANN that needs to learn
    to recognize images of dogs is provided with a dataset of images of dogs and images
    of other animals. The ANN uses these images to learn to distinguish between dogs
    and other animals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（ANNs）是一种可以从经验中学习的AI类型。这种学习之所以可能，是因为它们的结构类似于人脑。像人脑一样，神经网络由大量节点组成，这些节点通过连接相互连接。节点处理信息，连接决定了信息如何从一个节点传输到另一个节点。神经网络通过称为监督学习的过程从经验中学习。在这个过程中，神经网络被提供了一组期望的输入和输出示例的数据集。然后，神经网络使用这些数据来学习将输入映射到期望的输出。例如，需要学习识别狗的图像的神经网络被提供了一组狗的图像和其他动物的图像数据集。神经网络使用这些图像来学习区分狗和其他动物。
- en: Neural networks operate in parallel, enabling them to handle multiple data simultaneously,
    in contrast to serial computers, which process data individually and sequentially.
    Although individual neurons may be relatively slow, the parallel nature of neural
    networks accounts for the brain’s higher processing speed when tackling tasks
    requiring the simultaneous handling of numerous data points, such as visual object
    recognition. This remarkable system exhibits robust noise immunity, akin to a
    sophisticated statistical model. Even in the event of some unit malfunctions,
    the overall network performance may experience reductions, but a complete system
    shutdown is unlikely to occur.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络并行运行，这使得它们能够同时处理多个数据，与逐个且按顺序处理数据的串行计算机相比。尽管单个神经元可能相对较慢，但神经网络的并行特性解释了在处理需要同时处理大量数据点的任务时，如视觉物体识别，大脑的高处理速度。这个非凡的系统表现出强大的抗噪声能力，类似于复杂的统计模型。即使在某些单元出现故障的情况下，整体网络性能可能会下降，但完全的系统关闭不太可能发生。
- en: Nevertheless, the latest generation of neural network software demands a solid
    understanding of statistics. Despite their seemingly immediate usability, users
    must not be deceived, as they can quickly make predictions or classifications
    within certain limitations. From an industrial perspective, these networks prove
    effective when historical data is available for processing with neural algorithms.
    This capability is particularly valuable in production environments as it facilitates
    data extraction and model creation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最新一代的神经网络软件需要具备扎实的统计学知识。尽管它们看起来具有即时的可用性，但用户不应被欺骗，因为它们可以在某些限制内快速做出预测或分类。从工业角度来看，当有历史数据可用并使用神经网络算法进行处理时，这些网络证明是有效的。这种能力在生产环境中尤其有价值，因为它促进了数据提取和模型创建。
- en: It’s essential to note that while models generated by neural networks are highly
    efficient, they lack explanations in human symbolic language. The outcomes must
    be accepted as they are, leading to the characterization of neural networks as
    black boxes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意的是，虽然神经网络生成的模型非常高效，但它们缺乏人类符号语言中的解释。结果必须按原样接受，这导致神经网络被描述为黑盒。
- en: Like any modeling algorithm, the efficiency of neural networks relies heavily
    on the careful selection of predictive variables. These networks require a training
    phase to establish individual neuron weights, which can be time-consuming when
    dealing with many records and variables. Unlike conventional models, neural networks
    lack theorems or definitive guidelines, making the success of a network heavily
    dependent on the creator’s experience.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何建模算法一样，神经网络的效率在很大程度上依赖于预测变量的谨慎选择。这些网络需要一个训练阶段来建立单个神经元的权重，当处理大量记录和变量时，这个过程可能会很耗时。与传统的模型不同，神经网络缺乏定理或明确的指导方针，这使得网络的成功高度依赖于创造者的经验。
- en: Neural networks find their utility in scenarios where data may be partially
    inaccurate or where analytical models are unavailable for problem-solving. They
    are commonly used in Optical Character Recognition (OCR) software, facial recognition
    systems, and other applications that handle error-prone or noisy data. Moreover,
    they are widely employed in data mining analysis and serve as tools for forecasting
    in financial and meteorological domains. In recent years, their significance has
    substantially increased in the field of bioinformatics, where they are instrumental
    in identifying functional and structural patterns in nucleic acids and proteins.
    By providing a comprehensive set of input data, the network can produce the most
    probable output.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在数据可能部分不准确或分析模型不可用于解决问题的场景中找到其用途。它们通常用于光学字符识别（OCR）软件、面部识别系统和其他处理易出错或噪声数据的应用程序。此外，它们在数据挖掘分析和作为金融和气象领域预测工具的广泛使用。近年来，它们在生物信息学领域的意义显著增加，在那里它们在识别核酸和蛋白质中的功能和结构模式方面发挥着关键作用。通过提供一套全面的输入数据，网络可以产生最可能的输出。
- en: Understanding how perceptrons work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解感知器的工作原理
- en: The fundamental unit of a neural network is the **perceptron**, which emulates
    the essential functions of a biological neuron. It evaluates the strength of each
    input, accumulates these inputs, and then compares the sum to a specific threshold.
    Based on this comparison, the perceptron determines the output value. The neuron’s
    basic structure is well understood, and researchers have identified the primary
    biochemical reactions that govern its activity. As such, a neuron can be regarded
    as the elemental computational unit of the brain. Within the human brain, approximately
    100 distinct classes of neurons have been identified, each contributing to the
    intricate neural network responsible for our cognitive processes and abilities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本单元是**感知器**，它模拟了生物神经的基本功能。它评估每个输入的强度，累积这些输入，然后将总和与一个特定的阈值进行比较。基于这个比较，感知器确定输出值。神经元的基本结构已被充分理解，研究人员已经确定了控制其活动的主要生化反应。因此，神经元可以被视为大脑的基本计算单元。在人类大脑中，已经确定了大约100种不同的神经元类别，每种类别都为负责我们认知过程和能力的复杂神经网络做出贡献。
- en: '![Figure 5.1 – Understanding the perceptron scheme](img/B21156_05_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 理解感知器方案](img/B21156_05_01.jpg)'
- en: Figure 5.1 – Understanding the perceptron scheme
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 理解感知器方案
- en: The primary function of a biological neuron is to generate an electrical potential
    that travels along its **axon** (neuron output) when the electrical activity at
    the neuron’s cell body surpasses a specific threshold. The neuron’s input is received
    through a set of fibers called **dendrites**, which contact the axons of other
    neurons, transmitting electrical potentials from them. The point of connection
    between an axon of one neuron and the dendrite of another neuron is referred to
    as a **synapse**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的根本功能是在神经元细胞体的电活动超过特定阈值时，在其**轴突**（神经元输出）上产生一个电势。神经元的输入通过称为**树突**的一组纤维接收，这些树突接触其他神经元的轴突，从它们那里传递电势。一个神经元的轴突与另一个神经元的树突之间的连接点被称为**突触**。
- en: 'The synapse possesses the ability to regulate the electrical pulse emanating
    from the axon. The electrical potential generated by a neuron is essentially binary:
    an on/off state. If the neuron’s electrical activity surpasses a specific threshold,
    an impulse is generated; otherwise, no impulse occurs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 突触具有调节从轴突发出的电脉冲的能力。神经元产生的电势基本上是二元的：开/关状态。如果神经元的电活动超过特定阈值，就会产生一个脉冲；否则，不会产生脉冲。
- en: Notably, the intensity of the generated pulse remains consistent across different
    neurons. As the potential propagates along the axon and reaches the synapse connected
    to another neuron’s dendrite, the post-synaptic potential relies on the biochemical
    characteristics of the synapse. Despite having the same pre-synaptic potential,
    two distinct synapses may generate varying post-synaptic potentials. In other
    words, the synapse modulates and weighs the input potential before transmission.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，生成的脉冲强度在不同神经元之间保持一致。当潜力沿着轴突传播并达到连接到另一个神经元树突的突触时，突触后电位依赖于突触的生化特性。尽管具有相同的突触前电位，两个不同的突触可能会产生不同的突触后电位。换句话说，突触在传输之前调节并权衡输入电位。
- en: Post-synaptic potentials continue through the neuron’s dendrites and accumulate
    at the soma level. Only when the sum of these potentials surpasses a certain threshold
    does the neuron trigger the potential to propagate through its axon. Both biological
    neurons and artificial neurons receive multiple inputs through dendrites. The
    artificial neuron aggregates these various input values and computes the result.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 突触后电位通过神经元的树突继续传播并在细胞体水平积累。只有当这些潜力的总和超过某个特定阈值时，神经元才会触发潜力通过其轴突传播。生物神经元和人工神经元都通过树突接收多个输入。人工神经元将这些各种输入值聚合并计算结果。
- en: If the computed value exceeds a particular threshold, the artificial neuron
    produces an output signal or potential; otherwise, it remains inactive.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算值超过特定阈值，人工神经元会产生输出信号或潜力；否则，它保持不活跃。
- en: The initial function implemented by the artificial neuron is the algebraic sum
    of its inputs, which serves to construct the system’s response. When simulating
    a phenomenon, the system may encounter errors, necessitating appropriate corrections.
    To achieve this, each input is assigned a weight, a numerical value that modulates
    its impact on the total sum, determining the neuron’s potential.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元最初实现的功能是其输入的代数和，这有助于构建系统的响应。在模拟现象时，系统可能会遇到错误，需要适当的纠正。为了实现这一点，每个输入都被分配了一个权重，这是一个数值，它调节其对总和的影响，从而确定神经元的潜力。
- en: In other words, each input contributes differently to the determination of the
    threshold value and potential triggering, akin to the characteristic of biological
    neurons involving synapses between the axon of one neuron and the dendrite of
    another.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每个输入对确定阈值值和触发潜力的贡献各不相同，类似于涉及一个神经元轴突与另一个神经元树突之间突触的生物神经元特性。
- en: From the perspective of the post-synaptic neuron, the inputs consist of potentials
    from other neurons whose axons synapse with their dendrites, and these inputs
    are precisely modulated by the synapses. Some inputs may exert a stronger influence
    on the total sum, while others could even be inhibitory, reducing the overall
    sum and thereby lowering the probability of exceeding the threshold and triggering
    a potential.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从突触后神经元的视角来看，输入包括来自其他神经元的潜力，这些神经元的轴突与它们的树突形成突触，并且这些输入被突触精确调节。一些输入可能对总和产生更强的影响，而其他输入甚至可能是抑制性的，减少总和并因此降低超过阈值并触发潜力的概率。
- en: This essential property of biological systems is mathematically modeled in connection
    systems using the concept of weights. Each connection is assigned a numeric value
    as its weight, which is multiplied by the input value. Consequently, the input’s
    effect on the total sum is determined by the magnitude of its weight.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种生物系统的基本特性在连接系统中使用权重概念进行数学建模。每个连接都被分配一个数值作为其权重，该数值乘以输入值。因此，输入对总和的影响由其权重的幅度决定。
- en: Activation function to introduce non-linearity
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数用于引入非线性
- en: In our previous discussion, we explored the weighted sum function introduced
    by incorporating the concept of weights. Now, let’s delve into another property
    of the artificial neuron, once again inspired by the behavior of biological neurons.
    As mentioned earlier, the biological neuron sums up the post-synaptic potentials
    of its dendrites at the soma level. However, this summation is not a simple algebraic
    addition of these potentials. Various factors, such as the passive resistance
    of the neuron membrane, come into play, making the actual summation a function
    that is typically non-linear.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的讨论中，我们探讨了通过引入权重概念引入的加权求和函数。现在，让我们深入探讨人工神经元的一个特性，这个特性再次受到了生物神经元行为的启发。如前所述，生物神经元在树突水平上对其突触后电位进行求和。然而，这种求和并不是这些电位的简单代数加和。各种因素，如神经元膜的无被动电阻，都会发挥作用，使得实际的求和通常是非线性的函数。
- en: Similarly, artificial neurons compute the weighted inputs and then modify the
    result using a specific function. This function is referred to as the activation
    function, which is applied to the output of the neuron to determine its true potential.
    The activation function plays a crucial role in shaping the behavior of the neuron
    and ultimately influences the outcome of the neural network’s computation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，人工神经元计算加权输入，然后使用一个特定的函数修改结果。这个函数被称为激活函数，它应用于神经元的输出以确定其真实潜力。激活函数在塑造神经元行为和最终影响神经网络计算结果方面起着至关重要的作用。
- en: The activation function takes the weighted sum of inputs and an additional bias
    term and applies a specific mathematical operation to produce the neuron’s output.
    This output, often referred to as the activation or the post-activation value,
    is then used as input to the subsequent layers in the neural network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数对输入的加权总和以及一个额外的偏置项应用特定的数学运算以产生神经元的输出。这个输出，通常被称为激活或后激活值，然后作为输入传递到神经网络后续的层。
- en: 'Different types of activation functions are utilized in neural networks, each
    with its unique characteristics and applications. Common activation functions
    include the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中使用了不同类型的激活函数，每种都有其独特的特性和应用。常见的激活函数包括以下几种：
- en: '`1` if the input is greater than or equal to a threshold, and `0` otherwise:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果输入大于或等于一个阈值，则为 `1`，否则为 `0`：
- en: f(x) = 0 if x < 0, 1 if x ≥ 0
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = 0 if x < 0, 1 if x ≥ 0
- en: '`0` and `1`. It was commonly used in the past but has fallen out of favor in
    deeper networks due to the vanishing gradient problem:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` 和 `1`。它过去常用，但由于梯度消失问题，在更深层的网络中不再受欢迎：'
- en: f(x) =  1 _ 1 + e −x
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = 1 / (1 + e^(-x))
- en: '`0`. It helps mitigate the vanishing gradient problem and accelerates convergence
    in deep networks:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`。它有助于减轻梯度消失问题，并加速深层网络的收敛：'
- en: f(x) = 0 if x < 0, x if x ≥ 0
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = 0 if x < 0, x if x ≥ 0
- en: '`-1` and `1`. It is symmetric around the origin, providing a better range for
    the gradient compared to the sigmoid function:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-1` 和 `1`。它在原点周围是对称的，与sigmoid函数相比，提供了更好的梯度范围：'
- en: f(x) = tanh (x)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = tanh(x)
- en: '`1`:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`：'
- en: f (x) i =  e −x i _ ∑ i=1 k  e −x i
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = e^(-x_i) * ∑(i=1 to k) e^(-x_i)
- en: '**Exponential linear unit (ELU)**: ELU is an activation function used in ANNs.
    It is a smooth, non-saturating function that can handle both positive and negative
    inputs. ELU is a more recent activation function than ReLU, which is also widely
    used:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数线性单元（ELU）**：ELU是一种在人工神经网络中使用的激活函数。它是一个平滑、非饱和函数，可以处理正负输入。ELU比ReLU更晚出现，ReLU也是一种广泛使用的激活函数：'
- en: f(x) = x if x > 0
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = x if x > 0
- en: alpha * (exp(x) − 1) if x < 0
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: alpha * (exp(x) - 1) if x < 0
- en: Here, *x* is the input to the ELU function and *α* is a hyperparameter that
    controls the steepness of the negative slope. The default value of *α* is `1.0`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是ELU函数的输入，*α* 是一个控制负斜率陡度的超参数。*α* 的默认值是 `1.0`。
- en: The ELU function is a smooth, non-saturating function that can handle both positive
    and negative inputs. This makes it a good choice for tasks that involve both types
    of inputs, such as image recognition and natural language processing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ELU函数是一个平滑、非饱和函数，可以处理正负输入。这使得它对于涉及这两种类型输入的任务（如图像识别和自然语言处理）是一个很好的选择。
- en: The choice of the activation function influences the neural network’s performance,
    training speed, and ability to capture complex relationships in the data. Different
    activation functions may be used in different layers of the network, depending
    on the specific problem and architectural considerations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择会影响神经网络的性能、训练速度以及捕捉数据中复杂关系的能力。根据具体问题和架构考虑，网络的不同层可能会使用不同的激活函数。
- en: ANN’s architecture explained
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ANN架构解释
- en: Having discussed the properties of an artificial neuron, we will now delve into
    the architecture of a neural network. This entails both a physical illustration
    of the network’s structure and a determination of the role each neuron plays within
    this framework. Consider a scenario with multiple inputs and nodes, such that
    each input is connected to every node. Similarly, each output node receives connections
    from all inputs. Each output node possesses the characteristics described earlier
    and carries out its computations in conjunction with the other nodes. Upon introducing
    an input pattern, the output values are influenced either by the input values
    themselves or by the network’s weights. The weights play a crucial role in the
    network, determining the extent to which a particular input influences a specific
    node. The collection of nodes in the structure is commonly referred to as a layer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了人工神经元的特性之后，我们现在将深入研究神经网络的架构。这包括网络结构的物理说明以及确定每个神经元在这个框架中的作用。考虑一个有多个输入和节点的场景，其中每个输入都与每个节点相连。同样，每个输出节点也接收来自所有输入的连接。每个输出节点具有前面描述的特性，并与其他节点一起执行计算。在引入输入模式后，输出值要么受输入值本身的影响，要么受网络权重的影响。在网络中，节点集合通常被称为层。
- en: 'Neural networks are organized into layers, each serving a specific purpose
    in information processing. The primary types of layers in a neural network include
    the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络被组织成层，每一层在信息处理中都有其特定的作用。神经网络中的主要层类型包括以下几种：
- en: '**Input layer**: The initial layer that receives the input data and passes
    it on to the subsequent layers.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：接收输入数据并将其传递给后续层的初始层。'
- en: '**Hidden layers**: Intermediate layers between the input and output layers.
    These layers process the data and extract relevant features through complex transformations.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：位于输入层和输出层之间的中间层。这些层通过复杂的转换处理数据并提取相关特征。'
- en: '**Output layer**: The final layer that produces the network’s output or predictions
    based on the processed information from the hidden layers.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：产生网络输出或预测的最终层，基于从隐藏层处理过的信息。'
- en: The number of hidden layers and nodes within them, along with the choice of
    activation functions and weights, constitute the architecture of the neural network.
    The architecture (*Figure 5**.2*) significantly influences the network’s ability
    to learn from data, generalize to new examples, and perform specific tasks efficiently.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的数量以及其中的节点数，以及激活函数和权重的选择，构成了神经网络的架构。架构（*图5**.2*）显著影响了网络从数据中学习、泛化到新示例以及高效执行特定任务的能力。
- en: '![Figure 5.2 – ANN architecture with weights and activation function](img/B21156_05_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 带权重和激活函数的ANN架构](img/B21156_05_02.jpg)'
- en: Figure 5.2 – ANN architecture with weights and activation function
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 带权重和激活函数的ANN架构
- en: Neural networks can take the form of multiple layers, and each layer added enhances
    the network’s computational capacity. Inputs are numerical values that undergo
    evaluation through the weights of connections with the first layer of nodes, known
    as the hidden layer. In this hidden layer, each node conducts computations as
    described earlier, leading to the generation of a potential that then propagates
    to the nodes of the output layer. The potentials produced by the output nodes
    collectively represent the final output calculated by the neural network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以由多层组成，每增加一层都会增强网络的计算能力。输入是数值，通过与第一层节点（称为隐藏层）的连接权重进行评估。在这个隐藏层中，每个节点按照前面描述的方式进行计算，从而生成潜在的输出，然后传播到输出层的节点。输出节点产生的潜在值共同代表了神经网络计算出的最终输出。
- en: The architecture of a neural network refers to the specific way in which the
    nodes are interconnected. In the case of feedforward neural networks, which are
    characterized by the architecture shown in the previous figure, the activation
    of input nodes propagates forward through the hidden layer and further to the
    output layer. Changing the connections between nodes alters the network’s architecture.
    This not only yields practical consequences in terms of the network’s computational
    capacity but also carries significant theoretical implications related to the
    concept of learning. The arrangement of nodes in the network affects its ability
    to learn from data and perform specific tasks, making architecture design a crucial
    aspect of neural network development.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的架构指的是节点相互连接的具体方式。在前面图表中显示的前馈神经网络的情况下，输入节点的激活通过隐藏层向前传播，进一步传播到输出层。改变节点之间的连接会改变网络的架构。这不仅对网络的计算能力产生实际影响，而且与学习概念相关的理论意义也非常重大。网络中节点的排列会影响其从数据中学习以及执行特定任务的能力，使架构设计成为神经网络开发的一个关键方面。
- en: After analyzing the basic concepts of ANNs, we now need to pay attention to
    how these algorithms are trained.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析了ANN的基本概念之后，我们现在需要关注这些算法是如何被训练的。
- en: Training and testing an ANN model in MATLAB
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MATLAB中训练和测试ANN模型
- en: 'In the previous section, we saw the architecture of an ANN. It imposes two
    layers, input and output, which cannot be altered. Consequently, the critical
    factor lies in the number of hidden layers we consider. The size of a neural network
    is defined by the number of hidden neurons. Determining the optimal size of the
    network remains an ongoing challenge, as no analytical solution has been discovered
    to date. One approach to tackle this problem is to employ a heuristic method:
    creating various networks with increasing complexity, using a subset of the training
    data, and monitoring the error on a validation subset simultaneously. After completing
    the training process, the network with the lowest validation error is chosen as
    the preferred one.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了ANN的架构。它强加了两个层，输入层和输出层，这些层不能改变。因此，关键因素在于我们考虑的隐藏层数量。神经网络的大小由隐藏神经元的数量定义。确定网络的最佳大小仍然是一个持续性的挑战，因为迄今为止还没有发现解析解决方案。解决这个问题的方法之一是采用启发式方法：创建具有递增复杂性的各种网络，使用训练数据的一个子集，并同时在验证子集上监控错误。完成训练过程后，选择具有最低验证错误的网络作为首选。
- en: How to train an ANN
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何训练ANN
- en: Let’s discuss the process of choosing the number of layers. The number of input
    nodes is fixed based on the number of features in the input data, while the number
    of output nodes is determined by the number of outcomes to be modeled or the class
    levels in the outcome. The real challenge lies in determining the appropriate
    number of neurons for the hidden layer. Unfortunately, there is no analytical
    method to accomplish this task. The optimal number of neurons depends on various
    factors, such as the number of input nodes, the volume of training data, and the
    complexity of the learning algorithm, among others.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论选择层数的过程。输入节点的数量基于输入数据中的特征数量是固定的，而输出节点的数量则由要建模的结果数量或结果中的类别级别决定。真正的挑战在于确定隐藏层中适当神经元数量。不幸的是，没有解析方法来完成这项任务。最佳神经元数量取决于各种因素，例如输入节点的数量、训练数据量以及学习算法的复杂性等。
- en: Having more neurons in the hidden layer will lead to a model that better fits
    the training data, but it comes with the risk of overfitting, potentially resulting
    in poor generalization on future data. Additionally, neural networks with many
    nodes can be computationally expensive and slow to train.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中拥有更多神经元将导致一个更好地拟合训练数据的模型，但这也伴随着过拟合的风险，可能导致对未来数据的泛化能力较差。此外，具有许多节点的神经网络可能在计算上昂贵且训练缓慢。
- en: To address this, a heuristic approach can be adopted, where different configurations
    are experimented with to find an optimal balance. This trial-and-error method
    allows us to strike a balance between model complexity, accuracy, and computational
    efficiency.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，可以采用启发式方法，通过实验不同的配置来找到最佳平衡。这种试错方法使我们能够在模型复杂性、准确性和计算效率之间取得平衡。
- en: ANNs consist of simple elements that operate in parallel. The connections between
    these elements play a vital role as they dictate the network’s functionalities.
    These connections influence the output through their respective weights, which
    are adjusted during the neural network’s training phase.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由简单的元素组成，这些元素并行操作。这些元素之间的连接起着至关重要的作用，因为它们决定了网络的功能。这些连接通过各自的权重影响输出，这些权重在神经网络训练阶段进行调整。
- en: During training, the network is fine-tuned by modifying the connection weights,
    enabling specific inputs to yield desired outputs. For instance, the network can
    be calibrated by comparing its practical output with the target output we want
    to achieve. This iterative process continues until the network’s output aligns
    with the desired target.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，通过修改连接权重来微调网络，使特定的输入产生期望的输出。例如，可以通过将网络的实际输出与我们要达到的目标输出进行比较来校准网络。这个过程会持续迭代，直到网络的输出与期望的目标相一致。
- en: To obtain dependable results, a substantial number of input/target pairs are
    required to shape the network appropriately. This training process ensures that
    the neural network can accurately produce the desired outputs for a variety of
    inputs, making it a reliable tool for various tasks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得可靠的结果，需要大量的输入/目标对来适当地塑造网络。这个过程确保神经网络能够准确地产生各种输入的期望输出，使其成为各种任务的可靠工具。
- en: The adjustment of these weights is determined by the specific algorithm we choose
    to adopt. In the following practical examples, we will discuss and refer to various
    algorithms that govern the process of weight adjustment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重的调整取决于我们选择的特定算法。在以下实际示例中，我们将讨论和参考各种控制权重调整过程的算法。
- en: Introducing the MATLAB Neural Network Toolbox
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 MATLAB 神经网络工具箱
- en: The **Neural Network Toolbox** offers a range of algorithms, pre-trained models,
    and apps that enable users to create, train, visualize, and simulate neural networks.
    It supports both shallow neural networks (with one hidden layer) and deep neural
    networks (with multiple hidden layers). With these tools, various tasks, such
    as classification, regression, clustering, dimensionality reduction, time-series
    forecasting, and dynamic system modeling and control, can be performed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络工具箱**提供了一系列算法、预训练模型和应用程序，使用户能够创建、训练、可视化和模拟神经网络。它支持浅层神经网络（含一个隐藏层）和深层神经网络（含多个隐藏层）。使用这些工具，可以执行各种任务，如分类、回归、聚类、降维、时间序列预测以及动态系统建模和控制。'
- en: 'There are four primary ways to utilize the Neural Network Toolbox:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 利用神经网络工具箱有四种主要方式：
- en: '`nnstart` command, granting access to automatic tasks such as function fitting
    (`nftool`), pattern recognition (`nprtool`), data clustering (`nctool`), and time-series
    analysis (`ntstool`).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nnstart` 命令，提供对自动任务（如函数拟合 `nftool`）、模式识别 `nprtool`）、数据聚类 `nctool`）和时间序列分析
    `ntstool`）的访问。'
- en: '**Basic command-line operations**: For greater flexibility, users can utilize
    command-line operations. While more knowledge is required, this method allows
    users to have complete control over the process, without relying on menus and
    icons typically found in the GUI.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基本命令行操作**：为了提高灵活性，用户可以利用命令行操作。虽然需要更多的知识，但这种方法允许用户完全控制过程，而无需依赖于 GUI 中通常找到的菜单和图标。'
- en: '**Customizing the toolbox**: Users can customize the toolbox by creating their
    own neural networks with arbitrary connections. Existing toolbox training features
    in the GUI can be used to continue training these custom networks.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制工具箱**：用户可以通过创建自己的具有任意连接的神经网络来自定义工具箱。现有的工具箱训练功能在 GUI 中可以使用，以继续训练这些自定义网络。'
- en: '**Modifying toolbox functions**: All computational components in the toolbox
    are written in MATLAB code and are fully accessible, allowing users to modify
    and tailor them to specific needs.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改工具箱功能**：工具箱中的所有计算组件都使用 MATLAB 代码编写，并且完全可访问，使用户能够根据特定需求修改和定制它们。'
- en: This toolbox caters to users of all levels, from beginners to experts. It offers
    simple tools that guide new users through specific applications and more complex
    tools that enable experts to customize networks and experiment with new architectures.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具箱适用于所有级别的用户，从初学者到专家。它提供了简单的工具，引导新用户通过特定应用，以及更复杂的工具，使专家能够自定义网络并尝试新的架构。
- en: 'Regardless of the approach chosen, a proper analysis using neural networks
    should encompass the following steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种方法，使用神经网络进行适当的分析应包括以下步骤：
- en: Data collection
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Network creation
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络创建
- en: Network configuration
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络配置
- en: Weight and bias initialization
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重和偏置初始化
- en: Network training
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络训练
- en: Network validation
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络验证
- en: Network testing
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络测试
- en: 'By following these steps, users can effectively apply neural networks to various
    problems and tasks. They are explained in more detail here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，用户可以有效地将神经网络应用于各种问题和任务。这些步骤的详细解释如下：
- en: The first step in the process involves collecting the data to be analyzed, which
    is typically done outside the MATLAB environment. This data collection phase is
    crucial, as the quality of the data will significantly impact the final results
    and the ability to extract meaningful insights.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过程的第一步涉及收集要分析的数据，这通常在MATLAB环境之外完成。数据收集阶段至关重要，因为数据的质量将显著影响最终结果和提取有意义的见解的能力。
- en: Next, we proceed to create the neural network using various functions available
    in the toolbox. These functions allow us to build the network through the chosen
    algorithm, resulting in the creation of a neural network object. This object stores
    all the necessary information defining the neural network’s properties, such as
    its architecture, subobject structures, functions, and weight and bias values.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用工具箱中提供的各种函数来创建神经网络。这些函数允许我们通过选择的算法构建网络，从而创建一个神经网络对象。该对象存储了定义神经网络特性的所有必要信息，例如其架构、子对象结构、函数以及权重和偏置值。
- en: The third step is network configuration, where we examine input and output data,
    set the dimensions of the network to fit the data, and choose appropriate input
    and output processing settings to enhance network performance. This configuration
    step is usually performed automatically when the training function is called but
    can also be done manually using the configuration function.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三步是网络配置，其中我们检查输入和输出数据，设置网络的维度以适应数据，并选择合适的输入和输出处理设置以增强网络性能。此配置步骤通常在调用训练函数时自动执行，但也可以使用配置函数手动完成。
- en: After configuration, the fourth step involves initializing the weights and biases.
    We set initial values from which the network will begin its training process.
    This initialization is usually done automatically based on the chosen training
    algorithm, but users can also set custom values if needed.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置之后，第四步涉及初始化权重和偏置。我们设置初始值，网络将从这些值开始其训练过程。这种初始化通常基于选择的训练算法自动完成，但用户如果需要也可以设置自定义值。
- en: The fifth step is network training, which is a critical phase of the process.
    During training, the weights and biases are fine-tuned to optimize the network’s
    performance. This stage is crucial for the network’s ability to generalize well
    with new, unseen data. A portion of the collected data (typically around 70% of
    available cases) is used for training.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第五步是网络训练，这是过程中的关键阶段。在训练过程中，权重和偏置被微调以优化网络性能。这一阶段对于网络能够很好地泛化新、未见数据至关重要。通常，收集到的数据（大约为可用案例的70%）用于训练。
- en: Next, in the sixth step, network validation takes place. Here, a fraction of
    randomly selected data (usually around 15% of available cases) is passed through
    the network to estimate how well the model has been trained. The results obtained
    during this phase help determine whether the chosen model adequately reflects
    the initial expectations or whether adjustments are needed.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在第六步，进行网络验证。在这里，将随机选择的一部分数据（通常为可用案例的约15%）通过网络来估计模型训练的效果。这一阶段获得的结果有助于确定所选模型是否充分反映了初始期望，或者是否需要调整。
- en: Finally, in the last step, we utilize the trained network. A portion of the
    collected data (approximately 15% of available cases) is used to test the network’s
    performance. The trained neural network object can then be saved and employed
    multiple times with new data as needed. This allows for the reuse of the network
    to make predictions or analyze various datasets.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在最后一步，我们利用训练好的网络。收集到的数据的一部分（大约为可用案例的15%）用于测试网络性能。然后，训练好的神经网络对象可以被保存并多次使用，以便在需要时用新数据进行分析。这允许重复使用网络进行预测或分析各种数据集。
- en: 'The workflow for **neural network design** involves breaking down the collected
    data into three sets: the **training set**, the **validation set**, and the **test
    set**. Let’s describe each of them in detail:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络设计的流程涉及将收集到的数据分为三组：**训练集**、**验证集**和**测试集**。让我们详细描述每一组：
- en: '**Training set (usually 70% of the available cases)**: The training set is
    a collection of examples used to train the neural network and adjust its parameters.
    During the training process, the network learns from the input-output pairs in
    the training set to optimize its internal weights and biases. The goal is to find
    the optimal set of parameters that best captures the underlying patterns and relationships
    in the data. The neural network improves its performance through iterative adjustments
    during the training phase.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集（通常为可用案例的70%）**：训练集是一组示例，用于训练神经网络并调整其参数。在训练过程中，网络从训练集中的输入-输出对中学习，以优化其内部权重和偏差。目标是找到最佳参数集，以最好地捕捉数据中的潜在模式和关系。神经网络通过训练阶段的迭代调整来提高其性能。'
- en: '**Validation set (usually 15% of the available cases)**: The validation set
    is a separate set of examples used to fine-tune the network’s parameters and assess
    its performance during training. It serves as a validation mechanism to prevent
    overfitting, a condition where the network performs well on the training data
    but poorly on new, unseen data. By monitoring the network’s performance on the
    validation set, we can make decisions about the model’s complexity, such as determining
    the optimal number of hidden units or identifying a suitable stopping point for
    the training algorithm. Adjustments based on the validation set help ensure the
    network generalizes well to new data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集（通常为可用案例的15%）**：验证集是一组独立的示例，用于微调网络的参数并在训练期间评估其性能。它作为验证机制，防止过拟合，即网络在训练数据上表现良好，但在新的、未见过的数据上表现较差。通过监控网络在验证集上的性能，我们可以做出关于模型复杂性的决定，例如确定隐藏单元的最佳数量或确定训练算法的合适停止点。基于验证集的调整有助于确保网络对新数据具有良好的泛化能力。'
- en: '**Test set (usually 15% of the available cases)**: The test set is a separate
    and independent collection of examples used solely to evaluate the performance
    of a fully trained neural network. After the training and validation stages, the
    final model is assessed using the test set to estimate its error rate and validate
    its generalization capabilities. It is crucial to refrain from making any further
    adjustments to the model based on the test set evaluation to avoid bias or overfitting.
    The test set provides an unbiased measure of how well the neural network is likely
    to perform on new, real-world data.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集（通常为可用案例的15%）**：测试集是一组独立且独立的示例，仅用于评估完全训练好的神经网络的性能。在训练和验证阶段之后，使用测试集评估最终模型，以估计其错误率并验证其泛化能力。至关重要的一点是，在基于测试集评估的基础上避免对模型进行任何进一步的调整，以避免偏差或过拟合。测试集提供了对神经网络在新、真实世界数据上可能表现如何的无偏度量。'
- en: By dividing the data into these three distinct sets, the neural network design
    workflow ensures that the model is trained, validated, and tested in a controlled
    and reliable manner, allowing for accurate assessments of its performance and
    generalization capabilities.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据分为这三个不同的集合，神经网络设计工作流程确保模型以受控和可靠的方式进行训练、验证和测试，从而允许对其性能和泛化能力进行准确评估。
- en: With a clear understanding of this process, we are now ready to proceed with
    our work on analyzing a practical example of an ANN implementation in MATLAB.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对这一过程有了清晰的理解后，我们现在可以继续我们的工作，分析MATLAB中人工神经网络实现的实际示例。
- en: Understanding data fitting with ANNs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工神经网络理解数据拟合
- en: '**Data fitting** is the process of constructing a curve or mathematical function
    that best matches a given set of collected data points. This curve fitting can
    involve either interpolations, where exact data points are fitted, or smoothing,
    where a smooth function approximates the data. In the context of regression analysis,
    curve fitting is closely related to statistical inference, considering uncertainties
    arising from random errors in observed data.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据拟合**是指构建一个曲线或数学函数，使其与给定的数据点集最佳匹配的过程。这种曲线拟合可能涉及插值，其中精确地拟合数据点，或者平滑，其中平滑函数近似数据。在回归分析中，曲线拟合与统计推断密切相关，考虑到来自观察数据随机误差的不确定性。'
- en: The approximate curves obtained through data fitting have multiple applications.
    They can be used to visualize and display the data, predict function values in
    regions with no available data, and summarize the relationships between multiple
    variables. This process is valuable for understanding and interpreting complex
    datasets, making predictions, and gaining insights from the collected information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据拟合获得的大致曲线具有多种应用。它们可以用于可视化并显示数据，预测无可用数据区域中的函数值，以及总结多个变量之间的关系。这个过程对于理解和解释复杂数据集、进行预测以及从收集到的信息中获得见解非常有价值。
- en: Predicting the trend of a particular distribution using mathematical formulas
    can be challenging, and these formulas may not always accurately represent all
    the data or cover the entire range of existence. To address such cases, machine
    learning algorithms come to the rescue. These algorithms can build models without
    relying on complex mathematical formulas.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数学公式预测特定分布的趋势可能具有挑战性，这些公式可能并不总是能准确代表所有数据或涵盖存在的整个范围。为了解决这类情况，机器学习算法应运而生。这些算法可以构建模型，而不依赖于复杂的数学公式。
- en: ANNs are well suited for data-fitting tasks and trend prediction. They can adapt
    and learn from a given set of inputs and associated target outputs. **Function
    fitting** is the process of training a neural network with such input-output pairs,
    enabling it to form a generalization of the underlying input-output relationship.
    Once trained, the neural network can generate outputs for inputs it has not encountered
    during training.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络非常适合数据拟合和趋势预测任务。它们可以从给定的一组输入及其相关的目标输出中适应和学习。**函数拟合**是通过训练神经网络使用这样的输入-输出对来进行的，使其能够形成底层输入-输出关系的泛化。一旦训练完成，神经网络就可以为它在训练期间未遇到的输入生成输出。
- en: The flexibility of neural networks allows them to capture complex patterns and
    relationships in the data, making them powerful tools for data-fitting and prediction
    tasks. By utilizing machine learning algorithms such as neural networks, we can
    overcome the limitations of traditional mathematical formulas and achieve accurate
    predictions across various datasets and scenarios.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的灵活性使它们能够捕捉数据中的复杂模式和关系，成为数据拟合和预测任务的有力工具。通过利用如神经网络这样的机器学习算法，我们可以克服传统数学公式的局限性，并在各种数据集和场景中实现准确的预测。
- en: The performance of algorithms based on machine learning strongly depends on
    the quality of the data being worked on. The data collection process typically
    occurs outside the MATLAB environment, which means that you need to have a properly
    collected data file ready to initiate an analysis in MATLAB. However, if you don’t
    have access to the data yet and are here to learn, there’s no need to worry because
    MATLAB has a solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的算法的性能强烈依赖于正在处理的数据质量。数据收集过程通常发生在MATLAB环境之外，这意味着您需要准备好一个正确收集的数据文件，以便在MATLAB中启动分析。然而，如果您还没有访问数据，并且在这里学习，无需担心，因为MATLAB有解决方案。
- en: 'The Neural Network Toolbox software provides several sample datasets that you
    can use to experiment with the functionality of the toolbox. These sample datasets
    are readily available and can serve as a starting point for your analysis. To
    explore the available datasets, you can use the following command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络工具箱软件提供了几个样本数据集，您可以使用这些数据集来实验工具箱的功能。这些样本数据集随时可用，可以作为您分析的开端。要探索可用的数据集，您可以使用以下命令：
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A list of datasets sorted by application type will be returned. With these
    sample datasets at your disposal, you can begin your learning journey and gain
    hands-on experience with the Neural Network Toolbox in MATLAB. Now we will work
    on an example of data fitting in MATLAB:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将按应用类型排序的数据集列表将返回。有了这些样本数据集，您就可以开始学习之旅，并在MATLAB的神经网络工具箱中获得实践经验。现在，我们将通过MATLAB中的数据拟合示例来操作：
- en: 'Let’s focus our attention on the dataset for data fitting, specifically, `abalone_dataset`,
    which contains the *abalone shell rings dataset*. To load the dataset into the
    MATLAB workspace, let’s use the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们集中关注数据拟合的数据集，特别是`abalone_dataset`，它包含*鲍鱼壳环数据集*。要将数据集加载到MATLAB工作空间中，让我们使用以下命令：
- en: '[PRE1]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Executing this command will load the data into the arrays named `Input` and
    `Target`. The aim of the model is to estimate the age of an abalone, utilizing
    physical measurements to achieve this prediction. help abalone_dataset
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行此命令将数据加载到名为 `Input` 和 `Target` 的数组中。该模型的目标是估计鲍鱼年龄，通过使用物理测量来实现这一预测。帮助文档：abalone_dataset
- en: A comprehensive description of the dataset is provided, including details such
    as the number of attributes, the total number of items, and a list of variables.
    Additionally, the description offers valuable insights into potential use cases
    for the dataset.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了关于数据集的全面描述，包括属性数量、项目总数以及变量列表等详细信息。此外，描述还提供了关于数据集潜在用例的宝贵见解。
- en: 'Now that we have the data, we have to choose the training algorithm and set
    the architecture network. In MATLAB different functions are available for training.
    To get a list of training algorithms available, we can use the following command:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了数据，我们必须选择训练算法并设置网络架构。在 MATLAB 中，有各种函数可用于训练。要获取可用训练算法的列表，我们可以使用以下命令：
- en: '[PRE2]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LM backpropagation is an optimization algorithm used in training ANNs. It is
    an extension of the standard backpropagation algorithm that improves convergence
    and robustness, especially for nonlinear and ill-conditioned problems. In standard
    backpropagation, the algorithm adjusts the weights of the neural network using
    the gradient of the error with respect to the weights. However, in some cases,
    this process can be slow, and the algorithm might get stuck in local minima. LM
    backpropagation addresses this issue by incorporating the LM optimization method,
    which is commonly used in nonlinear least squares fitting problems. The LM algorithm
    combines the ideas of both gradient descent and Gauss-Newton methods.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LM 反向传播是一种用于训练人工神经网络的优化算法。它是标准反向传播算法的扩展，提高了收敛性和鲁棒性，尤其是在非线性和不稳定问题中。在标准反向传播中，算法通过使用权重相对于误差的梯度来调整神经网络的权重。然而，在某些情况下，这个过程可能很慢，算法可能会陷入局部最小值。LM
    反向传播通过结合 LM 优化方法来解决此问题，该方法常用于非线性最小二乘拟合问题。LM 算法结合了梯度下降和高斯-牛顿方法的思想。
- en: 'Here’s a basic outline of how LM backpropagation works:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是 LM 反向传播的基本工作原理概述：
- en: Calculate the gradient of the error function with respect to the weights using
    the standard backpropagation algorithm
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准反向传播算法计算误差函数相对于权重的梯度
- en: Calculate the Hessian matrix, which represents the curvature of the error surface
    with respect to the weights
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算表示误差表面相对于权重的曲率的 Hessian 矩阵
- en: Adjust the weights using a combination of gradient descent and the LM optimization
    method
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降和 LM 优化方法的组合来调整权重
- en: The LM backpropagation algorithm adapts the learning rate during training. When
    the error surface is steep, it behaves more like gradient descent, which helps
    in avoiding overshooting. When the error surface is relatively flat, it behaves
    more like the Gauss-Newton method, which speeds up convergence. This combination
    of techniques makes LM backpropagation an efficient and effective algorithm for
    training neural networks, particularly in cases where standard backpropagation
    might face convergence issues or slow learning rates. It is commonly used in various
    applications, including pattern recognition, function approximation, and nonlinear
    regression tasks.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LM 反向传播算法在训练过程中调整学习率。当误差表面陡峭时，它表现得更像梯度下降，这有助于避免过冲。当误差表面相对平坦时，它表现得更像高斯-牛顿方法，这可以加快收敛速度。这种技术的组合使得
    LM 反向传播成为训练神经网络的一种高效且有效的算法，尤其是在标准反向传播可能遇到收敛问题或学习率较慢的情况下。它常用于各种应用，包括模式识别、函数逼近和非线性回归任务。
- en: 'After that, we have to set the number of nodes in the hidden layer:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之后，我们必须设置隐藏层中的节点数：
- en: '[PRE3]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we have to create our ANN using the `fitnet()` function as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须使用 `fitnet()` 函数创建我们的 ANN，如下所示：
- en: '[PRE4]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `fitnet()` function is a built-in MATLAB function used to create and train
    a feedforward neural network with a single hidden layer for function fitting,
    pattern recognition, and regression tasks. The following parameters are passed:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`fitnet()` 函数是一个内置的 MATLAB 函数，用于创建和训练具有单个隐藏层的前馈神经网络，用于函数拟合、模式识别和回归任务。以下参数被传递：'
- en: '`LnodesNum`: This is a vector that specifies the number of neurons in each
    hidden layer.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LnodesNum`：这是一个向量，指定了每个隐藏层中的神经元数量。'
- en: '`Tfunc`: This parameter specifies the training function to be used for training
    the neural network. It represents the optimization algorithm that updates the
    network weights during the training process. Some commonly used training functions
    include `trainlm` (LM), `trainbfg` (BFGS Quasi-Newton), and `traingd` (gradient
    descent).'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tfunc`：此参数指定用于训练神经网络的训练函数。它代表在训练过程中更新网络权重的优化算法。一些常用的训练函数包括`trainlm`（LM）、`trainbfg`（BFGS
    Quasi-Newton）和`traingd`（梯度下降）。'
- en: 'Training the algorithm is necessary to preprocess the data, as we showed in
    the *Exploring data wrangling* section, in the *Exploring MATLAB for Machine Learning*
    chapter. In this case, we can use the neural network processing functions, available
    in the Neural Network Toolbox. To print all the general data preprocessing functions
    available, we can use the following command:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*探索数据整理*部分，我们在*探索MATLAB机器学习*章节中展示了训练算法是必要的，以预处理数据。在这种情况下，我们可以使用神经网络工具箱中可用的神经网络处理函数。要打印所有可用的通用数据预处理函数，我们可以使用以下命令：
- en: '[PRE5]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following functions are listed:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下函数被列出：
- en: '`removerows`: Eliminate rows from the matrix based on specified indices'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`removerows`：根据指定的索引从矩阵中消除行'
- en: '`mapminmax`: Map the minimum and maximum values of each matrix row to the range
    [-1, 1]'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mapminmax`：将每个矩阵行的最小值和最大值映射到范围[-1, 1]'
- en: '`processpca`: Perform principal component analysis on the matrix rows'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`processpca`：对矩阵行执行主成分分析'
- en: '`mapstd`: Map the row means and deviations of the matrix to standard values'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mapstd`：将矩阵的行平均值和偏差映射到标准值'
- en: '`fixunknowns`: Handle matrix rows with unknown values using a specific procedure'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fixunknowns`：使用特定程序处理具有未知值的矩阵行'
- en: 'The first two represent the default functions applied to feedforward multilayer
    networks and are therefore the ones we will apply to our case:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前两个代表应用于前馈多层网络的默认函数，因此我们将将其应用于我们的案例：
- en: '[PRE6]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As already mentioned, these are the default processing functions applied to
    both input and output. The first removes the constant records, as they do not
    bring any content for data adaptation, and the second instead maps the elements
    of a matrix or vector from their original range to a specified target range.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，这些是应用于输入和输出的默认处理函数。第一个移除常数记录，因为它们不会为数据适应带来任何内容，而第二个则将矩阵或向量的元素从其原始范围映射到指定的目标范围。
- en: 'Then the preprocessing of data is necessary to operate **data splitting**.
    This is a common technique used in machine learning and data analysis to divide
    a dataset into separate subsets for different purposes. The main reason for data
    splitting is to have distinct portions of data for training, validation, and testing,
    which allows for the evaluation and improvement of machine learning models. There
    are several techniques to split the data; in this case, the dataset is split into
    three parts: a training set, a validation set, and a testing set. The training
    set is used for model training, the validation set is used to tune hyperparameters
    and optimize the model, and the testing set is used for final evaluation:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后需要对数据进行预处理以进行**数据划分**。这是机器学习和数据分析中常用的技术，用于将数据集划分为用于不同目的的单独子集。数据划分的主要原因是为了拥有用于训练、验证和测试的不同数据部分，这允许评估和改进机器学习模型。有几种数据划分技术；在这种情况下，数据集被划分为三个部分：训练集、验证集和测试集。训练集用于模型训练，验证集用于调整超参数和优化模型，测试集用于最终评估：
- en: '[PRE7]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This attribute determines the dimensions of the target data to be partitioned
    when invoking the data division function. The default value is `sample` for static
    networks and `time` for dynamic networks. Alternatively, it can be set to `sampletime`
    to divide targets by both sample and timestep, `all` to partition targets at each
    scalar value, or `none` to keep the data undivided (meaning all data is used for
    training and none for validation or testing).
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此属性决定了在调用数据划分函数时目标数据的维度。对于静态网络，默认值为`sample`，对于动态网络，默认值为`time`。或者，它可以设置为`sampletime`以根据样本和时间步长划分目标，`all`以在每个标量值处划分目标，或者`none`以保持数据未划分（意味着所有数据用于训练，没有用于验证或测试）。
- en: 'Let’s now set the percentage of data to use for the different phases:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们现在设置用于不同阶段的数据百分比：
- en: '[PRE8]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It is essential to perform data splitting carefully to avoid issues such as
    data leakage and ensure reliable model evaluation.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仔细执行数据划分对于避免数据泄露等问题以及确保可靠的模型评估至关重要。
- en: 'Before training the ANN, it is necessary to choose something. To start with,
    we must choose the evaluation metrics to check the performance of the model that
    we are setting. Evaluation metrics are quantitative measures used to assess the
    performance and effectiveness of a model, algorithm, system, or process. In various
    fields, such as machine learning, data science, and information retrieval, evaluation
    metrics are essential for comparing different methods, tuning parameters, and
    understanding the strengths and weaknesses of a particular solution. The choice
    of evaluation metric depends on the specific problem being addressed and the goals
    of the analysis. The selection of appropriate metrics depends on the nature of
    the problem and the objectives of the analysis. It’s essential to choose evaluation
    metrics that align with the specific goals and requirements of the task at hand.
    We can use a command such as this:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练人工神经网络（ANN）之前，选择某些内容是必要的。首先，我们必须选择评估指标来检查我们设置的模型的性能。评估指标是用于评估模型、算法、系统或过程性能和有效性的定量度量。在各个领域，如机器学习、数据科学和信息检索中，评估指标对于比较不同方法、调整参数以及理解特定解决方案的优缺点至关重要。评估指标的选择取决于要解决的问题和分析的目标。适当指标的选择取决于问题的性质和分析的目标。选择与当前任务的具体目标和要求相一致的评估指标是至关重要的。我们可以使用如下命令：
- en: '[PRE9]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A list of the evaluation metrics with a short summary will be printed.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将评估指标列表及其简要说明打印出来。
- en: 'Finally, we must choose the plot functions to get a visual representation of
    the results. To do that, we can use the `plotFcns()` function, which contains
    a one-dimensional cell array of strings that defines the plot functions associated
    with a network. The neural network training window, accessible through the `train()`
    function, displays a button for each plot function. Simply click on the respective
    button during or after the training process to open the desired plot:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须选择绘图函数以获得结果的视觉表示。为此，我们可以使用`plotFcns()`函数，它包含一个一维字符串单元数组，定义了与网络关联的绘图函数。通过`train()`函数可访问的神经网络训练窗口显示每个绘图函数的按钮。只需在训练过程中或训练过程之后点击相应的按钮即可打开所需的绘图：
- en: '[PRE10]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A list of the plots available for an ANN with a short summary will be returned.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将返回一个ANN可用绘图列表及其简要说明。
- en: 'Now we can train the ANN already set; for that, we can use the `train()` function
    as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以训练已经设置的ANN；为此，我们可以使用如下`train()`函数：
- en: '[PRE11]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function trains a shallow neural network, and three arguments are passed:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此函数训练一个浅层神经网络，并传递三个参数：
- en: '`AbaFitNet`: The neural network model object.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AbaFitNet`：神经网络模型对象。'
- en: '`Input`: The input data used for training the neural network. It should be
    a matrix where each row represents a single input pattern. The abalone dataset
    is a collection of features describing the physical dimensions of an abalone.
    This dataset consists of eight features: sex, length, diameter, height, whole
    weight, shucked weight, viscera weight, and shell weight.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Input`：用于训练神经网络的输入数据。它应该是一个矩阵，其中每一行代表一个单独的输入模式。鲍鱼数据集是一组描述鲍鱼物理尺寸的特征集合。此数据集包含八个特征：性别、长度、直径、高度、总重量、去壳重量、内脏重量和壳重量。'
- en: '`Target`: The target data corresponding to the input patterns. It should be
    a matrix with the same number of rows as the input matrix, where each row contains
    the corresponding target values. The target is the age of an abalone, measured
    in the form of the number of rings.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Target`：与输入模式相对应的目标数据。它应该是一个矩阵，行数与输入矩阵相同，其中每一行包含相应的目标值。目标是鲍鱼的年龄，以环数的形式衡量。'
- en: The `train()` function performs the training process using the specified input
    data and target data. The type of training algorithm used depends on the specific
    neural network model and its settings. After training, the object returned will
    contain the trained neural network, which can be used for making predictions on
    new data using the tasks related to the trained network.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`train()`函数使用指定的输入数据和目标数据执行训练过程。所使用的训练算法类型取决于特定的神经网络模型及其设置。训练完成后，返回的对象将包含训练好的神经网络，可用于使用与训练网络相关的任务对新数据进行预测。'
- en: During the training phase, a new window will be opened (as shown in *Figure
    5**.3*).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练阶段，将打开一个新窗口（如图*5**.3*所示）。
- en: '![Figure 5.3 – Training results window](img/B21156_05_03.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 训练结果窗口](img/B21156_05_03.jpg)'
- en: Figure 5.3 – Training results window
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 训练结果窗口
- en: In *Figure 5**.3*, we can check the training progress, which shows information
    such as **Epoch**, **Elapsed Time**, **Performance**, **Gradient**, **Mu**, and
    **Validation** **Checks**. Furthermore, the properties set in the previous steps
    are summarized. Finally, several buttons related to the plot set are available.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.3*中，我们可以检查训练进度，它显示了诸如**Epoch**、**Elapsed Time**、**Performance**、**Gradient**、**Mu**和**Validation
    Checks**等信息。此外，总结了之前步骤中设置的属性。最后，有几个与绘图集相关的按钮可用。
- en: By clicking on the buttons at the bottom of this window, we can draw the specific
    plot.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击此窗口底部的按钮，我们可以绘制特定的图表。
- en: 'After the ANN is trained, it is time to test the network. Testing a neural
    network involves evaluating its performance on a separate test dataset that the
    network has not seen during the training phase. This step helps assess the generalization
    ability of the trained network and ensures it can make accurate predictions on
    unseen data. To test the ANN that we trained, we can apply the network to the
    unseen data, as follows:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练好ANN之后，是时候测试网络了。测试神经网络涉及评估它在训练阶段未见过的一个单独的测试数据集上的性能。这一步骤有助于评估训练网络的泛化能力，并确保它可以在未见过的数据上做出准确的预测。为了测试我们训练的ANN，我们可以将网络应用于未见过的数据，如下所示：
- en: '[PRE12]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this piece of code, we start evaluating the network on all the data, obtaining
    a first value of performance to compare with other ones.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，我们开始对所有数据进行网络评估，获得一个性能值以与其他值进行比较。
- en: 'The following performance is returned:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下性能值被返回：
- en: '[PRE13]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we have to recalculate training, validation, and testing performance as
    follows:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须按照以下方式重新计算训练、验证和测试性能：
- en: '[PRE14]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following performance values are returned:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下性能值被返回：
- en: '[PRE15]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, to show the plots, we can use the following commands:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，为了展示这些图表，我们可以使用以下命令：
- en: '[PRE16]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `plottrainstate()` function plots the training states returned by the `train()`
    function.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`plottrainstate()`函数绘制了`train()`函数返回的训练状态。'
- en: '[PRE17]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Finally, to evaluate the network’s ability to estimate the model target, we
    used the `plotregression()` function. This function plots the linear regression
    of targets relative to outputs.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，为了评估网络估计模型目标的能力，我们使用了`plotregression()`函数。此函数绘制了相对于输出的目标线性回归图。
- en: 'The following plots will be printed:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图表将被打印：
- en: '![Figure 5.4 – ANN plots](img/B21156_05_04.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – ANN图表](img/B21156_05_04.jpg)'
- en: Figure 5.4 – ANN plots
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – ANN图表
- en: In this way, we will be able to evaluate the trend of the training parameters
    throughout the process period by period. Furthermore, we will have a statistic
    of the distribution of errors and, finally, an indication of the position of the
    forecasts compared to the real values through the regression graph.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就能通过逐期评估训练参数的趋势。此外，我们还将有一个误差分布的统计，最后，通过回归图，我们可以得到预测值与真实值位置的指示。
- en: In *Figure 5**.4*, we can analyze the regression line of the model. The regression
    line is a key element in statistical data analysis, providing valuable information
    about a model’s ability to predict the data. This straight line represents the
    mathematical relationship between the independent and dependent variables, trying
    to minimize the difference between the observed values and those predicted by
    the model. The slope of the line reflects the average change in the dependent
    variable for each unit change in the independent variable. If the slope is close
    to `1`, the model accurately predicts the data. Additionally, the intercept indicates
    the value of the dependent variable when the independent variable is 0.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.4*中，我们可以分析模型的回归线。回归线是统计数据分析中的一个关键元素，提供了有关模型预测数据能力的宝贵信息。这条直线代表了自变量和因变量之间的数学关系，试图最小化观察值与模型预测值之间的差异。线的斜率反映了自变量每单位变化时因变量的平均变化。如果斜率接近`1`，则模型可以准确地预测数据。此外，截距表示当自变量为0时因变量的值。
- en: The goodness of fit of the model is measured through the correlation coefficient
    *R*, which measures the strength of the association between two variables. A high
    *R* indicates good predictive ability. It is important to note that the regression
    line may have limitations in its predictive ability if the data has complex or
    nonlinear patterns. In such cases, more advanced models may be necessary.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的拟合优度是通过相关系数 *R* 来衡量的，它衡量了两个变量之间关联的强度。高 *R* 值表示良好的预测能力。需要注意的是，如果数据具有复杂或非线性模式，回归线在预测能力上可能存在局限性。在这种情况下，可能需要更高级的模型。
- en: After having analyzed an example of data fitting in detail, we will now see
    how to tackle a classification problem.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细分析了数据拟合的例子之后，我们现在将看到如何处理分类问题。
- en: Discovering pattern recognition using ANNs
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工神经网络发现模式识别
- en: '**Pattern recognition** is a branch of machine learning and AI that focuses
    on the identification of patterns or regularities in data. It involves the automatic
    discovery and extraction of meaningful information from datasets, with the aim
    of categorizing or classifying data into different classes or groups. Overall,
    pattern recognition plays a crucial role in automating the process of identifying
    patterns and making decisions based on data, making it a fundamental component
    of many modern AI systems.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式识别**是机器学习和人工智能的一个分支，它专注于识别数据中的模式或规律。它涉及从数据集中自动发现和提取有意义的信息，目的是将数据分类或归类到不同的类别或组中。总的来说，模式识别在自动化识别模式和基于数据做出决策的过程中发挥着至关重要的作用，使其成为许多现代人工智能系统的基本组成部分。'
- en: Pattern recognition offers numerous benefits in automating decision-making and
    handling complex data. However, it also poses challenges related to data quality,
    interpretability, and computational requirements. To leverage its advantages effectively,
    practitioners need to carefully design and train models while being aware of potential
    limitations and biases in the data and algorithms.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模式识别在自动化决策处理复杂数据方面提供了许多好处。然而，它也带来了与数据质量、可解释性和计算需求相关的挑战。为了有效地利用其优势，从业者需要精心设计和训练模型，同时意识到数据和方法中可能存在的局限性和偏差。
- en: The classical approach to pattern recognition uses data collected by sensors
    as input to a classification system. This data generally represents typical measurements
    such as kinematic and kinetic data from a motion analysis system. The measurements
    are usually subjected to a preprocessing phase. This is done to improve the signal
    properties. Subsequent feature extraction provides a feature vector for subsequent
    classification. This vector describes the input measurements in feature space.
    In supervised classification, labeled feature vectors are presented to a classifier
    for training. The vectors used to train the classifier form the training set.
    These labels assign a feature vector to one of several possible classes. In the
    recognition phase, the trained classifier uses this decision rule and automatically
    assigns a feature vector to a class. Different classifiers can be used, using
    different learning strategies.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的图案识别方法使用传感器收集的数据作为分类系统的输入。这些数据通常代表典型的测量值，如运动分析系统中的运动学和动力学数据。这些测量值通常要经过预处理阶段。这样做是为了改善信号特性。后续的特征提取为后续分类提供特征向量。这个向量描述了特征空间中的输入测量值。在监督分类中，标记的特征向量被呈现给分类器进行训练。用于训练分类器的向量形成训练集。这些标签将特征向量分配给几个可能的类别之一。在识别阶段，训练好的分类器使用这个决策规则并自动将特征向量分配给一个类别。可以使用不同的分类器，使用不同的学习策略。
- en: 'Let’s now analyze a practical example of pattern recognition using MATLAB.
    In this section, our objective is to develop a classification model that can categorize
    thyroid disease based on various patient data. The steps involved are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将分析一个使用MATLAB的图案识别的实际例子。在本节中，我们的目标是开发一个分类模型，可以根据各种患者数据对甲状腺疾病进行分类。涉及的步骤如下：
- en: 'To initiate the process, we acquire the data for analysis. For this study,
    we will utilize an existing dataset that comes pre-packaged with MATLAB. As mentioned
    earlier, MATLAB offers several readily available databases that can be easily
    imported into the workspace using the `load` command, followed by the specific
    database name. In this instance, we will work with the `thyroid_dataset` as our
    chosen dataset:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动这个过程，我们首先获取用于分析的数据。对于这项研究，我们将使用一个预装了MATLAB的现有数据集。如前所述，MATLAB提供了几个易于导入工作区的数据库，可以使用`load`命令后跟特定数据库名称来实现。在这个例子中，我们将使用`thyroid_dataset`作为我们的选择数据集：
- en: '[PRE18]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the MATLAB workspace, we now have two variables:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在MATLAB工作区中，我们现在有两个变量：
- en: '`Target` variable, classes are represented by a `1` in the first, second, or
    third row. A `1` in the first row indicates the patient is classified as normal
    (not hyperthyroid), a `1` in the second row indicates hyperfunction (hyperthyroidism),
    and a `1` in the third row indicates subnormal functioning (hypothyroidism).'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Target`变量，类别由第一行、第二行或第三行中的`1`表示。第一行中的`1`表示患者被分类为正常（非甲状腺功能亢进），第二行中的`1`表示功能亢进（甲状腺功能亢进），第三行中的`1`表示亚正常功能（甲状腺功能减退）。'
- en: The specific problem at hand is to determine whether a patient referred to the
    clinic is hypothyroid. It is important to note that due to many patients not being
    hyperthyroid (92%), a successful classifier must achieve a significantly higher
    accuracy than 92% to be considered effective.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前具体问题是确定被转诊到诊所的患者是否患有甲状腺功能减退。需要注意的是，由于许多患者并非甲状腺功能亢进（92%），一个成功的分类器必须达到比92%显著更高的准确率才能被认为是有效的。
- en: 'Now, it is time to select the appropriate training function for the neural
    network. To set a specific training algorithm for the network, you can use the
    `trainFcn` property and assign the name of the desired function. Among several
    available algorithms, we will opt for the **scaled conjugate gradient** (**SCG**)
    backpropagation method. This method is an optimization algorithm commonly used
    in training ANNs, particularly for solving the problem of weight updates during
    the training process. It is an alternative to other optimization techniques, such
    as gradient descent, **stochastic gradient descent** (**SGD**), and various flavors
    of backpropagation. SCG is known for its efficiency and speed in converging to
    a minimum of the loss function. The advantages of the SCG algorithm include its
    ability to converge quickly and efficiently in many cases, making it a good choice
    for training neural networks with relatively small to moderate-sized datasets.
    However, it may not always outperform other optimization methods on large datasets
    or in more complex network architectures. The choice of optimization algorithm
    often depends on the specific problem, dataset size, and computational resources
    available:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候选择适合神经网络的适当训练函数了。为了为网络设置特定的训练算法，您可以使用`trainFcn`属性并分配所需函数的名称。在多种可用的算法中，我们将选择**缩放共轭梯度**（**SCG**）反向传播方法。这种方法是一种优化算法，常用于训练人工神经网络，尤其是在解决训练过程中权重更新问题。它是梯度下降、**随机梯度下降**（**SGD**）和各种反向传播变体的替代方案。SCG以其在收敛到损失函数最小值时的效率和速度而闻名。SCG算法的优点包括在许多情况下能够快速有效地收敛，使其成为训练相对较小到中等规模数据集的神经网络的不错选择。然而，它可能并不总是优于其他优化方法，尤其是在大型数据集或更复杂的网络架构中。优化算法的选择通常取决于具体问题、数据集大小和可用的计算资源：
- en: '[PRE19]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After selecting the training algorithm, the next step is to construct the neural
    network. To achieve this, we need to determine the number of nodes in the hidden
    layer. In this case, we have decided to build a function-fitting neural network
    with one hidden layer consisting of 10 nodes:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择训练算法之后，下一步是构建神经网络。为了实现这一点，我们需要确定隐藏层中的节点数。在这种情况下，我们决定构建一个包含10个节点的单隐藏层函数拟合神经网络：
- en: '[PRE20]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can construct a pattern recognition network using the `patternnet()` function.
    Pattern recognition networks are feedforward networks designed for classifying
    inputs based on target classes. The target data for these networks should consist
    of vectors with all `0` values except for a `1` in the element corresponding to
    the class they represent.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`patternnet()`函数构建一个模式识别网络。模式识别网络是设计用于根据目标类别对输入进行分类的前馈网络。这些网络的目标数据应包含所有值为`0`的向量，除了对应于它们所代表的类别的元素为`1`。
- en: 'The `patternnet()` function accepts the following arguments:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`patternnet()`函数接受以下参数：'
- en: '`hiddenSizes`: A row vector specifying one or more hidden layer sizes. The
    default is `10` if a value is not provided.'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hiddenSizes`：指定一个或多个隐藏层大小的行向量。如果没有提供值，默认为`10`。'
- en: '`trainFcn`: The training function to be used. The default is `trainscg` (SCG
    backpropagation).'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainFcn`：要使用的训练函数。默认为`trainscg`（SCG反向传播）。'
- en: '`performFcn`: The performance function used during training. The default is
    `crossentropy`.'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`performFcn`：训练期间使用的性能函数。默认为`crossentropy`。'
- en: 'The function returns a pattern recognition neural network with the specified
    architecture and settings:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数返回具有指定架构和设置的图案识别神经网络：
- en: '[PRE21]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once the network has been constructed and the data has been preprocessed, the
    next step is to partition the data into separate sets for training, validation,
    and testing.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦构建了网络并预处理了数据，下一步是将数据划分为单独的训练、验证和测试集。
- en: 'To achieve this, we typically divide the available dataset into three different
    subsets:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们通常将可用的数据集划分为三个不同的子集：
- en: '**Training set**: This subset is used to train the neural network. The network
    learns from the input data and corresponding target outputs during the training
    process.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：这个子集用于训练神经网络。网络在训练过程中从输入数据和相应的目标输出中学习。'
- en: '**Validation set**: The validation set is used to fine-tune the network’s hyperparameters
    and prevent overfitting. It helps in optimizing the network’s performance by evaluating
    its performance on data it has not seen during training.'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：验证集用于微调网络的超参数并防止过拟合。它通过在训练过程中评估其未见过数据上的性能来帮助优化网络性能。'
- en: '**Testing set**: This set is used to evaluate the final performance of the
    trained network. It provides an unbiased estimate of the network’s generalization
    ability on unseen data.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：这个集用于评估训练后网络的最终性能。它提供了对网络在未见数据上泛化能力的无偏估计。'
- en: 'The division of data can be achieved using various techniques, such as random
    sampling, stratified sampling, or time-based splitting, depending on the nature
    of the data and the specific problem:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用各种技术来划分数据，例如随机抽样、分层抽样或基于时间的分割，具体取决于数据的性质和特定问题：
- en: '[PRE22]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The significance of operations and the types of functions used in the network
    construction and data preprocessing steps was extensively covered previously.
    In case of any uncertainties, it is recommended that you review that paragraph
    for a more detailed understanding.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在网络构建和数据预处理步骤中操作的重要性以及所使用的函数类型之前已经进行了广泛讨论。如果有任何疑问，建议您回顾该段落以获得更详细的理解。
- en: Regarding measuring network performance, we will select the **cross-entropy
    performance** function. This specific performance function is well suited for
    classification and pattern recognition tasks. It quantifies the network’s performance
    by calculating the cross-entropy, which measures the difference between estimated
    and actual class memberships.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在衡量网络性能方面，我们将选择**交叉熵性能**函数。这个特定的性能函数非常适合分类和模式识别任务。它通过计算交叉熵来量化网络的性能，交叉熵衡量的是估计和实际类别成员之间的差异。
- en: By utilizing the cross-entropy performance function, we can effectively evaluate
    how well the neural network performs in classifying inputs and make informed decisions
    about its effectiveness for the given pattern recognition problem.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用交叉熵性能函数，我们可以有效地评估神经网络在分类输入方面的表现，并就其在给定模式识别问题上的有效性做出明智的决策。
- en: 'By dividing the data into these separate sets, we can ensure that the neural
    network’s performance is not only measured on the training data but also validated
    on unseen data, making the evaluation more reliable and indicative of the network’s
    true capabilities:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将这些数据划分为这些单独的集，我们可以确保神经网络的性能不仅是在训练数据上衡量，而且还在未见数据上验证，从而使评估更加可靠，更能反映网络的真正能力：
- en: '[PRE23]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can now set plot functions to visualize the results of the simulation:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们现在可以设置绘图函数来可视化模拟的结果：
- en: '[PRE24]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we are ready to initiate the training process for the network using the
    `train()` function:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备使用`train()`函数开始网络的训练过程：
- en: '[PRE25]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'While training the neural network, the **Neural Network Training** window will
    be displayed. This window comprises four sections, each offering valuable information
    throughout the training process:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练神经网络时，将显示**神经网络训练**窗口。这个窗口包含四个部分，每个部分在整个训练过程中都提供有价值的信息：
- en: '**Neural network**: This section provides a summary of the neural network’s
    architecture and configuration, including the number of layers, nodes in each
    layer, and the chosen training algorithm.'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：本节提供了神经网络架构和配置的摘要，包括层数、每层的节点数以及选择的训练算法。'
- en: '**Algorithms**: In this area, details about the training algorithm being used,
    such as the specific optimization technique and convergence criteria, are presented.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：在此区域，展示了正在使用的训练算法的详细信息，例如特定的优化技术和收敛标准。'
- en: '**Progress**: The progress section shows real-time updates on the training
    process, such as the current epoch, training error, and validation performance.
    It allows monitoring of the network’s performance as it improves over successive
    epochs.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进度**：进度部分显示了训练过程的实时更新，例如当前周期、训练错误和验证性能。它允许监控网络在连续周期中的性能改进。'
- en: '**Plots**: This section displays various plots, such as training and validation
    errors over epochs, enabling a visual assessment of the network’s learning progress
    and potential overfitting.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图表**：本节显示了各种图表，如训练和验证错误随周期的变化，这有助于对网络的学习进度和潜在的过拟合进行视觉评估。'
- en: The **Neural Network Training** window (*Figure 5**.5*) offers a comprehensive
    view of the training procedure, providing insights into the network’s behavior
    and performance at different stages. These insights help fine-tune the network
    and make informed decisions to optimize its performance for the pattern recognition
    task at hand.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**神经网络训练**窗口（*图 5**.5*）提供了对训练过程的全面视图，提供了关于网络在不同阶段的运行和行为性能的见解。这些见解有助于微调网络，并做出明智的决定以优化其在当前模式识别任务中的性能。'
- en: '![Figure 5.5 – Neural Network Training window for pattern recognition](img/B21156_05_05.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 用于模式识别的神经网络训练窗口](img/B21156_05_05.jpg)'
- en: Figure 5.5 – Neural Network Training window for pattern recognition
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 用于模式识别的神经网络训练窗口
- en: Once the network training is complete, we can employ the trained model to test
    its performance on the same input data used during the training phase. By doing
    so, we can obtain the results and utilize them for evaluation purposes.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络训练完成后，我们可以使用训练好的模型来测试它在训练阶段使用的相同输入数据上的性能。通过这样做，我们可以获得结果并利用它们进行评估。
- en: 'Testing the network on the same data helps us assess how well the model generalizes
    to familiar inputs and provides insights into its effectiveness in handling real-world
    scenarios. The evaluation results obtained from this process aid in understanding
    the network’s accuracy, precision, recall, and other relevant metrics, allowing
    us to make informed decisions about its overall performance and suitability for
    the intended pattern recognition task:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在相同的数据上测试网络有助于我们评估模型对熟悉输入的泛化能力，并提供了关于其在处理现实世界场景中的有效性的见解。从这个过程中获得的评价结果有助于理解网络的准确性、精确度、召回率和其他相关指标，使我们能够就其整体性能和适用于预期模式识别任务的可适用性做出明智的决定：
- en: '[PRE26]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `vec2ind()` function is used to convert vectors to indices. It allows indices
    to be represented either directly as themselves or as vectors with a `1` in the
    row corresponding to the index they represent. In the context of this problem,
    `TargetInd` and `SimDataInd` are vectors containing values `1`, `2`, or `3`, representing
    the classes to which the targets and outputs belong. The last row of these vectors
    contains the percentage of error occurrences.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`vec2ind()` 函数用于将向量转换为索引。它允许索引直接表示为自身，或者表示为包含一个 `1` 的行，该行对应于它们所代表的索引。在此问题中，`TargetInd`
    和 `SimDataInd` 是包含值 `1`、`2` 或 `3` 的向量，代表目标和输出所属的类别。这些向量的最后一行包含错误发生的百分比。'
- en: 'The following value is obtained:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 获得了以下值：
- en: '[PRE27]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let’s proceed with the evaluation of the network. The following commands
    extract the outputs and targets that pertain to the training, validation, and
    test subsets. This extracted data will be used in the subsequent step to construct
    the confusion matrix, which will aid in further assessing the performance of the
    network on each of these subsets:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行网络的评估。以下命令提取了与训练、验证和测试子集相关的输出和目标。这些提取的数据将在后续步骤中用于构建混淆矩阵，这将有助于进一步评估网络在每个子集上的性能：
- en: '[PRE28]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: A **confusion matrix** is a valuable tool that allows us to compare the classification
    results of our model to the real data. It provides insights into the nature and
    quantity of classification errors. The matrix consists of cells where the diagonal
    elements represent the number of cases that were correctly classified, while the
    off-diagonal elements show the misclassified cases.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**混淆矩阵**是一个有价值的工具，它允许我们比较我们模型的分类结果与真实数据。它提供了对分类错误性质和数量的洞察。矩阵由单元格组成，其中对角线元素表示正确分类的案例数量，而离对角线元素表示错误分类的案例。'
- en: In an ideal situation, a machine learning algorithm should perfectly discriminate
    between two populations (such as healthy and diseased) that are not overlapping
    (mutually exclusive). However, in real-world scenarios, the two populations often
    overlap to some extent, leading to the algorithm making some false positive and
    false negative predictions.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在理想情况下，机器学习算法应该完美地区分两个不重叠（互斥）的群体（例如健康和患病群体）。然而，在现实场景中，这两个群体往往在一定程度上重叠，导致算法产生一些错误阳性和错误阴性预测。
- en: '*Figure 5**.6* displays the confusion matrix for the training, testing, and
    validation phases, as well as a combined matrix that considers all three sets
    of data together.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图5.6*显示了训练、测试和验证阶段的混淆矩阵，以及考虑所有三组数据一起的综合矩阵。'
- en: '![](img/B21156_05_06.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/B21156_05_06.jpg)'
- en: Figure 5.6 – The confusion matrix for training, testing, and validation, and
    the three kinds of data combined
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 训练、测试和验证阶段的混淆矩阵，以及三种数据组合
- en: This visual representation will help us understand how well the model performed
    across different phases and its overall classification performance considering
    all data subsets. The confusion matrix has been computed for the training, testing,
    and validation phases, as well as for the combination of all three data subsets.
    This comprehensive matrix provides a detailed overview of the model’s classification
    performance across various evaluation stages and gives a complete picture of its
    effectiveness in handling different datasets.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种视觉表示将帮助我们了解模型在不同阶段的表现以及考虑所有数据子集的整体分类性能。混淆矩阵已经计算了训练、测试和验证阶段，以及所有三个数据子集的组合。这个综合矩阵提供了模型在各个评估阶段的分类性能的详细概述，并给出了其在处理不同数据集方面的有效性的完整图景。
- en: In *Figure 5**.6*, the blue cell located in the bottom-right corner represents
    the total percentage of correctly classified cases, which are depicted by green
    cells positioned diagonally. The red cells in the matrix signify the total percentage
    of misclassified cases located in other cells. The confusion matrix organizes
    data with rows representing the actual values and columns representing the predicted
    values. For instance, in the top-left plot of the figure, the first row indicates
    that 85 cases are correctly classified as `1` (normal), 5 cases are incorrectly
    classified as `2` (hyperfunction), and 8 cases are incorrectly classified as `3`
    (subnormal). By observing the blue cell in the bottom right of each plot in the
    confusion matrix, we can deduce that the classification accuracy is consistently
    high, exceeding 92%. These results demonstrate excellent recognition capabilities.
    If higher accuracy is required, retraining the data could be considered. It should
    be noted that the starting dataset is not correctly balanced across all classes.
    This can result in a high-performance value for one class and a low-performance
    value for another. It is therefore advisable to always work on datasets that are
    correctly balanced across all classes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.6*中，位于右下角的蓝色单元格代表正确分类案例的总百分比，这些案例通过位于对角线上的绿色单元格表示。矩阵中的红色单元格表示位于其他单元格中的错误分类案例的总百分比。混淆矩阵通过行表示实际值，列表示预测值来组织数据。例如，在图象的右上角，第一行表示85个案例被正确分类为`1`（正常），5个案例被错误分类为`2`（过度功能），8个案例被错误分类为`3`（亚正常）。通过观察混淆矩阵中每个图象右下角的蓝色单元格，我们可以推断出分类精度始终很高，超过92%。这些结果证明了出色的识别能力。如果需要更高的精度，可以考虑重新训练数据。需要注意的是，起始数据集在所有类别之间并不正确平衡。这可能导致一个类别的性能值很高，而另一个类别的性能值很低。因此，始终建议在所有类别都正确平衡的数据集上工作。
- en: 'Another method of evaluating network performance is through the **receiver
    operating characteristic** (**ROC**). The ROC curve is a valuable tool for assessing
    the model’s performance in terms of sensitivity and specificity across various
    classification thresholds. The subsequent command generates plots for the ROC
    during each evaluation phase and for the entire process:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 评估网络性能的另一种方法是通过**受试者工作特征**（**ROC**）。ROC曲线是评估模型在各个分类阈值下灵敏度和特异性的宝贵工具。后续命令生成每个评估阶段和整个过程的ROC图：
- en: '[PRE29]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The ROC is a metric used to evaluate the performance of classifiers. It assesses
    the quality of a classifier for each class by applying threshold values across
    the interval `[0, 1]` to its outputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ROC是用于评估分类器性能的指标。它通过在区间 `[0, 1]` 上应用阈值值来评估每个类的分类器的质量。
- en: In the following figure, ROC plots are displayed for the training, testing,
    and validation phases, as well as for the combined data from all three subsets.
    These ROC curves allow us to analyze the classifier’s sensitivity and specificity
    across different classification thresholds for each class, providing valuable
    insights into its discrimination capabilities and overall performance.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，显示了训练、测试和验证阶段的ROC图，以及来自所有三个子集的合并数据的ROC曲线。这些ROC曲线使我们能够分析分类器在不同分类阈值下每个类的灵敏度特异度，从而为它的判别能力和整体性能提供宝贵的见解。
- en: '![Figure 5.7 – ROC curves for training, testing, and validation, and the three
    kinds of data combined](img/B21156_05_07.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 训练、测试和验证以及三种数据组合的ROC曲线](img/B21156_05_07.jpg)'
- en: Figure 5.7 – ROC curves for training, testing, and validation, and the three
    kinds of data combined
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 训练、测试和验证以及三种数据组合的ROC曲线
- en: In the graph, the colored lines on each axis represent the ROC curves. The ROC
    curve illustrates the relationship between the **true positive rate** (**TPR,
    or sensitivity**) and the **false positive rate** (**FPR**) as the classification
    threshold is varied. TPR measures the proportion of actual positive cases that
    are correctly identified as positive by the classifier.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，每个轴上的彩色线条代表ROC曲线。ROC曲线说明了随着分类阈值的改变，**真正例率**（**TPR，或灵敏度**）和**假正例率**（**FPR**）之间的关系。TPR衡量的是分类器正确识别为正例的实际正例的比例。
- en: TPR = TP / (TP + FN)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: TPR = TP / (TP + FN)
- en: 'Here, we have the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*TP* = True positives'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TP* = 真正例'
- en: '*FN* = False negatives'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FN* = 假阴性'
- en: In other words, it calculates the ability of the model to detect all positive
    instances in the dataset. FPR measures the proportion of actual negative cases
    that are incorrectly classified as positive by the classifier. It quantifies the
    rate of false alarms or false positives made by the model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它计算模型检测数据集中所有正例的能力。FPR衡量的是分类器错误地将实际负例分类为正例的比例。它量化了模型产生的误报或假正例的比率。
- en: FPR = FP / (FP + TN)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: FPR = FP / (FP + TN)
- en: 'Here, we have the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*FP* is the number of false positives, which are the number of negative examples
    that are incorrectly classified as positive'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FP* 是假正例的数量，即被错误地分类为正例的负例数量'
- en: '*TN* is the number of true negatives, which are the number of negative examples
    that are correctly classified as negative'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TN* 是真负例的数量，即被正确分类为负例的负例数量'
- en: An ideal test would exhibit points located in the upper-left corner, indicating
    100% sensitivity and 100% specificity. The closer the lines approach the upper-left
    corner, the better the network’s performance, indicating its ability to achieve
    higher sensitivity while keeping the FPR low, leading to improved classification
    accuracy.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一个理想的测试应该显示出位于左上角的数据点，这表明100%的灵敏度和100%的特异性。线条越接近左上角，网络的性能就越好，这表明它能够在保持低FPR的同时实现更高的灵敏度，从而提高分类的准确性。
- en: After tackling a pattern recognition problem using the tools available in MATLAB,
    in the next practical example, we will see how to tackle a clustering problem
    with the help of the Neural Network tool.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用MATLAB中可用的工具解决模式识别问题之后，在下一个实际例子中，我们将看到如何借助神经网络工具解决聚类问题。
- en: Building a clustering application with an ANN
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ANN构建聚类应用程序
- en: Clustering is a popular unsupervised machine learning technique used for grouping
    similar data points together in a dataset. The goal of clustering is to partition
    data into clusters in such a way that data points within the same cluster are
    more like each other than those in other clusters. We examined this topic in depth
    in [*Chapter 4*](B21156_04.xhtml#_idTextAnchor084)*, Clustering Analysis and*
    *Dimensionality Reduction*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种流行的无监督机器学习技术，用于在数据集中将相似的数据点分组在一起。聚类的目标是将数据划分为簇，使得同一簇内的数据点比其他簇内的数据点更相似。我们在[*第4章*](B21156_04.xhtml#_idTextAnchor084)*，聚类分析和降维*中深入探讨了这一主题。
- en: 'In this section, we will see how to address a clustering problem using an ANN
    in the MATLAB environment. So far, to train a neural network in the MATLAB environment,
    we have used the commands available from the command line, or much more simply
    to be implemented in a script with the `.m` extension to reproduce the algorithm
    whenever we like. But MATLAB has out-of-the-box apps that let us use a wizard
    to train an ANN. We will do this to address a clustering problem as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何在MATLAB环境中使用ANN解决聚类问题。到目前为止，为了在MATLAB环境中训练神经网络，我们使用了命令行中可用的命令，或者更简单地说，通过`.m`扩展名的脚本实现，以便我们可以在任何时候重现算法。但是，MATLAB提供了现成的应用程序，允许我们使用向导来训练ANN。我们将这样做来解决以下聚类问题：
- en: 'To get an overview of the apps made available by the Neural Network tool, we
    can type the following on the command line:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要了解神经网络工具提供的应用程序概述，我们可以在命令行中键入以下内容：
- en: '[PRE30]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following window will open:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将打开以下窗口：
- en: '![Figure 5.8 – Neural Network start window](img/B21156_05_08.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 神经网络启动窗口](img/B21156_05_08.jpg)'
- en: Figure 5.8 – Neural Network start window
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 神经网络启动窗口
- en: 'Four apps are available: `nftool` command to start the `nprtool` command to
    open the **Pattern** **Recognition** app.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个应用程序可供使用：`nftool`命令用于启动`nprtool`命令以打开**模式识别**应用程序。
- en: 'In this example, we will use the **Clustering** app, which is started using
    the following command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用**聚类**应用程序，它可以通过以下命令启动：
- en: '[PRE31]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following window will open:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 将打开以下窗口：
- en: '![Figure 5.9 – MATLAB app for clustering using ANN](img/B21156_05_09.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – 使用ANN进行聚类的MATLAB应用程序](img/B21156_05_09.jpg)'
- en: Figure 5.9 – MATLAB app for clustering using ANN
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 使用ANN进行聚类的MATLAB应用程序
- en: In this app, we can use a wizard for the training of the ANN. Let’s see how.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在此应用程序中，我们可以使用向导来训练ANN。让我们看看如何操作。
- en: 'To start, we have to import the data. For this example, we will use a dataset
    already available in the MATLAB environment to explain the clustering application
    using an ANN. To import that dataset in the MATLAB workspace, we use the following
    command:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始，我们必须导入数据。对于本例，我们将使用MATLAB环境中已可用的数据集来解释使用ANN的聚类应用程序。为了在MATLAB工作空间中导入该数据集，我们使用以下命令：
- en: '[PRE32]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we have two matrices in the MATLAB workspace (`Input` and `Target`). We
    will use only the first (wx1,000 double) for a clustering problem, which is the
    `Input` matrix with two variables and 1,000 records. By examining the structure
    of the `Target` (4x1,000 double) matrix, we realize that four classes are available,
    so the data has `4` groups of data, which will be useful in justifying the results.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们在MATLAB工作空间中有两个矩阵（`Input`和`Target`）。我们将仅使用第一个（wx1,000 double）进行聚类问题，这是具有两个变量和1,000条记录的`Input`矩阵。通过检查`Target`（4x1,000
    double）矩阵的结构，我们意识到有四个类别可用，因此数据有`4`组数据，这将有助于证明结果的有效性。
- en: 'Now, we have to import the data in the app. To do this, we can click on the
    **Import** button in the window shown in *Figure 5**.9*. The window shown in *Figure
    5**.10* will open:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们必须在应用程序中导入数据。为此，我们可以在*图5.9*中显示的窗口中点击**导入**按钮。将打开*图5.10*中显示的窗口：
- en: '![Figure 5.10 – Import data in Clustering app](img/B21156_05_10.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10 – 在聚类应用程序中导入数据](img/B21156_05_10.jpg)'
- en: Figure 5.10 – Import data in Clustering app
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 在聚类应用程序中导入数据
- en: The right matrix was already detected for **Predictors**; if you want to change
    the data, you can select the right one using the browse button next to the **Predictors**
    field. In this case, we have to set the observations in columns, so we will have
    1,000 observations with 2 features. Let’s just click on the **OK** button to import
    the data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 已经检测到正确的矩阵用于**预测器**；如果您想更改数据，可以使用**预测器**字段旁边的浏览按钮选择正确的矩阵。在这种情况下，我们必须将观测值设置为列，因此我们将有1,000个观测值和2个特征。让我们点击**确定**按钮来导入数据。
- en: We have the ANN architecture already set in the app, so we can identify the
    number of inputs and the topology of the network. The output will be a network
    with 10x10 nodes (100 nodes). The `nctool` guides you in solving clustering problems
    by employing a **self-organizing map** (**SOM**). This map creates a condensed
    depiction of the input space, capturing both the density patterns of input vectors
    in that space and a compressed, two-dimensional representation of the input space’s
    topology.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经在应用中设置了ANN架构，因此我们可以确定输入的数量和网络拓扑。输出将是一个具有10x10节点（100个节点）的网络。`nctool`通过使用**自组织映射**（SOM）来指导你解决聚类问题。此图创建了对输入空间的浓缩表示，捕捉了该空间中输入向量的密度模式以及输入空间拓扑的压缩、二维表示。
- en: '**SOMs**, also known as **Kohonen maps**, are a type of ANN that belongs to
    the family of unsupervised learning algorithms. They were introduced by the Finnish
    professor Teuvo Kohonen in the 1980s. SOMs are used for tasks such as dimensionality
    reduction, data visualization, clustering, and feature extraction.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**SOMs**，也称为**Kohonen图**，是一种属于无监督学习算法家族的ANN（人工神经网络）。它们由芬兰教授Teuvo Kohonen在20世纪80年代提出。SOMs用于诸如降维、数据可视化、聚类和特征提取等任务。'
- en: The primary idea behind SOMs is to map high-dimensional input data onto a lower-dimensional
    grid or lattice in such a way that similar input data points are mapped to nearby
    grid cells. This results in a topological representation of the input data, where
    similar data points are located close to each other on the map, allowing for easier
    visualization and interpretation.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SOM（自组织映射）背后的主要思想是将高维输入数据映射到一个低维网格或晶格上，使得相似的输入数据点被映射到附近的网格单元。这导致了对输入数据的拓扑表示，在地图上相似的数据点彼此靠近，从而便于可视化和解释。
- en: 'SOMs consist of two essential layers: the input layer and the output layer,
    often referred to as the feature map. The input layer serves as the initial stage
    in a SOM. Each data point from the dataset competes for representation to identify
    its own characteristics. The process begins with weight vector initialization,
    kickstarting the mapping process of the SOM.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SOMs由两个基本层组成：输入层和输出层，通常被称为特征图。输入层是SOM的初始阶段。数据集中的每个数据点都竞争表示以识别其自身特征。过程从权重向量初始化开始，启动SOM的映射过程。
- en: The mapped vectors are subsequently scrutinized to identify the weight vector
    that best represents the chosen sample using a randomly selected sample vector.
    Nearby weights to each weighted vector are considered, and the chosen weight evolves
    into a vector for the random sample, fostering the map’s growth and the emergence
    of new patterns. In a two-dimensional feature space, these patterns often take
    on hexagonal or square shapes. This entire process is repeated over 1,000 times.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随后仔细检查映射向量，以识别使用随机选择的样本向量最佳代表所选样本的权重向量。考虑每个加权向量的邻近权重，所选权重演变成随机样本的向量，促进地图的增长和新模式的产生。在二维特征空间中，这些模式通常呈六边形或方形。整个过程重复1000次。
- en: 'In essence, learning occurs as follows:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从本质上讲，学习过程如下：
- en: Each node is analyzed to determine whether its weights are similar to the input
    vector. The node that best matches the input vector is termed the **best matching**
    **unit** (**BMU**).
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析每个节点以确定其权重是否与输入向量相似。与输入向量最佳匹配的节点被称为**最佳匹配单元**（BMU）。
- en: The neighborhood value of the BMU is then established, and over time, the number
    of neighbors tends to decrease.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后确定BMU的邻域值，随着时间的推移，邻居的数量往往会减少。
- en: The BMU’s weight vector further adapts to resemble the sample vector, leading
    to similar changes in the surrounding areas. The weight of a node changes more
    as it gets closer to the BMU and less as it moves away from its neighbors.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: BMU（最佳匹配单元）的权重向量进一步适应以类似于样本向量，导致周围区域发生相似的变化。节点越接近BMU，其权重变化越大；越远离邻居，其权重变化越小。
- en: Repeat step 2 for *N* iterations.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2 *N* 次迭代。
- en: This iterative process allows the SOM to refine its representation of the input
    data, ultimately leading to a more organized and compressed representation that
    captures the underlying structure of the data.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个迭代过程允许SOM细化其对输入数据的表示，最终导致一个更有组织和压缩的表示，能够捕捉数据的潜在结构。
- en: Now, we can train the network just by clicking on the **train** button of the
    app. After a few seconds, the ANN will be trained and ready for use.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们只需点击应用中的**训练**按钮就可以训练网络。几秒钟后，ANN将被训练并准备好使用。
- en: 'Finally, we can view the results using the plots available at the top of the
    **App** window. We will first use the neighbor distances, and the following plot
    will be drawn:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用**应用**窗口顶部的图表来查看结果。我们首先使用邻域距离，接下来的图表将绘制：
- en: '![Figure 5.11 – SOM neighbor weight distances](img/B21156_05_11.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – SOM邻域权重距离](img/B21156_05_11.jpg)'
- en: Figure 5.11 – SOM neighbor weight distances
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – SOM邻域权重距离
- en: This is a graphical representation that illustrates the distances between weight
    vectors of neighboring nodes in a SOM. This plot helps visualize how the weights
    of neighboring nodes change during the learning process, providing insights into
    the topology and organization of the SOM. We can identify more colored cells that
    separate the nodes in four areas using the class available in the `Target` matrix.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个图形表示，展示了SOM（自组织映射）中相邻节点权重向量之间的距离。这个图有助于可视化学习过程中相邻节点权重的变化，从而深入了解SOM的拓扑结构和组织方式。我们可以使用`Target`矩阵中可用的类别来识别更多分离四个区域的彩色细胞。
- en: The blue hexagons represent the neurons. The red lines connect neighboring neurons.
    The colors in the regions containing the red lines indicate the distances between
    neurons. The darker colors represent larger distances, and the lighter colors
    represent smaller distances.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色六边形代表神经元。红色线条连接相邻的神经元。包含红色线条的区域中的颜色表示神经元之间的距离。较深的颜色代表较大的距离，较浅的颜色代表较小的距离。
- en: One visualization tool for the SOM is the weight distance matrix (also called
    the U-matrix). To view the U-matrix, click **SOM Neighbor Distances** in the training
    window.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: SOM的一个可视化工具是权重距离矩阵（也称为U矩阵）。要查看U矩阵，请在训练窗口中点击**SOM邻域距离**。
- en: 'This shows us that there are four potential clusters in the data correctly
    detected by the algorithm. To have a confirmation of this indication, we can plot
    the weight position plot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明算法正确检测到了数据中的四个潜在簇。为了确认这一指示，我们可以绘制权重位置图：
- en: '![Figure 5.12 – SOM weight positions](img/B21156_05_12.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 – SOM权重位置](img/B21156_05_12.jpg)'
- en: Figure 5.12 – SOM weight positions
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – SOM权重位置
- en: In *Figure 5**.12*, the four clusters can be easily identified. In this diagram,
    the input vectors are depicted as green dots, and it illustrates the SOM’s classification
    of the input space by displaying blue-gray dots representing each neuron’s weight
    vector. Additionally, neighboring neurons relate to red lines, offering a visual
    representation of the SOM’s organizational structure.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图5.12**中，四个簇可以很容易地识别。在这个图中，输入向量被描绘为绿色点，它通过显示代表每个神经元权重向量的蓝色灰色点来展示SOM对输入空间的分类。此外，相邻的神经元通过红色线条相关联，提供了SOM组织结构的视觉表示。
- en: After having also analyzed a case of clustering with ANNs, to complete the topic,
    we will see how to optimize the results obtained.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析了ANNs（人工神经网络）的一个聚类案例之后，为了完成这个主题，我们将看到如何优化获得的结果。
- en: Exploring advanced optimization techniques
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索高级优化技术
- en: '**Advanced optimization techniques** are powerful methods used to enhance the
    efficiency and effectiveness of optimization algorithms. These techniques aim
    to overcome the limitations of traditional optimization approaches, particularly
    in complex, high-dimensional, or non-convex optimization problems.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级优化技术**是用于提高优化算法效率和效果的有效方法。这些技术旨在克服传统优化方法的局限性，尤其是在复杂、高维或非凸优化问题中。'
- en: In machine learning, advanced optimization techniques are essential for training
    complex models effectively, improving convergence, avoiding overfitting, and handling
    high-dimensional data. In the following subsection, some advanced optimization
    techniques commonly used in machine learning are listed.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，高级优化技术对于有效地训练复杂模型、提高收敛速度、避免过拟合和处理高维数据至关重要。在下一个小节中，将列出机器学习中常用的一些高级优化技术。
- en: Understanding SGD
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解随机梯度下降（SGD）
- en: '**SGD** is a popular and fundamental optimization algorithm used in machine
    learning for training models, especially in large-scale and complex settings.
    It’s a variant of the traditional gradient descent method designed to address
    efficiency and convergence issues when dealing with large datasets. The stochastic
    aspect of SGD comes from the fact that it uses a random mini-batch for each iteration,
    making the optimization process more stochastic (randomized) compared to the deterministic
    nature of regular gradient descent, which uses the entire dataset for each update.
    This stochasticity introduces noise into the gradient estimates, which can help
    the algorithm escape local minima, converge faster, and generalize better.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**SGD** 是一种在机器学习中用于训练模型的基本且流行的优化算法，特别是在大规模和复杂的环境中。它是传统梯度下降方法的一种变体，旨在解决处理大数据集时的效率和收敛问题。SGD的随机性源于它使用随机的小批量进行每次迭代，这使得优化过程比使用整个数据集进行每次更新的常规梯度下降的确定性性质更加随机（随机化）。这种随机性将噪声引入梯度估计中，这有助于算法逃离局部最小值，更快地收敛，并更好地泛化。'
- en: The primary objective of any optimization algorithm is to minimize the discrepancy
    between the predicted values of the model and the actual values observed in the
    data. The smaller the error between the observed and predicted values, the more
    effective the algorithm is at simulating the real-world scenario. Minimizing this
    discrepancy is equivalent to optimizing an objective function on which the model
    construction is based.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 任何优化算法的主要目标是使模型预测值与数据中观察到的实际值之间的差异最小化。观察值与预测值之间的误差越小，算法在模拟现实世界场景方面的效果就越好。最小化这种差异等同于优化基于模型构建的目标函数。
- en: 'Descent methods are iterative techniques that, commencing from an initial point
    x0 ∈ Rn, produce a sequence of points {xn} ∈ N based on the subsequent equation:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 下降法是一种迭代技术，从初始点 x_0 ∈ R^n 开始，根据以下方程生成一系列点 {x_n} ∈ N：
- en: x n+1 = x n + γ n * g n
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: x_{n+1} = x_n + γ_n * g_n
- en: 'In a descent method, the vector *g**n* represents the search direction, and
    the scalar *γ**n* serves as a positive parameter known as the step length, determining
    the distance of movement in the *g**n* direction. These choices of *g**n* and
    *γ**n* are made to ensure the reduction of the objective function *f* in each
    iteration, following the principle:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在下降法中，向量 *g_n* 表示搜索方向，标量 *γ_n* 作为步长参数，是一个已知的正参数，它决定了在 *g_n* 方向上的移动距离。这些 *g_n*
    和 *γ_n* 的选择是为了确保在每次迭代中减少目标函数 *f*，遵循以下原则：
- en: f x n+1 < f x n ∀ n ≥ 0
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: f(x_{n+1}) < f(x_n) ∀ n ≥ 0
- en: We select the vector *g**n* as a descent direction, ensuring that the line *x
    = x**n* *+ γ**n* ** g**n* creates an obtuse angle with the gradient vector ∇ f(xn).
    This guarantees the decrease of the objective function *f*, as long as the value
    of *γ**n* is sufficiently small. This approach allows for various descent methods,
    contingent on the specific choice of *g**n*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择向量 *g_n* 作为下降方向，确保直线 *x = x_n + γ_n * g_n* 与梯度向量 ∇f(x_n) 形成钝角。只要 *γ_n* 的值足够小，这保证了目标函数
    *f* 的减少。这种方法允许有各种下降法，这取决于 *g_n* 的具体选择。
- en: 'A gradient is a function that produces a vector, representing the slope of
    the tangent to the graph of a function. It points in the direction of the greatest
    increase in the function. Let’s examine the convex function illustrated in the
    following figure:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是一个产生向量的函数，表示函数图形切线的斜率。它指向函数增加最快的方向。让我们考察以下图中所示的双曲函数：
- en: '![Figure 5.13 – How the gradient descent algorithm searches the global optimum](img/B21156_05_13.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 – 梯度下降算法如何搜索全局最优解](img/B21156_05_13.jpg)'
- en: Figure 5.13 – How the gradient descent algorithm searches the global optimum
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – 梯度下降算法如何搜索全局最优解
- en: The primary objective of the gradient descent algorithm is to locate the function’s
    nadir, or lowest point. In more precise terms, the gradient functions as a derivative,
    indicating the incline or steepness of the objective function.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法的主要目标是找到函数的谷底，即最低点。更精确地说，梯度充当导数，指示目标函数的倾斜或陡峭程度。
- en: To provide a clearer analogy, let’s imagine we find ourselves lost in the mountains
    at night with limited visibility. Our perception is restricted to sensing the
    slope of the terrain beneath our feet. The aim is to reach the lowest point of
    the mountain. Achieving this goal involves taking successive steps in the direction
    of the steepest slope. We proceed iteratively, advancing step by step, until we
    finally arrive at the valley of the mountain.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个更清晰的类比，让我们想象我们在夜晚在山中迷路，能见度有限。我们的感知仅限于感知脚下地形的坡度。目标是到达山的最底部。实现这一目标涉及在最陡坡度的方向上连续迈步。我们迭代地进行，一步一步前进，直到最终到达山谷。
- en: 'We’ll examine a two-variable function, denoted as *f(x, y)*, with its gradient
    represented as a vector encompassing the partial derivatives of *f*. The first
    derivative pertains to *x*, while the second derivative pertains to *y*. Upon
    computation of these partial derivatives, the results are as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考察一个有两个变量的函数，记为 *f(x, y)*，其梯度由包含 *f* 的偏导数的向量表示。一阶导数与 *x* 相关，而二阶导数与 *y* 相关。计算这些偏导数后，结果如下：
- en: δf _ δx  ,  δf _ δy
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: δf/δx ， δf/δy
- en: 'The initial expression corresponds to the partial derivative concerning *x*,
    whereas the subsequent expression pertains to the partial derivative regarding
    *y*. The gradient is represented by the ensuing vector:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 初始表达式对应于关于 *x* 的偏导数，而后续表达式对应于关于 *y* 的偏导数。梯度由以下向量表示：
- en: ∇ f(x, y) = ⎡ ⎢ ⎣ δf _ δx   δf _ δy  ⎤ ⎥ ⎦
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ∇ f(x, y) = ⎡⎢⎣ δf/δx   δf/δy ⎤⎥⎦
- en: The given equation represents a function in a two-dimensional space, effectively
    forming a two-dimensional vector. Each component of this vector signifies the
    steepest ascent direction for the respective function variable. Consequently,
    the gradient points toward the direction where the function exhibits the most
    significant increase.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的方程表示二维空间中的一个函数，有效地形成了一个二维向量。这个向量的每个分量都表示相应函数变量的最陡上升方向。因此，梯度指向函数增长最显著的方向。
- en: 'Likewise, if we consider a function with five variables, the resulting gradient
    vector will encompass five partial derivatives. In general, a function with *n*
    variables gives rise to an *n*-dimensional gradient vector, exemplified as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们考虑一个有五个变量的函数，结果梯度向量将包含五个偏导数。一般来说，一个有 *n* 个变量的函数会产生一个 *n*-维梯度向量，如下所示：
- en: ∇ f(x, y, … .z) = ⎡ ⎢ ⎣  δf _ δx   δf _ δy  … …  δf _ δz  ⎤ ⎥ ⎦
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ∇ f(x, y, … .z) = ⎡⎢⎣ δf/δx   δf/δy   …   …   δf/δz ⎤⎥⎦
- en: When utilizing gradient descent, our objective isn’t to maximize *f* as rapidly
    as possible; instead, we aim to minimize it—specifically, to locate the smallest
    point that minimizes the function.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用梯度下降时，我们的目标不是尽可能快地最大化 *f*；相反，我们旨在最小化它——具体来说，是要找到使函数最小化的最小点。
- en: 'Consider a function *y = f(x)*. The foundation of gradient descent relies on
    the observation that when the function *f* is well defined and differentiable
    within a vicinity of *x*, it experiences a faster decrease as we proceed in the
    direction opposite to the negative gradient. Commencing from an initial value
    of *x*, we can express this as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个函数 *y = f(x)*。梯度下降的基础在于观察，当函数 *f* 在 *x* 的邻域内定义良好且可微时，它会在与负梯度相反的方向上经历更快的下降。从初始值
    *x* 开始，我们可以表示如下：
- en: '*x**n* *+ 1 =* *x**n* - *γ* ***∇ *f(**x**n**)*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '*x**n* + 1 = *x**n* - *γ* *∇ *f(*x**n**)*'
- en: where gamma (*γ*) is learning rate and delta (∇) is the gradient.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，gamma (*γ*) 是学习率，delta (∇) 是梯度。
- en: 'In this context, we have the following:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们有以下内容：
- en: '*γ* represents the learning rate'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ* 代表学习率'
- en: ∇ signifies the gradient
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∇ 表示梯度
- en: When using sufficiently small values of *γ*, the algorithm converges to the
    minimum value of the function *f* within a finite number of iterations. In essence,
    when the gradient is negative, it indicates a decreasing objective function at
    that point, implying that the parameter should shift toward larger values to approach
    a minimum point. Conversely, when the gradient is positive, the parameters should
    move toward smaller values to attain lower values of the objective function.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用足够小的 *γ* 值时，算法在有限次数的迭代中收敛到函数 *f* 的最小值。本质上，当梯度为负时，它表明在该点目标函数在减少，意味着参数应该向更大的值移动以接近最小点。相反，当梯度为正时，参数应该向较小的值移动以达到目标函数的较低值。
- en: Exploring Adam optimization
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Adam 优化
- en: '**Adaptive moment estimation** (**Adam**) is an advanced optimization algorithm
    commonly used for training deep learning models, particularly in neural networks.
    It is an extension of the SGD optimization method that adapts the learning rate
    for each parameter individually.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应矩估计**（**Adam**）是一种高级优化算法，常用于训练深度学习模型，尤其是在神经网络中。它是SGD优化方法的扩展，为每个参数单独调整学习率。'
- en: 'This gradient descent algorithm aims to locate the objective function’s minimum
    using an iterative approach. In each step, an approximation of the gradient is
    computed to guide the descent in the direction that effectively reduces the objective
    function. Within this process, the selection of the learning rate parameter holds
    significant importance. This parameter governs the speed at which we approach
    the optimal objective function values:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 此梯度下降算法旨在通过迭代方法找到目标函数的最小值。在每一步中，计算梯度的近似值以引导下降，朝着有效减少目标函数的方向移动。在这个过程中，学习率参数的选择具有重要意义。此参数控制我们接近最佳目标函数值的速度：
- en: If the learning rate is excessively small, a substantial number of iterations
    are required to converge toward the optimal values
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果学习率过小，则需要大量的迭代才能收敛到最佳值
- en: Conversely, an overly high learning rate might cause us to overlook the optimal
    solution
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，过高的学习率可能会导致我们忽略最佳解
- en: Adam adjusts the learning rates for each parameter based on the first-order
    moment (mean) and second-order moment (uncentered variance) of the gradients.
    This adaptability helps the algorithm converge more efficiently, especially in
    high-dimensional spaces with varying gradient magnitudes.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Adam根据梯度的第一阶矩（均值）和第二阶矩（未中心化方差）调整每个参数的学习率。这种适应性有助于算法更有效地收敛，尤其是在梯度幅度变化的高维空间中。
- en: This method addresses the issue of initial bias in the moments by correcting
    them. In the early stages of training, the moments’ estimates may be biased toward
    0, and Adam compensates for this bias, particularly when the learning rate is
    low.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过纠正矩来解决初始矩偏差的问题。在训练的早期阶段，矩的估计可能偏向于0，而Adam补偿了这种偏差，尤其是在学习率较低时。
- en: Adam combines the concepts of momentum (accumulating a fraction of the past
    gradients to enhance convergence) and **RMSProp** (scaling the learning rates
    based on the magnitudes of recent gradients) to perform well on a wide range of
    optimization problems.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Adam结合了动量（累积过去梯度的部分以增强收敛）和**RMSProp**（根据最近梯度的幅度缩放学习率）的概念，在广泛的优化问题上表现良好。
- en: 'The Adam optimization algorithm involves the following calculations during
    each iteration:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化算法在每次迭代中涉及以下计算：
- en: Compute the gradient of the loss with respect to the model’s parameters
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失相对于模型参数的梯度
- en: Update the first and second moments (mean and uncentered variance) of the gradients
    using a moving average
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用移动平均更新梯度的第一和第二矩（均值和未中心化方差）
- en: Correct the bias in the moments (especially in the early iterations)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修正矩（尤其是在早期迭代中）的偏差
- en: Update the parameters using the corrected moments and the learning rate
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用修正后的矩和学习率更新参数
- en: Overall, Adam is effective in many deep learning scenarios, often providing
    faster convergence than standard SGD with manually tuned learning rates. It’s
    a popular choice for training neural networks and has become a standard optimization
    algorithm in many deep learning frameworks due to its adaptive properties and
    strong performance.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Adam在许多深度学习场景中都很有效，通常比手动调整学习率的标准SGD收敛得更快。由于其自适应特性和强大的性能，它已成为训练神经网络的热门选择，并在许多深度学习框架中成为标准的优化算法。
- en: Introducing second-order methods
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入二阶方法
- en: '**Second-order optimization methods**, also known as **Newton-like methods**,
    are advanced techniques used for solving optimization problems. Unlike first-order
    methods (such as gradient descent) that primarily rely on gradients, second-order
    methods take advantage of both gradients and second-order derivatives (**Hessian
    matrix**) of the objective function. This additional information can lead to faster
    convergence and more accurate solutions, especially in complex and non-convex
    optimization landscapes.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**二阶优化方法**，也称为**牛顿法**，是用于解决优化问题的先进技术。与主要依赖梯度的第一阶方法（如梯度下降）不同，二阶方法利用了目标函数的梯度和二阶导数（Hessian矩阵）。这些额外的信息可以导致更快收敛和更精确的解，尤其是在复杂和非凸优化景观中。'
- en: Second-order methods consider not only the first-order derivative (gradient)
    of the function but also its second-order derivative (Hessian matrix). The Hessian
    matrix captures curvature information and provides insights into the shape of
    the objective function’s surface. These methods often use quadratic approximations
    of the objective function around the current point. These approximations consider
    the first and second derivatives and provide a more accurate representation of
    the local behavior of the function.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶方法不仅考虑函数的一阶导数（梯度），还考虑其二阶导数（Hessian矩阵）。Hessian矩阵捕捉曲率信息，并提供了关于目标函数表面形状的见解。这些方法通常使用目标函数在当前点周围的二次近似。这些近似考虑了一阶和二阶导数，并提供了对函数局部行为的更准确表示。
- en: The use of curvature information from the Hessian matrix can lead to faster
    convergence rates compared to first-order methods, especially when the objective
    function is well behaved and smooth. Second-order methods can be more robust against
    the choice of learning rates or step sizes, as they inherently adjust the step
    size based on the curvature of the function.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hessian矩阵的曲率信息可以比一阶方法更快地收敛，尤其是在目标函数表现良好且平滑时。二阶方法对学习率或步长选择的选择具有更强的鲁棒性，因为它们本质上会根据函数的曲率调整步长。
- en: Second-order methods are valuable tools for optimization, particularly when
    computational resources allow for the calculation of second-order derivatives.
    However, their application may depend on the specific characteristics of the problem
    being solved.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶方法是优化中的宝贵工具，尤其是在计算资源允许计算二阶导数时。然而，它们的应用可能取决于所解决问题的具体特征。
- en: An example of a second-order method is Newton’s method. This method directly
    minimizes a quadratic approximation of the objective function using the Hessian
    matrix. Newton’s method is a classic optimization algorithm used to find the minimum
    or maximum of a function. Named after Sir Isaac Newton, this method utilizes both
    the gradient (first derivative) and the second derivative (Hessian matrix) of
    the function to iteratively approach the optimal solution.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶方法的一个例子是牛顿法。这种方法直接最小化目标函数的二次近似，使用Hessian矩阵。牛顿法是一种经典的优化算法，用于寻找函数的最小值或最大值。以艾萨克·牛顿爵士命名，这种方法利用了函数的梯度（一阶导数）和二阶导数（Hessian矩阵）来迭代地接近最优解。
- en: 'This approach adopts the structure of Newton’s method, which is commonly used
    to locate the roots of a function, but in this case, it’s applied to the derivative
    of function *f*. The rationale behind this is that identifying the minimum point
    of function f is equivalent to finding the root of its first derivative f′. In
    this scenario, the updated formula can be expressed as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法采用了牛顿法的结构，牛顿法通常用于找到函数的根，但在此情况下，它应用于函数 *f* 的导数。这样做的原因是，识别函数 f 的最小点等同于找到其第一导数
    f′ 的根。在这种情况下，更新后的公式可以表示如下：
- en: x n+1 = x n −  f ′ ( x n) _ f ″  (x n)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: x n+1 = x n −  f ′ ( x n) _ f ″  (x n)
- en: 'In the given equation, we have the following:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中，我们有以下内容：
- en: f′ (xn) represents the first derivative of function *f*
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f′ (xn) 表示函数 *f* 的第一导数
- en: f″ (xn) signifies the second derivative of function *f*
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f″ (xn) 表示函数 *f* 的第二导数
- en: Second-order methods are typically favored over gradient descent due to their
    faster convergence rate, provided that the analytical expressions for both first
    and second derivatives are available. However, these methods converge regardless
    of whether they’re approaching minima or maxima. Variations of this method exist
    that ensure global convergence and reduce computational costs by sidestepping
    the need to solve the system for determining the search direction using direct
    methods.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一阶和二阶导数的解析表达式都可用，通常优先考虑二阶方法而不是梯度下降，因为它们的收敛速度更快。然而，这些方法无论接近极小值还是极大值都会收敛。存在这种方法的变化形式，可以确保全局收敛，并通过避免使用直接方法求解确定搜索方向的系统来降低计算成本。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we gained insight into simulating typical human brain activities
    using ANNs. We grasped the fundamental concepts behind ANNs, delving into the
    creation of a basic neural network architecture. This exploration encompassed
    elements such as input, hidden, and output layers, connection weights, and activation
    functions. Our understanding extended to crucial decisions regarding hidden layer
    count, node quantity within each layer, and network training algorithms.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了使用人工神经网络模拟典型人类大脑活动的方法。我们掌握了人工神经网络背后的基本概念，深入研究了基本神经网络架构的创建。这次探索包括输入层、隐藏层和输出层、连接权重和激活函数等元素。我们的理解扩展到了关于隐藏层数量、每层节点数量和网络训练算法的关键决策。
- en: Then we focused on data fitting and pattern recognition using neural networks.
    We engaged in script analysis to master the utilization of neural network functions
    via the command line. We then ventured into the Neural Network Toolbox, featuring
    algorithms, pre-trained models, and apps for crafting, training, visualizing,
    and simulating shallow and deep neural networks. The Neural Network Toolbox offers
    an accessible interface—the **Neural Network** getting started GUI—which serves
    as the launchpad for tasks such as neural network fitting, pattern recognition,
    clustering, and time-series analysis.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们专注于使用神经网络进行数据拟合和模式识别。我们通过脚本分析来掌握通过命令行使用神经网络函数的方法。随后，我们进入了神经网络工具箱，它包含算法、预训练模型和应用程序，用于构建、训练、可视化和模拟浅层和深层神经网络。神经网络工具箱提供了一个易于使用的界面——**神经网络**入门GUI，它作为神经网络拟合、模式识别、聚类和时间序列分析等任务的启动平台。
- en: Finally, we introduced some of the commonly used advanced optimization techniques.
    These methods are potent techniques employed to amplify the efficiency and efficacy
    of optimization algorithms. Their purpose is to surmount the constraints posed
    by conventional optimization methodologies, especially in intricate, high-dimensional,
    or non-convex optimization scenarios.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了一些常用的先进优化技术。这些方法是用来增强优化算法效率和效果的有效技术。它们的目的在于克服传统优化方法提出的限制，特别是在复杂、高维或非凸优化场景中。
- en: In the next chapter, we will understand the basic concepts of deep learning.
    We also will learn about the different types of deep learning and understand **convolutional****neural
    networks** (**CNNs**). Additionally, we will learn how to build a CNN using MATLAB
    and understand recurrent neural networks, long short-term memory networks, and
    transformer models.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将了解深度学习的基本概念。我们还将了解不同类型的深度学习，并理解**卷积神经网络**（**CNNs**）。此外，我们将学习如何使用MATLAB构建CNN，并了解循环神经网络、长短期记忆网络和转换器模型。
