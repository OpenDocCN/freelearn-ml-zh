- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Artificial Neural Network Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**) include data structures and algorithms
    for learning and classifying data. Through neural network techniques, a program
    can learn through examples and create an internal structure of rules to classify
    different inputs. MATLAB provides algorithms, pre-trained models, and apps to
    create, train, visualize, and simulate ANNs. In this chapter, we will see how
    to use MATLAB to build an ANN-based model to predict values and classify data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and testing an ANN model in MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data fitting with ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering pattern recognition using ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a clustering application with an ANN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring advanced optimization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic concepts relating to machine learning.
    To understand these topics, a basic knowledge of algebra and mathematical modeling
    is needed. You will also need a working knowledge of the MATLAB environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`ANNFitting.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ANNPatReg.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serial computers and their programs exhibit remarkable capabilities in executing
    tasks that involve repetitive, well-defined operations, prioritizing accuracy,
    reliability, and speed. While these information-processing systems are highly
    valuable, they lack true intelligence. The sole element of intelligence lies with
    the programmer who comprehends the task and formulates the program. To achieve
    true **artificial intelligence** (**AI**), a system must possess the ability to
    solve problems that humans consider simple, trivial, and intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts relating to ANNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ANNs are designed to emulate the intricate workings of biological nervous systems,
    comprising a vast array of nerve cells or neurons interconnected in a complex
    network. Typically, each neuron establishes connections with tens of thousands
    of other neurons, resulting in hundreds of billions of connections. The emergence
    of intelligent behavior stems from the myriad interactions among these interconnected
    units.
  prefs: []
  type: TYPE_NORMAL
- en: In this network, specific units serve distinct functions. Some units act as
    receivers of information from the environment, while others respond to stimuli
    in the environment. Certain units, known as hidden units, solely communicate within
    the network, concealed from direct interaction with the external environment.
    Overall, the neural network’s structure involves input units, output units, and
    hidden units working in concert to process information and exhibit intelligent
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each unit in the neural network executes a simple operation: it activates when
    the cumulative received signal surpasses an activation threshold. Once active,
    the unit transmits a signal to other connected units through communication channels.
    These connections act as filters, transforming the messages into excitatory or
    inhibitory signals, and adjusting their intensity based on individual characteristics.
    Remarkably, the network’s input-output link, or transfer function, is not explicitly
    programmed but rather acquired through a learning process using empirical data,
    which can be supervised, unsupervised, or reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are a type of AI that can learn from experience. This learning is possible
    thanks to their structure, which is similar to that of the human brain. Like the
    human brain, ANNs are composed of a large number of nodes, which are connected
    to each other by connections. Nodes process information, and connections determine
    how information is transmitted from one node to another. ANNs learn from experience
    through a process called supervised learning. In this process, ANNs are provided
    with a dataset of desired input and output examples. ANNs then use this data to
    learn to map inputs to desired outputs. For example, an ANN that needs to learn
    to recognize images of dogs is provided with a dataset of images of dogs and images
    of other animals. The ANN uses these images to learn to distinguish between dogs
    and other animals.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks operate in parallel, enabling them to handle multiple data simultaneously,
    in contrast to serial computers, which process data individually and sequentially.
    Although individual neurons may be relatively slow, the parallel nature of neural
    networks accounts for the brain’s higher processing speed when tackling tasks
    requiring the simultaneous handling of numerous data points, such as visual object
    recognition. This remarkable system exhibits robust noise immunity, akin to a
    sophisticated statistical model. Even in the event of some unit malfunctions,
    the overall network performance may experience reductions, but a complete system
    shutdown is unlikely to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the latest generation of neural network software demands a solid
    understanding of statistics. Despite their seemingly immediate usability, users
    must not be deceived, as they can quickly make predictions or classifications
    within certain limitations. From an industrial perspective, these networks prove
    effective when historical data is available for processing with neural algorithms.
    This capability is particularly valuable in production environments as it facilitates
    data extraction and model creation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s essential to note that while models generated by neural networks are highly
    efficient, they lack explanations in human symbolic language. The outcomes must
    be accepted as they are, leading to the characterization of neural networks as
    black boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Like any modeling algorithm, the efficiency of neural networks relies heavily
    on the careful selection of predictive variables. These networks require a training
    phase to establish individual neuron weights, which can be time-consuming when
    dealing with many records and variables. Unlike conventional models, neural networks
    lack theorems or definitive guidelines, making the success of a network heavily
    dependent on the creator’s experience.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks find their utility in scenarios where data may be partially
    inaccurate or where analytical models are unavailable for problem-solving. They
    are commonly used in Optical Character Recognition (OCR) software, facial recognition
    systems, and other applications that handle error-prone or noisy data. Moreover,
    they are widely employed in data mining analysis and serve as tools for forecasting
    in financial and meteorological domains. In recent years, their significance has
    substantially increased in the field of bioinformatics, where they are instrumental
    in identifying functional and structural patterns in nucleic acids and proteins.
    By providing a comprehensive set of input data, the network can produce the most
    probable output.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how perceptrons work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fundamental unit of a neural network is the **perceptron**, which emulates
    the essential functions of a biological neuron. It evaluates the strength of each
    input, accumulates these inputs, and then compares the sum to a specific threshold.
    Based on this comparison, the perceptron determines the output value. The neuron’s
    basic structure is well understood, and researchers have identified the primary
    biochemical reactions that govern its activity. As such, a neuron can be regarded
    as the elemental computational unit of the brain. Within the human brain, approximately
    100 distinct classes of neurons have been identified, each contributing to the
    intricate neural network responsible for our cognitive processes and abilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Understanding the perceptron scheme](img/B21156_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Understanding the perceptron scheme
  prefs: []
  type: TYPE_NORMAL
- en: The primary function of a biological neuron is to generate an electrical potential
    that travels along its **axon** (neuron output) when the electrical activity at
    the neuron’s cell body surpasses a specific threshold. The neuron’s input is received
    through a set of fibers called **dendrites**, which contact the axons of other
    neurons, transmitting electrical potentials from them. The point of connection
    between an axon of one neuron and the dendrite of another neuron is referred to
    as a **synapse**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The synapse possesses the ability to regulate the electrical pulse emanating
    from the axon. The electrical potential generated by a neuron is essentially binary:
    an on/off state. If the neuron’s electrical activity surpasses a specific threshold,
    an impulse is generated; otherwise, no impulse occurs.'
  prefs: []
  type: TYPE_NORMAL
- en: Notably, the intensity of the generated pulse remains consistent across different
    neurons. As the potential propagates along the axon and reaches the synapse connected
    to another neuron’s dendrite, the post-synaptic potential relies on the biochemical
    characteristics of the synapse. Despite having the same pre-synaptic potential,
    two distinct synapses may generate varying post-synaptic potentials. In other
    words, the synapse modulates and weighs the input potential before transmission.
  prefs: []
  type: TYPE_NORMAL
- en: Post-synaptic potentials continue through the neuron’s dendrites and accumulate
    at the soma level. Only when the sum of these potentials surpasses a certain threshold
    does the neuron trigger the potential to propagate through its axon. Both biological
    neurons and artificial neurons receive multiple inputs through dendrites. The
    artificial neuron aggregates these various input values and computes the result.
  prefs: []
  type: TYPE_NORMAL
- en: If the computed value exceeds a particular threshold, the artificial neuron
    produces an output signal or potential; otherwise, it remains inactive.
  prefs: []
  type: TYPE_NORMAL
- en: The initial function implemented by the artificial neuron is the algebraic sum
    of its inputs, which serves to construct the system’s response. When simulating
    a phenomenon, the system may encounter errors, necessitating appropriate corrections.
    To achieve this, each input is assigned a weight, a numerical value that modulates
    its impact on the total sum, determining the neuron’s potential.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, each input contributes differently to the determination of the
    threshold value and potential triggering, akin to the characteristic of biological
    neurons involving synapses between the axon of one neuron and the dendrite of
    another.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of the post-synaptic neuron, the inputs consist of potentials
    from other neurons whose axons synapse with their dendrites, and these inputs
    are precisely modulated by the synapses. Some inputs may exert a stronger influence
    on the total sum, while others could even be inhibitory, reducing the overall
    sum and thereby lowering the probability of exceeding the threshold and triggering
    a potential.
  prefs: []
  type: TYPE_NORMAL
- en: This essential property of biological systems is mathematically modeled in connection
    systems using the concept of weights. Each connection is assigned a numeric value
    as its weight, which is multiplied by the input value. Consequently, the input’s
    effect on the total sum is determined by the magnitude of its weight.
  prefs: []
  type: TYPE_NORMAL
- en: Activation function to introduce non-linearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our previous discussion, we explored the weighted sum function introduced
    by incorporating the concept of weights. Now, let’s delve into another property
    of the artificial neuron, once again inspired by the behavior of biological neurons.
    As mentioned earlier, the biological neuron sums up the post-synaptic potentials
    of its dendrites at the soma level. However, this summation is not a simple algebraic
    addition of these potentials. Various factors, such as the passive resistance
    of the neuron membrane, come into play, making the actual summation a function
    that is typically non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, artificial neurons compute the weighted inputs and then modify the
    result using a specific function. This function is referred to as the activation
    function, which is applied to the output of the neuron to determine its true potential.
    The activation function plays a crucial role in shaping the behavior of the neuron
    and ultimately influences the outcome of the neural network’s computation.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function takes the weighted sum of inputs and an additional bias
    term and applies a specific mathematical operation to produce the neuron’s output.
    This output, often referred to as the activation or the post-activation value,
    is then used as input to the subsequent layers in the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different types of activation functions are utilized in neural networks, each
    with its unique characteristics and applications. Common activation functions
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1` if the input is greater than or equal to a threshold, and `0` otherwise:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = 0 if x < 0, 1 if x ≥ 0
  prefs: []
  type: TYPE_NORMAL
- en: '`0` and `1`. It was commonly used in the past but has fallen out of favor in
    deeper networks due to the vanishing gradient problem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) =  1 _ 1 + e −x
  prefs: []
  type: TYPE_NORMAL
- en: '`0`. It helps mitigate the vanishing gradient problem and accelerates convergence
    in deep networks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = 0 if x < 0, x if x ≥ 0
  prefs: []
  type: TYPE_NORMAL
- en: '`-1` and `1`. It is symmetric around the origin, providing a better range for
    the gradient compared to the sigmoid function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = tanh (x)
  prefs: []
  type: TYPE_NORMAL
- en: '`1`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f (x) i =  e −x i _ ∑ i=1 k  e −x i
  prefs: []
  type: TYPE_NORMAL
- en: '**Exponential linear unit (ELU)**: ELU is an activation function used in ANNs.
    It is a smooth, non-saturating function that can handle both positive and negative
    inputs. ELU is a more recent activation function than ReLU, which is also widely
    used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = x if x > 0
  prefs: []
  type: TYPE_NORMAL
- en: alpha * (exp(x) − 1) if x < 0
  prefs: []
  type: TYPE_NORMAL
- en: Here, *x* is the input to the ELU function and *α* is a hyperparameter that
    controls the steepness of the negative slope. The default value of *α* is `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: The ELU function is a smooth, non-saturating function that can handle both positive
    and negative inputs. This makes it a good choice for tasks that involve both types
    of inputs, such as image recognition and natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the activation function influences the neural network’s performance,
    training speed, and ability to capture complex relationships in the data. Different
    activation functions may be used in different layers of the network, depending
    on the specific problem and architectural considerations.
  prefs: []
  type: TYPE_NORMAL
- en: ANN’s architecture explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having discussed the properties of an artificial neuron, we will now delve into
    the architecture of a neural network. This entails both a physical illustration
    of the network’s structure and a determination of the role each neuron plays within
    this framework. Consider a scenario with multiple inputs and nodes, such that
    each input is connected to every node. Similarly, each output node receives connections
    from all inputs. Each output node possesses the characteristics described earlier
    and carries out its computations in conjunction with the other nodes. Upon introducing
    an input pattern, the output values are influenced either by the input values
    themselves or by the network’s weights. The weights play a crucial role in the
    network, determining the extent to which a particular input influences a specific
    node. The collection of nodes in the structure is commonly referred to as a layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks are organized into layers, each serving a specific purpose
    in information processing. The primary types of layers in a neural network include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: The initial layer that receives the input data and passes
    it on to the subsequent layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layers**: Intermediate layers between the input and output layers.
    These layers process the data and extract relevant features through complex transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The final layer that produces the network’s output or predictions
    based on the processed information from the hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of hidden layers and nodes within them, along with the choice of
    activation functions and weights, constitute the architecture of the neural network.
    The architecture (*Figure 5**.2*) significantly influences the network’s ability
    to learn from data, generalize to new examples, and perform specific tasks efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – ANN architecture with weights and activation function](img/B21156_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – ANN architecture with weights and activation function
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can take the form of multiple layers, and each layer added enhances
    the network’s computational capacity. Inputs are numerical values that undergo
    evaluation through the weights of connections with the first layer of nodes, known
    as the hidden layer. In this hidden layer, each node conducts computations as
    described earlier, leading to the generation of a potential that then propagates
    to the nodes of the output layer. The potentials produced by the output nodes
    collectively represent the final output calculated by the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a neural network refers to the specific way in which the
    nodes are interconnected. In the case of feedforward neural networks, which are
    characterized by the architecture shown in the previous figure, the activation
    of input nodes propagates forward through the hidden layer and further to the
    output layer. Changing the connections between nodes alters the network’s architecture.
    This not only yields practical consequences in terms of the network’s computational
    capacity but also carries significant theoretical implications related to the
    concept of learning. The arrangement of nodes in the network affects its ability
    to learn from data and perform specific tasks, making architecture design a crucial
    aspect of neural network development.
  prefs: []
  type: TYPE_NORMAL
- en: After analyzing the basic concepts of ANNs, we now need to pay attention to
    how these algorithms are trained.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing an ANN model in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw the architecture of an ANN. It imposes two
    layers, input and output, which cannot be altered. Consequently, the critical
    factor lies in the number of hidden layers we consider. The size of a neural network
    is defined by the number of hidden neurons. Determining the optimal size of the
    network remains an ongoing challenge, as no analytical solution has been discovered
    to date. One approach to tackle this problem is to employ a heuristic method:
    creating various networks with increasing complexity, using a subset of the training
    data, and monitoring the error on a validation subset simultaneously. After completing
    the training process, the network with the lowest validation error is chosen as
    the preferred one.'
  prefs: []
  type: TYPE_NORMAL
- en: How to train an ANN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s discuss the process of choosing the number of layers. The number of input
    nodes is fixed based on the number of features in the input data, while the number
    of output nodes is determined by the number of outcomes to be modeled or the class
    levels in the outcome. The real challenge lies in determining the appropriate
    number of neurons for the hidden layer. Unfortunately, there is no analytical
    method to accomplish this task. The optimal number of neurons depends on various
    factors, such as the number of input nodes, the volume of training data, and the
    complexity of the learning algorithm, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Having more neurons in the hidden layer will lead to a model that better fits
    the training data, but it comes with the risk of overfitting, potentially resulting
    in poor generalization on future data. Additionally, neural networks with many
    nodes can be computationally expensive and slow to train.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, a heuristic approach can be adopted, where different configurations
    are experimented with to find an optimal balance. This trial-and-error method
    allows us to strike a balance between model complexity, accuracy, and computational
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs consist of simple elements that operate in parallel. The connections between
    these elements play a vital role as they dictate the network’s functionalities.
    These connections influence the output through their respective weights, which
    are adjusted during the neural network’s training phase.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the network is fine-tuned by modifying the connection weights,
    enabling specific inputs to yield desired outputs. For instance, the network can
    be calibrated by comparing its practical output with the target output we want
    to achieve. This iterative process continues until the network’s output aligns
    with the desired target.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain dependable results, a substantial number of input/target pairs are
    required to shape the network appropriately. This training process ensures that
    the neural network can accurately produce the desired outputs for a variety of
    inputs, making it a reliable tool for various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The adjustment of these weights is determined by the specific algorithm we choose
    to adopt. In the following practical examples, we will discuss and refer to various
    algorithms that govern the process of weight adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the MATLAB Neural Network Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Neural Network Toolbox** offers a range of algorithms, pre-trained models,
    and apps that enable users to create, train, visualize, and simulate neural networks.
    It supports both shallow neural networks (with one hidden layer) and deep neural
    networks (with multiple hidden layers). With these tools, various tasks, such
    as classification, regression, clustering, dimensionality reduction, time-series
    forecasting, and dynamic system modeling and control, can be performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four primary ways to utilize the Neural Network Toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nnstart` command, granting access to automatic tasks such as function fitting
    (`nftool`), pattern recognition (`nprtool`), data clustering (`nctool`), and time-series
    analysis (`ntstool`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Basic command-line operations**: For greater flexibility, users can utilize
    command-line operations. While more knowledge is required, this method allows
    users to have complete control over the process, without relying on menus and
    icons typically found in the GUI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizing the toolbox**: Users can customize the toolbox by creating their
    own neural networks with arbitrary connections. Existing toolbox training features
    in the GUI can be used to continue training these custom networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying toolbox functions**: All computational components in the toolbox
    are written in MATLAB code and are fully accessible, allowing users to modify
    and tailor them to specific needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This toolbox caters to users of all levels, from beginners to experts. It offers
    simple tools that guide new users through specific applications and more complex
    tools that enable experts to customize networks and experiment with new architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the approach chosen, a proper analysis using neural networks
    should encompass the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network configuration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weight and bias initialization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network validation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network testing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By following these steps, users can effectively apply neural networks to various
    problems and tasks. They are explained in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the process involves collecting the data to be analyzed, which
    is typically done outside the MATLAB environment. This data collection phase is
    crucial, as the quality of the data will significantly impact the final results
    and the ability to extract meaningful insights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we proceed to create the neural network using various functions available
    in the toolbox. These functions allow us to build the network through the chosen
    algorithm, resulting in the creation of a neural network object. This object stores
    all the necessary information defining the neural network’s properties, such as
    its architecture, subobject structures, functions, and weight and bias values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third step is network configuration, where we examine input and output data,
    set the dimensions of the network to fit the data, and choose appropriate input
    and output processing settings to enhance network performance. This configuration
    step is usually performed automatically when the training function is called but
    can also be done manually using the configuration function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After configuration, the fourth step involves initializing the weights and biases.
    We set initial values from which the network will begin its training process.
    This initialization is usually done automatically based on the chosen training
    algorithm, but users can also set custom values if needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fifth step is network training, which is a critical phase of the process.
    During training, the weights and biases are fine-tuned to optimize the network’s
    performance. This stage is crucial for the network’s ability to generalize well
    with new, unseen data. A portion of the collected data (typically around 70% of
    available cases) is used for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, in the sixth step, network validation takes place. Here, a fraction of
    randomly selected data (usually around 15% of available cases) is passed through
    the network to estimate how well the model has been trained. The results obtained
    during this phase help determine whether the chosen model adequately reflects
    the initial expectations or whether adjustments are needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, in the last step, we utilize the trained network. A portion of the
    collected data (approximately 15% of available cases) is used to test the network’s
    performance. The trained neural network object can then be saved and employed
    multiple times with new data as needed. This allows for the reuse of the network
    to make predictions or analyze various datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The workflow for **neural network design** involves breaking down the collected
    data into three sets: the **training set**, the **validation set**, and the **test
    set**. Let’s describe each of them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set (usually 70% of the available cases)**: The training set is
    a collection of examples used to train the neural network and adjust its parameters.
    During the training process, the network learns from the input-output pairs in
    the training set to optimize its internal weights and biases. The goal is to find
    the optimal set of parameters that best captures the underlying patterns and relationships
    in the data. The neural network improves its performance through iterative adjustments
    during the training phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set (usually 15% of the available cases)**: The validation set
    is a separate set of examples used to fine-tune the network’s parameters and assess
    its performance during training. It serves as a validation mechanism to prevent
    overfitting, a condition where the network performs well on the training data
    but poorly on new, unseen data. By monitoring the network’s performance on the
    validation set, we can make decisions about the model’s complexity, such as determining
    the optimal number of hidden units or identifying a suitable stopping point for
    the training algorithm. Adjustments based on the validation set help ensure the
    network generalizes well to new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test set (usually 15% of the available cases)**: The test set is a separate
    and independent collection of examples used solely to evaluate the performance
    of a fully trained neural network. After the training and validation stages, the
    final model is assessed using the test set to estimate its error rate and validate
    its generalization capabilities. It is crucial to refrain from making any further
    adjustments to the model based on the test set evaluation to avoid bias or overfitting.
    The test set provides an unbiased measure of how well the neural network is likely
    to perform on new, real-world data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By dividing the data into these three distinct sets, the neural network design
    workflow ensures that the model is trained, validated, and tested in a controlled
    and reliable manner, allowing for accurate assessments of its performance and
    generalization capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: With a clear understanding of this process, we are now ready to proceed with
    our work on analyzing a practical example of an ANN implementation in MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data fitting with ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data fitting** is the process of constructing a curve or mathematical function
    that best matches a given set of collected data points. This curve fitting can
    involve either interpolations, where exact data points are fitted, or smoothing,
    where a smooth function approximates the data. In the context of regression analysis,
    curve fitting is closely related to statistical inference, considering uncertainties
    arising from random errors in observed data.'
  prefs: []
  type: TYPE_NORMAL
- en: The approximate curves obtained through data fitting have multiple applications.
    They can be used to visualize and display the data, predict function values in
    regions with no available data, and summarize the relationships between multiple
    variables. This process is valuable for understanding and interpreting complex
    datasets, making predictions, and gaining insights from the collected information.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the trend of a particular distribution using mathematical formulas
    can be challenging, and these formulas may not always accurately represent all
    the data or cover the entire range of existence. To address such cases, machine
    learning algorithms come to the rescue. These algorithms can build models without
    relying on complex mathematical formulas.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are well suited for data-fitting tasks and trend prediction. They can adapt
    and learn from a given set of inputs and associated target outputs. **Function
    fitting** is the process of training a neural network with such input-output pairs,
    enabling it to form a generalization of the underlying input-output relationship.
    Once trained, the neural network can generate outputs for inputs it has not encountered
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of neural networks allows them to capture complex patterns and
    relationships in the data, making them powerful tools for data-fitting and prediction
    tasks. By utilizing machine learning algorithms such as neural networks, we can
    overcome the limitations of traditional mathematical formulas and achieve accurate
    predictions across various datasets and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The performance of algorithms based on machine learning strongly depends on
    the quality of the data being worked on. The data collection process typically
    occurs outside the MATLAB environment, which means that you need to have a properly
    collected data file ready to initiate an analysis in MATLAB. However, if you don’t
    have access to the data yet and are here to learn, there’s no need to worry because
    MATLAB has a solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Neural Network Toolbox software provides several sample datasets that you
    can use to experiment with the functionality of the toolbox. These sample datasets
    are readily available and can serve as a starting point for your analysis. To
    explore the available datasets, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A list of datasets sorted by application type will be returned. With these
    sample datasets at your disposal, you can begin your learning journey and gain
    hands-on experience with the Neural Network Toolbox in MATLAB. Now we will work
    on an example of data fitting in MATLAB:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus our attention on the dataset for data fitting, specifically, `abalone_dataset`,
    which contains the *abalone shell rings dataset*. To load the dataset into the
    MATLAB workspace, let’s use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Executing this command will load the data into the arrays named `Input` and
    `Target`. The aim of the model is to estimate the age of an abalone, utilizing
    physical measurements to achieve this prediction. help abalone_dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A comprehensive description of the dataset is provided, including details such
    as the number of attributes, the total number of items, and a list of variables.
    Additionally, the description offers valuable insights into potential use cases
    for the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the data, we have to choose the training algorithm and set
    the architecture network. In MATLAB different functions are available for training.
    To get a list of training algorithms available, we can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: LM backpropagation is an optimization algorithm used in training ANNs. It is
    an extension of the standard backpropagation algorithm that improves convergence
    and robustness, especially for nonlinear and ill-conditioned problems. In standard
    backpropagation, the algorithm adjusts the weights of the neural network using
    the gradient of the error with respect to the weights. However, in some cases,
    this process can be slow, and the algorithm might get stuck in local minima. LM
    backpropagation addresses this issue by incorporating the LM optimization method,
    which is commonly used in nonlinear least squares fitting problems. The LM algorithm
    combines the ideas of both gradient descent and Gauss-Newton methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s a basic outline of how LM backpropagation works:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the gradient of the error function with respect to the weights using
    the standard backpropagation algorithm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Hessian matrix, which represents the curvature of the error surface
    with respect to the weights
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the weights using a combination of gradient descent and the LM optimization
    method
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The LM backpropagation algorithm adapts the learning rate during training. When
    the error surface is steep, it behaves more like gradient descent, which helps
    in avoiding overshooting. When the error surface is relatively flat, it behaves
    more like the Gauss-Newton method, which speeds up convergence. This combination
    of techniques makes LM backpropagation an efficient and effective algorithm for
    training neural networks, particularly in cases where standard backpropagation
    might face convergence issues or slow learning rates. It is commonly used in various
    applications, including pattern recognition, function approximation, and nonlinear
    regression tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After that, we have to set the number of nodes in the hidden layer:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we have to create our ANN using the `fitnet()` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `fitnet()` function is a built-in MATLAB function used to create and train
    a feedforward neural network with a single hidden layer for function fitting,
    pattern recognition, and regression tasks. The following parameters are passed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`LnodesNum`: This is a vector that specifies the number of neurons in each
    hidden layer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tfunc`: This parameter specifies the training function to be used for training
    the neural network. It represents the optimization algorithm that updates the
    network weights during the training process. Some commonly used training functions
    include `trainlm` (LM), `trainbfg` (BFGS Quasi-Newton), and `traingd` (gradient
    descent).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training the algorithm is necessary to preprocess the data, as we showed in
    the *Exploring data wrangling* section, in the *Exploring MATLAB for Machine Learning*
    chapter. In this case, we can use the neural network processing functions, available
    in the Neural Network Toolbox. To print all the general data preprocessing functions
    available, we can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following functions are listed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`removerows`: Eliminate rows from the matrix based on specified indices'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapminmax`: Map the minimum and maximum values of each matrix row to the range
    [-1, 1]'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`processpca`: Perform principal component analysis on the matrix rows'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapstd`: Map the row means and deviations of the matrix to standard values'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fixunknowns`: Handle matrix rows with unknown values using a specific procedure'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two represent the default functions applied to feedforward multilayer
    networks and are therefore the ones we will apply to our case:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As already mentioned, these are the default processing functions applied to
    both input and output. The first removes the constant records, as they do not
    bring any content for data adaptation, and the second instead maps the elements
    of a matrix or vector from their original range to a specified target range.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then the preprocessing of data is necessary to operate **data splitting**.
    This is a common technique used in machine learning and data analysis to divide
    a dataset into separate subsets for different purposes. The main reason for data
    splitting is to have distinct portions of data for training, validation, and testing,
    which allows for the evaluation and improvement of machine learning models. There
    are several techniques to split the data; in this case, the dataset is split into
    three parts: a training set, a validation set, and a testing set. The training
    set is used for model training, the validation set is used to tune hyperparameters
    and optimize the model, and the testing set is used for final evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This attribute determines the dimensions of the target data to be partitioned
    when invoking the data division function. The default value is `sample` for static
    networks and `time` for dynamic networks. Alternatively, it can be set to `sampletime`
    to divide targets by both sample and timestep, `all` to partition targets at each
    scalar value, or `none` to keep the data undivided (meaning all data is used for
    training and none for validation or testing).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s now set the percentage of data to use for the different phases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is essential to perform data splitting carefully to avoid issues such as
    data leakage and ensure reliable model evaluation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before training the ANN, it is necessary to choose something. To start with,
    we must choose the evaluation metrics to check the performance of the model that
    we are setting. Evaluation metrics are quantitative measures used to assess the
    performance and effectiveness of a model, algorithm, system, or process. In various
    fields, such as machine learning, data science, and information retrieval, evaluation
    metrics are essential for comparing different methods, tuning parameters, and
    understanding the strengths and weaknesses of a particular solution. The choice
    of evaluation metric depends on the specific problem being addressed and the goals
    of the analysis. The selection of appropriate metrics depends on the nature of
    the problem and the objectives of the analysis. It’s essential to choose evaluation
    metrics that align with the specific goals and requirements of the task at hand.
    We can use a command such as this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A list of the evaluation metrics with a short summary will be printed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we must choose the plot functions to get a visual representation of
    the results. To do that, we can use the `plotFcns()` function, which contains
    a one-dimensional cell array of strings that defines the plot functions associated
    with a network. The neural network training window, accessible through the `train()`
    function, displays a button for each plot function. Simply click on the respective
    button during or after the training process to open the desired plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A list of the plots available for an ANN with a short summary will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we can train the ANN already set; for that, we can use the `train()` function
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function trains a shallow neural network, and three arguments are passed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`AbaFitNet`: The neural network model object.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Input`: The input data used for training the neural network. It should be
    a matrix where each row represents a single input pattern. The abalone dataset
    is a collection of features describing the physical dimensions of an abalone.
    This dataset consists of eight features: sex, length, diameter, height, whole
    weight, shucked weight, viscera weight, and shell weight.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Target`: The target data corresponding to the input patterns. It should be
    a matrix with the same number of rows as the input matrix, where each row contains
    the corresponding target values. The target is the age of an abalone, measured
    in the form of the number of rings.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `train()` function performs the training process using the specified input
    data and target data. The type of training algorithm used depends on the specific
    neural network model and its settings. After training, the object returned will
    contain the trained neural network, which can be used for making predictions on
    new data using the tasks related to the trained network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: During the training phase, a new window will be opened (as shown in *Figure
    5**.3*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Training results window](img/B21156_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Training results window
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.3*, we can check the training progress, which shows information
    such as **Epoch**, **Elapsed Time**, **Performance**, **Gradient**, **Mu**, and
    **Validation** **Checks**. Furthermore, the properties set in the previous steps
    are summarized. Finally, several buttons related to the plot set are available.
  prefs: []
  type: TYPE_NORMAL
- en: By clicking on the buttons at the bottom of this window, we can draw the specific
    plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the ANN is trained, it is time to test the network. Testing a neural
    network involves evaluating its performance on a separate test dataset that the
    network has not seen during the training phase. This step helps assess the generalization
    ability of the trained network and ensures it can make accurate predictions on
    unseen data. To test the ANN that we trained, we can apply the network to the
    unseen data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this piece of code, we start evaluating the network on all the data, obtaining
    a first value of performance to compare with other ones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following performance is returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we have to recalculate training, validation, and testing performance as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following performance values are returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, to show the plots, we can use the following commands:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `plottrainstate()` function plots the training states returned by the `train()`
    function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, to evaluate the network’s ability to estimate the model target, we
    used the `plotregression()` function. This function plots the linear regression
    of targets relative to outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following plots will be printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – ANN plots](img/B21156_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – ANN plots
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we will be able to evaluate the trend of the training parameters
    throughout the process period by period. Furthermore, we will have a statistic
    of the distribution of errors and, finally, an indication of the position of the
    forecasts compared to the real values through the regression graph.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.4*, we can analyze the regression line of the model. The regression
    line is a key element in statistical data analysis, providing valuable information
    about a model’s ability to predict the data. This straight line represents the
    mathematical relationship between the independent and dependent variables, trying
    to minimize the difference between the observed values and those predicted by
    the model. The slope of the line reflects the average change in the dependent
    variable for each unit change in the independent variable. If the slope is close
    to `1`, the model accurately predicts the data. Additionally, the intercept indicates
    the value of the dependent variable when the independent variable is 0.
  prefs: []
  type: TYPE_NORMAL
- en: The goodness of fit of the model is measured through the correlation coefficient
    *R*, which measures the strength of the association between two variables. A high
    *R* indicates good predictive ability. It is important to note that the regression
    line may have limitations in its predictive ability if the data has complex or
    nonlinear patterns. In such cases, more advanced models may be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: After having analyzed an example of data fitting in detail, we will now see
    how to tackle a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering pattern recognition using ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pattern recognition** is a branch of machine learning and AI that focuses
    on the identification of patterns or regularities in data. It involves the automatic
    discovery and extraction of meaningful information from datasets, with the aim
    of categorizing or classifying data into different classes or groups. Overall,
    pattern recognition plays a crucial role in automating the process of identifying
    patterns and making decisions based on data, making it a fundamental component
    of many modern AI systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Pattern recognition offers numerous benefits in automating decision-making and
    handling complex data. However, it also poses challenges related to data quality,
    interpretability, and computational requirements. To leverage its advantages effectively,
    practitioners need to carefully design and train models while being aware of potential
    limitations and biases in the data and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The classical approach to pattern recognition uses data collected by sensors
    as input to a classification system. This data generally represents typical measurements
    such as kinematic and kinetic data from a motion analysis system. The measurements
    are usually subjected to a preprocessing phase. This is done to improve the signal
    properties. Subsequent feature extraction provides a feature vector for subsequent
    classification. This vector describes the input measurements in feature space.
    In supervised classification, labeled feature vectors are presented to a classifier
    for training. The vectors used to train the classifier form the training set.
    These labels assign a feature vector to one of several possible classes. In the
    recognition phase, the trained classifier uses this decision rule and automatically
    assigns a feature vector to a class. Different classifiers can be used, using
    different learning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now analyze a practical example of pattern recognition using MATLAB.
    In this section, our objective is to develop a classification model that can categorize
    thyroid disease based on various patient data. The steps involved are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To initiate the process, we acquire the data for analysis. For this study,
    we will utilize an existing dataset that comes pre-packaged with MATLAB. As mentioned
    earlier, MATLAB offers several readily available databases that can be easily
    imported into the workspace using the `load` command, followed by the specific
    database name. In this instance, we will work with the `thyroid_dataset` as our
    chosen dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the MATLAB workspace, we now have two variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Target` variable, classes are represented by a `1` in the first, second, or
    third row. A `1` in the first row indicates the patient is classified as normal
    (not hyperthyroid), a `1` in the second row indicates hyperfunction (hyperthyroidism),
    and a `1` in the third row indicates subnormal functioning (hypothyroidism).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific problem at hand is to determine whether a patient referred to the
    clinic is hypothyroid. It is important to note that due to many patients not being
    hyperthyroid (92%), a successful classifier must achieve a significantly higher
    accuracy than 92% to be considered effective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, it is time to select the appropriate training function for the neural
    network. To set a specific training algorithm for the network, you can use the
    `trainFcn` property and assign the name of the desired function. Among several
    available algorithms, we will opt for the **scaled conjugate gradient** (**SCG**)
    backpropagation method. This method is an optimization algorithm commonly used
    in training ANNs, particularly for solving the problem of weight updates during
    the training process. It is an alternative to other optimization techniques, such
    as gradient descent, **stochastic gradient descent** (**SGD**), and various flavors
    of backpropagation. SCG is known for its efficiency and speed in converging to
    a minimum of the loss function. The advantages of the SCG algorithm include its
    ability to converge quickly and efficiently in many cases, making it a good choice
    for training neural networks with relatively small to moderate-sized datasets.
    However, it may not always outperform other optimization methods on large datasets
    or in more complex network architectures. The choice of optimization algorithm
    often depends on the specific problem, dataset size, and computational resources
    available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After selecting the training algorithm, the next step is to construct the neural
    network. To achieve this, we need to determine the number of nodes in the hidden
    layer. In this case, we have decided to build a function-fitting neural network
    with one hidden layer consisting of 10 nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can construct a pattern recognition network using the `patternnet()` function.
    Pattern recognition networks are feedforward networks designed for classifying
    inputs based on target classes. The target data for these networks should consist
    of vectors with all `0` values except for a `1` in the element corresponding to
    the class they represent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `patternnet()` function accepts the following arguments:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hiddenSizes`: A row vector specifying one or more hidden layer sizes. The
    default is `10` if a value is not provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trainFcn`: The training function to be used. The default is `trainscg` (SCG
    backpropagation).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`performFcn`: The performance function used during training. The default is
    `crossentropy`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function returns a pattern recognition neural network with the specified
    architecture and settings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the network has been constructed and the data has been preprocessed, the
    next step is to partition the data into separate sets for training, validation,
    and testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To achieve this, we typically divide the available dataset into three different
    subsets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Training set**: This subset is used to train the neural network. The network
    learns from the input data and corresponding target outputs during the training
    process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: The validation set is used to fine-tune the network’s hyperparameters
    and prevent overfitting. It helps in optimizing the network’s performance by evaluating
    its performance on data it has not seen during training.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing set**: This set is used to evaluate the final performance of the
    trained network. It provides an unbiased estimate of the network’s generalization
    ability on unseen data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The division of data can be achieved using various techniques, such as random
    sampling, stratified sampling, or time-based splitting, depending on the nature
    of the data and the specific problem:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The significance of operations and the types of functions used in the network
    construction and data preprocessing steps was extensively covered previously.
    In case of any uncertainties, it is recommended that you review that paragraph
    for a more detailed understanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Regarding measuring network performance, we will select the **cross-entropy
    performance** function. This specific performance function is well suited for
    classification and pattern recognition tasks. It quantifies the network’s performance
    by calculating the cross-entropy, which measures the difference between estimated
    and actual class memberships.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By utilizing the cross-entropy performance function, we can effectively evaluate
    how well the neural network performs in classifying inputs and make informed decisions
    about its effectiveness for the given pattern recognition problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By dividing the data into these separate sets, we can ensure that the neural
    network’s performance is not only measured on the training data but also validated
    on unseen data, making the evaluation more reliable and indicative of the network’s
    true capabilities:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can now set plot functions to visualize the results of the simulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to initiate the training process for the network using the
    `train()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While training the neural network, the **Neural Network Training** window will
    be displayed. This window comprises four sections, each offering valuable information
    throughout the training process:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Neural network**: This section provides a summary of the neural network’s
    architecture and configuration, including the number of layers, nodes in each
    layer, and the chosen training algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithms**: In this area, details about the training algorithm being used,
    such as the specific optimization technique and convergence criteria, are presented.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Progress**: The progress section shows real-time updates on the training
    process, such as the current epoch, training error, and validation performance.
    It allows monitoring of the network’s performance as it improves over successive
    epochs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plots**: This section displays various plots, such as training and validation
    errors over epochs, enabling a visual assessment of the network’s learning progress
    and potential overfitting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Neural Network Training** window (*Figure 5**.5*) offers a comprehensive
    view of the training procedure, providing insights into the network’s behavior
    and performance at different stages. These insights help fine-tune the network
    and make informed decisions to optimize its performance for the pattern recognition
    task at hand.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Neural Network Training window for pattern recognition](img/B21156_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Neural Network Training window for pattern recognition
  prefs: []
  type: TYPE_NORMAL
- en: Once the network training is complete, we can employ the trained model to test
    its performance on the same input data used during the training phase. By doing
    so, we can obtain the results and utilize them for evaluation purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Testing the network on the same data helps us assess how well the model generalizes
    to familiar inputs and provides insights into its effectiveness in handling real-world
    scenarios. The evaluation results obtained from this process aid in understanding
    the network’s accuracy, precision, recall, and other relevant metrics, allowing
    us to make informed decisions about its overall performance and suitability for
    the intended pattern recognition task:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `vec2ind()` function is used to convert vectors to indices. It allows indices
    to be represented either directly as themselves or as vectors with a `1` in the
    row corresponding to the index they represent. In the context of this problem,
    `TargetInd` and `SimDataInd` are vectors containing values `1`, `2`, or `3`, representing
    the classes to which the targets and outputs belong. The last row of these vectors
    contains the percentage of error occurrences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following value is obtained:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s proceed with the evaluation of the network. The following commands
    extract the outputs and targets that pertain to the training, validation, and
    test subsets. This extracted data will be used in the subsequent step to construct
    the confusion matrix, which will aid in further assessing the performance of the
    network on each of these subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A **confusion matrix** is a valuable tool that allows us to compare the classification
    results of our model to the real data. It provides insights into the nature and
    quantity of classification errors. The matrix consists of cells where the diagonal
    elements represent the number of cases that were correctly classified, while the
    off-diagonal elements show the misclassified cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In an ideal situation, a machine learning algorithm should perfectly discriminate
    between two populations (such as healthy and diseased) that are not overlapping
    (mutually exclusive). However, in real-world scenarios, the two populations often
    overlap to some extent, leading to the algorithm making some false positive and
    false negative predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 5**.6* displays the confusion matrix for the training, testing, and
    validation phases, as well as a combined matrix that considers all three sets
    of data together.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B21156_05_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.6 – The confusion matrix for training, testing, and validation, and
    the three kinds of data combined
  prefs: []
  type: TYPE_NORMAL
- en: This visual representation will help us understand how well the model performed
    across different phases and its overall classification performance considering
    all data subsets. The confusion matrix has been computed for the training, testing,
    and validation phases, as well as for the combination of all three data subsets.
    This comprehensive matrix provides a detailed overview of the model’s classification
    performance across various evaluation stages and gives a complete picture of its
    effectiveness in handling different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.6*, the blue cell located in the bottom-right corner represents
    the total percentage of correctly classified cases, which are depicted by green
    cells positioned diagonally. The red cells in the matrix signify the total percentage
    of misclassified cases located in other cells. The confusion matrix organizes
    data with rows representing the actual values and columns representing the predicted
    values. For instance, in the top-left plot of the figure, the first row indicates
    that 85 cases are correctly classified as `1` (normal), 5 cases are incorrectly
    classified as `2` (hyperfunction), and 8 cases are incorrectly classified as `3`
    (subnormal). By observing the blue cell in the bottom right of each plot in the
    confusion matrix, we can deduce that the classification accuracy is consistently
    high, exceeding 92%. These results demonstrate excellent recognition capabilities.
    If higher accuracy is required, retraining the data could be considered. It should
    be noted that the starting dataset is not correctly balanced across all classes.
    This can result in a high-performance value for one class and a low-performance
    value for another. It is therefore advisable to always work on datasets that are
    correctly balanced across all classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method of evaluating network performance is through the **receiver
    operating characteristic** (**ROC**). The ROC curve is a valuable tool for assessing
    the model’s performance in terms of sensitivity and specificity across various
    classification thresholds. The subsequent command generates plots for the ROC
    during each evaluation phase and for the entire process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The ROC is a metric used to evaluate the performance of classifiers. It assesses
    the quality of a classifier for each class by applying threshold values across
    the interval `[0, 1]` to its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, ROC plots are displayed for the training, testing,
    and validation phases, as well as for the combined data from all three subsets.
    These ROC curves allow us to analyze the classifier’s sensitivity and specificity
    across different classification thresholds for each class, providing valuable
    insights into its discrimination capabilities and overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – ROC curves for training, testing, and validation, and the three
    kinds of data combined](img/B21156_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – ROC curves for training, testing, and validation, and the three
    kinds of data combined
  prefs: []
  type: TYPE_NORMAL
- en: In the graph, the colored lines on each axis represent the ROC curves. The ROC
    curve illustrates the relationship between the **true positive rate** (**TPR,
    or sensitivity**) and the **false positive rate** (**FPR**) as the classification
    threshold is varied. TPR measures the proportion of actual positive cases that
    are correctly identified as positive by the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: TPR = TP / (TP + FN)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TP* = True positives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FN* = False negatives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, it calculates the ability of the model to detect all positive
    instances in the dataset. FPR measures the proportion of actual negative cases
    that are incorrectly classified as positive by the classifier. It quantifies the
    rate of false alarms or false positives made by the model.
  prefs: []
  type: TYPE_NORMAL
- en: FPR = FP / (FP + TN)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*FP* is the number of false positives, which are the number of negative examples
    that are incorrectly classified as positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TN* is the number of true negatives, which are the number of negative examples
    that are correctly classified as negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ideal test would exhibit points located in the upper-left corner, indicating
    100% sensitivity and 100% specificity. The closer the lines approach the upper-left
    corner, the better the network’s performance, indicating its ability to achieve
    higher sensitivity while keeping the FPR low, leading to improved classification
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: After tackling a pattern recognition problem using the tools available in MATLAB,
    in the next practical example, we will see how to tackle a clustering problem
    with the help of the Neural Network tool.
  prefs: []
  type: TYPE_NORMAL
- en: Building a clustering application with an ANN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a popular unsupervised machine learning technique used for grouping
    similar data points together in a dataset. The goal of clustering is to partition
    data into clusters in such a way that data points within the same cluster are
    more like each other than those in other clusters. We examined this topic in depth
    in [*Chapter 4*](B21156_04.xhtml#_idTextAnchor084)*, Clustering Analysis and*
    *Dimensionality Reduction*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will see how to address a clustering problem using an ANN
    in the MATLAB environment. So far, to train a neural network in the MATLAB environment,
    we have used the commands available from the command line, or much more simply
    to be implemented in a script with the `.m` extension to reproduce the algorithm
    whenever we like. But MATLAB has out-of-the-box apps that let us use a wizard
    to train an ANN. We will do this to address a clustering problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an overview of the apps made available by the Neural Network tool, we
    can type the following on the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following window will open:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Neural Network start window](img/B21156_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Neural Network start window
  prefs: []
  type: TYPE_NORMAL
- en: 'Four apps are available: `nftool` command to start the `nprtool` command to
    open the **Pattern** **Recognition** app.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use the **Clustering** app, which is started using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following window will open:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – MATLAB app for clustering using ANN](img/B21156_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – MATLAB app for clustering using ANN
  prefs: []
  type: TYPE_NORMAL
- en: In this app, we can use a wizard for the training of the ANN. Let’s see how.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we have to import the data. For this example, we will use a dataset
    already available in the MATLAB environment to explain the clustering application
    using an ANN. To import that dataset in the MATLAB workspace, we use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we have two matrices in the MATLAB workspace (`Input` and `Target`). We
    will use only the first (wx1,000 double) for a clustering problem, which is the
    `Input` matrix with two variables and 1,000 records. By examining the structure
    of the `Target` (4x1,000 double) matrix, we realize that four classes are available,
    so the data has `4` groups of data, which will be useful in justifying the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we have to import the data in the app. To do this, we can click on the
    **Import** button in the window shown in *Figure 5**.9*. The window shown in *Figure
    5**.10* will open:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Import data in Clustering app](img/B21156_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Import data in Clustering app
  prefs: []
  type: TYPE_NORMAL
- en: The right matrix was already detected for **Predictors**; if you want to change
    the data, you can select the right one using the browse button next to the **Predictors**
    field. In this case, we have to set the observations in columns, so we will have
    1,000 observations with 2 features. Let’s just click on the **OK** button to import
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: We have the ANN architecture already set in the app, so we can identify the
    number of inputs and the topology of the network. The output will be a network
    with 10x10 nodes (100 nodes). The `nctool` guides you in solving clustering problems
    by employing a **self-organizing map** (**SOM**). This map creates a condensed
    depiction of the input space, capturing both the density patterns of input vectors
    in that space and a compressed, two-dimensional representation of the input space’s
    topology.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SOMs**, also known as **Kohonen maps**, are a type of ANN that belongs to
    the family of unsupervised learning algorithms. They were introduced by the Finnish
    professor Teuvo Kohonen in the 1980s. SOMs are used for tasks such as dimensionality
    reduction, data visualization, clustering, and feature extraction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The primary idea behind SOMs is to map high-dimensional input data onto a lower-dimensional
    grid or lattice in such a way that similar input data points are mapped to nearby
    grid cells. This results in a topological representation of the input data, where
    similar data points are located close to each other on the map, allowing for easier
    visualization and interpretation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'SOMs consist of two essential layers: the input layer and the output layer,
    often referred to as the feature map. The input layer serves as the initial stage
    in a SOM. Each data point from the dataset competes for representation to identify
    its own characteristics. The process begins with weight vector initialization,
    kickstarting the mapping process of the SOM.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The mapped vectors are subsequently scrutinized to identify the weight vector
    that best represents the chosen sample using a randomly selected sample vector.
    Nearby weights to each weighted vector are considered, and the chosen weight evolves
    into a vector for the random sample, fostering the map’s growth and the emergence
    of new patterns. In a two-dimensional feature space, these patterns often take
    on hexagonal or square shapes. This entire process is repeated over 1,000 times.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In essence, learning occurs as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each node is analyzed to determine whether its weights are similar to the input
    vector. The node that best matches the input vector is termed the **best matching**
    **unit** (**BMU**).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The neighborhood value of the BMU is then established, and over time, the number
    of neighbors tends to decrease.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The BMU’s weight vector further adapts to resemble the sample vector, leading
    to similar changes in the surrounding areas. The weight of a node changes more
    as it gets closer to the BMU and less as it moves away from its neighbors.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 for *N* iterations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This iterative process allows the SOM to refine its representation of the input
    data, ultimately leading to a more organized and compressed representation that
    captures the underlying structure of the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we can train the network just by clicking on the **train** button of the
    app. After a few seconds, the ANN will be trained and ready for use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can view the results using the plots available at the top of the
    **App** window. We will first use the neighbor distances, and the following plot
    will be drawn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.11 – SOM neighbor weight distances](img/B21156_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – SOM neighbor weight distances
  prefs: []
  type: TYPE_NORMAL
- en: This is a graphical representation that illustrates the distances between weight
    vectors of neighboring nodes in a SOM. This plot helps visualize how the weights
    of neighboring nodes change during the learning process, providing insights into
    the topology and organization of the SOM. We can identify more colored cells that
    separate the nodes in four areas using the class available in the `Target` matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The blue hexagons represent the neurons. The red lines connect neighboring neurons.
    The colors in the regions containing the red lines indicate the distances between
    neurons. The darker colors represent larger distances, and the lighter colors
    represent smaller distances.
  prefs: []
  type: TYPE_NORMAL
- en: One visualization tool for the SOM is the weight distance matrix (also called
    the U-matrix). To view the U-matrix, click **SOM Neighbor Distances** in the training
    window.
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows us that there are four potential clusters in the data correctly
    detected by the algorithm. To have a confirmation of this indication, we can plot
    the weight position plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – SOM weight positions](img/B21156_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – SOM weight positions
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.12*, the four clusters can be easily identified. In this diagram,
    the input vectors are depicted as green dots, and it illustrates the SOM’s classification
    of the input space by displaying blue-gray dots representing each neuron’s weight
    vector. Additionally, neighboring neurons relate to red lines, offering a visual
    representation of the SOM’s organizational structure.
  prefs: []
  type: TYPE_NORMAL
- en: After having also analyzed a case of clustering with ANNs, to complete the topic,
    we will see how to optimize the results obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring advanced optimization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Advanced optimization techniques** are powerful methods used to enhance the
    efficiency and effectiveness of optimization algorithms. These techniques aim
    to overcome the limitations of traditional optimization approaches, particularly
    in complex, high-dimensional, or non-convex optimization problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, advanced optimization techniques are essential for training
    complex models effectively, improving convergence, avoiding overfitting, and handling
    high-dimensional data. In the following subsection, some advanced optimization
    techniques commonly used in machine learning are listed.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SGD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SGD** is a popular and fundamental optimization algorithm used in machine
    learning for training models, especially in large-scale and complex settings.
    It’s a variant of the traditional gradient descent method designed to address
    efficiency and convergence issues when dealing with large datasets. The stochastic
    aspect of SGD comes from the fact that it uses a random mini-batch for each iteration,
    making the optimization process more stochastic (randomized) compared to the deterministic
    nature of regular gradient descent, which uses the entire dataset for each update.
    This stochasticity introduces noise into the gradient estimates, which can help
    the algorithm escape local minima, converge faster, and generalize better.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary objective of any optimization algorithm is to minimize the discrepancy
    between the predicted values of the model and the actual values observed in the
    data. The smaller the error between the observed and predicted values, the more
    effective the algorithm is at simulating the real-world scenario. Minimizing this
    discrepancy is equivalent to optimizing an objective function on which the model
    construction is based.
  prefs: []
  type: TYPE_NORMAL
- en: 'Descent methods are iterative techniques that, commencing from an initial point
    x0 ∈ Rn, produce a sequence of points {xn} ∈ N based on the subsequent equation:'
  prefs: []
  type: TYPE_NORMAL
- en: x n+1 = x n + γ n * g n
  prefs: []
  type: TYPE_NORMAL
- en: 'In a descent method, the vector *g**n* represents the search direction, and
    the scalar *γ**n* serves as a positive parameter known as the step length, determining
    the distance of movement in the *g**n* direction. These choices of *g**n* and
    *γ**n* are made to ensure the reduction of the objective function *f* in each
    iteration, following the principle:'
  prefs: []
  type: TYPE_NORMAL
- en: f x n+1 < f x n ∀ n ≥ 0
  prefs: []
  type: TYPE_NORMAL
- en: We select the vector *g**n* as a descent direction, ensuring that the line *x
    = x**n* *+ γ**n* ** g**n* creates an obtuse angle with the gradient vector ∇ f(xn).
    This guarantees the decrease of the objective function *f*, as long as the value
    of *γ**n* is sufficiently small. This approach allows for various descent methods,
    contingent on the specific choice of *g**n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A gradient is a function that produces a vector, representing the slope of
    the tangent to the graph of a function. It points in the direction of the greatest
    increase in the function. Let’s examine the convex function illustrated in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – How the gradient descent algorithm searches the global optimum](img/B21156_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – How the gradient descent algorithm searches the global optimum
  prefs: []
  type: TYPE_NORMAL
- en: The primary objective of the gradient descent algorithm is to locate the function’s
    nadir, or lowest point. In more precise terms, the gradient functions as a derivative,
    indicating the incline or steepness of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: To provide a clearer analogy, let’s imagine we find ourselves lost in the mountains
    at night with limited visibility. Our perception is restricted to sensing the
    slope of the terrain beneath our feet. The aim is to reach the lowest point of
    the mountain. Achieving this goal involves taking successive steps in the direction
    of the steepest slope. We proceed iteratively, advancing step by step, until we
    finally arrive at the valley of the mountain.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll examine a two-variable function, denoted as *f(x, y)*, with its gradient
    represented as a vector encompassing the partial derivatives of *f*. The first
    derivative pertains to *x*, while the second derivative pertains to *y*. Upon
    computation of these partial derivatives, the results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: δf _ δx  ,  δf _ δy
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial expression corresponds to the partial derivative concerning *x*,
    whereas the subsequent expression pertains to the partial derivative regarding
    *y*. The gradient is represented by the ensuing vector:'
  prefs: []
  type: TYPE_NORMAL
- en: ∇ f(x, y) = ⎡ ⎢ ⎣ δf _ δx   δf _ δy  ⎤ ⎥ ⎦
  prefs: []
  type: TYPE_NORMAL
- en: The given equation represents a function in a two-dimensional space, effectively
    forming a two-dimensional vector. Each component of this vector signifies the
    steepest ascent direction for the respective function variable. Consequently,
    the gradient points toward the direction where the function exhibits the most
    significant increase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, if we consider a function with five variables, the resulting gradient
    vector will encompass five partial derivatives. In general, a function with *n*
    variables gives rise to an *n*-dimensional gradient vector, exemplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ∇ f(x, y, … .z) = ⎡ ⎢ ⎣  δf _ δx   δf _ δy  … …  δf _ δz  ⎤ ⎥ ⎦
  prefs: []
  type: TYPE_NORMAL
- en: When utilizing gradient descent, our objective isn’t to maximize *f* as rapidly
    as possible; instead, we aim to minimize it—specifically, to locate the smallest
    point that minimizes the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a function *y = f(x)*. The foundation of gradient descent relies on
    the observation that when the function *f* is well defined and differentiable
    within a vicinity of *x*, it experiences a faster decrease as we proceed in the
    direction opposite to the negative gradient. Commencing from an initial value
    of *x*, we can express this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x**n* *+ 1 =* *x**n* - *γ* ***∇ *f(**x**n**)*'
  prefs: []
  type: TYPE_NORMAL
- en: where gamma (*γ*) is learning rate and delta (∇) is the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*γ* represents the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∇ signifies the gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using sufficiently small values of *γ*, the algorithm converges to the
    minimum value of the function *f* within a finite number of iterations. In essence,
    when the gradient is negative, it indicates a decreasing objective function at
    that point, implying that the parameter should shift toward larger values to approach
    a minimum point. Conversely, when the gradient is positive, the parameters should
    move toward smaller values to attain lower values of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Adam optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adaptive moment estimation** (**Adam**) is an advanced optimization algorithm
    commonly used for training deep learning models, particularly in neural networks.
    It is an extension of the SGD optimization method that adapts the learning rate
    for each parameter individually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gradient descent algorithm aims to locate the objective function’s minimum
    using an iterative approach. In each step, an approximation of the gradient is
    computed to guide the descent in the direction that effectively reduces the objective
    function. Within this process, the selection of the learning rate parameter holds
    significant importance. This parameter governs the speed at which we approach
    the optimal objective function values:'
  prefs: []
  type: TYPE_NORMAL
- en: If the learning rate is excessively small, a substantial number of iterations
    are required to converge toward the optimal values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversely, an overly high learning rate might cause us to overlook the optimal
    solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam adjusts the learning rates for each parameter based on the first-order
    moment (mean) and second-order moment (uncentered variance) of the gradients.
    This adaptability helps the algorithm converge more efficiently, especially in
    high-dimensional spaces with varying gradient magnitudes.
  prefs: []
  type: TYPE_NORMAL
- en: This method addresses the issue of initial bias in the moments by correcting
    them. In the early stages of training, the moments’ estimates may be biased toward
    0, and Adam compensates for this bias, particularly when the learning rate is
    low.
  prefs: []
  type: TYPE_NORMAL
- en: Adam combines the concepts of momentum (accumulating a fraction of the past
    gradients to enhance convergence) and **RMSProp** (scaling the learning rates
    based on the magnitudes of recent gradients) to perform well on a wide range of
    optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Adam optimization algorithm involves the following calculations during
    each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the gradient of the loss with respect to the model’s parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the first and second moments (mean and uncentered variance) of the gradients
    using a moving average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct the bias in the moments (especially in the early iterations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the parameters using the corrected moments and the learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, Adam is effective in many deep learning scenarios, often providing
    faster convergence than standard SGD with manually tuned learning rates. It’s
    a popular choice for training neural networks and has become a standard optimization
    algorithm in many deep learning frameworks due to its adaptive properties and
    strong performance.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing second-order methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Second-order optimization methods**, also known as **Newton-like methods**,
    are advanced techniques used for solving optimization problems. Unlike first-order
    methods (such as gradient descent) that primarily rely on gradients, second-order
    methods take advantage of both gradients and second-order derivatives (**Hessian
    matrix**) of the objective function. This additional information can lead to faster
    convergence and more accurate solutions, especially in complex and non-convex
    optimization landscapes.'
  prefs: []
  type: TYPE_NORMAL
- en: Second-order methods consider not only the first-order derivative (gradient)
    of the function but also its second-order derivative (Hessian matrix). The Hessian
    matrix captures curvature information and provides insights into the shape of
    the objective function’s surface. These methods often use quadratic approximations
    of the objective function around the current point. These approximations consider
    the first and second derivatives and provide a more accurate representation of
    the local behavior of the function.
  prefs: []
  type: TYPE_NORMAL
- en: The use of curvature information from the Hessian matrix can lead to faster
    convergence rates compared to first-order methods, especially when the objective
    function is well behaved and smooth. Second-order methods can be more robust against
    the choice of learning rates or step sizes, as they inherently adjust the step
    size based on the curvature of the function.
  prefs: []
  type: TYPE_NORMAL
- en: Second-order methods are valuable tools for optimization, particularly when
    computational resources allow for the calculation of second-order derivatives.
    However, their application may depend on the specific characteristics of the problem
    being solved.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a second-order method is Newton’s method. This method directly
    minimizes a quadratic approximation of the objective function using the Hessian
    matrix. Newton’s method is a classic optimization algorithm used to find the minimum
    or maximum of a function. Named after Sir Isaac Newton, this method utilizes both
    the gradient (first derivative) and the second derivative (Hessian matrix) of
    the function to iteratively approach the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach adopts the structure of Newton’s method, which is commonly used
    to locate the roots of a function, but in this case, it’s applied to the derivative
    of function *f*. The rationale behind this is that identifying the minimum point
    of function f is equivalent to finding the root of its first derivative f′. In
    this scenario, the updated formula can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: x n+1 = x n −  f ′ ( x n) _ f ″  (x n)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the given equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: f′ (xn) represents the first derivative of function *f*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f″ (xn) signifies the second derivative of function *f*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second-order methods are typically favored over gradient descent due to their
    faster convergence rate, provided that the analytical expressions for both first
    and second derivatives are available. However, these methods converge regardless
    of whether they’re approaching minima or maxima. Variations of this method exist
    that ensure global convergence and reduce computational costs by sidestepping
    the need to solve the system for determining the search direction using direct
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained insight into simulating typical human brain activities
    using ANNs. We grasped the fundamental concepts behind ANNs, delving into the
    creation of a basic neural network architecture. This exploration encompassed
    elements such as input, hidden, and output layers, connection weights, and activation
    functions. Our understanding extended to crucial decisions regarding hidden layer
    count, node quantity within each layer, and network training algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Then we focused on data fitting and pattern recognition using neural networks.
    We engaged in script analysis to master the utilization of neural network functions
    via the command line. We then ventured into the Neural Network Toolbox, featuring
    algorithms, pre-trained models, and apps for crafting, training, visualizing,
    and simulating shallow and deep neural networks. The Neural Network Toolbox offers
    an accessible interface—the **Neural Network** getting started GUI—which serves
    as the launchpad for tasks such as neural network fitting, pattern recognition,
    clustering, and time-series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced some of the commonly used advanced optimization techniques.
    These methods are potent techniques employed to amplify the efficiency and efficacy
    of optimization algorithms. Their purpose is to surmount the constraints posed
    by conventional optimization methodologies, especially in intricate, high-dimensional,
    or non-convex optimization scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand the basic concepts of deep learning.
    We also will learn about the different types of deep learning and understand **convolutional****neural
    networks** (**CNNs**). Additionally, we will learn how to build a CNN using MATLAB
    and understand recurrent neural networks, long short-term memory networks, and
    transformer models.
  prefs: []
  type: TYPE_NORMAL
