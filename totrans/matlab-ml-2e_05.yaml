- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Artificial Neural Network Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**) include data structures and algorithms
    for learning and classifying data. Through neural network techniques, a program
    can learn through examples and create an internal structure of rules to classify
    different inputs. MATLAB provides algorithms, pre-trained models, and apps to
    create, train, visualize, and simulate ANNs. In this chapter, we will see how
    to use MATLAB to build an ANN-based model to predict values and classify data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with ANNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and testing an ANN model in MATLAB
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data fitting with ANNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering pattern recognition using ANNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a clustering application with an ANN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring advanced optimization techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic concepts relating to machine learning.
    To understand these topics, a basic knowledge of algebra and mathematical modeling
    is needed. You will also need a working knowledge of the MATLAB environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '`ANNFitting.m`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ANNPatReg.m`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with ANNs
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serial computers and their programs exhibit remarkable capabilities in executing
    tasks that involve repetitive, well-defined operations, prioritizing accuracy,
    reliability, and speed. While these information-processing systems are highly
    valuable, they lack true intelligence. The sole element of intelligence lies with
    the programmer who comprehends the task and formulates the program. To achieve
    true **artificial intelligence** (**AI**), a system must possess the ability to
    solve problems that humans consider simple, trivial, and intuitive.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts relating to ANNs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ANNs are designed to emulate the intricate workings of biological nervous systems,
    comprising a vast array of nerve cells or neurons interconnected in a complex
    network. Typically, each neuron establishes connections with tens of thousands
    of other neurons, resulting in hundreds of billions of connections. The emergence
    of intelligent behavior stems from the myriad interactions among these interconnected
    units.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In this network, specific units serve distinct functions. Some units act as
    receivers of information from the environment, while others respond to stimuli
    in the environment. Certain units, known as hidden units, solely communicate within
    the network, concealed from direct interaction with the external environment.
    Overall, the neural network’s structure involves input units, output units, and
    hidden units working in concert to process information and exhibit intelligent
    behavior.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Each unit in the neural network executes a simple operation: it activates when
    the cumulative received signal surpasses an activation threshold. Once active,
    the unit transmits a signal to other connected units through communication channels.
    These connections act as filters, transforming the messages into excitatory or
    inhibitory signals, and adjusting their intensity based on individual characteristics.
    Remarkably, the network’s input-output link, or transfer function, is not explicitly
    programmed but rather acquired through a learning process using empirical data,
    which can be supervised, unsupervised, or reinforcement learning.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每个单元执行一个简单的操作：当累积接收到的信号超过激活阈值时，它会被激活。一旦激活，该单元将通过通信通道向其他连接的单元发送信号。这些连接充当过滤器，将消息转换为兴奋或抑制信号，并根据个体特征调整其强度。值得注意的是，网络的输入-输出链接，或传递函数，不是通过显式编程获得的，而是通过使用经验数据的学习过程获得的，这个过程可以是监督学习、无监督学习或强化学习。
- en: ANNs are a type of AI that can learn from experience. This learning is possible
    thanks to their structure, which is similar to that of the human brain. Like the
    human brain, ANNs are composed of a large number of nodes, which are connected
    to each other by connections. Nodes process information, and connections determine
    how information is transmitted from one node to another. ANNs learn from experience
    through a process called supervised learning. In this process, ANNs are provided
    with a dataset of desired input and output examples. ANNs then use this data to
    learn to map inputs to desired outputs. For example, an ANN that needs to learn
    to recognize images of dogs is provided with a dataset of images of dogs and images
    of other animals. The ANN uses these images to learn to distinguish between dogs
    and other animals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（ANNs）是一种可以从经验中学习的AI类型。这种学习之所以可能，是因为它们的结构类似于人脑。像人脑一样，神经网络由大量节点组成，这些节点通过连接相互连接。节点处理信息，连接决定了信息如何从一个节点传输到另一个节点。神经网络通过称为监督学习的过程从经验中学习。在这个过程中，神经网络被提供了一组期望的输入和输出示例的数据集。然后，神经网络使用这些数据来学习将输入映射到期望的输出。例如，需要学习识别狗的图像的神经网络被提供了一组狗的图像和其他动物的图像数据集。神经网络使用这些图像来学习区分狗和其他动物。
- en: Neural networks operate in parallel, enabling them to handle multiple data simultaneously,
    in contrast to serial computers, which process data individually and sequentially.
    Although individual neurons may be relatively slow, the parallel nature of neural
    networks accounts for the brain’s higher processing speed when tackling tasks
    requiring the simultaneous handling of numerous data points, such as visual object
    recognition. This remarkable system exhibits robust noise immunity, akin to a
    sophisticated statistical model. Even in the event of some unit malfunctions,
    the overall network performance may experience reductions, but a complete system
    shutdown is unlikely to occur.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络并行运行，这使得它们能够同时处理多个数据，与逐个且按顺序处理数据的串行计算机相比。尽管单个神经元可能相对较慢，但神经网络的并行特性解释了在处理需要同时处理大量数据点的任务时，如视觉物体识别，大脑的高处理速度。这个非凡的系统表现出强大的抗噪声能力，类似于复杂的统计模型。即使在某些单元出现故障的情况下，整体网络性能可能会下降，但完全的系统关闭不太可能发生。
- en: Nevertheless, the latest generation of neural network software demands a solid
    understanding of statistics. Despite their seemingly immediate usability, users
    must not be deceived, as they can quickly make predictions or classifications
    within certain limitations. From an industrial perspective, these networks prove
    effective when historical data is available for processing with neural algorithms.
    This capability is particularly valuable in production environments as it facilitates
    data extraction and model creation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最新一代的神经网络软件需要具备扎实的统计学知识。尽管它们看起来具有即时的可用性，但用户不应被欺骗，因为它们可以在某些限制内快速做出预测或分类。从工业角度来看，当有历史数据可用并使用神经网络算法进行处理时，这些网络证明是有效的。这种能力在生产环境中尤其有价值，因为它促进了数据提取和模型创建。
- en: It’s essential to note that while models generated by neural networks are highly
    efficient, they lack explanations in human symbolic language. The outcomes must
    be accepted as they are, leading to the characterization of neural networks as
    black boxes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意的是，虽然神经网络生成的模型非常高效，但它们缺乏人类符号语言中的解释。结果必须按原样接受，这导致神经网络被描述为黑盒。
- en: Like any modeling algorithm, the efficiency of neural networks relies heavily
    on the careful selection of predictive variables. These networks require a training
    phase to establish individual neuron weights, which can be time-consuming when
    dealing with many records and variables. Unlike conventional models, neural networks
    lack theorems or definitive guidelines, making the success of a network heavily
    dependent on the creator’s experience.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何建模算法一样，神经网络的效率在很大程度上依赖于预测变量的谨慎选择。这些网络需要一个训练阶段来建立单个神经元的权重，当处理大量记录和变量时，这个过程可能会很耗时。与传统的模型不同，神经网络缺乏定理或明确的指导方针，这使得网络的成功高度依赖于创造者的经验。
- en: Neural networks find their utility in scenarios where data may be partially
    inaccurate or where analytical models are unavailable for problem-solving. They
    are commonly used in Optical Character Recognition (OCR) software, facial recognition
    systems, and other applications that handle error-prone or noisy data. Moreover,
    they are widely employed in data mining analysis and serve as tools for forecasting
    in financial and meteorological domains. In recent years, their significance has
    substantially increased in the field of bioinformatics, where they are instrumental
    in identifying functional and structural patterns in nucleic acids and proteins.
    By providing a comprehensive set of input data, the network can produce the most
    probable output.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在数据可能部分不准确或分析模型不可用于解决问题的场景中找到其用途。它们通常用于光学字符识别（OCR）软件、面部识别系统和其他处理易出错或噪声数据的应用程序。此外，它们在数据挖掘分析和作为金融和气象领域预测工具的广泛使用。近年来，它们在生物信息学领域的意义显著增加，在那里它们在识别核酸和蛋白质中的功能和结构模式方面发挥着关键作用。通过提供一套全面的输入数据，网络可以产生最可能的输出。
- en: Understanding how perceptrons work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解感知器的工作原理
- en: The fundamental unit of a neural network is the **perceptron**, which emulates
    the essential functions of a biological neuron. It evaluates the strength of each
    input, accumulates these inputs, and then compares the sum to a specific threshold.
    Based on this comparison, the perceptron determines the output value. The neuron’s
    basic structure is well understood, and researchers have identified the primary
    biochemical reactions that govern its activity. As such, a neuron can be regarded
    as the elemental computational unit of the brain. Within the human brain, approximately
    100 distinct classes of neurons have been identified, each contributing to the
    intricate neural network responsible for our cognitive processes and abilities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本单元是**感知器**，它模拟了生物神经的基本功能。它评估每个输入的强度，累积这些输入，然后将总和与一个特定的阈值进行比较。基于这个比较，感知器确定输出值。神经元的基本结构已被充分理解，研究人员已经确定了控制其活动的主要生化反应。因此，神经元可以被视为大脑的基本计算单元。在人类大脑中，已经确定了大约100种不同的神经元类别，每种类别都为负责我们认知过程和能力的复杂神经网络做出贡献。
- en: '![Figure 5.1 – Understanding the perceptron scheme](img/B21156_05_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 理解感知器方案](img/B21156_05_01.jpg)'
- en: Figure 5.1 – Understanding the perceptron scheme
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 理解感知器方案
- en: The primary function of a biological neuron is to generate an electrical potential
    that travels along its **axon** (neuron output) when the electrical activity at
    the neuron’s cell body surpasses a specific threshold. The neuron’s input is received
    through a set of fibers called **dendrites**, which contact the axons of other
    neurons, transmitting electrical potentials from them. The point of connection
    between an axon of one neuron and the dendrite of another neuron is referred to
    as a **synapse**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的根本功能是在神经元细胞体的电活动超过特定阈值时，在其**轴突**（神经元输出）上产生一个电势。神经元的输入通过称为**树突**的一组纤维接收，这些树突接触其他神经元的轴突，从它们那里传递电势。一个神经元的轴突与另一个神经元的树突之间的连接点被称为**突触**。
- en: 'The synapse possesses the ability to regulate the electrical pulse emanating
    from the axon. The electrical potential generated by a neuron is essentially binary:
    an on/off state. If the neuron’s electrical activity surpasses a specific threshold,
    an impulse is generated; otherwise, no impulse occurs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 突触具有调节从轴突发出的电脉冲的能力。神经元产生的电势基本上是二元的：开/关状态。如果神经元的电活动超过特定阈值，就会产生一个脉冲；否则，不会产生脉冲。
- en: Notably, the intensity of the generated pulse remains consistent across different
    neurons. As the potential propagates along the axon and reaches the synapse connected
    to another neuron’s dendrite, the post-synaptic potential relies on the biochemical
    characteristics of the synapse. Despite having the same pre-synaptic potential,
    two distinct synapses may generate varying post-synaptic potentials. In other
    words, the synapse modulates and weighs the input potential before transmission.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，生成的脉冲强度在不同神经元之间保持一致。当潜力沿着轴突传播并达到连接到另一个神经元树突的突触时，突触后电位依赖于突触的生化特性。尽管具有相同的突触前电位，两个不同的突触可能会产生不同的突触后电位。换句话说，突触在传输之前调节并权衡输入电位。
- en: Post-synaptic potentials continue through the neuron’s dendrites and accumulate
    at the soma level. Only when the sum of these potentials surpasses a certain threshold
    does the neuron trigger the potential to propagate through its axon. Both biological
    neurons and artificial neurons receive multiple inputs through dendrites. The
    artificial neuron aggregates these various input values and computes the result.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 突触后电位通过神经元的树突继续传播并在细胞体水平积累。只有当这些潜力的总和超过某个特定阈值时，神经元才会触发潜力通过其轴突传播。生物神经元和人工神经元都通过树突接收多个输入。人工神经元将这些各种输入值聚合并计算结果。
- en: If the computed value exceeds a particular threshold, the artificial neuron
    produces an output signal or potential; otherwise, it remains inactive.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算值超过特定阈值，人工神经元会产生输出信号或潜力；否则，它保持不活跃。
- en: The initial function implemented by the artificial neuron is the algebraic sum
    of its inputs, which serves to construct the system’s response. When simulating
    a phenomenon, the system may encounter errors, necessitating appropriate corrections.
    To achieve this, each input is assigned a weight, a numerical value that modulates
    its impact on the total sum, determining the neuron’s potential.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元最初实现的功能是其输入的代数和，这有助于构建系统的响应。在模拟现象时，系统可能会遇到错误，需要适当的纠正。为了实现这一点，每个输入都被分配了一个权重，这是一个数值，它调节其对总和的影响，从而确定神经元的潜力。
- en: In other words, each input contributes differently to the determination of the
    threshold value and potential triggering, akin to the characteristic of biological
    neurons involving synapses between the axon of one neuron and the dendrite of
    another.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每个输入对确定阈值值和触发潜力的贡献各不相同，类似于涉及一个神经元轴突与另一个神经元树突之间突触的生物神经元特性。
- en: From the perspective of the post-synaptic neuron, the inputs consist of potentials
    from other neurons whose axons synapse with their dendrites, and these inputs
    are precisely modulated by the synapses. Some inputs may exert a stronger influence
    on the total sum, while others could even be inhibitory, reducing the overall
    sum and thereby lowering the probability of exceeding the threshold and triggering
    a potential.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从突触后神经元的视角来看，输入包括来自其他神经元的潜力，这些神经元的轴突与它们的树突形成突触，并且这些输入被突触精确调节。一些输入可能对总和产生更强的影响，而其他输入甚至可能是抑制性的，减少总和并因此降低超过阈值并触发潜力的概率。
- en: This essential property of biological systems is mathematically modeled in connection
    systems using the concept of weights. Each connection is assigned a numeric value
    as its weight, which is multiplied by the input value. Consequently, the input’s
    effect on the total sum is determined by the magnitude of its weight.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种生物系统的基本特性在连接系统中使用权重概念进行数学建模。每个连接都被分配一个数值作为其权重，该数值乘以输入值。因此，输入对总和的影响由其权重的幅度决定。
- en: Activation function to introduce non-linearity
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数用于引入非线性
- en: In our previous discussion, we explored the weighted sum function introduced
    by incorporating the concept of weights. Now, let’s delve into another property
    of the artificial neuron, once again inspired by the behavior of biological neurons.
    As mentioned earlier, the biological neuron sums up the post-synaptic potentials
    of its dendrites at the soma level. However, this summation is not a simple algebraic
    addition of these potentials. Various factors, such as the passive resistance
    of the neuron membrane, come into play, making the actual summation a function
    that is typically non-linear.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, artificial neurons compute the weighted inputs and then modify the
    result using a specific function. This function is referred to as the activation
    function, which is applied to the output of the neuron to determine its true potential.
    The activation function plays a crucial role in shaping the behavior of the neuron
    and ultimately influences the outcome of the neural network’s computation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The activation function takes the weighted sum of inputs and an additional bias
    term and applies a specific mathematical operation to produce the neuron’s output.
    This output, often referred to as the activation or the post-activation value,
    is then used as input to the subsequent layers in the neural network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Different types of activation functions are utilized in neural networks, each
    with its unique characteristics and applications. Common activation functions
    include the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '`1` if the input is greater than or equal to a threshold, and `0` otherwise:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = 0 if x < 0, 1 if x ≥ 0
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '`0` and `1`. It was commonly used in the past but has fallen out of favor in
    deeper networks due to the vanishing gradient problem:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) =  1 _ 1 + e −x
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '`0`. It helps mitigate the vanishing gradient problem and accelerates convergence
    in deep networks:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = 0 if x < 0, x if x ≥ 0
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '`-1` and `1`. It is symmetric around the origin, providing a better range for
    the gradient compared to the sigmoid function:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = tanh (x)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '`1`:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f (x) i =  e −x i _ ∑ i=1 k  e −x i
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**Exponential linear unit (ELU)**: ELU is an activation function used in ANNs.
    It is a smooth, non-saturating function that can handle both positive and negative
    inputs. ELU is a more recent activation function than ReLU, which is also widely
    used:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = x if x > 0
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: alpha * (exp(x) − 1) if x < 0
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Here, *x* is the input to the ELU function and *α* is a hyperparameter that
    controls the steepness of the negative slope. The default value of *α* is `1.0`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The ELU function is a smooth, non-saturating function that can handle both positive
    and negative inputs. This makes it a good choice for tasks that involve both types
    of inputs, such as image recognition and natural language processing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the activation function influences the neural network’s performance,
    training speed, and ability to capture complex relationships in the data. Different
    activation functions may be used in different layers of the network, depending
    on the specific problem and architectural considerations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择会影响神经网络的性能、训练速度以及捕捉数据中复杂关系的能力。根据具体问题和架构考虑，网络的不同层可能会使用不同的激活函数。
- en: ANN’s architecture explained
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ANN架构解释
- en: Having discussed the properties of an artificial neuron, we will now delve into
    the architecture of a neural network. This entails both a physical illustration
    of the network’s structure and a determination of the role each neuron plays within
    this framework. Consider a scenario with multiple inputs and nodes, such that
    each input is connected to every node. Similarly, each output node receives connections
    from all inputs. Each output node possesses the characteristics described earlier
    and carries out its computations in conjunction with the other nodes. Upon introducing
    an input pattern, the output values are influenced either by the input values
    themselves or by the network’s weights. The weights play a crucial role in the
    network, determining the extent to which a particular input influences a specific
    node. The collection of nodes in the structure is commonly referred to as a layer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了人工神经元的特性之后，我们现在将深入研究神经网络的架构。这包括网络结构的物理说明以及确定每个神经元在这个框架中的作用。考虑一个有多个输入和节点的场景，其中每个输入都与每个节点相连。同样，每个输出节点也接收来自所有输入的连接。每个输出节点具有前面描述的特性，并与其他节点一起执行计算。在引入输入模式后，输出值要么受输入值本身的影响，要么受网络权重的影响。在网络中，节点集合通常被称为层。
- en: 'Neural networks are organized into layers, each serving a specific purpose
    in information processing. The primary types of layers in a neural network include
    the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络被组织成层，每一层在信息处理中都有其特定的作用。神经网络中的主要层类型包括以下几种：
- en: '**Input layer**: The initial layer that receives the input data and passes
    it on to the subsequent layers.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：接收输入数据并将其传递给后续层的初始层。'
- en: '**Hidden layers**: Intermediate layers between the input and output layers.
    These layers process the data and extract relevant features through complex transformations.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：位于输入层和输出层之间的中间层。这些层通过复杂的转换处理数据并提取相关特征。'
- en: '**Output layer**: The final layer that produces the network’s output or predictions
    based on the processed information from the hidden layers.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：产生网络输出或预测的最终层，基于从隐藏层处理过的信息。'
- en: The number of hidden layers and nodes within them, along with the choice of
    activation functions and weights, constitute the architecture of the neural network.
    The architecture (*Figure 5**.2*) significantly influences the network’s ability
    to learn from data, generalize to new examples, and perform specific tasks efficiently.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的数量以及其中的节点数，以及激活函数和权重的选择，构成了神经网络的架构。架构（*图5**.2*）显著影响了网络从数据中学习、泛化到新示例以及高效执行特定任务的能力。
- en: '![Figure 5.2 – ANN architecture with weights and activation function](img/B21156_05_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 带权重和激活函数的ANN架构](img/B21156_05_02.jpg)'
- en: Figure 5.2 – ANN architecture with weights and activation function
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 带权重和激活函数的ANN架构
- en: Neural networks can take the form of multiple layers, and each layer added enhances
    the network’s computational capacity. Inputs are numerical values that undergo
    evaluation through the weights of connections with the first layer of nodes, known
    as the hidden layer. In this hidden layer, each node conducts computations as
    described earlier, leading to the generation of a potential that then propagates
    to the nodes of the output layer. The potentials produced by the output nodes
    collectively represent the final output calculated by the neural network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以由多层组成，每增加一层都会增强网络的计算能力。输入是数值，通过与第一层节点（称为隐藏层）的连接权重进行评估。在这个隐藏层中，每个节点按照前面描述的方式进行计算，从而生成潜在的输出，然后传播到输出层的节点。输出节点产生的潜在值共同代表了神经网络计算出的最终输出。
- en: The architecture of a neural network refers to the specific way in which the
    nodes are interconnected. In the case of feedforward neural networks, which are
    characterized by the architecture shown in the previous figure, the activation
    of input nodes propagates forward through the hidden layer and further to the
    output layer. Changing the connections between nodes alters the network’s architecture.
    This not only yields practical consequences in terms of the network’s computational
    capacity but also carries significant theoretical implications related to the
    concept of learning. The arrangement of nodes in the network affects its ability
    to learn from data and perform specific tasks, making architecture design a crucial
    aspect of neural network development.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的架构指的是节点相互连接的具体方式。在前面图表中显示的前馈神经网络的情况下，输入节点的激活通过隐藏层向前传播，进一步传播到输出层。改变节点之间的连接会改变网络的架构。这不仅对网络的计算能力产生实际影响，而且与学习概念相关的理论意义也非常重大。网络中节点的排列会影响其从数据中学习以及执行特定任务的能力，使架构设计成为神经网络开发的一个关键方面。
- en: After analyzing the basic concepts of ANNs, we now need to pay attention to
    how these algorithms are trained.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析了ANN的基本概念之后，我们现在需要关注这些算法是如何被训练的。
- en: Training and testing an ANN model in MATLAB
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MATLAB中训练和测试ANN模型
- en: 'In the previous section, we saw the architecture of an ANN. It imposes two
    layers, input and output, which cannot be altered. Consequently, the critical
    factor lies in the number of hidden layers we consider. The size of a neural network
    is defined by the number of hidden neurons. Determining the optimal size of the
    network remains an ongoing challenge, as no analytical solution has been discovered
    to date. One approach to tackle this problem is to employ a heuristic method:
    creating various networks with increasing complexity, using a subset of the training
    data, and monitoring the error on a validation subset simultaneously. After completing
    the training process, the network with the lowest validation error is chosen as
    the preferred one.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了ANN的架构。它强加了两个层，输入层和输出层，这些层不能改变。因此，关键因素在于我们考虑的隐藏层数量。神经网络的大小由隐藏神经元的数量定义。确定网络的最佳大小仍然是一个持续性的挑战，因为迄今为止还没有发现解析解决方案。解决这个问题的方法之一是采用启发式方法：创建具有递增复杂性的各种网络，使用训练数据的一个子集，并同时在验证子集上监控错误。完成训练过程后，选择具有最低验证错误的网络作为首选。
- en: How to train an ANN
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何训练ANN
- en: Let’s discuss the process of choosing the number of layers. The number of input
    nodes is fixed based on the number of features in the input data, while the number
    of output nodes is determined by the number of outcomes to be modeled or the class
    levels in the outcome. The real challenge lies in determining the appropriate
    number of neurons for the hidden layer. Unfortunately, there is no analytical
    method to accomplish this task. The optimal number of neurons depends on various
    factors, such as the number of input nodes, the volume of training data, and the
    complexity of the learning algorithm, among others.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论选择层数的过程。输入节点的数量基于输入数据中的特征数量是固定的，而输出节点的数量则由要建模的结果数量或结果中的类别级别决定。真正的挑战在于确定隐藏层中适当神经元数量。不幸的是，没有解析方法来完成这项任务。最佳神经元数量取决于各种因素，例如输入节点的数量、训练数据量以及学习算法的复杂性等。
- en: Having more neurons in the hidden layer will lead to a model that better fits
    the training data, but it comes with the risk of overfitting, potentially resulting
    in poor generalization on future data. Additionally, neural networks with many
    nodes can be computationally expensive and slow to train.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中拥有更多神经元将导致一个更好地拟合训练数据的模型，但这也伴随着过拟合的风险，可能导致对未来数据的泛化能力较差。此外，具有许多节点的神经网络可能在计算上昂贵且训练缓慢。
- en: To address this, a heuristic approach can be adopted, where different configurations
    are experimented with to find an optimal balance. This trial-and-error method
    allows us to strike a balance between model complexity, accuracy, and computational
    efficiency.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，可以采用启发式方法，通过实验不同的配置来找到最佳平衡。这种试错方法使我们能够在模型复杂性、准确性和计算效率之间取得平衡。
- en: ANNs consist of simple elements that operate in parallel. The connections between
    these elements play a vital role as they dictate the network’s functionalities.
    These connections influence the output through their respective weights, which
    are adjusted during the neural network’s training phase.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: During training, the network is fine-tuned by modifying the connection weights,
    enabling specific inputs to yield desired outputs. For instance, the network can
    be calibrated by comparing its practical output with the target output we want
    to achieve. This iterative process continues until the network’s output aligns
    with the desired target.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: To obtain dependable results, a substantial number of input/target pairs are
    required to shape the network appropriately. This training process ensures that
    the neural network can accurately produce the desired outputs for a variety of
    inputs, making it a reliable tool for various tasks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The adjustment of these weights is determined by the specific algorithm we choose
    to adopt. In the following practical examples, we will discuss and refer to various
    algorithms that govern the process of weight adjustment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the MATLAB Neural Network Toolbox
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Neural Network Toolbox** offers a range of algorithms, pre-trained models,
    and apps that enable users to create, train, visualize, and simulate neural networks.
    It supports both shallow neural networks (with one hidden layer) and deep neural
    networks (with multiple hidden layers). With these tools, various tasks, such
    as classification, regression, clustering, dimensionality reduction, time-series
    forecasting, and dynamic system modeling and control, can be performed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four primary ways to utilize the Neural Network Toolbox:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '`nnstart` command, granting access to automatic tasks such as function fitting
    (`nftool`), pattern recognition (`nprtool`), data clustering (`nctool`), and time-series
    analysis (`ntstool`).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Basic command-line operations**: For greater flexibility, users can utilize
    command-line operations. While more knowledge is required, this method allows
    users to have complete control over the process, without relying on menus and
    icons typically found in the GUI.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizing the toolbox**: Users can customize the toolbox by creating their
    own neural networks with arbitrary connections. Existing toolbox training features
    in the GUI can be used to continue training these custom networks.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying toolbox functions**: All computational components in the toolbox
    are written in MATLAB code and are fully accessible, allowing users to modify
    and tailor them to specific needs.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This toolbox caters to users of all levels, from beginners to experts. It offers
    simple tools that guide new users through specific applications and more complex
    tools that enable experts to customize networks and experiment with new architectures.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the approach chosen, a proper analysis using neural networks
    should encompass the following steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network creation
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network configuration
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weight and bias initialization
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network training
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network validation
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Network testing
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By following these steps, users can effectively apply neural networks to various
    problems and tasks. They are explained in more detail here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the process involves collecting the data to be analyzed, which
    is typically done outside the MATLAB environment. This data collection phase is
    crucial, as the quality of the data will significantly impact the final results
    and the ability to extract meaningful insights.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we proceed to create the neural network using various functions available
    in the toolbox. These functions allow us to build the network through the chosen
    algorithm, resulting in the creation of a neural network object. This object stores
    all the necessary information defining the neural network’s properties, such as
    its architecture, subobject structures, functions, and weight and bias values.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third step is network configuration, where we examine input and output data,
    set the dimensions of the network to fit the data, and choose appropriate input
    and output processing settings to enhance network performance. This configuration
    step is usually performed automatically when the training function is called but
    can also be done manually using the configuration function.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After configuration, the fourth step involves initializing the weights and biases.
    We set initial values from which the network will begin its training process.
    This initialization is usually done automatically based on the chosen training
    algorithm, but users can also set custom values if needed.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fifth step is network training, which is a critical phase of the process.
    During training, the weights and biases are fine-tuned to optimize the network’s
    performance. This stage is crucial for the network’s ability to generalize well
    with new, unseen data. A portion of the collected data (typically around 70% of
    available cases) is used for training.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, in the sixth step, network validation takes place. Here, a fraction of
    randomly selected data (usually around 15% of available cases) is passed through
    the network to estimate how well the model has been trained. The results obtained
    during this phase help determine whether the chosen model adequately reflects
    the initial expectations or whether adjustments are needed.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, in the last step, we utilize the trained network. A portion of the
    collected data (approximately 15% of available cases) is used to test the network’s
    performance. The trained neural network object can then be saved and employed
    multiple times with new data as needed. This allows for the reuse of the network
    to make predictions or analyze various datasets.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The workflow for **neural network design** involves breaking down the collected
    data into three sets: the **training set**, the **validation set**, and the **test
    set**. Let’s describe each of them in detail:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set (usually 70% of the available cases)**: The training set is
    a collection of examples used to train the neural network and adjust its parameters.
    During the training process, the network learns from the input-output pairs in
    the training set to optimize its internal weights and biases. The goal is to find
    the optimal set of parameters that best captures the underlying patterns and relationships
    in the data. The neural network improves its performance through iterative adjustments
    during the training phase.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set (usually 15% of the available cases)**: The validation set
    is a separate set of examples used to fine-tune the network’s parameters and assess
    its performance during training. It serves as a validation mechanism to prevent
    overfitting, a condition where the network performs well on the training data
    but poorly on new, unseen data. By monitoring the network’s performance on the
    validation set, we can make decisions about the model’s complexity, such as determining
    the optimal number of hidden units or identifying a suitable stopping point for
    the training algorithm. Adjustments based on the validation set help ensure the
    network generalizes well to new data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test set (usually 15% of the available cases)**: The test set is a separate
    and independent collection of examples used solely to evaluate the performance
    of a fully trained neural network. After the training and validation stages, the
    final model is assessed using the test set to estimate its error rate and validate
    its generalization capabilities. It is crucial to refrain from making any further
    adjustments to the model based on the test set evaluation to avoid bias or overfitting.
    The test set provides an unbiased measure of how well the neural network is likely
    to perform on new, real-world data.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By dividing the data into these three distinct sets, the neural network design
    workflow ensures that the model is trained, validated, and tested in a controlled
    and reliable manner, allowing for accurate assessments of its performance and
    generalization capabilities.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: With a clear understanding of this process, we are now ready to proceed with
    our work on analyzing a practical example of an ANN implementation in MATLAB.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data fitting with ANNs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data fitting** is the process of constructing a curve or mathematical function
    that best matches a given set of collected data points. This curve fitting can
    involve either interpolations, where exact data points are fitted, or smoothing,
    where a smooth function approximates the data. In the context of regression analysis,
    curve fitting is closely related to statistical inference, considering uncertainties
    arising from random errors in observed data.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The approximate curves obtained through data fitting have multiple applications.
    They can be used to visualize and display the data, predict function values in
    regions with no available data, and summarize the relationships between multiple
    variables. This process is valuable for understanding and interpreting complex
    datasets, making predictions, and gaining insights from the collected information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the trend of a particular distribution using mathematical formulas
    can be challenging, and these formulas may not always accurately represent all
    the data or cover the entire range of existence. To address such cases, machine
    learning algorithms come to the rescue. These algorithms can build models without
    relying on complex mathematical formulas.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are well suited for data-fitting tasks and trend prediction. They can adapt
    and learn from a given set of inputs and associated target outputs. **Function
    fitting** is the process of training a neural network with such input-output pairs,
    enabling it to form a generalization of the underlying input-output relationship.
    Once trained, the neural network can generate outputs for inputs it has not encountered
    during training.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of neural networks allows them to capture complex patterns and
    relationships in the data, making them powerful tools for data-fitting and prediction
    tasks. By utilizing machine learning algorithms such as neural networks, we can
    overcome the limitations of traditional mathematical formulas and achieve accurate
    predictions across various datasets and scenarios.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The performance of algorithms based on machine learning strongly depends on
    the quality of the data being worked on. The data collection process typically
    occurs outside the MATLAB environment, which means that you need to have a properly
    collected data file ready to initiate an analysis in MATLAB. However, if you don’t
    have access to the data yet and are here to learn, there’s no need to worry because
    MATLAB has a solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The Neural Network Toolbox software provides several sample datasets that you
    can use to experiment with the functionality of the toolbox. These sample datasets
    are readily available and can serve as a starting point for your analysis. To
    explore the available datasets, you can use the following command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A list of datasets sorted by application type will be returned. With these
    sample datasets at your disposal, you can begin your learning journey and gain
    hands-on experience with the Neural Network Toolbox in MATLAB. Now we will work
    on an example of data fitting in MATLAB:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus our attention on the dataset for data fitting, specifically, `abalone_dataset`,
    which contains the *abalone shell rings dataset*. To load the dataset into the
    MATLAB workspace, let’s use the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Executing this command will load the data into the arrays named `Input` and
    `Target`. The aim of the model is to estimate the age of an abalone, utilizing
    physical measurements to achieve this prediction. help abalone_dataset
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A comprehensive description of the dataset is provided, including details such
    as the number of attributes, the total number of items, and a list of variables.
    Additionally, the description offers valuable insights into potential use cases
    for the dataset.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the data, we have to choose the training algorithm and set
    the architecture network. In MATLAB different functions are available for training.
    To get a list of training algorithms available, we can use the following command:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LM backpropagation is an optimization algorithm used in training ANNs. It is
    an extension of the standard backpropagation algorithm that improves convergence
    and robustness, especially for nonlinear and ill-conditioned problems. In standard
    backpropagation, the algorithm adjusts the weights of the neural network using
    the gradient of the error with respect to the weights. However, in some cases,
    this process can be slow, and the algorithm might get stuck in local minima. LM
    backpropagation addresses this issue by incorporating the LM optimization method,
    which is commonly used in nonlinear least squares fitting problems. The LM algorithm
    combines the ideas of both gradient descent and Gauss-Newton methods.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s a basic outline of how LM backpropagation works:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the gradient of the error function with respect to the weights using
    the standard backpropagation algorithm
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Hessian matrix, which represents the curvature of the error surface
    with respect to the weights
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the weights using a combination of gradient descent and the LM optimization
    method
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The LM backpropagation algorithm adapts the learning rate during training. When
    the error surface is steep, it behaves more like gradient descent, which helps
    in avoiding overshooting. When the error surface is relatively flat, it behaves
    more like the Gauss-Newton method, which speeds up convergence. This combination
    of techniques makes LM backpropagation an efficient and effective algorithm for
    training neural networks, particularly in cases where standard backpropagation
    might face convergence issues or slow learning rates. It is commonly used in various
    applications, including pattern recognition, function approximation, and nonlinear
    regression tasks.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After that, we have to set the number of nodes in the hidden layer:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we have to create our ANN using the `fitnet()` function as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `fitnet()` function is a built-in MATLAB function used to create and train
    a feedforward neural network with a single hidden layer for function fitting,
    pattern recognition, and regression tasks. The following parameters are passed:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`LnodesNum`: This is a vector that specifies the number of neurons in each
    hidden layer.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tfunc`: This parameter specifies the training function to be used for training
    the neural network. It represents the optimization algorithm that updates the
    network weights during the training process. Some commonly used training functions
    include `trainlm` (LM), `trainbfg` (BFGS Quasi-Newton), and `traingd` (gradient
    descent).'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training the algorithm is necessary to preprocess the data, as we showed in
    the *Exploring data wrangling* section, in the *Exploring MATLAB for Machine Learning*
    chapter. In this case, we can use the neural network processing functions, available
    in the Neural Network Toolbox. To print all the general data preprocessing functions
    available, we can use the following command:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following functions are listed:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`removerows`: Eliminate rows from the matrix based on specified indices'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapminmax`: Map the minimum and maximum values of each matrix row to the range
    [-1, 1]'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`processpca`: Perform principal component analysis on the matrix rows'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapstd`: Map the row means and deviations of the matrix to standard values'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fixunknowns`: Handle matrix rows with unknown values using a specific procedure'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two represent the default functions applied to feedforward multilayer
    networks and are therefore the ones we will apply to our case:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As already mentioned, these are the default processing functions applied to
    both input and output. The first removes the constant records, as they do not
    bring any content for data adaptation, and the second instead maps the elements
    of a matrix or vector from their original range to a specified target range.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then the preprocessing of data is necessary to operate **data splitting**.
    This is a common technique used in machine learning and data analysis to divide
    a dataset into separate subsets for different purposes. The main reason for data
    splitting is to have distinct portions of data for training, validation, and testing,
    which allows for the evaluation and improvement of machine learning models. There
    are several techniques to split the data; in this case, the dataset is split into
    three parts: a training set, a validation set, and a testing set. The training
    set is used for model training, the validation set is used to tune hyperparameters
    and optimize the model, and the testing set is used for final evaluation:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This attribute determines the dimensions of the target data to be partitioned
    when invoking the data division function. The default value is `sample` for static
    networks and `time` for dynamic networks. Alternatively, it can be set to `sampletime`
    to divide targets by both sample and timestep, `all` to partition targets at each
    scalar value, or `none` to keep the data undivided (meaning all data is used for
    training and none for validation or testing).
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s now set the percentage of data to use for the different phases:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It is essential to perform data splitting carefully to avoid issues such as
    data leakage and ensure reliable model evaluation.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before training the ANN, it is necessary to choose something. To start with,
    we must choose the evaluation metrics to check the performance of the model that
    we are setting. Evaluation metrics are quantitative measures used to assess the
    performance and effectiveness of a model, algorithm, system, or process. In various
    fields, such as machine learning, data science, and information retrieval, evaluation
    metrics are essential for comparing different methods, tuning parameters, and
    understanding the strengths and weaknesses of a particular solution. The choice
    of evaluation metric depends on the specific problem being addressed and the goals
    of the analysis. The selection of appropriate metrics depends on the nature of
    the problem and the objectives of the analysis. It’s essential to choose evaluation
    metrics that align with the specific goals and requirements of the task at hand.
    We can use a command such as this:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A list of the evaluation metrics with a short summary will be printed.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we must choose the plot functions to get a visual representation of
    the results. To do that, we can use the `plotFcns()` function, which contains
    a one-dimensional cell array of strings that defines the plot functions associated
    with a network. The neural network training window, accessible through the `train()`
    function, displays a button for each plot function. Simply click on the respective
    button during or after the training process to open the desired plot:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A list of the plots available for an ANN with a short summary will be returned.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we can train the ANN already set; for that, we can use the `train()` function
    as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function trains a shallow neural network, and three arguments are passed:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`AbaFitNet`: The neural network model object.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Input`: The input data used for training the neural network. It should be
    a matrix where each row represents a single input pattern. The abalone dataset
    is a collection of features describing the physical dimensions of an abalone.
    This dataset consists of eight features: sex, length, diameter, height, whole
    weight, shucked weight, viscera weight, and shell weight.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Target`: The target data corresponding to the input patterns. It should be
    a matrix with the same number of rows as the input matrix, where each row contains
    the corresponding target values. The target is the age of an abalone, measured
    in the form of the number of rings.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `train()` function performs the training process using the specified input
    data and target data. The type of training algorithm used depends on the specific
    neural network model and its settings. After training, the object returned will
    contain the trained neural network, which can be used for making predictions on
    new data using the tasks related to the trained network.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: During the training phase, a new window will be opened (as shown in *Figure
    5**.3*).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Training results window](img/B21156_05_03.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Training results window
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.3*, we can check the training progress, which shows information
    such as **Epoch**, **Elapsed Time**, **Performance**, **Gradient**, **Mu**, and
    **Validation** **Checks**. Furthermore, the properties set in the previous steps
    are summarized. Finally, several buttons related to the plot set are available.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: By clicking on the buttons at the bottom of this window, we can draw the specific
    plot.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'After the ANN is trained, it is time to test the network. Testing a neural
    network involves evaluating its performance on a separate test dataset that the
    network has not seen during the training phase. This step helps assess the generalization
    ability of the trained network and ensures it can make accurate predictions on
    unseen data. To test the ANN that we trained, we can apply the network to the
    unseen data, as follows:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this piece of code, we start evaluating the network on all the data, obtaining
    a first value of performance to compare with other ones.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following performance is returned:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we have to recalculate training, validation, and testing performance as
    follows:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following performance values are returned:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, to show the plots, we can use the following commands:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `plottrainstate()` function plots the training states returned by the `train()`
    function.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Finally, to evaluate the network’s ability to estimate the model target, we
    used the `plotregression()` function. This function plots the linear regression
    of targets relative to outputs.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following plots will be printed:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – ANN plots](img/B21156_05_04.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – ANN plots
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we will be able to evaluate the trend of the training parameters
    throughout the process period by period. Furthermore, we will have a statistic
    of the distribution of errors and, finally, an indication of the position of the
    forecasts compared to the real values through the regression graph.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.4*, we can analyze the regression line of the model. The regression
    line is a key element in statistical data analysis, providing valuable information
    about a model’s ability to predict the data. This straight line represents the
    mathematical relationship between the independent and dependent variables, trying
    to minimize the difference between the observed values and those predicted by
    the model. The slope of the line reflects the average change in the dependent
    variable for each unit change in the independent variable. If the slope is close
    to `1`, the model accurately predicts the data. Additionally, the intercept indicates
    the value of the dependent variable when the independent variable is 0.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The goodness of fit of the model is measured through the correlation coefficient
    *R*, which measures the strength of the association between two variables. A high
    *R* indicates good predictive ability. It is important to note that the regression
    line may have limitations in its predictive ability if the data has complex or
    nonlinear patterns. In such cases, more advanced models may be necessary.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: After having analyzed an example of data fitting in detail, we will now see
    how to tackle a classification problem.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Discovering pattern recognition using ANNs
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pattern recognition** is a branch of machine learning and AI that focuses
    on the identification of patterns or regularities in data. It involves the automatic
    discovery and extraction of meaningful information from datasets, with the aim
    of categorizing or classifying data into different classes or groups. Overall,
    pattern recognition plays a crucial role in automating the process of identifying
    patterns and making decisions based on data, making it a fundamental component
    of many modern AI systems.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Pattern recognition offers numerous benefits in automating decision-making and
    handling complex data. However, it also poses challenges related to data quality,
    interpretability, and computational requirements. To leverage its advantages effectively,
    practitioners need to carefully design and train models while being aware of potential
    limitations and biases in the data and algorithms.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The classical approach to pattern recognition uses data collected by sensors
    as input to a classification system. This data generally represents typical measurements
    such as kinematic and kinetic data from a motion analysis system. The measurements
    are usually subjected to a preprocessing phase. This is done to improve the signal
    properties. Subsequent feature extraction provides a feature vector for subsequent
    classification. This vector describes the input measurements in feature space.
    In supervised classification, labeled feature vectors are presented to a classifier
    for training. The vectors used to train the classifier form the training set.
    These labels assign a feature vector to one of several possible classes. In the
    recognition phase, the trained classifier uses this decision rule and automatically
    assigns a feature vector to a class. Different classifiers can be used, using
    different learning strategies.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now analyze a practical example of pattern recognition using MATLAB.
    In this section, our objective is to develop a classification model that can categorize
    thyroid disease based on various patient data. The steps involved are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'To initiate the process, we acquire the data for analysis. For this study,
    we will utilize an existing dataset that comes pre-packaged with MATLAB. As mentioned
    earlier, MATLAB offers several readily available databases that can be easily
    imported into the workspace using the `load` command, followed by the specific
    database name. In this instance, we will work with the `thyroid_dataset` as our
    chosen dataset:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the MATLAB workspace, we now have two variables:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Target` variable, classes are represented by a `1` in the first, second, or
    third row. A `1` in the first row indicates the patient is classified as normal
    (not hyperthyroid), a `1` in the second row indicates hyperfunction (hyperthyroidism),
    and a `1` in the third row indicates subnormal functioning (hypothyroidism).'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific problem at hand is to determine whether a patient referred to the
    clinic is hypothyroid. It is important to note that due to many patients not being
    hyperthyroid (92%), a successful classifier must achieve a significantly higher
    accuracy than 92% to be considered effective.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, it is time to select the appropriate training function for the neural
    network. To set a specific training algorithm for the network, you can use the
    `trainFcn` property and assign the name of the desired function. Among several
    available algorithms, we will opt for the **scaled conjugate gradient** (**SCG**)
    backpropagation method. This method is an optimization algorithm commonly used
    in training ANNs, particularly for solving the problem of weight updates during
    the training process. It is an alternative to other optimization techniques, such
    as gradient descent, **stochastic gradient descent** (**SGD**), and various flavors
    of backpropagation. SCG is known for its efficiency and speed in converging to
    a minimum of the loss function. The advantages of the SCG algorithm include its
    ability to converge quickly and efficiently in many cases, making it a good choice
    for training neural networks with relatively small to moderate-sized datasets.
    However, it may not always outperform other optimization methods on large datasets
    or in more complex network architectures. The choice of optimization algorithm
    often depends on the specific problem, dataset size, and computational resources
    available:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After selecting the training algorithm, the next step is to construct the neural
    network. To achieve this, we need to determine the number of nodes in the hidden
    layer. In this case, we have decided to build a function-fitting neural network
    with one hidden layer consisting of 10 nodes:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can construct a pattern recognition network using the `patternnet()` function.
    Pattern recognition networks are feedforward networks designed for classifying
    inputs based on target classes. The target data for these networks should consist
    of vectors with all `0` values except for a `1` in the element corresponding to
    the class they represent.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `patternnet()` function accepts the following arguments:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hiddenSizes`: A row vector specifying one or more hidden layer sizes. The
    default is `10` if a value is not provided.'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trainFcn`: The training function to be used. The default is `trainscg` (SCG
    backpropagation).'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`performFcn`: The performance function used during training. The default is
    `crossentropy`.'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function returns a pattern recognition neural network with the specified
    architecture and settings:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once the network has been constructed and the data has been preprocessed, the
    next step is to partition the data into separate sets for training, validation,
    and testing.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To achieve this, we typically divide the available dataset into three different
    subsets:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Training set**: This subset is used to train the neural network. The network
    learns from the input data and corresponding target outputs during the training
    process.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: The validation set is used to fine-tune the network’s hyperparameters
    and prevent overfitting. It helps in optimizing the network’s performance by evaluating
    its performance on data it has not seen during training.'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing set**: This set is used to evaluate the final performance of the
    trained network. It provides an unbiased estimate of the network’s generalization
    ability on unseen data.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The division of data can be achieved using various techniques, such as random
    sampling, stratified sampling, or time-based splitting, depending on the nature
    of the data and the specific problem:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The significance of operations and the types of functions used in the network
    construction and data preprocessing steps was extensively covered previously.
    In case of any uncertainties, it is recommended that you review that paragraph
    for a more detailed understanding.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Regarding measuring network performance, we will select the **cross-entropy
    performance** function. This specific performance function is well suited for
    classification and pattern recognition tasks. It quantifies the network’s performance
    by calculating the cross-entropy, which measures the difference between estimated
    and actual class memberships.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By utilizing the cross-entropy performance function, we can effectively evaluate
    how well the neural network performs in classifying inputs and make informed decisions
    about its effectiveness for the given pattern recognition problem.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By dividing the data into these separate sets, we can ensure that the neural
    network’s performance is not only measured on the training data but also validated
    on unseen data, making the evaluation more reliable and indicative of the network’s
    true capabilities:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can now set plot functions to visualize the results of the simulation:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we are ready to initiate the training process for the network using the
    `train()` function:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'While training the neural network, the **Neural Network Training** window will
    be displayed. This window comprises four sections, each offering valuable information
    throughout the training process:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Neural network**: This section provides a summary of the neural network’s
    architecture and configuration, including the number of layers, nodes in each
    layer, and the chosen training algorithm.'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithms**: In this area, details about the training algorithm being used,
    such as the specific optimization technique and convergence criteria, are presented.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Progress**: The progress section shows real-time updates on the training
    process, such as the current epoch, training error, and validation performance.
    It allows monitoring of the network’s performance as it improves over successive
    epochs.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plots**: This section displays various plots, such as training and validation
    errors over epochs, enabling a visual assessment of the network’s learning progress
    and potential overfitting.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Neural Network Training** window (*Figure 5**.5*) offers a comprehensive
    view of the training procedure, providing insights into the network’s behavior
    and performance at different stages. These insights help fine-tune the network
    and make informed decisions to optimize its performance for the pattern recognition
    task at hand.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Neural Network Training window for pattern recognition](img/B21156_05_05.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Neural Network Training window for pattern recognition
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Once the network training is complete, we can employ the trained model to test
    its performance on the same input data used during the training phase. By doing
    so, we can obtain the results and utilize them for evaluation purposes.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Testing the network on the same data helps us assess how well the model generalizes
    to familiar inputs and provides insights into its effectiveness in handling real-world
    scenarios. The evaluation results obtained from this process aid in understanding
    the network’s accuracy, precision, recall, and other relevant metrics, allowing
    us to make informed decisions about its overall performance and suitability for
    the intended pattern recognition task:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `vec2ind()` function is used to convert vectors to indices. It allows indices
    to be represented either directly as themselves or as vectors with a `1` in the
    row corresponding to the index they represent. In the context of this problem,
    `TargetInd` and `SimDataInd` are vectors containing values `1`, `2`, or `3`, representing
    the classes to which the targets and outputs belong. The last row of these vectors
    contains the percentage of error occurrences.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following value is obtained:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let’s proceed with the evaluation of the network. The following commands
    extract the outputs and targets that pertain to the training, validation, and
    test subsets. This extracted data will be used in the subsequent step to construct
    the confusion matrix, which will aid in further assessing the performance of the
    network on each of these subsets:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: A **confusion matrix** is a valuable tool that allows us to compare the classification
    results of our model to the real data. It provides insights into the nature and
    quantity of classification errors. The matrix consists of cells where the diagonal
    elements represent the number of cases that were correctly classified, while the
    off-diagonal elements show the misclassified cases.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In an ideal situation, a machine learning algorithm should perfectly discriminate
    between two populations (such as healthy and diseased) that are not overlapping
    (mutually exclusive). However, in real-world scenarios, the two populations often
    overlap to some extent, leading to the algorithm making some false positive and
    false negative predictions.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 5**.6* displays the confusion matrix for the training, testing, and
    validation phases, as well as a combined matrix that considers all three sets
    of data together.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B21156_05_06.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.6 – The confusion matrix for training, testing, and validation, and
    the three kinds of data combined
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: This visual representation will help us understand how well the model performed
    across different phases and its overall classification performance considering
    all data subsets. The confusion matrix has been computed for the training, testing,
    and validation phases, as well as for the combination of all three data subsets.
    This comprehensive matrix provides a detailed overview of the model’s classification
    performance across various evaluation stages and gives a complete picture of its
    effectiveness in handling different datasets.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.6*, the blue cell located in the bottom-right corner represents
    the total percentage of correctly classified cases, which are depicted by green
    cells positioned diagonally. The red cells in the matrix signify the total percentage
    of misclassified cases located in other cells. The confusion matrix organizes
    data with rows representing the actual values and columns representing the predicted
    values. For instance, in the top-left plot of the figure, the first row indicates
    that 85 cases are correctly classified as `1` (normal), 5 cases are incorrectly
    classified as `2` (hyperfunction), and 8 cases are incorrectly classified as `3`
    (subnormal). By observing the blue cell in the bottom right of each plot in the
    confusion matrix, we can deduce that the classification accuracy is consistently
    high, exceeding 92%. These results demonstrate excellent recognition capabilities.
    If higher accuracy is required, retraining the data could be considered. It should
    be noted that the starting dataset is not correctly balanced across all classes.
    This can result in a high-performance value for one class and a low-performance
    value for another. It is therefore advisable to always work on datasets that are
    correctly balanced across all classes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method of evaluating network performance is through the **receiver
    operating characteristic** (**ROC**). The ROC curve is a valuable tool for assessing
    the model’s performance in terms of sensitivity and specificity across various
    classification thresholds. The subsequent command generates plots for the ROC
    during each evaluation phase and for the entire process:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The ROC is a metric used to evaluate the performance of classifiers. It assesses
    the quality of a classifier for each class by applying threshold values across
    the interval `[0, 1]` to its outputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, ROC plots are displayed for the training, testing,
    and validation phases, as well as for the combined data from all three subsets.
    These ROC curves allow us to analyze the classifier’s sensitivity and specificity
    across different classification thresholds for each class, providing valuable
    insights into its discrimination capabilities and overall performance.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – ROC curves for training, testing, and validation, and the three
    kinds of data combined](img/B21156_05_07.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – ROC curves for training, testing, and validation, and the three
    kinds of data combined
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: In the graph, the colored lines on each axis represent the ROC curves. The ROC
    curve illustrates the relationship between the **true positive rate** (**TPR,
    or sensitivity**) and the **false positive rate** (**FPR**) as the classification
    threshold is varied. TPR measures the proportion of actual positive cases that
    are correctly identified as positive by the classifier.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: TPR = TP / (TP + FN)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '*TP* = True positives'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FN* = False negatives'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, it calculates the ability of the model to detect all positive
    instances in the dataset. FPR measures the proportion of actual negative cases
    that are incorrectly classified as positive by the classifier. It quantifies the
    rate of false alarms or false positives made by the model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: FPR = FP / (FP + TN)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '*FP* is the number of false positives, which are the number of negative examples
    that are incorrectly classified as positive'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TN* is the number of true negatives, which are the number of negative examples
    that are correctly classified as negative'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ideal test would exhibit points located in the upper-left corner, indicating
    100% sensitivity and 100% specificity. The closer the lines approach the upper-left
    corner, the better the network’s performance, indicating its ability to achieve
    higher sensitivity while keeping the FPR low, leading to improved classification
    accuracy.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: After tackling a pattern recognition problem using the tools available in MATLAB,
    in the next practical example, we will see how to tackle a clustering problem
    with the help of the Neural Network tool.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Building a clustering application with an ANN
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a popular unsupervised machine learning technique used for grouping
    similar data points together in a dataset. The goal of clustering is to partition
    data into clusters in such a way that data points within the same cluster are
    more like each other than those in other clusters. We examined this topic in depth
    in [*Chapter 4*](B21156_04.xhtml#_idTextAnchor084)*, Clustering Analysis and*
    *Dimensionality Reduction*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will see how to address a clustering problem using an ANN
    in the MATLAB environment. So far, to train a neural network in the MATLAB environment,
    we have used the commands available from the command line, or much more simply
    to be implemented in a script with the `.m` extension to reproduce the algorithm
    whenever we like. But MATLAB has out-of-the-box apps that let us use a wizard
    to train an ANN. We will do this to address a clustering problem as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an overview of the apps made available by the Neural Network tool, we
    can type the following on the command line:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following window will open:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Neural Network start window](img/B21156_05_08.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Neural Network start window
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Four apps are available: `nftool` command to start the `nprtool` command to
    open the **Pattern** **Recognition** app.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use the **Clustering** app, which is started using
    the following command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following window will open:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – MATLAB app for clustering using ANN](img/B21156_05_09.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – MATLAB app for clustering using ANN
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: In this app, we can use a wizard for the training of the ANN. Let’s see how.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we have to import the data. For this example, we will use a dataset
    already available in the MATLAB environment to explain the clustering application
    using an ANN. To import that dataset in the MATLAB workspace, we use the following
    command:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we have two matrices in the MATLAB workspace (`Input` and `Target`). We
    will use only the first (wx1,000 double) for a clustering problem, which is the
    `Input` matrix with two variables and 1,000 records. By examining the structure
    of the `Target` (4x1,000 double) matrix, we realize that four classes are available,
    so the data has `4` groups of data, which will be useful in justifying the results.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we have to import the data in the app. To do this, we can click on the
    **Import** button in the window shown in *Figure 5**.9*. The window shown in *Figure
    5**.10* will open:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Import data in Clustering app](img/B21156_05_10.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Import data in Clustering app
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The right matrix was already detected for **Predictors**; if you want to change
    the data, you can select the right one using the browse button next to the **Predictors**
    field. In this case, we have to set the observations in columns, so we will have
    1,000 observations with 2 features. Let’s just click on the **OK** button to import
    the data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: We have the ANN architecture already set in the app, so we can identify the
    number of inputs and the topology of the network. The output will be a network
    with 10x10 nodes (100 nodes). The `nctool` guides you in solving clustering problems
    by employing a **self-organizing map** (**SOM**). This map creates a condensed
    depiction of the input space, capturing both the density patterns of input vectors
    in that space and a compressed, two-dimensional representation of the input space’s
    topology.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SOMs**, also known as **Kohonen maps**, are a type of ANN that belongs to
    the family of unsupervised learning algorithms. They were introduced by the Finnish
    professor Teuvo Kohonen in the 1980s. SOMs are used for tasks such as dimensionality
    reduction, data visualization, clustering, and feature extraction.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The primary idea behind SOMs is to map high-dimensional input data onto a lower-dimensional
    grid or lattice in such a way that similar input data points are mapped to nearby
    grid cells. This results in a topological representation of the input data, where
    similar data points are located close to each other on the map, allowing for easier
    visualization and interpretation.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'SOMs consist of two essential layers: the input layer and the output layer,
    often referred to as the feature map. The input layer serves as the initial stage
    in a SOM. Each data point from the dataset competes for representation to identify
    its own characteristics. The process begins with weight vector initialization,
    kickstarting the mapping process of the SOM.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The mapped vectors are subsequently scrutinized to identify the weight vector
    that best represents the chosen sample using a randomly selected sample vector.
    Nearby weights to each weighted vector are considered, and the chosen weight evolves
    into a vector for the random sample, fostering the map’s growth and the emergence
    of new patterns. In a two-dimensional feature space, these patterns often take
    on hexagonal or square shapes. This entire process is repeated over 1,000 times.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In essence, learning occurs as follows:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each node is analyzed to determine whether its weights are similar to the input
    vector. The node that best matches the input vector is termed the **best matching**
    **unit** (**BMU**).
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The neighborhood value of the BMU is then established, and over time, the number
    of neighbors tends to decrease.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The BMU’s weight vector further adapts to resemble the sample vector, leading
    to similar changes in the surrounding areas. The weight of a node changes more
    as it gets closer to the BMU and less as it moves away from its neighbors.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 for *N* iterations.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This iterative process allows the SOM to refine its representation of the input
    data, ultimately leading to a more organized and compressed representation that
    captures the underlying structure of the data.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we can train the network just by clicking on the **train** button of the
    app. After a few seconds, the ANN will be trained and ready for use.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can view the results using the plots available at the top of the
    **App** window. We will first use the neighbor distances, and the following plot
    will be drawn:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.11 – SOM neighbor weight distances](img/B21156_05_11.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – SOM neighbor weight distances
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: This is a graphical representation that illustrates the distances between weight
    vectors of neighboring nodes in a SOM. This plot helps visualize how the weights
    of neighboring nodes change during the learning process, providing insights into
    the topology and organization of the SOM. We can identify more colored cells that
    separate the nodes in four areas using the class available in the `Target` matrix.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: The blue hexagons represent the neurons. The red lines connect neighboring neurons.
    The colors in the regions containing the red lines indicate the distances between
    neurons. The darker colors represent larger distances, and the lighter colors
    represent smaller distances.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: One visualization tool for the SOM is the weight distance matrix (also called
    the U-matrix). To view the U-matrix, click **SOM Neighbor Distances** in the training
    window.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows us that there are four potential clusters in the data correctly
    detected by the algorithm. To have a confirmation of this indication, we can plot
    the weight position plot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – SOM weight positions](img/B21156_05_12.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – SOM weight positions
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.12*, the four clusters can be easily identified. In this diagram,
    the input vectors are depicted as green dots, and it illustrates the SOM’s classification
    of the input space by displaying blue-gray dots representing each neuron’s weight
    vector. Additionally, neighboring neurons relate to red lines, offering a visual
    representation of the SOM’s organizational structure.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: After having also analyzed a case of clustering with ANNs, to complete the topic,
    we will see how to optimize the results obtained.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Exploring advanced optimization techniques
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Advanced optimization techniques** are powerful methods used to enhance the
    efficiency and effectiveness of optimization algorithms. These techniques aim
    to overcome the limitations of traditional optimization approaches, particularly
    in complex, high-dimensional, or non-convex optimization problems.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, advanced optimization techniques are essential for training
    complex models effectively, improving convergence, avoiding overfitting, and handling
    high-dimensional data. In the following subsection, some advanced optimization
    techniques commonly used in machine learning are listed.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SGD
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SGD** is a popular and fundamental optimization algorithm used in machine
    learning for training models, especially in large-scale and complex settings.
    It’s a variant of the traditional gradient descent method designed to address
    efficiency and convergence issues when dealing with large datasets. The stochastic
    aspect of SGD comes from the fact that it uses a random mini-batch for each iteration,
    making the optimization process more stochastic (randomized) compared to the deterministic
    nature of regular gradient descent, which uses the entire dataset for each update.
    This stochasticity introduces noise into the gradient estimates, which can help
    the algorithm escape local minima, converge faster, and generalize better.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: The primary objective of any optimization algorithm is to minimize the discrepancy
    between the predicted values of the model and the actual values observed in the
    data. The smaller the error between the observed and predicted values, the more
    effective the algorithm is at simulating the real-world scenario. Minimizing this
    discrepancy is equivalent to optimizing an objective function on which the model
    construction is based.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Descent methods are iterative techniques that, commencing from an initial point
    x0 ∈ Rn, produce a sequence of points {xn} ∈ N based on the subsequent equation:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: x n+1 = x n + γ n * g n
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'In a descent method, the vector *g**n* represents the search direction, and
    the scalar *γ**n* serves as a positive parameter known as the step length, determining
    the distance of movement in the *g**n* direction. These choices of *g**n* and
    *γ**n* are made to ensure the reduction of the objective function *f* in each
    iteration, following the principle:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: f x n+1 < f x n ∀ n ≥ 0
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: We select the vector *g**n* as a descent direction, ensuring that the line *x
    = x**n* *+ γ**n* ** g**n* creates an obtuse angle with the gradient vector ∇ f(xn).
    This guarantees the decrease of the objective function *f*, as long as the value
    of *γ**n* is sufficiently small. This approach allows for various descent methods,
    contingent on the specific choice of *g**n*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'A gradient is a function that produces a vector, representing the slope of
    the tangent to the graph of a function. It points in the direction of the greatest
    increase in the function. Let’s examine the convex function illustrated in the
    following figure:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – How the gradient descent algorithm searches the global optimum](img/B21156_05_13.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – How the gradient descent algorithm searches the global optimum
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: The primary objective of the gradient descent algorithm is to locate the function’s
    nadir, or lowest point. In more precise terms, the gradient functions as a derivative,
    indicating the incline or steepness of the objective function.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: To provide a clearer analogy, let’s imagine we find ourselves lost in the mountains
    at night with limited visibility. Our perception is restricted to sensing the
    slope of the terrain beneath our feet. The aim is to reach the lowest point of
    the mountain. Achieving this goal involves taking successive steps in the direction
    of the steepest slope. We proceed iteratively, advancing step by step, until we
    finally arrive at the valley of the mountain.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll examine a two-variable function, denoted as *f(x, y)*, with its gradient
    represented as a vector encompassing the partial derivatives of *f*. The first
    derivative pertains to *x*, while the second derivative pertains to *y*. Upon
    computation of these partial derivatives, the results are as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: δf _ δx  ,  δf _ δy
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial expression corresponds to the partial derivative concerning *x*,
    whereas the subsequent expression pertains to the partial derivative regarding
    *y*. The gradient is represented by the ensuing vector:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: ∇ f(x, y) = ⎡ ⎢ ⎣ δf _ δx   δf _ δy  ⎤ ⎥ ⎦
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The given equation represents a function in a two-dimensional space, effectively
    forming a two-dimensional vector. Each component of this vector signifies the
    steepest ascent direction for the respective function variable. Consequently,
    the gradient points toward the direction where the function exhibits the most
    significant increase.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, if we consider a function with five variables, the resulting gradient
    vector will encompass five partial derivatives. In general, a function with *n*
    variables gives rise to an *n*-dimensional gradient vector, exemplified as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ∇ f(x, y, … .z) = ⎡ ⎢ ⎣  δf _ δx   δf _ δy  … …  δf _ δz  ⎤ ⎥ ⎦
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: When utilizing gradient descent, our objective isn’t to maximize *f* as rapidly
    as possible; instead, we aim to minimize it—specifically, to locate the smallest
    point that minimizes the function.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a function *y = f(x)*. The foundation of gradient descent relies on
    the observation that when the function *f* is well defined and differentiable
    within a vicinity of *x*, it experiences a faster decrease as we proceed in the
    direction opposite to the negative gradient. Commencing from an initial value
    of *x*, we can express this as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '*x**n* *+ 1 =* *x**n* - *γ* ***∇ *f(**x**n**)*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: where gamma (*γ*) is learning rate and delta (∇) is the gradient.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, we have the following:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '*γ* represents the learning rate'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∇ signifies the gradient
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using sufficiently small values of *γ*, the algorithm converges to the
    minimum value of the function *f* within a finite number of iterations. In essence,
    when the gradient is negative, it indicates a decreasing objective function at
    that point, implying that the parameter should shift toward larger values to approach
    a minimum point. Conversely, when the gradient is positive, the parameters should
    move toward smaller values to attain lower values of the objective function.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Adam optimization
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adaptive moment estimation** (**Adam**) is an advanced optimization algorithm
    commonly used for training deep learning models, particularly in neural networks.
    It is an extension of the SGD optimization method that adapts the learning rate
    for each parameter individually.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'This gradient descent algorithm aims to locate the objective function’s minimum
    using an iterative approach. In each step, an approximation of the gradient is
    computed to guide the descent in the direction that effectively reduces the objective
    function. Within this process, the selection of the learning rate parameter holds
    significant importance. This parameter governs the speed at which we approach
    the optimal objective function values:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: If the learning rate is excessively small, a substantial number of iterations
    are required to converge toward the optimal values
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversely, an overly high learning rate might cause us to overlook the optimal
    solution
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam adjusts the learning rates for each parameter based on the first-order
    moment (mean) and second-order moment (uncentered variance) of the gradients.
    This adaptability helps the algorithm converge more efficiently, especially in
    high-dimensional spaces with varying gradient magnitudes.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: This method addresses the issue of initial bias in the moments by correcting
    them. In the early stages of training, the moments’ estimates may be biased toward
    0, and Adam compensates for this bias, particularly when the learning rate is
    low.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Adam combines the concepts of momentum (accumulating a fraction of the past
    gradients to enhance convergence) and **RMSProp** (scaling the learning rates
    based on the magnitudes of recent gradients) to perform well on a wide range of
    optimization problems.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'The Adam optimization algorithm involves the following calculations during
    each iteration:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Compute the gradient of the loss with respect to the model’s parameters
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the first and second moments (mean and uncentered variance) of the gradients
    using a moving average
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct the bias in the moments (especially in the early iterations)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the parameters using the corrected moments and the learning rate
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, Adam is effective in many deep learning scenarios, often providing
    faster convergence than standard SGD with manually tuned learning rates. It’s
    a popular choice for training neural networks and has become a standard optimization
    algorithm in many deep learning frameworks due to its adaptive properties and
    strong performance.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Introducing second-order methods
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Second-order optimization methods**, also known as **Newton-like methods**,
    are advanced techniques used for solving optimization problems. Unlike first-order
    methods (such as gradient descent) that primarily rely on gradients, second-order
    methods take advantage of both gradients and second-order derivatives (**Hessian
    matrix**) of the objective function. This additional information can lead to faster
    convergence and more accurate solutions, especially in complex and non-convex
    optimization landscapes.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Second-order methods consider not only the first-order derivative (gradient)
    of the function but also its second-order derivative (Hessian matrix). The Hessian
    matrix captures curvature information and provides insights into the shape of
    the objective function’s surface. These methods often use quadratic approximations
    of the objective function around the current point. These approximations consider
    the first and second derivatives and provide a more accurate representation of
    the local behavior of the function.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: The use of curvature information from the Hessian matrix can lead to faster
    convergence rates compared to first-order methods, especially when the objective
    function is well behaved and smooth. Second-order methods can be more robust against
    the choice of learning rates or step sizes, as they inherently adjust the step
    size based on the curvature of the function.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Second-order methods are valuable tools for optimization, particularly when
    computational resources allow for the calculation of second-order derivatives.
    However, their application may depend on the specific characteristics of the problem
    being solved.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: An example of a second-order method is Newton’s method. This method directly
    minimizes a quadratic approximation of the objective function using the Hessian
    matrix. Newton’s method is a classic optimization algorithm used to find the minimum
    or maximum of a function. Named after Sir Isaac Newton, this method utilizes both
    the gradient (first derivative) and the second derivative (Hessian matrix) of
    the function to iteratively approach the optimal solution.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach adopts the structure of Newton’s method, which is commonly used
    to locate the roots of a function, but in this case, it’s applied to the derivative
    of function *f*. The rationale behind this is that identifying the minimum point
    of function f is equivalent to finding the root of its first derivative f′. In
    this scenario, the updated formula can be expressed as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: x n+1 = x n −  f ′ ( x n) _ f ″  (x n)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'In the given equation, we have the following:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: f′ (xn) represents the first derivative of function *f*
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f″ (xn) signifies the second derivative of function *f*
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second-order methods are typically favored over gradient descent due to their
    faster convergence rate, provided that the analytical expressions for both first
    and second derivatives are available. However, these methods converge regardless
    of whether they’re approaching minima or maxima. Variations of this method exist
    that ensure global convergence and reduce computational costs by sidestepping
    the need to solve the system for determining the search direction using direct
    methods.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained insight into simulating typical human brain activities
    using ANNs. We grasped the fundamental concepts behind ANNs, delving into the
    creation of a basic neural network architecture. This exploration encompassed
    elements such as input, hidden, and output layers, connection weights, and activation
    functions. Our understanding extended to crucial decisions regarding hidden layer
    count, node quantity within each layer, and network training algorithms.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Then we focused on data fitting and pattern recognition using neural networks.
    We engaged in script analysis to master the utilization of neural network functions
    via the command line. We then ventured into the Neural Network Toolbox, featuring
    algorithms, pre-trained models, and apps for crafting, training, visualizing,
    and simulating shallow and deep neural networks. The Neural Network Toolbox offers
    an accessible interface—the **Neural Network** getting started GUI—which serves
    as the launchpad for tasks such as neural network fitting, pattern recognition,
    clustering, and time-series analysis.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced some of the commonly used advanced optimization techniques.
    These methods are potent techniques employed to amplify the efficiency and efficacy
    of optimization algorithms. Their purpose is to surmount the constraints posed
    by conventional optimization methodologies, especially in intricate, high-dimensional,
    or non-convex optimization scenarios.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand the basic concepts of deep learning.
    We also will learn about the different types of deep learning and understand **convolutional****neural
    networks** (**CNNs**). Additionally, we will learn how to build a CNN using MATLAB
    and understand recurrent neural networks, long short-term memory networks, and
    transformer models.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
