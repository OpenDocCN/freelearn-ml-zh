- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to ML Engineering on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of us started our **machine learning** (**ML**) journey by training our
    first ML model using a sample dataset on our laptops or home computers. Things
    are somewhat straightforward until we need to work with much larger datasets and
    run our ML experiments in the cloud. It also becomes more challenging once we
    need to deploy our trained models to production-level inference endpoints or web
    servers. There are a lot of things to consider when designing and building ML
    systems and these are just some of the challenges data scientists and ML engineers
    face when working on real-life requirements. That said, we must use the right
    platform, along with the right set of tools, when performing ML experiments and
    deployments in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering why we should even use a cloud platform
    when running our workloads. *Can’t we build this platform ourselves*? Perhaps
    you might be thinking that building and operating your own data center is a relatively
    easy task. In the past, different teams and companies have tried setting up infrastructure
    within their data centers and on-premise hardware. Over time, these companies
    started migrating their workloads to the cloud as they realized how hard and expensive
    it was to manage and operate data centers. A good example of this would be the
    *Netflix* team, which migrated their resources to the **AWS** cloud. Migrating
    to the cloud allowed them to scale better and allowed them to have a significant
    increase in service availability.
  prefs: []
  type: TYPE_NORMAL
- en: The **Amazon Web Services** (**AWS**) platform provides a lot of services and
    capabilities that can be used by professionals and companies around the world
    to manage different types of workloads in the cloud. These past couple of years,
    AWS has announced and released a significant number of services, capabilities,
    and features that can be used for production-level ML experiments and deployments
    as well. This is due to the increase in ML workloads being migrated to the cloud
    globally. As we go through each of the chapters in this book, we will have a better
    understanding of how different services are used to solve the challenges when
    productionizing ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the hands-on journey for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Hands-on journey for this chapter ](img/B18638_01_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Hands-on journey for this chapter
  prefs: []
  type: TYPE_NORMAL
- en: In this introductory chapter, we will focus on getting our feet wet by trying
    out different options when building an ML model on AWS. As shown in the preceding
    diagram, we will use a variety of **AutoML** services and solutions to build ML
    models that can help us predict if a hotel booking will be cancelled or not based
    on the information available. We will start by setting up a **Cloud9** environment,
    which will help us run our code through an **integrated development environment**
    (**IDE**) in our browser. In this environment, we will generate a realistic synthetic
    dataset using a **deep learning** model called the **Conditional Generative Adversarial
    Network**. We will upload this dataset to **Amazon S3** using the **AWS CLI**.
    Inside the Cloud9 environment, we will also install **AutoGluon** and run an **AutoML**
    experiment to train and generate multiple models using the synthetic dataset.
    Finally, we will use **SageMaker Canvas** and **SageMaker Autopilot** to run AutoML
    experiments using the uploaded dataset in S3\. If you are wondering what these
    fancy terms are, keep reading as we demystify each of these in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is expected from ML engineers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How ML engineers can get the most out of AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essential prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML with AutoGluon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with SageMaker and SageMaker Canvas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No-code machine learning with SageMaker Canvas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML with SageMaker Autopilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to getting our feet wet using key ML services, libraries, and tools
    to perform AutoML experiments, this introductory chapter will help us gain a better
    understanding of several ML and ML engineering concepts that will be relevant
    to the succeeding chapters of this book. With this in mind, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we must have an AWS account. If you do not have an AWS account
    yet, simply create an account here: [https://aws.amazon.com/free/](https://aws.amazon.com/free/).
    You may proceed with the next steps once the account is ready.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jupyter notebooks, source code, and other files for each chapter are available
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  prefs: []
  type: TYPE_NORMAL
- en: What is expected from ML engineers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML engineering involves using ML and **software engineering** concepts and techniques
    to design, build, and manage production-level ML systems, along with pipelines.
    In a team working to build ML-powered applications, **ML engineers** are generally
    expected to build and operate the ML infrastructure that’s used to train and deploy
    models. In some cases, data scientists may also need to work on infrastructure-related
    requirements, especially if there is no clear delineation between the roles and
    responsibilities of ML engineers and data scientists in an organization.
  prefs: []
  type: TYPE_NORMAL
- en: There are several things an ML engineer should consider when designing and building
    ML systems and platforms. These would include the *quality* of the deployed ML
    model, along with the *security*, *scalability*, *evolvability*, *stability*,
    and *overall cost* of the ML infrastructure used. In this book, we will discuss
    the different strategies and best practices to achieve the different objectives
    of an ML engineer.
  prefs: []
  type: TYPE_NORMAL
- en: ML engineers should also be capable of designing and building automated ML workflows
    using a variety of solutions. Deployed models degrade over time and **model retraining**
    becomes essential in ensuring the quality of deployed ML models. Having automated
    ML pipelines in place helps enable automated model retraining and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are excited to learn more about how to build custom ML pipelines on
    AWS, then you should check out the last section of this book: *Designing and building
    end-to-end MLOps pipelines*. You should find several chapters dedicated to deploying
    complex ML pipelines on AWS!'
  prefs: []
  type: TYPE_NORMAL
- en: How ML engineers can get the most out of AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many services and capabilities in the AWS platform that an ML engineer
    can choose from. Professionals who are already familiar with using virtual machines
    can easily spin up **EC2** instances and run ML experiments using deep learning
    frameworks inside these virtual private servers. Services such as **AWS Glue**,
    **Amazon EMR**, and **AWS Athena** can be utilized by ML engineers and data engineers
    for different data management and processing needs. Once the ML models need to
    be deployed into dedicated inference endpoints, a variety of options become available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – AWS machine learning stack ](img/B18638_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – AWS machine learning stack
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, data scientists, developers, and ML engineers
    can make use of multiple services and capabilities from the **AWS machine learning
    stack**. The services grouped under **AI services** can easily be used by developers
    with minimal ML experience. To use the services listed here, all we need would
    be some experience working with data, along with the software development skills
    required to use SDKs and APIs. If we want to quickly build ML-powered applications
    with features such as language translation, text-to-speech, and product recommendation,
    then we can easily do that using the services under the AI Services bucket. In
    the middle, we have **ML services** and their capabilities, which help solve the
    more custom ML requirements of data scientists and ML engineers. To use the services
    and capabilities listed here, a solid understanding of the ML process is needed.
    The last layer, **ML frameworks and infrastructure**, offers the highest level
    of flexibility and customizability as this includes the ML infrastructure and
    framework support needed by more advanced use cases.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can ML engineers make the most out of the AWS machine learning stack?
    The ability of ML engineers to design, build, and manage ML systems improves as
    they become more familiar with the services, capabilities, and tools available
    in the AWS platform. They may start with AI services to quickly build AI-powered
    applications on AWS. Over time, these ML engineers will make use of the different
    services, capabilities, and infrastructure from the lower two layers as they become
    more comfortable dealing with intermediate ML engineering requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Essential prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will prepare the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Cloud9 environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The S3 bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synthetic dataset, which will be generated using a deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Cloud9 environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the more convenient options when performing ML experiments inside a virtual
    private server is to use the **AWS Cloud9** service. AWS Cloud9 allows developers,
    data scientists, and ML engineers to manage and run code within a development
    environment using a browser. The code is stored and executed inside an EC2 instance,
    which provides an environment similar to what most developers have.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use an **Identity and Access Management** (**IAM**) user
    with limited permissions instead of the root account when running the examples
    in this book. We will discuss this along with other security best practices in
    detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*, Security, Governance,
    and Compliance Strategies*. If you are just starting to use AWS, you may proceed
    with using the root account in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create a Cloud9 environment where we will generate the
    synthetic dataset and run the **AutoGluon AutoML** experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type `cloud9` in the search bar. Select **Cloud9** from the list of results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Navigating to the Cloud9 console ](img/B18638_01_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Navigating to the Cloud9 console
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the region is currently set to `us-west-2`). Make sure
    that you change this to where you want the resources to be created.
  prefs: []
  type: TYPE_NORMAL
- en: Next, click **Create environment**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the `mle-on-aws`) and click **Next step**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under **Environment type**, choose **Create a new EC2 instance for environment
    (direct access)**. Select **m5.large** for **Instance type** and then **Ubuntu
    Server (18.04 LTS)** for **Platform**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Configuring the Cloud9 environment settings ](img/B18638_01_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Configuring the Cloud9 environment settings
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that there are other options for the instance type. In the
    meantime, we will stick with **m5.large** as it should be enough to run the hands-on
    solutions in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For the **Cost-saving setting** option, choose **After four hours** from the
    list of drop-down options. This means that the server where the Cloud9 environment
    is running will automatically shut down after 4 hours of inactivity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `vpc-abcdefg (default)`. For the `subnet-abcdefg | Default in us-west-2a`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended that you use the default VPC since the networking configuration
    is simple. This will help you avoid issues, especially if you’re just getting
    started with VPCs. If you encounter any VPC-related issues when launching a Cloud9
    instance, you may need to check if the selected subnet has been configured with
    internet access via the route table configuration in the VPC console. You may
    retry launching the instance using another subnet or by using a new VPC altogether.
    If you are planning on creating a new VPC, navigate to [https://go.aws/3sRSigt](https://go.aws/3sRSigt)
    and create a **VPC with a Single Public Subnet**. If none of these options work,
    you may try launching the Cloud9 instance in another region. We’ll discuss **Virtual
    Private Cloud** (**VPC**) networks in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*,
    Security, Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Next Step**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the review page, click **Create environment**. This should redirect you
    to the Cloud9 environment, which should take a minute or so to load. The Cloud9
    **IDE** is shown in the following screenshot. This is where we can write our code
    and run the scripts and commands needed to work on some of the hands-on solutions
    in this book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.5 – AWS Cloud9 interface ](img/B18638_01_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – AWS Cloud9 interface
  prefs: []
  type: TYPE_NORMAL
- en: Using this IDE is fairly straightforward as it looks very similar to code editors
    such as **Visual Studio Code** and **Sublime Text**. As shown in the preceding
    screenshot, we can find the **menu bar** at the top (**A**). The **file tree**
    can be found on the left-hand side (**B**). The **editor** covers a major portion
    of the screen in the middle (**C**). Lastly, we can find the **terminal** at the
    bottom (**D**).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'If this is your first time using AWS Cloud9, here is a 4-minute introduction
    video from AWS to help you get started: [https://www.youtube.com/watch?v=JDHZOGMMkj8](https://www.youtube.com/watch?v=JDHZOGMMkj8).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our Cloud9 environment ready, it is time we configure it with
    a larger storage space.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing Cloud9’s storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a Cloud9 instance is created, the attached volume only starts with 10GB
    of disk space. Given that we will be installing different libraries and frameworks
    while running ML experiments in this instance, we will need more than 10GB of
    disk space. We will resize the volume programmatically using the `boto3` library.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If this is your first time using the `boto3` library, it is the **AWS SDK for
    Python**, which gives us a way to programmatically manage the different AWS resources
    in our AWS accounts. It is a service-level SDK that helps us list, create, update,
    and delete AWS resources such as EC2 instances, S3 buckets, and EBS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to download and run some scripts to increase the volume
    disk space from 10GB to 120GB:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the terminal of our Cloud9 environment (right after the `$` sign at the
    bottom of the screen), run the following bash command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will download the script file located at [https://bit.ly/3ea96tW](https://bit.ly/3ea96tW).
    Here, we are simply using a URL shortener, which would map the shortened link
    to [https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are using the big `O` flag instead of a small `o` or a zero (`0`)
    when using the `wget` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s inside the file we just downloaded? Let’s quickly inspect the file before
    we run the script. Double-click the `resize_and_reboot.py` file in the file tree
    (located on the left-hand side of the screen) to open the Python script file in
    the editor pane. As shown in the following screenshot, the `resize_and_reboot.py`
    script has three major sections. The first block of code focuses on importing
    the prerequisites needed to run the script. The second block of code focuses on
    resizing the volume of a selected EC2 instance using the `boto3` library. It makes
    use of the `describe_volumes()` method to get the volume ID of the current instance,
    and then makes use of the `modify_volume()` method to update the volume size to
    120GB. The last section involves a single line of code that simply reboots the
    EC2 instance. This line of code uses the `os.system()` method to run the `sudo
    reboot` shell command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.6 – The resize_and_reboot.py script file ](img/B18638_01_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – The resize_and_reboot.py script file
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `resize_and_reboot.py` script file in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py).
    Note that for this script to work, the `EC2_INSTANCE_ID` environment variable
    must be set to select the correct target instance. We’ll set this environment
    variable a few steps from now before we run the `resize_and_reboot.py` script.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will upgrade the version of `boto3` using `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If this is your first time using `pip`, it is the package installer for Python.
    It makes it convenient to install different packages and libraries using the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: You may use `python3 -m pip show boto3` to check the version you are using.
    This book assumes that you are using version `1.20.26` or later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining statements focus on getting the Cloud9 environment’s `instance_id`
    from the instance metadata service and storing this value in the `EC2_INSTANCE_ID`
    variable. Let’s run the following in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us an EC2 instance ID with a format similar to `i-01234567890abcdef`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the `EC2_INSTANCE_ID` environment variable set with the appropriate
    value, we can run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will run the Python script we downloaded earlier using the `wget` command.
    After performing the volume resize operation using `boto3`, the script will reboot
    the instance. You should see a **Reconnecting…** notification at the top of the
    page while the Cloud9 environment’s EC2 instance is being restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to run the `lsblk` command after the instance has been restarted.
    This should help you verify that the volume of the Cloud9 environment instance
    has been resized to 120GB.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully resized the volume to 120GB, we should be able
    to work on the next set of solutions without having to worry about disk space
    issues inside our Cloud9 environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Python prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to install and update several Python packages inside the
    Cloud9 environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the terminal of our Cloud9 environment (right after the `$` sign at the
    bottom of the screen), run the following commands to update `pip`, `setuptools`,
    and `wheel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upgrading these versions will help us make sure that the other installation
    steps work smoothly. This book assumes that you are using the following versions
    or later: `pip` – `21.3.1`, `setuptools` – `59.6.0`, and `wheel` – `0.37.1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To check the versions, you may use the `python3 -m pip show <package>` command
    in the terminal. Simply replace `<package>` with the name of the package. An example
    of this would be `python3 -m pip show wheel`. If you want to install a specific
    version of a package, you may use `python3 -m pip install -U <package>==<version>`.
    For example, if you want to install `wheel` version `0.37.1`, you can run `python3
    -m pip install -U wheel==0.37.1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, install `ipython` by running the following command. **IPython** provides
    a lot of handy utilities that help professionals use Python interactively. We
    will see how easy it is to use IPython later in the *Performing your first AutoGluon
    AutoML experiment* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This book assumes that you are using `ipython` – `7.16.2` or later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s install `ctgan`. CTGAN allows us to utilize **Generative Adversarial
    Network** (**GAN**) deep learning models to generate synthetic datasets. We will
    discuss this shortly in the *Generating a synthetic dataset using a deep learning
    model* section, after we have installed the Python prerequisites:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This book assumes that you are using `ctgan` – `0.5.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'This step may take around 5 to 10 minutes to complete. While waiting, let’s
    talk about what CTGAN is. **CTGAN** is an open source library that uses deep learning
    to learn about the properties of an existing dataset and generates a new dataset
    with columns, values, and properties similar to the original dataset. For more
    information, feel free to check its GitHub page here: [https://github.com/sdv-dev/CTGAN](https://github.com/sdv-dev/CTGAN).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, install `pandas_profiling` by running the following command. This
    allows us to easily generate a profile report for our dataset, which will help
    us with our **exploratory data analysis** (**EDA**) work. We will see this in
    action in the *Exploratory data analysis* section, after we have generated the
    synthetic dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This book assumes that you are using `pandas_profiling` – `3.1.0` or later.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished installing the Python prerequisites, we can start
    generating a realistic synthetic dataset using a deep learning model!
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build multiple ML models that will *predict whether
    a hotel booking will be cancelled or not based on the information available*.
    Hotel cancellations cause a lot of issues for hotel owners and managers, so trying
    to predict which reservations will be cancelled is a good use of our ML skills.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start with our ML experiments, we will need a dataset that can be
    used when training our ML models. We will generate a realistic synthetic dataset
    similar to the *Hotel booking demands* dataset from *Nuno Antonio*, *Ana de Almeida*,
    and *Luis Nunes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The synthetic dataset will have a total of 21 columns. Here are some of the
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`is_cancelled`: Indicates whether the hotel booking was cancelled or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lead_time`: [*arrival date*] – [*booking date*]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adr`: Average daily rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adults`: Number of adults'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`days_in_waiting_list`: Number of days a booking stayed on the waiting list
    before getting confirmed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assigned_room_type`: The type of room that was assigned'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_of_special_requests`: The total number of special requests made by the
    customer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will not discuss each of the fields in detail, but this should help us understand
    what data is available for us to use. For more information, you can find the original
    version of this dataset at [https://www.kaggle.com/jessemostipak/hotel-booking-demand](https://www.kaggle.com/jessemostipak/hotel-booking-demand)
    and [https://www.sciencedirect.com/science/article/pii/S2352340918315191](https://www.sciencedirect.com/science/article/pii/S2352340918315191).
  prefs: []
  type: TYPE_NORMAL
- en: Generating a synthetic dataset using a deep learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the cool applications of ML would be having a **deep learning** model
    “absorb” the properties of an existing dataset and generate a new dataset with
    a similar set of fields and properties. We will use a pre-trained **Generative
    Adversarial Network** (**GAN**) model to generate the synthetic dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative modeling** involves learning patterns from the values of an input
    dataset, which are then used to generate a new dataset with a similar set of values.
    GANs are popular when it comes to generative modeling. For example, research papers
    have focused on how GANs can be used to generate “deepfakes,” where realistic
    images of humans are generated from a source dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating and using a synthetic dataset has a lot of benefits, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to generate a much larger dataset than the original dataset that
    was used to train the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to anonymize any sensitive information in the original dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being able to have a cleaner version of the dataset after data generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That said, let’s start generating the synthetic dataset by running the following
    commands in the terminal of our Cloud9 environment (right after the `$` sign at
    the bottom of the screen):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from where we left off in the *Installing the Python prerequisites*
    section, run the following command to create an empty directory named `tmp` in
    the current working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that this is different from the `/tmp` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s download the `utils.py` file using the `wget` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `utils.py` file contains the `block()` function, which will help us read
    and troubleshoot the logs generated by our scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to download the pre-built GAN model into the Cloud9
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have a serialized pickle file that contains the properties of the deep
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of ways to save and load ML models. One of the options would
    be to use the **Pickle** module to serialize a Python object and store it in a
    file. This file can later be loaded and deserialized back to a Python object with
    a similar set of properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an empty `data_generator.py` script file using the `touch` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that the `data_generator.py`, `hotel_bookings.gan.pkl`,
    and `utils.py` files are in the same directory so that the synthetic data generator
    script works.
  prefs: []
  type: TYPE_NORMAL
- en: Double-click the `data_generator.py` file in the file tree (located on the left-hand
    side of the Cloud9 environment) to open the empty Python script file in the editor
    pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following lines of code to import the prerequisites needed to run the
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s add the following lines of code to load the pre-trained GAN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command in the terminal (right after the `$` sign at the
    bottom of the screen) to test if our initial blocks of code in the script are
    working as intended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should give us a set of logs similar to what is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – GAN model successfully loaded by the script ](img/B18638_01_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – GAN model successfully loaded by the script
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the pre-trained GAN model was loaded successfully using
    the `CTGANSynthesizer.load()` method. Here, we can also see what `block` (from
    the `utils.py` file we downloaded earlier) does to improve the readability of
    our logs. It simply helps mark the start and end of the execution of a block of
    code so that we can easily debug our scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to the editor pane (where we are editing `data_generator.py`)
    and add the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run the script later, these lines of code will generate `10000` records
    and store them inside the `synthetic_data` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s add the following block of code, which will save the generated
    data to a CSV file inside the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s add the following lines of code to complete the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This block of code will analyze the synthetic dataset and generate a profile
    report to help us analyze the properties of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a copy of the `data_generator.py` file here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything ready, let’s run the following command in the terminal (right
    after the `$` sign at the bottom of the screen):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It should take about a minute or so for the script to finish. Running the script
    should give us a set of logs similar to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Logs generated by data_generator.py ](img/B18638_01_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Logs generated by data_generator.py
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, running the `data_generator.py` script generates multiple blocks
    of logs, which should make it easy for us to read and debug what’s happening while
    the script is running. In addition to loading the CTGAN model, the script will
    generate the synthetic dataset using the deep learning model (`tmp` directory
    (`tmp/bookings.all.csv`) (`pandas_profiling` (**C**).
  prefs: []
  type: TYPE_NORMAL
- en: Wasn’t that easy? Before proceeding to the next section, feel free to use the
    file tree (located on the left-hand side of the Cloud9 environment) to check the
    generated files stored in the `tmp` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we should have a synthetic dataset with `10000` rows. You might
    be wondering what our data looks like. Does our dataset contain invalid values?
    Do we have to worry about missing records? We must have a good understanding of
    our dataset since we may need to clean and process the data first before we do
    any model training work. EDA is a key step when analyzing datasets before they
    can be used to train ML models. There are different ways to analyze datasets and
    generate reports — using `pandas_profiling` is one of the faster ways to perform
    EDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, let’s check the report that was generated by the `pandas_profiling`
    Python library. Right-click on `tmp/profile-report.xhtml` in the file tree (located
    on the left-hand side of the Cloud9 environment) and then select **Preview** from
    the list of options. We should find a report similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Generated report ](img/B18638_01_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Generated report
  prefs: []
  type: TYPE_NORMAL
- en: 'The report has multiple sections: **Overview**, **Variables**, **Interactions**,
    **Correlations** **Missing Values**, and **Sample**. In the **Overview** section,
    we can find a quick summary of the dataset statistics and the variable types.
    This includes the number of variables, number of records (observations), number
    of missing cells, number of duplicate rows, and other relevant statistics. In
    the **Variables** section, we can find the statistics and the distribution of
    values for each variable (column) in the dataset. In the **Interactions** and
    **Correlations** sections, we can see different patterns and observations regarding
    the potential relationship of the variables in the dataset. In the **Missing values**
    section, we can see if there are columns with missing values that we need to take
    care of. Finally, in the **Sample** section, we can see the first 10 and last
    10 rows of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to read through the report before proceeding to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Train-test split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have finished performing EDA, what do we do next? Assuming that
    our data is clean and ready for model training, do we just use all of the 10,000
    records that were generated to train and build our ML model? Before we train our
    binary classifier model, we must split our dataset into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Train-test split ](img/B18638_01_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Train-test split
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the **training set** is used to build the model and update its
    parameters during the training phase. The **test set** is then used to evaluate
    the final version of the model on data it has not seen before. What’s not shown
    here is the **validation set**, which is used to evaluate a model to fine-tune
    the **hyperparameters** during the model training phase. In practice, the ratio
    when dividing the dataset into training, validation, and test sets is generally
    around **60:20:20**, where the training set gets the majority of the records.
    In this chapter, we will no longer need to divide the training set further into
    smaller training and validation sets since the AutoML tools and services will
    automatically do this for us.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding with the hands-on solutions in this section, we must have
    an idea of what hyperparameters and parameters are. `y = m * x`, where `m` is
    a parameter, `x` is a single predictor variable, and `y` is the target variable.
    For example, if we are testing the relationship between cancellations (`y`) and
    income (`x`), then `m` is the parameter that defines this relationship. If `m`
    is positive, cancellations go up as income goes up. If it is negative, cancellations
    lessen as income increases. On the other hand, **hyperparameters** are configurable
    values that are tweaked before the model is trained. These variables affect how
    our chosen ML models “model” the relationship. Each ML model has its own set of
    hyperparameters, depending on the algorithm used. These concepts will make more
    sense once we have looked at a few more examples in [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041),
    *Deep Learning AMIs*, and [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060), *Deep
    Learning Containers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a script that will help us perform the train-test split:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the terminal of our Cloud9 environment (right after the `$` sign at the
    bottom of the screen), run the following command to create an empty file called
    `train_test_split.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using the file tree (located on the left-hand side of the Cloud9 environment),
    double-click the `train_test_split.py` file to open the file in the editor pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the editor pane, add the following lines of code to import the prerequisites
    to run the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following block of code, which will read the contents of a CSV file
    and store it inside a `DataFrame`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s use the `train_test_split()` function from scikit-learn to divide
    the dataset we have generated into a training set and a test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, add the following lines of code to save the training and test sets
    into their respective CSV files inside the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a copy of the `train_test_split.py` file here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have completed our script file, let’s run the following command
    in the terminal (right after the `$` sign at the bottom of the screen):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should generate a set of logs similar to what is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Train-test split logs   ](img/B18638_01_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Train-test split logs
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our training dataset contains 7,000 records, while the
    test set contains 3,000 records.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can upload our dataset to **Amazon S3**.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the dataset to Amazon S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon S3 is the object storage service for AWS and is where we can store different
    types of files, such as dataset CSV files and output artifacts. When using the
    different services of AWS, it is important to note that these services sometimes
    require the input data and files to be stored in an S3 bucket first or in a resource
    created using another service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Uploading the dataset to S3 should be easy. Continuing where we left off in
    the *Train-test split* section, we will run the following commands in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands in the terminal. Here, we are going to create a
    new S3 bucket that will contain the data we will be using in this chapter. Make
    sure that you replace the value of `<INSERT BUCKET NAME HERE>` with a bucket name
    that is globally unique across all AWS users:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For more information on S3 bucket naming rules, feel free to check out [https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the S3 bucket has been created, let’s upload the training and test
    datasets using the **AWS CLI**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that everything is ready, we can proceed with the exciting part! It’s about
    time we perform multiple **AutoML** experiments using a variety of solutions and
    services.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML with AutoGluon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we discussed what **hyperparameters** are. When training and tuning
    ML models, it is important for us to know that the performance of an ML model
    depends on the algorithm, the training data, and the hyperparameter configuration
    that’s used when training the model. Other input configuration parameters may
    also affect the performance of the model, but we’ll focus on these three for now.
    Instead of training a single model, teams build multiple models using a variety
    of hyperparameter configurations. Changes and tweaks in the hyperparameter configuration
    affect the performance of a model – some lead to better performance, while others
    lead to worse performance. It takes time to try out all possible combinations
    of hyperparameter configurations, especially if the model tuning process is not
    automated.
  prefs: []
  type: TYPE_NORMAL
- en: These past couple of years, several libraries, frameworks, and services have
    allowed teams to make the most out of **automated machine learning** (**AutoML**)
    to automate different parts of the ML process. Initially, AutoML tools focused
    on automating the **hyperparameter optimization** (**HPO**) processes to obtain
    the optimal combination of hyperparameter values. Instead of spending hours (or
    even days) manually trying different combinations of hyperparameters when running
    training jobs, we’ll just need to configure, run, and wait for this automated
    program to help us find the optimal set of hyperparameter values. For years, several
    tools and libraries that focus on automated hyperparameter optimization were available
    for ML practitioners for use. After a while, other aspects and processes of the
    ML workflow were automated and included in the AutoML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several tools and services available for AutoML and one of the most
    popular options is **AutoGluon**. With **AutoGluon**, we can train multiple models
    using different algorithms and evaluate them with just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – AutoGluon leaderboard – models trained using a variety of algorithms
    ](img/B18638_01_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – AutoGluon leaderboard – models trained using a variety of algorithms
  prefs: []
  type: TYPE_NORMAL
- en: Similar to what is shown in the preceding screenshot, we can also compare the
    generated models using a leaderboard. In this chapter, we’ll use AutoGluon with
    a tabular dataset. However, it is important to note that AutoGluon also supports
    performing AutoML tasks for text and image data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up and installing AutoGluon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before using AutoGluon, we need to install it. It should take a minute or so
    to complete the installation process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands in the terminal to install and update the prerequisites
    before we install AutoGluon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This book assumes that you are using the following versions or later: `mxnet`
    – `1.9.0`, `numpy` – `1.19.5`, and `cython` – `0.29.26`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the following command to install `autogluon`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This book assumes that you are using `autogluon` version `0.3.1` or later.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take around 5 to 10 minutes to complete. Feel free to grab a cup
    of coffee or tea!
  prefs: []
  type: TYPE_NORMAL
- en: With AutoGluon installed in our Cloud9 environment, let’s proceed with our first
    AutoGluon AutoML experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Performing your first AutoGluon AutoML experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have used `fit()` and `predict()`. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, run the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will open the **IPython** **Read-Eval-Print-Loop** (**REPL**)/interactive
    shell. We will use this similar to how we use the **Python shell**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the console, type in (or copy) the following block of code. Make sure
    that you press *Enter* after typing the closing parenthesis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s load the synthetic data stored in the `bookings.train.csv` and `bookings.test.csv`
    files into the `train_data` and `test_data` variables, respectively, by running
    the following statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since the parent class of AutoGluon, `TabularDataset`, is a pandas DataFrame,
    we can use different methods on `train_data` and `test_data` such as `head()`,
    `describe()`, `memory_usage()`, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specify `is_cancelled` as the target variable of the AutoML task and
    the `tmp` directory as the location where the generated models will be stored.
    This block of code will use the training data we have provided to train multiple
    models using different algorithms. AutoGluon will automatically detect that we
    are dealing with a binary classification problem and generate multiple binary
    classifier models using a variety of ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Inside the `tmp/models` directory, we should find `CatBoost`, `ExtraTreesEntr`,
    and `ExtraTreesGini`, along with other directories corresponding to the algorithms
    used in the AutoML task. Each of these directories contains a `model.pkl` file
    that contains the serialized model. Why do we have multiple models? Behind the
    scenes, AutoGluon runs a significant number of training experiments using a variety
    of algorithms, along with different combinations of hyperparameter values, to
    produce the “best” model. The “best” model is selected using a certain evaluation
    metric that helps identify which model performs better than the rest. For example,
    if the evaluation metric that’s used is *accuracy*, then a model with an accuracy
    score of 90% (which gets 9 correct answers every 10 tries) is “better” than a
    model with an accuracy score of 80% (which gets 8 correct answers every 10 tries).
    That said, once the models have been generated and evaluated, AutoGluon simply
    chooses the model with the highest evaluation metric value (for example, *accuracy*)
    and tags it as the “best model.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our “best model” ready, what do we do next? The next step
    is for us to evaluate the “best model” using the test dataset. That said, let’s
    prepare the test dataset for inference by removing the target label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With everything ready, let’s use the `predict()` method to predict the `is_cancelled`
    column value of the test dataset provided as the payload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the actual *y* values (`y_test`) and the predicted *y* values
    (`y_pred`), let’s quickly check the performance of the trained model by using
    the `evaluate_predictions()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous block of code should yield performance metric values similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: In this step, we compare the actual values with the predicted values for the
    target column using a variety of formulas that compare how close these values
    are to each other. Here, the goal of the trained models is to make “the least
    number of mistakes” as possible over unseen data. Better models generally have
    better scores for performance metrics such as **accuracy**, **Matthews correlation
    coefficient** (**MCC**), and **F1-score**. We won’t go into the details of how
    model performance metrics work here. Feel free to check out [https://bit.ly/3zn2crv](https://bit.ly/3zn2crv)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are done with our quick experiment, let’s exit the **IPython**
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more we can do using AutoGluon but this should help us appreciate how
    easy it is to use AutoGluon for AutoML experiments. There are other methods we
    can use, such as `leaderboard()`, `get_model_best()`, and `feature_importance()`,
    so feel free to check out [https://auto.gluon.ai/stable/index.xhtml](https://auto.gluon.ai/stable/index.xhtml)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with SageMaker and SageMaker Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When performing ML and ML engineering on AWS, professionals should consider
    using one or more of the capabilities and features of **Amazon SageMaker**. If
    this is your first time learning about SageMaker, it is a fully managed ML service
    that helps significantly speed up the process of preparing, training, evaluating,
    and deploying ML models.
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering what these capabilities are, check out some of the capabilities
    tagged under **ML SERVICES** in *Figure 1.2* from the *How ML engineers can get
    the most out of AWS* section. We will tackle several capabilities of SageMaker
    as we go through the different chapters of this book. In the meantime, we will
    start with SageMaker Studio as we will need to set it up first before we work
    on the SageMaker Canvas and SageMaker Autopilot examples.
  prefs: []
  type: TYPE_NORMAL
- en: Onboarding with SageMaker Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SageMaker Studio** provides a feature-rich IDE for ML practitioners. One
    of the great things about SageMaker Studio is its tight integration with the other
    capabilities of SageMaker, which allows us to manage different SageMaker resources
    by just using the interface.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For us to have a good idea of what it looks like and how it works, let’s proceed
    with setting up and configuring SageMaker Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: In the search bar of the AWS console, type `sagemaker studio`. Select **SageMaker
    Studio** under **Features**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose **Standard setup**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Setup SageMaker Domain ](img/B18638_01_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – Setup SageMaker Domain
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, **Standard setup** should give us more configuration options
    to tweak over **Quick setup**. Before clicking the **Configure** button, make
    sure that you are using the same region where the S3 bucket and training and test
    datasets are located.
  prefs: []
  type: TYPE_NORMAL
- en: Under **Authentication**, select **AWS Identity and Access Management (IAM)**.
    For the default execution role under **Permission**, choose **Create a new role**.
    Choose **Any S3 bucket**. Then, click **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under `us-west-2a`), similar to what is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Network and Storage Section ](img/B18638_01_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Network and Storage Section
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have also configured the SageMaker Domain to use the default SageMaker
    internet access by selecting **Public Internet Only**. Under **Encryption key**,
    we leave this unchanged by choosing **No Custom Encryption**. Review the configuration
    and then click **Next**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that for production environments, the security configuration specified
    in the last few steps needs to be reviewed and upgraded further. In the meantime,
    this should do the trick since we’re dealing with a sample dataset. We will discuss
    how to secure environments in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*,
    Security, Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: Under **Studio settings**, leave everything as-is and click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, under **General settings** | **RStudio Workbench**, click **Submit**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have completed these steps, you should see the **Preparing SageMaker
    Domain** loading message. This step should take around 3 to 5 minutes to complete.
    Once complete, you should see a notification stating **The SageMaker Domain is
    ready**.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a user to an existing SageMaker Domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that our **SageMaker Domain** is ready, let’s create a user. Creating a
    user is straightforward. So, let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **SageMaker Domain/Control Panel** page, click **Add user**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the name of the user under **Name**. Under **Default execution role**,
    select the execution role that you created in the previous step. Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Studio settings** | **SageMaker Projects and JumpStart**, click **Next.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **RStudio settings** | **Rstudio Workbench**, click **Submit.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This should do the trick for now. In [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*,
    Security, Governance, and Compliance Strategies*, we will review how we can improve
    the configuration here to improve the security of our environment.
  prefs: []
  type: TYPE_NORMAL
- en: No-code machine learning with SageMaker Canvas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we proceed with using the more comprehensive set of SageMaker capabilities
    to perform ML experiments and deployments, let’s start by building a model using
    **SageMaker Canvas**. One of the great things about SageMaker Canvas is that no
    coding work is needed to build models and use them to perform predictions. Of
    course, **SageMaker Autopilot** would have a more powerful and flexible set of
    features, but SageMaker Canvas should help business analysts, data scientists,
    and junior ML engineers understand the ML process and get started building models
    right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our dataset has already been uploaded to the S3 bucket, we can start
    building and training our first SageMaker Canvas model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **SageMaker Domain/Control Panel** page, locate the row of the user
    we just created and click **Launch app**. Choose **Canvas** from the list of options
    available in the drop-down menu, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Launching SageMaker Canvas ](img/B18638_01_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Launching SageMaker Canvas
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can launch SageMaker Canvas from the **SageMaker Domain/Control
    Panel** page. We can launch SageMaker Studio here as well, which we’ll do later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **New model**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.16 – The SageMaker Canvas Models page ](img/B18638_01_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – The SageMaker Canvas Models page
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have the SageMaker Canvas **Models** page, which should list the models
    we have trained. Since we have not trained anything yet, we should see the **You
    haven’t created any models yet** message.
  prefs: []
  type: TYPE_NORMAL
- en: In the `first-model`) and click **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you see the **Getting Started** guide window, click **Skip intro**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `booking.train.csv` and `booking.test.csv` files inside the `Amazon S3/<S3
    BUCKET>/datasets/bookings` folder of the S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.17 – Choose files to import ](img/B18638_01_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – Choose files to import
  prefs: []
  type: TYPE_NORMAL
- en: Select the necessary CSV files, as shown in the preceding screenshot, and click
    **Import data**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that you may have a hard time locating the S3 bucket we created in the
    *Uploading the dataset to S3* section if you have a significant number of S3 buckets
    in your account. Feel free to use the search box (with the **Search Amazon S3**
    placeholder) located on the right-hand side, just above the table that lists the
    different S3 buckets and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Once the files have been imported, click the radio button of the row that contains
    `bookings.train.csv`. Click **Select dataset**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `is_cancelled` from the list of drop-down options for the **Target column**
    field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, click **Preview model** (under the **Quick build** button), as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.18 – The Build tab ](img/B18638_01_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – The Build tab
  prefs: []
  type: TYPE_NORMAL
- en: After a few minutes, we should get an estimated accuracy of around 70%. Note
    that you might get a different set of numbers in this step.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Quick build** and wait for the model to be ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take up to 15 minutes to complete. While waiting, let’s quickly
    discuss the difference between **Quick build** and **Standard build**. Quick build
    uses fewer records for training and generally lasts around 2 to 15 minutes, while
    Standard build lasts much longer – generally around 2 to 4 hours. It is important
    to note that models that are trained using Quick build can’t be shared with other
    data scientists or ML engineers in SageMaker Studio. On the other hand, models
    trained using Standard build can be shared after the build has been completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the results are available, you may open the **Scoring** tab by clicking
    the tab highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.19 – The Analyze tab ](img/B18638_01_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – The Analyze tab
  prefs: []
  type: TYPE_NORMAL
- en: We should see a quick chart showing the number of records that were used to
    analyze the model, along with the number of correct versus incorrect predictions
    the model has made.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have built an ML model that we can use to predict whether
    a booking will be cancelled or not. Since the accuracy score in this example is
    only around 70%, we’re expecting the model to get about 7 correct answers every
    10 tries. In [*Chapter 11*](B18638_11.xhtml#_idTextAnchor231)*, Machine Learning
    Pipelines with SageMaker Pipelines*, we will train an improved version of this
    model with an accuracy score of around 88%.
  prefs: []
  type: TYPE_NORMAL
- en: Once we are done checking the different numbers and charts in the **Analyze**
    tab, we can proceed by clicking the **Predict** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `bookings.test.csv` and click **Generate predictions**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the **Status** column value is set to **Ready**, hover over the **Status**
    column of the row, click the 3 dots (which will appear after hovering over the
    row), and then select **Preview** from the list of options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Batch prediction results ](img/B18638_01_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Batch prediction results
  prefs: []
  type: TYPE_NORMAL
- en: We should see a table of values, similar to what is shown in the preceding screenshot.
    In the first column, we should have the predicted values for the `is_cancelled`
    field for each of the rows of our test dataset. In the second column, we should
    find the probability of the prediction being correct.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can also perform a single prediction by using the interface provided
    after clicking **Single prediction** under **Predict target values**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s log out of our session. Click the **Account** icon in the left
    sidebar and select the **Log out** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you always log out of the current session after using SageMaker
    Canvas to avoid any unexpected charges. For more information, go to [https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Wasn’t that easy? Now that we have a good idea of how to use SageMaker Canvas,
    let’s run an AutoML experiment using SageMaker Autopilot.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML with SageMaker Autopilot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SageMaker Autopilot** allows ML practitioners to build high-quality ML models
    without having to write a single line of code. Of course, it is possible to programmatically
    configure, run, and manage SageMaker Autopilot experiments using the **SageMaker
    Python SDK**, but we will focus on using the SageMaker Studio interface to run
    the AutoML experiment. Before jumping into configuring our first Autopilot experiment,
    let’s see what happens behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21 – AutoML with SageMaker Autopilot   ](img/B18638_01_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – AutoML with SageMaker Autopilot
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we can see the different steps that are performed
    by SageMaker Autopilot when we run the AutoML experiment. It starts with the **data
    pre-processing** step and proceeds with the **generation of candidate models**
    (pipeline and algorithm pair) step. Then, it continues to perform the **feature
    engineering** and **model tuning** steps, which would yield multiple trained models
    from different model families, hyperparameter values, and model performance metric
    values. The generated model with the best performance metric values is tagged
    as the “best model” by the Autopilot job. Next, two reports are generated: the
    **explainability report** and the **insights report**. Finally, the model is deployed
    to an inference endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive a bit deeper into what is happening in each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pre-processing**: Data is cleaned automatically and missing values are
    automatically imputed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Candidate definition generation**: Multiple “candidate definitions” (composed
    of a data processing job and a training job) are generated, all of which will
    be used on the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Here, data transformations are applied to perform
    automated feature engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model tuning**: The **Automatic Model Tuning** (hyperparameter tuning) capability
    of SageMaker is used to generate multiple models using a variety of hyperparameter
    configuration values to find the “best model.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainability report generation**: The model explainability report, which
    makes use of SHAP values to help explain the behavior of the generated model,
    is generated using tools provided by **SageMaker Clarify** (another capability
    of SageMaker focused on AI **fairness** and **explainability**). We’ll dive a
    bit deeper into this topic later in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insights report generation**: The insights report, which includes data insights
    such as scalar metrics, which help us understand our dataset better, is generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: The best model is deployed to a dedicated inference endpoint.
    Here, the value of the objective metric is used to determine which is the best
    model out of all the models trained during the model tuning step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering if AutoML solutions would fully “replace” data scientists,
    then a quick answer to your question would be “no” or “not anytime soon.” There
    are specific areas of the ML process that require domain knowledge to be available
    to data scientists. AutoML solutions help provide a good starting point that data
    scientists and ML practitioners can build on top of. For example, white box AutoML
    solutions such as SageMaker Autopilot can generate scripts and notebooks that
    can be modified by data scientists and ML practitioners to produce custom and
    complex data processing, experiment, and deployment flows and pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better idea of what happens during an Autopilot experiment,
    let’s run our first Autopilot experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Control Panel** page, click the **Launch app** drop-down menu and
    choose **Studio** from the list of drop-down options, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.22 – Opening SageMaker Studio ](img/B18638_01_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.22 – Opening SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: Note that it may take around 5 minutes for **SageMaker Studio** to load if this
    is your first time opening it.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: AWS releases updates and upgrades for SageMaker Studio regularly. To ensure
    that you are using the latest version, make sure that you shut down and update
    SageMaker Studio and Studio Apps. For more information, go to [https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the **File** menu and click **Experiment** under the **New** submenu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.23 – Using the File menu to create a new experiment   ](img/B18638_01_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.23 – Using the File menu to create a new experiment
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have multiple options under the **New** submenu. We will explore the
    other options throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will configure the Autopilot experiment, similar
    to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.24 – Configuring the Autopilot experiment ](img/B18638_01_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.24 – Configuring the Autopilot experiment
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the different configuration options that are available before
    running the Autopilot experiment. Note that the actual Autopilot experiment settings
    form only has a single column instead of two.
  prefs: []
  type: TYPE_NORMAL
- en: Specify the `first-automl-job`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `bookings.train.csv` we uploaded earlier by clicking **Browse**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Target** drop-down menu, choose **is_cancelled**. Click **Next: Training
    method**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leave everything else as is, and then click **Next**: **Deployment and advanced
    settings**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that the **Auto deploy**? configuration is set to Yes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You may opt to set the **Auto deploy** configuration to **No** instead so that
    an inference endpoint will not be created by the Autopilot job. If you have set
    this to **Yes** make sure that you delete the inference endpoint if you are not
    using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Advanced Settings** (**optional**) **> Runtime**, set **Max Candidates**
    to **20** (or alternatively, setting both **Max trial runtime Minutes** and **Max
    job runtime Minutes** to **20**). Click **Next: Review and create**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Setting the value for `20` means that Autopilot will train and consider only
    20 candidate models for this Autopilot job. Of course, we can set this to a higher
    number, which would increase the chance of finding a candidate with a higher evaluation
    metric score (for example, a model that performs better). However, this would
    mean that it would take longer for Autopilot to run since we’ll be running more
    training jobs. Since we are just trying out this capability, we should be fine
    setting `20` in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Review all the configuration parameters we have set in the previous steps and
    click **Create experiment**. When asked if you want to auto-deploy the best model,
    click **Confirm**. Once the AutoML job has started, we should see a loading screen
    similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.25 – Waiting for the AutoML job to complete ](img/B18638_01_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.25 – Waiting for the AutoML job to complete
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the Autopilot job involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Candidate Definitions Generated**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature Engineering**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Tuning**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Explainability Report Generated**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Insights Report Generated**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploying Model**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we have set the **Auto deploy** configuration to **Yes,** the best model
    is deployed automatically into an inference endpoint that will run 24/7\.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take around 30 minutes to 1 hour to complete. Feel free to get
    a cup of coffee or tea!
  prefs: []
  type: TYPE_NORMAL
- en: 'After about an hour, we should see a list of trials, along with several models
    that have been generated by multiple training jobs, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.26 – Autopilot job results ](img/B18638_01_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.26 – Autopilot job results
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also see two buttons on the top right-hand side of the page: **Open
    candidate generation notebook** and **Open data exploration notebook**. Since
    these two notebooks are generated early in the process, we may see the buttons
    appear about 10 to 15 minutes after the experiment started.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Open candidate generation notebook** and **Open data exploration
    notebook** buttons to open the notebooks that were generated by SageMaker Autopilot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.27 – The Data Exploration Report (left) and the Candidate Definition
    Notebook (right) ](img/B18638_01_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.27 – The Data Exploration Report (left) and the Candidate Definition
    Notebook (right)
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the **Data Exploration Report** on the left-hand side and the
    **Candidate Definition Notebook** on the right. The **Data Exploration Report**
    helps data scientists and ML engineers identify issues in the given dataset. It
    contains a column analysis report that shows the percentage of missing values,
    along with some count statistics and descriptive statistics. On the other hand,
    the **Candidate Definition Notebook** contains the suggested ML algorithm, along
    with the prescribed hyperparameter ranges. In addition to these, it contains the
    recommended pre-processing steps before the training step starts.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about these generated notebooks is that we can modify certain
    sections of these notebooks as needed. This makes SageMaker Autopilot easy for
    beginners to use while still allowing intermediate users to customize certain
    parts of the AutoML process.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know more about SageMaker Autopilot, including the output artifacts
    generated by the AutoML experiment, check out [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132),
    *SageMaker Training and Debugging Solutions*, of the book *Machine Learning with
    Amazon SageMaker Cookbook*. You should find several recipes there that focus on
    programmatically running and managing an Autopilot experiment using the **SageMaker
    Python SDK**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate back to the tab containing the results of the Autopilot job. Right-click
    on the row with the **Best Model** tag and choose **Open in model details** from
    the options in the context menu. This should open a page similar to what is shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.28 – The model details page ](img/B18638_01_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.28 – The model details page
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that **reserved_room_type, lead_time, and adr** are the most
    important features that affect the chance of a hotel booking getting canceled.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that you may get a different set of results from what we have in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should see the following information on the model details page as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location of the input and output artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model metric values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter values used to train the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you delete the inference endpoint(s) created after running the
    SageMaker Autopilot experiment. To find the running inference endpoints, simply
    navigate to [https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints)
    and manually delete the unused resources. Note that the link provided assumes
    that the inference endpoint has been created in the **Oregon** (**us-west-2**)
    region. We will skip performing sample predictions using the inference endpoint
    for now. We will cover this, along with deployment strategies, in [*Chapter 7*](B18638_07.xhtml#_idTextAnchor151),
    *SageMaker Deployment Solutions*.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have a good grasp of how to use several AutoML solutions
    such as **AutoGluon**, **SageMaker Canvas**, and **SageMaker Autopilot**. As we
    saw in the hands-on solutions of this section, we have a significant number of
    options when using SageMaker Autopilot to influence the process of finding the
    best model. If we are more comfortable with a simpler UI with fewer options, then
    we may use SageMaker Canvas instead. If we are more comfortable developing and
    engineering ML solutions through code, then we can consider using AutoGluon as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got our feet wet by performing multiple AutoML experiments
    using a variety of services, capabilities, and tools on AWS. This included using
    AutoGluon within a Cloud9 environment and SageMaker Canvas and SageMaker Autopilot
    to run AutoML experiments. The solutions presented in this chapter helped us have
    a better understanding of the fundamental ML and ML engineering concepts as well.
    We were able to see some of the steps in the ML process in action, such as EDA,
    train-test split, model training, evaluation, and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on how the **AWS Deep Learning AMIs** help
    speed up the ML experimentation process. We will also take a closer look at how
    AWS pricing works for EC2 instances so that we are better equipped when managing
    the overall cost of running ML workloads in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information regarding the topics that were covered in this chapter,
    check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*AutoGluon: AutoML for Text, Image, and Tabular Data* ([https://auto.gluon.ai/stable/index.xhtml](https://auto.gluon.ai/stable/index.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automate model development with Amazon SageMaker Autopilot* (https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker Canvas Pricing* (https://aws.amazon.com/sagemaker/canvas/pricing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning with Amazon SageMaker Cookbook*, by Joshua Arvin Lat ([https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/](https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
