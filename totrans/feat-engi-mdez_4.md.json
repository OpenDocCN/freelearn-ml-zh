["```py\n# import pandas as pd\n```", "```py\nX = pd.DataFrame({'city':['tokyo', None, 'london', 'seattle', 'san francisco', 'tokyo'], \n                  'boolean':['yes', 'no', None, 'no', 'no', 'yes'], \n                  'ordinal_column':['somewhat like', 'like', 'somewhat like', 'like', 'somewhat like', 'dislike'], \n                  'quantitative_column':[1, 11, -.5, 10, None, 20]})\n```", "```py\nprint X\n```", "```py\nX.isnull().sum()\n>>>>\nboolean                1\ncity                   1\nordinal_column         0\nquantitative_column    1\ndtype: int64\n```", "```py\n# Let's find out what our most common category is in our city column\nX['city'].value_counts().index[0]\n\n>>>>\n'tokyo'\n```", "```py\n# fill empty slots with most common category\nX['city'].fillna(X['city'].value_counts().index[0])\n\n```", "```py\n0            tokyo\n1            tokyo\n2           london\n3          seattle\n4    san francisco\n5            tokyo\nName: city, dtype: object\n```", "```py\nfrom sklearn.base import TransformerMixin\n\nclass CustomCategoryImputer(TransformerMixin):\n    def __init__(self, cols=None):\n        self.cols = cols\n\n    def transform(self, df):\n        X = df.copy()\n        for col in self.cols:\n            X[col].fillna(X[col].value_counts().index[0], inplace=True)\n        return X\n\n    def fit(self, *_):\n        return self\n```", "```py\nfrom sklearn.base import TransformerMixin\n```", "```py\nclass CustomCategoryImputer(TransformerMixin):\n    def __init__(self, cols=None):\n        self.cols = cols\n```", "```py\ndef transform(self, df):\n        X = df.copy()\n        for col in self.cols:\n            X[col].fillna(X[col].value_counts().index[0], inplace=True)\n        return X\n```", "```py\ndef fit(self, *_):\n        return self\n```", "```py\n# Implement our custom categorical imputer on our categorical columns.\n\ncci = CustomCategoryImputer(cols=['city', 'boolean'])\n\n```", "```py\ncci.fit_transform(X)\n```", "```py\n# Lets make an imputer that can apply a strategy to select columns by name\n\nfrom sklearn.preprocessing import Imputer\nclass CustomQuantitativeImputer(TransformerMixin):\n    def __init__(self, cols=None, strategy='mean'):\n        self.cols = cols\n        self.strategy = strategy\n\n    def transform(self, df):\n        X = df.copy()\n        impute = Imputer(strategy=self.strategy)\n        for col in self.cols:\n            X[col] = impute.fit_transform(X[[col]])\n        return X\n\n    def fit(self, *_):\n        return self\n```", "```py\ncqi = CustomQuantitativeImputer(cols=['quantitative_column'], strategy='mean')\n\ncqi.fit_transform(X)\n```", "```py\n# import Pipeline from sklearn\nfrom sklearn.pipeline import Pipeline\n```", "```py\nimputer = Pipeline([('quant', cqi), ('category', cci)]) imputer.fit_transform(X)\n```", "```py\npd.get_dummies(X, \n               columns = ['city', 'boolean'],  # which columns to dummify\n               prefix_sep='__')  # the separator between the prefix (column name) and cell value\n\n```", "```py\n# create our custom dummifier\nclass CustomDummifier(TransformerMixin):\n    def __init__(self, cols=None):\n        self.cols = cols\n\n    def transform(self, X):\n        return pd.get_dummies(X, columns=self.cols)\n\n    def fit(self, *_):\n        return self\n\n```", "```py\n# set up a list with our ordinal data corresponding the list index\nordering = ['dislike', 'somewhat like', 'like']  # 0 for dislike, 1 for somewhat like, and 2 for like\n# before we map our ordering to our ordinal column, let's take a look at the column\n\nprint X['ordinal_column']\n>>>>\n0 somewhat like \n1 like \n2 somewhat like \n3 like \n4 somewhat like \n5 dislike \nName: ordinal_column, dtype: object\n```", "```py\nlambda x: ordering.index(x)\n```", "```py\n# now map our ordering to our ordinal column:\nprint X['ordinal_column'].map(lambda x: ordering.index(x))\n>>>>\n0    1\n1    2\n2    1\n3    2\n4    1\n5    0\nName: ordinal_column, dtype: int64\n```", "```py\nclass CustomEncoder(TransformerMixin):\n    def __init__(self, col, ordering=None):\n        self.ordering = ordering\n        self.col = col\n\n    def transform(self, df):\n        X = df.copy()\n        X[self.col] = X[self.col].map(lambda x: self.ordering.index(x))\n        return X\n\n    def fit(self, *_):\n        return self\n```", "```py\nce = CustomEncoder(col='ordinal_column', ordering = ['dislike', 'somewhat like', 'like'])\n\nce.fit_transform(X)\n```", "```py\n# name of category is the bin by default\npd.cut(X['quantitative_column'], bins=3)\n```", "```py\n0     (-0.52, 6.333]\n1    (6.333, 13.167]\n2     (-0.52, 6.333]\n3    (6.333, 13.167]\n4                NaN\n5     (13.167, 20.0]\nName: quantitative_column, dtype: category\nCategories (3, interval[float64]): [(-0.52, 6.333] < (6.333, 13.167] < (13.167, 20.0]]\n```", "```py\n# using no labels\npd.cut(X['quantitative_column'], bins=3, labels=False)\n```", "```py\n0    0.0\n1    1.0\n2    0.0\n3    1.0\n4    NaN\n5    2.0\nName: quantitative_column, dtype: float64\n```", "```py\nclass CustomCutter(TransformerMixin):\n    def __init__(self, col, bins, labels=False):\n        self.labels = labels\n        self.bins = bins\n        self.col = col\n\n    def transform(self, df):\n        X = df.copy()\n        X[self.col] = pd.cut(X[self.col], bins=self.bins, labels=self.labels)\n        return X\n\n    def fit(self, *_):\n        return self\n```", "```py\ncc = CustomCutter(col='quantitative_column', bins=3)\n\ncc.fit_transform(X)\n```", "```py\nfrom sklearn.pipeline import Pipeline\n\n```", "```py\npipe = Pipeline([(\"imputer\", imputer), ('dummify', cd), ('encode', ce), ('cut', cc)])\n# will use our initial imputer\n# will dummify variables first\n# then encode the ordinal column\n# then bucket (bin) the quantitative column\n```", "```py\n# take a look at our data before fitting our pipeline\nprint X \n```", "```py\n# now fit our pipeline\npipe.fit(X)\n\n>>>>\nPipeline(memory=None,\n     steps=[('imputer', Pipeline(memory=None,\n     steps=[('quant', <__main__.CustomQuantitativeImputer object at 0x128bf00d0>), ('category', <__main__.CustomCategoryImputer object at 0x13666bf50>)])), ('dummify', <__main__.CustomDummifier object at 0x128bf0ed0>), ('encode', <__main__.CustomEncoder object at 0x127e145d0>), ('cut', <__main__.CustomCutter object at 0x13666bc90>)])\n```", "```py\npipe.transform(X)\n```", "```py\ndf = pd.read_csv('../data/activity_recognizer/1.csv', header=None)\ndf.columns = ['index', 'x', 'y', 'z', 'activity']\n```", "```py\ndf.head()\n```", "```py\ndf['activity'].value_counts(normalize=True)\n\n7    0.515369\n1    0.207242\n4    0.165291\n3    0.068793\n5    0.019637\n6    0.017951\n2    0.005711\n0    0.000006\nName: activity, dtype: float64\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n```", "```py\nX = df[['x', 'y', 'z']]\n# create our feature matrix by removing the response variable\ny = df['activity']\n```", "```py\n# our grid search variables and instances\n\n# KNN parameters to try\nknn_params = {'n_neighbors':[3, 4, 5, 6]}\n```", "```py\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn, knn_params)\ngrid.fit(X, y)\n```", "```py\nprint grid.best_score_, grid.best_params_\n\n0.720752487677 {'n_neighbors': 5}\n```", "```py\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n```", "```py\nX_poly = poly.fit_transform(X)\nX_poly.shape\n\n(162501, 9)\n```", "```py\npd.DataFrame(X_poly, columns=poly.get_feature_names()).head()\n```", "```py\n%matplotlib inline\nimport seaborn as sns\n```", "```py\nsns.heatmap(pd.DataFrame(X_poly, columns=poly.get_feature_names()).corr())\n```", "```py\npoly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True) X_poly = poly.fit_transform(X) print X_poly.shape\n (162501, 6)\n```", "```py\npd.DataFrame(X_poly, columns=poly.get_feature_names()).head()\n```", "```py\nsns.heatmap(pd.DataFrame(X_poly,\ncolumns=poly.get_feature_names()).corr())\n```", "```py\npipe_params = {'poly_features__degree':[1, 2, 3], 'poly_features__interaction_only':[True, False], 'classify__n_neighbors':[3, 4, 5, 6]}\n```", "```py\npipe = Pipeline([('poly_features', poly), ('classify', knn)])\n```", "```py\ngrid = GridSearchCV(pipe, pipe_params)\ngrid.fit(X, y)\n\nprint grid.best_score_, grid.best_params_\n\n0.721189408065 {'poly_features__degree': 2, 'poly_features__interaction_only': True, 'classify__n_neighbors': 5}\n```", "```py\ntweets = pd.read_csv('../data/twitter_sentiment.csv', encoding='latin1')\n```", "```py\ntweets.head()\n```", "```py\ndel tweets['ItemID']\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n```", "```py\nX = tweets['SentimentText']\ny = tweets['Sentiment']\n```", "```py\nvect = CountVectorizer()\n_ = vect.fit_transform(X)\nprint _.shape\n\n(99989, 105849)\n```", "```py\nvect = CountVectorizer(stop_words='english')  # removes a set of english stop words (if, a, the, etc)\n_ = vect.fit_transform(X)\nprint _.shape\n\n(99989, 105545)\n```", "```py\nvect = CountVectorizer(min_df=.05)  # only includes words that occur in at least 5% of the corpus documents\n# used to skim the number of features\n_ = vect.fit_transform(X)\nprint _.shape\n\n(99989, 31)\n```", "```py\nvect = CountVectorizer(max_df=.8)  # only includes words that occur at most 80% of the documents\n# used to \"Deduce\" stop words\n_ = vect.fit_transform(X)\nprint _.shape\n\n(99989, 105849)\n```", "```py\nvect = CountVectorizer(ngram_range=(1, 5))  # also includes phrases up to 5 words\n_ = vect.fit_transform(X)\nprint _.shape  # explodes the number of features\n\n(99989, 3219557)\n```", "```py\nvect = CountVectorizer(analyzer='word')  # default analyzer, decides to split into words\n_ = vect.fit_transform(X)\nprint _.shape\n\n(99989, 105849)\n```", "```py\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\n```", "```py\nstemmer.stem('interesting')\nu'interest'\n```", "```py\n# define a function that accepts text and returns a list of lemmas\ndef word_tokenize(text, how='lemma'):\n    words = text.split(' ')  # tokenize into words\n    return [stemmer.stem(word) for word in words]\n```", "```py\nword_tokenize(\"hello you are very interesting\")\n\n[u'hello', u'you', u'are', u'veri', u'interest']\n```", "```py\nvect = CountVectorizer(analyzer=word_tokenize)\n_ = vect.fit_transform(X)\nprint _.shape  # fewer features as stemming makes words smaller\n\n(99989, 154397)\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```", "```py\nvect = CountVectorizer()\n_ = vect.fit_transform(X)\nprint _.shape, _[0,:].mean()\n\n(99989, 105849) 6.61319426731e-05\n```", "```py\nvect = TfidfVectorizer()\n_ = vect.fit_transform(X)\nprint _.shape, _[0,:].mean() # same number of rows and columns, different cell values\n\n(99989, 105849) 2.18630609758e-05\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB # for faster predictions with large number of features...\n```", "```py\n# get the null accuracy\n y.value_counts(normalize=True)\n\n 1 0.564632 0 0.435368 Name: Sentiment, dtype: float64\n```", "```py\n# set our pipeline parameters\n pipe_params = {'vect__ngram_range':[(1, 1), (1, 2)], 'vect__max_features':[1000, 10000], 'vect__stop_words':[None, 'english']}\n\n # instantiate our pipeline\n pipe = Pipeline([('vect', CountVectorizer()), ('classify', MultinomialNB())])\n\n # instantiate our gridsearch object\n grid = GridSearchCV(pipe, pipe_params)\n # fit the gridsearch object\n grid.fit(X, y)\n\n # get our results\n print grid.best_score_, grid.best_params_\n\n 0.755753132845 {'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__max_features': 10000}\n```", "```py\nfrom sklearn.pipeline import FeatureUnion\n# build a separate featurizer object\nfeaturizer = FeatureUnion([('tfidf_vect', TfidfVectorizer()), ('count_vect', CountVectorizer())])\n```", "```py\n_ = featurizer.fit_transform(X)\n print _.shape # same number of rows , but twice as many columns as either CV or TFIDF\n\n (99989, 211698)\n```", "```py\nfeaturizer.set_params(tfidf_vect__max_features=100, count_vect__ngram_range=(1, 2),\n count_vect__max_features=300)\n # the TfidfVectorizer will only keep 100 words while the CountVectorizer will keep 300 of 1 and 2 word phrases \n _ = featurizer.fit_transform(X)\n print _.shape # same number of rows , but twice as many columns as either CV or TFIDF\n (99989, 400)\n```", "```py\npipe_params = {'featurizer__count_vect__ngram_range':[(1, 1), (1, 2)], 'featurizer__count_vect__max_features':[1000, 10000], 'featurizer__count_vect__stop_words':[None, 'english'],\n 'featurizer__tfidf_vect__ngram_range':[(1, 1), (1, 2)], 'featurizer__tfidf_vect__max_features':[1000, 10000], 'featurizer__tfidf_vect__stop_words':[None, 'english']}\n pipe = Pipeline([('featurizer', featurizer), ('classify', MultinomialNB())])\n grid = GridSearchCV(pipe, pipe_params)\n grid.fit(X, y)\n print grid.best_score_, grid.best_params_\n 0.758433427677 {'featurizer__tfidf_vect__max_features': 10000, 'featurizer__tfidf_vect__stop_words': 'english', 'featurizer__count_vect__stop_words': None, 'featurizer__count_vect__ngram_range': (1, 2), 'featurizer__count_vect__max_features': 10000, 'featurizer__tfidf_vect__ngram_range': (1, 1)}\n```"]