- en: Constructing a Classifier
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple classifier
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a logistic regression classifier
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Naive Bayes classifier
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting a dataset for training and testing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating accuracy using cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a confusion matrix
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting a performance report
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating cars based on their characteristics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting validation curves
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting learning curves
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating a income bracket
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the quality of wine
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newsgroup trending topics classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To work on the recipes in this chapter, you need the following files (available
    on GitHub):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '`simple_classifier.py`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logistic_regression.py`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`naive_bayes.py`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_multivar.txt`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splitting_dataset.py`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confusion_matrix.py`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `performance_report.py` ``'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`car.py`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`car.data.txt`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`income.py`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adult.data.txt`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wine.quality.py`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wine.txt`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post.classification`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the field of machine learning, **classification** refers to the process of
    using the characteristics of data to separate it into a certain number of classes.
    This is different than regression, which we discussed in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml),
    *The Realm of Supervised Learning*, where the output is a real number. A supervised
    learning classifier builds a model using labeled training data and then uses this
    model to classify unknown data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: A classifier can be any algorithm that implements classification. In simple
    cases, a classifier can be a straightforward mathematical function. In more real-world
    cases, a classifier can take very complex forms. In the course of study, we will
    see that classification can be either binary, where we separate data into two
    classes, or it can be multi-class, where we separate data into more than two classes.
    The mathematical techniques that are devised to deal with classification problems
    tend to deal with two classes, so we extend them in different ways to deal with
    multi-class problems as well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the accuracy of a classifier is vital for machine learning. What
    we need to know is, how we can use the available data, and get a glimpse of how
    the model performs in the real world. In this chapter, we will look at recipes
    that deal with all these things.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple classifier
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **classifier** is a system with some characteristics that allow you to identify
    the class of the sample examined. In different classification methods, groups
    are called **classes**. The goal of a classifier is to establish the classification
    criterion to maximize performance. The performance of a classifier is measured
    by evaluating the capacity for generalization. **Generalization** means attributing
    the correct class to each new experimental observation. The way in which these
    classes are identified discriminates between the different methods that are available.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifiers identify the class of a new objective, based on knowledge that's
    been extracted from a series of samples (a dataset). Starting from a dataset,
    a classifier extracts a model, which is then used to classify new instances.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a simple classifier using some training data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `simple_classifier.py` file, already provided to you as a reference.
    To start, we import the `numpy` and `matplotlib.pyplot` packages, as we did in
    [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml), *The Realm of Supervised
    Learning*, and then we create some sample data:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s assign some labels to these points:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As we have only two classes, the `y` list contains 0''s and 1''s. In general,
    if you have *N* classes, then the values in `y` will range from 0 to *N-1*. Let''s
    separate the data into classes based on the labels:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To get an idea about our data, let''s plot it, as follows:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is a **scatterplot**, where we use squares and crosses to plot the points.
    In this context, the `marker` parameter specifies the shape you want to use. We
    use squares to denote points in `class_0` and crosses to denote points in `class_1`. If
    you run this code, you will see the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/542cb6f5-7a7c-486e-a1be-823c65eb12d8.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding two lines, we just use the mapping between `X` and `y` to
    create two lists. If you were asked to inspect the datapoints visually and draw
    a separating line, what would you do? You would simply draw a line in between
    them. Let''s go ahead and do this:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We just created a line with the mathematical equation *y = x*. Let''s plot
    it, as follows:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you run this code, you should see the following output:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fde419aa-36d1-4312-9a2c-f49b054f62c8.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: The preceding shows how that construction of a separation line between the two
    classes was simple. In this simple example, this operation was easy, but in many
    cases, building a line of separation between two classes can be very difficult.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we showed how simple it is to build a classifier. We started
    from a series of identifying pairs of as many points on a plane (*x, y*). We therefore
    assigned a class to each of these points (0,1) so as to divide them into two groups.
    To understand the spatial arrangement of these points, we visualized them by associating
    a different marker to each class. Finally, to divide the two groups, we have drew
    the line of the *y = x* equation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We built a simple classifier using the following rule—the input point (*a, b*)
    belongs to `class_0` if *a* is greater than or equal to *b;* otherwise, it belongs
    to `class_1`. If you inspect the points one by one, you will see that this is,
    in fact, true. That's it! You just built a linear classifier that can classify
    unknown data. It's a linear classifier because the separating line is a straight
    line. If it's a curve, then it becomes a *nonlinear* classifier.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: This formation worked well, because there were a limited number of points, and
    we could visually inspect them. What if there were thousands of points? How would
    we generalize this process? Let's discuss that in the next recipe.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式工作得很好，因为点的数量有限，我们可以直观地检查它们。如果有成千上万的点呢？我们将如何泛化这个过程？让我们在下一个菜谱中讨论这个问题。
- en: See also
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考阅读
- en: The official documentation of the NumPy library ([http://www.numpy.org/](http://www.numpy.org/))
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 库的官方文档([http://www.numpy.org/](http://www.numpy.org/))
- en: The official documentation of the Matplotlib library ([https://matplotlib.org/](https://matplotlib.org/))
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib 库的官方文档([https://matplotlib.org/](https://matplotlib.org/))
- en: Building a logistic regression classifier
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建逻辑回归分类器
- en: Despite the word *regression* being present in the name, logistic regression
    is actually used for classification purposes. Given a set of datapoints, our goal
    is to build a model that can draw linear boundaries between our classes. It extracts
    these boundaries by solving a set of equations derived from the training data.
    In this recipe, we will build a logistic regression classifier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称中包含 `regression` 一词，但逻辑回归实际上用于分类目的。给定一组数据点，我们的目标是构建一个模型，该模型可以在我们的类别之间绘制线性边界。它通过解决从训练数据导出的一组方程来提取这些边界。在这个菜谱中，我们将构建一个逻辑回归分类器。
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Logistic regression is a non-linear regression model used when the dependent
    variable is dichotomous. The purpose is to establish the probability with which
    an observation can generate one or the other value of the dependent variable;
    it can also be used to classify observations, according to their characteristics,
    into two categories.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种非线性回归模型，当因变量是二元时使用。目的是确定一个观测值可以生成因变量一个或另一个值的概率；它也可以根据其特征将观测值分类到两个类别中。
- en: How to do it…
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现它……
- en: 'Let''s see how to build a logistic regression classifier:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个逻辑回归分类器：
- en: 'Let''s see how to do this in Python. We will use the `logistic_regression.py`
    file, provided to you as a reference. Assuming that you imported the necessary
    packages, let''s create some sample data, along with training labels:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Python 中实现这个功能。我们将使用提供的 `logistic_regression.py` 文件作为参考。假设你已经导入了必要的包，让我们创建一些样本数据，以及相应的训练标签：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we assume that we have three classes (`0`, `1`, and `2`).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设我们有三个类别（`0`、`1` 和 `2`）。
- en: 'Let''s initialize the logistic regression classifier:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们初始化逻辑回归分类器：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There are a number of input parameters that can be specified for the preceding
    function, but a couple of important ones are `solver` and `C`. The `solver` parameter
    specifies the type of `solver` that the algorithm will use to solve the system
    of equations. The `C` parameter controls the regularization strength. A lower
    value indicates higher regularization strength.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的函数，可以指定许多输入参数，但其中两个重要的参数是 `solver` 和 `C`。`solver` 参数指定算法将用于解决方程组的 `solver`
    类型。`C` 参数控制正则化强度。较低的值表示较高的正则化强度。
- en: 'Let''s train the classifier:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练分类器：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s draw datapoints and boundaries. To do this, first, we need to define
    ranges to plot the diagram, as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制数据点和边界。为此，首先，我们需要定义绘图的范围，如下所示：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding values indicate the range of values that we want to use in our
    figure. The values usually range from the minimum value to the maximum value present
    in our data. We add some buffers, such as `1.0`, to the preceding lines, for clarity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的值表示我们想在图中使用的值的范围。这些值通常从我们数据中的最小值到最大值。为了清晰起见，我们在前面的行中添加了一些缓冲区，例如 `1.0`。
- en: 'In order to plot the boundaries, we need to evaluate the function across a
    grid of points and plot it. Let''s go ahead and define the grid:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了绘制边界，我们需要在点网格上评估函数并绘制它。让我们继续定义网格：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `x_values` and `y_values` variables contain the grid of points where the
    function will be evaluated.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_values` 和 `y_values` 变量包含函数将被评估的点网格。'
- en: 'Let''s compute the output of the classifier for all these points:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算分类器对所有这些点的输出：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s plot the boundaries using colored regions:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用彩色区域绘制边界：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is basically a 3D plotter that takes the 2D points and the associated values
    to draw different regions using a color scheme.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是一个 3D 绘图器，它接受 2D 点和相关的值，使用颜色方案绘制不同的区域。
- en: 'Let''s overlay the training points on the plot:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在图上叠加训练点：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, `plt.scatter` plots the points on the 2D graph. `X[:, 0]` specifies that
    we should take all the values along the 0 axis (the *x* axis in our case), and
    `X[:, 1]` specifies axis 1 (the *y* axis). The `c=y` parameter indicates the color
    sequence. We use the target labels to map to colors using `cmap`. Basically, we
    want different colors that are based on the target labels. Hence, we use `y` as
    the mapping. The limits of the display figure are set using `plt.xlim` and `plt.ylim`. In
    order to mark the axes with values, we need to use `plt.xticks` and `plt.yticks`. These
    functions mark the axes with values so that it's easier for us to see where the
    points are located. In the preceding code, we want the ticks to lie between the
    minimum and maximum values with a buffer of one unit. Also, we want these ticks
    to be integers. So, we use the `int()` function to round off the values.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this code, you should see the following output:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/34c330f7-3444-4265-b0cd-f37c27e72382.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see how the `C` parameter affects our model. The `C` parameter indicates
    the penalty for misclassification. If we set it to `1.0`, we will get the following:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/72defd40-8554-499b-b9ef-562ef7291b64.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'If we set `C` to `10000`, we get the following:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ac2af76f-7377-4329-a287-9e74512b7094.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: As we increase `C`, there is a higher penalty for misclassification. Hence,
    the boundaries become more optimized.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Logistic regression** is a classification method within the family of supervised
    learning algorithms. Using statistical methods, logistic regression allows us
    to generate a result that, in fact, represents a probability that a given input
    value belongs to a given class. In binomial logistic regression problems, the
    probability that output belongs to a class will be *P*, whereas the probability
    of it belonging to another class will be *1-P* (where *P* is a number between
    0 and 1 because it expresses probability).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression uses the logistic function to determine the classification
    of input values. Also called the **sigmoid** function, the logistic function is
    an S-shaped curve that can take any number of of a real value and map it to a
    value between 0 and 1, extremes excluded. It can be described by the following
    equation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00e3c0f7-9412-4655-95a8-116901dcc226.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: This function transforms the real values into numbers between 0 and 1.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To obtain the logistic regression equation expressed in probabilistic terms,
    we need to include the probabilities in the logistic regression equation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/416b339e-3214-4a8f-bab8-433e2a986a65.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Recalling that the `e` function is the opposite of the natural logarithm (`ln`),
    we can write:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ff52b3b-5cbb-41a7-b55b-cd2e44a6804e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: This function is called a **logit** function. The logit function, on the other
    hand, allows us to associate the probabilities (therefore, a value included between
    0 and 1) to the whole range of real numbers. It is a link function and represents
    the inverse of the logistic function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Logit Models for Binary Data*, Princeton University: [https://data.princeton.edu/](https://data.princeton.edu/wws509/notes/c3.pdf)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression Analysis with R*, Giuseppe Ciaburro, Packt Publishing'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[wws509/notes/c3.pdf](https://data.princeton.edu/wws509/notes/c3.pdf)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib color scheme options: [https://matplotlib.org/examples/color/colormaps_reference.html](http://matplotlib.org/examples/color/colormaps_reference.html)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Naive Bayes classifier
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classifier solves the problem of identifying sub-populations of individuals
    with certain features in a larger set, with the possible use of a subset of individuals
    known as a priori (a training set). A Naive Bayes classifier is a supervised learning
    classifier that uses Bayes' theorem to build the model. In this recipe, we will
    build a Naive Bayes classifier.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The underlying principle of a Bayesian classifier is that some individuals belong
    to a class of interest with a given probability based on some observations. This
    probability is based on the assumption that the characteristics observed can be
    either dependent or independent from one another; in this second case, the Bayesian
    classifier is called Naive because it assumes that the presence or absence of
    a particular characteristic in a given class of interest is not related to the
    presence or absence of other characteristics, greatly simplifying the calculation.
    Let's go ahead and build a Naive Bayes classifier.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a Naive Bayes classifier:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `naive_bayes.py`, provided to you as a reference. Let''s import
    some libraries:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You were provided with a `data_multivar.txt` file. This contains data that
    we will use here. This contains comma-separated numerical data in each line. Let''s
    load the data from this file:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We have now loaded the input data into `X` and the labels into `y`. There are
    four labels: 0, 1, 2, and 3.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build the Naive Bayes classifier:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `gauusiannb` function specifies the Gaussian Naive Bayes model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute the `accuracy` measure of the classifier:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following accuracy is returned:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s plot the data and the boundaries. We will use the procedure followed
    in the previous recipe, *Building a logistic regression classifier*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8c4578e-99bb-4249-ba9e-1658a56cffc9.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: There is no restriction on the boundaries to be linear here. In the preceding
    recipe, *Building a logistic regression classifier*, we used up all the data for
    training. A good practice in machine learning is to have non-overlapping data
    for training and testing. Ideally, we need some unused data for testing so that
    we can get an accurate estimate of how the model performs on unknown data. There
    is a provision in `scikit-learn` that handles this very well, as shown in the
    next recipe.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Bayesian classifier** is a classifier based on the application of Bayes'
    theorem. This classifier requires the knowledge of a priori and conditional probabilities
    related to the problem; quantities that, in general, are not known but are typically
    estimable. If reliable estimates of the probabilities involved in the theorem
    can be obtained, the Bayesian classifier is generally reliable and potentially
    compact.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability that a given event (*E*) occurs, is the ratio between the number
    (*s*) of favorable cases of the event itself and the total number (*n*) of the
    possible cases, provided all the considered cases are equally probable. This can
    be better represented using the following formula:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6c805cf-487c-462a-aeb1-c403af96d89d.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Given two events, *A* and *B*, if the two events are independent (the occurrence
    of one does not affect the probability of the other), the joint probability of
    the event is equal to the product of the probabilities of *A* and *B*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79748dbb-b02c-44a6-b528-0b310abefd67.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'If the two events are dependent (that is, the occurrence of one affects the
    probability of the other), then the same rule may apply, provided *P(B | A)* is
    the probability of event *A* given that event *B* has occurred. This condition
    introduces conditional probability, which we are going to dive into now:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53491427-e7b8-4df8-bddc-9a084ac16632.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'The probability that event *A* occurs, calculated on the condition that event
    *B* occurred, is called **conditional probability**, and is indicated by *P(A
    | B)*. It is calculated using the following formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cba40f96-9806-4cb3-8c79-9854a24051cd.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Let *A* and *B* be two dependent events, as we stated that the joint probability
    between them is calculated using the following formula:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08b9a04b-428f-404e-b067-96923b283638.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Or, similarly, we can use the following formula:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bc29719-ee15-483d-b8e6-a1e6125e4f59.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: 'By looking at the two formulas, we see that they have the first equal member.
    This shows that even the second members are equal, so the following equation can
    be written:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a06d2fd7-d73f-471d-a77f-e3675deab9c8.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'By solving these equations for conditional probability, we get the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a145cfc1-adaf-4937-8bd2-f1bca05834ef.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: The proposed formulas represent the mathematical statement of Bayes' theorem.
    The use of one or the other depends on what we are looking for.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 1763, an article by Reverend Thomas Bayes was published in England; the
    article became famous for its implications. According to the article, making predictions
    about a phenomenon depends not only on the observations that the scientist obtains
    from his experiments, but also on what he himself thinks and understands of the
    phenomenon studied, even before proceeding to the experiment itself. These premises
    were developed in the 1900s by distinguished scholars, such as Bruno de Finetti
    (*La prévision: ses lois logiques, ses sources subjectives*, 1937), L J Savage
    (*The Fondations of statistics Reconsidered*, 1959), and others.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Keras 2.x Projects*, Giuseppe Ciaburro, Packt Publishing.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayes'' Theorem*, Stanford Encyclopedia of Philosophy: [https://plato.stanford.edu/entries/bayes-theorem/](https://plato.stanford.edu/entries/bayes-theorem/)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.naive_bayes.GaussianNB` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting a dataset for training and testing
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how to split our data properly into training and testing datasets.
    As we said in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml), *The Realm
    of Supervised Learning*, in the *Building a linear regressor* recipe, when we
    build a machine learning model, we need a way to validate our model to check whether
    it is performing at a satisfactory level. To do this, we need to separate our
    data into two groups—a **training** dataset and a **testing** dataset. The training
    dataset will be used to build the model, and the testing dataset will be used
    to see how this trained model performs on unknown data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to split the dataset for training and testing
    phases.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fundamental objective of a model based on machine learning is to make accurate
    predictions. Before using a model to make predictions, it is necessary to evaluate
    the predictive performance of the model. To estimate the quality of a model''s
    predictions, it is necessary to use data that you have never seen before. Training
    a predictive model and testing it on the same data is a methodological error:
    a model that simply classifies the labels of samples it has just seen would have
    a high score but would not be able to predict the new data class. Under these
    conditions, the generalization capacity of the model would be less.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to split the dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the recipe is similar to the previous recipe, *Building a
    Naive Bayes classifier* (load the `Splitting_dataset.py` file):'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we allocated 25% of the data for testing, as specified by the `test_size`
    parameter. The remaining 75% of the data will be used for training.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s evaluate the classifier on the test data:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s compute the `accuracy` measure of the classifier:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following result is printed:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s plot the datapoints and the boundaries on the test data:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在测试数据上绘制数据点和边界：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should see the following:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该看到以下内容：
- en: '![](img/562ba592-f77d-4143-b8f7-ebeadc718a95.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/562ba592-f77d-4143-b8f7-ebeadc718a95.png)'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we split the data using the `train_test_split()` function of
    the `scikit-learn` library. This function splits arrays or matrices into random
    train and testing subsets. Random division of input data into data sources for
    training and testing ensures that data distribution is similar for training and
    testing data sources. You choose this option when it is not necessary to preserve
    the order of the input data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用`scikit-learn`库的`train_test_split()`函数来分割数据。这个函数将数组或矩阵分割成随机的训练和测试子集。将输入数据随机分割成训练和测试数据源确保了训练和测试数据源的数据分布相似。当不需要保留输入数据的顺序时，您可以选择此选项。
- en: There's more...
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The performance estimate depends on the data used. Therefore, simply dividing
    data randomly into a training and a testing set does not guarantee that the results
    are statistically significant. The repetition of the evaluation on different random
    divisions and the calculation of the performance in terms of the average and standard
    deviation of the individual evaluations creates a more reliable estimate.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 性能估计取决于所使用的数据。因此，简单地将数据随机分割成训练和测试集并不能保证结果具有统计学意义。在不同随机分割上重复评估以及计算性能的平均值和标准差可以创建一个更可靠的估计。
- en: However, even the repetition of evaluations on different random divisions could
    prevent the most complex data being classified in the testing (or training) phase.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在不同的随机分割上重复评估，也可能防止最复杂的数据在测试（或训练）阶段被分类。
- en: See also
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考阅读
- en: 'The official documentation of the `sklearn.model_selection.train_test_split`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.model_selection.train_test_split`函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)'
- en: '*Data Splitting*, Charles University: [https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据分割*，查尔斯大学：[https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf)'
- en: Evaluating accuracy using cross-validation metrics
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证指标评估精度
- en: '**Cross-validation** is an important concept in machine learning. In the previous
    recipe, we split the data into training and testing datasets. However, in order
    to make it more robust, we need to repeat this process with different subsets.
    If we just fine-tune it for a particular subset, we may end up overfitting the
    model. **Overfitting** refers to a situation where we fine-tune a model to a dataset
    too much and it fails to perform well on unknown data. We want our machine learning
    model to perform well on unknown data. In this recipe, we will learn how to evaluate
    model accuracy using cross-validation metrics.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**是机器学习中的一个重要概念。在前一个菜谱中，我们将数据分割成训练和测试数据集。然而，为了使其更加稳健，我们需要用不同的子集重复这个过程。如果我们只为特定的子集微调，我们可能会过度拟合模型。**过度拟合**是指我们将模型过度微调到数据集上，以至于它在未知数据上的表现不佳。我们希望我们的机器学习模型在未知数据上表现良好。在这个菜谱中，我们将学习如何使用交叉验证指标来评估模型精度。'
- en: Getting ready…
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作…
- en: When we are dealing with machine learning models, we usually care about three
    things—precision, recall, and F1 score. We can get the required performance metric
    using parameter scoring. **Precision** refers to the number of items that are
    correctly classified as a percentage of the overall number of items in the list.
    **Recall** refers to the number of items that are retrieved as a percentage of
    the overall number of items in the training list.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理机器学习模型时，我们通常关注三个指标——精确度、召回率和F1分数。我们可以使用参数评分来获取所需性能指标。**精确度**是指正确分类的项目数占列表中所有项目总数的百分比。**召回率**是指检索到的项目数占训练列表中所有项目总数的百分比。
- en: How to do it…
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let''s see how to evaluate model accuracy using cross-validation metrics:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用交叉验证指标来评估模型精度：
- en: 'We will use the classifier just used in the *Building a Naive Bayes* classifier
    recipe (load the `naive_bayes.py` file). We will start with the `accuracy` measure:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will use the preceding function to compute `precision`, `recall`, and the
    `F1` score as well:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a test dataset containing 100 items, out of which 82 are of
    interest to us. Now, we want our classifier to identify these 82 items for us.
    Our classifier picks out 73 items as the items of interest. Out of these 73 items,
    only 65 are actually items of interest, and the remaining 8 are misclassified.
    We can compute precision in the following way:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The number of correct identifications = 65
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of identifications = 73
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision = 65 / 73 = 89.04%
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To compute recall, we use the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The total number of items of interest in the dataset = 82
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of items retrieved correctly = 65
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall = 65 / 82 = 79.26%
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good machine learning model needs to have good precision and good recall
    simultaneously. It''s easy to get one of them to 100%, but the other metric suffers!
    We need to keep both metrics high at the same time. To quantify this, we use an
    F1 score, which is a combination of precision and recall. This is actually the
    harmonic mean of precision and recall:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9aa3ca0-efcb-46da-a9ef-329b618d3b7e.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding case, the F1 score will be as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1af8b3e1-849e-432e-816b-5b30076ce4c4.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: There's more...
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In cross-validation, all available data is used, in groups of a fixed size,
    alternatively as a testing and as a training set. Therefore, each pattern is either
    classified (at least once) or used for training. The performances obtained depend,
    however, on the particular division. Therefore, it may be useful to repeat cross-validation
    several times in order to become independent of the particular division.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.model_selection.cross_val_score`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cross-validation* (from scikit-learn''s official documentation): [http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/cross_validation.html](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/cross_validation.html)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a confusion matrix
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **confusion matrix** is a table that we use to understand the performance
    of a classification model. This helps us understand how we classify testing data
    into different classes. When we want to fine-tune our algorithms, we need to understand
    how data gets misclassified before we make these changes. Some classes are worse
    than others, and the confusion matrix will help us understand this. Let''s look
    at the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1564065-1bf8-4576-ac73-5c69187fd71f.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see how we categorize data into different classes.
    Ideally, we want all the non-diagonal elements to be 0\. This would indicate perfect
    classification! Let's consider class 0\. Overall, 52 items actually belong to
    class 0\. We get 52 if we sum up the numbers in the first row. Now, 45 of these
    items are being predicted correctly, but our classifier says that 4 of them belong
    to class 1 and three of them belong to class 2\. We can apply the same analysis
    to the remaining 2 rows as well. An interesting thing to note is that 11 items
    from class 1 are misclassified as class 0\. This constitutes around 16% of the
    datapoints in this class. This is an insight that we can use to optimize our model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A confusion matrix identifies the nature of the classification errors, as our
    classification results are compared to real data. In this matrix, the diagonal
    cells show the number of cases that were correctly classified; all the others
    cells show the misclassified cases.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to visualize the confusion matrix:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `confusion_matrix.py` file that we already provided to you
    as a reference. Let''s see how to extract the confusion matrix from our data:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We use some sample data here. We have 4 classes with values ranging from 0 to
    3\. We have predicted labels as well. We use the `confusion_matrix` method to
    extract the confusion matrix and plot it.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and define this function:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We use the `imshow` function to plot the confusion matrix. Everything else in
    the function is straightforward! We just set the title, color bar, ticks, and
    the labels using the relevant functions. The `tick_marks` argument range from
    0 to 3 because we have 4 distinct labels in our dataset. The `np.arange` function
    gives us this `numpy` array.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the data (real and predicted) and then we will call the `confusion_matrix`
    function:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you run the preceding code, you will see the following:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a194b459-0515-4dde-9a5c-fa1fd33ecbe9.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: The diagonal colors are strong, and we want them to be strong. The black color
    indicates zero. There are a couple of gray squares in the non-diagonal spaces,
    which indicate misclassification. For example, when the real label is 0, the predicted
    label is 1, as we can see in the first row. In fact, all the misclassifications
    belong to class 1 in the sense that the second column contains 3 rows that are
    non-zero. It's easy to see this from the matrix.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A confusion matrix displays information about the actual and predicted classifications
    made by a model. The performance of such systems is evaluated with the help of
    data in the matrix.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the confusion matrix for a two-class classifier:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PREDICTED POSITIVE | PREDICTED NEGATIVE |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| **Actual TRUE** | TP | FN |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| **Actual FALSE** | FP | TN |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: 'The entries in the confusion matrix have the following meanings:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: TP is the number of correct predictions that an instance is positive
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FN is the number of incorrect predictions that an instance is negative
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP is the number of incorrect predictions that an instance is positive
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TN is the number of correct predictions that an instance is negative
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confusion matrix shows us the performance of an algorithm. Each row returns
    the instances in an actual class, while each column returns the instances in an
    expected class. The term *confusion matrix* results from the fact that it makes
    it easy to see whether the system is confusing two classes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.metrics.confusion_matrix()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Confusion Matrix*, University of Notre Dame: [https://www3.nd.edu/~busiforc/Confusion_Matrix.html](https://www3.nd.edu/~busiforc/Confusion_Matrix.html)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting a performance report
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Evaluating accuracy using cross-validation metrics* recipe, we calculated
    some metrics to measure the accuracy of the model. Let's remember its meaning.
    The accuracy returns the percentage of correct classifications. Precision returns
    the percentage of positive classifications that are correct. Recall (sensitivity)
    returns the percentage of positive elements of the testing set that have been
    classified as positive. Finally, in F1, both the precision and the recall are
    used to compute the score. In this recipe, we will learn how to extract a performance
    report.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also have a function in `scikit-learn` that can directly print the precision,
    recall, and F1 scores for us. Let's see how to do this.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to extract a performance report:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following lines to a new Python file (load the `performance_report.py`
    file):'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If you run this code, you will see the following on your Terminal:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7e6c43dc-441a-4ce6-a256-d1fa06905490.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Instead of computing these metrics separately, you can directly use the preceding
    function to extract those statistics from your model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we used the `classification_report ()` function of the scikit-learn
    library to extract a performance report. This function builds a text report showing
    the main classification metrics. A text summary of the precision, recall, and
    the F1 score for each class is returned. Referring to the terms introduced in
    the confusion matrix addressed in the previous recipe, these metrics are calculated
    as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The precision is the ratio tp / (tp + fp), where tp is the number of true positives
    and fp the number of false positives. The precision is the ability of the classifier
    to not label a sample that is negative as positive.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recall is the ratio tp / (tp + fn), where tp is the number of true positives
    and fn the number of false negatives. The recall is the ability of the classifier
    to find the positive samples.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F1 score is said to be a weighted harmonic mean of the precision and recall,
    where an F-beta score reaches its peak value at 1 and its lowest score at 0.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reported averages include the **micro average** (averaging the total true
    positives, false negatives, and false positives), the **macro average** (averaging
    the unweighted mean per label), the **weighted** **average** (averaging the support-weighted
    mean per label), and the **sample average** (only for multilabel classification).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.metrics.classification_report()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating cars based on their characteristics
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, let''s see how we can apply classification techniques to a
    real-world problem. We will use a dataset that contains some details about cars,
    such as number of doors, boot space, maintenance costs, and so on. Our goal is
    to determine the quality of the car. For the purposes of classification, quality
    can take four values: unacceptable, acceptable, good, or very good.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the dataset at [https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to treat each value in the dataset as a string. We consider six attributes
    in the dataset. Here are the attributes along with the possible values they can
    take:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '`buying`: These will be `vhigh`, `high`, `med`, and `low`.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maint`: These will be `vhigh`, `high`, `med`, and `low`.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doors`: These will be `2`, `3`, `4`, `5`, and `more`.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persons`: These will be `2`, `4`, and `more`.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lug_boot`: These will be `small`, `med`, and `big`.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety`: These will be `low`, `med`, and `high`.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that each line contains strings, we need to assume that all the features
    are strings and design a classifier. In the previous chapter, we used random forests
    to build a regressor. In this recipe, we will use random forests as a classifier.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to evaluate cars based on their characteristics:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `car.py` file that we already provided to you as reference.
    Let''s go ahead and import a couple of packages:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s load the dataset:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Each line contains a comma-separated list of words. Therefore, we parse the
    input file, split each line, and then append the list to the main data. We ignore
    the last character on each line because it's a newline character. Python packages
    only work with numerical data, so we need to transform these attributes into something
    that those packages will understand.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed label encoding. That is what we will
    use here to convert strings to numbers:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As each attribute can take a limited number of values, we can use the label
    encoder to transform them into numbers. We need to use different label encoders
    for each attribute. For example, the `lug_boot` attribute can take three distinct
    values, and we need a label encoder that knows how to encode this attribute. The
    last value on each line is the class, so we assign it to the *y* variable.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the classifier:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can play around with the `n_estimators` and `max_depth` parameters to see
    how they affect classification accuracy. We will actually do this soon in a standardized
    way.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform cross-validation:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once we train the classifier, we need to see how it performs. We use three-fold
    cross-validation to calculate the accuracy here. The following result is returned:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'One of the main goals of building a classifier is to use it on isolated and
    unknown data instances. Let''s use a single datapoint and see how we can use this
    classifier to categorize it:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The first step was to convert that data into numerical data. We need to use
    the label encoders that we used during training because we want it to be consistent.
    If there are unknown values in the input datapoint, the label encoder will complain
    because it doesn't know how to handle that data. For example, if you change the
    first value in the list from `high` to `abcd`, then the label encoder won't work
    because it doesn't know how to interpret this string. This acts like an error
    check to see whether the input datapoint is valid.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to predict the output class for this datapoint:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We use the `predict()` method to estimate the output class. If we output the
    encoded output label, it won''t mean anything to us. Therefore, we use the `inverse_transform`
    method to convert this label back to its original form and print out the output
    class. The following result is returned:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How it works...
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **random forest** was developed by Leo Breiman (University of California,
    Berkeley, USA) based on the use of classification trees. He has extended the classification
    tree technique by integrating it into a Monte Carlo simulation procedure and named
    it **random forest**. It is based on the creation of a large set of tree classifiers,
    each of which is proposed to classify a single instance, wherein some features
    have been evaluated. Comparing the classification proposals provided by each tree
    in the forest shows the class to which to attribute the request: it is the one
    that received the most votes.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Random forest has three adjustment parameters: the number of trees, the minimum
    amplitude of the terminal nodes, and the number of variables sampled in each node.
    The absence of overfitting makes the first two parameters important only from
    a computational point of view.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.ensemble.RandomForestClassifier()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Random Forests* by Leo Breiman and Adele Cutler (from the University of California,
    Berkeley): [https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting validation curves
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We used random forests to build a classifier in the previous recipe, *Evaluating
    cars based on their characteristics*, but we don''t exactly know how to define
    the parameters. In our case, we dealt with two parameters: `n_estimators` and
    `max_depth`. They are called **hyperparameters**, and the performance of the classifier
    depends on them. It would be nice to see how the performance gets affected as
    we change the hyperparameters. This is where validation curves come into the picture.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Validation curves help us understand how each hyperparameter influences the
    training score. Basically, all other parameters are kept constant and we vary
    the hyperparameter of interest according to our range. We will then be able to
    visualize how this affects the score.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to extract validation curves:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the same Python file as in the previous recipe, *Evaluating
    cars based on their characteristics*:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In this case, we defined the classifier by fixing the `max_depth` parameter.
    We want to estimate the optimal number of estimators to use, and so have defined
    our search space using `parameter_grid`. It is going to extract training and validation
    scores by iterating from 25 to 200 in 8 steps.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run it, you will see the following on your Terminal:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1d6021de-2c8e-4628-aeeb-5e64c53c3e5f.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot it:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here is what you''ll get:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c6c67e06-26df-495b-9a64-833b85df460e.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Let''s do the same for the `max_depth` parameter:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We fixed the `n_estimators` parameter at 20 to see how the performance varies
    with `max_depth`. Here is the output on the Terminal:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9cd9ace-c7e5-44aa-835e-8745adf5c9ef.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot it:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you run this code, you will get the following:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/359e4feb-d95f-438a-ab51-087000626f5d.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the `validation_curve` function of the scikit-learn
    library to plot the validation curve. This function determines training and test
    scores for varying parameter values and computes scores for an estimator with
    different values of a specified parameter.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing an estimator's hyperparameters is a fundamental procedure for setting
    up a model. Among the available procedures, grid search is one of the most used.
    This procedure selects the hyperparameter with the maximum score on a validation
    set or a multiple validation set.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.model_selection.validation_curve()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Validation curves: plotting scores to evaluate models* (from scikit-learn''s
    official documentation): [https://scikit-learn.org/stable/modules/learning_curve.html](https://scikit-learn.org/stable/modules/learning_curve.html)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting learning curves
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning curves help us understand how the size of our training dataset influences
    the machine learning model. This is very useful when you have to deal with computational
    constraints. Let's go ahead and plot learning curves by varying the size of our
    training dataset.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A learning curve shows the validation and training score of an estimator for
    varying numbers of training samples.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to extract learning curves:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the same Python file as in the previous recipe, *Extracting
    validation curves*:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We want to evaluate the performance metrics using training datasets of 200,
    500, 800, and 1,100 samples. We use five-fold cross-validation, as specified by
    the cv parameter in the `validation_curve` method.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this code, you will get the following output on the Terminal:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/47a5108a-7027-4eda-a025-d5a845516da1.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot it:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here is the output:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5e35fc89-5c17-435c-a647-8ce915bf76a3.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Although smaller training sets seem to give better accuracy, they are prone
    to overfitting. If we choose a bigger training dataset, it consumes more resources.
    Therefore, we need to make a trade-off here to pick the right size for the training
    dataset.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the `validation_curve` function of the scikit-learn
    library to plot the learning curve. This function determines cross-validated training
    and testing scores for different training set sizes.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A learning curve allows us to check whether the addition of training data leads
    to a benefit. It also allows us to estimate the contribution deriving from variance
    error and bias error. If the validation score and the training score converge
    with the size of the training set too low, we will not benefit from further training
    data.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.model_selection.validation_curve`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning curve* (from scikit-learn''s official documentation): [https://scikit-learn.org/stable/modules/learning_curve.html](https://scikit-learn.org/stable/modules/learning_curve.html)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the income bracket
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will build a classifier to estimate the income bracket of a person based
    on 14 attributes. The possible output classes are higher than 50,000 or lower
    than or equal to 50,000\. There is a slight twist in this dataset, in the sense
    that each datapoint is a mixture of numbers and strings. Numerical data is valuable,
    and we cannot use a label encoder in these situations. We need to design a system
    that can deal with numerical and non-numerical data at the same time.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the census income dataset available at [https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/census+income).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has the following characteristics:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of instances: 48,842'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of attributes: 14'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a list of attributes:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Age: continuous'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workclass: text'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fnlwgt: continuous'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Education: text'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Education-num: continuous'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marital-status: text'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Occupation: text'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Relationship: text'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Race: text'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sex: female or male'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Capital-gain: continuous'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Capital-loss: continuous'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hours-per-week: continuous'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Native-country: text'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to estimate the income bracket:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `income.py` file, already provided to you as a reference. We
    will use a Naive Bayes classifier to achieve this. Let''s import a couple of packages:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s load the dataset:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We will use 20,000 datapoints from the datasets—10,000 for each class to avoid
    class imbalance. During training, if you use many datapoints that belong to a
    single class, the classifier tends to get biased toward that class. Therefore,
    it''s better to use the same number of datapoints for each class:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: It's a comma-separated file again. We just loaded the data in the `X` variable
    just as before.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to convert string attributes to numerical data while leaving out the
    original numerical data:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `isdigit()` function helps us to identify numerical data. We converted string
    data to numerical data and stored all the label encoders in a list so that we
    can use it when we want to classify unknown data.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the classifier:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s split the data into training and testing to extract performance metrics:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s extract performance metrics:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following result is returned:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Let''s see how to classify a single datapoint. We need to convert the datapoint
    into something that our classifier can understand:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We are now ready to classify it:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Just as before, we use the `predict` method to get the `output` class and the
    `inverse_transform` method to convert this label back to its original form to
    print it out on the Terminal. The following result is returned:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The underlying principle of a Bayesian classifier is that some individuals belong
    to a class of interest with a given probability based on some observations. This
    probability is based on the assumption that the characteristics observed can be
    dependent or independent from one another; in the second case, the Bayesian classifier
    is called *naive* because it assumes that the presence or absence of a particular
    characteristic in a given class of interest is not related to the presence or
    absence of other characteristics, greatly simplifying the calculation. Let's go
    ahead and build a Naive Bayes classifier.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-422
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of Bayes applied to classification is very intuitive: if I look
    at a particular measurable feature, I can estimate the probability that this feature
    represents a certain class after the observation.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `sklearn.naive_bayes.GaussianNB` function: [https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the quality of wine
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will predict the quality of wine based on the chemical properties of
    wines grown. The code uses a wine dataset, which contains a DataFrame with 177
    rows and 13 columns; the first column contains the class labels. This data is
    obtained from the chemical analyses of wines grown in the same region in Italy
    (Piemonte) but derived from three different cultivars—namely, the Nebbiolo, Barberas,
    and Grignolino grapes. The wine from the Nebbiolo grape is called Barolo.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data consists of the amounts of several constituents found in each of the
    three types of wines, as well as some spectroscopic variables. The attributes
    are as follows:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Alcohol
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malic acid
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ash
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alcalinity of ash
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magnesium
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total phenols
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flavanoids
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonflavanoid phenols
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proanthocyanins
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Color intensity
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hue
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OD280/OD315 of diluted wines
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proline
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first column of the DataFrame contains the class which indicates one of
    three types of wine as (0, 1, or 2).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to predict the quality of wine:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `wine.quality.py` file, already provided to you as a reference. We
    start, as always, by importing the NumPy library and loading the data (`wine.txt`):'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Two arrays are returned: `X` (input data), and `y` (target).'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to separate our data into two groups: a training dataset and a testing dataset.
    The training dataset will be used to build the model, and the testing dataset
    will be used to see how this trained model performs on unknown data:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Four arrays are returned: `X_train`, `X_test`, `y_train`, and `y_test`. This
    data will be used to train and validate the model.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the classifier:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: To train the model, a decision tree algorithm has been used. A decision tree
    algorithm is based on a non-parametric supervised learning method used for classification
    and regression. The aim is to build a model that predicts the value of a target
    variable using decision rules inferred from the data features.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to the compute accuracy of the classifier:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The following result is returned:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, a confusion matrix will be calculated to compute the model performance:'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The following result is returned:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Values not present on the diagonals represent classification errors. So, only
    four errors were committed by the classifier.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, the quality of wine based on the chemical properties of wines
    grown was predicted. To do this, a decision tree algorithm was used. A decision
    tree shows graphically the choices made or proposed. It does not happen so often
    that things are so clear that the choice between two solutions is immediate. Often,
    a decision is determined by a series of cascading conditions. Representing this
    concept with tables and numbers is difficult. In fact, even if a table represents
    a phenomenon, it may confuse the reader because the justification for the choice
    is not obvious.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A tree structure allows us to extract the information with clear legibility
    by highlighting the branch we have inserted to determine the choice or evaluation.
    Decision tree technology is useful for identifying a strategy or pursuing a goal
    by creating a model with probable results. The decision tree graph immediately
    orients the reading of the result. A plot is much more eloquent than a table full
    of numbers. The human mind prefers to see a solution first and then go back to
    understand a justification of the solution, instead of a series of algebraic descriptions,
    percentages, and data to describe a result.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.tree.DecisionTreeClassifier()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decision Trees* (from the University of Hildesheim, Germany): [https://www.ismll.uni-hildesheim.de/lehre/ml-06w/skript/ml-4up-04-decisiontrees.pdf](https://www.ismll.uni-hildesheim.de/lehre/ml-06w/skript/ml-4up-04-decisiontrees.pdf)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decision Trees* (from scikit-learn''s official documentation): [https://scikit-learn.org/stable/modules/tree.html#tree](https://scikit-learn.org/stable/modules/tree.html#tree)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newsgroup trending topics classification
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Newsgroups are discussion groups on many issues and are made available by news-servers,
    located all over the world, which collect messages from clients and transmit them,
    on the one hand, to all their users and, on the other, to other news-servers connected
    to the network. The success of this technology is due to user interaction in discussions.
    Everyone has to respect the rules of the group.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-474
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will build a classifier that will allow us to classify the
    membership of a topic into a particular discussion group. This operation will
    be useful to verify whether the topic is relevant to the discussion group. We
    will use the data contained in the 20 newsgroups dataset, available at the following
    URL: [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/).'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a collection of about 20,000 newsgroup documents, divided into 20 different
    newsgroups. Originally collected by Ken Lang, and published in *Newsweeder paper:
    Learning to filter netnews,* the dataset is particularly useful for dealing with
    text classification problems.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-477
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to perform newsgroup trending topics classification:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `post.classification.py` file, already provided to you as a
    reference. We start importing the dataset as follows:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This dataset is contained in the `sklearn.datasets` library; in this way, it
    will be very easy for us to recover the data. As anticipated, the dataset contains
    posts related to 20 newsgroups. We will limit our analysis to only the following
    two newsgroups:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Download the data:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The data has two attributes: `data` and `target`. Obviously, `data` represents
    the input and `target` is the output. Let''s check which newsgroups have been
    selected:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The following results are printed:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s check the shape:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The following results are returned:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To extract features from texts, we will use the `CountVectorizer()` function
    as follows:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following result is returned:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In this way, we have made a count of the occurrences of words.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s divide the number of occurrences of each word in a document by the
    total number of words in the document:'
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now we can build the classifier:'
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, we will compute the accuracy of the classifier:'
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The following result is returned:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: How it works...
  id: totrans-506
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we built a classifier to classify the membership of a topic
    into a particular discussion group. To extract features from the text, a **tokenization**
    procedure was needed. In the tokenization phase, within each single sentence,
    atomic elements called **tokens** are identified; based on the token identified,
    it's possible to carry out an analysis and evaluation of the sentence itself. Once
    the characteristics of the text had been extracted, a classifier based on the
    multinomial Naive Bayes algorithm was constructed.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes multinomial algorithm is used for text and images when features
    represent the frequency of words (textual or visual) in a document.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the *Dataset loading utilities*: [https://scikit-learn.org/stable/datasets/index.html](https://scikit-learn.org/stable/datasets/index.html)'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.feature_extraction.text.CountVectorizer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.feature_extraction.text.TfidfTransformer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.naive_bayes.MultinomialNB()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
