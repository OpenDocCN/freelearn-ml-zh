- en: Facial Feature Tracking and Classification with dlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll learn about dlib and how to locate faces from images
    and videos with the help of some examples. We will also learn about facial recognition
    using dlib.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing dlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial landmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding 68 facial landmarks in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faces in videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing dlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: dlib is a general-purpose, cross-platform software library written in the programming
    language C++. We are going to learn dlib and understand how to find and use human
    facial features from images and videos. According to their own website, [dlib.net](http://dlib.net/),
    dlib is a modern C++ tool containing machine learning algorithms and tools for
    creating complex software in C++ to solve real-world problems. It is a C++ toolkit
    and, just like OpenCV, it contains a very nice set of Python bindings that will
    work very well for our applications.
  prefs: []
  type: TYPE_NORMAL
- en: dlib is a very rich library and contains a whole lot of algorithms and features,
    which are very well documented on their website. This makes it easy to learn from,
    and it has a whole lot of examples similar to what we're going to do in this chapter
    and for your customized projects. It is recommended that you check their website
    if you're interested in dlib and want to learn how to use it for your applications.
    The *High Quality Portable Code* section on the [http://dlib.net/](http://dlib.net/) website
    has efficient code for Microsoft Windows, Linux, and macOS, and just like Python,
    contains a very rich set of machine learning algorithms, including state-of-the-art
    deep learning, which we are using in this chapter, although we're going to use
    TensorFlow for our purposes. It also has **Support Vector Machines** (**SVMs**),
    which we saw in Chapter 5, *Handwritten Digit Recognition with scikit-learn and
    TensorFlow* on handwritten digit recognition, and a wide variety of other things
    for object detection and clustering, K-means, and so forth. It also has a rich
    set of numerical algorithms, linear algebra, **singular value decomposition**
    (**SVD**), and a whole lot optimization algorithms, as well as graphical model
    inference algorithms, and image processing (which is very useful for us). It has
    routines for reading and writing common image formats (although we won't use them
    as we're going to use the tools that we've already seen for reading and writing
    images) and **Speeded-Up Robust Features** (**SURF**), **Histogram of Oriented
    Gradient** (**HOG**), and FHOG, which are useful for image detection and recognition.
    What's interesting for now are the tools for detecting objects, including frontal
    face detection, pose estimation, and facial feature recognition. So, we'll talk
    about that in this chapter. There are some other features of dlib, such as threading,
    networking, **Graphical User Interface** (**GUI**) development, data compression,
    and a bunch of other utilities. [http://dlib.net/](http://dlib.net/) includes
    examples in C++ and examples in Python. What we're going to be interested in is
    face detection, facial landmark detection, and recognition. So, we're going to
    go through similar examples to see what we have here.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to learn all about facial landmarks in dlib. Before we can run any
    code, we need to grab some data that's used for facial features themselves. We'll
    see what these facial features are and exactly what details we're looking for.
    This is not included with Python dlib distributions, so you will have to download
    this. We'll go to the [dlib.net/files/](http://dlib.net/files) site, where you
    can see all the source code files; scroll to the bottom and you can see the `shape_predictor_68_face_landmarks.dat.bz2`
    file. Click on it and then save it wherever you keep your Jupyter Notebooks for
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so, what exactly is that? What are these 68 landmarks? Well, these landmarks
    are a common feature set that was generated by training alpha datasets from something
    called iBUG ([https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/](https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/)),
    the intelligent behavior understanding group. So, this is a pre-trained model,
    a database of a whole bunch of human faces of people from all over the world,
    male/female, different age groups, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we''ll work on a variety of cases, and what we''re looking for is a bunch
    of points around the outline of the face, as you can see in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03f332b3-640a-46e9-a413-1d8ef79dca42.png)'
  prefs: []
  type: TYPE_IMG
- en: Points **1** through **17** are the outline of the face, points **18** through
    **22** are the right eyebrow, **23** to **27** the left eyebrow, **28** to **31**
    the ridge of the nose, **30** to **36** the base of the nose, **37** to **42**
    forms the right eye, and **43** to **48** outlines the left eye, and then there
    are a whole bunch of points for the mouth, including both sides of the upper lip
    and both sides of the lower lip.
  prefs: []
  type: TYPE_NORMAL
- en: So, these are common features that all human faces will have, and this will
    allow us to do a whole lot of things like facial recognition and identification,
    pose estimation, possibly age estimation, gender estimation, and even neat things
    like facial stitching and facial blending. A lot of very interesting things can
    be done just with this information, and these are just based on pure intensity
    values on the face. So, there are no SURF features, **Scale Invariant Feature
    Transform** (**SIFT**) features, HOG features, or anything like that. These are
    just detectable from pixel values. So, effectively you can convert from RGB to
    black and white to monochrome, and you can run this model if it's an ensemble
    of regression trees.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the iBUG dataset and train your own model, and you can actually
    vary the number of features. There are datasets with more features than this,
    but this is more than adequate for our purposes. You can train it if you want
    to run it on a variety of faces or particular faces, but you'll find that this
    pre-trained dataset is going to work in a wide variety of cases. So, iBUG is powerful
    in and of itself. We're going to use it here, and we're going to see how to run
    the code that will find all these features for some images and for some videos.
    Then, we're going to apply that to the facial recognition problem, where we differentiate
    between faces in a given set. After you have downloaded the `shape_predictor_68_face_landmarks.dat.bz2`
    file, you can put the file in your directory where you have your Jupyter Notebook
    and with that, we can get started with our code.
  prefs: []
  type: TYPE_NORMAL
- en: Finding 68 facial landmarks in images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''re going to see our first example, where we find 68 facial
    landmarks and images with single people and with multiple people. So, let''s open
    our Jupyter Notebook for this section. Take a look at this first cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We've got to do some basic setup, as we did in the previous chapters. We're
    going to initialize `%pylab notebook`. Again, that will load NumPy and PyPlot
    and some other stuff, and we're going to perform `notebook` for now, which will
    be good for close-up views of images, though we're going to switch it to `inline`
    for the second example because we'll need that for looking at videos. Then, we
    have to import our other libraries. dlib is the focus of this section, of course.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to use a few utilities from OpenCV, but it's just additional annotation
    and working with videos. We're going to use `tkinter` so we have a nice file dialog
    display. So, rather than hardcoding the filename into our code, we'll just prompt
    the user for the file that we want to analyze. We'll import `display` from `IPython`
    in order to watch the movie for the second example, and we have to set up `tkinter`;
    we want to make sure that we're in the working directory with all our files. You
    might not need this, but you can do it just to be sure.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we''re going to select the cell, hit *Ctrl* + *Enter*, and then, if everything
    worked correctly, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8e2d5f1-8a71-4a80-9fa6-856e48514edb.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see `Populating the interactive namespace` and your current working
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so let''s see the first example now that we''re set up, and we''ll actually
    use those 68 features from that file that we downloaded; we''ll see how easy it
    is to do this within dlib. Now, we''re going to see that this is only just a little
    bit of code, but it does something really cool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we''re going to ask the user for the filename. So, this is using `tkinter`
    and we''re going to open a filename; it''ll start searching in the current working
    directory using the `initialdir=os.getcwd()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll read that in using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `img.flags['WRITEABLE']=True` line is kind of a quirk of dlib, not really
    a big deal but, depending on how you loaded the file, `flags` for a `WRITEABLE`
    might be set `False`. That happens with `imread`. It depends on how you load it,
    but just to be sure, `WRITEABLE` needs to be set to `True`. Otherwise, dlib will
    throw an error. Depending on how you load, this might not be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to create an image that we can write on, where we can actually display
    where the landmarks were found, so we''re going to create a copy of our image
    that we loaded earlier, the image that has the face in it, so we can write to
    it without clobbering the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will load the data from the file we downloaded. `shape_predictor_68_face_landmarks.dat.bz2` comes
    in `.bz2` format; if you have not already unzipped it, you can unzip it to the
    `.dat` format. If you're on Windows, then it is recommend to use 7-zip. If you're
    on Linux or macOS, there should be a built-in utility you can just double-click
    on and it should be pretty straightforward to extract that.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we''ll set the path and keep it in the current directory, and we need to
    initialize our objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, there are two stages here. First, you need to detect where the faces are.
    This is similar to what Haar cascades would do if you''ve used OpenCV and those
    examples before, but we use `dlib.get_frontal_face_detector`, which is just built-in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we create the `detector` object, get it from `dlib.get_frontal_face_detector`,
    initialize that, and then there''s the `predictor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Once we've detected where the face is, we know how many faces there are, and
    there can be more than one. dlib works fine for multiple faces, as we'll see.
    Once you know where the faces are, then you can run the `predictor`, which actually
    finds where those 68 landmarks previously mentioned are. So, we create our `detector`
    object and our `predictor` object, again making sure `predictor_path` is set up
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll set our `font` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`font` is just displaying landmark data on the annotated image. So, you can
    change that if you want. Okay, now we get to the fun part of the code. First,
    do the detection, and find where exactly the faces are. Here''s one really simple
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to just print out the number of faces detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This can be useful for debugging purposes, although we'll see the output image
    where it actually detected the faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to do a `for` loop here, and this will handle the case where
    we could have more than one face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: So, we're going to iterate over each one. The length of `dets` could be one,
    more than one, or zero, but we're not going to do that in this case. If you're
    not sure, then you might want to put this in a `try...catch` block, but we're
    only going to deal with images that have visible faces here.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we''ll iterate over the faces, and display where exactly the bounding box
    is for each face on the `Left`, `Top`, `Right`, and `Bottom`; where exactly did
    those go? Note the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is where the magic happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to find the shape, then we''re going to find those 68 landmarks,
    and just do a sanity check by printing out the first couple of landmarks just
    to make sure that it''s working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Okay, so, we have our landmarks for the face, and now we want to actually display
    it to understand what exactly we have here. We want to scale the `font` to make
    sure that fits the image because, depending on the size of the image, you could
    have a high resolution such as a 4,000 × 2,000 image, or you could have a low
    resolution such as a 300 × 200 (or something similar) and the heads could be very
    large in the image, as if the subject is close to the camera, or the reverse,
    small if it's far away.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we want to scale our `font` to the size of the head in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, here we're just computing `head_width`. `shape` is a predictor object that
    has a `part` method, and you pass in the index of the landmark that you want to
    find and each landmark is going to have an `x` and a `y` part. So, `head_width`
    is `16` here, which is dependent on your perspective. `head_width` is just width
    in terms of pixels of the head. Then, we're going to scale the font size based
    on `head_width`, and `650` is just a nice factor that works well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have all the data, we''re going to iterate over each of the points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we''ll hardcode `68`, because we know that we have `68` points, but if
    you were using another kind of shape finder, such as a pre-trained shape finder,
    then you might want to change this number. We iterate over the points and then
    we get the `x` and the `y` coordinates for each of the landmarks that were shown
    before. We extract the `x` and `y` coordinates using `shape.part` and update the
    annotated image. We need `cv2` to put the text into the image. dlib does have
    something similar to this, but `cv2` is better, and we can have just one interface
    for that anyway. So, we''re going to use OpenCV here and then we''re going to
    create a figure and display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'So, that''s all regarding the code, and hopefully that seems pretty straightforward
    to you. Read it at your leisure. When we execute the code, we can see a dialog
    box of stock photos. We can select any photo among those; for instance, here is
    the photo of a man wearing a hat. So, it''ll take just a little bit to compute
    that, and here you go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8131ab8a-d1ba-4a20-a662-b96e70e491fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We see this guy with all 68 points. We have labeled it from 0 to **67**, because
    of Python''s index from 0 convention, but we can see that, just like before, we
    have all of the points; so, you can see point 0 on the left side, point 16 on
    the right side, depending on your perspective, and then it continues all the way
    around. Here''s a zoomed view for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2c13edd-8383-479c-8956-52562790372c.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, some of the points are close together, but you can get the idea
    here of what's what. It looks pretty clear. So, this is pretty cool, and there's
    a whole lot you can do with this, as mentioned before. This guy is looking straight
    into the camera, so you might be asking what happens if somebody has their head
    tilted? Alright, we're going to run this again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s select the stock photo woman here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/234d6149-587a-4198-b328-933943ad6e4a.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see her head is turned, and yet this still works fine. It won't always
    work in extreme cases; if somebody's head is turned so much that landmarks aren't
    there, then this can fail for reasonable cases, you can see that this actually
    works very nicely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so what about multiple faces? Does this work for that? Let''s have a
    look at another group photo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98189d50-d116-461c-aee8-f01e2699f76d.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see we have six people here in various poses. Given the resolution here,
    it's not possible to read those annotations, but that's perfectly okay because
    you have seen where they are, and we can see that we actually detected all six
    faces very nicely. So, hopefully you can get some ideas here of how you can use
    this in your own code, and just how easy dlib makes it for you in terms of detection
    phases.
  prefs: []
  type: TYPE_NORMAL
- en: Faces in videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re going to see our second example from what we learned in the last section
    on faces in photos. The still image example was neat, but you might be asking
    about videos. Okay, let''s look at that for our next example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We change to `%pylab inline` because having all those widgets can actually cause
    a problem with Jupyter when you want to display a video sequence. We'll need the
    same code to get started with as shown in the previous example, and only replace
    `notebook` with `inline`. Then, we run the same code again.
  prefs: []
  type: TYPE_NORMAL
- en: 'After its execution, we move on with the next part. This is actually very close
    to the same thing because all you have to do is iterate over each frame, and it
    will work just the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you see this code is pretty much the same as the previous example. If you
    want, you can do this with your webcam. It''s actually pretty neat to watch. We''ll
    not be using a webcam here, but for your custom project, if you want to use a
    webcam you can add the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re assuming here that you only have one webcam. If you have more than one
    camera and you don''t want to use the first one, then you might need to change
    that `0` to something else. If you don''t want to use your webcam, add the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are not using a webcam. We want to create a figure that we''re going
    to display, and we''ll name it `100` to make sure it has its own unique ID. We''ll
    use the same `font` as in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It sounds really complicated, but it''s just an ordinary font. We''re going
    to create a `while` loop, which is going to go over each and every frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So, we have `cap` as our video capture object from OpenCV, and then all we have
    to do to read the frames is `cap.read()`. `ret` is just code that makes sure that
    we actually read a frame. Then, `img` is the actual image that is returned, and
    again make sure that the `WRITEABLE` flag is set, otherwise dlib could produce
    an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to try to find a face and, if the face is not found, then we''re
    going to release and break out of our loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You might not want this for your application, but one neat thing here is if
    you're using a webcam, an easy way to just stop this loop from running indefinitely
    is to just put your hand in front of the face. You put your hand in front of the
    camera, or turn your head, or whatever, and that will automatically stop it, hands-free.
    Otherwise, you can send a kernel interrupt and just make sure you do `cap.release()`,
    otherwise the video source will stay open and you might get an error later.
  prefs: []
  type: TYPE_NORMAL
- en: According to the preceding code block, we grab the image, detect the faces,
    and take the shape. For this code, we'll assume that there's only one face, but
    you can see from the previous example how to deal with multiple faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create the blank image or the image that''s a copy of the original,
    which we can write without distorting the original. Set the `head_width` and `fontsize`,
    and then just do exactly what we did before. Find the `x` and `y` points, and
    then write to them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to display our results, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note the color, `BGR2RGB`. That's because OpenCV uses **blue green red** (**BGR**)
    by default, and the colors will look really funny if you don't change that for
    the display. Then, there's some stuff here that will make sure that our window
    is updating while the script is still running. Otherwise, it will actually just
    run the entire script and you won't see what's happening in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then hit *Shift* + *Enter*. It might take a second to load and then it''ll
    run pretty slowly, largely because it''s part of Jupyter Notebook. You can take
    the code out and run it as an independent program, and probably you''ll want to
    create a `cv2` named window, but this will do for our purposes. When you execute
    the cell, you''ll see two women:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a43cb3c-24ab-4896-ba42-1162aeaa7572.png)'
  prefs: []
  type: TYPE_IMG
- en: One face is kind of obscured, so it's not going to detect her, but for the one
    who's in the foreground, as you can see, her face is being tracked pretty nicely
    and the landmarks are being found. This can work in real time depending on your
    hardware, and this isn't the kind of thing that you want to run in a Jupyter Notebook.
    You can watch this as long as you want to, but you get the idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, that''s how easy it is to work with video. Switch to the other woman in
    the background, and the first one''s face is turned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80e7597f-ba7f-449f-957a-977d6f6f3974.png)'
  prefs: []
  type: TYPE_IMG
- en: That's how easy it is to work with video, and you can detect multiple faces
    and do whatever you want with this information.
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to see how we can perform facial recognition with dlib with a relatively
    small amount of code. Facial recognition here means that we're going to look at
    an image and see whether or not this person is the same as the person in a different
    image. We're going to keep it simple here and just compare two faces to see whether
    they're the same, but this can easily be generalized, as we'll see later.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we're going to do something similar to the first example, where we're
    going to prompt the user to open two files, each with a face that is going to
    be compared to another. For this, we are going to use some faces from **Labeled
    Faces in the Wild** (**LFW**). It's a nice database that has thousands of faces
    from various celebrities. You can download the entire set from [http://vis-www.cs.umass.edu/lfw/](http://vis-www.cs.umass.edu/lfw/)
    and get a whole lot of examples that you can play with. So, we are just going
    to use a small subset of examples from the dataset to do our example here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We prompt a user to select two different facial images. We''re going to start
    the initial directory in the `faces` subdirectory of the project folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There are two additional files that you're going to need from [dlib.net/files](http://dlib.net/files),
    and they are the `shape_predictor_5_face_landmarks.dat` file and the `dlib_face_recognition_resnet_model_v1.dat`
    file. Again, they're going to be in `bz2` format. So, interestingly, we're only
    using five facial landmarks for this, but combined with the descriptors that's
    actually very adequate for describing a human face. Hence, we are not using 68
    face landmarks, but just 5\. We'll see just how nicely that works. Download those
    files and unzip `bz2`, just as we did in the first example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we set the path to the proper file locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `predictor` works similarly to the 68 face landmarks, but again a link comes
    up with five results, and we're going to use a pre-trained recognition model.
    It works on a variety of faces; you won't have to retrain it now. Here, we don't
    have to do any complicated deep learning modeling for this. There are ways to
    train your own models, but you'll see that this will actually work very nicely
    for a wide variety of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we create our `detector`, as before. That doesn''t require any additional
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to create our shape finder, similar to the previous example, and
    again we''re using the five facial landmarks detector. We''re going to create
    a new `facerec` object that comes from `dlib.face_recognition_model_v1`, passing
    in the path as `face_rec_model_path`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now, what `facerec` does is it takes a mapping, given our detected face and
    given the shapes and the location of where those landmarks are, and it's going
    to create a 128-length float vector, called a descriptor, that's going describe
    the face. So, it actually creates something that will be a description of a face,
    and is something that will capture the essence of a face. If you have the same
    person in two different pictures, where in one picture the person is far away
    from the camera and in another their face might be turned, it could be as many
    pictures and there could be different lighting conditions and so forth. The descriptor
    should be pretty much invariant to that. The descriptor is never exactly the same,
    but the same person should get a similar enough face descriptor, regardless of
    their orientation, the lighting conditions, and so forth. Even if they change
    their hair or they're wearing a hat, you should get a similar descriptor, and
    `facerec` actually does a good job of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code just performs the detection and the shape finding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''re going to perform the operation that we described previously: given
    the detection, spatial features, and the landmarks, we''re going to compute the
    128-point vector, and we can inspect it a little bit. Then, we''re going to look
    at the faces side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to know how similar the faces are, so we''re going to compute
    the Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: What that means is you take each point, 1 through 128, and you subtract the
    second one from the first one, you square each one, you sum them together, and
    take the square root, and that's going to give you a single number. That number
    is going to be used to determine whether or not these two images are of the same
    person's face.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s a magic number, `0.6`, which we''re going to use here, and which has
    been determined empirically to work very well. If the 128-dimensional distance
    is less than `0.6`, we say that these two images are of the same person. If it''s
    more than `0.6`, or equal to `0.6` as in this case, we''re going to say that these
    are different people. So, we look at the two images, compute all those metrics,
    and then we''re going to say if it is `<0.6`, the faces match, and if it is `>0.6`,
    the faces are different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run the code. You''ll see a dialog of celebrity photos from the
    LFW. We''ll pick one of Alec Baldwin and one of Sylvester Stallone:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc0a96d9-6198-4055-9983-9231eeee5be9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Baldwin and Sylvester Stallone are classified as two different people. That
    is exactly as we expected, as the faces are different. Now, let''s do it for another
    pair. Let''s compare Alec Baldwin to Alec Baldwin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8b1916d-cc5f-4b2b-8de4-fba8bed71416.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see that their faces match. Let''s do a few more comparisons
    for fun. So, Yao Ming and Winona Ryder look different from each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/befb8a98-2baf-4d94-a90f-d9b7f9e57314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we take two different pictures of Winona Ryder, and the faces match:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/677b9fbb-8d52-4b3a-bffb-d7eafceb104d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can do all kinds of combinations of this. Okay, so this is pretty easy.
    It might be useful to take a look at the facial descriptor; you can just hit *Shift*
    + *Tab,* and you can see what the vectors look like, which is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/520480df-9d83-4c50-b02f-2b4f8351549a.png)'
  prefs: []
  type: TYPE_IMG
- en: It's not very human-understandable, but is available just in case you're curious
    about it. That is enough to capture the essence of a human face, and just using
    a simple comparison, we can actually do a pretty good job of telling whether the
    two pictures are of the same face. This actually has a greater than 99% accuracy
    on the LFW dataset. So, you'll actually have a difficult time finding two faces
    that get bad results, whether two faces of the same person that are said not to
    match, or two of different people that are said to match.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if you wanted to adapt this to your own needs, what you can do is
    get your own database, just your own directory of faces of people that you want
    to recognize, and then when you have a new face, just go through each of the faces
    in your database. Just do a `for` loop and compare your new face to each one.
    For the Euclidean distance, computed here simply by using the NumPy linear algebra
    norm (`np.linalg.norm`), if that distance is less than 0.6, then you can say that
    you have a match. If you are concerned with false positives, you can have multiple
    faces of a person and compare to each one, then do a majority rule.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, suppose you have ten faces and you want to make sure that all ten
    of them match. If you really want to make sure that you did not get a false positive,
    you can just get ten really good images and then compare your new test image to
    all ten. But in any case, you can see from this example that it does not take
    a whole lot of code, and this method can be adapted to a wide variety of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had a brief introduction to the dlib library and learned
    how to use it for facial recognition. We then learned how to generate the outline
    for a face using the 68 facial landmarks pre-trained model. Later, we learned
    how to find the facial landmarks for a single person, multiple people, and for
    people in videos.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 7](ffda7469-1745-4a0a-8375-43426248af4d.xhtml), *Deep
    Learning Image Classification with TensorFlow*, we'll learn how to classify images
    with TensorFlow using a pre-trained model, and later we'll use our own custom
    images.
  prefs: []
  type: TYPE_NORMAL
