<html><head></head><body>
		<div id="_idContainer042">
			<h1 id="_idParaDest-58" class="chapter-number"><a id="_idTextAnchor182"/><st c="0">2</st></h1>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor183"/><a id="_idTextAnchor184"/><st c="2">Encoding Categorical Variables</st></h1>
			<p><strong class="bold"><st c="32">Categorical variables</st></strong><st c="54"> are those </st><a id="_idIndexMarker107"/><st c="65">whose values are selected from a group of categories or labels. </st><st c="129">For example, the </st><strong class="source-inline"><st c="146">Home owner</st></strong><st c="156"> variable with the values of </st><strong class="source-inline"><st c="185">owner</st></strong><st c="190"> and </st><strong class="source-inline"><st c="195">non-owner</st></strong><st c="204"> is categorical, and so is the </st><strong class="source-inline"><st c="235">Marital status</st></strong><st c="249"> variable with the values of </st><strong class="source-inline"><st c="278">never married</st></strong><st c="291">, </st><strong class="source-inline"><st c="293">married</st></strong><st c="300">, </st><strong class="source-inline"><st c="302">divorced</st></strong><st c="310">, and </st><strong class="source-inline"><st c="316">widowed</st></strong><st c="323">. In some categorical variables, the labels have an intrinsic order; for example, in the </st><strong class="source-inline"><st c="412">Student's grade</st></strong><st c="427"> variable, the values of </st><strong class="source-inline"><st c="452">A</st></strong><st c="453">, </st><strong class="source-inline"><st c="455">B</st></strong><st c="456">, </st><strong class="source-inline"><st c="458">C</st></strong><st c="459">, and </st><strong class="source-inline"><st c="465">Fail</st></strong><st c="469"> are ordered, with </st><strong class="source-inline"><st c="488">A</st></strong><st c="489"> being the highest grade and </st><strong class="source-inline"><st c="518">Fail</st></strong><st c="522"> being the lowest. </st><st c="541">These </st><a id="_idIndexMarker108"/><st c="547">are called </st><strong class="bold"><st c="558">ordinal categorical variables</st></strong><st c="587">. Variables in </st><a id="_idIndexMarker109"/><st c="602">which the categories do not have an intrinsic order </st><a id="_idIndexMarker110"/><st c="654">are called </st><strong class="bold"><st c="665">nominal categorical variables</st></strong><st c="694">, such</st><a id="_idIndexMarker111"/><st c="700"> as the </st><strong class="source-inline"><st c="708">City</st></strong><st c="712"> variable, with the values of </st><strong class="source-inline"><st c="742">London</st></strong><st c="748">, </st><strong class="source-inline"><st c="750">Manchester</st></strong><st c="760">, </st><strong class="source-inline"><st c="762">Bristol</st></strong><st c="769">, and </st><span class="No-Break"><st c="775">so on.</st></span></p>
			<p><st c="781">The values of categorical variables are often encoded as strings. </st><st c="848">To train most machine learning models, we need to transform those strings into numbers. </st><st c="936">The act of replacing strings </st><a id="_idTextAnchor185"/><st c="965">with numbers is </st><a id="_idIndexMarker112"/><st c="981">called </st><strong class="bold"><st c="988">categorical encoding</st></strong><st c="1008">. In this chapter, we will discuss multiple categorical </st><span class="No-Break"><st c="1064">encoding methods.</st></span></p>
			<p><st c="1081">This chapter will cover the </st><span class="No-Break"><st c="1110">following recipes:</st></span></p>
			<ul>
				<li><st c="1128">Creating binary variables through </st><span class="No-Break"><st c="1163">one-hot encoding</st></span></li>
				<li><st c="1179">Performing one-hot encoding of </st><span class="No-Break"><st c="1211">frequent categories</st></span></li>
				<li><st c="1230">Replacing categories with counts or the frequency </st><span class="No-Break"><st c="1281">of observations</st></span></li>
				<li><st c="1296">Replacing categories with </st><span class="No-Break"><st c="1323">ordinal numbers</st></span></li>
				<li><st c="1338">Performing ordinal encoding based on the </st><span class="No-Break"><st c="1380">target value</st></span></li>
				<li><st c="1392">Implementing target </st><span class="No-Break"><st c="1413">mean encoding</st></span></li>
				<li><st c="1426">Encoding with the Weight </st><span class="No-Break"><st c="1452">of Evidence</st></span></li>
				<li><st c="1463">Grouping rare or </st><span class="No-Break"><st c="1481">infrequent categories</st></span></li>
				<li><st c="1502">Performing </st><span class="No-Break"><st c="1514">binary en</st><a id="_idTextAnchor186"/><st c="1523">c</st><a id="_idTextAnchor187"/><a id="_idTextAnchor188"/><st c="1525">oding</st></span></li>
			</ul>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor189"/><st c="1530">Technical requirements</st></h1>
			<p><st c="1553">In this chapter, we will use the </st><strong class="source-inline"><st c="1587">Matplotlib</st></strong><st c="1597">, </st><strong class="source-inline"><st c="1599">pandas</st></strong><st c="1605">, </st><strong class="source-inline"><st c="1607">NumPy</st></strong><st c="1612">, </st><strong class="source-inline"><st c="1614">scikit-learn</st></strong><st c="1626">, </st><strong class="source-inline"><st c="1628">feature-engine</st></strong><st c="1642">, and Category Encoders Python libraries. </st><st c="1684">If you need to install Python, the free Anaconda Python distribution (</st><a href="https://www.anaconda.com/"><st c="1754">https://www.anaconda.com/</st></a><st c="1780">) includes most numerical </st><span class="No-Break"><st c="1807">computing libraries.</st></span></p>
			<p><strong class="source-inline"><st c="1827">feature-engine</st></strong><st c="1842"> can be installed </st><span class="No-Break"><st c="1860">with </st></span><span class="No-Break"><strong class="source-inline"><st c="1865">pip</st></strong></span><span class="No-Break"><st c="1868">:</st></span></p>
			<pre class="console"><st c="1870">
pip install feature-engine</st></pre>			<p><st c="1897">If you use Anaconda, you can install </st><strong class="source-inline"><st c="1935">feature-engine</st></strong> <span class="No-Break"><st c="1949">with </st></span><span class="No-Break"><strong class="source-inline"><st c="1955">conda</st></strong></span><span class="No-Break"><st c="1960">:</st></span></p>
			<pre class="console"><st c="1962">
conda install -c conda-forge feature_engine</st></pre>			<p><st c="2006">To install Category Encoders, use </st><strong class="source-inline"><st c="2041">pip</st></strong> <span class="No-Break"><st c="2044">as follows:</st></span></p>
			<pre class="console"><st c="2056">
pip install category_encoders</st></pre>			<p><st c="2086">We will use the </st><strong class="bold"><st c="2103">Credit Approval</st></strong><st c="2118"> dataset from the </st><em class="italic"><st c="2136">UCI Machine Learning Repository</st></em><st c="2167"> (</st><a href="https://archive.ics.uci.edu/"><st c="2169">https://archive.ics.uci.edu/</st></a><st c="2197">), licensed under the CC BY 4.0 creative commons attribution: </st><a href="https://creativecommons.org/licenses/by/4.0/legalcode"><st c="2260">https://creativecommons.org/licenses/by/4.0/legalcode</st></a><st c="2313">. You’ll find the dataset at this </st><span class="No-Break"><st c="2347">link: </st></span><a href="http://archive.ics.uci.edu/dataset/27/credit+approval"><span class="No-Break"><st c="2353">http://archive.ics.uci.edu/dataset/27/credit+approval</st></span></a><span class="No-Break"><st c="2406">.</st></span></p>
			<p><st c="2407">I downloaded and modified the data as shown in this </st><span class="No-Break"><st c="2460">notebook: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/credit-approval-dataset.ipynb"><span class="No-Break"><st c="2470">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/credit-approval-dataset.ipynb</st></span></a><span class="No-Break"><st c="2620">.</st></span></p>
			<p><st c="2621">You’ll find a copy of the modified data set in the accompanying GitHub </st><span class="No-Break"><st c="2693">repository: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/"><span class="No-Break"><st c="2705">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/</st></span></a><span class="No-Break"><st c="2826">.</st></span></p>
			<p class="callout-heading"><st c="2827">Note</st></p>
			<p class="callout"><st c="2832">Before encoding categorical variables, you might want to impute their missing data. </st><st c="2917">Check out the imputation methods for categorical variables in </st><a href="B22396_01.xhtml#_idTextAnchor020"><span class="No-Break"><em class="italic"><st c="2979">Chapter 1</st></em></span></a><st c="2988">, </st><em class="italic"><st c="2990">Imputing </st></em><span class="No-Break"><em class="italic"><st c="2999">Missing </st><a id="_idTextAnchor190"/><a id="_idTextAnchor191"/><st c="3007">Data</st></em></span><span class="No-Break"><st c="3011">.</st></span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor192"/><st c="3012">Creating binary variables through one-hot encoding</st></h1>
			<p><strong class="bold"><st c="3063">One-hot encoding</st></strong><st c="3080"> is a </st><a id="_idIndexMarker113"/><st c="3086">method used to represent categorical data, where each category is represented by a binary variable. </st><st c="3186">The </st><a id="_idTextAnchor193"/><st c="3190">binary variable</st><a id="_idIndexMarker114"/><st c="3205"> takes a value of </st><strong class="source-inline"><st c="3223">1</st></strong><st c="3224"> if the category is present, or </st><span class="No-Break"><strong class="source-inline"><st c="3256">0</st></strong></span><span class="No-Break"><st c="3257"> otherwise.</st></span></p>
			<p><st c="3268">The following</st><a id="_idIndexMarker115"/><st c="3282"> table shows the one-hot encoded representation of the </st><strong class="source-inline"><st c="3337">Smoker</st></strong><st c="3343"> variable with the categories of </st><strong class="source-inline"><st c="3376">Smoker</st></strong> <span class="No-Break"><st c="3382">and </st></span><span class="No-Break"><strong class="source-inline"><st c="3387">Non-Smoker</st></strong></span><span class="No-Break"><st c="3397">:</st></span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B22396_02_01.jpg" alt="Figure 2.1 – One-hot encoded representation of the Smoker variable"/><st c="3399"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="3475">Figure 2.1 – One-hot encoded representation of the Smoker variable</st></p>
			<p><st c="3541">As shown in </st><span class="No-Break"><em class="italic"><st c="3554">Figure 2</st></em></span><em class="italic"><st c="3562">.1</st></em><st c="3564">, from the </st><strong class="source-inline"><st c="3575">Smoker</st></strong><st c="3581"> variable, we can derive a binary variable for </st><strong class="source-inline"><st c="3628">Smoker</st><a id="_idTextAnchor194"/></strong><st c="3634">, which shows the value of </st><strong class="source-inline"><st c="3661">1</st></strong><st c="3662"> for smokers,</st><a id="_idTextAnchor195"/><st c="3675"> or the binary variable for </st><strong class="source-inline"><st c="3703">Non-Smoker</st></strong><st c="3713">, which takes the value of </st><strong class="source-inline"><st c="3740">1</st></strong><st c="3741"> for those who do </st><span class="No-Break"><st c="3759">not smoke.</st></span></p>
			<p><st c="3769">For the </st><strong class="source-inline"><st c="3778">Color</st></strong><st c="3783"> categorical variable with the values of </st><strong class="source-inline"><st c="3824">red</st></strong><st c="3827">, </st><strong class="source-inline"><st c="3829">blue</st></strong><st c="3833">, and </st><strong class="source-inline"><st c="3839">green</st></strong><st c="3844">, we can create three variables called </st><strong class="source-inline"><st c="3883">red</st></strong><st c="3886">, </st><strong class="source-inline"><st c="3888">blue</st></strong><st c="3892">, and </st><strong class="source-inline"><st c="3898">green</st></strong><st c="3903">. These variables will be assigned a value of </st><strong class="source-inline"><st c="3949">1</st></strong><st c="3950"> if the observation corresponds to the respective color, and </st><strong class="source-inline"><st c="4011">0</st></strong><st c="4012"> if it </st><span class="No-Break"><st c="4019">does not.</st></span></p>
			<p><st c="4028">A categorical variable with </st><em class="italic"><st c="4057">k</st></em><st c="4058"> unique categories can be encoded using </st><em class="italic"><st c="4098">k-1</st></em><st c="4101"> binary variables. </st><st c="4120">For </st><strong class="source-inline"><st c="4124">Smoker</st></strong><st c="4130">, </st><em class="italic"><st c="4132">k </st></em><st c="4134">is </st><em class="italic"><st c="4137">2</st></em><st c="4138"> as it contains two labels (</st><strong class="source-inline"><st c="4166">Smoker</st></strong><st c="4173"> and </st><strong class="source-inline"><st c="4178">Non-Smoker</st></strong><st c="4188">), so we only need one binary variable (</st><em class="italic"><st c="4229">k - 1 = 1</st></em><st c="4239">) to capture all the information. </st><st c="4274">For the </st><strong class="source-inline"><st c="4282">Color</st></strong><st c="4287"> variable, which has 3 categories (</st><em class="italic"><st c="4322">k = 3</st></em><st c="4328">; </st><strong class="source-inline"><st c="4331">red</st></strong><st c="4334">, </st><strong class="source-inline"><st c="4336">blue</st></strong><st c="4340">, and </st><strong class="source-inline"><st c="4346">green</st></strong><st c="4351">), we need 2 (</st><em class="italic"><st c="4366">k - 1 = 2</st></em><st c="4376">) binary </st><a id="_idIndexMarker116"/><st c="4386">variables to capture all the information so that the </st><span class="No-Break"><st c="4439">following occurs:</st></span></p>
			<ul>
				<li><st c="4456">If the observation is red, it will be captured by the </st><strong class="source-inline"><st c="4511">red</st></strong><st c="4514"> variable (</st><strong class="source-inline"><st c="4525">red</st></strong><st c="4529"> = </st><strong class="source-inline"><st c="4532">1</st></strong><st c="4533">, </st><strong class="source-inline"><st c="4535">blue</st></strong><st c="4539"> = </st><span class="No-Break"><strong class="source-inline"><st c="4542">0</st></strong></span><span class="No-Break"><st c="4543">)</st></span></li>
				<li><st c="4544">If the observation is blue, it will be captured by the </st><strong class="source-inline"><st c="4599">blue</st></strong><st c="4603"> variable (</st><strong class="source-inline"><st c="4614">red</st></strong><st c="4618"> = </st><strong class="source-inline"><st c="4621">0</st></strong><st c="4622">, </st><strong class="source-inline"><st c="4624">blue</st></strong><st c="4628"> = </st><span class="No-Break"><strong class="source-inline"><st c="4631">1</st></strong></span><span class="No-Break"><st c="4632">)</st></span></li>
				<li><st c="4633">If the observation is green, it will be captured by the combination of </st><strong class="source-inline"><st c="4704">red</st></strong><st c="4707"> and </st><strong class="source-inline"><st c="4712">blue</st></strong><st c="4716"> (</st><strong class="source-inline"><st c="4718">red</st></strong><st c="4721"> = </st><strong class="source-inline"><st c="4724">0</st></strong><st c="4725">, </st><strong class="source-inline"><st c="4727">blue</st></strong><st c="4731"> = </st><span class="No-Break"><strong class="source-inline"><st c="4734">0</st></strong></span><span class="No-Break"><st c="4735">)</st></span></li>
			</ul>
			<p><st c="4736">Encoding</st><a id="_idIndexMarker117"/><st c="4744"> into </st><em class="italic"><st c="4750">k-1</st></em><st c="4753"> binary variables is well suited for linear models. </st><st c="4805">There are a few occasions in which we may prefer to encode the categorical variables with </st><em class="italic"><st c="4895">k</st></em> <span class="No-Break"><st c="4896">binary variables:</st></span></p>
			<ul>
				<li><st c="4913">When training decision trees, since they do not evaluate the entire feature space at the </st><span class="No-Break"><st c="5003">same time</st></span></li>
				<li><st c="5012">When selecting </st><span class="No-Break"><st c="5028">features recursively</st></span></li>
				<li><st c="5048">When determining the importance of each category within </st><span class="No-Break"><st c="5105">a variable</st></span></li>
			</ul>
			<p><st c="5115">In this recipe, we will compare the one-hot encoding implementations of </st><strong class="source-inline"><st c="5188">pandas</st></strong><st c="5194">, </st><strong class="source-inline"><st c="5196">scikit-learn</st></strong><st c="5208">, </st><span class="No-Break"><st c="5210">and </st></span><span class="No-Break"><strong class="source-inline"><st c="5214">featur</st><a id="_idTextAnchor196"/><a id="_idTextAnchor197"/><st c="5220">e-engine</st></strong></span><span class="No-Break"><st c="5229">.</st></span></p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor198"/><st c="5230">How to do it...</st></h2>
			<p><st c="5246">Fir</st><a id="_idTextAnchor199"/><st c="5250">st, let’s make a few imports an</st><a id="_idTextAnchor200"/><st c="5282">d get the </st><span class="No-Break"><st c="5293">data ready:</st></span></p>
			<ol>
				<li><st c="5304">Import </st><strong class="source-inline"><st c="5312">pandas</st></strong><st c="5318"> and the </st><strong class="source-inline"><st c="5327">train_test_split</st></strong><st c="5343"> function </st><span class="No-Break"><st c="5353">from </st></span><span class="No-Break"><strong class="source-inline"><st c="5358">scikit-learn</st></strong></span><span class="No-Break"><st c="5370">:</st></span><pre class="source-code"><st c="5372">
import pandas as pd
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="5445">Let’s load the Credit </st><span class="No-Break"><st c="5468">Approval dataset:</st></span><pre class="source-code"><st c="5485">
data = pd.read_csv("credit_approval_uci.csv")</st></pre></li>				<li><st c="5531">Let’s separate the data into train and </st><span class="No-Break"><st c="5571">test sets:</st></span><pre class="source-code"><st c="5581">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="5721">Let’s </st><a id="_idIndexMarker118"/><st c="5728">inspect the unique categories</st><a id="_idIndexMarker119"/><st c="5757"> of the </st><span class="No-Break"><strong class="source-inline"><st c="5765">A4</st></strong></span><span class="No-Break"><st c="5767"> variable:</st></span><pre class="source-code"><st c="5777">
X_train["A4"].unique()</st></pre><p class="list-inset"><st c="5800">We can see the unique values of </st><strong class="source-inline"><st c="5833">A4</st></strong><st c="5835"> in the </st><span class="No-Break"><st c="5843">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="5860">array(['u', 'y', 'Missing', 'l'], dtype=obje</st><a id="_idTextAnchor201"/><st c="5905">ct)</st></strong></pre></li>				<li><st c="5909">Let’s encode </st><strong class="source-inline"><st c="5923">A4</st></strong><st c="5925"> into</st><a id="_idTextAnchor202"/> <em class="italic"><st c="5930">k-1</st></em><st c="5934"> binary variables using </st><strong class="source-inline"><st c="5958">pandas</st></strong><st c="5964"> and then inspect the first five rows of the </st><span class="No-Break"><st c="6009">resulting DataFrame:</st></span><pre class="source-code"><st c="6029">
dummies = pd.get_dummies(
    X_train["A4"], drop_first=True)
dummies.head()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="6102">Note</st></p>
			<p class="callout"><st c="6107">With </st><strong class="source-inline"><st c="6113">pandas</st></strong><st c="6119">’ </st><strong class="source-inline"><st c="6122">get_dummies()</st></strong><st c="6135">, we can either ignore or encode missing data through the </st><strong class="source-inline"><st c="6193">dummy_na</st></strong><st c="6201"> parameter. </st><st c="6213">By setting </st><strong class="source-inline"><st c="6224">dummy_na=True</st></strong><st c="6237">, missing data will be encoded in a new binary variable. </st><st c="6294">To encode the variable into </st><em class="italic"><st c="6322">k</st></em><st c="6323"> dummies, use </st><span class="No-Break"><strong class="source-inline"><st c="6337">drop_first=False</st></strong></span><span class="No-Break"><st c="6353"> instead.</st></span></p>
			<p class="list-inset"><st c="6362">Here, we can see the output of </st><em class="italic"><st c="6394">Step 5</st></em><st c="6400">, where each label is now a </st><span class="No-Break"><st c="6428">binary variable:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="6444">      Missing        l        u        y</st></strong>
<strong class="bold"><st c="6458">596     False</st></strong><strong class="bold"><st c="6468">  False   True  False</st></strong>
<strong class="bold"><st c="6485">303     False  False   True  False</st></strong>
<strong class="bold"><st c="6512">204     False  False  </st></strong><strong class="bold"><st c="6529">False   True</st></strong>
<strong class="bold"><st c="6539">351     False  False  False   True</st></strong>
<strong class="bold"><st c="6566">118     False  False   True  False</st></strong></pre>			<ol>
				<li value="6"><st c="6593">Now, let’s </st><a id="_idIndexMarker120"/><st c="6605">encode </st><a id="_idIndexMarker121"/><st c="6612">all the categorical variables into </st><span class="No-Break"><em class="italic"><st c="6647">k-1</st></em></span><span class="No-Break"><st c="6650"> binaries:</st></span><pre class="source-code"><st c="6660">
X_train_enc = pd.get_dummies(X_train, drop_first=True)
X_test_enc = pd.get_dummies(X_test, drop_first=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="6768">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="6773">pandas</st></strong><st c="6780">’ </st><strong class="source-inline"><st c="6783">get_dummies()</st></strong><st c="6796">will encode all variables of the object, string, or category type by default. </st><st c="6875">To encode a subset of the variables, pass the variable names in a list to the </st><span class="No-Break"><strong class="source-inline"><st c="6953">columns</st></strong></span><span class="No-Break"><st c="6960"> par</st><a id="_idTextAnchor203"/><st c="6964">ameter.</st></span></p>
			<ol>
				<li value="7"><st c="6972">Let’s inspect the first</st><a id="_idTextAnchor204"/><st c="6996"> five rows of the </st><span class="No-Break"><st c="7014">resulting DataFrame:</st></span><pre class="source-code"><st c="7034">
X_train_enc.head()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="7053">Note</st></p>
			<p class="callout"><st c="7058">When encoding more than one variable, </st><strong class="source-inline"><st c="7097">get_dummies()</st></strong><st c="7110"> captures the variable name – say, </st><strong class="source-inline"><st c="7145">A1</st></strong><st c="7147"> – and places an underscore followed by the category name to identify the resulting </st><span class="No-Break"><st c="7231">binary variables.</st></span></p>
			<p class="list-inset"><st c="7248">We can see the binary variables in the </st><span class="No-Break"><st c="7288">follo</st><a id="_idTextAnchor205"/><st c="7293">wing output:</st></span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B22396_02_02.jpg" alt="Figure 2.2 – A transformed DataFrame showing the numerical variables followed by the one-hot encoded representation of the categorical variables"/><st c="7306"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7967">Figure 2.2 – A transformed DataFrame showing the numerical variables followed by the one-hot encoded representation of the categorical variables</st></p>
			<p class="callout-heading"><st c="8111">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="8116">pandas</st></strong><st c="8123">’ </st><strong class="source-inline"><st c="8126">get_dummies()</st></strong><st c="8139">will create one binary variable per category seen in a DataFrame. </st><st c="8206">Hence, if there are more categories in the train set than in the test set, </st><strong class="source-inline"><st c="8281">get_dummies()</st></strong><st c="8294"> will return more columns in the transformed train set than in the transformed test set, and vice versa. </st><st c="8399">To avoid this, it is better to carry out one-hot encoding with </st><strong class="source-inline"><st c="8462">scikit-learn</st></strong> <span class="No-Break"><st c="8474">or </st></span><span class="No-Break"><strong class="source-inline"><st c="8478">feature-engine</st></strong></span><span class="No-Break"><st c="8492">.</st></span></p>
			<p class="list-inset"><st c="8493">Let’s do one-hot encoding using </st><span class="No-Break"><strong class="source-inline"><st c="8526">scikit-learn</st></strong></span><span class="No-Break"><st c="8538"> instead.</st></span></p>
			<ol>
				<li value="8"><st c="8547">Let’s </st><a id="_idIndexMarker122"/><st c="8554">import the encoder</st><a id="_idIndexMarker123"/><st c="8572"> and </st><strong class="source-inline"><st c="8577">ColumnTransformer</st></strong> <span class="No-Break"><st c="8594">from </st></span><span class="No-Break"><strong class="source-inline"><st c="8600">scikit-learn</st></strong></span><span class="No-Break"><st c="8612">:</st></span><pre class="source-code"><st c="8614">
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer</st></pre></li>				<li><st c="8708">Let’s create a list with the names of the </st><span class="No-Break"><st c="8751">categorical variables:</st></span><pre class="source-code"><st c="8773">
cat_vars = X_train.select_dtypes(
    include="O").columns.to_list()</st></pre></li>				<li><st c="8838">Let’s set up the encoder to create </st><em class="italic"><st c="8874">k-1</st></em> <span class="No-Break"><st c="8877">binary variables:</st></span><pre class="source-code"><st c="8895">
encoder = OneHotEncoder(drop="first",
    sparse_output=False)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="8954">Note</st></p>
			<p class="callout"><st c="8959">To encode variables into </st><em class="italic"><st c="8985">k</st></em><st c="8986"> dummies, set the </st><strong class="source-inline"><st c="9004">drop</st></strong><st c="9008"> parameter to </st><strong class="source-inline"><st c="9022">None</st></strong><st c="9026">. To encode only binary variables into </st><em class="italic"><st c="9065">k-1</st></em><st c="9068">, set the </st><strong class="source-inline"><st c="9078">drop</st></strong><st c="9082"> parameter to </st><strong class="source-inline"><st c="9096">if_binary</st></strong><st c="9105">. The latter is useful because encoding binary variables into </st><em class="italic"><st c="9167">k</st></em><st c="9168"> dummies </st><span class="No-Break"><st c="9177">is redundant.</st></span></p>
			<ol>
				<li value="11"><st c="9190">Let’s </st><a id="_idIndexMarker124"/><st c="9197">restrict the encoding to the </st><span class="No-Break"><st c="9226">categorical variables:</st></span><pre class="source-code"><st c="9248">
ct = ColumnTransformer(
    [("encoder", encoder, cat_vars)],
    remainder="passthrough",
    force_int_remainder_cols=False,
).set_output(transform="pandas")</st></pre></li>				<li><st c="9396">Let’s fit </st><a id="_idIndexMarker125"/><st c="9407">the encoder so that it identifies the categories </st><span class="No-Break"><st c="9456">to encode:</st></span><pre class="source-code"><st c="9466">
ct.fit(X_train)</st></pre></li>				<li><st c="9482">Let’s inspect the categories that will be represented with </st><span class="No-Break"><st c="9542">binary variables:</st></span><pre class="source-code"><st c="9559">
ct.named_transformers_["encoder"].categories_</st></pre><p class="list-inset"><st c="9605">The transformer will add binary variables for the </st><span class="No-Break"><st c="9656">follow</st><a id="_idTextAnchor206"/><st c="9662">ing categories:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B22396_02_03.jpg" alt="Figure 2.3 – Arrays with the categories that will be encoded into binary variables (one array per variable)"/><st c="9678"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="10132">Figure 2.3 – Arrays with the categories that will be encoded into binary variables (one array per variable)</st></p>
			<p class="callout-heading"><st c="10239">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="10244">scikit-learn</st></strong><st c="10257">’s </st><strong class="source-inline"><st c="10261">OneHotEncoder()</st></strong><st c="10276"> will only encode the categories learned from the train set. </st><st c="10337">If there are new categories in the test set, we can instruct the encoder to ignore them, return an error, or replace them with an infrequent category, by setting the </st><strong class="source-inline"><st c="10503">handle_unknown</st></strong><st c="10517"> parameter to </st><strong class="source-inline"><st c="10531">ignore</st></strong><st c="10537">, </st><strong class="source-inline"><st c="10539">error</st></strong><st c="10544">, </st><span class="No-Break"><st c="10546">or </st></span><span class="No-Break"><strong class="source-inline"><st c="10549">infrequent_if</st><a id="_idTextAnchor207"/><st c="10562">_exists</st></strong></span><span class="No-Break"><st c="10570">.</st></span></p>
			<ol>
				<li value="14"><st c="10571">Let’s </st><a id="_idIndexMarker126"/><st c="10578">encode the </st><span class="No-Break"><st c="10589">categorical variables:</st></span><pre class="source-code"><st c="10611">
X_train_enc = ct.transform(X_train)
X_test_enc = ct.transform(X_test)</st></pre><p class="list-inset"><st c="10681">Make sure to inspect the result by </st><span class="No-Break"><st c="10717">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="10727">X_test_enc.head()</st></strong></span><span class="No-Break"><st c="10744">.</st></span></p></li>				<li><st c="10745">To get</st><a id="_idIndexMarker127"/><st c="10752"> familiar with the output, let’s print the variable names of the </st><span class="No-Break"><st c="10817">resulting DataFrame:</st></span><pre class="source-code"><st c="10837">
ct.get_feature_names_out()</st></pre><p class="list-inset"><st c="10864">In the following image, we see the names of the variables in the </st><span class="No-Break"><st c="10930">transf</st><a id="_idTextAnchor208"/><st c="10936">ormed DataFrame:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B22396_02_04.jpg" alt="Figure 2.4 – Arrays with the names of the variables in the resulting DataFrame"/><st c="10953"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11696">Figure 2.4 – Arrays with the names of the variables in the resulting DataFrame</st></p>
			<p class="callout-heading"><st c="11774">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="11779">ColumnTransformer()</st></strong><st c="11799"> changes the name and order of the variables during the transformation. </st><st c="11871">If the variable was encoded, it will append the </st><strong class="source-inline"><st c="11919">encoder</st></strong><st c="11926"> prefix and if the variable was not modified, it will append the </st><span class="No-Break"><strong class="source-inline"><st c="11991">remainder</st></strong></span><span class="No-Break"><st c="12000"> prefix.</st></span></p>
			<p class="list-inset"><st c="12008">To wrap up the recipe, let’s perform one-hot encoding </st><span class="No-Break"><st c="12063">with </st></span><span class="No-Break"><strong class="source-inline"><st c="12068">featu</st><a id="_idTextAnchor209"/><st c="12073">re-engine</st></strong></span><span class="No-Break"><st c="12083">.</st></span></p>
			<ol>
				<li value="16"><st c="12084">Let’s</st><a id="_idIndexMarker128"/><st c="12090"> import </st><a id="_idIndexMarker129"/><st c="12098">the encoder </st><span class="No-Break"><st c="12110">from </st></span><span class="No-Break"><strong class="source-inline"><st c="12115">f</st></strong></span><span class="No-Break"><strong class="source-inline"><st c="12116">eature-engine</st></strong></span><span class="No-Break"><st c="12129">:</st></span><pre class="source-code"><st c="12131">
from feature_engine.encoding import O</st><a id="_idTextAnchor210"/><st c="12169">neHotEncoder</st></pre></li>				<li><st c="12182">Let’s set up the encoder so that it returns </st><em class="italic"><st c="12227">k-1</st></em> <span class="No-Break"><st c="12230">binary variables:</st></span><pre class="source-code"><st c="12248">
ohe_enc = OneHotEncoder(drop_last=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="12288">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="12293">feature-engine</st></strong><st c="12308">’s </st><strong class="source-inline"><st c="12312">OneHotEncoder()</st></strong><st c="12327"> encodes all categorical variables by default. </st><st c="12374">To encode a subset of the variables, pass the variable names in a list: </st><strong class="source-inline"><st c="12446">OneHotEncoder(variables=["A1", "A4"]</st></strong><st c="12482">). </st><st c="12486">To encode numerical variables, set the </st><strong class="source-inline"><st c="12525">ignore_format</st></strong><st c="12538"> parameter to </st><strong class="source-inline"><st c="12552">True</st></strong><st c="12556"> or cast the variables </st><span class="No-Break"><st c="12579">as objects.</st></span></p>
			<ol>
				<li value="18"><st c="12590">Let’s fit the encoder to the train set so that it learns the categories and variables </st><span class="No-Break"><st c="12677">to encode:</st></span><pre class="source-code"><st c="12687">
ohe_enc.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="12708">Note</st></p>
			<p class="callout"><st c="12713">To encode binary variables into </st><em class="italic"><st c="12746">k-1</st></em><st c="12749">, and other categorical variables into </st><em class="italic"><st c="12788">k</st></em><st c="12789"> dummies, set the </st><strong class="source-inline"><st c="12807">drop_last_binary</st></strong><st c="12823"> parameter </st><span class="No-Break"><st c="12834">to </st></span><span class="No-Break"><strong class="source-inline"><st c="12837">True</st></strong></span><span class="No-Break"><st c="12841">.</st></span></p>
			<ol>
				<li value="19"><st c="12842">Let’s explore the variables that will </st><span class="No-Break"><st c="12881">be encoded:</st></span><pre class="source-code"><st c="12892">
ohe_enc.variables_</st></pre><p class="list-inset"><st c="12911">The transformer found and stored the variables of the object or categorical type, as shown in the </st><span class="No-Break"><st c="13010">following output:</st></span></p><pre class="source-code"><st c="13027">['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A</st><a id="_idTextAnchor211"/><st c="13081">13']</st></pre></li>				<li><st c="13086">Let’s</st><a id="_idIndexMarker130"/><st c="13092"> explore the c</st><a id="_idTextAnchor212"/><st c="13106">ategories for which dummy variables will </st><span class="No-Break"><st c="13148">be created:</st></span><pre class="source-code"><st c="13159">
ohe_enc.encoder_dict_</st></pre><p class="list-inset"><st c="13181">The</st><a id="_idIndexMarker131"/><st c="13185"> following dictionary contains the categories that will be encoded in </st><span class="No-Break"><st c="13255">each variable:</st></span></p><pre class="source-code"><strong class="bold"><st c="13269"> {'A1': ['a', 'b'],</st></strong>
<strong class="bold"><st c="13288"> 'A4': ['u', 'y', 'Missing'],</st></strong>
<strong class="bold"><st c="13317"> 'A5': ['g', 'p', 'Missing'],</st></strong>
<strong class="bold"><st c="13346"> 'A6': ['c', 'q', 'w', 'ff', 'm', 'i', 'e', 'cc', 'x', 'd', 'k', 'j', 'Missing', 'aa'],</st></strong>
<strong class="bold"><st c="13433"> 'A7': ['v', 'ff', 'h', 'dd', 'z', 'bb', 'j', 'Missing', 'n'],</st></strong>
<strong class="bold"><st c="13495"> 'A9': ['t'],</st></strong>
<strong class="bold"><st c="13508"> 'A10': ['t'],</st></strong>
<strong class="bold"><st c="13522"> 'A12': ['t'],</st></strong>
<strong class="bold"><st c="13536"> 'A13': ['g', 's']}</st></strong></pre></li>				<li><st c="13555">Let’s encode the categorical variables in train and </st><span class="No-Break"><st c="13608">test sets:</st></span><pre class="source-code"><st c="13618">
X_train_enc = ohe_enc.transform(X_train)
X_test_enc = ohe_enc.transform(X_test)</st></pre><p class="list-inset"><st c="13698">If we execute </st><strong class="source-inline"><st c="13713">X_train_enc.head()</st></strong><st c="13731">, we will see the </st><span class="No-Break"><st c="13749">f</st><a id="_idTextAnchor213"/><st c="13750">ollowing DataFrame:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B22396_02_05.jpg" alt="Figure 2.5 – Transformed DataFrame with numerical variables followed by the one-hot encoded representation of the categorical variables"/><st c="13769"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="14176">Figure 2.5 – Transformed DataFrame with numerical variables followed by the one-hot encoded representation of the categorical variables</st></p>
			<p class="list-inset"><st c="14311">Note </st><a id="_idIndexMarker132"/><st c="14317">how the </st><strong class="source-inline"><st c="14325">A4</st></strong><st c="14327"> cate</st><a id="_idTextAnchor214"/><st c="14332">gorical </st><a id="_idIndexMarker133"/><st c="14341">variable was replaced with </st><strong class="source-inline"><st c="14368">A4_u</st></strong><st c="14372">, </st><strong class="source-inline"><st c="14374">A4_y</st></strong><st c="14378">, and </st><span class="No-Break"><st c="14384">so on.</st></span></p>
			<p class="callout-heading"><st c="14390">Note</st></p>
			<p class="callout"><st c="14395">We can get the names of all the variables in the transformed dataset by </st><span class="No-Break"><st c="14468">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="14478">ohe_enc.get</st><a id="_idTextAnchor215"/><a id="_idTextAnchor216"/><st c="14489">_feature_names_out()</st></strong></span><span class="No-Break"><st c="14510">.</st></span></p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor217"/><st c="14511">How it works...</st></h2>
			<p><st c="14527">In this recipe, we performed a one-hot encoding of categorical variables using </st><strong class="source-inline"><st c="14607">pandas</st></strong><st c="14613">, </st><strong class="source-inline"><st c="14615">scikit-learn</st></strong><st c="14627">, </st><span class="No-Break"><st c="14629">and </st></span><span class="No-Break"><strong class="source-inline"><st c="14633">feature-engine</st></strong></span><span class="No-Break"><st c="14647">.</st></span></p>
			<p><strong class="source-inline"><st c="14648">pandas</st></strong><st c="14655">’ </st><strong class="source-inline"><st c="14658">get_dummies()</st></strong><st c="14671"> replaced the categorical variables with a set of binary variables representing each of the categories. </st><st c="14775">When used on the entire dataset, it returned the numerical variables, followed by the one-hot encoded representation of each seen category in every variable of type object, string, </st><span class="No-Break"><st c="14956">or categorical.</st></span></p>
			<p class="callout-heading"><st c="14971">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="14976">pandas</st></strong><st c="14983"> will return binary variables for every category seen in a dataset. </st><st c="15051">In practice, to avoid data leakage and anticipate deployment eventualities, we want to return dummy variables for categories seen in a training set only. </st><st c="15205">So, it is safer to use </st><strong class="source-inline"><st c="15228">scikit-learn</st></strong> <span class="No-Break"><st c="15240">and </st></span><span class="No-Break"><strong class="source-inline"><st c="15245">feature-engine</st></strong></span><span class="No-Break"><st c="15259">.</st></span></p>
			<p><strong class="source-inline"><st c="15260">OneHotEncoder()</st></strong><st c="15276"> from </st><strong class="source-inline"><st c="15282">scikit-learn</st></strong><st c="15294"> or </st><strong class="source-inline"><st c="15298">feature-engine</st></strong><st c="15312"> learned the categories that should be represented by binary variables from the train set when we applied </st><strong class="source-inline"><st c="15418">fit()</st></strong><st c="15423">. With </st><strong class="source-inline"><st c="15430">transform()</st></strong><st c="15441">, </st><strong class="source-inline"><st c="15443">scikit-learn</st></strong><st c="15455"> returned just the binary variables, whereas </st><strong class="source-inline"><st c="15500">feature-engine</st></strong><st c="15514"> returned the numerical variables followed by the one-hot encoded representation of the </st><span class="No-Break"><st c="15602">categorical ones.</st></span></p>
			<p><strong class="source-inline"><st c="15619">scikit-learn</st></strong><st c="15632">’s </st><strong class="source-inline"><st c="15636">OneHotEncoder()</st></strong><st c="15651"> encodes all variables by default. </st><st c="15686">To restrict the encoding to </st><a id="_idIndexMarker134"/><st c="15714">categorical variables, we </st><a id="_idIndexMarker135"/><st c="15740">used </st><strong class="source-inline"><st c="15745">ColumnTransformer()</st></strong><st c="15764">. We set the output of </st><strong class="source-inline"><st c="15787">transform()</st></strong><st c="15798">to </st><strong class="source-inline"><st c="15802">pandas</st></strong><st c="15808"> to obtain the resulting data as </st><span class="No-Break"><st c="15841">a DataFrame.</st></span></p>
			<p class="callout-heading"><st c="15853">Note</st></p>
			<p class="callout"><st c="15858">One-hot encoding is suitable for linear models. </st><st c="15907">It also expands the feature space. </st><st c="15942">If your dataset contains many categorical variables or highly cardinal variables, you can restrict the number of binary variables by encoding the most frequent categories only. </st><st c="16119">You can do this automatically with both </st><strong class="source-inline"><st c="16159">scikit-learn</st></strong><st c="16171"> and </st><strong class="source-inline"><st c="16176">feature-engine</st></strong><st c="16190"> as we describe in the </st><em class="italic"><st c="16213">Performing one-hot encoding of freq</st><a id="_idTextAnchor218"/><a id="_idTextAnchor219"/><st c="16248">uent </st></em><span class="No-Break"><em class="italic"><st c="16254">categories</st></em></span><span class="No-Break"><st c="16264"> recipe.</st></span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor220"/><st c="16272">There’s more...</st></h2>
			<p><st c="16288">We can also perform one-hot encoding using the Category Encoders Python </st><span class="No-Break"><st c="16361">library: </st></span><a href="https://contrib.scikit-learn.org/category_encoders/onehot.html"><span class="No-Break"><st c="16370">https://</st></span><span class="No-Break"><st c="16378">contrib.scikit-learn.org/category_encoders/onehot.html</st></span></a><span class="No-Break"><st c="16433">.</st></span></p>
			<p><st c="16434">To limit the number of binary variables, we can choose which categories to encode and which to ignore; check out a Python demo in the following </st><span class="No-Break"><st c="16579">article: </st></span><span class="No-Break"><st c="16588">https://www.blog.trainindata.com/one-hot-encoding-</st><a id="_idTextAnchor221"/><a id="_idTextAnchor222"/><st c="16638">categorical-variables</st></span><span class="No-Break"><span class="P---URL"><st c="16660">/</st></span></span><span class="No-Break"><st c="16662">.</st></span></p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor223"/><st c="16663">Performing one-hot encoding of frequent categories</st></h1>
			<p><st c="16714">One-hot encoding </st><a id="_idIndexMarker136"/><st c="16732">represents each variable’s category with a binary variable.</st><a id="_idTextAnchor224"/><st c="16791"> Hence, one-hot encoding of highly cardinal variables or datasets with multiple categorical features can expand the feature space dramatically. </st><st c="16935">This, in turn, may increase the computational cost of using machine learning models or deteriorate their performance. </st><st c="17053">To reduce the number of binary variables, we can perform one-hot encoding of the most frequent categories. </st><st c="17160">One-hot encoding </st><a id="_idIndexMarker137"/><st c="17177">the top categories is equivalent to treating the remaining, less frequent categories as a single, </st><span class="No-Break"><st c="17275">unique category.</st></span></p>
			<p><st c="17291">In this recipe, we will implement one-hot encoding of the most popular categories using </st><strong class="source-inline"><st c="17380">pandas</st></strong><st c="17386">, </st><strong class="source-inline"><st c="17388">Scikit-l</st><a id="_idTextAnchor225"/><a id="_idTextAnchor226"/><st c="17396">earn</st></strong><st c="17401">, </st><span class="No-Break"><st c="17403">and </st></span><span class="No-Break"><strong class="source-inline"><st c="17407">feature-engine</st></strong></span><span class="No-Break"><st c="17421">.</st></span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor227"/><st c="17422">How to do it...</st></h2>
			<p><st c="17438">First, let’s import </st><a id="_idIndexMarker138"/><st c="17459">the necessary </st><a id="_idIndexMarker139"/><st c="17473">Python libraries and get the </st><span class="No-Break"><st c="17502">dataset ready:</st></span></p>
			<ol>
				<li><st c="17516">Import the required Python libraries, functions, </st><span class="No-Break"><st c="17566">and classes:</st></span><pre class="source-code"><st c="17578">
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="17670">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="17739">test sets:</st></span><pre class="source-code"><st c="17749">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="17935">Note</st></p>
			<p class="callout"><st c="17940">The most frequent categories need to be determined in the train set. </st><st c="18010">This is to avoid </st><span class="No-Break"><st c="18027">data leakage.</st></span></p>
			<ol>
				<li value="3"><st c="18040">Let’s inspect the unique categories of the </st><span class="No-Break"><strong class="source-inline"><st c="18084">A6</st></strong></span><span class="No-Break"><st c="18086"> variable:</st></span><pre class="source-code"><st c="18096">
X_train["A6"].unique()</st></pre><p class="list-inset"><st c="18119">The unique values of </st><strong class="source-inline"><st c="18141">A6</st></strong><st c="18143"> are displayed in the </st><span class="No-Break"><st c="18165">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="18182">array(['c', 'q', 'w', 'ff', 'm', 'i', 'e', 'cc', 'x', 'd', 'k', 'j', 'Missing, 'aa', 'r']</st><a id="_idTextAnchor228"/><st c="18272">, dtype=object)</st></strong></pre></li>				<li><st c="18287">Let’s count </st><a id="_idIndexMarker140"/><st c="18300">the number of observations per category of </st><strong class="source-inline"><st c="18343">A6</st></strong><st c="18345">, sort them in decreasing order, and then display the five most </st><span class="No-Break"><st c="18409">frequent categories:</st></span><pre class="source-code"><st c="18429">
X_train["A6"].value_counts().sort_values(
    ascending=False).head(5)</st></pre><p class="list-inset"><st c="18496">We can</st><a id="_idIndexMarker141"/><st c="18503"> see the five most frequent categories and the number of observations per category in the </st><span class="No-Break"><st c="18593">following output:</st></span></p><pre class="source-code"><strong class="bold"><st c="18610">A6</st></strong>
<strong class="bold"><st c="18613">c      93</st></strong>
<strong class="bold"><st c="18618">q      56</st></strong>
<strong class="bold"><st c="18623">w      48</st></strong>
<strong class="bold"><st c="18628">i      41</st></strong>
<strong class="bold"><st c="18633">ff     38</st></strong>
<strong class="bold"><st c="18639">Name: count, dtype: int64</st></strong></pre></li>				<li><st c="18665">Now, let’s capture the most frequent categories of </st><strong class="source-inline"><st c="18717">A6</st></strong><st c="18719"> in a list by using the code in </st><em class="italic"><st c="18751">Step 4</st></em><st c="18757"> inside a </st><span class="No-Break"><st c="18767">list comprehension:</st></span><pre class="source-code"><st c="18786">
top_5 = [x for x in X_train[
</st><strong class="bold"/><st c="18816">" A6"].value_counts().sort_values(
</st><strong class="bold"/><st c="18851">ascending=False).head(5).index
]</st></pre></li>				<li><st c="18883">Let’s add a binary variable per top category to a copy of the train and </st><span class="No-Break"><st c="18956">test sets:</st></span><pre class="source-code"><st c="18966">
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()
for label in top_5:
    X_train_enc[f"A6_{label}"] = np.where(
        X_train["A6"] == label, 1, 0)
    X_test_enc[f"A6_{label}"] = np.where(
        X_test["A6"] </st><a id="_idTextAnchor229"/><st c="19163">== label, 1, 0)</st></pre></li>				<li><st c="19178">Let’s display</st><a id="_idIndexMarker142"/><st c="19192"> the top </st><strong class="source-inline"><st c="19201">10</st></strong><st c="19203"> rows </st><a id="_idIndexMarker143"/><st c="19209">of the original and encoded variable, </st><strong class="source-inline"><st c="19247">A6</st></strong><st c="19249">, in the </st><span class="No-Break"><st c="19258">train set:</st></span><pre class="source-code"><st c="19268">
X_train_enc[["A6"] + [f"A6_{
    label}" for label in top_5]].head(10)</st></pre><p class="list-inset"><st c="19335">In the output of </st><em class="italic"><st c="19353">Step 7</st></em><st c="19359">, we can see the </st><strong class="source-inline"><st c="19376">A6</st></strong><st c="19378"> variable, followed by the </st><span class="No-Break"><st c="19405">binary variables:</st></span></p><pre class="source-code"><strong class="bold"><st c="19422">      A6  A6_c  A6_q  </st></strong><strong class="bold"><st c="19436">A6_w  A6_i  A6_ff</st></strong>
<strong class="bold"><st c="19451">596   c      1      0      0      0        0</st></strong>
<strong class="bold"><st c="19467">303   q      </st></strong><strong class="bold"><st c="19474">0      1      0      0        0</st></strong>
<strong class="bold"><st c="19483">204   w      0      0      1      0        </st></strong><strong class="bold"><st c="19498">0</st></strong>
<strong class="bold"><st c="19499">351  ff      0      0      0      0        1</st></strong>
<strong class="bold"><st c="19515">118   m      0      </st></strong><strong class="bold"><st c="19524">0      0      0        0</st></strong>
<strong class="bold"><st c="19531">247   q      0      1      0      0        0</st></strong>
<strong class="bold"><st c="19547">652</st></strong><strong class="bold"><st c="19551">   i      0      0      0      1        0</st></strong>
<strong class="bold"><st c="19563">513   e      0      0      </st></strong><strong class="bold"><st c="19574">0      0        0</st></strong>
<strong class="bold"><st c="19579">230  cc      0      0      0      0        0</st></strong>
<strong class="bold"><st c="19596">250   </st></strong><strong class="bold"><st c="19601">e      0      0      0      0        0</st></strong></pre><p class="list-inset"><st c="19612">Let’s now automate one-hot encoding of frequent categories </st><span class="No-Break"><st c="19672">with </st></span><span class="No-Break"><strong class="source-inline"><st c="19677">scikit-learn</st></strong></span><span class="No-Break"><st c="19689">.</st></span></p></li>				<li><st c="19690">Let’s import </st><span class="No-Break"><st c="19704">the encoder:</st></span><pre class="source-code"><st c="19716">
from sklearn.preprocessing import OneHotEncoder</st></pre></li>				<li><st c="19764">Let’s set </st><a id="_idIndexMarker144"/><st c="19775">up the encoder to encode categories shown in at least </st><strong class="source-inline"><st c="19829">39</st></strong><st c="19831"> observations and limit the number of categories to encode </st><span class="No-Break"><st c="19890">to </st></span><span class="No-Break"><strong class="source-inline"><st c="19893">5</st></strong></span><span class="No-Break"><st c="19894">:</st></span><pre class="source-code"><st c="19895">
encoder = OneHotEncoder(
    min_frequency=39,
    max_categories=5,
    sparse_output=False,
).set_output(transform="pandas")</st></pre></li>				<li><st c="20010">Finally, let’s</st><a id="_idIndexMarker145"/><st c="20025"> fit the transformer to the two high cardinal variables and then transform </st><span class="No-Break"><st c="20100">the data:</st></span><pre class="source-code"><st c="20109">
X_train_enc = encoder.fit_transform(X_train[
    ['A6', 'A7']])
X_test_enc = encoder.transform(X_test[['A6', 'A7']])</st></pre><p class="list-inset"><st c="20222">If you execute </st><strong class="source-inline"><st c="20238">X_train_enc.head()</st></strong><st c="20256"> you’ll see the </st><span class="No-Break"><st c="20272">resulting DataFrame:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B22396_02_06.jpg" alt="Figure 2.6 – Transformed DataFrame containing binary variables for those categories with at least 39 observations and an additional binary representing all remaining categories"/><st c="20292"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="20596">Figure 2.6 – Transformed DataFrame containing binary variables for those categories with at least 39 observations and an additional binary representing all remaining categories</st></p>
			<p class="list-inset"><st c="20772">To wrap up the recipe, let’s encode the most frequent categories </st><span class="No-Break"><st c="20838">with </st></span><span class="No-Break"><strong class="source-inline"><st c="20843">feature-engine</st></strong></span><span class="No-Break"><st c="20857">.</st></span></p>
			<ol>
				<li value="11"><st c="20858">Let’s set up</st><a id="_idIndexMarker146"/><st c="20871"> the one-hot encoder to encode the five most frequent categories of the </st><strong class="source-inline"><st c="20943">A6</st></strong><st c="20945"> and </st><span class="No-Break"><strong class="source-inline"><st c="20950">A7</st></strong></span><span class="No-Break"><st c="20952"> variables:</st></span><pre class="source-code"><st c="20963">
From feature_engine.encoding import OneHotEncoder
ohe_enc = OneHotEncoder(
    top_categories=5,
    variables=["A6", "A7"]
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="21081">Note</st></p>
			<p class="callout"><st c="21086">The number of frequent categories to encode is arbitrarily dete</st><a id="_idTextAnchor230"/><st c="21150">rmined by </st><span class="No-Break"><st c="21161">the user.</st></span></p>
			<ol>
				<li value="12"><st c="21170">Let’s fit</st><a id="_idIndexMarker147"/><st c="21180"> the encoder to the train set so that it learns and stores the most frequent categories of </st><strong class="source-inline"><st c="21271">A6</st></strong> <span class="No-Break"><st c="21273">and </st></span><span class="No-Break"><strong class="source-inline"><st c="21278">A7</st></strong></span><span class="No-Break"><st c="21280">:</st></span><pre class="source-code"><st c="21282">
ohe_enc.fit(X_train)</st></pre></li>				<li><st c="21303">Finally, let’s encode </st><strong class="source-inline"><st c="21326">A6</st></strong><st c="21328"> and </st><strong class="source-inline"><st c="21333">A7</st></strong><st c="21335"> in the train and </st><span class="No-Break"><st c="21353">test sets:</st></span><pre class="source-code"><st c="21363">
X_train_enc = ohe_enc.transform(X_train)
X_test_enc = ohe_enc.transform(X_test)</st></pre><p class="list-inset"><st c="21443">You can view the new binary variables in the transformed DataFrame by executing </st><strong class="source-inline"><st c="21524">X_train_enc.head()</st></strong><st c="21542">. You can also find the top five categories learned by the encoder b</st><a id="_idTextAnchor231"/><a id="_idTextAnchor232"/><st c="21610">y </st><span class="No-Break"><st c="21613">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="21623">ohe_enc.encoder_dict_</st></strong></span><span class="No-Break"><st c="21644">.</st></span></p></li>			</ol>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor233"/><st c="21645">How it works...</st></h2>
			<p><st c="21661">In the first </st><a id="_idIndexMarker148"/><st c="21675">part of this recipe, we worked with the </st><strong class="source-inline"><st c="21715">A6</st></strong><st c="21717"> categorical variable. </st><st c="21740">We inspected its unique categories with </st><strong class="source-inline"><st c="21780">pandas</st></strong><st c="21786">’ </st><strong class="source-inline"><st c="21789">unique()</st></strong><st c="21797">. Next, we counted the number of observations per category using </st><strong class="source-inline"><st c="21862">pandas</st></strong><st c="21868">’ </st><strong class="source-inline"><st c="21871">value_counts()</st></strong><st c="21885">, which returned a </st><strong class="source-inline"><st c="21904">pandas</st></strong><st c="21910"> series with the categories as the index and the number of observations as values. </st><st c="21993">Next, we sorted the categories from the one with the most to the one with the least observations using </st><strong class="source-inline"><st c="22096">pandas</st></strong><st c="22102">’</st><a id="_idTextAnchor234"/> <strong class="source-inline"><st c="22104">sort_values()</st></strong><st c="22117">. We then reduced the series to the five most popular categories by using </st><strong class="source-inline"><st c="22191">pandas</st></strong><st c="22197">’ </st><strong class="source-inline"><st c="22200">head()</st></strong><st c="22206">. We used this series in a list comprehension to capture the names of the most frequent categories. </st><st c="22306">After that, we looped over each category, and with NumPy’s </st><strong class="source-inline"><st c="22365">where()</st></strong><st c="22372">, we created binary variables by placing a value of </st><strong class="source-inline"><st c="22424">1</st></strong><st c="22425"> if the observation showed the category, or </st><span class="No-Break"><strong class="source-inline"><st c="22469">0</st></strong></span><span class="No-Break"><st c="22470"> otherwise.</st></span></p>
			<p><st c="22481">We discussed how to use </st><strong class="source-inline"><st c="22506">OneHotEncoder()</st></strong><st c="22521"> from </st><strong class="source-inline"><st c="22527">scikit-learn</st></strong><st c="22539"> and </st><strong class="source-inline"><st c="22544">feature-engine</st></strong><st c="22558"> in the </st><em class="italic"><st c="22566">Creating binary variables through one-hot encoding</st></em><st c="22616"> recipe. </st><st c="22625">Here, I will only highlight the parameters needed to encode the most </st><span class="No-Break"><st c="22694">frequent categories.</st></span></p>
			<p><st c="22714">To encode</st><a id="_idIndexMarker149"/><st c="22724"> frequent categories with </st><strong class="source-inline"><st c="22750">scikit-learn</st></strong><st c="22762">, we set the </st><strong class="source-inline"><st c="22775">min_frequency</st></strong><st c="22788"> parameter to </st><strong class="source-inline"><st c="22802">39</st></strong><st c="22804">. Hence, categories shown in less than </st><strong class="source-inline"><st c="22843">39</st></strong><st c="22845"> observations were grouped into an additional binary variable </st><span class="No-Break"><st c="22907">called </st></span><span class="No-Break"><strong class="source-inline"><st c="22914">infrequent_sklearn</st></strong></span><span class="No-Break"><st c="22932">.</st></span></p>
			<p><st c="22933">To encode frequent categories with </st><strong class="source-inline"><st c="22969">feature-engine</st></strong><st c="22983">, we set the </st><strong class="source-inline"><st c="22996">top_categories</st></strong><st c="23010"> parameter to </st><strong class="source-inline"><st c="23024">5</st></strong><st c="23025">. Hence, the transformer created binary variables for the 5 most frequent categories only. </st><st c="23116">Less frequent categories will s</st><a id="_idTextAnchor235"/><a id="_idTextAnchor236"/><st c="23147">how a </st><strong class="source-inline"><st c="23154">0</st></strong><st c="23155"> in all the </st><span class="No-Break"><st c="23167">binary variables.</st></span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor237"/><st c="23184">There’s more...</st></h2>
			<p><st c="23200">This recipe is based on the winning solution of the </st><strong class="bold"><st c="23253">Knowledge Discovery and Data </st></strong><st c="23282">(</st><strong class="bold"><st c="23283">KDD</st></strong><st c="23286">) 2009 mining cup, </st><em class="italic"><st c="23306">Winning the KDD Cup Orange Challenge with Ensemble Selection</st></em><st c="23366"> (http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf), where the author's limited one-hot encoding to the 10 most f</st><a id="_idTextAnchor238"/><a id="_idTextAnchor239"/><st c="23490">requent categories of </st><span class="No-Break"><st c="23513">each variable.</st></span></p>
			<h1 id="_idParaDest-69"><st c="23527">Replacing categories with counts or th</st><a id="_idTextAnchor240"/><st c="23566">e frequency of observations</st></h1>
			<p><st c="23594">In count </st><a id="_idIndexMarker150"/><st c="23604">with counts or frequency of observatio</st><a id="_idTextAnchor241"/><st c="23642">ns” or frequency encoding, we replace the categories with the count or the fraction of observations showing that category. </st><st c="23766">That is, if 10 out of 100 observations show the </st><strong class="source-inline"><st c="23814">blue</st></strong><st c="23818"> category for the </st><strong class="source-inline"><st c="23836">Color</st></strong><st c="23841"> variable, we would replace </st><strong class="source-inline"><st c="23869">blue</st></strong><st c="23873"> with </st><strong class="source-inline"><st c="23879">10</st></strong><st c="23881"> when doing count encoding, or with </st><strong class="source-inline"><st c="23917">0.1</st></strong><st c="23920"> if performing frequency encoding. </st><st c="23955">These encoding methods are useful when there is a relationship between the category frequency and the target. </st><st c="24065">For example, in sales, the frequency of a product may indicate </st><span class="No-Break"><st c="24128">its popularity.</st></span></p>
			<p class="callout-heading"><st c="24143">Note</st></p>
			<p class="callout"><st c="24148">If two different categories are present in the same number of observations, they will be replaced by the same value, which may lead to </st><span class="No-Break"><st c="24284">information loss.</st></span></p>
			<p><st c="24301">In this recipe, we will perform count and frequency enco</st><a id="_idTextAnchor242"/><a id="_idTextAnchor243"/><st c="24358">ding using </st><strong class="source-inline"><st c="24370">pandas</st></strong> <span class="No-Break"><st c="24376">and </st></span><span class="No-Break"><strong class="source-inline"><st c="24381">feature-engine</st></strong></span><span class="No-Break"><st c="24395">.</st></span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor244"/><st c="24396">How to do it...</st></h2>
			<p><st c="24412">We’ll start by encoding one variable with </st><strong class="source-inline"><st c="24455">pandas</st></strong><st c="24461"> and then we’ll automate the process </st><span class="No-Break"><st c="24498">with </st></span><span class="No-Break"><strong class="source-inline"><st c="24503">feature-engine</st></strong></span><span class="No-Break"><st c="24517">:</st></span></p>
			<ol>
				<li><st c="24519">Let’s start with </st><span class="No-Break"><st c="24536">the imports:</st></span><pre class="source-code"><st c="24548">
import pandas as pd
from sklearn.model_selection import train_test_split
from feature_engine.encoding import CountFrequencyEncoder</st></pre></li>				<li><st c="24679">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="24748">test sets:</st></span><pre class="source-code"><st c="24758">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="24944">Let’s </st><a id="_idIndexMarker151"/><st c="24951">with counts or frequency of observations” capture the number of observations per category of the </st><strong class="source-inline"><st c="25048">A7</st></strong><st c="25050"> variable in </st><span class="No-Break"><st c="25063">a dictionary:</st></span><pre class="source-code"><st c="25076">
counts = X_train["A7"].value_counts().to_dict()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="25124">Note</st></p>
			<p class="callout"><st c="25129">To find the frequency instead, </st><span class="No-Break"><st c="25161">execute </st></span><span class="No-Break"><strong class="source-inline"><st c="25169">X_train["A7"].value_counts(normalize=True).to_dict()</st></strong></span><span class="No-Break"><st c="25221">.</st></span></p>
			<p class="list-inset"><st c="25222">If we execute </st><strong class="source-inline"><st c="25237">print(counts)</st></strong><st c="25250">, we’ll see the count of observations per category </st><span class="No-Break"><st c="25301">of </st></span><span class="No-Break"><strong class="source-inline"><st c="25304">A7</st></strong></span><span class="No-Break"><st c="25306">:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="25308">{'v': 277, 'h': 101, 'ff': 41, 'bb': 39, 'z': 7, 'dd': 5, 'j': 5, 'Missing': 4, 'n': 3, 'o': 1}</st></strong></pre>			<ol>
				<li value="4"><st c="25403">Let’s replace the categories in </st><strong class="source-inline"><st c="25436">A7</st></strong><st c="25438"> with the counts in a copy of the </st><span class="No-Break"><st c="25472">data sets:</st></span><pre class="source-code"><st c="25482">
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()
X_train_enc["A7"] = X_train_enc["A7"].map(counts)
X_test_enc["A7"] = X_test_enc["A7"].map(counts)</st></pre><p class="list-inset"><st c="25636">Go ahead and inspect the data by executing </st><strong class="source-inline"><st c="25680">X_train_enc.head()</st></strong><st c="25698"> to corroborate that the categories have been replaced by </st><span class="No-Break"><st c="25756">the counts.</st></span></p><p class="list-inset"><st c="25767">To apply this procedure to multiple variables, we can </st><span class="No-Break"><st c="25822">use </st></span><span class="No-Break"><strong class="source-inline"><st c="25826">feature-engine</st></strong></span><span class="No-Break"><st c="25840">.</st></span></p></li>				<li><st c="25841">Let’s set up the encoder so that it encodes all categorical variables with the count </st><span class="No-Break"><st c="25927">of observations:</st></span><pre class="source-code"><st c="25943">
count_enc = CountFrequencyEncoder(
    encoding_method="count", variables=None,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="26021">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="26026">CountFrequencyEncoder()</st></strong><st c="26050"> will automatically find and encode all categorical variables in the train set. </st><st c="26130">To encode only a subset of the variables, pass the variable names in a list to the </st><strong class="source-inline"><st c="26213">variables</st></strong><st c="26222"> argument. </st><st c="26233">To encode with the frequency instead, </st><span class="No-Break"><st c="26271">use </st></span><span class="No-Break"><strong class="source-inline"><st c="26275">encoding_method="frequency"</st></strong></span><span class="No-Break"><st c="26302">.</st></span></p>
			<ol>
				<li value="6"><st c="26303">Let’s fit the</st><a id="_idIndexMarker152"/><st c="26317"> encoder to the train set so that it stores the number of observations per category </st><span class="No-Break"><st c="26401">per variable:</st></span><pre class="source-code"><st c="26414">
count_enc.fit(X_train)</st></pre></li>				<li><st c="26437">The encoder found the categorical variables automatically. </st><st c="26497">Let’s check </st><span class="No-Break"><st c="26509">them out:</st></span><pre class="source-code"><st c="26518">
count_enc.variables_</st></pre><p class="list-inset"><st c="26539">The previous command returns the names of the categorical variables in the </st><span class="No-Break"><st c="26615">train set:</st></span></p><pre class="source-code"><strong class="bold"><st c="26625">['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']</st></strong></pre></li>				<li><st c="26683">Let’s print the count of observations per category </st><span class="No-Break"><st c="26735">per variable:</st></span><pre class="source-code">
<strong class="bold"><st c="26748">count_enc.encoder_dict_</st></strong></pre><p class="list-inset"><st c="26772">The previous attribute stores the mappings that will be used to replace </st><span class="No-Break"><st c="26845">the categories:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B22396_02_07.jpg" alt="Figure 2.7 – Dictionary containing the number of observations per category, for each variable; these values will be used to encode the categorical variables"/><st c="26860"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27366">Figure 2.7 – Dictionary containing the number of observations per category, for each variable; these values will be used to encode the categorical variables</st></p>
			<ol>
				<li value="9"><st c="27522">Finally, let’s </st><a id="_idIndexMarker153"/><st c="27538">with counts or frequency of observations” replace the categories with counts in the train and </st><span class="No-Break"><st c="27632">test sets:</st></span><pre class="source-code"><st c="27642">
X_train_enc = count_enc.transform(X_train)
X_test_enc = count_enc.transform(X_test)</st></pre></li>			</ol>
			<p><st c="27726">Check out the result by executing </st><strong class="source-inline"><st c="27761">X_train_enc.head()</st></strong><st c="27779">. The encoder returns </st><strong class="source-inline"><st c="27801">pandas</st></strong><st c="27807"> DataFrames with the strings of the categorical variables replaced with the counts of observations, leaving the variables r</st><a id="_idTextAnchor245"/><a id="_idTextAnchor246"/><st c="27930">eady to use in machine </st><span class="No-Break"><st c="27954">learning models.</st></span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor247"/><st c="27970">How it works...</st></h2>
			<p><st c="27986">In this recipe, we replaced categories with the count of observations using </st><strong class="source-inline"><st c="28063">pandas</st></strong> <span class="No-Break"><st c="28069">and </st></span><span class="No-Break"><strong class="source-inline"><st c="28074">feature-engine</st></strong></span><span class="No-Break"><st c="28088">.</st></span></p>
			<p><st c="28089">Using </st><strong class="source-inline"><st c="28096">pandas</st></strong><st c="28102">’ </st><strong class="source-inline"><st c="28105">value_counts()</st></strong><st c="28119">, we determined the number of observations per category of the </st><strong class="source-inline"><st c="28182">A7</st></strong><st c="28184"> variable, and with </st><strong class="source-inline"><st c="28204">pandas</st></strong><st c="28210">’ </st><strong class="source-inline"><st c="28213">to_dict()</st></strong><st c="28222">, we captured these values in a </st><a id="_idIndexMarker154"/><st c="28254">with counts or frequency of observations” dictionary, where each key was a unique category, and each value the number of observations for that category. </st><st c="28407">With </st><strong class="source-inline"><st c="28412">pandas</st></strong><st c="28418">’ </st><strong class="source-inline"><st c="28421">map()</st></strong><st c="28426"> and using this dictionary, we replaced the categories with the observation counts in both the train and </st><span class="No-Break"><st c="28531">test sets.</st></span></p>
			<p class="callout-heading"><st c="28541">Note</st></p>
			<p class="callout"><st c="28546">The count of observations for the encoding should be obtained from the train set to avoid data leakage. </st><st c="28651">Note that new categories in the test set will not have a corresponding mapping and hence will be replaced by </st><strong class="source-inline"><st c="28760">nan</st></strong><st c="28763">. To avoid this, use f</st><strong class="source-inline"><st c="28785">eature-engine</st></strong><st c="28799">. Alternatively, you can replace the </st><strong class="source-inline"><st c="28836">nan</st></strong> <span class="No-Break"><st c="28839">with </st></span><span class="No-Break"><strong class="source-inline"><st c="28845">0</st></strong></span><span class="No-Break"><st c="28846">.</st></span></p>
			<p><st c="28847">To perform count encoding with </st><strong class="source-inline"><st c="28879">feature-engine</st></strong><st c="28893">, we used </st><strong class="source-inline"><st c="28903">CountFrequencyEncoder()</st></strong><st c="28926"> and set </st><strong class="source-inline"><st c="28935">encoding_method</st></strong><st c="28950"> to </st><strong class="source-inline"><st c="28954">'count'</st></strong><st c="28961">. We left the </st><strong class="source-inline"><st c="28975">variables</st></strong><st c="28984"> argument set to </st><strong class="source-inline"><st c="29001">None</st></strong><st c="29005"> so that the encoder automatically finds all the categorical variables in the dataset. </st><st c="29092">With </st><strong class="source-inline"><st c="29097">fit()</st></strong><st c="29102">, the transformer found the categorical variables and stored the observation counts per category in the </st><strong class="source-inline"><st c="29206">encoder_dict_</st></strong><st c="29219"> attribute. </st><st c="29231">With </st><strong class="source-inline"><st c="29236">transform()</st></strong><st c="29247">, the transformer replaced the categories with the counts, returning a </st><span class="No-Break"><strong class="source-inline"><st c="29318">pandas</st></strong></span><span class="No-Break"><st c="29324"> DataFrame.</st></span></p>
			<p class="callout-heading"><st c="29335">Note</st></p>
			<p class="callout"><st c="29340">If there are categories in the test set that were not present in the train set, the encoder will raise an error by default. </st><st c="29465">You can make it ignore them, in which case they will appear as </st><strong class="source-inline"><st c="29528">nan</st></strong><st c="29531">, or encode them </st><span class="No-Break"><st c="29548">as </st></span><span class="No-Break"><strong class="source-inline"><st c="29551">0</st></strong></span><span class="No-Break"><st c="29552">.</st></span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor248"/><st c="29553">See also</st></h2>
			<p><st c="29562">You can also carry out count and frequency encoding with the Python library Category </st><span class="No-Break"><st c="29648">Encoders: </st></span><a href="https://contrib.scikit-learn.org/category_encoders/count.html"><span class="No-Break"><st c="29658">https://contrib.scikit-learn.org/category_encoders/count.html</st></span></a><span class="No-Break"><st c="29719">.</st></span></p>
			<p><st c="29720">For some useful applications of count encoding, check out this </st><span class="No-Break"><st c="29784">article: </st></span><a href="https://letsdatascience.com/frequency-encoding/"><span class="No-Break"><st c="29793">https://</st><span id="_idTextAnchor249"/><span id="_idTextAnchor250"/><st c="29801">letsdatascience.com/frequency-encoding/</st></span></a><span class="No-Break"><st c="29841">.</st></span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor251"/><st c="29842">Replacing categories with ordinal numbers</st></h1>
			<p><st c="29884">Ordinal encoding </st><a id="_idIndexMarker155"/><st c="29902">consists of replacing the categories with digits from </st><em class="italic"><st c="29956">1</st></em><st c="29957"> to </st><em class="italic"><st c="29961">k</st></em><st c="29962"> (or </st><em class="italic"><st c="29967">0</st></em><st c="29968"> to </st><em class="italic"><st c="29972">k-1</st></em><st c="29975">, depending on the implementation), where </st><em class="italic"><st c="30017">k</st></em><st c="30018"> is the number of distinct categories of the variable. </st><st c="30073">The numbers are assigned arbitrarily. </st><st c="30111">Ordinal encoding is better suited for non-linear machine learning models, which can navigate through arbitrarily assigned numbers to find patterns that relate to </st><span class="No-Break"><st c="30273">the target.</st></span></p>
			<p><st c="30284">In this recipe, we will </st><a id="_idIndexMarker156"/><st c="30309">perform ordinal encoding using</st><a id="_idTextAnchor252"/><a id="_idTextAnchor253"/> <strong class="source-inline"><st c="30339">pandas</st></strong><st c="30346">, </st><strong class="source-inline"><st c="30348">scikit-learn</st></strong><st c="30360">, </st><span class="No-Break"><st c="30362">and </st></span><span class="No-Break"><strong class="source-inline"><st c="30366">feature-engine</st></strong></span><span class="No-Break"><st c="30380">.</st></span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor254"/><st c="30381">How to do it...</st></h2>
			<p><st c="30397">First, let’s make the import and prepare </st><span class="No-Break"><st c="30439">the dataset:</st></span></p>
			<ol>
				<li><st c="30451">Import </st><strong class="source-inline"><st c="30459">pandas</st></strong><st c="30465"> and the data </st><span class="No-Break"><st c="30479">split function:</st></span><pre class="source-code"><st c="30494">
import pandas as pd
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="30567">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="30636">test sets:</st></span><pre class="source-code"><st c="30646">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="30832">To </st><a id="_idIndexMarker157"/><st c="30836">encode the </st><strong class="source-inline"><st c="30847">A7</st></strong><st c="30849"> variable, let’s make a dictionary of </st><span class="No-Break"><st c="30887">category-to-integer pairs:</st></span><pre class="source-code"><st c="30913">
ordinal_mapping = {k: i for i, k in enumerate(
    X_train["A7"].unique(), 0)
}</st></pre><p class="list-inset"><st c="30989">If we execute </st><strong class="source-inline"><st c="31004">print(ordinal_mapping)</st></strong><st c="31026">, we will see the digits that will replace </st><span class="No-Break"><st c="31069">each category:</st></span></p><pre class="source-code"><strong class="bold"><st c="31083">{'v': 0, 'ff': 1, 'h': 2, 'dd': 3, 'z': 4, 'bb': 5, 'j': 6, 'Missing': 7, 'n': 8, 'o': 9}</st></strong></pre></li>				<li><st c="31173">Now, let’s </st><a id="_idIndexMarker158"/><st c="31185">replace the categories in a copy of </st><span class="No-Break"><st c="31221">the DataFrames:</st></span><pre class="source-code"><st c="31236">
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()
X_train_enc["A7"] = X_train_enc["A7"].map(ordinal_mapping)
X_test_enc["A7"] = X_test_enc["A7"].map(ordinal_mapping)</st></pre><p class="list-inset"><st c="31408">Go ahead and execute </st><strong class="source-inline"><st c="31430">print(X_train["A7"].head())</st></strong><st c="31457"> to see the result of the </st><span class="No-Break"><st c="31483">previous operation.</st></span></p><p class="list-inset"><st c="31502">Next, we’ll carry out ordinal encoding </st><span class="No-Break"><st c="31542">using </st></span><span class="No-Break"><strong class="source-inline"><st c="31548">scikit-learn</st></strong></span><span class="No-Break"><st c="31560">.</st></span></p></li>				<li><st c="31561">Let’s import the </st><span class="No-Break"><st c="31579">required classes:</st></span><pre class="source-code"><st c="31596">
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer</st></pre></li>			</ol>
			<p class="callout-heading"><st c="31691">Note</st></p>
			<p class="callout"><st c="31696">Do not confuse </st><strong class="source-inline"><st c="31712">OrdinalEncoder()</st></strong><st c="31728"> with </st><strong class="source-inline"><st c="31734">LabelEncoder()</st></strong><st c="31748"> from </st><strong class="source-inline"><st c="31754">scikit-learn</st></strong><st c="31766">. The former is intended to encode predictive features, whereas the latter is intended to modify the </st><span class="No-Break"><st c="31867">target variable.</st></span></p>
			<ol>
				<li value="6"><st c="31883">Let’s set</st><a id="_idIndexMarker159"/><st c="31893"> up </st><span class="No-Break"><st c="31897">the encoder:</st></span><pre class="source-code"><st c="31909">
enc = OrdinalEncoder()</st></pre></li>				<li><st c="31932">Let’s make a list containing the categorical variables </st><span class="No-Break"><st c="31988">to encode:</st></span><pre class="source-code"><st c="31998">
cat_vars = X_train.select_dtypes(include="O").columns.to_list()</st></pre></li>				<li><st c="32062">Let’s restrict </st><a id="_idIndexMarker160"/><st c="32078">the encoding to the </st><span class="No-Break"><st c="32098">categorical variables:</st></span><pre class="source-code"><st c="32120">
ct = ColumnTransformer(
    [("encoder", enc, cat_vars)],
    remainder="passthrough",
    force_int_remainder_cols=False,
).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="32264">Note</st></p>
			<p class="callout"><st c="32269">Remember to set </st><strong class="source-inline"><st c="32286">remainder</st></strong><st c="32295"> to </st><strong class="source-inline"><st c="32299">"passthrough"</st></strong><st c="32312"> to make the </st><strong class="source-inline"><st c="32325">ColumnTransformer()</st></strong><st c="32344"> return the un-transformed variables </st><span class="No-Break"><st c="32381">as well.</st></span></p>
			<ol>
				<li value="9"><st c="32389">Let’s fit the encoder to the train set so that it creates and stores representations of categories </st><span class="No-Break"><st c="32489">to digits:</st></span><pre class="source-code"><st c="32499">
ct.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="32515">Note</st></p>
			<p class="callout"><st c="32520">By executing </st><strong class="source-inline"><st c="32534">ct.named_transformers_["encoder"].categories_</st></strong><st c="32579">, you can visualize the unique categories </st><span class="No-Break"><st c="32621">per variable.</st></span></p>
			<ol>
				<li value="10"><st c="32634">Now, let’s encode the categorical variables in the train and </st><span class="No-Break"><st c="32696">test sets:</st></span><pre class="source-code"><st c="32706">
X_train_enc = ct.transform(X_train)
X_test_enc = ct.transform(X_test)</st></pre><p class="list-inset"><st c="32776">Go ahead and execute </st><strong class="source-inline"><st c="32798">X_train_enc.head()</st></strong><st c="32816"> to check out the </st><span class="No-Break"><st c="32834">resulting DataFrame.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="32854">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="32859">ColumnTransformer()</st></strong><st c="32879"> will mark the encoded variables by appending </st><strong class="source-inline"><st c="32925">encoder</st></strong><st c="32932"> to the variable name. </st><st c="32955">The variables that were not modified show the </st><span class="No-Break"><strong class="source-inline"><st c="33001">remainder</st></strong></span><span class="No-Break"><st c="33010"> prefix.</st></span></p>
			<p class="list-inset"><st c="33018">Now, let’s do</st><a id="_idIndexMarker161"/><st c="33032"> ordinal encoding </st><a id="_idIndexMarker162"/><span class="No-Break"><st c="33050">with </st></span><span class="No-Break"><strong class="source-inline"><st c="33055">feature-engine</st></strong></span><span class="No-Break"><st c="33069">.</st></span></p>
			<ol>
				<li value="11"><st c="33070">Let’s import </st><span class="No-Break"><st c="33084">the encoder:</st></span><pre class="source-code"><st c="33096">
from feature_engine.encoding import OrdinalEncoder</st></pre></li>				<li><st c="33147">Let’s set up the encoder so that it replaces categories with arbitrary integers in the categorical variables specified in </st><span class="No-Break"><em class="italic"><st c="33270">Step 7</st></em></span><span class="No-Break"><st c="33276">:</st></span><pre class="source-code"><st c="33278">
enc = OrdinalEncoder(
    encoding_method="arbitrary",
    variables=cat_vars,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="33351">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="33356">feature-engine</st></strong><st c="33371">’s </st><strong class="source-inline"><st c="33375">OrdinalEncoder()</st></strong><st c="33391"> automatically finds and encodes all categorical variables if the </st><strong class="source-inline"><st c="33457">variables</st></strong><st c="33466"> parameter is </st><strong class="source-inline"><st c="33480">None</st></strong><st c="33484">. Alternatively, it will encode the variables indicated in the list. </st><st c="33553">In addition, it can assign the integers according to the target mean value (see the </st><em class="italic"><st c="33637">Performing ordinal encoding based on the target </st></em><span class="No-Break"><em class="italic"><st c="33685">value</st></em></span><span class="No-Break"><st c="33690"> recipe).</st></span></p>
			<ol>
				<li value="13"><st c="33699">Let’s fit the</st><a id="_idIndexMarker163"/><st c="33713"> encoder to the train set so </st><a id="_idIndexMarker164"/><st c="33742">that it learns and stores the </st><span class="No-Break"><st c="33772">category-to-integer mappings:</st></span><pre class="source-code"><st c="33801">
enc.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="33818">Note</st></p>
			<p class="callout"><st c="33823">The category to integer mappings are stored in the </st><strong class="source-inline"><st c="33875">encoder_dict_</st></strong><st c="33888"> attribute and can be accessed by </st><span class="No-Break"><st c="33922">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="33932">enc.encoder_dict_</st></strong></span><span class="No-Break"><st c="33949">.</st></span></p>
			<ol>
				<li value="14"><st c="33950">Finally, let’s encode the categorical variables in the train and </st><span class="No-Break"><st c="34016">test sets:</st></span><pre class="source-code"><st c="34026">
X_train_enc = enc.transform(X_train)
X_test_enc = enc.transform(X_test)</st></pre></li>			</ol>
			<p><strong class="source-inline"><st c="34098">feature-engine</st></strong><st c="34113"> returns </st><strong class="source-inline"><st c="34122">pandas</st></strong><st c="34128"> DataFrames where the values of the original variables are replaced with numbers, leaving the </st><a id="_idTextAnchor255"/><a id="_idTextAnchor256"/><st c="34222">DataFrame ready to use in machine </st><span class="No-Break"><st c="34256">learning models.</st></span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor257"/><st c="34272">How it works...</st></h2>
			<p><st c="34288">In this recipe, we replaced categories with integers </st><span class="No-Break"><st c="34342">assigned arbitrarily.</st></span></p>
			<p><st c="34363">We used </st><strong class="source-inline"><st c="34372">pandas</st></strong><st c="34378">’ </st><strong class="source-inline"><st c="34381">unique()</st></strong><st c="34389"> to find the unique categories of the </st><strong class="source-inline"><st c="34427">A7</st></strong><st c="34429"> variable. </st><st c="34440">Next, we created a dictionary of category-to-integer and passed it to </st><strong class="source-inline"><st c="34510">pandas</st></strong><st c="34516">’ </st><strong class="source-inline"><st c="34519">map()</st></strong><st c="34524"> to replace the strings in </st><strong class="source-inline"><st c="34551">A7</st></strong><st c="34553"> with </st><span class="No-Break"><st c="34559">the integers.</st></span></p>
			<p><st c="34572">Next, we carried out ordinal encoding using </st><strong class="source-inline"><st c="34617">scikit-learn</st></strong><st c="34629">’s </st><strong class="source-inline"><st c="34633">OrdinalEncoder() </st></strong><st c="34650">and used </st><strong class="source-inline"><st c="34659">ColumnTransformer()</st></strong><st c="34678"> to restrict the encoding to categorical variables. </st><st c="34730">With </st><strong class="source-inline"><st c="34735">fit()</st></strong><st c="34740">, the transformer created the category-to-integer mappings based on the categories in the train set. </st><st c="34841">With </st><strong class="source-inline"><st c="34846">transform()</st></strong><st c="34857">, the categories were replaced with integers. </st><st c="34903">By setting the </st><strong class="source-inline"><st c="34918">remainder</st></strong><st c="34927"> parameter to </st><strong class="source-inline"><st c="34941">passthrough</st></strong><st c="34952">, we made </st><strong class="source-inline"><st c="34962">ColumnTransformer()</st></strong><st c="34981"> concatenate the variables that are not encoded at the back of the </st><span class="No-Break"><st c="35048">encoded features.</st></span></p>
			<p><st c="35065">To perform ordinal encoding with </st><strong class="source-inline"><st c="35099">feature-engine</st></strong><st c="35113">, we used </st><strong class="source-inline"><st c="35123">OrdinalEncoder()</st></strong><st c="35139">, indicating that the integers should be assigned arbitrarily through </st><strong class="source-inline"><st c="35209">encoding_method</st></strong><st c="35224">, and passed a list with the variables to encode in the </st><strong class="source-inline"><st c="35280">variables</st></strong><st c="35289"> argument. </st><st c="35300">With </st><strong class="source-inline"><st c="35305">fit()</st></strong><st c="35310">, the encoder assigned integers to each variable’s categories, which </st><a id="_idIndexMarker165"/><st c="35379">were stored in the </st><strong class="source-inline"><st c="35398">encoder_dict_</st></strong><st c="35411"> attribute. </st><st c="35423">These</st><a id="_idIndexMarker166"/><st c="35428"> mappings were then used by the </st><strong class="source-inline"><st c="35460">transform()</st></strong><st c="35471"> method to replace the categories in the train and test sets, </st><span class="No-Break"><st c="35533">returning DataFrames.</st></span></p>
			<p class="callout-heading"><st c="35554">Note</st></p>
			<p class="callout"><st c="35559">When a category in the test set is not present in the training set, it will not have a mapping to a digit. </st><strong class="source-inline"><st c="35667">OrdinalEncoder()</st></strong><st c="35683"> from </st><strong class="source-inline"><st c="35689">scikit-learn</st></strong><st c="35701"> and </st><strong class="source-inline"><st c="35706">feature-engine</st></strong><st c="35720"> will raise an error by default. </st><st c="35753">However, they have the option to replace unseen categories with a user-defined value or </st><strong class="source-inline"><st c="35841">-</st></strong><span class="No-Break"><strong class="source-inline"><st c="35842">1</st></strong></span><span class="No-Break"><st c="35843">, respectively.</st></span></p>
			<p><strong class="source-inline"><st c="35858">scikit-learn</st></strong><st c="35871">’s </st><strong class="source-inline"><st c="35875">OrdinalEncoder()</st></strong><st c="35891"> can restrict the encoding to those categories with a minimum frequency. </st><strong class="source-inline"><st c="35964">feature-engine</st></strong><st c="35978">’s </st><strong class="source-inline"><st c="35982">OrdinalEncoder()</st></strong><st c="35998"> can assign the numbers based on the target</st><a id="_idTextAnchor258"/><a id="_idTextAnchor259"/><st c="36041"> mean value, as we will see in the </st><span class="No-Break"><st c="36076">following recipe.</st></span></p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor260"/><st c="36093">There’s more...</st></h2>
			<p><st c="36109">You can also carry out ordinal encoding with </st><strong class="source-inline"><st c="36155">OrdinalEncoder()</st></strong><st c="36171"> from Category Encoders. </st><st c="36196">Check it out </st><span class="No-Break"><st c="36209">at </st></span><a href="http://contrib.scikit-learn.org/category_encoders/ordinal.html"><span class="No-Break"><st c="36212">http://cont</st><span id="_idTextAnchor261"/><st c="36223">rib.scikit-l</st><span id="_idTextAnchor262"/><span id="_idTextAnchor263"/><span id="_idTextAnchor264"/><st c="36236">earn.org/category_encoders/ordinal.html</st></span></a><span class="No-Break"><st c="36276">.</st></span></p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor265"/><st c="36277">Performing ordinal encoding based on the target value</st></h1>
			<p><st c="36331">In the previous</st><a id="_idIndexMarker167"/><st c="36347"> recipe, we replaced categories with integers, which were assigned arbitrarily. </st><st c="36427">We can also assign integers to the categories given the target values. </st><st c="36498">To do this, first, we calculate the mean value of the target per category. </st><st c="36573">Next, we order the categories from the one with the lowest to the one with the highest target mean value. </st><st c="36679">Finally, we assign digits to the ordered categories, starting with </st><em class="italic"><st c="36746">0</st></em><st c="36747"> to the first category up to </st><em class="italic"><st c="36776">k-1</st></em><st c="36779"> to the last category, where </st><em class="italic"><st c="36808">k</st></em><st c="36809"> is the number of </st><span class="No-Break"><st c="36827">distinct categories.</st></span></p>
			<p><st c="36847">This encoding method creates a monotonic relationship between the categorical variable and the response and therefore makes the variables more adequate for use in </st><span class="No-Break"><st c="37011">linear models.</st></span></p>
			<p><st c="37025">In this recipe, we will</st><a id="_idIndexMarker168"/><st c="37049"> encode categories while follo</st><a id="_idTextAnchor266"/><a id="_idTextAnchor267"/><st c="37079">wing the target value using </st><strong class="source-inline"><st c="37108">pandas</st></strong> <span class="No-Break"><st c="37114">and </st></span><span class="No-Break"><strong class="source-inline"><st c="37119">feature-engine</st></strong></span><span class="No-Break"><st c="37133">.</st></span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor268"/><st c="37134">How to do it...</st></h2>
			<p><st c="37150">First, let’s import the necessary Python libraries and get the </st><span class="No-Break"><st c="37214">dataset ready:</st></span></p>
			<ol>
				<li><st c="37228">Import the required Python libraries, functions, </st><span class="No-Break"><st c="37278">and classes:</st></span><pre class="source-code"><st c="37290">
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="37395">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="37464">test sets:</st></span><pre class="source-code"><st c="37474">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="37660">Let’s determine the mean target value per category in </st><strong class="source-inline"><st c="37715">A7</st></strong><st c="37717">, then sort the categories from that with the lowest to that with the highest </st><span class="No-Break"><st c="37795">target value:</st></span><pre class="source-code"><st c="37808">
y_train.groupby(X_train["A7"]).mean().sort_values()</st></pre><p class="list-inset"><st c="37860">The following is the output of the </st><span class="No-Break"><st c="37896">preceding command:</st></span></p><pre class="source-code"><strong class="bold"><st c="37914">A7</st></strong>
<strong class="bold"><st c="37917">o          0.000000</st></strong>
<strong class="bold"><st c="37928">ff         0.146341</st></strong>
<strong class="bold"><st c="37940">j          0.200000</st></strong>
<strong class="bold"><st c="37951">dd         0.400000</st></strong>
<strong class="bold"><st c="37963">v          0.418773</st></strong>
<strong class="bold"><st c="37974">bb         0.512821</st></strong>
<strong class="bold"><st c="37986">h          0.603960</st></strong>
<strong class="bold"><st c="37997">n          0.666667</st></strong>
<strong class="bold"><st c="38008">z          0.714286</st></strong>
<strong class="bold"><st c="38019">Missing    1.000000</st></strong>
<strong class="bold"><st c="38036">Name: target, dtype: float64</st></strong></pre></li>				<li><st c="38065">Now, let’s </st><a id="_idIndexMarker169"/><st c="38077">repeat the computation in </st><em class="italic"><st c="38103">Step 3</st></em><st c="38109">, but this time, let’s retain the ordered </st><span class="No-Break"><st c="38151">category names:</st></span><pre class="source-code"><st c="38166">
ordered_labels = y_train.groupby(
    X_train["A7"]).mean().sort_values().index</st></pre><p class="list-inset"><st c="38242">To display the output of the preceding command, we can execute </st><strong class="source-inline"><st c="38306">print(ordered_labels)</st></strong><st c="38327">: </st><strong class="source-inline"><st c="38330">Index(['o', 'ff', 'j', 'dd', 'v', 'bb', 'h', 'n', 'z', 'Missing'], </st></strong><span class="No-Break"><strong class="source-inline"><st c="38397">dtype='object', name='A7')</st></strong></span><span class="No-Break"><st c="38423">.</st></span></p></li>				<li><st c="38424">Let’s create a dictionary of category-to-integer pairs, using the ordered list we created in </st><span class="No-Break"><em class="italic"><st c="38518">Step 4</st></em></span><span class="No-Break"><st c="38524">:</st></span><pre class="source-code"><st c="38526">
ordinal_mapping = {
    k: i for i, k in enumerate(ordered_labels, 0)
}</st></pre><p class="list-inset"><st c="38594">We can visualize the result of the preceding code by </st><span class="No-Break"><st c="38648">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="38658">print(ordinal_mapping)</st></strong></span><span class="No-Break"><st c="38680">:</st></span></p><pre class="source-code"><strong class="bold"><st c="38682">{'o': 0, 'ff': 1, 'j': 2, 'dd': 3, 'v': 4, 'bb': 5, 'h': 6, 'n': 7, 'z': 8, 'Missing': 9}</st></strong></pre></li>				<li><st c="38771">Let’s use</st><a id="_idIndexMarker170"/><st c="38781"> the dictionary we created in </st><em class="italic"><st c="38811">Step 5</st></em><st c="38817"> to replace the categories in </st><strong class="source-inline"><st c="38847">A7</st></strong><st c="38849"> in a copy of </st><span class="No-Break"><st c="38863">the datasets:</st></span><pre class="source-code"><st c="38876">
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()
X_train_enc["A7"] = X_train_enc["A7"].map(
    ordinal_mapping)
X_test_enc["A7"] = X_test_enc["A7"].map(
    ordinal_mapping)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="39050">Note</st></p>
			<p class="callout"><st c="39055">If the test set contains a category that is not present in the train set, the preceding code will </st><span class="No-Break"><st c="39154">introduce </st></span><span class="No-Break"><strong class="source-inline"><st c="39164">np.nan</st></strong></span><span class="No-Break"><st c="39170">.</st></span></p>
			<p class="list-inset"><st c="39171">To visualize the effect of this encoding, let’s plot the relationship of the categories of the </st><strong class="source-inline"><st c="39267">A7</st></strong><st c="39269"> variable with the target before and after </st><span class="No-Break"><st c="39312">the encoding.</st></span></p>
			<ol>
				<li value="7"><st c="39325">Let’s plot the mean target response per category of the </st><span class="No-Break"><strong class="source-inline"><st c="39382">A7</st></strong></span><span class="No-Break"><st c="39384"> variable:</st></span><pre class="source-code"><st c="39394">
y_train.groupby(X_train["A7"]).mean().plot()
plt.title("Relationship between A7 and the target")
plt.ylabel("Mean of target")
plt.show()</st></pre><p class="list-inset"><st c="39531">We can see the non-monotonic relationship between</st><a id="_idTextAnchor269"/><st c="39581"> categories of </st><strong class="source-inline"><st c="39596">A7</st></strong><st c="39598"> and the target in the </st><span class="No-Break"><st c="39621">following plot:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B22396_02_08.jpg" alt="Figure 2.8 – Mean target value per category of A7 before the encoding"/><st c="39636"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="39747">Figure 2.8 – Mean target value per category of A7 before the encoding</st></p>
			<ol>
				<li value="8"><st c="39816">Let’s plot</st><a id="_idIndexMarker171"/><st c="39827"> the mean target value per category in the </st><span class="No-Break"><st c="39870">encoded variable:</st></span><pre class="source-code"><st c="39887">
y_train.groupby(X_train_enc["A7"]).mean().plot()
plt.title("Relationship between A7 and the target")
plt.ylabel("Mean of target")
plt.show()</st></pre><p class="list-inset"><st c="40028">The encoded variable shows a monotonic relationship with the target – the higher the mean targe</st><a id="_idTextAnchor270"/><st c="40124">t value, the higher the digit assigned to </st><span class="No-Break"><st c="40167">the category:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B22396_02_09.jpg" alt="Figure 2.9 – Mean target value per category of A7 after the encoding."/><st c="40180"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="40273">Figure 2.9 – Mean target value per category of A7 after the encoding.</st></p>
			<p class="list-inset"><st c="40342">Now, let’s </st><a id="_idIndexMarker172"/><st c="40354">perform ordered ordinal encoding </st><span class="No-Break"><st c="40387">using </st></span><span class="No-Break"><strong class="source-inline"><st c="40393">feature-engine</st></strong></span><span class="No-Break"><st c="40407">.</st></span></p>
			<ol>
				<li value="9"><st c="40408">Let’s import </st><span class="No-Break"><st c="40422">the encoder:</st></span><pre class="source-code"><st c="40434">
from f</st><a id="_idTextAnchor271"/><st c="40441">eature_engine.encoding import OrdinalEncoder</st></pre></li>				<li><st c="40486">Next, let’s set up the encoder so that it assigns integers based on the target mean value to all categorical variables in </st><span class="No-Break"><st c="40609">the dataset:</st></span><pre class="source-code"><st c="40621">
ordinal_enc = OrdinalEncoder(
    encoding_method="ordered",
    variables=None)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="40694">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="40699">OrdinalEncoder()</st></strong><st c="40716"> will find and encode all categorical variables automatically. </st><st c="40779">To restrict the encoding to a subset of variables, pass their names in a list to the </st><strong class="source-inline"><st c="40864">variables</st></strong><st c="40873"> argument. </st><st c="40884">To encode numerical variables, </st><span class="No-Break"><st c="40915">set </st></span><span class="No-Break"><strong class="source-inline"><st c="40919">ignore_format=True</st></strong></span><span class="No-Break"><st c="40937">.</st></span></p>
			<ol>
				<li value="11"><st c="40938">Let’s fit the encoder to the train set so that it finds the categorical variables, and then stores the </st><a id="_idIndexMarker173"/><st c="41042">category and </st><span class="No-Break"><st c="41055">integer m</st><a id="_idTextAnchor272"/><st c="41064">appings:</st></span><pre class="source-code"><st c="41073">
ordinal_enc.fit(X_train, y_train)</st></pre></li>				<li><st c="41107">Finally, let’s replace the categories with numbers in the train and </st><span class="No-Break"><st c="41176">test sets:</st></span><pre class="source-code"><st c="41186">
X_train_enc = ordinal_enc.transform(X_train)
X_test_enc = ordinal_enc.transform(X_test)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="41274">Note</st></p>
			<p class="callout"><st c="41279">You’ll find the digits that will replace each category in the </st><span class="No-Break"><strong class="source-inline"><st c="41342">encoder_dict_</st></strong></span><span class="No-Break"><st c="41355"> attribute.</st></span></p>
			<p><st c="41366">Check out the outp</st><a id="_idTextAnchor273"/><a id="_idTextAnchor274"/><st c="41385">ut of the transformation by </st><span class="No-Break"><st c="41414">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="41424">X_train_enc.head()</st></strong></span><span class="No-Break"><st c="41442">.</st></span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor275"/><st c="41443">How it works...</st></h2>
			<p><st c="41459">In this recipe, we replaced the categories with integers according to the </st><span class="No-Break"><st c="41534">target mean.</st></span></p>
			<p><st c="41546">In the first part of this recipe, we worked with the </st><strong class="source-inline"><st c="41600">A7</st></strong><st c="41602"> categorical variable. </st><st c="41625">With </st><strong class="source-inline"><st c="41630">pandas</st></strong><st c="41636">’ </st><strong class="source-inline"><st c="41639">groupby()</st></strong><st c="41648">, we grouped the data based on the categories of </st><strong class="source-inline"><st c="41697">A7</st></strong><st c="41699">, and with </st><strong class="source-inline"><st c="41710">pandas</st></strong><st c="41716">’ </st><strong class="source-inline"><st c="41719">mean()</st></strong><st c="41725">, we determined the mean value of the target for each of those categories. </st><st c="41800">Next, we ordered the categories with </st><strong class="source-inline"><st c="41837">pandas</st></strong><st c="41843">’ </st><strong class="source-inline"><st c="41846">sort_values()</st></strong><st c="41859"> from the ones with the lowest to the ones with the highest target mean response. </st><st c="41941">The output of this operation was a </st><strong class="source-inline"><st c="41976">pandas</st></strong><st c="41982"> series, with the categories as indices and the target mean as values. </st><st c="42053">With </st><strong class="source-inline"><st c="42058">pandas</st></strong><st c="42064">’ </st><strong class="source-inline"><st c="42067">index</st></strong><st c="42072">, we captured the ordered categories in an array; then, with Python dictionary comprehension, we created a dictionary of category-to-integer pairs. </st><st c="42220">Finally, we used this dictionary to replace the category with integers using </st><span class="No-Break"><strong class="source-inline"><st c="42297">pandas</st></strong></span><span class="No-Break"><st c="42303">’ </st></span><span class="No-Break"><strong class="source-inline"><st c="42306">map()</st></strong></span><span class="No-Break"><st c="42311">.</st></span></p>
			<p class="callout-heading"><st c="42312">Note</st></p>
			<p class="callout"><st c="42317">To avoid data leakage, we determine the ca</st><a id="_idTextAnchor276"/><st c="42360">tegory-to-integer mappings from the </st><span class="No-Break"><st c="42397">train set.</st></span></p>
			<p><st c="42407">To perform the encoding with </st><strong class="source-inline"><st c="42437">feature-engine</st></strong><st c="42451">, we used </st><strong class="source-inline"><st c="42461">OrdinalEncoder()</st></strong><st c="42477">, setting the </st><strong class="source-inline"><st c="42491">encoding_method</st></strong><st c="42506"> to </st><strong class="source-inline"><st c="42510">ordered</st></strong><st c="42517">. We left the argument variables set to </st><strong class="source-inline"><st c="42557">None</st></strong><st c="42561"> so that the encoder automatically detects all categorical variables in the dataset. </st><st c="42646">With </st><strong class="source-inline"><st c="42651">fit()</st></strong><st c="42656">, the encoder</st><a id="_idIndexMarker174"/><st c="42669"> found the categorical variables and assigned digits to their categories according to the target mean value. </st><st c="42778">The categorical variables’ names and dictionaries with category-to-digit pairs were stored in the </st><strong class="source-inline"><st c="42876">variables_</st></strong><st c="42886"> and </st><strong class="source-inline"><st c="42891">encoder_dict_</st></strong><st c="42904"> attributes, respectively. </st><st c="42931">Finally, using </st><strong class="source-inline"><st c="42946">transform()</st></strong><st c="42957">, we replaced the categories with digit</st><a id="_idTextAnchor277"/><a id="_idTextAnchor278"/><st c="42996">s in the train and test sets, returning </st><span class="No-Break"><strong class="source-inline"><st c="43037">pandas</st></strong></span><span class="No-Break"><st c="43043"> DataFrames.</st></span></p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor279"/><st c="43055">See also</st></h2>
			<p><st c="43064">For an implementation of this recipe with Category Encoders, visit this book’s GitHub </st><span class="No-Break"><st c="43151">repository: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-05-Ordered-ordinal-encoding.ipynb"><span class="No-Break"><st c="43163">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-cate</st><span id="_idTextAnchor280"/><span id="_idTextAnchor281"/><st c="43267">gorical-encoding/Recipe-05-Ordered-ordinal-encod</st><span id="_idTextAnchor282"/><st c="43316">ing.ipynb</st></span></a><span class="No-Break"><st c="43326">.</st></span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor283"/><st c="43327">Implementing target mean encoding</st></h1>
			<p><strong class="bold"><st c="43361">Mean encoding</st></strong><st c="43375"> or </st><strong class="bold"><st c="43379">target encoding</st></strong><st c="43394"> maps </st><a id="_idIndexMarker175"/><st c="43400">each category to the probability estimate of the target attribute. </st><st c="43467">If the target is binary, the numerical mapping is the posterior probability of the target conditioned to the value of the category. </st><st c="43599">If the target is continuous, the numerical representation is given by the expected value of the target given the value of </st><span class="No-Break"><st c="43721">the category.</st></span></p>
			<p><st c="43734">In its simplest form, the</st><a id="_idIndexMarker176"/><st c="43760"> numerical representation for each category is given by the mean value of the target variable for a particular category group. </st><st c="43887">For example, if we have a </st><strong class="source-inline"><st c="43913">City</st></strong><st c="43917"> variable, with the categories of </st><strong class="source-inline"><st c="43951">London</st></strong><st c="43957">, </st><strong class="source-inline"><st c="43959">Manchester</st></strong><st c="43969">, and </st><strong class="source-inline"><st c="43975">Bristol</st></strong><st c="43982">, and we want to predict the default rate (the target takes values of </st><strong class="source-inline"><st c="44052">0</st></strong><st c="44053"> and </st><strong class="source-inline"><st c="44058">1</st></strong><st c="44059">); if the default rate for </st><strong class="source-inline"><st c="44086">London</st></strong><st c="44092"> is 30%, we replace </st><strong class="source-inline"><st c="44112">London</st></strong><st c="44118"> with </st><strong class="source-inline"><st c="44124">0.3</st></strong><st c="44127">; if the default rate for </st><strong class="source-inline"><st c="44154">Manchester</st></strong><st c="44164"> is 20%, we replace </st><strong class="source-inline"><st c="44184">Manchester</st></strong><st c="44194"> with </st><a id="_idTextAnchor284"/><strong class="source-inline"><st c="44200">0.2</st></strong><st c="44203">; and so on. </st><st c="44217">If the target is continuous – say we want to predict income – then we would replace </st><strong class="source-inline"><st c="44301">London</st></strong><st c="44307">, </st><strong class="source-inline"><st c="44309">Manchester</st></strong><st c="44319">, and </st><strong class="source-inline"><st c="44325">Bristol</st></strong><st c="44332"> with the mean income earned in </st><span class="No-Break"><st c="44364">each city.</st></span></p>
			<p><st c="44374">In mathematical terms, if the target is binary, the replacement value, </st><em class="italic"><st c="44446">S</st></em><st c="44447">, is determined </st><span class="No-Break"><st c="44463">like so:</st></span></p>
			<p><img src="image/1.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.483em;height:1.147em;width:5.200em"/><st c="44471"/></p>
			<p><st c="44480">Here, the numerator is the number of observations with a target value of </st><em class="italic"><st c="44553">1</st></em><st c="44554"> for category </st><em class="italic"><st c="44568">i</st></em><st c="44569"> and the denominator is the number of observations with a category value </st><span class="No-Break"><st c="44642">of </st></span><span class="No-Break"><em class="italic"><st c="44645">i</st></em></span><span class="No-Break"><st c="44646">.</st></span></p>
			<p><st c="44647">If the target is continuous, </st><em class="italic"><st c="44677">S</st></em><st c="44678">, this is determined by the </st><span class="No-Break"><st c="44706">following formula:</st></span></p>
			<p><img src="image/2.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.568em;height:1.834em;width:2.732em"/><st c="44724"/></p>
			<p><st c="44764">Here, the</st><a id="_idIndexMarker177"/><st c="44773"> numerator is the sum of the target across observations in category </st><em class="italic"><st c="44841">i</st></em><st c="44842"> and </st><img src="image/3.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.655em"/><st c="44847"/><st c="44848"> is the total number of observations in </st><span class="No-Break"><st c="44888">category </st></span><span class="No-Break"><em class="italic"><st c="44897">i</st></em></span><span class="No-Break"><st c="44898">.</st></span></p>
			<p><st c="44899">These formulas provide a good approximation of the target estimate if there is a sufficiently large number of observations with each category value – in other words, if </st><img src="image/4.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.701em"/><st c="45069"/><st c="45080"> is large. </st><st c="45090">However, in many datasets, there will be categories present in a few observations. </st><st c="45173">In these cases, target estimates derived from the precedent formulas can </st><span class="No-Break"><st c="45246">be unreliable.</st></span></p>
			<p><st c="45260">To mitigate poor estimates returned for rare categories, the target estimates can be determined as a mixture of two probabilities: those returned by the preceding formulas and the prior probability of the target based on the entire training. </st><st c="45503">The two probabilities are </st><em class="italic"><st c="45529">blended</st></em><st c="45536"> using a weighting factor, which is a function of the category </st><span class="No-Break"><st c="45599">group size:</st></span></p>
			<p><img src="image/5.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.651em;height:1.810em;width:9.447em"/><st c="45610"/></p>
			<p><st c="45633">In this formula, </st><img src="image/6.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.817em"/><st c="45650"/><st c="45675"> is the total number of cases where the target takes a value of </st><em class="italic"><st c="45738">1</st></em><st c="45739">, </st><em class="italic"><st c="45741">N</st></em><st c="45742"> is the size of the train set, and </st><em class="italic"><st c="45777">𝜆</st></em><st c="45779"> is the </st><span class="No-Break"><st c="45787">weighting factor.</st></span></p>
			<p><st c="45804">When the category group is large, </st><em class="italic"><st c="45839">𝜆</st></em><st c="45841"> tends to </st><em class="italic"><st c="45851">1</st></em><st c="45852">, so more weight is given to the first term of the equation. </st><st c="45913">When the category </st><a id="_idTextAnchor285"/><st c="45931">group size is small, then </st><em class="italic"><st c="45957">𝜆</st></em><st c="45959"> tends to </st><em class="italic"><st c="45969">0</st></em><st c="45970">, so the estimate is mostly driven by the second term of the equation – that is, the target’s prior probability. </st><st c="46083">In other words, if the group size is small, knowing the value of the category does not tell us anything about the value of </st><span class="No-Break"><st c="46206">the target.</st></span></p>
			<p><st c="46217">The weighting factor, </st><em class="italic"><st c="46240">𝜆</st></em><st c="46242">, is determined differently in different open-source implementations. </st><st c="46312">In Category Encoders, </st><em class="italic"><st c="46334">𝜆</st></em><st c="46336"> is a function of the group size, </st><em class="italic"><st c="46370">k</st></em><st c="46371">, and a smoothing parameter, </st><em class="italic"><st c="46400">f</st></em><st c="46401">, which controls the rate of transition between the first </st><a id="_idTextAnchor286"/><st c="46459">and second term of the </st><span class="No-Break"><st c="46482">preceding equation:</st></span></p>
			<p><img src="image/7.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.543em;height:1.508em;width:4.858em"/><st c="46501"/></p>
			<p><st c="46520">Here, </st><em class="italic"><st c="46526">k</st></em><st c="46527"> is half of the minimal size for which we </st><em class="italic"><st c="46569">fully trust</st></em><st c="46580"> the first term of the equation. </st><st c="46613">The </st><em class="italic"><st c="46617">f</st></em><st c="46618"> parameter is selected by the user either arbitrarily or </st><span class="No-Break"><st c="46675">with optimization.</st></span></p>
			<p><st c="46693">In </st><strong class="source-inline"><st c="46697">scikit-learn</st></strong><st c="46709"> and </st><strong class="source-inline"><st c="46714">feature-engine</st></strong><st c="46728">, </st><em class="italic"><st c="46730">𝜆</st></em><st c="46732"> is a function of the target variance for the entire dataset and within the category, and is determined </st><span class="No-Break"><st c="46836">as follows:</st></span></p>
			<p><img src="image/8.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;λ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.451em;height:1.430em;width:4.673em"/><st c="46847"/></p>
			<p><st c="46861">Here, </st><em class="italic"><st c="46867">t</st></em><st c="46868"> is the target</st><a id="_idIndexMarker178"/><st c="46882"> variance in the entire dataset and </st><em class="italic"><st c="46918">s</st></em><st c="46919"> is the target variance within the category. </st><st c="46964">Both implementations are equivalent, but it is important to know the equations because they will help you set up the parameters in </st><span class="No-Break"><st c="47095">the transformers.</st></span></p>
			<p class="callout-heading"><st c="47112">Note</st></p>
			<p class="callout"><st c="47117">Mean encoding was designed to encode highly cardinal categorical variables without expanding the feature space. </st><st c="47230">For more details, check out the following article: Micci-Barreca D. </st><st c="47298">A., </st><em class="italic"><st c="47302">Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems</st></em><st c="47408">. ACM SIGKDD Explorations </st><span class="No-Break"><st c="47434">Newsletter, 2001.</st></span></p>
			<p><st c="47451">In this recipe, we will perf</st><a id="_idTextAnchor287"/><a id="_idTextAnchor288"/><st c="47480">orm mean encoding using </st><strong class="source-inline"><st c="47505">scikit-learn</st></strong> <span class="No-Break"><st c="47517">and </st></span><span class="No-Break"><strong class="source-inline"><st c="47522">feature-engine</st></strong></span><span class="No-Break"><st c="47536">.</st></span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor289"/><st c="47537">How to do it...</st></h2>
			<p><st c="47553">Let’s begin with </st><span class="No-Break"><st c="47571">this recipe:</st></span></p>
			<ol>
				<li><st c="47583">Import </st><strong class="source-inline"><st c="47591">pandas</st></strong><st c="47597"> and the data </st><span class="No-Break"><st c="47611">split function:</st></span><pre class="source-code"><st c="47626">
import pandas as pd
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="47699">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="47768">test sets:</st></span><pre class="source-code"><st c="47778">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="47964">Let’s import the transformers </st><span class="No-Break"><st c="47995">from </st></span><span class="No-Break"><strong class="source-inline"><st c="48000">scikit-learn</st></strong></span><span class="No-Break"><st c="48012">:</st></span><pre class="source-code"><st c="48014">
from sklearn.preprocessing import TargetEncoder
from sklearn.compose import ColumnTransformer</st></pre></li>				<li><st c="48108">Let’s </st><a id="_idIndexMarker179"/><st c="48115">make a list with the names of the </st><span class="No-Break"><st c="48149">categorical variables:</st></span><pre class="source-code"><st c="48171">
cat_vars = X_train.select_dtypes(
    include="O").columns.to_list()</st></pre></li>				<li><st c="48236">Let’s set up the encoder to use the target variance to determine the weighting factor, as described at the beginning of </st><span class="No-Break"><st c="48357">the recipe:</st></span><pre class="source-code"><st c="48368">
enc = TargetEncoder(smooth="auto", random_state=9)</st></pre></li>				<li><st c="48419">Let’s restrict the imputation to </st><span class="No-Break"><st c="48453">categorical variables:</st></span><pre class="source-code"><st c="48475">
ct = ColumnTransformer(
    [("encoder", enc, cat_vars)],
    remainder="passthrough",
).set_output(transform="pandas")</st></pre></li>				<li><st c="48587">Let’s fit the encoder and transform </st><span class="No-Break"><st c="48624">the datasets:</st></span><pre class="source-code"><st c="48637">
X_train_enc = ct.fit_transform(X_train, y_train)
X_test_enc = ct.transform(X_test)</st></pre><p class="list-inset"><st c="48720">Check out the result by </st><span class="No-Break"><st c="48745">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="48755">X_train_enc.head()</st></strong></span><span class="No-Break"><st c="48773">.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="48774">Note</st></p>
			<p class="callout"><st c="48779">The </st><strong class="source-inline"><st c="48784">fit_transform()</st></strong><st c="48799"> method of </st><strong class="source-inline"><st c="48810">scikit-learn</st></strong><st c="48822">’s </st><strong class="source-inline"><st c="48826">TargetEncoder()</st></strong><st c="48841"> is not equivalent to applying </st><strong class="source-inline"><st c="48872">fit().transform()</st></strong><st c="48889">. With </st><strong class="source-inline"><st c="48896">fit_transform()</st></strong><st c="48911">, the resulting dataset is encoded based on partial fits over the training folds of a cross-validation scheme. </st><st c="49022">This functionality was intentionally designed to prevent overfitting the machine learning model to the </st><span class="No-Break"><st c="49125">train set.</st></span></p>
			<p class="list-inset"><st c="49135">Now, let’s perform target encoding </st><span class="No-Break"><st c="49171">with </st></span><span class="No-Break"><strong class="source-inline"><st c="49176">feature-engine</st></strong></span><span class="No-Break"><st c="49190">:</st></span></p>
			<ol>
				<li value="8"><st c="49192">Let’s import</st><a id="_idIndexMarker180"/> <span class="No-Break"><st c="49204">the encoder:</st></span><pre class="source-code"><st c="49217">
from feature_engine.encoding import MeanEncoder</st></pre></li>				<li><st c="49265">Let’s set up the target mean encoder to encode all categorical variables while </st><span class="No-Break"><st c="49345">applying smoothing:</st></span><pre class="source-code"><st c="49364">
mean_enc = MeanEncoder(smoothing="auto",
    variables=None)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="49421">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="49426">MeanEncoder()</st></strong><st c="49440"> does not apply smoothing by default. </st><st c="49478">Make sure you set it to </st><strong class="source-inline"><st c="49502">auto</st></strong><st c="49506"> or to an integer to control the blend b</st><a id="_idTextAnchor290"/><st c="49546">etween prior and posterior </st><span class="No-Break"><st c="49574">target estimates.</st></span></p>
			<ol>
				<li value="10"><st c="49591">Let’s fit the transformer to the train set so that it learns and stores the mean target value per category </st><span class="No-Break"><st c="49699">per variable:</st></span><pre class="source-code"><st c="49712">
mean_enc.fit(X_train, y_train)</st></pre></li>				<li><st c="49743">Finally, let’s encode the train and </st><span class="No-Break"><st c="49780">test sets:</st></span><pre class="source-code"><st c="49790">
X_train_enc = mean_enc.transform(X_train)
X_test_enc = mean_enc.transform(X_test)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="49872">Note</st></p>
			<p class="callout"><st c="49877">The category-to-number pairs are stored as a dictionary of dictionaries in the </st><strong class="source-inline"><st c="49957">encoder_dict_</st></strong><st c="49970"> attribute. </st><st c="49982">To disp</st><a id="_idTextAnchor291"/><a id="_idTextAnchor292"/><st c="49989">lay the stored parameters, </st><span class="No-Break"><st c="50017">execute </st></span><span class="No-Break"><strong class="source-inline"><st c="50025">mean_enc.encoder_dict_.</st></strong></span></p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor293"/><st c="50048">How it works…</st></h2>
			<p><st c="50062">In this recipe, we</st><a id="_idIndexMarker181"/><st c="50081"> replaced the categories with the mean target value using </st><strong class="source-inline"><st c="50139">scikit-learn</st></strong> <span class="No-Break"><st c="50151">and </st></span><span class="No-Break"><strong class="source-inline"><st c="50156">feature-engine</st></strong></span><span class="No-Break"><st c="50170">.</st></span></p>
			<p><st c="50171">To encode with </st><strong class="source-inline"><st c="50187">scikit-learn</st></strong><st c="50199">, we used </st><strong class="source-inline"><st c="50209">TargetEncoder()</st></strong><st c="50224">, leaving the </st><strong class="source-inline"><st c="50238">smooth</st></strong><st c="50244"> parameter to its default value of </st><strong class="source-inline"><st c="50279">auto</st></strong><st c="50283">. Like this, the transformer used the target variance to determine the weighting factor for the blend of probabilities. </st><st c="50403">With </st><strong class="source-inline"><st c="50408">fit()</st></strong><st c="50413">, the transformer learned the value it should use to replace the categories, and with </st><strong class="source-inline"><st c="50499">transform()</st></strong><st c="50510">, it replaced </st><span class="No-Break"><st c="50524">the categories.</st></span></p>
			<p><st c="50539">Note that for </st><strong class="source-inline"><st c="50554">TargetEncoder()</st></strong><st c="50569">, the </st><strong class="source-inline"><st c="50575">fit()</st></strong><st c="50580"> method followed by </st><strong class="source-inline"><st c="50600">transform()</st></strong><st c="50611"> do not return the same dataset as the </st><strong class="source-inline"><st c="50650">fit_transform()</st></strong><st c="50665">method. </st><st c="50674">The latter encodes the training set based on mappings found with cross-validation. </st><st c="50757">The idea is to use </st><strong class="source-inline"><st c="50776">fit_transform()</st></strong><st c="50791"> within a pipeline, so the machine learning model does not overfit. </st><st c="50859">However, and here is where it gets confusing, the mappings stored in the </st><strong class="source-inline"><st c="50932">encodings_</st></strong><st c="50942"> attribute are the same after </st><strong class="source-inline"><st c="50972">fit()</st></strong><st c="50977"> and </st><strong class="source-inline"><st c="50982">fit_transform()</st></strong><st c="50997">, and this is done intentionally so that when we apply </st><strong class="source-inline"><st c="51052">transform()</st></strong><st c="51063"> to a new dataset, we obtain the same result regardless of whether we apply </st><strong class="source-inline"><st c="51139">fit()</st></strong><st c="51144"> or </st><strong class="source-inline"><st c="51148">fit_transform()</st></strong><st c="51163">to the </st><span class="No-Break"><st c="51171">training set.</st></span></p>
			<p class="callout-heading"><st c="51184">Note</st></p>
			<p class="callout"><st c="51189">Unseen categories are encoded with the target mean by </st><strong class="source-inline"><st c="51244">scikit-learn</st></strong><st c="51256">’s </st><strong class="source-inline"><st c="51260">TargetEncoder()</st></strong><st c="51275">. </st><strong class="source-inline"><st c="51277">feature-engine</st></strong><st c="51291">’s </st><strong class="source-inline"><st c="51295">MeanEncoder()</st></strong><st c="51308"> can either return an error, replace the unseen categories with </st><strong class="source-inline"><st c="51372">nan</st></strong><st c="51375">, or with the </st><span class="No-Break"><st c="51389">target mean.</st></span></p>
			<p><st c="51401">To perform the target encoding with </st><strong class="source-inline"><st c="51438">feature-engine</st></strong><st c="51452">, we used </st><strong class="source-inline"><st c="51462">MeanEncoder(),</st></strong><st c="51476"> setting the </st><strong class="source-inline"><st c="51489">smoothing</st></strong><st c="51498"> parameter to </st><strong class="source-inline"><st c="51512">auto</st></strong><st c="51516">. With </st><strong class="source-inline"><st c="51523">fit()</st></strong><st c="51528">, the transformer found and stored the categorical variables and the values to encode each category. </st><st c="51629">With </st><strong class="source-inline"><st c="51634">transform()</st></strong><st c="51645">, it replace</st><a id="_idTextAnchor294"/><a id="_idTextAnchor295"/><st c="51657">d the categories with numbers, returning </st><span class="No-Break"><strong class="source-inline"><st c="51699">pandas</st></strong></span><span class="No-Break"><st c="51705"> DataFrames.</st></span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor296"/><st c="51717">There’s more…</st></h2>
			<p><st c="51731">If you want to implement target encoding with </st><strong class="source-inline"><st c="51778">pandas</st></strong><st c="51784"> or Category Encoders, check out the notebook in the accompanying GitHub </st><span class="No-Break"><st c="51857">repository: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb"><span class="No-Break"><st c="51869">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb</st></span></a><span class="No-Break"><st c="52026">.</st></span></p>
			<p><st c="52027">There is an alternative way to return </st><em class="italic"><st c="52066">better</st></em><st c="52072"> target estimates when the category groups are small. </st><st c="52126">The replacement valu</st><a id="_idTextAnchor297"/><st c="52146">e for each category is determined </st><span class="No-Break"><st c="52181">as follows:</st></span></p>
			<p><img src="image/9.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.600em;height:1.884em;width:6.799em"/><st c="52192"/></p>
			<p><st c="52214">Here, </st><img src="image/10.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.415em;height:0.863em;width:2.452em"/><st c="52220"/><st c="52227">is the target mean for category </st><em class="italic"><st c="52259">i</st></em><st c="52260"> and </st><img src="image/11.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.672em"/><st c="52265"/><st c="52266"> is the number of observations with category </st><em class="italic"><st c="52311">i</st></em><st c="52312">. The </st><a id="_idIndexMarker182"/><st c="52318">target prior is given by </st><em class="italic"><st c="52343">pY</st></em><st c="52345"> and </st><em class="italic"><st c="52350">m</st></em><st c="52351"> is the weighting factor. </st><st c="52377">With this adjustment, the only parameter that we have </st><a id="_idTextAnchor298"/><st c="52431">to set is the weight, </st><em class="italic"><st c="52453">m</st></em><st c="52454">. If </st><em class="italic"><st c="52459">m</st></em><st c="52460"> is large, then more importance is given to the target’s prior probability. </st><st c="52536">This adjustment affects target estimates for all categories but mostly for those with fewer observations because, in such cases, </st><em class="italic"><st c="52665">m</st></em><st c="52666"> could be much larger than </st><img src="image/12.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.683em"/><st c="52693"/><st c="52694"> in the </st><span class="No-Break"><st c="52702">formula’s denominator.</st></span></p>
			<p class="callout-heading"><st c="52724">Note</st></p>
			<p class="callout"><st c="52729">This method is a good alternative to Category Encoders’ </st><strong class="source-inline"><st c="52786">TargetEncoder()</st></strong><st c="52801"> because, in Category Encoders’ implementation of target encoding, we need to optimize two parameters instead of one (as we did with </st><strong class="source-inline"><st c="52934">feature-engine</st></strong><st c="52948"> and </st><strong class="source-inline"><st c="52953">scikit-learn</st></strong><st c="52965">) to control </st><span class="No-Break"><st c="52979">the smoothing.</st></span></p>
			<p><st c="52993">For an implementation of this encoding method using </st><strong class="source-inline"><st c="53046">MEstimateEncoder()</st></strong><st c="53064">, visit this book’s GitHub </st><span class="No-Break"><st c="53091">repository: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb"><span class="No-Break"><st c="53103">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb</st></span></a><span class="No-Break"><st c="53260">.</st></span></p>
			<h1 id="_idParaDest-85"><st c="53261">E</st><a id="_idTextAnchor299"/><st c="53263">ncoding with Weight of E</st><a id="_idTextAnchor300"/><st c="53287">vidence</st></h1>
			<p><strong class="bold"><st c="53295">Weight of Evidence</st></strong><st c="53314"> (</st><strong class="bold"><st c="53316">WoE</st></strong><st c="53319">) was</st><a id="_idIndexMarker183"/><st c="53325"> developed primarily for credit and financial industries to facilitate variable screening and exploratory </st><a id="_idIndexMarker184"/><st c="53431">analysis and to build more predictive linear models to evaluate the risk of </st><span class="No-Break"><st c="53507">loan defaults.</st></span></p>
			<p><st c="53521">The WoE is computed from the basic </st><span class="No-Break"><st c="53557">odds ratio:</st></span></p>
			<p><img src="image/13.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;WoE&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.675em;height:1.858em;width:14.321em"/><st c="53568"/></p>
			<p><st c="53632">Here, positive and negative refer to the values of the target being </st><em class="italic"><st c="53700">1</st></em><st c="53701"> or </st><em class="italic"><st c="53705">0</st></em><st c="53706">, respectively. </st><st c="53722">The proportion of positive cases per category is determined as the sum of positive cases per category group divided by the total positive cases in the training set. </st><st c="53887">The proportion of negative </st><a id="_idIndexMarker185"/><st c="53914">cases per category is determined as the sum of negative cases per category group divided by the total number of negative observations in the </st><span class="No-Break"><st c="54055">training set.</st></span></p>
			<p><st c="54068">WoE has the </st><span class="No-Break"><st c="54081">following characteristics:</st></span></p>
			<ul>
				<li><st c="54107">WoE = </st><em class="italic"><st c="54114">0</st></em><st c="54115"> if </st><em class="italic"><st c="54119">p(positive)</st></em><st c="54130"> / </st><em class="italic"><st c="54133">p(negative)</st></em><st c="54144"> = </st><em class="italic"><st c="54147">1</st></em><st c="54148">; that is, if the outcome </st><span class="No-Break"><st c="54174">is random</st></span></li>
				<li><st c="54183">WoE &gt; </st><em class="italic"><st c="54190">0</st></em><st c="54191"> if </st><em class="italic"><st c="54195">p(positive)</st></em><st c="54206"> &gt; </st><span class="No-Break"><em class="italic"><st c="54209">p(negative)</st></em></span></li>
				<li><st c="54220">WoE &lt; </st><em class="italic"><st c="54227">0</st></em><st c="54228"> if </st><em class="italic"><st c="54232">p(negative)</st></em><st c="54243"> &gt; </st><span class="No-Break"><em class="italic"><st c="54246">p(positive)</st></em></span></li>
			</ul>
			<p><st c="54257">This allows us to directly visualize the predictive power of the category in the variable: the higher the WoE, the more likely the event will occur. </st><st c="54407">If the WoE is positive, the event is likely </st><span class="No-Break"><st c="54451">to occur.</st></span></p>
			<p><st c="54460">Logistic regression models a binary response, </st><em class="italic"><st c="54507">Y</st></em><st c="54508">, based on </st><em class="italic"><st c="54519">X</st></em><st c="54520"> predictor variables, assuming that there is a linear relationship between </st><em class="italic"><st c="54595">X</st></em><st c="54596"> and the log of odds </st><span class="No-Break"><st c="54617">of </st></span><span class="No-Break"><em class="italic"><st c="54620">Y</st></em></span><span class="No-Break"><st c="54621">:</st></span></p>
			<p><img src="image/14.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.756em;height:2.039em;width:16.998em"/><st c="54622"/></p>
			<p><st c="54655">Here, </st><em class="italic"><st c="54661">log (p(Y=1)/p(Y=0))</st></em><st c="54680"> is the log of odds. </st><st c="54701">As you can see, the WoE encodes the categories in the same scale – that is, the log of odds – as the outcome of the </st><span class="No-Break"><st c="54817">logistic regression.</st></span></p>
			<p><st c="54837">Therefore, by using WoE, the predictors are prepared and coded on the same scale, and the parameters in the logistic regression model – that is, the coefficients – can be </st><span class="No-Break"><st c="55009">directly compared.</st></span></p>
			<p><st c="55027">In this recipe</st><a id="_idTextAnchor301"/><a id="_idTextAnchor302"/><st c="55042">, we will perform WoE encoding using </st><strong class="source-inline"><st c="55079">pandas</st></strong> <span class="No-Break"><st c="55085">and </st></span><span class="No-Break"><strong class="source-inline"><st c="55090">feature-engine</st></strong></span><span class="No-Break"><st c="55104">.</st></span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor303"/><st c="55105">How to do it...</st></h2>
			<p><st c="55121">Let’s begin by making some imports and preparing </st><span class="No-Break"><st c="55171">the data:</st></span></p>
			<ol>
				<li><st c="55180">Import the required libraries </st><span class="No-Break"><st c="55211">and functions:</st></span><pre class="source-code"><st c="55225">
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="55317">Let’s load </st><a id="_idIndexMarker186"/><st c="55329">the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="55386">test sets:</st></span><pre class="source-code"><st c="55396">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="55582">Let’s get the inverse of the target values to be able to calculate the </st><span class="No-Break"><st c="55654">negative cases:</st></span><pre class="source-code"><st c="55669">
neg_y_train = pd.Series(
    np.where(y_train == 1, 0, 1),
    index=y_train.index
)</st></pre></li>				<li><st c="55746">Let’s determine the number of observations where the target variable takes a value of </st><strong class="source-inline"><st c="55833">1</st></strong> <span class="No-Break"><st c="55834">or </st></span><span class="No-Break"><strong class="source-inline"><st c="55837">0</st></strong></span><span class="No-Break"><st c="55838">:</st></span><pre class="source-code"><st c="55839">
to</st><a id="_idTextAnchor304"/><st c="55842">tal_pos = y_train.sum()
total_neg = neg_y_train.sum()</st></pre></li>				<li><st c="55896">Now, let’s calculate the numerator and denominator of the WoE’s formula, which we discussed earlier in </st><span class="No-Break"><st c="56000">this recipe:</st></span><pre class="source-code"><st c="56012">
pos = y_train.groupby(
    X_train["A1"]).sum() / total_pos
neg = neg_y_train.groupby(
    X_train["A1"]).sum() / total_neg</st></pre></li>				<li><st c="56128">Now, let’s</st><a id="_idIndexMarker187"/><st c="56139"> calculate the WoE </st><span class="No-Break"><st c="56158">per category:</st></span><pre class="source-code"><st c="56171">
woe = np.log(pos/neg)</st></pre><p class="list-inset"><st c="56193">We can display the series with the category to WoE pairs by </st><span class="No-Break"><st c="56254">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="56264">print(woe)</st></strong></span><span class="No-Break"><st c="56274">:</st></span></p><pre class="source-code"><strong class="bold"><st c="56276">A1</st></strong>
<strong class="bold"><st c="56278">Missing    0.203599</st></strong>
<strong class="bold"><st c="56295">a          0.092373</st></strong>
<strong class="bold"><st c="56306">b         -0.042410</st></strong>
<strong class="bold"><st c="56318">dtype: float64</st></strong></pre></li>				<li><st c="56333">Finally, let’s replace the categories of </st><strong class="source-inline"><st c="56375">A1</st></strong><st c="56377"> with the WoE in a copy of </st><span class="No-Break"><st c="56404">the datasets:</st></span><pre class="source-code"><st c="56417">
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()
X_train_enc["A1"] = X_train_enc["A1"].map(woe)
X_test_enc["A1"] = X_test_enc["A1"].map(woe)</st></pre><p class="list-inset"><st c="56565">You can inspect the e</st><a id="_idTextAnchor305"/><st c="56587">ncoded variable by </st><span class="No-Break"><st c="56607">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="56617">X_train_enc["A1"].head()</st></strong></span><span class="No-Break"><st c="56641">.</st></span></p><p class="list-inset"><st c="56642">Now, let’s perform WoE encoding </st><span class="No-Break"><st c="56675">using </st></span><span class="No-Break"><strong class="source-inline"><st c="56681">feature-engine</st></strong></span><span class="No-Break"><st c="56695">.</st></span></p></li>				<li><st c="56696">Let’s import </st><span class="No-Break"><st c="56710">the encoder:</st></span><pre class="source-code">
<strong class="source-inline"><st c="56722">from feature_engine.encoding import WoEEncoder</st></strong></pre></li>				<li><st c="56769">Next, let’s set up the encoder to encode three </st><span class="No-Break"><st c="56817">categorical variables:</st></span><pre class="source-code"><st c="56839">
woe_enc = WoEEncoder(variables = ["A1", "A9", "A12"])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="56893">Note</st></p>
			<p class="callout"><st c="56898">For rare categories, it might happen that </st><strong class="source-inline"><st c="56941">p(0)=0</st></strong><st c="56947"> or </st><strong class="source-inline"><st c="56951">p(1)=0</st></strong><st c="56957">, and then the division or the logarithm is not defined. </st><st c="57014">To avoid this, group infrequent categories as shown in the </st><em class="italic"><st c="57073">Grouping rare or infrequent </st></em><span class="No-Break"><em class="italic"><st c="57101">categories</st></em></span><span class="No-Break"><st c="57111"> recipe.</st></span></p>
			<ol>
				<li value="10"><st c="57119">Let’s fit the</st><a id="_idIndexMarker188"/><st c="57133"> transformer to the train set so that it learns and stores the WoE of the </st><span class="No-Break"><st c="57207">different categories:</st></span><pre class="source-code"><st c="57228">
woe_enc.fit(X_train, y_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="57258">Note</st></p>
			<p class="callout"><st c="57263">We can display the dictionaries with the categori</st><a id="_idTextAnchor306"/><st c="57313">es to WoE pairs by </st><span class="No-Break"><st c="57333">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="57343">woe_enc.encoder_dict_</st></strong></span><span class="No-Break"><st c="57364">.</st></span></p>
			<ol>
				<li value="11"><st c="57365">Finally, let’s encode the three categorical variables in the train and </st><span class="No-Break"><st c="57437">test sets:</st></span><pre class="source-code"><st c="57447">
X_train_enc = woe_enc.transform(X_train)
X_test_enc = woe_enc.transform(X_test)</st></pre></li>			</ol>
			<p><strong class="source-inline"><st c="57527">feature-engine</st></strong><st c="57542"> returns </st><strong class="source-inline"><st c="57551">pandas</st></strong><st c="57557"> DataFrames, which contain the enco</st><a id="_idTextAnchor307"/><a id="_idTextAnchor308"/><st c="57592">ded categorical variables ready to use in machine </st><span class="No-Break"><st c="57643">learning models.</st></span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor309"/><st c="57659">How it works...</st></h2>
			<p><st c="57675">In this recipe, we encoded categorical variables using the WoE with </st><strong class="source-inline"><st c="57744">pandas</st></strong> <span class="No-Break"><st c="57750">and </st></span><span class="No-Break"><strong class="source-inline"><st c="57755">feature-engine</st></strong></span><span class="No-Break"><st c="57769">.</st></span></p>
			<p><st c="57770">We combined the use of </st><strong class="source-inline"><st c="57794">pandas</st></strong><st c="57800">’ </st><strong class="source-inline"><st c="57803">sum()</st></strong><st c="57808"> and </st><strong class="source-inline"><st c="57813">groupby()</st></strong><st c="57822"> and </st><strong class="source-inline"><st c="57827">numpy</st></strong><st c="57832">’s </st><strong class="source-inline"><st c="57836">log()</st></strong><st c="57841"> to determine the WoE as we described at the beginning of </st><span class="No-Break"><st c="57899">this recipe.</st></span></p>
			<p><st c="57911">Next, we automated the procedure with </st><strong class="source-inline"><st c="57950">feature-engine</st></strong><st c="57964">. We used the </st><strong class="source-inline"><st c="57978">WoEEncoder()</st></strong><st c="57990">, which learned the WoE per category with the </st><strong class="source-inline"><st c="58036">fit()</st></strong><st c="58041"> method, and then used </st><strong class="source-inline"><st c="58064">tra</st><a id="_idTextAnchor310"/><a id="_idTextAnchor311"/><st c="58067">nsform()</st></strong><st c="58076"> to replace the categories with the </st><span class="No-Break"><st c="58112">corresponding numbers.</st></span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor312"/><st c="58134">See also</st></h2>
			<p><st c="58143">For an implementation of WoE with Category Encoders, visit this book’s GitHub </st><span class="No-Break"><st c="58222">repository: </st></span><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-07-Weight-of-evidence.ipynb"><span class="No-Break"><st c="58234">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/m</st><span id="_idTextAnchor313"/><span id="_idTextAnchor314"/><st c="58325">ain/ch02-categorical-encoding/Recipe-07-Weight-of-evidence.ipynb</st></span></a><span class="No-Break"><st c="58390">.</st></span></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor315"/><st c="58391">Grouping rare or infrequent categories</st></h1>
			<p><st c="58430">Rare categories are those </st><a id="_idIndexMarker189"/><st c="58457">presen</st><a id="_idTextAnchor316"/><st c="58463">t only in a small fraction of the observations. </st><st c="58512">There is no rule of thumb to determine how small a small fraction is, b</st><a id="_idTextAnchor317"/><st c="58583">ut typically, any value below 5% can be </st><span class="No-Break"><st c="58624">considered rare.</st></span></p>
			<p><st c="58640">Infrequent labels often</st><a id="_idIndexMarker190"/><st c="58664"> appear only on the train set or only on the test set, thus making the algorithms prone to overfitting or being unable to score an observation. </st><st c="58808">In addition, when encoding categories to numbers, we only create mappings for those categories observed in the train set, so we won’t know how to encode new labels. </st><st c="58973">T</st><a id="_idTextAnchor318"/><st c="58974">o avoid these complications, we can group infrequent categories into a single category called </st><strong class="source-inline"><st c="59068">Rare</st></strong> <span class="No-Break"><st c="59072">or </st></span><span class="No-Break"><strong class="source-inline"><st c="59076">Other</st></strong></span><span class="No-Break"><st c="59081">.</st></span></p>
			<p><st c="59082">In this recipe, </st><a id="_idTextAnchor319"/><a id="_idTextAnchor320"/><st c="59099">we will group infrequent categories using </st><strong class="source-inline"><st c="59141">pandas</st></strong> <span class="No-Break"><st c="59147">and </st></span><span class="No-Break"><strong class="source-inline"><st c="59152">feature-engine</st></strong></span><span class="No-Break"><st c="59166">.</st></span></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor321"/><st c="59167">How to do it...</st></h2>
			<p><st c="59183">First, let’s import the necessary Python libraries and get the </st><span class="No-Break"><st c="59247">dataset ready:</st></span></p>
			<ol>
				<li><st c="59261">Import the necessary Python libraries, functions, </st><span class="No-Break"><st c="59312">and classes:</st></span><pre class="source-code"><st c="59324">
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from feature_engine.encoding import RareLabelEncoder</st></pre></li>				<li><st c="59469">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="59538">test sets:</st></span><pre class="source-code"><st c="59548">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
    da</st><a id="_idTextAnchor322"/><st c="59688">ta["target"],
    test_size=0</st><a id="_idTextAnchor323"/><st c="59714">.3,
    random_state=0,
)</st></pre></li>				<li><st c="59735">Let’s </st><a id="_idIndexMarker191"/><st c="59742">capture the fraction of observations per category in </st><strong class="source-inline"><st c="59795">A7</st></strong><st c="59797"> in </st><span class="No-Break"><st c="59801">a variable:</st></span><pre class="source-code"><st c="59812">
freqs = X_train["A7"].value_counts(normalize=True)</st></pre><p class="list-inset"><st c="59863">We can see the percentage of observations per category of </st><strong class="source-inline"><st c="59922">A7</st></strong><st c="59924">, expressed as decimals, in the following output after </st><span class="No-Break"><st c="59979">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="59989">print(freqs)</st></strong></span><span class="No-Break"><st c="60001">:</st></span></p><pre class="source-code"><strong class="bold"><st c="60003">v 0.573499</st></strong>
<strong class="bold"><st c="60013">h 0.209110</st></strong>
<strong class="bold"><st c="60024">ff 0.084886</st></strong>
<strong class="bold"><st c="60036">bb 0.080745</st></strong>
<strong class="bold"><st c="60048">z 0.014493</st></strong>
<strong class="bold"><st c="60059">dd 0.010352</st></strong>
<strong class="bold"><st c="60071">j 0.010352</st></strong>
<strong class="bold"><st c="60082">Missing 0.008282</st></strong>
<strong class="bold"><st c="60099">n 0.006211</st></strong>
<strong class="bold"><st c="60110">o 0.002070</st></strong>
<strong class="bold"><st c="60121">Name: A7, dtype: float64</st></strong></pre><p class="list-inset"><st c="60146">If we consider those labels present in less than 5% of the observations as rar</st><a id="_idTextAnchor324"/><st c="60225">e, then </st><strong class="source-inline"><st c="60234">z</st></strong><st c="60235">, </st><strong class="source-inline"><st c="60237">dd</st></strong><st c="60239">, </st><strong class="source-inline"><st c="60241">j</st></strong><st c="60242">, </st><strong class="source-inline"><st c="60244">Missing</st></strong><st c="60251">, </st><strong class="source-inline"><st c="60253">n</st></strong><st c="60254">, and </st><strong class="source-inline"><st c="60260">o</st><a id="_idTextAnchor325"/></strong><st c="60261"> are </st><span class="No-Break"><st c="60266">rare categories.</st></span></p></li>				<li><st c="60282">Let’s create a list containing the names of the categories present in more than 5% of </st><span class="No-Break"><st c="60369">the observations:</st></span><pre class="source-code"><st c="60386">
frequent_cat = [
    x for x in freqs.loc[freqs &gt; 0.05].index.values]</st></pre><p class="list-inset"><st c="60452">If we execute </st><strong class="source-inline"><st c="60467">print(frequent_cat)</st></strong><st c="60486">, we will see the frequent categories </st><span class="No-Break"><st c="60524">of </st></span><span class="No-Break"><strong class="source-inline"><st c="60527">A7</st></strong></span><span class="No-Break"><st c="60529">:</st></span></p><pre class="source-code"><strong class="bold"><st c="60531">['v', 'h', 'ff', 'bb'].</st></strong></pre></li>				<li><st c="60554">Let’s </st><a id="_idIndexMarker192"/><st c="60561">replace rare labels – that is, those present in &lt;= 5% of the observations – with the </st><strong class="source-inline"><st c="60646">Rare</st></strong><st c="60650"> string in a copy of </st><span class="No-Break"><st c="60671">the datasets:</st></span><pre class="source-code"><st c="60684">
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()
X_train_enc["A7"] = np.where(X_train["A7"].isin(
    frequent_cat), X_train["A7"], "Rare")
X_test_enc["A7"] = np.where(X_test["A7"].isin(
    frequent_cat), X_test["A7"], "Rare")</st></pre></li>				<li><st c="60911">Let’s determine the percentage of observations in the </st><span class="No-Break"><st c="60966">encoded variable:</st></span><pre class="source-code"><st c="60983">
X_train["A7"].value_counts(normalize=True)</st></pre><p class="list-inset"><st c="61026">We can see that the infrequent labels have now been re-grouped into the </st><span class="No-Break"><strong class="source-inline"><st c="61099">Rare</st></strong></span><span class="No-Break"><st c="61103"> category:</st></span></p><pre class="source-code"><strong class="bold"><st c="61113">v       0.573499</st></strong>
<strong class="bold"><st c="61124">h       0.209110</st></strong>
<strong class="bold"><st c="61135">ff      0.084886</st></strong>
<strong class="bold"><st c="61147">bb      0.080745</st></strong>
<strong class="bold"><st c="61159">Rare    0.051760</st></strong>
<strong class="bold"><st c="61173">Name: A7, dtype: f</st><a id="_idTextAnchor326"/><st c="61192">loat64</st></strong></pre><p class="list-inset"><st c="61199">Now, let’s group rare labe</st><a id="_idTextAnchor327"/><st c="61226">ls </st><span class="No-Break"><st c="61230">using </st></span><span class="No-Break"><strong class="source-inline"><st c="61236">feature-engine</st></strong></span><span class="No-Break"><st c="61250">.</st></span></p></li>				<li><st c="61251">Let’s</st><a id="_idIndexMarker193"/><st c="61257"> create a rare label encoder that groups categories present in less than 5% of the observations, provided that the categorical variable has more than four </st><span class="No-Break"><st c="61412">distinct values:</st></span><pre class="source-code"><st c="61428">
rare_encoder = RareLabelEncoder(tol=0.05,
    n_categories=4)</st></pre></li>				<li><st c="61486">Let’s fit the encoder so that it finds the categorical variables and then learns their most </st><span class="No-Break"><st c="61579">frequent categories:</st></span><pre class="source-code"><st c="61599">
rare_encoder.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="61625">Note</st></p>
			<p class="callout"><st c="61630">Upon fitting, the transformer will raise warnings, indicating that many categorical variables have less than four categories, thus their values will not be grouped. </st><st c="61796">The transformer just lets you know that this </st><span class="No-Break"><st c="61841">is happening.</st></span></p>
			<p class="list-inset"><st c="61854">We can display the frequent categories per variable by executing </st><strong class="source-inline"><st c="61920">rare_encoder.encoder_dict_</st></strong><st c="61946">, as well as the variables that will be encoded by </st><span class="No-Break"><st c="61997">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="62007">rare_encoder.variables_</st></strong></span><span class="No-Break"><st c="62030">.</st></span></p>
			<ol>
				<li value="9"><st c="62031">Finally, let’s group rare labels in the train and </st><span class="No-Break"><st c="62082">test sets:</st></span><pre class="source-code"><st c="62092">
X_train_enc = rare_encoder.transform(X_train)
X_test_enc = rare_encoder.transform(X_test)</st></pre></li>			</ol>
			<p><st c="62182">Now that we have grouped rare labels, we are ready to encode the catego</st><a id="_idTextAnchor328"/><a id="_idTextAnchor329"/><st c="62254">rical variables, as we’ve done in the previous recipes in </st><span class="No-Break"><st c="62313">this chapter.</st></span></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor330"/><st c="62326">How it works...</st></h2>
			<p><st c="62342">In this recipe, we grouped infrequent categories using </st><strong class="source-inline"><st c="62398">pandas</st></strong> <span class="No-Break"><st c="62404">and </st></span><span class="No-Break"><strong class="source-inline"><st c="62409">feature-engine</st></strong></span><span class="No-Break"><st c="62423">.</st></span></p>
			<p><st c="62424">We determined</st><a id="_idIndexMarker194"/><st c="62438"> the fraction of observations per category of the </st><strong class="source-inline"><st c="62488">A7</st></strong><st c="62490"> variable using </st><strong class="source-inline"><st c="62506">pandas</st></strong><st c="62512">’ </st><strong class="source-inline"><st c="62515">value_counts()</st></strong><st c="62529"> by setting the </st><strong class="source-inline"><st c="62545">normalize</st></strong><st c="62554"> parameter to </st><strong class="source-inline"><st c="62568">True</st></strong><st c="62572">. Using list comprehension, we captured the names of the variables present in more than 5% of the observations. </st><st c="62684">Finally, u</st><a id="_idTextAnchor331"/><st c="62694">sing NumPy’s </st><strong class="source-inline"><st c="62708">where()</st></strong><st c="62715">, we searched each row of </st><strong class="source-inline"><st c="62741">A7</st></strong><st c="62743">, and if </st><a id="_idTextAnchor332"/><st c="62752">the observation was one of the frequent categories in the list, which we checked using </st><strong class="source-inline"><st c="62839">pandas</st></strong><st c="62845">’ </st><strong class="source-inline"><st c="62848">isin()</st></strong><st c="62854">, its value was kept; otherwise, it was replaced </st><span class="No-Break"><st c="62903">with </st></span><span class="No-Break"><strong class="source-inline"><st c="62908">Rare</st></strong></span><span class="No-Break"><st c="62912">.</st></span></p>
			<p><st c="62913">We automated the preceding steps for multiple categorical variables using </st><strong class="source-inline"><st c="62988">feature-engine</st></strong><st c="63002">’s </st><strong class="source-inline"><st c="63006">RareLabelEncoder()</st></strong><st c="63024">. By setting </st><strong class="source-inline"><st c="63037">tol</st></strong><st c="63040"> to </st><strong class="source-inline"><st c="63044">0.05</st></strong><st c="63048">, we retained categories present in more than 5% of the observations. </st><st c="63118">By setting </st><strong class="source-inline"><st c="63129">n_categories</st></strong><st c="63141"> to </st><strong class="source-inline"><st c="63145">4</st></strong><st c="63146">, we only grouped categories in variables with more than four unique values. </st><st c="63223">With </st><strong class="source-inline"><st c="63228">fit()</st></strong><st c="63233">, the transformer identified the categorical variables and then learned and stored their frequent categories. </st><st c="63343">With </st><strong class="source-inline"><st c="63348">transform</st><a id="_idTextAnchor333"/><a id="_idTextAnchor334"/><st c="63357">()</st></strong><st c="63360">, the transformer replaced infrequent categories with the </st><span class="No-Break"><strong class="source-inline"><st c="63418">Rare</st></strong></span><span class="No-Break"><st c="63422"> string.</st></span></p>
			<h1 id="_idParaDest-92"><st c="63430">Performing b</st><a id="_idTextAnchor335"/><st c="63443">inary encoding</st></h1>
			<p><strong class="bold"><st c="63458">Binary encoding</st></strong><st c="63474"> uses </st><a id="_idIndexMarker195"/><st c="63480">binary code – that is, a sequence of zeroes and ones – to represent the different categories of the variable. </st><st c="63590">How does it work? </st><st c="63608">First, the categories </st><a id="_idIndexMarker196"/><st c="63630">are arbitrarily replaced with ordinal numbers, as shown in the intermediate step of the following table. </st><st c="63735">Then, those numbers are converted into binary code. </st><st c="63787">For example, integer </st><strong class="source-inline"><st c="63808">1</st></strong><st c="63809"> can be represented with the sequence of </st><strong class="source-inline"><st c="63850">1-0</st></strong><st c="63853">, integer </st><strong class="source-inline"><st c="63863">2</st></strong><st c="63864"> with </st><strong class="source-inline"><st c="63870">0-1</st></strong><st c="63873">, integer </st><strong class="source-inline"><st c="63883">3</st></strong><st c="63884"> with </st><strong class="source-inline"><st c="63890">1-1</st></strong><st c="63893">, and integer </st><strong class="source-inline"><st c="63907">0</st></strong><st c="63908"> with </st><strong class="source-inline"><st c="63914">0-0</st></strong><st c="63917">. The digits in the two positions of the binary string become the</st><a id="_idTextAnchor336"/><st c="63982"> columns, which are the encoded representations of the </st><span class="No-Break"><st c="64037">original variable:</st></span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B22396_02_10.jpg" alt="Figure 2.10 – Table showing the steps required for binary encoding the color variable"/><st c="64055"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="64134">Figure 2.10 – Table showing the steps required for binary encoding the color variable</st></p>
			<p><st c="64219">Binary encoding </st><a id="_idIndexMarker197"/><st c="64236">encodes the data in fewer dimensions than one-hot encoding. </st><st c="64296">In our example, the </st><strong class="source-inline"><st c="64316">Color</st></strong><st c="64321"> variable would be encoded into </st><em class="italic"><st c="64353">k-1</st></em><st c="64356"> categories by one-hot encoding – that is, three variables – but with binary encoding, we can represent the variable with only two features. </st><st c="64497">More generally, we determine the number of binary features needed to encode a variable as </st><em class="italic"><st c="64587">log2(number of distinct</st><a id="_idTextAnchor337"/><st c="64610"> categories)</st></em><st c="64622">; in our example, </st><em class="italic"><st c="64641">log2(4) = 2</st></em> <span class="No-Break"><st c="64652">binary features.</st></span></p>
			<p><st c="64669">Binary encoding is an alternative method to one-hot encoding where we do not lose information about the variable, yet we obtain fewer features after the encoding. </st><st c="64833">This is particularly useful when we have highly cardinal variables. </st><st c="64901">For example, if a variable contains 128 unique categories, with one-hot encoding, we would need 127 features to encode the variable, whereas with binary encoding, we will only need </st><em class="italic"><st c="65082">7 (log2(128)=7)</st></em><st c="65097">. Thus, this encoding prevents the feature space from exploding. </st><st c="65162">In addition, binary-encoded features are also suitable for linear models. </st><st c="65236">On the downside, the derived binary features lack human interpretability, so if we need to interpret the decisions made by our models, this encoding method may not be a </st><span class="No-Break"><st c="65405">suitable option.</st></span></p>
			<p><st c="65421">In this rec</st><a id="_idTextAnchor338"/><a id="_idTextAnchor339"/><st c="65433">ipe, we will learn how to perform binary encoding using </st><span class="No-Break"><st c="65490">Category Encoders.</st></span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor340"/><st c="65508">How to do it...</st></h2>
			<p><st c="65524">First, let’s import the necessary Python libraries and get the </st><span class="No-Break"><st c="65588">dataset ready:</st></span></p>
			<ol>
				<li><st c="65602">Import the required Python libraries, functions, </st><span class="No-Break"><st c="65652">and classes:</st></span><pre class="source-code"><st c="65664">
import pandas as pd
from sklearn.model_selection import train_test_split
from category_encoders.binary import BinaryEncoder</st></pre></li>				<li><st c="65788">Let’s load the Credit Approval dataset and divide it into train and </st><span class="No-Break"><st c="65857">test sets:</st></span><pre class="source-code"><st c="65867">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=["target"], axis=1),
</st><a id="_idTextAnchor341"/><st c="66005">    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="66053">Let’s inspect</st><a id="_idIndexMarker198"/><st c="66067"> the unique categories </st><span class="No-Break"><st c="66090">in </st></span><span class="No-Break"><strong class="source-inline"><st c="66093">A7</st></strong></span><span class="No-Break"><st c="66095">:</st></span><pre class="source-code"><st c="66097">
X_train["A7"].unique()</st></pre><p class="list-inset"><st c="66120">In the following output, we can see that </st><strong class="source-inline"><st c="66162">A7</st></strong><st c="66164"> has 10 </st><span class="No-Break"><st c="66172">different categories:</st></span></p><pre class="source-code"><strong class="bold"><st c="66193">array(['v', 'ff', 'h', 'dd', 'z', 'bb', 'j', 'Missing', 'n', 'o'], dtype=object)</st></strong></pre></li>				<li><st c="66274">Let’s create a binary encoder to </st><span class="No-Break"><st c="66308">encode </st></span><span class="No-Break"><strong class="source-inline"><st c="66315">A7</st></strong></span><span class="No-Break"><st c="66317">:</st></span><pre class="source-code"><st c="66319">
encoder = BinaryEncoder(cols=["A7"],
    drop_invariant=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="66377">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="66382">BinaryEncoder()</st></strong><st c="66398">, as well as other encoders from the Category Encoders package, allow us to select the variables to encode. </st><st c="66506">We simply pass the column names in a list to the </st><span class="No-Break"><strong class="source-inline"><st c="66555">cols</st></strong></span><span class="No-Break"><st c="66559"> argument.</st></span></p>
			<ol>
				<li value="5"><st c="66569">Let’s fit the transformer to the train set so that it calculates how many binary variables it needs and creates the variable-to-binary </st><span class="No-Break"><st c="66705">code representations:</st></span><pre class="source-code"><st c="66726">
encoder.fit(X_train)</st></pre></li>				<li><st c="66747">Finally, let’s encode </st><strong class="source-inline"><st c="66770">A7</st></strong><st c="66772"> in the train and </st><span class="No-Break"><st c="66790">test sets:</st></span><pre class="source-code"><st c="66800">
X_train_enc</st><a id="_idTextAnchor342"/><st c="66812"> = encoder.transform(X_train)
X_test_enc = encoder.transform(X_test)</st></pre><p class="list-inset"><st c="66880">We can</st><a id="_idIndexMarker199"/><st c="66887"> display the top rows of the transformed train set </st><a id="_idTextAnchor343"/><st c="66938">by executing </st><strong class="source-inline"><st c="66951">print(X_train_enc.head())</st></strong><st c="66976">, which returns the </st><span class="No-Break"><st c="66996">following output:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B22396_02_11.jpg" alt="Figure 2.11 – DataFrame with the variables after binary encoding"/><st c="67013"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="67333">Figure 2.11 – DataFrame with the variables after binary encoding</st></p>
			<p class="list-inset"><st c="67397">Binary encoding returned four binary variables for </st><strong class="source-inline"><st c="67449">A7</st></strong><st c="67451">, which are </st><strong class="source-inline"><st c="67463">A7_0</st></strong><st c="67467">, </st><strong class="source-inline"><st c="67469">A7_1</st></strong><st c="67473">, </st><strong class="source-inline"><st c="67475">A7_2</st></strong><st c="67479">, and </st><strong class="source-inline"><st c="67485">A</st><a id="_idTextAnchor344"/><a id="_idTextAnchor345"/><st c="67486">7_3</st></strong><st c="67489">, instead of the nine that would have been returned by </st><span class="No-Break"><st c="67544">one-hot encoding.</st></span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor346"/><st c="67561">How it works...</st></h2>
			<p><st c="67577">In this recipe, we performed binary encoding using the Category Encoders package. </st><st c="67660">We used </st><strong class="source-inline"><st c="67668">BinaryEncoder()</st></strong><st c="67683"> to encode the </st><strong class="source-inline"><st c="67698">A</st><a id="_idTextAnchor347"/><st c="67699">7</st></strong><st c="67700"> variable. </st><st c="67711">With the </st><strong class="source-inline"><st c="67720">fit()</st></strong><st c="67725"> method, </st><strong class="source-inline"><st c="67734">BinaryEncoder()</st></strong><st c="67749"> created a mapping from a category to a set of binary columns, and with the </st><strong class="source-inline"><st c="67825">transform()</st></strong> <a id="_idTextAnchor348"/><a id="_idTextAnchor349"/><a id="_idTextAnchor350"/><st c="67836">method, the encoder encoded the </st><strong class="source-inline"><st c="67869">A7</st></strong><st c="67871"> variable in both the train and </st><span class="No-Break"><st c="67903">test sets.</st></span></p>
		</div>
	<div id="charCountTotal" value="67913"/></body></html>