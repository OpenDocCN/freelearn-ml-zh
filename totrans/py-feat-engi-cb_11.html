<html><head></head><body>
		<div id="_idContainer205">
			<h1 id="_idParaDest-297" class="chapter-number"><a id="_idTextAnchor1459"/><st c="0">11</st></h1>
			<h1 id="_idParaDest-298"><a id="_idTextAnchor1460"/><a id="_idTextAnchor1461"/><st c="3">Extracting Features from Text Variables</st></h1>
			<p><st c="43">Text can be one of the variables in our datasets. </st><st c="94">For example, in insurance, information describing the circumstances of an incident can come from free text fields in a form. </st><st c="219">If a company gathers customer reviews, this information will be collected as short pieces of text provided by the users. </st><st c="340">Text data does not show</st><a id="_idIndexMarker827"/><st c="363"> the </st><strong class="bold"><st c="368">tabular</st></strong><st c="375"> pattern of the datasets that we have worked with throughout this book. </st><st c="447">Instead, information in texts can vary in length and content, as well as writing style. </st><st c="535">We can extract a lot of information from text variables to use as predictive features in machine learning models. </st><st c="649">The techniques we will cover in this chapter belong to the realm of </st><strong class="bold"><st c="717">Natural Language Processing</st></strong><st c="744"> (</st><strong class="bold"><st c="746">NLP</st></strong><st c="749">). </st><st c="753">NLP is a subfield of linguistics and computer science. </st><st c="808">It is</st><a id="_idIndexMarker828"/><st c="813"> concerned with the interactions between computer and human language, or, in other words, how to program computers to understand human language. </st><st c="958">NLP includes a multitude of techniques to understand the syntax, semantics, and discourse of text. </st><st c="1057">Therefore, to do this field justice would require an </st><span class="No-Break"><st c="1110">entire book.</st></span></p>
			<p><st c="1122">In this chapter, we will discuss the methods that will allow us to quickly extract features from short pieces of text to complement our predictive models. </st><st c="1278">Specifically, we will discuss how to capture a piece of text’s complexity by looking at some statistical parameters of the text, such as the word length and count, the number of words and unique words used, the number of sentences, and so on. </st><st c="1521">We will use the </st><strong class="source-inline"><st c="1537">pandas</st></strong><st c="1543"> and </st><strong class="source-inline"><st c="1548">scikit-learn</st></strong><st c="1560"> libraries, and we will make a shallow dive into a very useful Python NLP toolkit</st><a id="_idIndexMarker829"/><st c="1641"> called the </st><strong class="bold"><st c="1653">Natural Language </st></strong><span class="No-Break"><strong class="bold"><st c="1670">Toolkit</st></strong></span><span class="No-Break"><st c="1677"> (</st></span><span class="No-Break"><strong class="bold"><st c="1679">NLTK</st></strong></span><span class="No-Break"><st c="1683">).</st></span></p>
			<p><st c="1686">This chapter includes the </st><span class="No-Break"><st c="1713">following recipes:</st></span></p>
			<ul>
				<li><st c="1731">Counting characters, words, </st><span class="No-Break"><st c="1760">and vocabulary</st></span></li>
				<li><st c="1774">Estimating text complexity by </st><span class="No-Break"><st c="1805">counting sentences</st></span></li>
				<li><st c="1823">Creating features with bag-of-words </st><span class="No-Break"><st c="1860">and n-grams</st></span></li>
				<li><st c="1871">Implementing term frequency-inverse </st><span class="No-Break"><st c="1908">document frequency</st></span></li>
				<li><st c="1926">Cleaning and stemming </st><span class="No-Break"><st c="1949">text variabl</st><a id="_idTextAnchor1462"/><a id="_idTextAnchor1463"/><st c="1961">es</st></span></li>
			</ul>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor1464"/><st c="1964">Technical requirements</st></h1>
			<p><st c="1987">In this chapter, we will use the </st><strong class="source-inline"><st c="2021">pandas</st></strong><st c="2027">, </st><strong class="source-inline"><st c="2029">matplotlib</st></strong><st c="2039">, and </st><strong class="source-inline"><st c="2045">scikit-learn</st></strong><st c="2057"> Python libraries. </st><st c="2076">We will also use </st><strong class="source-inline"><st c="2093">NLTK</st></strong><st c="2097">, a comprehensive Python library for NLP and text analysis. </st><st c="2157">You can find the instructions to install </st><strong class="source-inline"><st c="2198">NLTK</st></strong> <span class="No-Break"><st c="2202">at </st></span><a href="http://www.nltk.org/install.html"><span class="No-Break"><st c="2206">http://www.nltk.org/install.html</st></span></a><span class="No-Break"><st c="2238">.</st></span></p>
			<p><st c="2239">If you are using the Python Anaconda distribution, follow the instructions to install </st><strong class="source-inline"><st c="2326">NLTK</st></strong> <span class="No-Break"><st c="2330">at </st></span><a href="https://anaconda.org/anaconda/nltk"><span class="No-Break"><st c="2334">https://anaconda.org/anaconda/nltk</st></span></a><span class="No-Break"><st c="2368">.</st></span></p>
			<p><st c="2369">After you have installed </st><strong class="source-inline"><st c="2395">NLTK</st></strong><st c="2399">, open up a Python console and execute </st><span class="No-Break"><st c="2438">the following:</st></span></p>
			<pre class="console"><st c="2452">
import nltk
nltk.download('punkt')
nltk.download('stopwords')</st></pre>			<p><st c="2514">These commands will download the necessary data for you to be able to run the recipes in this </st><span class="No-Break"><st c="2609">chapter successfully.</st></span></p>
			<p class="callout-heading"><st c="2630">Note</st></p>
			<p class="callout"><st c="2635">If you haven’t downloaded these or the other data sources necessary for </st><strong class="source-inline"><st c="2708">NLTK</st></strong><st c="2712"> functionality, </st><strong class="source-inline"><st c="2728">NLTK</st></strong><st c="2732"> will raise an error. </st><st c="2754">Read the error message carefully because it will direct you to download the data required to run the command that you are trying </st><span class="No-Break"><st c="2883">to execut</st><a id="_idTextAnchor1465"/><a id="_idTextAnchor1466"/><st c="2892">e.</st></span></p>
			<h1 id="_idParaDest-300"><a id="_idTextAnchor1467"/><st c="2895">Counting characters, words, and vocabulary</st></h1>
			<p><st c="2938">One of the </st><a id="_idIndexMarker830"/><st c="2950">sal</st><a id="_idTextAnchor1468"/><st c="2953">ient</st><a id="_idIndexMarker831"/><st c="2958"> characteri</st><a id="_idTextAnchor1469"/><st c="2969">stics of text i</st><a id="_idTextAnchor1470"/><st c="2985">s its </st><a id="_idIndexMarker832"/><st c="2992">complexity. </st><st c="3004">Long descriptions are more likely to contain more information than short descriptions. </st><st c="3091">Texts rich in different, unique words are more likely to be richer in detail than texts that repeat the same words over and over. </st><st c="3221">In the same way, when we speak, we use many short words such as articles and prepositions to build the sentence structure, yet the main concept is often derived from the nouns and adjectives we use, which tend to be longer words. </st><st c="3451">So, as you can see, even without reading the text, we can start inferring how much information the text provides by determining the number of words, the number of unique words (non-repeated occurrences of a word), the lexical diversity, and the length of those words. </st><st c="3719">In this recipe, we will learn how to extract these features from a text variable </st><span class="No-Break"><st c="3800">using </st></span><span class="No-Break"><strong class="source-inline"><st c="3806">pa</st><a id="_idTextAnchor1471"/><a id="_idTextAnchor1472"/><st c="3808">ndas</st></strong></span><span class="No-Break"><st c="3813">.</st></span></p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor1473"/><st c="3814">Getting ready</st></h2>
			<p><st c="3828">We are</st><a id="_idTextAnchor1474"/><st c="3835"> going to use the </st><strong class="bold"><st c="3853">20 Newsgroup</st></strong><st c="3865"> datase</st><a id="_idTextAnchor1475"/><st c="3872">t that come</st><a id="_idTextAnchor1476"/><st c="3884">s with </st><strong class="source-inline"><st c="3892">scikit-learn</st></strong><st c="3904">, which comprises around 18,000 news posts on 20 different topics. </st><st c="3971">More details about this dataset can be found on the </st><span class="No-Break"><st c="4023">following sites:</st></span></p>
			<ul>
				<li><st c="4039">The scikit</st><a id="_idTextAnchor1477"/><st c="4050">-learn dataset </st><span class="No-Break"><st c="4066">website: </st></span><a href="https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset"><span class="No-Break"><st c="4075">https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset</st></span></a></li>
				<li><st c="4150">The hom</st><a id="_idTextAnchor1478"/><st c="4158">e page for the 20 Newsgroup </st><span class="No-Break"><st c="4187">dataset: </st></span><a href="http://qwone.com/~jason/20Newsgroups/"><span class="No-Break"><st c="4196">http://qwone.com/~jason/20Newsgroups/</st></span></a></li>
			</ul>
			<p><st c="4233">Before jumping into the recipe, let’s discuss the features that we are going to derive from these </st><a id="_idIndexMarker833"/><st c="4332">text </st><a id="_idIndexMarker834"/><st c="4337">pieces. </st><st c="4345">We mentioned that longer descriptions, more </st><a id="_idIndexMarker835"/><st c="4389">words in the article, a greater variety of unique words, and longer words tend to correlate with the amount of information that the article provides. </st><st c="4539">Hence, we can capture text complexity by extracting the following information about </st><span class="No-Break"><st c="4623">the text:</st></span></p>
			<ul>
				<li><st c="4632">The total number </st><span class="No-Break"><st c="4650">of characters</st></span></li>
				<li><st c="4663">The total number </st><span class="No-Break"><st c="4681">of words</st></span></li>
				<li><st c="4689">The total number of </st><span class="No-Break"><st c="4710">unique words</st></span></li>
				<li><st c="4722">Lexical diversity (total number of words divided by number of </st><span class="No-Break"><st c="4785">unique words)</st></span></li>
				<li><st c="4798">Word average length (number of characters divided by number </st><span class="No-Break"><st c="4859">of words)</st></span></li>
			</ul>
			<p><st c="4868">In this recipe, we will extract these numerical features using </st><strong class="source-inline"><st c="4932">pandas</st></strong><st c="4938">, which has extensive string processing functionalities that can be accessed via the </st><strong class="source-inline"><st c="5023">str</st></strong><st c="5026"> vectorized string functions </st><span class="No-Break"><st c="5055">for</st><a id="_idTextAnchor1479"/><a id="_idTextAnchor1480"/><st c="5058"> series.</st></span></p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor1481"/><st c="5066">How to do it...</st></h2>
			<p><st c="5082">Let’s begin by loading </st><strong class="source-inline"><st c="5106">pandas</st></strong><st c="5112"> and getting the </st><span class="No-Break"><st c="5129">dataset ready:</st></span></p>
			<ol>
				<li><st c="5143">Load </st><strong class="source-inline"><st c="5149">pandas</st></strong><st c="5155"> and the dataset </st><span class="No-Break"><st c="5172">from </st></span><span class="No-Break"><strong class="source-inline"><st c="5177">scikit-learn</st></strong></span><span class="No-Break"><st c="5189">:</st></span><pre class="source-code"><st c="5191">
import pandas as pd
from sklearn.datasets import fetch_20newsgroups</st></pre></li>				<li><st c="5259">L</st><a id="_idTextAnchor1482"/><st c="5261">et’s load </st><a id="_idTextAnchor1483"/><st c="5271">the train set part of the 20 Newsgrou</st><a id="_idTextAnchor1484"/><st c="5308">p dataset into a </st><span class="No-Break"><strong class="source-inline"><st c="5326">pandas</st></strong></span><span class="No-Break"><st c="5332"> DataFrame:</st></span><pre class="source-code"><st c="5343">
data = fetch_20newsgroups(subset='train')
df = pd.DataFrame(data.data, columns=['text'])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="5432">Tip</st></p>
			<p class="callout"><st c="5436">You can print an example of a text from the DataFrame by executing </st><strong class="source-inline"><st c="5504">print(df['text'][1])</st></strong><st c="5524">. Change the number between </st><strong class="source-inline"><st c="5552">[</st></strong><st c="5553"> and </st><strong class="source-inline"><st c="5558">]</st></strong><st c="5559"> to display different texts. </st><st c="5588">Note how every text description is a single string composed of letters, numbers, punctuation, and spaces. </st><st c="5694">You can check the datatype by </st><span class="No-Break"><st c="5724">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="5734">type(df["text"][1])</st></strong></span><span class="No-Break"><st c="5753">.</st></span></p>
			<p class="list-inset"><st c="5754">Now </st><a id="_idIndexMarker836"/><st c="5759">that </st><a id="_idIndexMarker837"/><st c="5764">we </st><a id="_idIndexMarker838"/><st c="5767">have the text variable in a </st><strong class="source-inline"><st c="5795">pandas</st></strong><st c="5801"> DataFrame, we are ready to extract </st><span class="No-Break"><st c="5837">the features.</st></span></p>
			<ol>
				<li value="3"><st c="5850">Let’s capture the number of characters in each text piece in a </st><span class="No-Break"><st c="5914">new column:</st></span><pre class="source-code"><st c="5925">
df['num_char'] = df['text'].str.len()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="5963">Tip</st></p>
			<p class="callout"><st c="5967">You can remove trailing white spaces in a string, including those from new lines, before counting the number of characters by adding the </st><strong class="source-inline"><st c="6105">strip()</st></strong><st c="6112"> method before the </st><strong class="source-inline"><st c="6131">len()</st></strong><st c="6136"> method, as shown here: </st><strong class="source-inline"><st c="6160">df['num_char'] = </st></strong><span class="No-Break"><strong class="source-inline"><st c="6177">df['text'].str.strip().str.len()</st></strong></span><span class="No-Break"><st c="6209">.</st></span></p>
			<ol>
				<li value="4"><st c="6210">Let’s capture the number of words in each text in a </st><span class="No-Break"><st c="6263">new column:</st></span><pre class="source-code"><st c="6274">
df['num_words'] = df['text'].str.split().str.len()</st></pre><p class="list-inset"><st c="6325">To count words, we use the </st><strong class="source-inline"><st c="6353">pandas</st></strong><st c="6359"> library’s </st><strong class="source-inline"><st c="6370">split()</st></strong><st c="6377"> method, which splits a text at white spaces. </st><st c="6423">Check out the output of </st><strong class="source-inline"><st c="6447">split()</st></strong><st c="6454"> by executing, for instance, </st><strong class="source-inline"><st c="6483">df["text"].loc[1].split()</st></strong><st c="6508"> to separate the words of the second text of </st><span class="No-Break"><st c="6553">the DataFrame.</st></span></p></li>				<li><st c="6567">Let’s capture the number of </st><em class="italic"><st c="6596">unique</st></em><st c="6602"> words in each text in a </st><span class="No-Break"><st c="6627">new column:</st></span><pre class="source-code"><st c="6638">
df['num_vocab']df[
    'text'].str.lower().str.split().apply(
        set).str.len()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="6711">Note</st></p>
			<p class="callout"><st c="6716">Python interprets the same word as two different words if one has a capital letter. </st><st c="6801">To avoid this behavior, we can apply the </st><strong class="source-inline"><st c="6842">lower()</st></strong><st c="6849"> method before the </st><span class="No-Break"><strong class="source-inline"><st c="6868">split()</st></strong></span><span class="No-Break"><st c="6875"> method.</st></span></p>
			<ol>
				<li value="6"><a id="_idTextAnchor1485"/><st c="6883">Let’s create</st><a id="_idIndexMarker839"/><st c="6896"> a </st><a id="_idIndexMarker840"/><st c="6899">feature that captures </st><a id="_idIndexMarker841"/><st c="6921">the lexical diversity – that is, t</st><a id="_idTextAnchor1486"/><st c="6955">he total number of words (</st><em class="italic"><st c="6982">step</st><a id="_idTextAnchor1487"/><st c="6987"> 4</st></em><st c="6989">) compare</st><a id="_idTextAnchor1488"/><st c="6998">d to the number of unique words (</st><span class="No-Break"><em class="italic"><st c="7032">step 5</st></em></span><span class="No-Break"><st c="7039">):</st></span><pre class="source-code"><st c="7042">
df['lexical_div'] = df['num_words'] / df['num_vocab']</st></pre></li>				<li><st c="7096">Let’s calculate the average word length by dividing the number of characters (</st><em class="italic"><st c="7175">step 3</st></em><st c="7182">) by the number of words (</st><span class="No-Break"><em class="italic"><st c="7209">step 4</st></em></span><span class="No-Break"><st c="7216">):</st></span><pre class="source-code"><st c="7219">
df['ave_word_length'] = df[
    'num_char'] / df['num_words']</st></pre><p class="list-inset"><st c="7277">If we execute </st><strong class="source-inline"><st c="7292">df.head()</st></strong><st c="7301">, we will see the first five rows of data with the text and the newly </st><span class="No-Break"><st c="7371">crea</st><a id="_idTextAnchor1489"/><st c="7375">ted features:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B22396_11_01.jpg" alt="Figure 11.1 – A DataFrame with the text variable and features that summarize some of the text’s characteristics"/><st c="7389"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7855">Figure 11.1 – A DataFrame with the text variable and features that summarize some of the text’s characteristics</st></p>
			<p><st c="7966">With that, we have extracted five different features that capture the text complexity, which we can use </st><a id="_idIndexMarker842"/><st c="8071">as </st><a id="_idIndexMarker843"/><st c="8074">inputs for our machine </st><span class="No-Break"><st c="8097">learning </st></span><span class="No-Break"><a id="_idIndexMarker844"/></span><span class="No-Break"><st c="8106">algorithms.</st></span></p>
			<p class="callout-heading"><st c="8117">Note</st></p>
			<p class="callout"><st c="8122">In this recipe, we created new features from the raw data straight away without doing any data cleaning, removing punctuation, or even stemming words. </st><st c="8274">Note that these are steps that are performed ahead of most standard NLP procedures. </st><st c="8358">To learn more about this, visit the </st><em class="italic"><st c="8394">Cleaning and stemming text variables</st></em><st c="8430"> recipe at the end </st><a id="_idTextAnchor1490"/><a id="_idTextAnchor1491"/><st c="8449">of </st><span class="No-Break"><st c="8452">this chapter.</st></span></p>
			<h2 id="_idParaDest-303"><st c="8465">How it</st><a id="_idTextAnchor1492"/><st c="8472"> works...</st></h2>
			<p><st c="8481">In this recipe, we created </st><a id="_idTextAnchor1493"/><st c="8509">five new features that captur</st><a id="_idTextAnchor1494"/><st c="8538">e text complexity by utilizing pandas’ </st><strong class="source-inline"><st c="8578">str</st></strong><st c="8581"> to access the built-in </st><strong class="source-inline"><st c="8605">pandas</st></strong><st c="8611"> functionality to work with strings. </st><st c="8648">We worked with the text column of the </st><strong class="source-inline"><st c="8686">train</st></strong><st c="8691"> subset of the 20 Newsgroup dataset that comes with </st><strong class="source-inline"><st c="8743">scikit-learn</st></strong><st c="8755">. Each row in this dataset is composed of a string </st><span class="No-Break"><st c="8806">with text.</st></span></p>
			<p><st c="8816">We used pandas’ </st><strong class="source-inline"><st c="8833">str</st></strong><st c="8836">, followed by </st><strong class="source-inline"><st c="8850">len()</st></strong><st c="8855">, to count the number of characters in each string – that is, the total number of letters, numbers, symbols, and spaces. </st><st c="8976">We also combined </st><strong class="source-inline"><st c="8993">str.len()</st></strong><st c="9002"> with </st><strong class="source-inline"><st c="9008">str.strip()</st></strong><st c="9019"> to remove trailing white spaces at the beginning and end of the string and in new lines, before counting the number </st><span class="No-Break"><st c="9136">of characters.</st></span></p>
			<p><st c="9150">To count the number of words, we used pandas’ </st><strong class="source-inline"><st c="9197">str</st></strong><st c="9200">, followed by </st><strong class="source-inline"><st c="9214">split()</st></strong><st c="9221">, to divide the string into a list of words. </st><st c="9266">The </st><strong class="source-inline"><st c="9270">split()</st></strong><st c="9277"> method creates a list of words by breaking the string at the white spaces between words. </st><st c="9367">Next, we counted those words with </st><strong class="source-inline"><st c="9401">str.len()</st></strong><st c="9410">, obtaining the number of words </st><span class="No-Break"><st c="9442">per string.</st></span></p>
			<p class="callout-heading"><st c="9453">Note</st></p>
			<p class="callout"><st c="9458">We can change the behavior of </st><strong class="source-inline"><st c="9489">str.split()</st></strong><st c="9500"> by passing a string or character that we would like to use to split the string. </st><st c="9581">For example, </st><strong class="source-inline"><st c="9594">df['text'].str.split(';')</st></strong><st c="9619"> divides a string at each occurrence </st><span class="No-Break"><st c="9656">of </st></span><span class="No-Break"><strong class="source-inline"><st c="9659">;</st></strong></span><span class="No-Break"><st c="9660">.</st></span></p>
			<p><st c="9661">To determine the number of unique words, we used pandas’ </st><strong class="source-inline"><st c="9719">str.split()</st></strong><st c="9730"> function to divide the string into a list of words. </st><st c="9783">Next, we applied the built-in Python </st><strong class="source-inline"><st c="9820">set()</st></strong><st c="9825"> method within pandas’ </st><strong class="source-inline"><st c="9848">apply()</st></strong><st c="9855"> to return a set of words. </st><st c="9882">Remember that a set contains </st><em class="italic"><st c="9911">unique occurrences</st></em><st c="9929"> of the elements in a list – that is, unique words. </st><st c="9981">Next, we counted those words with pandas’ </st><strong class="source-inline"><st c="10023">str.len()</st></strong><st c="10032"> function to return the </st><strong class="bold"><st c="10056">vocabulary</st></strong><st c="10066">, or in other words, the number of unique words in the string. </st><st c="10129">Python interprets words that are written in uppercase differently from those in lowercase; therefore, we introduced pandas’ </st><strong class="source-inline"><st c="10253">lower()</st></strong><st c="10260"> function to set all the characters to lowercase before splitting the string and counting the number of </st><span class="No-Break"><st c="10364">unique words.</st></span></p>
			<p><st c="10377">To create the</st><a id="_idIndexMarker845"/><st c="10391"> lexical </st><a id="_idIndexMarker846"/><st c="10400">diversity and average word length </st><a id="_idTextAnchor1495"/><st c="10434">features, </st><a id="_idTextAnchor1496"/><st c="10444">we</st><a id="_idIndexMarker847"/><st c="10446"> simply pe</st><a id="_idTextAnchor1497"/><st c="10456">rformed a vectorized di</st><a id="_idTextAnchor1498"/><st c="10480">vision of two </st><strong class="source-inline"><st c="10495">pandas</st></strong><st c="10501"> s</st><a id="_idTextAnchor1499"/><st c="10503">eries. </st><st c="10510">That’s it; we created five new features with information about the comp</st><a id="_idTextAnchor1500"/><a id="_idTextAnchor1501"/><st c="10581">lexity of </st><span class="No-Break"><st c="10592">the text.</st></span></p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor1502"/><st c="10601">There’s more...</st></h2>
			<p><st c="10617">We can check out the distribution of the features extracted from text in each of the 20 different news topics present in the dataset by </st><span class="No-Break"><st c="10754">using visualizations</st><a id="_idTextAnchor1503"/><st c="10774">.</st></span></p>
			<p><st c="10775">To make histogram plots of the newly created features, after you run all of the steps in the </st><em class="italic"><st c="10869">How it works...</st></em><st c="10884"> section of this recipe, follow </st><span class="No-Break"><st c="10916">these steps:</st></span></p>
			<ol>
				<li><span class="No-Break"><st c="10928">Import </st></span><span class="No-Break"><strong class="source-inline"><st c="10936">matplotlib</st></strong></span><span class="No-Break"><st c="10946">:</st></span><pre class="source-code"><st c="10948">
import matplotlib.pyplot as plt</st></pre></li>				<li><st c="10980">Add the target with the news topics to the 20 </st><span class="No-Break"><st c="11027">Newsgroup DataFrame:</st></span><pre class="source-code"><st c="11047">
df['target'] = data.target</st></pre></li>				<li><st c="11074">Create a function that displays a histogram of a feature of your choice for each of the </st><span class="No-Break"><st c="11163">news topics:</st></span><pre class="source-code"><st c="11175">
def plot_features(df, text_var):
    nb_rows = 5
    nb_cols = 4
    fig, axs = plt.subplots(
        nb_rows, nb_cols,figsize=(12, 12))
    plt.subplots_adjust(wspace=None, hspace=0.4)
    n = 0
    for i in range(0, nb_rows):
        for j in range(0, nb_cols):
            axs[i, j].hist(
                df[df.target==n][text_var], bins=30)
            axs[i, j].set_title(
                text_var + ' | ' + str(n))
                 n += </st><a id="_idTextAnchor1504"/><st c="11506">1
    plt.show()</st></pre></li>				<li><st c="11518">Run the function </st><a id="_idIndexMarker848"/><st c="11536">for </st><a id="_idIndexMarker849"/><st c="11540">the number of </st><a id="_idIndexMarker850"/><span class="No-Break"><st c="11554">words feature:</st></span><pre class="source-code"><st c="11568">
plot_features(df, 'num_words')</st></pre><p class="list-inset"><st c="11599">The previous command returns the following plot, where you can see the distribution of the number of words in each of the 20 news topics, numbered from 0 to </st><a id="_idTextAnchor1505"/><st c="11757">19 in the </st><span class="No-Break"><st c="11767">plot title:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B22396_11_02.jpg" alt="Figure 11.2 – Histograms showing the distribution of the number of words per text, segregated by topic discussed in each text"/><st c="11778"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="12760">Figure 11.2 – Histograms showing the distribution of the number of words per text, segregated by topic discussed in each text</st></p>
			<p><st c="12885">The number of </st><a id="_idIndexMarker851"/><st c="12900">words</st><a id="_idIndexMarker852"/><st c="12905"> shows a different </st><a id="_idIndexMarker853"/><st c="12924">distribution across the different news topics</st><a id="_idTextAnchor1506"/><st c="12969">. Therefore, this feature is likely useful in a classification algorithm to predic</st><a id="_idTextAnchor1507"/><a id="_idTextAnchor1508"/><st c="13051">t the topic of </st><span class="No-Break"><st c="13067">the text.</st></span></p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor1509"/><st c="13076">See also</st></h2>
			<p><st c="13085">To learn more about pandas’ b</st><a id="_idTextAnchor1510"/><st c="13115">uilt-in string processing functionality </st><span class="No-Break"><st c="13156">visit </st></span><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary"><span class="No-Break"><st c="13162">https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary</st></span></a><span class="No-Break"><st c="13242">.</st></span></p>
			<h1 id="_idParaDest-306"><st c="13243">Estimating text complexity by counting senten</st><a id="_idTextAnchor1511"/><st c="13289">ces</st></h1>
			<p><st c="13293">One aspect of a</st><a id="_idIndexMarker854"/><st c="13309"> piece of text that </st><a id="_idTextAnchor1512"/><st c="13329">we can capture in features is its complexity. </st><st c="13375">Usually, longer descriptions that contain multiple sentences spread over several paragraphs tend to provide more information than descriptions with very few sentences. </st><st c="13543">Therefore, capturing the number of sentences may provide some insight into the amou</st><a id="_idTextAnchor1513"/><st c="13626">nt of information provided by the text. </st><st c="13667">This process is</st><a id="_idIndexMarker855"/><st c="13682"> called </st><strong class="bold"><st c="13690">sentence tokenization</st></strong><st c="13711">. Tokenization is the process of splitting a string into a list of pieces or tokens. </st><st c="13796">In the </st><em class="italic"><st c="13803">Counting characters, words, and vocabulary</st></em><st c="13845"> recipe, we did word tokenization – that is, we divided the string into words. </st><st c="13924">In this recipe, we will divide the string into sentences, and then we will count them. </st><st c="14011">We will use the </st><strong class="source-inline"><st c="14027">NLTK</st></strong><st c="14031"> Python library, which pr</st><a id="_idTextAnchor1514"/><a id="_idTextAnchor1515"/><st c="14056">ovides </st><span class="No-Break"><st c="14064">this functionality.</st></span></p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor1516"/><st c="14083">Getting ready</st></h2>
			<p><st c="14097">In this recipe, we will use the </st><strong class="source-inline"><st c="14130">NLTK</st></strong><st c="14134"> Python library. </st><st c="14151">For guidelines on how to install </st><strong class="source-inline"><st c="14184">NLTK</st></strong><st c="14188">, check out the </st><em class="italic"><st c="14204">Technical requirement</st><a id="_idTextAnchor1517"/><a id="_idTextAnchor1518"/><st c="14225">s</st></em><st c="14227"> section of </st><span class="No-Break"><st c="14239">this chapter.</st></span></p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor1519"/><st c="14252">How to do it...</st></h2>
			<p><st c="14268">Let’s begin by importing the required libraries </st><span class="No-Break"><st c="14317">and dataset:</st></span></p>
			<ol>
				<li><st c="14329">Let’s load </st><strong class="source-inline"><st c="14341">pandas</st></strong><st c="14347">, the sentence tokenizer from </st><strong class="source-inline"><st c="14377">NLTK</st></strong><st c="14381">, and the dataset </st><span class="No-Break"><st c="14399">from </st></span><span class="No-Break"><strong class="source-inline"><st c="14404">scikit-learn</st></strong></span><span class="No-Break"><st c="14416">:</st></span><pre class="source-code"><st c="14418">
import pandas as pd
from nltk.tokenize import sent_tokenize
from sklearn.datasets import fetch_20newsgroups</st></pre></li>				<li><st c="14526">To understand the functionality of the sentence tokenizer from </st><strong class="source-inline"><st c="14590">NLTK</st></strong><st c="14594">, let’s create a variable that contains a string with </st><span class="No-Break"><st c="14648">multiple sentences:</st></span><pre class="source-code"><st c="14667">
text = """
The alarm rang at 7 in the morning as it usually did on Tuesdays. </st><st c="14745">She rolled over, stretched her arm, and stumbled to the button till she finally managed to switch it off. </st><st c="14851">Reluctantly, she got up and went for a shower. </st><st c="14898">The water was cold as the day before the engineers did not manage to get the boiler working. </st><st c="14991">Good thing it was still summer.
</st><st c="15023">Upstairs, her cat waited eagerly for his morning snack. </st><st c="15079">Miaow! </st><st c="15086">He voiced with excitement as he saw her climb the stai</st><a id="_idTextAnchor1520"/><st c="15140">rs.
</st><st c="15145">"""</st></pre></li>				<li><st c="15148">Now, let’s </st><a id="_idIndexMarker856"/><st c="15160">separate the string from </st><em class="italic"><st c="15185">step 2</st></em><st c="15191"> into sentences using </st><strong class="source-inline"><st c="15213">NLTK</st></strong><st c="15217"> library‘s </st><span class="No-Break"><st c="15228">sentence tokenizer:</st></span><pre class="source-code"><st c="15247">
sent_tokenize(text)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="15267">Tip</st></p>
			<p class="callout"><st c="15271">If you encounter an error in </st><em class="italic"><st c="15301">step 3</st></em><st c="15307">, read the error message carefully and download the data source required by </st><strong class="source-inline"><st c="15383">NLTK</st></strong><st c="15387">, as described in the error message. </st><st c="15424">For more details, check out the </st><em class="italic"><st c="15456">Technical </st></em><span class="No-Break"><em class="italic"><st c="15466">requirements</st></em></span><span class="No-Break"><st c="15478"> section.</st></span></p>
			<p class="list-inset"><st c="15487">The sentence tokenizer returns the list of sentences shown in the </st><span class="No-Break"><st c="15554">following output:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="15571">['\nThe alarm rang at 7 in the morning as it usually did on Tuesdays.',</st></strong>
<strong class="bold"><st c="15643"> 'She rolled over,\nstretched her arm, and stumbled to the button till she finally managed to switch it off.',</st></strong>
<strong class="bold"><st c="15753"> 'Reluctantly, she got up and went for a shower.',</st></strong>
<strong class="bold"><st c="15803"> 'The water was cold as the day before the engineers\ndid not manage to get the boiler working.',</st></strong>
<strong class="bold"><st c="15900"> 'Good thing it was still summer.',</st></strong>
<strong class="bold"><st c="15935"> 'Upstairs, her cat waited eagerly for his morning snack.',</st></strong>
<strong class="bold"><st c="15994"> 'Miaow!',</st></strong>
<strong class="bold"><st c="16004"> 'He voiced with excitement\nas he saw her climb the stairs.']</st></strong></pre>			<p class="callout-heading"><st c="16066">Note</st></p>
			<p class="callout"><st c="16071">The escape character followed by the letter, </st><strong class="source-inline"><st c="16117">\n</st></strong><st c="16119">, indic</st><a id="_idTextAnchor1521"/><st c="16126">ates a </st><span class="No-Break"><st c="16134">new line.</st></span></p>
			<ol>
				<li value="4"><st c="16143">Let’s c</st><a id="_idTextAnchor1522"/><st c="16151">ount the number of sentences in the </st><span class="No-Break"><strong class="source-inline"><st c="16188">text</st></strong></span><span class="No-Break"><st c="16192"> variable:</st></span><pre class="source-code"><st c="16202">
len(sent_tokenize(text))</st></pre><p class="list-inset"><st c="16227">The previous command returns </st><strong class="source-inline"><st c="16257">8</st></strong><st c="16258">, which is the number of sentences in our </st><strong class="source-inline"><st c="16300">text</st></strong><st c="16304"> variable. </st><st c="16315">Now, let’s determine the number of sentences in an </st><span class="No-Break"><st c="16366">entire DataFrame.</st></span></p></li>				<li><st c="16383">Let’s load </st><a id="_idIndexMarker857"/><st c="16395">the </st><strong class="source-inline"><st c="16399">train</st></strong><st c="16404"> subset of the 20 Newsgroup dataset into a </st><span class="No-Break"><strong class="source-inline"><st c="16447">pandas</st></strong></span><span class="No-Break"><st c="16453"> DataFrame:</st></span><pre class="source-code"><st c="16464">
data = fetch_20newsgroups(subset='train')
df = pd.DataFrame(data.data, columns=['text'])</st></pre></li>				<li><st c="16553">To speed up the following steps, we will only work with the first </st><strong class="source-inline"><st c="16620">10</st></strong><st c="16622"> rows of </st><span class="No-Break"><st c="16631">the DataFrame:</st></span><pre class="source-code"><st c="16645">
df = df.loc[1:10]</st></pre></li>				<li><st c="16663">Let’s also remove the first part of the text, which contains information about the email sender, subject, and other details that we are not interested in. </st><st c="16819">Most of this information comes before the word </st><strong class="source-inline"><st c="16866">Lines</st></strong><st c="16871"> followed by </st><strong class="source-inline"><st c="16884">:</st></strong><st c="16885">, so let’s split the string at </st><strong class="source-inline"><st c="16916">Lines:</st></strong><st c="16922"> and capture the second part of </st><span class="No-Break"><st c="16954">the string:</st></span><pre class="source-code"><st c="16965">
df['text'] = df['text'].str.split('Lines:').apply(
    lambda x: x[1])</st></pre></li>				<li><st c="17032">Fin</st><a id="_idTextAnchor1523"/><st c="17036">ally, let’s create a v</st><a id="_idTextAnchor1524"/><st c="17059">ariable containing the number of sentences </st><span class="No-Break"><st c="17103">per </st></span><span class="No-Break"><strong class="source-inline"><st c="17107">text</st></strong></span><span class="No-Break"><st c="17111">:</st></span><pre class="source-code"><st c="17113">
df['num_sent'] = df['text'].apply(
    sent_tokenize).apply(len)</st></pre><p class="list-inset"><st c="17174">With the </st><strong class="source-inline"><st c="17184">df</st></strong><st c="17186"> command, you can display the entire DataFrame with the </st><strong class="source-inline"><st c="17242">text</st></strong><st c="17246"> variable and the new feature containing the nu</st><a id="_idTextAnchor1525"/><st c="17293">mber of sentences </st><span class="No-Break"><st c="17312">per text:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B22396_11_03.jpg" alt="Figure 11.3 – A DataFrame with the text variable and the number of sentences per text"/><st c="17321"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17841">Figure 11.3 – A DataFrame with the text variable and the number of sentences per text</st></p>
			<p><st c="17926">Now, we can use this</st><a id="_idIndexMarker858"/><st c="17947"> new feature as input to </st><a id="_idTextAnchor1526"/><a id="_idTextAnchor1527"/><st c="17972">machine </st><span class="No-Break"><st c="17980">learning algorithms.</st></span></p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor1528"/><st c="18000">How it works...</st></h2>
			<p><st c="18016">In this recipe, we separated a string with text into sentences using </st><strong class="source-inline"><st c="18086">sent_tokenizer</st></strong><st c="18100"> from the </st><strong class="source-inline"><st c="18110">NLTK</st></strong><st c="18114"> library. </st><strong class="source-inline"><st c="18124">sent_tokenizer</st></strong><st c="18138"> has been pre-trained to recognize capitalization and different types of punctuation that signal the beginning and the end of </st><span class="No-Break"><st c="18264">a sentence.</st></span></p>
			<p><st c="18275">First, we applied </st><strong class="source-inline"><st c="18294">sent_tokenizer</st></strong><st c="18308"> to a manually created string to become familiar with its functionality. </st><st c="18381">The tokenizer divided the text into a list of eight sentences. </st><st c="18444">We combined the tokenizer with the built-in Python </st><strong class="source-inline"><st c="18495">len()</st></strong><st c="18500"> method to count the number of sentences in </st><span class="No-Break"><st c="18544">the string.</st></span></p>
			<p><st c="18555">Next, we loaded a dataset with text and, to speed up the computation, we only retained the first 10 rows of the DataFrame using pandas’ </st><strong class="source-inline"><st c="18692">loc[]</st></strong><st c="18697"> function. </st><st c="18708">Next, we removed the first part of the text, which contained information about the email sender and subject. </st><st c="18817">To do this, we split the string at </st><strong class="source-inline"><st c="18852">Lines:</st></strong><st c="18858"> using pandas’ </st><strong class="source-inline"><st c="18873">str.split("Lines:")</st></strong><st c="18892"> function, which returned a list with two elements: the strings before and after </st><strong class="source-inline"><st c="18973">Lines:</st></strong><st c="18979">. Utilizing a lambda function within </st><strong class="source-inline"><st c="19016">apply()</st></strong><st c="19023">, we retained the second part of the text – that is, the second string in the list returned </st><span class="No-Break"><st c="19115">by </st></span><span class="No-Break"><strong class="source-inline"><st c="19118">split()</st></strong></span><span class="No-Break"><st c="19125">.</st></span></p>
			<p><st c="19126">Finally, we applied </st><strong class="source-inline"><st c="19147">sent_tokenizer</st></strong><st c="19161"> to each row in the DataFrame with the pandas </st><strong class="source-inline"><st c="19207">apply()</st></strong><st c="19214"> method to separate the strings into sentences, and then applied the built-</st><a id="_idTextAnchor1529"/><st c="19289">in Python </st><strong class="source-inline"><st c="19300">len()</st></strong><st c="19305"> method to th</st><a id="_idTextAnchor1530"/><st c="19318">e list of sentences to return the number of sentences per string. </st><st c="19385">This way, we </st><a id="_idIndexMarker859"/><st c="19398">created a new feature that contained the </st><a id="_idTextAnchor1531"/><a id="_idTextAnchor1532"/><st c="19439">number of sentences </st><span class="No-Break"><st c="19459">per text.</st></span></p>
			<h2 id="_idParaDest-310"><st c="19468">There’s mo</st><a id="_idTextAnchor1533"/><st c="19479">re...</st></h2>
			<p><strong class="source-inline"><st c="19485">NLTK</st></strong><st c="19490"> has functionalities </st><a id="_idIndexMarker860"/><st c="19511">for word tokenization among other useful features, which we can use instead of </st><strong class="source-inline"><st c="19590">pandas</st></strong><st c="19596"> to count and return the number of words. </st><st c="19638">You can find out more about </st><strong class="source-inline"><st c="19666">NLTK</st></strong><st c="19670">’s </st><span class="No-Break"><st c="19674">functionality here:</st></span></p>
			<ul>
				<li><em class="italic"><st c="19693">Python 3 Text Processing with NLTK 3 Cookbook</st></em><st c="19739">, by Jacob Per</st><a id="_idTextAnchor1534"/><st c="19753">kins, </st><span class="No-Break"><st c="19760">Packt Publishing</st></span></li>
				<li><st c="19776">The </st><strong class="source-inline"><st c="19781">NLTK</st></strong><st c="19785"> document</st><a id="_idTextAnchor1535"/><a id="_idTextAnchor1536"/><st c="19794">ation </st><span class="No-Break"><st c="19801">at </st></span><a href="http://www.nltk.org/"><span class="No-Break"><st c="19804">http://www.nltk.org/</st></span></a><span class="No-Break"><st c="19824">.</st></span></li>
			</ul>
			<h1 id="_idParaDest-311"><a id="_idTextAnchor1537"/><st c="19825">Creating features with bag-of-words and n-grams</st></h1>
			<p><st c="19873">A </st><strong class="bold"><st c="19876">Bag-of-Words</st></strong><st c="19888"> (</st><strong class="bold"><st c="19890">BoW</st></strong><st c="19893">)</st><a id="_idTextAnchor1538"/><st c="19895"> is a </st><a id="_idIndexMarker861"/><st c="19901">simplified</st><a id="_idIndexMarker862"/><st c="19911"> representation</st><a id="_idTextAnchor1539"/> <a id="_idIndexMarker863"/><st c="19926">of a piece of</st><a id="_idTextAnchor1540"/><st c="19940"> text that </st><a id="_idIndexMarker864"/><a id="_idTextAnchor1541"/><st c="19951">captures the wor</st><a id="_idTextAnchor1542"/><st c="19967">ds that are present in the text and the number of times each word appears in the text. </st><st c="20055">So, for the text string </st><em class="italic"><st c="20079">Dogs like cats, but cats do not like dogs</st></em><st c="20120">, the derived BoW is </st><span class="No-Break"><st c="20141">as follows:</st></span></p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B22396_11_04.jpg" alt="Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats do not﻿ like dogs"/><st c="20152"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="20191">Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats do not</st><a id="_idTextAnchor1543"/><st c="20270"> like dogs</st></p>
			<p><st c="20280">Here, each word</st><a id="_idTextAnchor1544"/><st c="20296"> becomes a variable, and th</st><a id="_idTextAnchor1545"/><st c="20323">e value of the varia</st><a id="_idTextAnchor1546"/><st c="20344">ble represents the number of times the word appears in the string. </st><st c="20412">As you can see, the BoW captures multiplicity but does not retain word order or grammar. </st><st c="20501">That is why it is a simple, yet useful way of extracting features and capturing some information about the texts we are </st><span class="No-Break"><st c="20621">working with.</st></span></p>
			<p><st c="20634">To capture some syntax, BoW can be used together with </st><strong class="bold"><st c="20689">n-grams</st></strong><st c="20696">. An n-gram is a contiguous </st><a id="_idIndexMarker865"/><st c="20724">sequence of </st><em class="italic"><st c="20736">n</st></em><st c="20737"> items in a </st><a id="_idIndexMarker866"/><st c="20749">given </st><a id="_idIndexMarker867"/><st c="20755">text. </st><st c="20761">Continuing</st><a id="_idIndexMarker868"/><st c="20771"> with the sentence </st><em class="italic"><st c="20790">Dogs like cats, but cats do not like dogs</st></em><st c="20831">, the derived 2-grams are </st><span class="No-Break"><st c="20857">as follows:</st></span></p>
			<ul>
				<li><span class="No-Break"><st c="20868">Dogs like</st></span></li>
				<li><span class="No-Break"><st c="20878">like cats</st></span></li>
				<li><span class="No-Break"><st c="20888">cats but</st></span></li>
				<li><span class="No-Break"><st c="20897">but do</st></span></li>
				<li><span class="No-Break"><st c="20904">do not</st></span></li>
				<li><span class="No-Break"><st c="20911">like dogs</st></span></li>
			</ul>
			<p><st c="20921">We can create, together with a BoW, a bag of n-grams, where the additional variables are given by the 2-grams and the values for each 2-gram are the number of times they appear in each string; for this example, the value is 1. </st><st c="21149">So, our final BoW with 2-grams would look </st><span class="No-Break"><st c="21191">like this:</st></span></p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B22396_11_05.jpg" alt="Figure 11.5 – The BoW with 2-grams"/><st c="21201"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="21305">Figure 11.5 – The BoW with 2-grams</st></p>
			<p><st c="21339">In this recipe, we will learn how to create BoWs with or</st><a id="_idTextAnchor1547"/><a id="_idTextAnchor1548"/><st c="21396"> without n-grams </st><span class="No-Break"><st c="21413">using </st></span><span class="No-Break"><strong class="source-inline"><st c="21419">sciki</st><a id="_idTextAnchor1549"/><st c="21424">t-learn</st></strong></span><span class="No-Break"><st c="21432">.</st></span></p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor1550"/><st c="21433">Getting ready</st></h2>
			<p><st c="21447">Before jumping into this</st><a id="_idTextAnchor1551"/><st c="21472"> recipe, let’s get familiar with some o</st><a id="_idTextAnchor1552"/><st c="21511">f the parameters of a B</st><a id="_idTextAnchor1553"/><st c="21535">oW that we can adjust to make the BoW comprehensive. </st><st c="21589">When creating a BoW over several pieces of text, a new feature is created for each unique word that appears at least once in </st><em class="italic"><st c="21714">any</st></em><st c="21717"> of the text pieces we are analyzing. </st><st c="21755">If the word appears only in one piece of text, it will show a value of 1 for that particular text and 0 for all of the others. </st><st c="21882">Therefore, BoWs tend to be sparse matrices, where most of the values </st><span class="No-Break"><st c="21951">are zeros.</st></span></p>
			<p><st c="21961">The number of columns – that is, the number of words – in a BoW can be quite large if we work with huge text corpora, and even larger if we also include n-grams. </st><st c="22124">To limit the number of columns and the sparsity of the returned matrix, we can retain words that appear across multiple texts; or, in better words, we can retain words that appear in, at least, a certain percentage </st><span class="No-Break"><st c="22339">of texts.</st></span></p>
			<p><st c="22348">To reduce the number of columns and sparsity of the BoW, we should also work with words in the </st><a id="_idIndexMarker869"/><st c="22444">same</st><a id="_idIndexMarker870"/><st c="22448"> case – for </st><a id="_idIndexMarker871"/><st c="22460">example, lowercase – as </st><a id="_idIndexMarker872"/><st c="22484">Python identifies words in a different case as different words. </st><st c="22548">We can also reduce the number of columns and sparsity by </st><a id="_idIndexMarker873"/><st c="22605">removing </st><a id="_idTextAnchor1554"/><strong class="bold"><st c="22614">stop words</st></strong><st c="22624">. Stop words are very frequently used words that make sentences flow, but that do not, per se, carry any useful information. </st><st c="22749">Examples of stop words are pronouns such as I, you, and he, as well as prepositions </st><span class="No-Break"><st c="22833">and articles.</st></span></p>
			<p><st c="22846">In this recipe, we will learn how to set words in lowercase, remove stop words, retain words with a minimum acceptable frequency, and capture n-grams all together with a single transfor</st><a id="_idTextAnchor1555"/><a id="_idTextAnchor1556"/><st c="23032">mer from </st><span class="No-Break"><strong class="source-inline"><st c="23042">scikit-learn</st></strong></span><span class="No-Break"><st c="23054">: </st></span><span class="No-Break"><strong class="source-inline"><st c="23057">CountVectorizer()</st></strong></span><span class="No-Break"><st c="23074">.</st></span></p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor1557"/><st c="23075">How to do it...</st></h2>
			<p><st c="23091">Let’s begin by loading the necessary libraries and getting the </st><span class="No-Break"><st c="23155">dataset ready:</st></span></p>
			<ol>
				<li><st c="23169">Load </st><strong class="source-inline"><st c="23175">pandas</st></strong><st c="23181">, </st><strong class="source-inline"><st c="23183">CountVectorizer</st></strong><st c="23198">, and the dataset </st><span class="No-Break"><st c="23216">from </st></span><span class="No-Break"><strong class="source-inline"><st c="23221">scikit-learn</st></strong></span><span class="No-Break"><st c="23233">:</st></span><pre class="source-code"><st c="23235">
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import (
</st><a id="_idTextAnchor1558"/><st c="23350">    Count</st><a id="_idTextAnchor1559"/><st c="23355">Vectorizer
)</st></pre></li>				<li><st c="23368">Let’s load the </st><a id="_idTextAnchor1560"/><st c="23384">train set part</st><a id="_idTextAnchor1561"/><st c="23398"> of the 20 Newsgroup dataset into a </st><span class="No-Break"><st c="23434">pandas DataFrame:</st></span><pre class="source-code"><st c="23451">
data = fetch_20newsgroups(subset='train')
df = pd.DataFrame(data.data, columns=['text'])</st></pre></li>				<li><st c="23540">To make interpreting the results easier, let’s remove punctuation and numbers from the </st><span class="No-Break"><st c="23628">text variable:</st></span><pre class="source-code"><st c="23642">
df['text'] = df['text'].str.replace(
    ‹[^\w\s]›,››, regex=True).str.replace(
    ‹\d+›,››, regex=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="23740">Note</st></p>
			<p class="callout"><st c="23745">To learn more about regex with Python, follow this </st><span class="No-Break"><st c="23797">link: </st></span><a href="https://docs.python.org/3/howto/regex.html"><span class="No-Break"><st c="23803">https://docs.python.org/3/howto/regex.html</st></span></a></p>
			<ol>
				<li value="4"><st c="23845">Now, let’s </st><a id="_idIndexMarker874"/><st c="23857">set up </st><strong class="source-inline"><st c="23864">CountVectorizer()</st></strong><st c="23881"> so </st><a id="_idIndexMarker875"/><st c="23885">that, before </st><a id="_idIndexMarker876"/><st c="23898">creating </st><a id="_idIndexMarker877"/><st c="23907">the BoW, it puts the text in lowercase, removes stop words, and retains words that appear in, at least, 5% of the </st><span class="No-Break"><st c="24021">text pieces:</st></span><pre class="source-code"><st c="24033">
vectorizer = CountVectorizer(
    lowercase=True,
    stop_words='english',
    ngram_range=(1, 1),
    min_df=0.05)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="24134">Note</st></p>
			<p class="callout"><st c="24139">To introduce n-grams as part of the returned columns, we can change the value of </st><strong class="source-inline"><st c="24221">ngrams_range</st></strong><st c="24233"> to, for example, </st><strong class="source-inline"><st c="24251">(1,2)</st></strong><st c="24256">. The tuple provides the lower and upper boundaries of the range of n-values for different n-grams. </st><st c="24356">In the case of </st><strong class="source-inline"><st c="24371">(1,2)</st></strong><st c="24376">, </st><strong class="source-inline"><st c="24378">CountVectorizer()</st></strong><st c="24395"> will return single words and arrays of two </st><span class="No-Break"><st c="24439">consecutive words.</st></span></p>
			<ol>
				<li value="5"><st c="24457">Let’s fit </st><strong class="source-inline"><st c="24468">CountVectorizer()</st></strong><st c="24485"> so that it learns which words should be used in </st><span class="No-Break"><st c="24534">the BoW:</st></span><pre class="source-code"><st c="24542">
vectorizer.fit(df['text'])</st></pre></li>				<li><st c="24569">Now, let’s create </st><span class="No-Break"><st c="24588">the BoW:</st></span><pre class="source-code"><st c="24596">
X = vectorizer.</st><a id="_idTextAnchor1562"/><st c="24612">transform(df['text'])</st><a id="_idTextAnchor1563"/></pre></li>				<li><st c="24634">Finally, let’s capture</st><a id="_idTextAnchor1564"/><st c="24657"> the BoW</st><a id="_idTextAnchor1565"/><st c="24665"> in a DataFrame with the corresponding </st><span class="No-Break"><st c="24704">feature names:</st></span><pre class="source-code"><st c="24718">
bagofwords = pd.DataFrame(
    X.toarray(),
    columns = vectorizer.get_feature_names_out()
)</st></pre><p class="list-inset"><st c="24805">With </st><a id="_idIndexMarker878"/><st c="24811">that, we</st><a id="_idIndexMarker879"/><st c="24819"> have</st><a id="_idIndexMarker880"/><st c="24824"> created </st><a id="_idIndexMarker881"/><st c="24833">a </st><strong class="source-inline"><st c="24835">pandas</st></strong><st c="24841"> DataFrame that contains words as columns and the number of times they appeared in each text as values. </st><st c="24945">You can in</st><a id="_idTextAnchor1566"/><st c="24955">spect the result by </st><span class="No-Break"><st c="24976">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="24986">bagofwords.head()</st></strong></span><span class="No-Break"><st c="25003">:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B22396_11_06.jpg" alt="Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset"/><st c="25005"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="25367">Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset</st></p>
			<p><st c="25445">We can use</st><a id="_idTextAnchor1567"/><a id="_idTextAnchor1568"/><st c="25456"> this BoW as input for a machine </st><span class="No-Break"><st c="25489">learning model.</st></span></p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor1569"/><st c="25504">How it works...</st></h2>
			<p><st c="25520">scikit-learn’s </st><strong class="source-inline"><st c="25536">CountVectorizer()</st></strong><st c="25553"> converts a collection of text documents into a matrix of token counts. </st><st c="25625">These tokens can be individual words or arrays of two or more consecutive words – that is, n-grams. </st><st c="25725">In this recipe, we created a B</st><a id="_idTextAnchor1570"/><st c="25755">oW from a text variable in </st><span class="No-Break"><st c="25783">a D</st><a id="_idTextAnchor1571"/><st c="25786">ataFrame.</st></span></p>
			<p><st c="25796">We loaded the 20 Newsgroup text data</st><a id="_idTextAnchor1572"/><st c="25833">set from </st><strong class="source-inline"><st c="25843">scikit-l</st><a id="_idTextAnchor1573"/><st c="25851">earn</st></strong><st c="25856"> and removed punctuation and numbers from the text rows using pandas’ </st><strong class="source-inline"><st c="25926">replace()</st></strong><st c="25935"> function, which can be accessed through pandas’ </st><strong class="source-inline"><st c="25984">str</st></strong><st c="25987"> module, to replace digits, </st><strong class="source-inline"><st c="26015">'\d+'</st></strong><st c="26020">, or symbols, </st><strong class="source-inline"><st c="26034">'[^\w\s]'</st></strong><st c="26043">, with empty strings, </st><strong class="source-inline"><st c="26065">''</st></strong><st c="26067">. Then, we used </st><strong class="source-inline"><st c="26083">CountVectorizer()</st></strong><st c="26100"> to create the BoW. </st><st c="26120">We set the </st><strong class="source-inline"><st c="26131">lowercase</st></strong><st c="26140"> parameter to </st><strong class="source-inline"><st c="26154">True</st></strong><st c="26158"> to put the words in lowercase before extracting the BoW. </st><st c="26216">We set the </st><strong class="source-inline"><st c="26227">stop_words</st></strong><st c="26237"> argument to </st><strong class="source-inline"><st c="26250">english</st></strong><st c="26257"> to ignore stop words – that is, to avoid stop words in the BoW. </st><st c="26322">We set </st><strong class="source-inline"><st c="26329">ngram_range</st></strong><st c="26340"> to the </st><strong class="source-inline"><st c="26348">(1,1)</st></strong><st c="26353"> tuple to return only single words as columns. </st><st c="26400">Finally, we set </st><strong class="source-inline"><st c="26416">min_df</st></strong><st c="26422"> to </st><strong class="source-inline"><st c="26426">0.05</st></strong><st c="26430"> to return words that appeared in at least 5% of the texts, or, in other words, in 5% of the rows in </st><span class="No-Break"><st c="26531">the DataFrame.</st></span></p>
			<p><st c="26545">After </st><a id="_idIndexMarker882"/><st c="26552">setting </st><a id="_idIndexMarker883"/><st c="26560">up</st><a id="_idIndexMarker884"/><st c="26562"> the</st><a id="_idIndexMarker885"/><st c="26566"> transformer, we used the </st><strong class="source-inline"><st c="26592">fit()</st></strong><st c="26597"> method to allow the transformer to find the words that fulfill the preceding criteria. </st><st c="26685">Finally, using the </st><strong class="source-inline"><st c="26704">transform()</st></strong><st c="26715"> method, the transformer returned an object containing the BoW with its fea</st><a id="_idTextAnchor1574"/><a id="_idTextAnchor1575"/><st c="26790">ture names, which we captured in a </st><span class="No-Break"><strong class="source-inline"><st c="26826">pandas</st></strong></span><span class="No-Break"><st c="26832"> DataFrame.</st></span></p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor1576"/><st c="26843">See also</st></h2>
			<p><st c="26852">For more de</st><a id="_idTextAnchor1577"/><st c="26864">tails about </st><strong class="source-inline"><st c="26877">CountVectorizer()</st></strong><st c="26894">, visit the </st><strong class="source-inline"><st c="26906">scikit-learn</st></strong><st c="26918"> library’s documentation </st><span class="No-Break"><st c="26943">at </st></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"><span class="No-Break"><st c="26946">https://scikit-learn.org/stable/modules/generated/s</st><span id="_idTextAnchor1578"/><span id="_idTextAnchor1579"/><st c="26997">klearn.feature_extraction.text.CountVectorizer.html</st></span></a><span class="No-Break"><st c="27049">.</st></span></p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor1580"/><st c="27050">Implementing term frequency-inverse document frequency</st></h1>
			<p><strong class="bold"><st c="27105">Term Frequency-Inverse Document Freq</st><a id="_idTextAnchor1581"/><st c="27142">uency</st></strong><st c="27148"> (</st><strong class="bold"><st c="27150">TF-IDF</st></strong><st c="27156">) is a numerical statistic that captures </st><a id="_idIndexMarker886"/><st c="27198">how relevant a word is in a document considering the entire collection of documents. </st><st c="27283">What does this mean? </st><st c="27304">Some words will appear a lot within a text document as well as across documents, such as the English words </st><em class="italic"><st c="27411">the</st></em><st c="27414">, </st><em class="italic"><st c="27416">a</st></em><st c="27417">, and </st><em class="italic"><st c="27423">is</st></em><st c="27425">, for example. </st><st c="27440">These words generally convey little information about the actual content of the document and don’t make the text stand out from the crowd. </st><st c="27579">TF-IDF provides a way to </st><em class="italic"><st c="27604">weigh</st></em><st c="27609"> the importance of a word by considering how many times it appears in a document with regards to how often it appears across documents. </st><st c="27745">Hence, commonly occurring words such as </st><em class="italic"><st c="27785">the</st></em><st c="27788">, </st><em class="italic"><st c="27790">a</st></em><st c="27791">, or </st><em class="italic"><st c="27796">is</st></em><st c="27798"> will have a low weight, and words that are more specific to a topic, such as </st><em class="italic"><st c="27876">leopard</st></em><st c="27883">, will have a </st><span class="No-Break"><st c="27897">higher weight.</st></span></p>
			<p><st c="27911">TF-IDF is the product of two </st><a id="_idIndexMarker887"/><st c="27941">statistics: </st><strong class="bold"><st c="27953">Term Frequency</st></strong><st c="27967"> (</st><strong class="bold"><st c="27969">tf</st></strong><st c="27971">) and </st><strong class="bold"><st c="27978">Inverse Document Frequency</st></strong><st c="28004"> (</st><strong class="bold"><st c="28006">idf</st></strong><st c="28009">), represented </st><a id="_idIndexMarker888"/><st c="28025">as follows: </st><strong class="bold"><st c="28037">tf-idf = td × idf</st></strong><st c="28054">. tf is, in its simplest form, the count of the word in an individual text. </st><st c="28130">So, for term </st><em class="italic"><st c="28143">t</st></em><st c="28144">, the tf is calculated as </st><em class="italic"><st c="28170">tf(t) = count(t)</st></em><st c="28186"> and is determined on a text-by-text basis. </st><st c="28230">The idf is a measure of how common the word is across </st><em class="italic"><st c="28284">all</st></em><st c="28287"> documents and is usually calculated on a logarithmic scale. </st><st c="28348">A common implementation is given by </st><span class="No-Break"><st c="28384">the following:</st></span></p>
			<p><img src="image/48.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.767em;height:2.041em;width:8.193em"/><st c="28398"/></p>
			<p><st c="28416">Here, </st><em class="italic"><st c="28422">n</st></em><st c="28423"> is the total number of documents, and </st><em class="italic"><st c="28462">df(t)</st></em><st c="28467"> is the number of documents in which the term </st><em class="italic"><st c="28513">t</st></em><st c="28514"> appears. </st><st c="28524">The bigger the value of </st><em class="italic"><st c="28548">df(t)</st></em><st c="28553">, the lower the weighting for the term. </st><st c="28593">The importance of a word will be high if it appears a lot of times in a text (high </st><em class="italic"><st c="28676">tf</st></em><st c="28678">) or few times a</st><a id="_idTextAnchor1582"/><st c="28695">cross texts (</st><span class="No-Break"><st c="28709">high </st></span><span class="No-Break"><em class="italic"><st c="28715">idf</st></em></span><span class="No-Break"><st c="28718">).</st></span></p>
			<p class="callout-heading"><st c="28721">Note</st></p>
			<p class="callout"><st c="28726">TF-IDF can be used together with n-grams. </st><st c="28769">Similarly, to weigh an n-gram, we compound the n-gram frequency in a certain document with the frequency of the n-gram </st><span class="No-Break"><st c="28888">across documents.</st></span></p>
			<p><st c="28905">In this recipe, we will learn how to extract features u</st><a id="_idTextAnchor1583"/><a id="_idTextAnchor1584"/><st c="28961">sing TF-IDF with or without n-grams </st><span class="No-Break"><st c="28998">using </st></span><span class="No-Break"><strong class="source-inline"><st c="29004">scikit-learn</st></strong></span><span class="No-Break"><st c="29016">.</st></span></p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor1585"/><st c="29017">Getting ready</st></h2>
			<p><strong class="source-inline"><st c="29031">scikit-learn</st></strong><st c="29044"> uses a slightly different way to calculate the </st><span class="No-Break"><st c="29092">IDF statistic:</st></span></p>
			<p><img src="image/49.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.817em;height:1.841em;width:10.479em"/><st c="29106"/></p>
			<p><st c="29134">This formulation ensures that a word that appears in all texts receives the lowest weight of 1. </st><st c="29230">In addition, after calculating the TF-IDF for every word, </st><strong class="source-inline"><st c="29288">scikit-learn</st></strong><st c="29300"> normalizes the feature vector (that wit</st><a id="_idTextAnchor1586"/><st c="29340">h all the words) to its Euclidean norm. </st><st c="29381">For more details on the exact formula, visit the </st><strong class="source-inline"><st c="29430">scikit-learn</st></strong><st c="29442"> documentation </st><span class="No-Break"><st c="29457">at </st></span><a href="https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"><span class="No-Break"><st c="29460">https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</st></span></a><span class="No-Break"><st c="29544">.</st></span></p>
			<p><st c="29545">TF-IDF shares the characteristics of BoW when creating the term matrix – that is, high feature space and sparsity. </st><st c="29661">To reduce the number of features and sparsity, we can remove stop words, set the characters to lowercase, and retain words that appear in a minimum percentage</st><a id="_idIndexMarker889"/><st c="29819"> of observations. </st><st c="29837">If you are unfamiliar with these terms, visit the </st><em class="italic"><st c="29887">Creating features with bag-of-words and n-grams</st></em><st c="29934"> recipe in this chapter for </st><span class="No-Break"><st c="29962">a recap.</st></span></p>
			<p><st c="29970">In this recipe, we will learn how to set words into lowercase, remove stop words, retain words with a minimum acceptable frequency, capture n-grams, and then return the TF-IDF statistic of words, all using a </st><a id="_idTextAnchor1587"/><a id="_idTextAnchor1588"/><st c="30179">single transformer from </st><span class="No-Break"><st c="30203">scikit-learn: </st></span><span class="No-Break"><strong class="source-inline"><st c="30217">TfidfVectorizer()</st></strong></span><span class="No-Break"><st c="30234">.</st></span></p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor1589"/><st c="30235">How to do it...</st></h2>
			<p><st c="30251">Let’s begin by loading the necessary libraries and getting the </st><span class="No-Break"><st c="30315">dataset ready:</st></span></p>
			<ol>
				<li><st c="30329">Load </st><strong class="source-inline"><st c="30335">pandas</st></strong><st c="30341">, </st><strong class="source-inline"><st c="30343">TfidfVectorizer()</st></strong><st c="30360">, and the dataset </st><span class="No-Break"><st c="30378">from </st></span><span class="No-Break"><strong class="source-inline"><st c="30383">scikit-learn</st></strong></span><span class="No-Break"><st c="30395">:</st></span><pre class="source-code"><st c="30397">
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.</st><a id="_idTextAnchor1590"/><st c="30498">text import (
    TfidfVectorizer
)</st></pre></li>				<li><st c="30530">Let’s load the train set part of the 20 Newsgroup dataset into a </st><span class="No-Break"><st c="30596">pandas DataFrame:</st></span><pre class="source-code"><st c="30613">
data = fetch_20newsgroups(subset='train')
df = pd.DataFrame(data.data, columns=['text'])</st></pre></li>				<li><st c="30702">To make interpreting the results easier, let’s remove punctuation and numbers from the </st><span class="No-Break"><st c="30790">text variable:</st></span><pre class="source-code"><st c="30804">
df['text'] = df['text'].str.replace(
    ‹[^\w\s]›,››, regex=True).str.replace(
    '\d+','', regex=True)</st></pre></li>				<li><st c="30902">Now, let’s set up </st><strong class="source-inline"><st c="30921">TfidfVectorize</st><a id="_idTextAnchor1591"/><st c="30935">r()</st></strong><st c="30939"> from </st><strong class="source-inline"><st c="30945">scikit-learn</st></strong><st c="30957"> so that, before creating the TF-IDF metrics, it puts all text in lowercase, removes stop words, and </st><a id="_idIndexMarker890"/><st c="31058">retains words that appear in at least 5% of the </st><span class="No-Break"><st c="31106">text pieces:</st></span><pre class="source-code"><st c="31118">
vectorizer = TfidfVectorizer(
    lowercase=True,
    stop_words='english',
    ngram_range=(1, 1),
    min_df=0.05)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="31219">Note</st></p>
			<p class="callout"><st c="31224">To introduce n-grams as part of the returned columns, we can change the value of </st><strong class="source-inline"><st c="31306">ngrams_range</st></strong><st c="31318"> to, for example, </st><strong class="source-inline"><st c="31336">(1,2)</st></strong><st c="31341">. The tuple provides the lower and upper boundaries of the range of n-values for different n-grams. </st><st c="31441">In the case of </st><strong class="source-inline"><st c="31456">(1,2)</st></strong><st c="31461">, </st><strong class="source-inline"><st c="31463">TfidfVectorizer()</st></strong><st c="31480"> will return single words and arrays of two consecutive words </st><span class="No-Break"><st c="31542">as columns.</st></span></p>
			<ol>
				<li value="5"><st c="31553">Let’s fit </st><strong class="source-inline"><st c="31564">TfidfVectorizer()</st></strong><st c="31581"> so that it learns which words should be introduced as columns of the TF-IDF matrix and determines the </st><span class="No-Break"><st c="31684">words’ </st></span><span class="No-Break"><strong class="source-inline"><st c="31691">idf</st></strong></span><span class="No-Break"><st c="31694">:</st></span><pre class="source-code"><st c="31696">
vectorizer.fit(df['text'])</st></pre></li>				<li><st c="31723">Now, let’s create the </st><span class="No-Break"><st c="31746">TF-IDF matrix:</st></span><pre class="source-code"><st c="31760">
X</st><a id="_idTextAnchor1592"/><st c="31762"> = vectorizer.transform(df['text'])</st></pre></li>				<li><st c="31797">Finally, let’s capture the TF-IDF matrix in a DataFrame with the corresponding </st><span class="No-Break"><st c="31877">feature names:</st></span><pre class="source-code"><st c="31891">
tfidf = pd.DataFrame(
    X.toarray(),
    columns = vectorizer.get_feature_names_out()
)</st></pre><p class="list-inset"><st c="31973">With that, we </st><a id="_idIndexMarker891"/><st c="31988">have created a </st><strong class="source-inline"><st c="32003">pandas</st></strong><st c="32009"> DataFrame that contains words as columns and the TF-IDF as valu</st><a id="_idTextAnchor1593"/><st c="32073">es. </st><st c="32078">You can inspect the result by </st><span class="No-Break"><st c="32108">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="32118">tfidf.head()</st></strong></span><span class="No-Break"><st c="32130">:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B22396_11_07.jpg" alt="Figure 11.7 – A DataFrame with features resulting from TF-IDF"/><st c="32132"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="32918">Figure 11.7 – A DataFrame with features resulting from TF-IDF</st></p>
			<p><st c="32979">Now, we can use this t</st><a id="_idTextAnchor1594"/><a id="_idTextAnchor1595"/><st c="33002">erm frequency DataFrame to train machine </st><span class="No-Break"><st c="33044">lea</st><a id="_idTextAnchor1596"/><st c="33047">rning models.</st></span></p>
			<h2 id="_idParaDest-319"><a id="_idTextAnchor1597"/><st c="33061">How it works...</st></h2>
			<p><st c="33077">In this recipe, we extracted the TF-IDF values of words present in at least 5% of the documents by utilizing </st><strong class="source-inline"><st c="33187">TfidfVectorizer()</st></strong> <span class="No-Break"><st c="33204">from scikit-learn.</st></span></p>
			<p><st c="33223">We loaded the 20 Newsgroup text dataset from </st><strong class="source-inline"><st c="33269">scikit-learn</st></strong><st c="33281"> and then removed punctuation and numbers from the text rows using pandas’ </st><strong class="source-inline"><st c="33356">replace()</st></strong><st c="33365">, which can be accessed through pandas’ </st><strong class="source-inline"><st c="33405">str</st></strong><st c="33408">, to replace digits, </st><strong class="source-inline"><st c="33429">'\d+'</st></strong><st c="33434">, or symbols, </st><strong class="source-inline"><st c="33448">'[^\w\s]'</st></strong><st c="33457">, with empty strings, </st><strong class="source-inline"><st c="33479">''</st></strong><st c="33481">. Then, we used </st><strong class="source-inline"><st c="33497">TfidfVectorizer()</st></strong><st c="33514"> to create TF-IDF statistics for words. </st><st c="33554">We set the </st><strong class="source-inline"><st c="33565">lowercase</st></strong><st c="33574"> parameter to </st><strong class="source-inline"><st c="33588">True</st></strong><st c="33592"> to put words into lowercase before making the calculations. </st><st c="33653">We set the </st><strong class="source-inline"><st c="33664">stop_words</st></strong><st c="33674"> argument to </st><strong class="source-inline"><st c="33687">english</st></strong><st c="33694"> to avoid stop words in the returned matrix. </st><st c="33739">We set </st><strong class="source-inline"><st c="33746">ngram_range</st></strong><st c="33757"> to the </st><strong class="source-inline"><st c="33765">(1,1)</st></strong><st c="33770"> tuple to return single words as features. </st><st c="33813">Finally, we set the </st><strong class="source-inline"><st c="33833">min_df</st></strong><st c="33839"> argument to </st><strong class="source-inline"><st c="33852">0.05</st></strong><st c="33856"> to return words that appear at least in 5% of the texts or, in other words, in 5% of </st><span class="No-Break"><st c="33942">the rows.</st></span></p>
			<p><st c="33951">After setting up the transformer, we applied the </st><strong class="source-inline"><st c="34001">fi</st><a id="_idTextAnchor1598"/><st c="34003">t()</st></strong><st c="34007"> method to let the transformer find the words to</st><a id="_idIndexMarker892"/><st c="34055"> retain in the final term matrix. </st><st c="34089">With the </st><strong class="source-inline"><st c="34098">transform()</st></strong><st c="34109"> method, the transformer returned an object with the words and their TF-IDF values, which we then captured in a pandas DataFrame with the appropriate feature names. </st><st c="34274">We </st><a id="_idTextAnchor1599"/><a id="_idTextAnchor1600"/><st c="34277">can now use these features in machine </st><span class="No-Break"><st c="34315">learning algorithms.</st></span></p>
			<h2 id="_idParaDest-320"><st c="34335">See als</st><a id="_idTextAnchor1601"/><st c="34343">o</st></h2>
			<p><st c="34345">For more details</st><a id="_idIndexMarker893"/><st c="34361"> on </st><strong class="source-inline"><st c="34365">TfidfVectorizer()</st></strong><st c="34382">, visit scikit-learn’s </st><span class="No-Break"><st c="34405">documentation: </st></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"><span class="No-Break"><st c="34420">https://scikit-learn.org/stable/modules/gen</st><span id="_idTextAnchor1602"/><span id="_idTextAnchor1603"/><st c="34463">erated/sklearn.feature_extraction.text.TfidfVectori</st><span id="_idTextAnchor1604"/><st c="34515">zer.html</st></span></a></p>
			<h1 id="_idParaDest-321"><st c="34524">Cleani</st><a id="_idTextAnchor1605"/><st c="34531">ng and stemming text variables</st></h1>
			<p><st c="34562">Some variables in our</st><a id="_idIndexMarker894"/><st c="34584"> dataset come from free text fields, which are manually </st><a id="_idIndexMarker895"/><st c="34640">completed by users. </st><st c="34660">People have different writing styles, and we use a variety of punctuation marks, capitalization patterns, and verb conjugations to convey the content, as well as the emotions surrounding it. </st><st c="34851">We can extract (some) information from text without taking the trouble to read it by creating statistical parameters that summarize the text’s complexity, keywords, and relevance of words in a document. </st><st c="35054">We discussed these methods in the previous recipes of this chapter. </st><st c="35122">However, to derive these statistics and aggregated features, we should clean the</st><a id="_idTextAnchor1606"/><st c="35202"> text </st><span class="No-Break"><st c="35208">variables first.</st></span></p>
			<p><st c="35224">Text cleaning or preprocessing involves punctuation removal, stop word elimination, character case setting, and word stemming. </st><st c="35352">Punctuation removal consists of deleting characters that are not letters, numbers, or spaces; in some cases, we also remove numbers. </st><st c="35485">The elimination of stop words refers to removing common words that are used in our language to allow for the sentence structure and flow, but that individually convey little or no information. </st><st c="35678">Examples of stop words include articles such as </st><em class="italic"><st c="35726">the</st></em><st c="35729"> and </st><em class="italic"><st c="35734">a</st></em><st c="35735"> for the English language, as well as pronouns such as </st><em class="italic"><st c="35790">I</st></em><st c="35791">, </st><em class="italic"><st c="35793">you</st></em><st c="35796"> and </st><em class="italic"><st c="35801">they</st></em><st c="35805">, and commonly used verbs in their various conjugations, such as the verbs </st><em class="italic"><st c="35880">to be</st></em><st c="35885"> and </st><em class="italic"><st c="35890">to have</st></em><st c="35897">, as well as the auxiliary verbs </st><em class="italic"><st c="35930">would</st></em> <span class="No-Break"><st c="35935">and </st></span><span class="No-Break"><em class="italic"><st c="35940">do</st></em></span><span class="No-Break"><st c="35942">.</st></span></p>
			<p><st c="35943">To allow computers to identify words correctly, it is also necessary to set all the words in the same case, since the words </st><em class="italic"><st c="36068">Toy</st></em><st c="36071"> and </st><em class="italic"><st c="36076">toy</st></em><st c="36079"> would be identified as being different by a computer due to the uppercase </st><em class="italic"><st c="36154">T</st></em><st c="36155"> in the </st><span class="No-Break"><st c="36163">first one.</st></span></p>
			<p><st c="36173">Finally, to focus on the </st><em class="italic"><st c="36199">message</st></em><st c="36206"> of the text, we don’t want computers to consider words differently if they show different conjugations. </st><st c="36311">Hence, we would use word stemming as part of the preprocessing pipeline. </st><st c="36384">Word stemming refers to reducing each word to its root or base so that the words </st><em class="italic"><st c="36465">playing</st></em><st c="36472">, </st><em class="italic"><st c="36474">plays</st></em><st c="36479">, and </st><em class="italic"><st c="36485">played</st></em><st c="36491"> become </st><em class="italic"><st c="36499">play</st></em><st c="36503">, which, in essence, conveys the same or very </st><span class="No-Break"><st c="36549">similar meaning.</st></span></p>
			<p><st c="36565">In this recipe, we will learn how to remove punctuation and stop words, set words </st><a id="_idTextAnchor1607"/><a id="_idTextAnchor1608"/><st c="36648">in lowercase, and perform word stemming with pandas </st><span class="No-Break"><st c="36700">and </st></span><span class="No-Break"><strong class="source-inline"><st c="36704">NLTK</st></strong></span><span class="No-Break"><st c="36708">.</st></span></p>
			<h2 id="_idParaDest-322"><a id="_idTextAnchor1609"/><st c="36709">Getting ready</st></h2>
			<p><st c="36723">We are going to use the </st><strong class="source-inline"><st c="36748">NLTK</st></strong><st c="36752"> stem package to perform word stemming, which incorporates different algorithms to stem words from English and other languages. </st><st c="36880">Each method differs in the algorithm it uses to find the </st><em class="italic"><st c="36937">root</st></em><st c="36941"> of the word; therefore, they may output slightly different results. </st><st c="37010">I recommend reading more about it, trying different methods, and </st><a id="_idIndexMarker896"/><st c="37075">choosing </st><a id="_idIndexMarker897"/><st c="37084">the </st><a id="_idTextAnchor1610"/><st c="37088">one that serves the project you are </st><span class="No-Break"><st c="37124">working on.</st></span></p>
			<p><st c="37135">More information about NLTK st</st><a id="_idTextAnchor1611"/><a id="_idTextAnchor1612"/><st c="37166">emmers can be found </st><span class="No-Break"><st c="37187">at </st></span><a href="https://www.nltk.org/api/nltk.stem.html"><span class="No-Break"><st c="37190">https://www.nlt</st><span id="_idTextAnchor1613"/><st c="37205">k.org/api/nltk.stem.html</st></span></a><span class="No-Break"><a id="_idTextAnchor1614"/></span><span class="No-Break"><st c="37230">.</st></span></p>
			<h2 id="_idParaDest-323"><a id="_idTextAnchor1615"/><st c="37231">How to do it...</st></h2>
			<p><st c="37247">Let’s begin by loading the necessary libraries and getting the </st><span class="No-Break"><st c="37311">dataset ready:</st></span></p>
			<ol>
				<li><st c="37325">Load </st><strong class="source-inline"><st c="37331">pandas</st></strong><st c="37337">, </st><strong class="source-inline"><st c="37339">stopwords</st></strong><st c="37348">, and </st><strong class="source-inline"><st c="37354">SnowballStemmer</st></strong><st c="37369"> from </st><strong class="source-inline"><st c="37375">NLTK</st></strong><st c="37379"> and the dataset </st><span class="No-Break"><st c="37396">from </st></span><span class="No-Break"><strong class="source-inline"><st c="37401">scikit-learn</st></strong></span><span class="No-Break"><st c="37413">:</st></span><pre class="source-code"><st c="37415">
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn.datasets import fetch_20newsgroups</st></pre></li>				<li><st c="37564">Let’s load the train set part of the 20 Newsgroup dataset into a </st><span class="No-Break"><st c="37630">pandas DataFrame:</st></span><pre class="source-code"><st c="37647">
data = fetch_20newsgroups(subset='train')
df = pd.DataFrame(data.data, columns=['text'])</st></pre><p class="list-inset"><st c="37736">Now, let’s begin with the </st><span class="No-Break"><st c="37763">text cleaning.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="37777">Note</st></p>
			<p class="callout"><st c="37782">After executing each of the commands in this recipe, print some example texts by executing, for example, </st><strong class="source-inline"><st c="37888">print(df['text'][10])</st></strong><st c="37909"> so that you can visualize the changes introduced to the text. </st><st c="37972">Go ahead and do it now, and then repeat the command after </st><span class="No-Break"><st c="38030">each step.</st></span></p>
			<ol>
				<li value="3"><st c="38040">Let’s begin by removing </st><span class="No-Break"><st c="38065">the punctuation:</st></span><pre class="source-code"><st c="38081">
df["text"] = df['text'].str.replace('[^\w\s]','')</st></pre></li>			</ol>
			<p class="callout-heading"><st c="38131">Tip</st></p>
			<p class="callout"><st c="38135">You can also remove the punctuation using the built-in </st><strong class="source-inline"><st c="38191">string</st></strong><st c="38197"> module from Python. </st><st c="38218">First, import the module by executing </st><strong class="source-inline"><st c="38256">import string</st></strong><st c="38269"> and then execute </st><strong class="source-inline"><st c="38287">df['text'] = </st></strong><span class="No-Break"><strong class="source-inline"><st c="38300">df['text'].str.replace('[{}]</st><a id="_idTextAnchor1616"/><st c="38328">'.format(string.punctuatio</st><a id="_idTextAnchor1617"/><st c="38355">n), '')</st></strong></span><span class="No-Break"><st c="38363">.</st></span></p>
			<ol>
				<li value="4"><st c="38364">We can also</st><a id="_idIndexMarker898"/><st c="38376"> remove </st><a id="_idIndexMarker899"/><st c="38384">characters that are numbers, leaving only letters, </st><span class="No-Break"><st c="38435">as follows:</st></span><pre class="source-code"><st c="38446">
df['text'] = df['text'].str.replace(
    '\d+', '', regex=True)</st></pre></li>				<li><st c="38506">Now, let’s set all words </st><span class="No-Break"><st c="38532">into lowercase:</st></span><pre class="source-code"><st c="38547">
df['text'] = df['text'].str.lower()</st></pre><p class="list-inset"><st c="38583">Now, let’s start the p</st><a id="_idTextAnchor1618"/><st c="38606">rocess of removing </st><span class="No-Break"><st c="38626">stop words.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="38637">Note</st></p>
			<p class="callout"><em class="italic"><st c="38642">Step 6</st></em><st c="38649"> may fail if you did not download the </st><strong class="source-inline"><st c="38687">NLTK</st></strong><st c="38691"> library’s </st><strong class="source-inline"><st c="38702">stopwords</st></strong><st c="38711">. Visit the </st><em class="italic"><st c="38723">Technical requirements</st></em><st c="38745"> section in this chapter for </st><span class="No-Break"><st c="38774">more details.</st></span></p>
			<ol>
				<li value="6"><st c="38787">Let’s create a function that splits a string into a list of words, removes the stop words, and finally concatenates the remaining words back into </st><span class="No-Break"><st c="38934">a string:</st></span><pre class="source-code"><st c="38943">
def remove_stopwords(text):
    stop = set(stopwords.words('english'))
    text = [word
    for word in text.split() if word not in stop]
    text = ‹ ‹.join(x for x in text)
    return text</st></pre></li>			</ol>
			<p class="callout-heading"><st c="39114">Note</st></p>
			<p class="callout"><st c="39119">To be able to process the data with the </st><strong class="source-inline"><st c="39160">scikit-learn</st></strong><st c="39172"> librar</st><a id="_idTextAnchor1619"/><st c="39179">y’s </st><strong class="source-inline"><st c="39184">CountVectorizer()</st></strong><st c="39201"> or </st><strong class="source-inline"><st c="39205">TfidfVecto</st><a id="_idTextAnchor1620"/><st c="39215">rizer()</st></strong><st c="39223">, we need the text to be in string format. </st><st c="39266">Therefore, after removing the stop words, we need to return the words as a single string. </st><st c="39356">We have transformed the NLTK library’s stop words list into a set because sets are faster to scan than lists. </st><st c="39466">This improves the </st><span class="No-Break"><st c="39484">computation time.</st></span></p>
			<ol>
				<li value="7"><st c="39501">Now, let’s use</st><a id="_idIndexMarker900"/><st c="39516"> the </st><a id="_idIndexMarker901"/><st c="39521">function from </st><em class="italic"><st c="39535">step 6</st></em><st c="39541"> to remove stop words from the </st><span class="No-Break"><strong class="source-inline"><st c="39572">text</st></strong></span><span class="No-Break"><st c="39576"> variable:</st></span><pre class="source-code"><st c="39586">
df['text'] = df['text'].apply(remove_stopwords)</st></pre><p class="list-inset"><st c="39634">If you want to know which words are</st><a id="_idTextAnchor1621"/><st c="39670"> stop word</st><a id="_idTextAnchor1622"/><st c="39680">s, </st><span class="No-Break"><st c="39684">execute </st></span><span class="No-Break"><strong class="source-inline"><st c="39692">stopwords.words('english')</st></strong></span><span class="No-Break"><st c="39718">.</st></span></p><p class="list-inset"><st c="39719">Finally, let’s stem the words in our data. </st><st c="39763">We will use </st><strong class="source-inline"><st c="39775">SnowballStemmer</st></strong><st c="39790"> from </st><strong class="source-inline"><st c="39796">NLTK</st></strong><st c="39800"> to </st><span class="No-Break"><st c="39804">do so.</st></span></p></li>				<li><st c="39810">Let’s create an instance of </st><strong class="source-inline"><st c="39839">SnowballStemer</st></strong><st c="39853"> for the </st><span class="No-Break"><st c="39862">English language:</st></span><pre class="source-code"><st c="39879">
stemmer = SnowballStemmer("english")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="39916">Tip</st></p>
			<p class="callout"><st c="39920">Try the stemmer in a single word to see how it works; for example, run </st><strong class="source-inline"><st c="39992">stemmer.stem('running')</st></strong><st c="40015">. You should see </st><strong class="source-inline"><st c="40032">run</st></strong><st c="40035"> as the result of that command. </st><st c="40067">Try </st><span class="No-Break"><st c="40071">different words!</st></span></p>
			<ol>
				<li value="9"><st c="40087">Let’s create a function that splits a string into a list of words, applies </st><strong class="source-inline"><st c="40163">stemmer</st></strong><st c="40170"> to each word, and finally concatenates the stemmed word list back into </st><span class="No-Break"><st c="40242">a string:</st></span><pre class="source-code"><st c="40251">
def stemm_words(text):
    text = [
        stemmer.stem(word) for word in text.split()
    ]
    text = ‹ ‹.join(x for x in text)
    return text</st></pre></li>				<li><st c="40374">Let’s use</st><a id="_idIndexMarker902"/><st c="40384"> the </st><a id="_idIndexMarker903"/><st c="40389">function from </st><em class="italic"><st c="40403">step 9</st></em><st c="40409"> to stem the words in </st><span class="No-Break"><st c="40431">our data:</st></span><pre class="source-code"><st c="40440">
df['text'] = df['text'].apply(stemm_words)</st></pre><p class="list-inset"><st c="40483">Now, our text is ready to create features based on character and word counts, as well as create BoWs or TF-IDF matrices, as described in the previous recipes of </st><span class="No-Break"><st c="40645">this chapter.</st></span></p><p class="list-inset"><st c="40658">If we execute </st><strong class="source-inline"><st c="40673">print(df['text'][10])</st></strong><st c="40694">, we will see a text example </st><span class="No-Break"><st c="40723">after cleaning:</st></span></p><pre class="source-code"><strong class="bold"><st c="40738">irwincmptrclonestarorg irwin arnstein subject recommend duc summari what worth distribut usa expir sat may gmt organ computrac inc richardson tx keyword ducati gts much line line ducati gts model k clock run well paint bronzebrownorang fade leak bit oil pop st hard accel shop fix tran oil leak sold bike owner want think like k opinion pleas email thank would nice stabl mate beemer ill get jap bike call axi motor tuba irwin honk therefor computracrichardsontx irwincmptrclonestarorg dod r</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="41230">Note</st></p>
			<p class="callout"><st c="41235">If you are counting sentences, you need to do that before removing punctuation, as punctuation and</st><a id="_idTextAnchor1623"/><a id="_idTextAnchor1624"/><st c="41334"> capitalization are needed to define the b</st><a id="_idTextAnchor1625"/><st c="41376">oundaries of </st><span class="No-Break"><st c="41390">each sentence.</st></span></p>
			<h2 id="_idParaDest-324"><a id="_idTextAnchor1626"/><st c="41404">How it works...</st></h2>
			<p><st c="41420">In this recipe, we </st><a id="_idTextAnchor1627"/><st c="41440">removed punctuation, numbers, and stop words from a text variable, set the words in lowercase, and finally, stemmed the words to their root. </st><st c="41581">We removed punctuation and numbers from the text variable using pandas’ </st><strong class="source-inline"><st c="41653">replace()</st></strong><st c="41662">, which can be accessed through pandas’ </st><strong class="source-inline"><st c="41702">str</st></strong><st c="41705">, to replace digits, </st><strong class="source-inline"><st c="41726">'\d+'</st></strong><st c="41731">, or symbols, </st><strong class="source-inline"><st c="41745">'[^\w\s]'</st></strong><st c="41754">, with empty strings, </st><strong class="source-inline"><st c="41776">''</st></strong><st c="41778">. Alternatively, we can use the </st><strong class="source-inline"><st c="41810">punctuation</st></strong><st c="41821"> module from the built-in </st><span class="No-Break"><strong class="source-inline"><st c="41847">string</st></strong></span><span class="No-Break"><st c="41853"> package.</st></span></p>
			<p class="callout-heading"><st c="41862">Tip</st></p>
			<p class="callout"><st c="41866">Run </st><strong class="source-inline"><st c="41871">string.punctuation</st></strong><st c="41889"> in your Python console after importing </st><strong class="source-inline"><st c="41929">string</st></strong><st c="41935"> to check out the symbols that will be replaced with </st><span class="No-Break"><st c="41988">empty strings.</st></span></p>
			<p><st c="42002">Next, utilizing</st><a id="_idIndexMarker904"/><st c="42018"> pandas’ string </st><a id="_idIndexMarker905"/><st c="42034">processing functionality through </st><strong class="source-inline"><st c="42067">str</st></strong><st c="42070">, we set all of the words to lowercase with the </st><strong class="source-inline"><st c="42118">lower()</st></strong><st c="42125"> method. </st><st c="42134">To remove stop words from the text, we used the </st><strong class="source-inline"><st c="42182">stopwords</st></strong><st c="42191"> module from </st><strong class="source-inline"><st c="42204">NLTK</st></strong><st c="42208">, which contains a list of words that are considered frequent – that is, the stop words. </st><st c="42297">We created a function that takes a string and splits it into a list of words using pandas’ </st><strong class="source-inline"><st c="42388">str.split()</st></strong><st c="42399">, and then, with list comprehension, we looped over the words in the list and retained the non-stop words. </st><st c="42506">Finally, with the </st><strong class="source-inline"><st c="42524">join()</st></strong><st c="42530"> method, we concatenated the retained words back into a string. </st><st c="42594">We used the built-in Python </st><strong class="source-inline"><st c="42622">set()</st></strong><st c="42627"> method over the </st><strong class="source-inline"><st c="42644">NLTK</st></strong><st c="42648"> stop words list to improve computation efficiency since it is faster to iterate over sets than over lists. </st><st c="42756">Finally, with pandas’ </st><strong class="source-inline"><st c="42778">apply()</st></strong><st c="42785">, we applied the function to each row of our </st><span class="No-Break"><st c="42830">text data.</st></span></p>
			<p class="callout-heading"><st c="42840">Tip</st></p>
			<p class="callout"><st c="42844">Run </st><strong class="source-inline"><st c="42849">stopwords.words('english')</st></strong><st c="42875"> in your Python console after importing </st><strong class="source-inline"><st c="42915">stopwords</st></strong><st c="42924"> from </st><strong class="source-inline"><st c="42930">NLTK</st></strong><st c="42934"> to visualize the list with the stop words that will </st><span class="No-Break"><st c="42987">be removed.</st></span></p>
			<p><st c="42998">Finally, we stemmed the words using </st><strong class="source-inline"><st c="43035">SnowballStemmer</st></strong><st c="43050"> from </st><strong class="source-inline"><st c="43056">NLTK</st></strong><st c="43060">. </st><strong class="source-inline"><st c="43062">SnowballStemmer</st></strong><st c="43077"> works one word at a time. </st><st c="43104">Therefore, we created a function that takes a string and splits it into a list of words using pandas’ </st><strong class="source-inline"><st c="43206">str.split()</st></strong><st c="43217">. In a list comprehension, we applied </st><strong class="source-inline"><st c="43255">SnowballStemmer</st></strong><st c="43270"> word per word and then concatenated the list of stemmed words back into a string using the </st><strong class="source-inline"><st c="43362">join()</st></strong><st c="43368"> method. </st><st c="43377">With pandas’ </st><strong class="source-inline"><st c="43390">apply()</st></strong><st c="43397">, we applied the</st><a id="_idTextAnchor1628"/><st c="43413"> function to stem words to each row of </st><span class="No-Break"><st c="43452">the DataFrame.</st></span></p>
			<p><st c="43466">The c</st><a id="_idTextAnchor1629"/><st c="43472">leaning steps we performed in this recipe resulted in strings containing the original text, without punctuation or numbers, in lowercase, without common words, and with the root of the word instead of its conjugated form. </st><st c="43695">The data, as it is returned, can be used to derive features, as described in the </st><em class="italic"><st c="43776">Counting characters, words, and vocabulary</st></em><st c="43818"> recipe, or to create BoWs and TI-IDF matrices, as described in the </st><em class="italic"><st c="43886">Creating features with bag-of-words and n-g</st><a id="_idTextAnchor1630"/><st c="43929">rams</st></em><st c="43934"> and </st><em class="italic"><st c="43939">Implementing term frequency-inverse document </st></em><span class="No-Break"><em class="italic"><st c="43984">frequency</st></em></span><span class="No-Break"><st c="43993"> recipes.</st></span></p>
			<p><st c="44002">Cleaning the texts as</st><a id="_idIndexMarker906"/><st c="44024"> we </st><a id="_idIndexMarker907"/><st c="44028">have shown in this recipe can incur data loss, depending on the characteristics of the text, and if we seek to interpret the models after creating BoW or TF-IDF matrices, understanding the importance of stemmed words may not be </st><span class="No-Break"><st c="44256">so straightforward.</st></span></p>
		</div>
	<div id="charCountTotal" value="44275"/></body></html>