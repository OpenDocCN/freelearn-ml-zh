<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Networks on Mobile</h1>
                </header>
            
            <article>
                
<p>In <a href="1b52495b-c6cb-4197-8fcd-a1e764c1f1c2.xhtml" target="_blank">Chapter 2</a>, <em>Supervised and Unsupervised Learning Algorithms,</em> when we introduced you to TensorFlow, its components, and how it works, we talked briefly about <strong>convolutional neural networks</strong> <span>(<strong>CNNs</strong>) </span>and how they work. In this chapter, we will delve into the basic concepts of neural networks. We will explore the similarities and variations between machine learning and neural networks.</p>
<p>We will also go through some of the challenges of executing deep learning algorithms on mobile devices. We will briefly go through the various deep learning and neural network SDKs available for mobile applications that can be run on mobile devices directly. Toward the end of this chapter, we will create an interesting assignment that will utilize both TensorFlow and Core ML. </p>
<p><span>In this chapter, we will be cover the following topics:</span></p>
<ul>
<li>Creating a TensorFlow image recognition model</li>
<li>Converting the TensorFlow model into a Core ML model</li>
<li>Creating an iOS mobile application that utilizes the Core ML model</li>
<li>Introduction to Keras </li>
<li>Creating a handwritten digit recognition solution</li>
</ul>
<p>In this chapter, we are going to implement all of the major topics we have gone through in this book. Before proceeding, make sure you have gone through all the previous chapters in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to neural networks</h1>
                </header>
            
            <article>
                
<p><span>A neural network is a system of hardware and/or software that is modeled on the operation of neurons in the human brain. The design behind neural networks is inspired by the human brain and its functionality. Let's understand the design of the human brain. The neuron is the basic working unit of the brain. It's a specialized cell that can transmit information to other nerve cells. The brain is made up of approximately 100,000,000,000 neurons. A neuron's main function is to process and transmit information.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Communication steps of  a neuron</h1>
                </header>
            
            <article>
                
<p>Neuron communication follows a four-step path:</p>
<ul>
<li>A neuron receives information from the external environment or from other neurons.</li>
<li>The neuron integrates, or processes, the information from all of its input and determines whether to send an output signal. This integration takes place both in time (the duration of the input and the time between input) and space (across the surface of the neuron).</li>
<li>The neuron propagates the signal along its length at a high speed.</li>
<li>The neuron converts this electrical signal to a chemical one and transmits it to another neuron or to an<span> </span>effect such as a muscle or gland.</li>
</ul>
<div class="packt_infobox"><span>To get a better understanding of how neurons—the basic building blocks of the human brain—work, check out <a href="http://www.biologyreference.com/Mo-Nu/Neuron.html#ixzz5ZD78t97u">http://www.biologyreference.com/Mo-Nu/Neuron.html#ixzz5ZD78t97u</a>.</span></div>
<p><span>Now, coming to the neurons' artificial neural networks, the function of these neurons is to take in some input and fire an output.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The activation function</h1>
                </header>
            
            <article>
                
<p><span>To express this categorically, the neuron is a placeholder function that takes in inputs, processes them by applying the function on the input, and produces the output. Any simple function can be put in the defined placeholder:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/78af82c6-15f8-4bd3-b35b-b92752f37f05.png" style="width:31.08em;height:13.58em;"/></p>
<p><span>The function that's used in a neuron is generally called an activation function. </span>In the human body, there are three types of neurons: sensory neurons, motor neurons, and interneurons. In the artificial world, the activation function would probably create the different capability and functionality of the neuron.</p>
<p><span>Here are a few commonly used activation functions:</span></p>
<ul>
<li><span>step</span></li>
<li><span>sigmoid</span></li>
<li><span>tanh</span></li>
<li><span>ReLU-Rectified</span></li>
<li><span>Linear Unit (used mostly in deep learning)</span></li>
</ul>
<p>It is outside the scope of this book to delve into the details of each function. However, it will be good for you to understand these functions and their intricacies if you want to study neural networks further.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Arrangement of neurons</h1>
                </header>
            
            <article>
                
<p>Let's look at the arrangement of neurons in the human body. A typical neuron has several dendrites, normally arranged in an extremely branched fashion, in order to establish contact with many other neurons. Neurons in the human body are also arranged in layers. The number of these layers varies across different parts of the body and brain, but normally is ranges from three to six layers. </p>
<p>In the artificial world, these neurons are also arranged as layers. The following diagram will help you understand the organization of neurons:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9de5a4d1-7ae1-4dca-9f82-aed22bd28c18.png" style="width:28.75em;height:20.25em;"/></p>
<p><span>The leftmost layer of the network is called the </span><strong>input layer</strong><span>, and the rightmost layer is called the </span><strong>output layer</strong><span>. The middle layer of neurons is called the </span><strong>hidden layer</strong><span> because its values are not observed in the training set.</span></p>
<p><span>In this sample neural network, there are three inputs, three hidden units, and one output unit. Any neural network will have at least one input and one output layer. The number of hidden layers can vary.</span></p>
<p>The activation function used in each hidden layer can be different for the same network. This means that the activation function for hidden layer 1 and the b activation function for hidden layer 2 of the same network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of neural networks</h1>
                </header>
            
            <article>
                
<p>Neural networks vary based on the number of hidden layers and the activation functions used in each layer. Here are some of the common types of neural networks:</p>
<ul>
<li><strong>Deep neural networks</strong>: Networks with more than one hidden layer.</li>
</ul>
<ul>
<li><strong>CNN</strong>: Commonly used in computer-vision-related learning problems. The CNN hidden layer uses convolution functions as the activation function.</li>
<li><strong>Recurrent neural networks</strong>: Commonly used in problems related to natural language processing.</li>
</ul>
<p>Current projects/research in the field of improving neural networks in mobile devices include the following:</p>
<ul>
<li>MobileNet</li>
<li>MobileNet V2</li>
<li>MNasNet—implementing reinforcement learning in mobile devices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image recognition solution</h1>
                </header>
            
            <article>
                
<p>Imagine you go to a restaurant with your friends. Assume you are a fitness freak and though you have come to the party to enjoy the buffet, as a fitness freak, you are calorie conscious and don't want to go overboard.</p>
<p>Now, imagine you have a mobile application that comes to your rescue: it takes a picture of the dish, identifies its ingredients, and calculates the caloric value of the food! You could take a picture of every dish and calculate its caloric value and can then decide whether to put it on your plate. Further, this app keeps on learning the different dishes that you take pictures of and continues to learn and master itself in this trade so that it can take very good care of your health.</p>
<p>I can see the sparkle in your eyes. Yes, this is the mobile application we want to try in this chapter. We also want to utilize both TensorFlow and Core ML to accomplish this activity. We will be performing the following steps to create the application that we just discussed:</p>
<ol>
<li><span>Create the TensorFlow image recognition model</span></li>
<li><span>Convert it into a <kbd>.ml</kbd> model file</span></li>
<li><span>Create an iOS/SWIFT app to use that model</span></li>
</ol>
<p>We will go through each of these steps in detail in the upcoming sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a TensorFlow image recognition model</h1>
                </header>
            
            <article>
                
<p>TensorFlow is an open source software library for data flow programming across a range of tasks. It is a symbolic math library and is also used for machine learning applications, such as neural networks. It is used for both research and production at Google, often replacing its closed source predecessor, DistBelief. TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015.</p>
<p>TensorFlow is cross-platform. It runs on nearly everything: GPUs and CPUs<span>–</span>including mobile and embedded platforms<span>–</span>and even <strong>tensor processing units</strong> (<strong>TPUs</strong>), which are specialized hardware for performing tensor math.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What does TensorFlow do?</h1>
                </header>
            
            <article>
                
<p>To keep it simple, let's assume you want two numbers. Now, if you want to write a program in a regular programming language, such as Python, you would use the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>a = 1</em></p>
<p class="CDPAlignCenter CDPAlign"><em>b = 2</em></p>
<p class="CDPAlignCenter CDPAlign"><em>print(a+b)</em></p>
<p>If you run the program, you will see the output as <em>3</em>, and then you'll see the same implementation on <kbd>tensorflow</kbd>:</p>
<pre>import tensorflow as tf<br/>x = tf.constant(35, name='x')<br/>y = tf.Variable(x + 5, name='y')<br/>model = tf.global_variables_initializer()<br/>with tf.Session() as session:<br/>    session.run(model)<br/>    print(session.run(y))</pre>
<p>Let me explain the preceding code. First, we are creating a constant with node name <kbd>x</kbd>, adding <kbd>5</kbd> to it, and storing it in another variable/node <kbd>y</kbd>. If you can see the output of the console of y at this point, you will find the definition of the node, but not the value of 40.</p>
<p>Here, you are defining the nodes of the graph and its corresponding operations. You can make use of the graph once you initialize the variables and create and get a session/instance of the graph.</p>
<p>The following diagram will help you understand this concept:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-952 image-border" src="assets/30384711-7433-40c5-823d-61651eae7f2e.png" style="width:9.08em;height:9.08em;"/></p>
<p>In TensorFlow, all of the constants, placeholders, and variables we will use to create the definition and the linkage between nodes will create one graph, which is just like your class concept in object-oriented programming. Think of the graph as a class and the nodes as data members, <kbd>tf.globalvariableinitilizer()</kbd> as calling the static method to initialize the constants and variable, and <kbd>session.run()</kbd> as calling the constructor of a class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Retraining the model</h1>
                </header>
            
            <article>
                
<p>To create an image classifier, we need to go through many things and do a lot of coding. To keep it simple, we will be showing you how to create it using the Google Code Lab provided code. The following content was taken from Google's Code Lab tutorial.</p>
<p>This was made using CNNs. Explaining all of this is outside the scope of this book. We briefly explored CNN in the introduction of this chapter. However that is very less, compared to what is an ocean. For more information, interested readers can check out <a href="https://colah.github.io/posts/2014-07-Conv-Nets-Modular/">https://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a>.</p>
<p>Let's see how easily we can create an image classifier in <kbd>tensorflow</kbd>. To get started, we need to install anaconda and then run the following commands:</p>
<pre><strong>conda create -n tensorflow pip python=3.6</strong></pre>
<p>Once you run the preceding command, you will get the following prompt:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-953 image-border" src="assets/d3c99d03-b167-4c91-8900-b847e37cbf0c.png" style="width:39.58em;height:19.75em;"/></p>
<p>Type <kbd>y</kbd> to proceed. Once the command has successfully executed, you will see the following screen:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-954 image-border" src="assets/8cd762ea-10d1-40bb-82c2-41f442366a7c.png" style="width:39.08em;height:19.67em;"/></p>
<p>Type <kbd>activate</kbd> project. Once the project has been activated, you will see the prompt, like so:</p>
<pre><strong>(project) D:\Users\vavinas&gt;</strong></pre>
<p>Then, type the following commands:</p>
<pre><strong>pip install tensorflow</strong></pre>
<p>Use the following command to verify the installed packages:</p>
<pre><strong>pip list</strong></pre>
<p>It has to produce the following result. If you don't see some of these packages in your machine, reinstall them:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-955 image-border" src="assets/7e896abf-5d1e-4f8b-b96e-020738b91779.png" style="width:40.08em;height:19.92em;"/></p>
<p>Now, we have successfully installed <kbd>tensorflow</kbd> and its dependencies. Let's get the code from Google Code Labs that will do the classification. For this, make sure you have installed Git on your machine. There are several ways to install it, but the simplest way is through <kbd>npm</kbd>.</p>
<p>To check that Git is properly installed, type <kbd>git</kbd> in the opened command prompt. You will see all the options available for that command. If it is prompting as <kbd>invalid command</kbd>, please try to install it correctly. Now, let's execute the command to clone the repository:</p>
<pre><strong> git clone</strong> <strong>https://github.com/googlecodelabs/tensorflow-for-poets-2</strong></pre>
<p>Once you are done, go to <kbd>tensorflow-for-poets-2</kbd> using the following command:</p>
<pre><strong>cd tensorflow-for-poets-2</strong></pre>
<p>The following folder contains all of the that are scripts required to train a model for image recognition. If you check the <kbd>tf_file</kbd> folder, it will be empty. Here, we will be using this folder to keep the training images and train the model using the scripts in the scripts folder.</p>
<p>To input the images, you need to first download the images. For our sample, we are using food images with four class labels. You can download it from our Git repository, <kbd>project/food_photos</kbd>, and then paste that folder into <kbd>tf_files</kbd>. If you are unable to execute this command, open the folder in Internet Explorer, and then download the in <kbd>tensorflow-for-poets-2/tf_files</kbd> file.</p>
<p>Extract the files into flat files, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6af75a10-bc53-4b36-9597-133b76e45a2f.png" style="width:49.25em;height:14.08em;"/></p>
<p>Now, we are going to retrain the model using the following script. Execute the following command:</p>
<pre>python -m scripts.retrain \<br/>  --bottleneck_dir=tf_files/bottlenecks \<br/>  --how_many_training_steps=500 \<br/>  --model_dir=tf_files/models/ \<br/>  --summaries_dir=tf_files/training_summaries/ mobilenet_0.50_224  \<br/>  --output_graph=tf_files/retrained_graph.pb \<br/>  --output_labels=tf_files/retrained_labels.txt \<br/>  --architecture=mobilenet_0.50_224  \<br/>  --image_dir=tf_files/food_photos</pre>
<p>The previous Python script is used to retrain a model that has many arguments, but we will use and discuss only a few important arguments, as follows:</p>
<ul>
<li><kbd>bottleneck_dir</kbd>: This will <span>save these files to the bottlenecks/ directory.</span></li>
<li><kbd>how_many_training_steps</kbd>: This will a number below 4,000. A higher number will give your model greater accuracy, but takes too much time to build, and the model file will be too big.</li>
</ul>
<ul>
<li><kbd>model_dir</kbd>:<strong> </strong>This tells us where to save the model.</li>
<li><kbd>summaries_dir</kbd>: Contains the training summaries.</li>
<li><kbd>output_graph</kbd>:<strong> </strong>Where to save the output graph. This is the resultant model that we will use in mobiles.</li>
<li><kbd>output_labels</kbd>:<strong> </strong>This is the file that holds the class labels. Usually, the class label for an image is the folder name.</li>
<li><kbd>architecture</kbd>:<strong> </strong>This tells us which architecture to use. Here, we are using the mobilenet model with a 0.50 relative size of the model and a 244 image size.</li>
<li><kbd>image_dir</kbd>:<strong> </strong>Inputs the images directory, in this case, <kbd>food_photos</kbd>.</li>
</ul>
<p>Executing the previous command will give you the following as output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-956 image-border" src="assets/8238586a-3f12-49b0-8fc9-2eac78dba889.png" style="width:45.83em;height:22.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">About bottlenecks</h1>
                </header>
            
            <article>
                
<p>Here, we will try to understand how the retraining process works. The ImageNet models we are using are made up of many layers stacked on top of each other. These layers are pre-trained and already have sufficient information that will help in image classification. All we are trying to do is train the very last layer, <kbd><span>final_training_ops</span></kbd> , when all the previous layers retrain their already trained state.</p>
<p>The following screenshot is taken from TensorBoard. You can open TensorBoard in your browser to get a better look at it. You will find it in the <span class="packt_screen">Graphs</span> tab:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/61b41369-49ff-4f91-9bd5-e310aaf4f503.png" style="width:49.83em;height:50.75em;"/></p>
<p><span>In the preceding diagram, the <span class="packt_screen">softmax</span> node on the left-hand side is the output layer of the original model. All the nodes to the right of <span class="packt_screen">softmax</span> were added by the retraining script.</span></p>
<div class="packt_tip">Note that this will only work after the retrain script finishes generating the <strong>bottleneck</strong> files.</div>
<p>Bottleneck is the term used to refer to the layer just before the final output layer that does the classification. Bottleneck does not imply its conventional meaning of something that slows down the whole process. We use the term bottleneck because, near the output, the representation is much more compact than in the main body of the network.</p>
<p>Every image is reused multiple times during training. Calculating the layers behind the bottleneck for each image takes a significant amount of time. Since these lower layers of the network are not being modified, their output can be cached and reused. Now, you have the TensorFlow retrained model in your hand. Let's test the model that we just trained using the following command:</p>
<pre>python -m scripts.label_image \<br/>    --graph=tf_files/retrained_graph.pb  \<br/>    --image=tf_files\food_photos\pizza\1.jpg</pre>
<p>Executing the previous code block will give you the class that the food image belongs to. Now, let's go to the next task: converting the <kbd>tensorflow</kbd> model into the Core ML format.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting the TensorFlow model into the Core ML model</h1>
                </header>
            
            <article>
                
<p>The TensorFlow team has developed a package that is used to convert the models created in TensorFlow into Core ML, which in through is used in iOS apps. To use this, you must have macOS with Python 3.6 and TensorFlow installed. Using this, we can convert the TensorFlow model file (<kbd>.pb</kbd>) into the Core ML format (<kbd>.mlmodel</kbd>). First, you need to execute the following command:</p>
<pre><strong>Pip install tfcoreml</strong></pre>
<p>Once this is installed, write the following code in your Python file, name it <kbd>inspect.py</kbd>, and save it:</p>
<pre>import tensorflow as tf<br/>from tensorflow.core.framework import graph_pb2<br/>import time<br/>import operator<br/>import sys<br/> <br/>def inspect(model_pb, output_txt_file):<br/>    graph_def = graph_pb2.GraphDef()<br/>    with open(model_pb, "rb") as f:<br/>        graph_def.ParseFromString(f.read())<br/> <br/>    tf.import_graph_def(graph_def)<br/> <br/>    sess = tf.Session()<br/>    OPS = sess.graph.get_operations()<br/> <br/>    ops_dict = {}<br/> <br/>    sys.stdout = open(output_txt_file, 'w')<br/>    for i, op in enumerate(OPS):<br/>        print('---------------------------------------------------------------------------------------------------------------------------------------------')<br/>        print("{}: op name = {}, op type = ( {} ), inputs = {}, outputs = {}".format(i, op.name, op.type, ", ".join([x.name for x in op.inputs]), ", ".join([x.name for x in op.outputs])))<br/>        print('@input shapes:')<br/>        for x in op.inputs:<br/>            print("name = {} : {}".format(x.name, x.get_shape()))<br/>        print('@output shapes:')<br/>        for x in op.outputs:<br/>            print("name = {} : {}".format(x.name, x.get_shape()))<br/>        if op.type in ops_dict:<br/>            ops_dict[op.type] += 1<br/>        else:<br/>            ops_dict[op.type] = 1<br/> <br/>    print('---------------------------------------------------------------------------------------------------------------------------------------------')<br/>    sorted_ops_count = sorted(ops_dict.items(), key=operator.itemgetter(1))<br/>    print('OPS counts:')<br/>    for i in sorted_ops_count:<br/>        print("{} : {}".format(i[0], i[1]))<br/> <br/>if __name__ == "__main__":<br/>    """<br/>    Write a summary of the frozen TF graph to a text file.<br/>    Summary includes op name, type, input and output names and shapes.<br/> <br/>    Arguments<br/>    ----------<br/>    - path to the frozen .pb graph<br/>    - path to the output .txt file where the summary is written<br/> <br/>    Usage<br/>    ----------<br/>    python inspect_pb.py frozen.pb text_file.txt<br/> <br/>    """<br/>    if len(sys.argv) != 3:<br/>        raise ValueError("Script expects two arguments. " +<br/>              "Usage: python inspect_pb.py /path/to/the/frozen.pb /path/to/the/output/text/file.txt")<br/>    inspect(sys.argv[1], sys.argv[2])</pre>
<p>The preceding code will take the model file as an input argument, and save all the operations and input/output node names with a description in a text file that we supply as input. To run this, enter the following command:</p>
<pre><strong>Python inspect.py retrained_graph.pb summeries.txt</strong></pre>
<p>In this command, you are executing the <kbd>inspect.py</kbd> code you saved before. This will also input the graph file obtained from the previous section and, path of a text file where you want to save the summaries.</p>
<p>Once you execute this command, <kbd>summeries.txt</kbd> will be created with all the summaries, as shown here. These will be added into that file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4b83a234-07e4-42b9-8eb8-5e64f215a9fd.png" style="width:53.67em;height:30.42em;"/></p>
<p>In this file, you can see all the operations, input and output names, and their shapes; you can also see the overall operators:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ba72d5a4-2aab-4881-9f57-f3df430b5dd2.png" style="width:20.67em;height:19.50em;"/></p>
<p>Toward the end of the file, you will find the definition of the end node; in our case, it is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e6d02c2-44c4-4846-bf2b-7c3c39f4e891.png"/></p>
<p>Here, you can see that the end node operation type is <kbd>Softmax</kbd>, and the output that it will produce will be stored in the <kbd>final_result:0</kbd> name. Now, check out the following code block, which is used to generate a corresponding Core ML model:</p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/5c1fdba4-e5f6-40ec-a2fe-db4c29da5825.png" style="width:30.50em;height:11.83em;"/></span></p>
<p>Let's understand the previous code block in detail. You must have noticed that we imported the <kbd>tfcoreml</kbd> package in the first line, and then used its <strong>convert</strong> function. The following are its arguments:</p>
<ul>
<li><kbd>Tf_model_path</kbd>: The (<kbd>.pb</kbd>) file path that you generated in the previous section, <em>Converting the TensorFlow model into the Core ML model</em>.</li>
<li><kbd>Mlmodel_path</kbd>: The output model file path where you want to generate the model.</li>
<li><kbd>Output_feature_names</kbd>: In this, we will get the output variable name that you obtained from the previous text file that was generated by our model-inspection code.</li>
<li><kbd>Image_input_names</kbd>: Name you want to give for the image input. In Core ML/iOS, this will be the image buffer.</li>
<li><kbd>Class_labels</kbd>: This is the file you will get in the training step.</li>
</ul>
<p>Once you run the preceding code, you will see the generated <kbd>converted</kbd><kbd>.mlmodel</kbd> file in your directory. You can import this into your Xcode project and make use of it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the iOS mobile application</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to create an app to make use of the image recognition model that we've created to predict images using your iOS mobile camera.</p>
<p>To start, you need a Mac PC running Xcode version 9+. Download the source code (x-code project) from the Git repository and navigate to the project folder. Open the <kbd>recognition.xcodeproj</kbd> image in Xcode. The following screenshot shows the folder structure of the project:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6d539950-49bf-4128-9e77-b64d2d50d73d.png" style="width:18.33em;height:22.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"><span>The main file we are going to view is </span><kbd>con</kbd><kbd>troller.swift</kbd><span>. It contains the following code: </span></p>
<pre><strong>import UIKit<br/></strong><strong>class ViewController: UIViewController {<br/></strong><strong>    @IBOutlet weak var pictureImageView :UIImageView!<br/></strong><strong>    @IBOutlet weak var titleLabel :UILabel!</strong></pre>
<p>These are the outlets for the image-view control and title-label control in the main storyboard:</p>
<pre><strong>private var model : converted = converted()</strong></pre>
<p>This is the instance of the model that was generated when we added the <kbd>core-ml</kbd> file we created in the previous section:</p>
<pre><strong>    var content : [ String : String ] = [<br/></strong><strong>        "cheeseburger" : "A cheeseburger is a hamburger topped with cheese. Traditionally, the slice of cheese is placed on top of the meat patty, but the burger can include many variations in structure, ingredients, and composition.\nIt has 303 calories per 100 grams.",<br/></strong><strong>        "carbonara" : "Carbonara is an Italian pasta dish from Rome made with egg, hard cheese, guanciale, and pepper. The recipe is not fixed by a specific type of hard cheese or pasta. The cheese is usually Pecorino Romano.",<br/></strong><strong>        "meat loaf" : "Meatloaf is a dish of ground meat mixed with other ingredients and formed into a loaf shape, then baked or smoked. The shape is created by either cooking it in a loaf pan, or forming it by hand on a flat pan.\nIt has 149 calories / 100 grams",<br/></strong><strong>        "pizza" : "Pizza is a traditional Italian dish consisting of a yeasted flatbread typically topped with tomato sauce and cheese and baked in an oven. It can also be topped with additional vegetables, meats, and condiments, and can be made without cheese.\nIt has 285 calories / 100 grams"<em><br/></em></strong><strong><em>                                    ]</em></strong></pre>
<p class="mce-root">We hardcoded the contents to display in the title label for the corresponding class label we trained:</p>
<pre><strong>    let images = ["burger.jpg","pizza.png", "pasta.jpg","meatloaf.png"]</strong></pre>
<p>These are the images we have added to the project; they'll serve as input for our prediction app:</p>
<pre><strong>    var index = 0<br/></strong><strong>override func viewDidLoad() {<br/></strong><strong>        super.viewDidLoad()<br/></strong><strong>        nextImage()<br/></strong><strong>    }<br/></strong><strong>    @IBAction func nextButtonPressed() {<br/></strong><strong>        nextImage()<br/></strong><strong>    }<br/></strong><strong>    func nextImage() {<br/></strong><strong>        defer { index = index &lt; images.count - 1 ? index + 1 : 0 }</strong><strong> <br/></strong><strong>        let filename = images[index]<br/></strong><strong>        guard let img = UIImage(named: filename) else {<br/></strong><strong>            self.titleLabel.text = "Failed to load image \(filename)"<br/></strong><strong>            return<br/></strong><strong>        }<br/></strong><strong>        self.pictureImageView.image = img<br/></strong><strong>        let resizedImage = img.resizeTo(size: CGSize(width: 224, height: 224))<br/></strong><strong>        guard let buffer = resizedImage.toBuffer() else {<br/></strong><strong>            self.titleLabel.text = "Failed to make buffer from image \(filename)"<br/></strong><strong>            return<br/></strong><strong>        }</strong></pre>
<p>As we trained our model with 224 px images, we are also resizing the images of the input and converting it into an image buffer, which we want to give to the prediction method:</p>
<pre><strong>        do {<br/></strong><strong>            let prediction = try self.model.prediction(input: MymodelInput(input__0: buffer))</strong></pre>
<p>Here, we are inputting the image and getting the prediction results:</p>
<pre><strong>            if content.keys.contains(prediction.classLabel) {<br/></strong><strong>                self.titleLabel.text = content[prediction.classLabel]<br/></strong><strong>            }<br/></strong><strong>            else<br/></strong><strong>            {<br/></strong><strong>                self.titleLabel.text = prediction.classLabel;<br/></strong><strong>            }</strong></pre>
<p>In the preceding code, depending on the class label, we are displaying the content to the user:</p>
<pre><strong>        } catch let error {<br/></strong><strong>            self.titleLabel.text = error.localizedDescription<br/></strong><strong>        }<br/></strong><strong>    }<br/></strong><strong>}</strong></pre>
<p>This completes the application's creation. Now, we will execute the application to find the following images as output:</p>
<p class="CDPAlignCenter CDPAlign"> <img src="assets/10423fe5-1a92-4003-a7fd-b82cdca9a808.png" style="width:23.58em;height:42.08em;"/></p>
<p>Click on <span class="packt_screen">Next</span> to find the our next image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8ac3a3ed-ef50-4d24-88d0-4cb66649060e.png" style="width:20.83em;height:37.08em;"/> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handwritten digit recognition solution</h1>
                </header>
            
            <article>
                
<p>Previously, we created an application that <span>helped us get insights into the</span><span> </span><span>implementation of a neural network image recognition program using the TensorFlow model for mobile devices</span>. Now, we will create another application that uses the concept of a neural network and Keras for an image recognition program of handwritten digits. In this section, we will create an <span>application for a handwritten digit recognition solution on mobile devices using Keras. Then, we will convert this Keras model into a Core ML model and use it to build an iOS mobile application. Let's start by introducing you to Keras.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Keras</h1>
                </header>
            
            <article>
                
<p>Keras is a high-level neural network API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with the aim of enabling fast experimentation. </p>
<p>Here are some of the key uses of Keras:</p>
<ul>
<li>Allows for easy and fast prototyping (through user-friendliness, modularity, and extensibility)</li>
<li>Supports both convolutional networks and recurrent networks, as well as a combination of the two</li>
<li>Runs seamlessly on CPU and GPU</li>
</ul>
<p>Keras was designed on the following principles:</p>
<ul>
<li>User-friendliness</li>
<li>Modularity</li>
<li>Easy extensibility</li>
<li>Compatibility with Python</li>
</ul>
<p>To learn more about Keras, check out<span> </span><a href="https://keras.io/">https://keras.io/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Keras</h1>
                </header>
            
            <article>
                
<p>As we already discussed, Keras doesn't have its own backend system. As it is running on top of TensorFlow, CNTK, or Theano, we need to install one of these—personally, we recommend TensorFlow.</p>
<p>We need to install the<span> </span><kbd>h5py</kbd><span> </span>library, with the help of the<span> </span><kbd>pip</kbd><span> </span>package manager, in order to save the Keras models to disk:</p>
<pre>pip install tensorflow<br/>pip install keras<br/>pip install h5py</pre>
<p>The preceding commands will install the basic required libraries for the model, which we are going to create now.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Solving the problem</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to see a practical implementation of a neural network. We will define the problem statement, then we will understand the dataset we are going to use to solve the problem, whereupon we will create the model in Keras to solve the problem. Once the model is created in Keras, we will convert it into a model that's compatible with Core ML. This Core ML model will be imported into an iOS application, and a program will be written to use this model and interpret the handwritten digits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the problem statement</h1>
                </header>
            
            <article>
                
<p>We are going to tackle the problem of recognizing handwritten digits through a machine learning model that we'll implement in an iOS mobile application. The first step is to have the database of handwritten digits that can be used for model training and testing.</p>
<p><span>The MNIST digits dataset (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>) provides a database of handwritten digits, and has a training set of 60,000 examples and a test set of 10,000 examples. It is a subset of a larger set that's available from MNIST. The digits have been size-normalized and centered in a fixed-size image. </span>It is a good database for people who want to learn techniques and pattern recognition methods on real-world data while exerting minimal effort on preprocessing and formatting.</p>
<p>Before solving this problem, we will spend some time understanding the problem to see where the neural network can help. We can split the problem of recognizing handwritten digits into two sub-problems. Suppose we are given a handwritten number, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/11dd29c2-990c-4b00-a60e-24afae4c2880.png" style="width:18.00em;height:3.75em;"/></p>
<p>First, we need to break an image containing many digits into a sequence of separate images, each containing a single digit. For example, we'd like to break this image into seven separate images, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-990 image-border" src="assets/ae98dc4e-966b-481c-a6ee-c709aff7d024.png" style="width:30.83em;height:6.75em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>For humans, the digits can be easily separated, but it is very challenging for machines to do this simple task. Once the digits are separated, the program needs to classify each individual digit. So, for instance, we'd like our program to recognize that the first digit is a</span><span> </span><strong>5</strong><span>.</span></p>
<p>We are now trying to focus on the second part of the problem: to recognize the individual digits and classify them. We are going to use a neural network to solve the problem of recognizing individual, handwritten digits.</p>
<p>We can solve this problem using a 3-layer neural network, with the output layers having 10 neurons. The input layer and the hidden layers are where the processing happens. in the output layer, based on the neuron that fires, we can easily infer the digit that was recognized. Neurons 0 to 9 each identify one digit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem solution</h1>
                </header>
            
            <article>
                
<p>The problem solution consists of the following key steps:</p>
<ol>
<li>Preparing the data</li>
<li>Defining the model</li>
<li>Training and fitting the model</li>
<li>Converting the trained Keras model into a Core ML model</li>
<li>Writing the iOS mobile application</li>
</ol>
<p>Now, let's go through the steps one by one and see what we need to do in each of these steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>The first activity is the data preparation. To start, let's import all the required libraries. As we discussed earlier, we are going to use the MNIST database for the dataset of handwritten digits:</p>
<pre>from __future__ import print_function<br/>from matplotlib import pyplot as plt<br/>import keras<br/>from keras.datasets import mnist</pre>
<p><kbd>mnist</kbd><span> </span>is the dataset that contains the handwritten digits database, so we need to import that, as follows:</p>
<pre>from keras.models import Sequential</pre>
<p><span>The preceding code imports the <kbd>Sequential</kbd> model type from Keras. This is simply a linear stack of neural network layers:</span></p>
<pre>from keras.layers import Dense, Dropout, Flatten</pre>
<p><span>Now, we need to import the core layers from Keras. These are the layers that are used in almost any neural network:</span></p>
<pre>from keras.layers import Conv2D, MaxPooling2D</pre>
<p><span>Import the CNN layers from Keras. These are the convolutional layers that will help us efficiently train on image data:</span></p>
<pre>from keras.utils import np_utils</pre>
<p>Import Utils. This will help us do data transformation later:</p>
<pre>from keras import backend as K <br/>import coremltools</pre>
<p><kbd>coremltools</kbd><span> </span>will help us convert the Keras model into the Core ML model:</p>
<pre>(x_train, y_train), (x_val, y_val) = mnist.load_data()</pre>
<p><span>Load the pre-shuffled MNIST data into train and test sets:</span></p>
<pre># Inspect x data<br/>print('x_train shape: ', x_train.shape)<br/>print(x_train.shape[0], 'training samples')<br/>print('x_val shape: ', x_val.shape)<br/>print(x_val.shape[0], 'validation samples')<br/>print('First x sample\n', x_train[0])</pre>
<p>If you run the preceding code, it will show the shape of X, Y, and also the first record of X.</p>
<p><span>So, we have 60,000 samples in our training set, and the images are 28 x 28 pixels each. We can confirm this by plotting the first sample in <kbd>matplotlib</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60504c64-3e01-40f5-8435-9597e1956068.png" style="width:33.92em;height:21.75em;"/></p>
<pre>plt.imshow(x_train[0])</pre>
<p>This statement will use the<span> </span><kbd>matplotlib</kbd><span> </span>library to plot the first record of<span> </span><kbd>x_train</kbd>, which will give the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-957 image-border" src="assets/9a458001-b1a7-4714-8b7c-a5d60ec0a224.png" style="width:33.42em;height:28.00em;"/></p>
<p>The following lines will print the <kbd>y_train</kbd> shape and the first 10 elements in <kbd>y_train</kbd>: </p>
<pre>print('y_train shape: ', y_train.shape)<br/>print('First 10 y_train elements:', y_train[:10])</pre>
<p><span>The following code will find the input shape of the image. The MNIST image data values are of the</span> <kbd>uint8</kbd><span> type, in the <em>[0, 255]</em> range, but Keras needs values of the</span><span> </span><kbd>float32</kbd><span> type in the <em>[0, 1]</em> range:</span></p>
<pre>img_rows, img_cols = x_train.shape[1], x_train.shape[2]<br/>num_classes = 10<br/><br/># Set input_shape for channels_first or channels_last<br/>if K.image_data_format() == 'channels_first': <br/>x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)<br/>x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)<br/>input_shape = (1, img_rows, img_cols)<br/>else: <br/>    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)<br/>    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)<br/>    input_shape = (img_rows, img_cols, 1)                     <br/><br/>print('x_train shape:', x_train.shape)<br/># x_train shape: (60000, 28, 28, 1)<br/>print('x_val shape:', x_val.shape)<br/># x_val shape: (10000, 28, 28, 1)<br/>print('input_shape:', input_shape)</pre>
<p>Using the following code, we are converting the datatype to be compatible with the datatype that is defined in Keras:</p>
<pre>x_train = x_train.astype('float32')<br/>x_val = x_val.astype('float32')<br/>x_train /= 255<br/>x_val /= 255</pre>
<p>Now, we have a one-dimensional of 60,000 elements in<span> </span><kbd>y</kbd>. Let's convert it into a 60,000 x 10 array, as follows:</p>
<pre>y_train = np_utils.to_categorical(y_train, num_classes)<br/>y_val = np_utils.to_categorical(y_val, num_classes)<br/>print('New y_train shape: ', y_train.shape)<br/># (60000, 10)<br/>print('New y_train shape: ', y_train.shape)<br/># (60000, 10)<br/>print('First 10 y_train elements, reshaped:\n', y_train[:10])</pre>
<p>Now,<span> </span><kbd>y_train</kbd><span> </span>will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ba98ae88-78d7-428e-90ae-8f42166b6a71.png" style="width:17.42em;height:13.33em;"/></p>
<p>In the preceding array, we can find that for the presence of digits, the corresponding position will be filled with 1—all others will be filled with 0. For the first record, we can understand that the predicted digit is 5, because the 6th position (starting from 0) was filled with 1.</p>
<p>Now that the data preparation is complete, we need to define the model's architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the model's architecture</h1>
                </header>
            
            <article>
                
<p><span>Once the data preparation is completed, the next step is to define the model and create it, so let's create the model:</span></p>
<pre>model_m = Sequential()</pre>
<p>The preceding line will create a sequential model that will process the layers in the sequential way they are arranged. There are two ways to build Keras models,<span> </span>sequential<span> </span>and<span> </span>functional:</p>
<ul>
<li><strong>The<span> </span>sequential<span> </span>API</strong>: This allows us to create models layer-by-layer. Through this, we cannot create models that share layers or have multiple input or output.</li>
<li><strong>The<span> </span>functional<span> </span></strong><span><strong>API</strong>: This allows us to create models that are more than and can have complex connection layers—you can literally connect from any layer to any other layer:</span></li>
</ul>
<pre style="padding-left: 60px">model_m.add(Conv2D(32, (5, 5), input_shape=(1,28,28), activation='relu'))</pre>
<p>The input shape parameter should be the shape of <kbd>1</kbd> sample. In this case, it's the same<span> </span><kbd>(1, 28, 28)</kbd><span>, which</span> corresponds to the (depth, width, height) of each digit image.</p>
<p>But what do the other parameters represent? They correspond to the number of convolutional filters to use, the number of rows in each convolution kernel, and the number of columns in each convolution kernel, respectively:</p>
<pre>model_m.add(MaxPooling2D(pool_size=(2, 2)))</pre>
<p><span><kbd>MaxPooling2D</kbd> is a way to reduce the number of parameters in our model by sliding a 2 x 2 pooling filter across the previous layer and taking the max of the 4 values in the 2 x 2 filter:</span></p>
<pre>model_m.add(Dropout(0.5))</pre>
<p><span>This is a method for regularizing our model in order to prevent overfitting:</span></p>
<pre>model_m.add(Conv2D(64, (3, 3), activation='relu'))<br/>model_m.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_m.add(Dropout(0.2))<br/>model_m.add(Conv2D(128, (1, 1), activation='relu'))<br/>model_m.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_m.add(Dropout(0.2))<br/>model_m.add(Flatten())<br/>model_m.add(Dense(128, activation='relu'))<br/>model_m.add(Dense(num_classes, activation='softmax'))<br/>print(model_m.summary())</pre>
<p>Once you run the preceding lines of code, the model architecture's names of the layers will be printed in the console:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-958 image-border" src="assets/e2d7f91a-f566-4497-bec5-356f3064bb0f.png" style="width:33.25em;height:33.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling and fitting the model</h1>
                </header>
            
            <article>
                
<p>The next step is to compile and train the model. We put the model through the training phase with a series of iterations. <span>Epochs determine the number of iterations to be done on a model in the training phase. The weights will be passed to the layers defined in the model. A good number of Epochs will give greater accuracy and minimum loss. Here, we are using 10 Epochs.</span></p>
<p>Keras has a callback mechanism that will be called during each training iteration of the model, that is, at the end of each Epoch. In the callback method, we save the computed weights of that Epoch:</p>
<pre>callbacks_list = [<br/>    keras.callbacks.ModelCheckpoint(<br/>        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',<br/>        monitor='val_loss', save_best_only=True),<br/>    keras.callbacks.EarlyStopping(monitor='acc', patience=1)]</pre>
<p>Now, compile the model using the following code:</p>
<pre>model_m.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])</pre>
<p>The <kbd>categorical_crossentropy</kbd> loss function measures the distance between the probability distribution calculated by the CNN, and the true distribution of the labels.</p>
<p>An <kbd>optimizer</kbd> is the stochastic gradient descent algorithm that tries to minimize the loss function by following the gradient at just the right speed. <kbd>accuracy</kbd> the fraction of the images that were correctly classified—<span>this</span> is the most common metric monitored during training and testing:</p>
<pre># Hyper-parameters<br/>batch_size = 200<br/>epochs = 10</pre>
<p><span>Now, fit the model using the following code:</span></p>
<pre># Enable validation to use ModelCheckpoint and EarlyStopping callbacks.model_m.fit(<br/>    x_train, y_train, batch_size=batch_size, epochs=epochs,    callbacks=callbacks_list, validation_data=(x_val, y_val), verbose=1)</pre>
<p>Once the program finishes executing, you will find files in your running directory with the <kbd>best_model.01-0.15.h5</kbd><span> </span>name. This states<span> </span><kbd>best_model.{epoch number}-{loss value}.h5</kbd>.</p>
<p>This the Keras model that was created and trained for the given dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting the Keras model into the Core ML model</h1>
                </header>
            
            <article>
                
<p>Now that the Keras model has been created, the next step is to convert the Keras model into the Core ML model. For the first argument, use the filename of the newest<span> </span><kbd>.h5</kbd><span> </span>file in the notebook folder:</p>
<pre><span>output_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']</span><br/>coreml_mnist = coremltools.converters.keras.convert(<br/>    'best_model.10-0.04.h5', input_names=['image'], output_names=['output'],    class_labels=output_labels, image_input_names='image')<br/>coreml_mnist.save("minsit_classifier.mlmodel")</pre>
<p>Once you successfully run the code, you will find the<span> </span><kbd>minsit_classifer.mlmodel</kbd><span> </span>file created in your directory. We are going to use this to create an iOS mobile application to detect the digits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the iOS mobile application</h1>
                </header>
            
            <article>
                
<p>Now, we are going to create the iOS app. You can download the code from our Packt GitHub repository in the<span> </span><kbd>ImageClassificationwithVisionandCoreML</kbd><span> </span>folder.</p>
<p>Open the project in Xcode9+; the project structure will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e59ce2a-bd3d-4f4d-b86f-ce982b01de99.png" style="width:14.92em;height:20.00em;"/></p>
<p>If you open<span> </span><kbd>main.storyboard</kbd><span> </span>in your designer, you will see the following UI:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bade5a63-117d-4e93-ad49-65b8b28b932f.png" style="width:24.25em;height:41.50em;"/></p>
<p>Most code is common iOS code. Check out the following piece of code, which is of specific interest to us, and includes the handwritten digit prediction code:</p>
<pre>lazy var classificationRequest: VNCoreMLRequest = {<br/>        // Load the ML model through its generated class and create a Vision request for it.<br/>        do {<br/>            let model = try VNCoreMLModel(for: MNISTClassifier().model)<br/>            return VNCoreMLRequest(model: model, completionHandler: self.handleClassification)<br/>        } catch {<br/>            fatalError("can't load Vision ML model: \(error)")<br/>        }<br/>    }()<br/>    func handleClassification(request: VNRequest, error: Error?) {<br/>        guard let observations = request.results as? [VNClassificationObservation]<br/>            else { fatalError("unexpected result type from VNCoreMLRequest") }<br/>        guard let best = observations.first<br/>            else { fatalError("can't get best result") }        DispatchQueue.main.async {<br/>            self.classificationLabel.text = "Classification: \"\(best.identifier)\" Confidence: \(best.confidence)"<br/>        }<br/>    }</pre>
<p>It contains two buttons at the bottom: one to pick an image from mobile and another option to take a snapshot. Please note that the camera will not work if you are running this in simulators.</p>
<p>You can build and run the app in a simulator. Once the app successfully opens in a simulator, drag the image of the handwritten digit 6 into the folder example image into the simulator<span>–</span>this will save the file in the simulator's memory.</p>
<p>Return to the app and select the dragged image that was saved in the device's memory. It will show the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/add4696c-39dc-4307-9f16-b797634bef23.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered the concept of neural networks and their use in the field of mobile machine learning. We created an application to recognize images using TensorFlow and Core ML in iOS and Xcode. We also explored the Keras deep learning framework. We tried to solve the handwritten digit recognition problem using a neural network in Keras. We built the Keras machine learning model to solve this problem. Then, we converted this model into a Core ML model using Core ML conversion tools. We used this Core ML model in an iOS mobile application to perform the handwritten digit recognition.</p>
<p>In the next chapter, we will learn how to use the Google Cloud Vision label detection technique in Android.</p>


            </article>

            
        </section>
    </body></html>