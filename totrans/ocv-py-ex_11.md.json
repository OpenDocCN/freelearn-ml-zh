["```py\nimport argparse\n\nimport cv2\nimport numpy as np\n\ndef build_arg_parser():\n    parser = argparse.ArgumentParser(description='Find fundamental matrix \\\n            using the two input stereo images and draw epipolar lines')\n    parser.add_argument(\"--img-left\", dest=\"img_left\", required=True,\n            help=\"Image captured from the left view\")\n    parser.add_argument(\"--img-right\", dest=\"img_right\", required=True,\n            help=\"Image captured from the right view\")\n    parser.add_argument(\"--feature-type\", dest=\"feature_type\",\n            required=True, help=\"Feature extractor that will be used; can be either 'sift' or 'surf'\")\n    return parser\n\ndef draw_lines(img_left, img_right, lines, pts_left, pts_right):\n    h,w = img_left.shape\n    img_left = cv2.cvtColor(img_left, cv2.COLOR_GRAY2BGR)\n    img_right = cv2.cvtColor(img_right, cv2.COLOR_GRAY2BGR)\n\n    for line, pt_left, pt_right in zip(lines, pts_left, pts_right):\n        x_start,y_start = map(int, [0, -line[2]/line[1] ])\n        x_end,y_end = map(int, [w, -(line[2]+line[0]*w)/line[1] ])\n        color = tuple(np.random.randint(0,255,2).tolist())\n        cv2.line(img_left, (x_start,y_start), (x_end,y_end), color,1)\n        cv2.circle(img_left, tuple(pt_left), 5, color, -1)\n        cv2.circle(img_right, tuple(pt_right), 5, color, -1)\n\n    return img_left, img_right\n\ndef get_descriptors(gray_image, feature_type):\n    if feature_type == 'surf':\n        feature_extractor = cv2.SURF()\n\n    elif feature_type == 'sift':\n        feature_extractor = cv2.SIFT()\n\n    else:\n        raise TypeError(\"Invalid feature type; should be either 'surf' or 'sift'\")\n\n    keypoints, descriptors = feature_extractor.detectAndCompute(gray_image, None)\n    return keypoints, descriptors\n\nif __name__=='__main__':\n    args = build_arg_parser().parse_args()\n    img_left = cv2.imread(args.img_left,0)  # left image\n    img_right = cv2.imread(args.img_right,0)  # right image\n    feature_type = args.feature_type\n\n    if feature_type not in ['sift', 'surf']:\n        raise TypeError(\"Invalid feature type; has to be either 'sift' or 'surf'\")\n\n    scaling_factor = 1.0\n    img_left = cv2.resize(img_left, None, fx=scaling_factor,\n                fy=scaling_factor, interpolation=cv2.INTER_AREA)\n    img_right = cv2.resize(img_right, None, fx=scaling_factor,\n                fy=scaling_factor, interpolation=cv2.INTER_AREA)\n\n    kps_left, des_left = get_descriptors(img_left, feature_type)\n    kps_right, des_right = get_descriptors(img_right, feature_type)\n\n    # FLANN parameters\n    FLANN_INDEX_KDTREE = 0\n    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n    search_params = dict(checks=50)\n\n    # Get the matches based on the descriptors\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n    matches = flann.knnMatch(des_left, des_right, k=2)\n\n    pts_left_image = []\n    pts_right_image = []\n\n    # ratio test to retain only the good matches\n    for i,(m,n) in enumerate(matches):\n        if m.distance < 0.7*n.distance:\n            pts_left_image.append(kps_left[m.queryIdx].pt)\n            pts_right_image.append(kps_right[m.trainIdx].pt)\n\n    pts_left_image = np.float32(pts_left_image)\n    pts_right_image = np.float32(pts_right_image)\n    F, mask = cv2.findFundamentalMat(pts_left_image, pts_right_image, cv2.FM_LMEDS)\n\n    # Selecting only the inliers\n    pts_left_image = pts_left_image[mask.ravel()==1]\n    pts_right_image = pts_right_image[mask.ravel()==1]\n\n    # Drawing the lines on left image and the corresponding feature points on the right image\n    lines1 = cv2.computeCorrespondEpilines (pts_right_image.reshape(-1,1,2), 2, F)\n    lines1 = lines1.reshape(-1,3)\n    img_left_lines, img_right_pts = draw_lines(img_left, img_right, lines1, pts_left_image, pts_right_image)\n\n    # Drawing the lines on right image and the corresponding feature points on the left image\n    lines2 = cv2.computeCorrespondEpilines (pts_left_image.reshape(-1,1,2), 1,F)\n    lines2 = lines2.reshape(-1,3)\n    img_right_lines, img_left_pts = draw_lines(img_right, img_left, lines2, pts_right_image, pts_left_image)\n\n    cv2.imshow('Epi lines on left image', img_left_lines)\n    cv2.imshow('Feature points on right image', img_right_pts)\n    cv2.imshow('Epi lines on right image', img_right_lines)\n    cv2.imshow('Feature points on left image', img_left_pts)\n    cv2.waitKey()\n    cv2.destroyAllWindows()\n```", "```py\nimport argparse\n\nimport cv2\nimport numpy as np\n\ndef build_arg_parser():\n    parser = argparse.ArgumentParser(description='Reconstruct the 3D map from \\\n            the two input stereo images. Output will be saved in \\'output.ply\\'')\n    parser.add_argument(\"--image-left\", dest=\"image_left\", required=True,\n            help=\"Input image captured from the left\")\n    parser.add_argument(\"--image-right\", dest=\"image_right\", required=True,\n            help=\"Input image captured from the right\")\n    parser.add_argument(\"--output-file\", dest=\"output_file\", required=True,\n            help=\"Output filename (without the extension) where the point cloud will be saved\")\n    return parser\n\ndef create_output(vertices, colors, filename):\n    colors = colors.reshape(-1, 3)\n    vertices = np.hstack([vertices.reshape(-1,3), colors])\n\n    ply_header = '''ply\n        format ascii 1.0\n        element vertex %(vert_num)d\n        property float x\n        property float y\n        property float z\n        property uchar red\n        property uchar green\n        property uchar blue\n        end_header\n    '''\n\n    with open(filename, 'w') as f:\n        f.write(ply_header % dict(vert_num=len(vertices)))\n        np.savetxt(f, vertices, '%f %f %f %d %d %d')\n\nif __name__ == '__main__':\n    args = build_arg_parser().parse_args()\n    image_left = cv2.imread(args.image_left)\n    image_right = cv2.imread(args.image_right)\n    output_file = args.output_file + '.ply'\n\n    if image_left.shape[0] != image_right.shape[0] or \\\n            image_left.shape[1] != image_right.shape[1]:\n        raise TypeError(\"Input images must be of the same size\")\n\n    # downscale images for faster processing\n    image_left = cv2.pyrDown(image_left)\n    image_right = cv2.pyrDown(image_right)\n\n    # disparity range is tuned for 'aloe' image pair\n    win_size = 1\n    min_disp = 16\n    max_disp = min_disp * 9\n    num_disp = max_disp - min_disp   # Needs to be divisible by 16\n    stereo = cv2.StereoSGBM(minDisparity = min_disp,\n        numDisparities = num_disp,\n        SADWindowSize = win_size,\n        uniquenessRatio = 10,\n        speckleWindowSize = 100,\n        speckleRange = 32,\n        disp12MaxDiff = 1,\n        P1 = 8*3*win_size**2,\n        P2 = 32*3*win_size**2,\n        fullDP = True\n    )\n\n    print \"\\nComputing the disparity map ...\"\n    disparity_map = stereo.compute(image_left, image_right).astype(np.float32) / 16.0\n\n    print \"\\nGenerating the 3D map ...\"\n    h, w = image_left.shape[:2]\n    focal_length = 0.8*w\n\n    # Perspective transformation matrix\n    Q = np.float32([[1, 0, 0, -w/2.0],\n                    [0,-1, 0,  h/2.0],\n                    [0, 0, 0, -focal_length],\n                    [0, 0, 1, 0]])\n\n    points_3D = cv2.reprojectImageTo3D(disparity_map, Q)\n    colors = cv2.cvtColor(image_left, cv2.COLOR_BGR2RGB)\n    mask_map = disparity_map > disparity_map.min()\n    output_points = points_3D[mask_map]\n    output_colors = colors[mask_map]\n\n    print \"\\nCreating the output file ...\\n\"\n    create_output(output_points, output_colors, output_file)\n```"]