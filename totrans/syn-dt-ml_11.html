<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer083">
<h1 class="chapter-number" id="_idParaDest-185"><a id="_idTextAnchor188"/>11</h1>
<h1 id="_idParaDest-186"><a id="_idTextAnchor189"/>Case Study 2 – Natural Language Processing</h1>
<p>This chapter introduces you to <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), where synthetic data is a key player. You will explore various applications of NLP models. Additionally, you will learn why these models usually require large-scale training datasets to converge and perform well in practice. At the same time, you will comprehend why synthetic data is the future of NLP. The discussion will be supported by a practical, hands-on example, as well as many interesting case studies from research and <span class="No-Break">industry fields.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>A brief introduction <span class="No-Break">to NLP</span></li>
<li>The need for large-scale training datasets <span class="No-Break">in NLP</span></li>
<li>Hands-on practical example <span class="No-Break">with ChatGPT</span></li>
<li>Synthetic data as a solution for <span class="No-Break">NLP problems</span></li>
</ul>
<h1 id="_idParaDest-187"><a id="_idTextAnchor190"/>A brief introduction to NLP</h1>
<p>NLP is an interdisciplinary field that combines computer science, ML, and linguistics. It gives computers <a id="_idIndexMarker493"/>the ability to understand, analyze, and respond to natural language texts, written or spoken. The field of NLP is evolving for many reasons, including the availability of big data and powerful computational <a id="_idIndexMarker494"/>resources such as <strong class="bold">Graphic</strong><strong class="bold">s</strong><strong class="bold"> Processing Units</strong> (<strong class="bold">GPUs</strong>) and <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>). Examples of state-of-the-art <a id="_idIndexMarker495"/>NLP models include <em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> (<a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>), <em class="italic">ChatGPT</em> (<a href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt</a>), and <em class="italic">Google Bard</em> (<a href="https://bard.google.com">https://bard.google.com</a>). Next, let’s explore some of the key applications of NLP models <span class="No-Break">in practice.</span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor191"/>Applications of NLP in practice</h2>
<p>Some common <a id="_idIndexMarker496"/>applications of NLP models are shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 11.1 – Samples of key applications of NLP models in practice" height="426" src="image/Figure_11_01_B18494.jpg" width="903"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Samples of key applications of NLP models in practice</p>
<p>Let’s now discuss some of these applications in <span class="No-Break">more detail.</span></p>
<h3>Text and speech translation</h3>
<p>This is the task of translating text or a speech from one language to another. Usually, a large-scale <a id="_idIndexMarker497"/>text corpus, composed of a huge number of sentences translated from one language to another, is used to train such models. <em class="italic">Google Translate</em> (<a href="https://translate.google.co.uk">https://translate.google.co.uk</a>), <em class="italic">Microsoft Translator</em> (<a href="https://translator.microsoft.com">https://translator.microsoft.com</a>), and <em class="italic">iTranslate</em> (<a href="https://itranslate.com">https://itranslate.com</a>) are all examples of generic translation NLP models. There are also domain- or field-specific NLP-based translators, such as <em class="italic">Lingua Custodia</em> (<a href="https://www.linguacustodia.finance">https://www.linguacustodia.finance</a>) and <em class="italic">Trados</em> (<a href="https://www.trados.com">https://www.trados.com</a>), which are more specific to the <span class="No-Break">financial field.</span></p>
<h3>Sentiment analysis</h3>
<p>This is a major <a id="_idIndexMarker498"/>task in the NLP field. It aims at analyzing and classifying texts based on the sentiment or emotions embedded in the text. It is usually used by companies to understand customer feedback, assess their services, and identify issues. For example, it is commonly employed to classify customer reviews on items or services as <em class="italic">positive</em>, <em class="italic">negative</em>, or <em class="italic">neutral</em>. Additionally, it is often applied to identify emotions in text, such as anger, sadness, dissatisfaction, frustration, and happiness. For example, <em class="italic">Medallia Text Analytics</em> utilizes NLP to provide a quick summary of <a id="_idIndexMarker499"/>market trends and customer feedback and comments on services and products. For more details, please refer to the Medallia Text Analytics website (<a href="https://www.medallia.com/resource/text-analytics-solution-brochure">https://www.medallia.com/resource/text-analytics-solution-brochure</a>). Furthermore, for recent examples of using sentiment analysis in practice, please refer to <em class="italic">Identification of opinion trends using sentiment analysis of airlines passengers’ reviews</em> (<a href="https://doi.org/10.1016/j.jairtraman.2022.102232">https://doi.org/10.1016/j.jairtraman.2022.102232</a>) and <em class="italic">A Novel Approach for Sentiment Analysis and Opinion Mining on Social Media </em><span class="No-Break"><em class="italic">Tweets</em></span><span class="No-Break"> (</span><a href="https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15"><span class="No-Break">https://link.springer.com/chapter/10.1007/978-981-19-2358-6_15</span></a><span class="No-Break">).</span></p>
<h3>Text summarization</h3>
<p>This is the task of generating a summary of text or a document in a human-like way by capturing the essence or the main points. It is a complex process as the NLP model needs to learn how to focus on the essential parts of the text, which is a context-dependent task. However, NLP models have shown great progress in this area recently. There are many examples of NLP models that can summarize large blocks of text, such as the <em class="italic">plnia Text Summarization API</em> (<a href="https://www.plnia.com/products/text-summarization-api">https://www.plnia.com/products/text-summarization-api</a>) and <em class="italic">NLP Cloud’s Summarization </em><span class="No-Break"><em class="italic">API</em></span><span class="No-Break"> (</span><a href="https://nlpcloud.com/nlp-text-summarization-api.xhtml"><span class="No-Break">https://nlpcloud.com/nlp-text-summarization-api.xhtml</span></a><span class="No-Break">).</span></p>
<h3>Test-to-scene generation</h3>
<p>This is another essential task that relies extensively on <a id="_idIndexMarker500"/>NLP. It aims at generating virtual scenes given descriptive textual input. It has many interesting applications in game development, the metaverse, advertising, and education. One of the main advantages of text-to-scene generation is that it allows users to generate diverse and photorealistic scenes without requiring a background in computer graphics, game development, and programming. Text-to-scene methods are usually based on GANs, VAEs, diffusion models, and Transformers. For more information, please refer to <em class="italic">Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields</em> (<a href="https://arxiv.org/pdf/2305.11588.pdf">https://arxiv.org/pdf/2305.11588.pdf</a>) and <em class="italic">SceneSeer: 3D Scene Design with Natural </em><span class="No-Break"><em class="italic">Language</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/1703.00050.pdf"><span class="No-Break">https://arxiv.org/pdf/1703.00050.pdf</span></a><span class="No-Break">).</span></p>
<h3>Text-to-image generation</h3>
<p>In this task, NLP models generate images based on textual descriptions provided by the user. In this task, the model aims at creating a visual representation controlled by the textual input. The task of generating images from text has many attractive applications, such as data augmentation, content generation, e-commerce, and advertising. You can take <em class="italic">DALL-E2</em> (<a href="https://openai.com/product/dall-e-2">https://openai.com/product/dall-e-2</a>) and <em class="italic">Stable Diffusion</em> (<a href="https://stablediffusionweb.com">https://stablediffusionweb.com</a>) as examples. They can generate photo-realistic images given a <span class="No-Break">descriptive text.</span></p>
<p>In the next section, we will learn why we need large-scale datasets to successfully train <span class="No-Break">NLP models.</span></p>
<h1 id="_idParaDest-189"><a id="_idTextAnchor192"/>The need for large-scale training datasets in NLP</h1>
<p>NLP models require large-scale training datasets to perform well in practice. In this section, you will understand why NLP models need a substantial amount of training data <span class="No-Break">to converge.</span></p>
<p>ML models in general required a huge number of training samples to cover in practice. NLP models require even more training data compared to other ML fields. There are many reasons for that. Next, let’s discuss the main ones, which are <span class="No-Break">as follows:</span></p>
<ul>
<li>Human <span class="No-Break">language complexity</span></li>
<li><span class="No-Break">Contextual dependence</span></li>
<li><span class="No-Break">Generalization</span></li>
</ul>
<h2 id="_idParaDest-190"><a id="_idTextAnchor193"/>Human language complexity</h2>
<p>Recent research shows that a huge proportion of our brains is used for language understanding. At the same time, it is still a research problem to understand how different brain regions communicate with each other while reading, writing, or carrying out other language-related activities. For more information, please refer to <em class="italic">A review and synthesis of the first 20years of PET and fMRI studies of heard speech, spoken language and reading</em> (<a href="https://doi.org/10.1016/j.neuroimage.2012.04.062">https://doi.org/10.1016/j.neuroimage.2012.04.062</a>). Additionally, infants’ basic speech and vision functionalities are developed by 8 to 12 months of age. However, it takes them a few years to use verbal or textual communication appropriately. Thus, language processing is not only hard for computers but also for humans. What makes the problem much harder for machines is the need to learn grammar, expressions, and metaphors. Thus, NLP models require substantial training data to learn these hidden rules <span class="No-Break">and patterns.</span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor194"/>Contextual dependence</h2>
<p>While most ML tasks are still context dependent, such as computer vision, the contextual dependence is more severe and intense with NLP problems. For example, the meaning of a sentence can change from declarative to interrogative based on the speaker’s tone of voice. The ordering of the words and the previous and next few sentences may also change the meaning and imply different interpretations. Even the same words sometimes have different meanings based on the context. For example, “light” can be used as a noun to mean “illumination” or an adjective to mean “little weight.” Thus, to master these scenarios, the NLP models need to be trained on a diverse dataset that includes enough examples to cover <span class="No-Break">these contexts.</span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor195"/>Generalization</h2>
<p>Another issue that makes the NLP task even harder is the huge number of languages, dialects, and varieties. For instance, British English is different from American English in some ways. It is not just different in the spelling but also in the words, idioms, and expressions used. Even within the same country with the same language, people in different cities use different dialects. For NLP models to be useful and practical, they should accommodate most of these variations. Thus, large-scale training data is usually required to capture <span class="No-Break">these variations.</span></p>
<p>In the next section, we will learn how to generate synthetic data for a practical example <span class="No-Break">in NLP.</span></p>
<h1 id="_idParaDest-193"><a id="_idTextAnchor196"/>Hands-on practical example with ChatGPT</h1>
<p>Let’s imagine we want to train an NLP model to identify positive and negative reviews of a book about ML. We will use <strong class="bold">ChatGPT</strong> (<a href="https://chat.openai.com">https://chat.openai.com</a>) to generate a list of positive and negative reviews. Then, we will deploy these reviews to augment our real <span class="No-Break">training dataset.</span></p>
<p>To get 10 positive reviews, we can simply use the following prompt: <strong class="source-inline">Give me 10 good reviews about a book for Machine Learning</strong>. ChatGPT will generate the <span class="No-Break">following reviews:</span></p>
<ul>
<li>Mastering Machine Learning is an exceptional book for both beginners and seasoned professionals. The author’s clear and concise explanations make complex concepts easy to grasp, ensuring a solid foundation for anyone diving into the world <span class="No-Break">of ML.</span></li>
<li>This book strikes the perfect balance between theory and practical implementation. The hands-on exercises and real-world examples provide valuable insights, making it an invaluable resource for anyone looking to apply ML in <span class="No-Break">their projects.</span></li>
<li>I appreciate how the author covers a wide range of ML algorithms and techniques, going beyond the usual suspects. It’s a well-rounded guide that truly equips readers with the knowledge needed to tackle diverse <span class="No-Break">ML challenges.</span></li>
<li>The author’s writing style is engaging and approachable, making it feel like a friendly mentor guiding you through the ML journey. Complex topics are broken down into manageable pieces, making the learning <span class="No-Break">process enjoyable.</span></li>
<li>The book’s emphasis on <a id="_idIndexMarker501"/>understanding the math behind ML algorithms sets it apart from others. It builds a solid mathematical foundation without overwhelming readers, which is crucial for aspiring <span class="No-Break">ML practitioners.</span></li>
<li>The book’s code samples are well-documented and easily reproducible. The practical exercises are a fantastic way to reinforce learning and experiment with ML concepts in a <span class="No-Break">hands-on manner.</span></li>
<li>One of the highlights of this book is its practical advice on model selection and hyperparameter tuning. These insights save time and help avoid common pitfalls in <span class="No-Break">ML projects.</span></li>
<li>As someone with a non-technical background, I found this book extremely accessible. The author takes the time to explain even the most complex concepts in a manner that anyone <span class="No-Break">can understand.</span></li>
<li>The book stays up-to-date with the latest trends and advancements in the field of ML. It includes coverage of recent breakthroughs and contemporary applications, making it a relevant and <span class="No-Break">forward-looking resource.</span></li>
<li>This book is a true gem for aspiring data scientists and ML enthusiasts. It not only teaches ML techniques but also imparts a problem-solving mindset, empowering readers to tackle real-world data challenges <span class="No-Break">with confidence.</span></li>
</ul>
<p>Similarly, to get negative reviews, we may use the <strong class="source-inline">Give me 10 negative reviews about a book for Machine Learning</strong> prompt. We get the following <span class="No-Break">negative reviews:</span></p>
<ul>
<li>This book was a complete disappointment. The explanations were convoluted, and the author seemed more interested in showcasing their knowledge than helping readers understand the concepts. I felt lost throughout the entire <span class="No-Break">reading experience.</span></li>
<li>The book claims to be for beginners, but it assumes a level of prior knowledge that I just didn’t have. The lack of clear explanations and practical examples made it nearly impossible for me to <span class="No-Break">follow along.</span></li>
<li>I found numerous errors in the code samples provided. It was frustrating to spend hours trying to debug the examples when I was supposed to be learning. Sloppy editing and poor code quality were <span class="No-Break">major drawbacks.</span></li>
<li>The book’s content felt outdated, given the rapidly evolving field of ML. I expected to learn about cutting-edge techniques, but instead, I got a rehash of old material that’s easily <span class="No-Break">available online.</span></li>
<li>The book had a promising start, but it quickly became overly technical and hard to digest. The author assumed readers had a PhD in mathematics, making it inaccessible to anyone without an <span class="No-Break">advanced background.</span></li>
<li>I was excited to see practical exercises, but the ones in this book were poorly explained and poorly executed. The lack of step-by-step guidance made it impossible to apply the <span class="No-Break">concepts effectively.</span></li>
<li>The book’s organization was chaotic, jumping between topics without any clear progression. It left me feeling confused and struggling to connect the dots <span class="No-Break">between chapters.</span></li>
<li>The examples provided were unrealistic and didn’t reflect real-world scenarios. I had trouble relating the concepts to practical applications, which defeated the purpose of learning ML in the <span class="No-Break">first place.</span></li>
<li>The book promised to be comprehensive, but it lacked depth in crucial areas like neural networks and deep learning. I was hoping for a well-rounded understanding, but instead, I found huge gaps in <span class="No-Break">the content.</span></li>
<li>The writing style was dry and monotonous, making it difficult to maintain interest in the subject matter. I found myself dozing off during what should have been exciting topics in the world of <span class="No-Break">Machine Learning.</span></li>
</ul>
<p>These positive and negative synthetic reviews can be used to augment our training dataset. As you can see, it is straightforward to generate an unlimited number of training samples to augment and complement our training data. The generated samples are diverse, relevant, accurate, detailed, and structured. Thus, including such samples in our training dataset can improve the performance of our <span class="No-Break">NLP model.</span></p>
<p>Next, let’s delve into three practical examples of using NLP models <span class="No-Break">in industry.</span></p>
<h1 id="_idParaDest-194"><a id="_idTextAnchor197"/>Synthetic data as a solution for NLP problems</h1>
<p>In this section, you will understand how companies are leveraging synthetic data as a solution for their NLP-based problems. We will look at four <span class="No-Break">case studies:</span></p>
<ul>
<li>SYSTRAN Soft’s use of <span class="No-Break">synthetic data</span></li>
<li>Telefónica’s use of <span class="No-Break">synthetic data</span></li>
<li>Clinical text mining utilizing <span class="No-Break">synthetic data</span></li>
<li>The Alexa virtual <span class="No-Break">assistant model</span></li>
</ul>
<h2 id="_idParaDest-195"><a id="_idTextAnchor198"/>SYSTRAN Soft’s use of synthetic data</h2>
<p><strong class="bold">Neural Machine Translation</strong> (<strong class="bold">NMT</strong>) is a promising approach in NLP. It utilizes neural networks to learn statistical models and thus perform the translation task. The typical architecture is composed of an encoder-decoder, which is usually trained on large-scale training datasets. These models were shown to achieve excellent results in practice. However, they also have <a id="_idIndexMarker502"/>some limitations, as we will see with the SYSTRAN <span class="No-Break">case study.</span></p>
<p>SYSTRAN is one of the few pioneering companies in the field of machine translation technology (<a href="https://www.systransoft.com">https://www.systransoft.com</a>). While their standard and traditional NLP models achieved state-of-the-art results, they struggled under two main scenarios: translating long sentences and translating short titles, such as titles of news articles. To solve these issues, they explored augmenting their real training data with synthetic data specially generated for that aim. They were able to solve these issues and boost the overall performance. For more information, please refer to <em class="italic">SYSTRAN’s Pure Neural Machine Translation </em><span class="No-Break"><em class="italic">Systems</em></span><span class="No-Break"> (</span><a href="https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf"><span class="No-Break">https://blog.systransoft.com/wp-content/uploads/2016/10/SystranNMTReport.pdf</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor199"/>Telefónica’s use of synthetic data</h2>
<p>In telecommunication industries, it is essential to collect data about customers to analyze their needs, identify issues, and customize the provided services. This helps these companies <a id="_idIndexMarker503"/>to establish a stronger reputation and thus be more successful in the market. The issue is usually not data availability but the regulations <a id="_idIndexMarker504"/>that limit utilizing customers’ data to train NLP or ML models <span class="No-Break">in general.</span></p>
<p>Telefónica deployed an elegant solution to address these issues. They used the <em class="italic">MOSTLY AI</em> synthetic data platform to synthesize a new dataset from the original customer dataset <em class="italic">Telefónica’s CRM Datamart</em>. The newly generated synthetic data now meets the requirements of GDPR as it does not contain any real information about customers. At the same time, the synthetic dataset has patterns, correlations, and statistical properties that can be seen in the original real dataset. Thus, it can be used as a replica of the real dataset to train NLP models. This allowed the company to use up to 85% of the customer data, which was not possible with real data-based <span class="No-Break">NLP models.</span></p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor200"/>Clinical text mining utilizing synthetic data</h2>
<p>A recent study conducted by researchers at Rice University and Texas A&amp;M University, as well as <a id="_idIndexMarker505"/>other collaborators, investigated the usability of synthetic data generation models such as ChatGPT on clinical text mining. Their <a id="_idIndexMarker506"/>aim was to use <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) to help with clinical texting mining. They deployed LLMs to recognize biological named entities from unstructured healthcare textual data. They interestingly found that using ChatGPT, which was directly trained on real data for this task, did not achieve a satisfactory performance. Developing a synthetic data generation pipeline and generating the necessary synthetic data dramatically improved the performance of their models. The F1-score increased from 23.37% to 63.99%, which is a significant increase. Additionally, they highlighted that their synthetic-data-based model now better addresses and mitigates privacy concerns compared to the real-data-based one. For more information, please refer to <em class="italic">Does Synthetic Data Generation of LLMs Help Clinical Text </em><span class="No-Break"><em class="italic">Mining?</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/2303.04360.pdf"><span class="No-Break">https://arxiv.org/pdf/2303.04360.pdf</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor201"/>The Alexa virtual assistant model</h2>
<p>Virtual assistant models, such as Alexa by Amazon, Siri by Apple, and Google Assistant by Google, are becoming an integral part of our modern lives. They provide enormous services, such as <a id="_idIndexMarker507"/>ordering products, controlling home appliances, and voice searching. For these tools to become beneficial for a wider audience, they need to support many languages and dialects, which requires large-scale <span class="No-Break">training datasets.</span></p>
<p>One of the <a id="_idIndexMarker508"/>main issues the Alexa virtual assistant encountered when Amazon launched the model for three new languages, Hindi, US Spanish, and Brazilian Portuguese, was the scarcity of real training data. As a solution, Amazon leveraged the available limited real data to create “templates.” Then, they deployed these templates to generate synthetic data that augmented and complemented the real data. For example, they utilized the available real data in these languages to learn the essential grammar and syntax of the languages. Then, they leveraged the trained models to generate a sufficiently large synthetic training dataset, which consisted of novel sentences following the grammar and syntax of these languages. This elegant synthetic-data-based solution helped Amazon to mitigate real data insufficiency and thus helped the company to provide more accurate virtual assistants for a broader audience with even better performance. Consequently, Amazon successfully got more orders and higher profitability. For more information, please refer to <em class="italic">Tools for generating synthetic data helped bootstrap Alexa’s new-language </em><span class="No-Break"><em class="italic">releases</em></span><span class="No-Break"> (</span><a href="https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases"><span class="No-Break">https://www.amazon.science/blog/tools-for-generating-synthetic-data-helped-bootstrap-alexas-new-language-releases</span></a><span class="No-Break">).</span></p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor202"/>Summary</h1>
<p>In this chapter, we introduced NLP models and explored the main applications of these models in practice. Additionally, we learned that NLP models require large-scale datasets. Then, we thoroughly discussed the main reasons for that. Following this, we studied a few examples from industry and research where synthetic data was successfully deployed. In the next chapter, we will delve into another set of interesting case studies where synthetic data has been successfully deployed in the predictive <span class="No-Break">analytics field.</span></p>
</div>
</div></body></html>