- en: '*Chapter 6*: Feature Engineering and Labeling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：特征工程和标注'
- en: In the previous chapter, we learned how to clean our data and do basic statistical
    analysis. In this chapter, we will delve into two more types of actions we must
    perform before we can start our ML training. These two steps are the most important
    of all besides efficiently cleaning your dataset, and to be good at them, you
    will require a high amount of experience. This chapter will give you a basis to
    build upon.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何清理我们的数据并进行基本统计分析。在本章中，我们将深入探讨在开始我们的机器学习训练之前必须执行的两种更多类型的操作。这两个步骤是所有步骤中最重要的，除了高效地清理数据集之外，而且要擅长它们，你需要有大量的经验。本章将为你提供一个基础来构建。
- en: In the first section, we will learn about feature engineering. We will understand
    the process, how to select predictive features from our dataset, and what methods
    exist to transform features from our dataset to make them usable for our ML algorithm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们将学习特征工程。我们将了解这个过程，如何从我们的数据集中选择预测特征，以及将我们的数据集中的特征转换为可用于我们的机器学习算法的方法。
- en: In the second section, we will look at data labeling. Most ML algorithms fall
    into the category of supervised learning, which means they require labeled training
    data. We will look at some typical scenarios that require labels and learn how
    Azure Machine Learning can help with this tedious task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将探讨数据标注。大多数机器学习算法属于监督学习类别，这意味着它们需要标注的训练数据。我们将探讨一些需要标签的典型场景，并学习Azure机器学习如何帮助完成这项繁琐的任务。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding and applying feature engineering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和应用特征工程
- en: Handling data labeling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据标注
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will use the following Python libraries and versions to
    perform feature engineering on different datasets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来对不同的数据集进行特征工程。
- en: '`azureml-sdk 1.34.0`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: '`azureml-widgets 1.34.0`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-widgets 1.34.0`'
- en: '`azureml-dataprep 2.20.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-dataprep 2.20.0`'
- en: '`pandas 1.3.2`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.2`'
- en: '`numpy 1.19.5`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy 1.19.5`'
- en: '`scikit-learn 0.24.2`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn 0.24.2`'
- en: '`seaborn 0.11.2`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn 0.11.2`'
- en: '`plotly 5.3.1`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotly 5.3.1`'
- en: '`umap_learn 0.5.1`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`umap_learn 0.5.1`'
- en: '`statsmodels 0.13.0`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsmodels 0.13.0`'
- en: '`missingno 0.5.0`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missingno 0.5.0`'
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，你可以使用本地Python解释器或Azure机器学习中的笔记本环境执行此代码。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter06](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter06).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter06](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter06)。
- en: Understanding and applying feature engineering
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和应用特征工程
- en: '**Feature engineering** is the general term that describes the process of transforming
    existing features in our dataset, creating missing features, and eventually selecting
    the most predictive features from our dataset to start the ML training process
    with a given ML algorithm. These cannot just be seen as some mathematical functions
    we must apply to our data. This is an art form and doing it well makes the difference
    between a mediocre and highly performing predictive model. If you want to understand
    where you should invest your time, feature engineering is the step where you can
    have the most impact on the quality of your final ML model. To create this impact
    and be efficient, we must consider the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**是一个通用术语，描述了将我们数据集中的现有特征进行转换、创建缺失特征，并最终从我们的数据集中选择最具有预测性的特征以使用给定的机器学习算法开始机器学习训练过程的过程。这些不能仅仅被视为我们必须应用于我们的数据的某些数学函数。这是一种艺术形式，做得好可以区分一个平庸和高度表现的预测模型。如果你想知道你应该在哪里投入时间，特征工程是你可以对最终机器学习模型的质量产生最大影响的步骤。为了产生这种影响并提高效率，我们必须考虑以下因素：'
- en: '**ML algorithm requirements**: Do the features have to be in a specific format
    or range? How do I best avoid overfitting and underfitting the model?'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习算法要求**：特征是否需要特定的格式或范围？我如何最好地避免模型过拟合和欠拟合？'
- en: '**Domain knowledge**: Are the given features sufficient for our model? Can
    we create additional features or derive features that contain more predictive
    information?'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域知识**：给定的特征是否足够用于我们的模型？我们能否创建包含更多预测信息的附加特征或派生特征？'
- en: In this section, we'll define the different classes of feature engineering techniques
    and then look at some of the most prominent methods to apply to different types
    of datasets.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义不同的特征工程技术类别，然后探讨一些应用于不同类型数据集的最显著方法。
- en: Important Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Keep in mind that the usefulness of a specific feature engineering method depends
    on the utilized type of features (categorical, continuous, text, image, audio)
    and the chosen ML algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，特定特征工程方法的有用性取决于所使用的特征类型（分类、连续、文本、图像、音频）以及所选的机器学习算法。
- en: Classifying feature engineering techniques
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程技术分类
- en: 'Broadly speaking, feature engineering methods can be grouped into the following
    categories:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，特征工程方法可以归纳为以下类别：
- en: '**Feature creation**: Create new features from the given set of features or
    additional information sources.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征创建**：从给定的特征集或额外的信息源中创建新的特征。'
- en: '**Feature transformation**: Transform single features to make them useful and
    stable for the utilized ML algorithm.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征转换**：转换单个特征，使其对所使用的机器学习算法有用且稳定。'
- en: '**Feature extraction**: Create derived features from the original data.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：从原始数据中创建派生特征。'
- en: '**Feature selection**: Choose the most prominent and predictive features.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：选择最突出和最具预测性的特征。'
- en: Let's look at each of these categories and what they entail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些类别及其包含的内容。
- en: Feature creation
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征创建
- en: The first step to take in feature engineering is finding all the features that
    should be included in the model. To be good at this, you must have an intimate
    understanding of the relevant domain or know someone who is a **subject matter
    expert** (**SME**) in the domain. In the end, we want to be sure that we consider
    any type of data point that is predictive and that is feasible to acquire in a
    reasonable amount of time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的第一步是找到模型中应包含的所有特征。要擅长这一点，你必须对相关领域有深入了解，或者知道该领域的**领域专家**（SME）。最后，我们想要确保我们考虑了任何具有预测性且在合理时间内可以获取的数据点。
- en: 'In turn, we must understand all the methods that can help us create new features
    in our dataset, either taken from additional sources or the initial dataset. Typically,
    these methods can be classified as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，我们必须理解所有可以帮助我们在数据集中创建新特征的方法，无论是来自额外来源还是初始数据集。通常，这些方法可以按以下方式分类：
- en: '**Adding missing predictive features**: We add external information that is
    missing to achieve a more predictive model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加缺失的预测特征**：我们添加外部缺失信息，以实现更具有预测性的模型。'
- en: '**Combining the available features**: We create new features by combining already
    available features in our dataset.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结合可用特征**：我们通过结合数据集中已有的特征来创建新的特征。'
- en: Why do we have to change already existing features in our dataset?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们必须更改数据集中已经存在的特征？
- en: The reason for this is that a lot of connections between features and labels,
    that we understand, may not be clear to the utilized ML algorithm. Therefore,
    it is a good idea to think about what features or representations of the available
    features we would assume are necessary to make it easier for the ML algorithm
    to grasp the intrinsic connections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于，我们理解的许多特征与标签之间的联系可能对所使用的机器学习算法来说并不明显。因此，考虑哪些特征或可用特征的表示我们认为对于使机器学习算法更容易把握内在联系是很有帮助的。
- en: Let's look at some examples to understand this better.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些例子，以便更好地理解这一点。
- en: Imagine that you have a dataset for predicting house prices, like the one we
    examined in [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing
    Data Analysis and Visualization*. Furthermore, imagine that the features we have
    are the **length** and **width** of the house or apartment. In this case, it is
    probably useful to combine these two features to create a new one called the **surface
    area**. In addition, if the **type** of building is missing (house, flat, condo,
    and so on), we may want to add this from other sources since we know the type
    has an impact on the price of a property.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一个用于预测房价的数据集，就像我们在[*第五章*](B17928_05_ePub.xhtml#_idTextAnchor085)“执行数据分析与可视化”中考察的那样。此外，想象一下我们拥有的特征是房屋或公寓的**长度**和**宽度**。在这种情况下，将这两个特征结合起来创建一个名为**面积**的新特征可能是有用的。此外，如果缺少**建筑类型**（房屋、公寓、联排别墅等），我们可能需要从其他来源添加这个信息，因为我们知道类型会影响房产的价格。
- en: Important Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you create new features from existing ones, it is typically wise to only
    stick with the newly created feature by dropping those initial features from the
    dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从现有特征中创建新特征，通常明智的做法是只保留新创建的特征，从数据集中删除那些初始特征。
- en: Now, imagine the amount of money a person spends throughout their life. Being
    young, this might be very little. When they grow older, they may have mortgages
    and children and eventually, their spending may drop when their children move
    out of the house, and they are nearing retirement. As this would form something
    of a parabolic relationship between **age** and **cost of living**, it may not
    be easy for an ML algorithm to grasp this. Therefore, one possible option is to
    square the values of the **cost of living** feature to emphasize higher costs
    and deemphasize lower costs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下一个人在其一生中花费的金额。年轻时，这可能会非常少。随着年龄的增长，他们可能会有抵押贷款和子女，最终，当他们的子女搬出家时，他们的支出可能会下降，他们接近退休。由于这会在**年龄**和**生活成本**之间形成某种抛物线关系，因此，对于机器学习算法来说，可能不容易掌握这一点。因此，一个可能的选择是将**生活成本**特征的值平方，以强调更高的成本，并降低较低的成本的重要性。
- en: In the previous two examples, we used our domain knowledge to create new features.
    But what if we do not have this at our disposal?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个例子中，我们使用了我们的领域知识来创建新的特征。但如果我们没有这种知识怎么办？
- en: 'There is a way to create new features mathematically using the so-called **polynomial
    extension**. The idea is to create new features by raising the value of a feature
    to a certain power and multiplying it by one or multiple other features. Here,
    we define the **degree** as the maximum power a single feature can be raised to,
    and we define the **order** as the number of features we allow to be multiplied
    by each other. The following diagram shows all the possible combinations for a
    degree of 2 and order of 2 on the left-hand side, and a degree of 3 and order
    of 3 on the right-hand side:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种方法可以通过所谓的**多项式扩展**在数学上创建新特征。这个想法是通过将一个特征的值提升到一定的幂，并乘以一个或多个其他特征来创建新特征。在这里，我们定义**度**为单个特征可以提升到的最大幂，我们定义**顺序**为我们允许相互乘积的特征的数量。以下图表显示了左侧阶数为2，顺序为2的所有可能组合，以及右侧阶数为3，顺序为3的所有可能组合：
- en: '![Figure 6.1 – Possible combinations for polynomial extension  (degree=2, order=2
    on the left/degree=2, order=3 on the right) ](img/B17928_06_01.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 多项式扩展的可能组合（左侧为阶数=2，顺序=2；右侧为阶数=2，顺序=3）](img/B17928_06_01.jpg)'
- en: Figure 6.1 – Possible combinations for polynomial extension (degree=2, order=2
    on the left/degree=2, order=3 on the right)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 多项式扩展的可能组合（左侧为阶数=2，顺序=2；右侧为阶数=3，顺序=3）
- en: You should only consider a maximum order of 3 because, as shown in the preceding
    diagram, even with a degree of 2, this operation already creates too many combinations.
    Still, this automatic process may lead to much better predictive features than
    the originating ones.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该只考虑最大阶数为3，因为，如图所示，即使阶数为2，这个操作也已经产生了太多的组合。然而，这个自动过程可能比原始的特征产生更好的预测特征。
- en: To try this method, you can use the `PolynomialFeatures` class from the `sklearn`
    library ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试这种方法，你可以使用`sklearn`库中的`PolynomialFeatures`类（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)）。
- en: With all these methods in mind, we can create new features in our dataset that
    might be easier for our ML algorithm to handle and contain more precise, predictive
    information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了所有这些方法之后，我们可以在我们的数据集中创建新的特征，这些特征可能更容易被我们的机器学习算法处理，并且包含更精确、更具预测性的信息。
- en: Next, let's look at some methods that let us change a single feature by transforming
    its values or its representation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一些让我们可以通过转换其值或其表示来改变单个特征的方法。
- en: Feature transformation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征转换
- en: '**Feature transformation** is about manipulating a feature to change its value
    or create a new representation of the same. The following list covers the types
    of transformations we can perform on single features:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征转换**是关于操纵特征以改变其值或创建相同特征的新表示。以下列表涵盖了我们可以对单个特征执行的转换类型：'
- en: '**Discretization**: Divide feature values into different groups or intervals
    to reduce complexity. This can be done on numerical or categorical features.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散化**：将特征值划分为不同的组或区间以降低复杂性。这可以在数值或分类特征上完成。'
- en: '**Splitting**: Split a feature into multiple elements. This is typically done
    on datetime and string values.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拆分**：将特征拆分为多个元素。这通常是在日期时间和字符串值上进行的。'
- en: '**Categorical encoding**: Represent a categorical feature numerically, by creating
    new numerical features while following specific methods.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类编码**：通过创建新的数值特征并遵循特定方法来数值化分类特征。'
- en: '**Scaling**: Transform a continuous feature so that it fits into a specified
    range of values.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放**：将连续特征转换为一个适合特定值范围的值。'
- en: '**Standardization**: Transform a continuous feature so that it represents a
    normal distribution with a mean of 0 and a standard deviation of 1.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：将连续特征转换为一个具有均值为0和标准差为1的正态分布。'
- en: '**Normalization**: Transform a vector (row) of multiple continuous features
    individually into a so-called unit norm (unit magnitude).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**：将多个连续特征的向量（行）分别转换为一个所谓的单位范数（单位大小）。'
- en: '`square`, `square root`, `exp`, `log`, and so on).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`square`、`square root`、`exp`、`log`等。'
- en: 'In [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis
    and Visualization*, we used the `log` function to calculate the logarithm of all
    house price values. We did this to reduce the impact that a handful of outliers
    would have on our ML training. Therefore, the main reason to transform features
    is to adapt the feature to the possible mathematical requirements of the given
    ML algorithm. Often, you may run into the following requirements of the ML algorithm:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B17928_05_ePub.xhtml#_idTextAnchor085)，“执行数据分析与可视化”中，我们使用了`log`函数来计算所有房价值的对数。我们这样做是为了减少少数异常值对我们机器学习训练的影响。因此，转换特征的主要原因是使特征适应给定机器学习算法的可能数学要求。通常，你可能会遇到以下机器学习算法的要求：
- en: '**Numerical format**: The algorithm requires all the features to be numerical.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值格式**：算法要求所有特征都是数值的。'
- en: '**Same scale**: The algorithm requires all the predictive features to be on
    the same scale, maybe even with a mean of 0 and a standard deviation of 1.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相同尺度**：算法要求所有预测特征都在相同的尺度上，甚至可能具有均值为0和标准差为1。'
- en: '**Mathematical theory**: The domain itself may require certain transformations
    based on mathematical theory. For example, a price feature for predictions concerning
    economic theory should nearly always be transformed with the natural logarithm.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学理论**：域本身可能需要根据数学理论进行某些转换。例如，对于涉及经济理论的预测，价格特征几乎总是需要用自然对数进行转换。'
- en: '`[-1,1]`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[-1,1]`.'
- en: '**Complexity**: Most algorithms require very precise features. Therefore, reducing
    the complexity of the possible values a feature can take is often worthwhile.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：大多数算法都需要非常精确的特征。因此，降低特征可能取值的复杂性通常是有价值的。'
- en: An example would be discretizing features. One such method is called **binning**,
    which transforms numerical continuous values into a handful of discrete values.
    We will see this in action on text data in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，离散化特征。其中一种方法称为**分箱**，它将数值连续值转换为少量离散值。我们将在[*第七章*](B17928_07_ePub.xhtml#_idTextAnchor112)，“使用NLP的高级特征提取”中看到这一方法的应用。
- en: Another example would be splitting datetime features. Imagine that we want to
    predict the amount of traffic on a certain road at specific times of the day.
    Let's assume that we got a feature denoting the **date and time** of our recording
    and the **number of cars** we tracked at that point. To make a better prediction,
    one idea would be to create three new features, denoting whether it is a *workday*,
    *weekend*, or *holiday*. There will be less traffic on a Sunday at 7 A.M. compared
    to a workday morning at 7 A.M.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是将日期时间特征分割。想象一下，我们想要预测一天中特定时间某条道路上的交通量。假设我们得到了一个表示我们记录的**日期和时间**以及在那个点追踪的**汽车数量**的特征。为了做出更好的预测，一个想法是创建三个新的特征，表示是否是**工作日**、**周末**或**假日**。与工作日上午7点相比，星期天上午7点的交通量会更少。
- en: 'Let''s learn how to perform this transformation. The following screenshot shows
    our initial small dataset and the first transformation adding `day of the week`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何执行这种转换。以下截图显示了我们的初始小型数据库和添加`星期几`的第一个转换：
- en: '![Figure 6.2 – Dataset with a new weekday feature   ](img/B17928_06_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 包含新工作日特征的数据库](img/B17928_06_02.jpg)'
- en: Figure 6.2 – Dataset with a new weekday feature
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 包含新工作日特征的数据库
- en: 'In the next step, we must enrich the data by adding a new categorical feature
    called `daytype`, which denotes whether a day is either a weekday, a weekend,
    or a holiday:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们必须通过添加一个名为`daytype`的新分类特征来丰富数据，该特征表示一天是工作日、周末还是假日：
- en: '![Figure 6.3 – Dataset enrichment  ](img/B17928_06_03.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 数据库丰富化](img/B17928_06_03.jpg)'
- en: Figure 6.3 – Dataset enrichment
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 数据库丰富化
- en: 'Theoretically, we are done. But our ML algorithm may beg to differ here. Our
    ML model may make up a natural order for our categorical data that does not exist
    or it simply cannot handle categorical data. In this case, it is prudent to **encode**
    our categorical data with numerical values. One such method is called **one-hot
    encoding**, which transforms a categorical feature into multiple numerical features
    by creating a new feature with two valid values (0 or 1) for every existing category.
    The following screenshot shows this encoding for our example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们已经完成了。但我们的机器学习算法可能在这里有不同的看法。我们的机器学习模型可能会为我们的分类数据创建一个不存在的自然顺序，或者它简单地无法处理分类数据。在这种情况下，明智的做法是将我们的分类数据用数值进行**编码**。一种这样的方法称为**独热编码**，它通过为每个现有类别创建一个具有两个有效值（0或1）的新特征，将分类特征转换为多个数值特征。以下截图显示了我们对示例的这种编码：
- en: '![ Figure 6.4 – One-hot encoding the new feature ](img/B17928_06_04.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 对新特征进行独热编码](img/B17928_06_04.jpg)'
- en: Figure 6.4 – One-hot encoding the new feature
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 对新特征进行独热编码
- en: Here, we created three new features named `holiday`, `weekday`, and `weekend`,
    each representing our initial categories. Where a sample had this initial category,
    the value of that feature is set to `1`; otherwise, it is set to `0`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了三个新的特征，分别命名为`holiday`、`weekday`和`weekend`，每个特征代表我们的初始类别。如果一个样本具有这个初始类别，那么该特征的值设置为`1`；否则，设置为`0`。
- en: What have we done in this example? We transformed a very unintuitive datetime
    feature into something with more predictive power by splitting the feature into
    components, adding external knowledge through feature creation, and performing
    categorical encoding on the created feature.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们做了什么？我们通过分割特征，添加外部知识通过特征创建，并在创建的特征上执行分类编码，将一个非常不直观的日期时间特征转换成具有更多预测力的特征。
- en: Now that we have a good grasp of feature transformation, let's look at what
    falls under feature extraction.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地掌握了特征转换，让我们看看什么是特征提取的范畴。
- en: Feature extraction
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取
- en: With **feature extraction**, we group all the methods that do not manipulate
    features by simple means but extract useful information from a high-dimensional
    dataset. This is typically done by using complex mathematical algorithms or ML
    algorithms.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**特征提取**，我们将所有不通过简单手段操纵特征但能从高维数据集中提取有用信息的方法分组在一起。这通常是通过使用复杂的数学算法或机器学习算法来完成的。
- en: Extraction is often required when the underlying dataset is too complex to be
    processed, so it needs to be brought into a simplified form while keeping its
    predictive value.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '当底层数据集过于复杂而难以处理时，通常需要提取，同时保持其预测价值，将其转化为简化的形式。 '
- en: 'The following are some typical extraction types for different scenarios:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些不同场景下的典型提取类型：
- en: '**High-dimensional reduction**: Create representative features based on an
    *n*-dimensional dataset.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维降维**：基于n维数据集创建代表性特征。'
- en: '**Feature detection**: Find points of interest in every image in an image dataset.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征检测**：在图像数据集中的每张图像中找到感兴趣点。'
- en: '**Word embeddings**: Create numeric encodings for words in a text dataset.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：为文本数据集中的单词创建数值编码。'
- en: '**Signal processing**: Extract the characteristics of sound waves from an audio
    dataset.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信号处理**：从音频数据集中提取声音波的特征。'
- en: We discussed high-dimensional reduction methods in [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085),
    *Performing Data Analysis and Visualization*, when we looked at visualizing high-dimensional
    datasets. In a process like **principal component analysis** (**PCA**), the dataset
    is projected onto a two- or three-dimensional space by creating principal component
    vectors. Instead of only using this method for visualization, we could use these
    calculated vectors as derived and less complex features that represent our dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第5章*](B17928_05_ePub.xhtml#_idTextAnchor085)“执行数据分析和可视化”中讨论了高维降维方法，当时我们探讨了可视化高维数据集。在**主成分分析**（PCA）这样的过程中，数据集通过创建主成分向量被投影到二维或三维空间。我们不仅可以使用这种方法进行可视化，还可以使用这些计算向量作为派生和更简单的特征，这些特征代表我们的数据集。
- en: Important Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: High-dimensional reduction techniques can be used for feature extraction, but
    keep in mind that we lose our intrinsic understanding of the features. Instead
    of features called suburbs or rooms, we end up with features called Principal
    Component 1 and Principal Component 2.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 高维降维技术可用于特征提取，但请注意，我们失去了对特征的内禀理解。我们最终得到的不是称为郊区或房间的特征，而是称为主成分1和主成分2的特征。
- en: Looking at the other scenarios, it seems that extraction typically happens when
    we are working with complex datasets made up of text, image, or audio data. In
    all these cases, there are specific methods to consider when extracting information
    from the raw data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 观察其他场景，似乎提取通常发生在我们处理由文本、图像或音频数据组成的复杂数据集时。在这些所有情况下，当我们从原始数据中提取信息时，都有特定的方法需要考虑。
- en: In the case of an image dataset, we might be interested in key areas or points
    of interest, including finding edges and objects. In [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, you will see that such image extraction
    steps are done automatically by **deep neural networks**, removing the need to
    perform manual feature extraction on images in a lot of cases.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像数据集的情况下，我们可能对关键区域或感兴趣点感兴趣，包括寻找边缘和对象。在[*第10章*](B17928_10_ePub.xhtml#_idTextAnchor165)“在Azure上训练深度神经网络”中，你会看到这样的图像提取步骤是由**深度神经网络**自动完成的，从而消除了在许多情况下对图像进行手动特征提取的需要。
- en: In the case of text data, we can use extraction methods such as **bag of words**
    and **TF-IDF**, both of which help create numerical representations of text, capturing
    meaning and semantic relationships. We will have an in-depth look at these methods
    in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112), *Advanced Feature Extraction
    with NLP*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本数据的情况下，我们可以使用诸如**词袋模型**和**TF-IDF**之类的提取方法，这两种方法都有助于创建文本的数值表示，捕捉意义和语义关系。我们将在[*第7章*](B17928_07_ePub.xhtml#_idTextAnchor112)“使用NLP的高级特征提取”中深入探讨这些方法。
- en: In the case of audio data, we can use signal processing to extract information
    and new features from the source. In this scenario, there are also two domains
    – the time domain and the frequency domain – that we can pull information from.
    From the time domain, we would typically extract something like the **amplitude
    envelope**, which is the maximum amplitude of the signal per frame, the **root
    mean square energy**, which hints at the loudness of the signal, and the **zero-crossing
    rate**, which is the number of times the wave is crossing the horizontal time
    axis. If you must work with data from this domain, make yourself comfortable with
    such processing techniques.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频数据的情况下，我们可以使用信号处理从源数据中提取信息和新的特征。在这种情况下，也存在两个领域——时域和频域——我们可以从中提取信息。从时域来看，我们通常会提取诸如**幅度包络**这样的内容，它是每帧信号的峰值幅度，**均方根能量**，它暗示了信号的响度，以及**过零率**，即波穿越水平时间轴的次数。如果你必须处理来自这个领域的数据，请让自己熟悉这样的处理技术。
- en: Important Note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A lot of feature extraction and feature transformation techniques are already
    embedded in common ML frameworks and algorithms, removing the need for you to
    manually touch features. Have a good understanding of what the algorithm does
    by itself and what you need to do manually when you're preprocessing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 许多特征提取和特征转换技术已经嵌入到常见的机器学习框架和算法中，无需您手动触摸特征。通过理解算法本身做什么以及您在预处理时需要手动做什么，来获得良好的理解。
- en: So far, we've learned how to create new features, transform features, and extract
    features from our dataset. Now, let's look at some methods that can help us select
    the most predictive feature from our feature set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何创建新特征、转换特征以及从我们的数据集中提取特征。现在，让我们看看一些可以帮助我们从特征集中选择最具预测性的特征的方法。
- en: Feature selection
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: With **feature selection**, we define all the methods that help us understand
    how valuable and predictive a feature is for the target so that we can choose
    a useful subset of our feature variables for training. The reasons to reduce complexity
    are two-fold. On the one hand, we want the simplicity to make the model **explainable**
    while on the other, we want to avoid **overfitting** the model. With too much
    input information, we will end up with a model that, in most cases, will perfectly
    fit our training data and nothing else but will perform poorly on unseen data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**特征选择**，我们定义了所有帮助我们理解特征对目标有价值性和预测性的方法，以便我们可以选择有用的特征变量子集进行训练。减少复杂性的原因有两个。一方面，我们希望简单性使模型**可解释**；另一方面，我们希望避免模型**过拟合**。当输入信息过多时，我们最终会得到一个模型，在大多数情况下，这个模型会完美地拟合我们的训练数据，但除了这些之外，它在未见过的数据上的表现会很差。
- en: 'Generally, there are three different types of feature selection methods, as
    follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有三种不同类型的特征选择方法，如下所示：
- en: '**Filter-based methods**: These define a derived metric, that is not the target
    error rate, to measure the quality of a subset of features.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于过滤的方法**：这些方法定义了一个派生指标，即不是目标错误率，来衡量特征子集的质量。'
- en: '**Wrapper-based methods**: These use greedy search algorithms to run a prediction
    model on different combinations of feature subsets.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于包装的方法**：这些方法使用贪婪搜索算法在不同的特征子集组合上运行预测模型。'
- en: '**Embedded methods**: These are specific selection methods that are already
    embedded into our final ML model.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入式方法**：这些是已经嵌入到我们最终机器学习模型中的特定选择方法。'
- en: Filter-based methods can be very efficient in terms of computational resources
    but are only evaluated against a simpler filter. Typically, statistical measures
    such as correlation, mutual information, and entropy are used as metrics in these
    approaches.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过滤的方法在计算资源方面可以非常高效，但仅与一个更简单的过滤方法进行评估。通常，这些方法中使用统计指标，如相关性、互信息和熵作为度量标准。
- en: 'On the other hand, wrapper-based methods are computationally intense. At the
    same time, they can find a great performing feature set since the same error function
    or metric is being used for the selection of the features as the one that''s being
    used in the actual model training. The downside of this approach is that without
    an independent metric, the selected subset is only useful for the chosen ML training
    algorithm. Typically, this is done by performing one of the following processes:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于包装的方法计算密集。同时，它们可以找到性能极佳的特征集，因为用于特征选择的错误函数或指标与实际模型训练中使用的相同。这种方法的不利之处在于，如果没有独立的指标，选定的子集仅对所选的机器学习训练算法有用。通常，这是通过执行以下过程之一来完成的：
- en: '**Step forward feature selection**: Features are added one by one based on
    the training results of each feature until the model does not improve its performance.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步前进特征选择**：根据每个特征的训练结果逐个添加特征，直到模型不再提高其性能。'
- en: '**Step backward feature selection**: The model is evaluated with the full set
    of features. These features are subsequently removed until a predefined number
    of features is reached. This removal is done in a round-robin fashion.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步后退特征选择**：使用完整特征集评估模型。然后，这些特征被逐一移除，直到达到预定义的特征数量。这种移除是循环进行的。'
- en: '**Exhaustive feature selection**: All the feature subsets are evaluated, which
    is the most expensive method.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举特征选择**：评估所有特征子集，这是最昂贵的方法。'
- en: Finally, a selection method is called an embedded method when the selection
    step is part of the model learning algorithm itself. Embedded methods often combine
    the qualities of filter and wrapper methods through the fact that the learning
    algorithm takes advantage of its selection process and performs selection and
    training at the same time. Typical examples of embedded methods are ensemble models,
    **Lasso**, and **Ridge**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当选择步骤是模型学习算法本身的一部分时，选择方法被称为嵌入式方法。嵌入式方法通常通过学习算法利用其选择过程，同时进行选择和训练，从而结合过滤器和包装方法的特性。嵌入式方法的典型例子是集成模型、**Lasso**和**Ridge**。
- en: You may have realized this by now, but we used such methods in [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085),
    *Performing Data Analysis and Visualization*. The **Pearson correlation coefficient**
    we used for generating a correlation matrix is a derived metric, so it falls under
    the filter-based selection methods. In addition, we used an **ensemble decision
    tree model** to calculate feature importance for our dataset. This helped us get
    a clear understanding of which features may have more influence on the target
    than others. This ensemble method utilizes the **random forest** approach. A random
    forest not only implements the so-called **bagging** technique to randomly select
    a subset of samples to train on but also takes a random selection of features
    rather than using all the features to grow each tree. Therefore, for feature selection,
    random forests fall into the embedded category.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能现在已经意识到了，我们在[*第5章*](B17928_05_ePub.xhtml#_idTextAnchor085)，*执行数据分析与可视化*中使用了这样的方法。我们用于生成相关矩阵的**皮尔逊相关系数**是一个派生指标，因此它属于基于过滤器的选择方法。此外，我们还使用了一个**集成决策树模型**来计算数据集的特征重要性。这有助于我们清楚地了解哪些特征可能比其他特征对目标有更大的影响。这种集成方法利用了**随机森林**方法。随机森林不仅实现了所谓的**袋装**技术，随机选择样本子集进行训练，而且还随机选择特征，而不是使用所有特征来生长每一棵树。因此，对于特征选择，随机森林属于嵌入式类别。
- en: We will have a more detailed look at the tree-based ensemble classifier, as
    well as bagging and boosting, in [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152),
    *Building ML Models Using Azure Machine Learning*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第9章*](B17928_09_ePub.xhtml#_idTextAnchor152)，*使用Azure机器学习构建ML模型*中更详细地查看基于树的集成分类器，以及袋装和提升。
- en: Besides all these mathematical approaches to feature selection, sometimes, a
    more manual approach might be far superior. For example, when we removed the postal
    code from our **Melbourne housing dataset** in [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085),
    *Performing Data Analysis and Visualization*, we did so because we understood
    that the postal code and the suburbs contain the same information, which made
    them redundant. We did this because we have domain knowledge and understand the
    relationship between postal codes and suburbs. Note that this additional knowledge
    lessens the burden for the model to learn these connections by itself.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所有这些特征选择的数学方法之外，有时，更手动的方法可能更优越。例如，当我们从[*第5章*](B17928_05_ePub.xhtml#_idTextAnchor085)，*执行数据分析与可视化*中的**墨尔本住房数据集**中删除邮政编码时，我们这样做是因为我们理解邮政编码和郊区包含相同的信息，这使得它们是冗余的。我们这样做是因为我们具有领域知识，并了解邮政编码和郊区之间的关系。请注意，这种额外的知识减轻了模型自己学习这些联系的压力。
- en: Important Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For feature engineering, the more outside knowledge about the data or the domain,
    the simpler a lot of these preprocessing steps can get, or they become avoidable
    altogether.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征工程，对数据或领域了解的更多外部知识，可以使许多预处理步骤变得更加简单，或者完全避免。
- en: We will iterate this notion throughout this book as it needs to be ingrained
    into everything you do so that you get more efficient and better at working with
    data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书中反复阐述这一概念，因为它需要融入你做的每一件事，以便你更高效、更擅长处理数据。
- en: We now have a general understanding of the general types of feature engineering
    we can perform. In the next section, we will provide an overview of the most prominent
    methods and drill deeper into some of them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对可以执行的一般特征工程类型有了总体了解。在下一节中，我们将概述最显著的方法，并深入探讨其中的一些方法。
- en: Discovering feature transformation and extraction methods
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现特征转换和提取方法
- en: 'Now that we have a good grasp of the types of feature engineering action we
    can apply to our feature, let''s look at some of the most prominent feature engineering
    techniques and their names. The following table provides a good overview of most
    of the well-known methods in the different categories we have learned about:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地掌握了我们可以应用于特征的特征工程动作类型，让我们来看看一些最突出的特征工程技术和它们的名称。以下表格提供了我们所学不同类别中大多数已知方法的良好概述：
- en: '![Figure 6.5 – Overview of different feature engineering methods  ](img/B17928_06_05.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 不同特征工程方法的概述](img/B17928_06_05.jpg)'
- en: Figure 6.5 – Overview of different feature engineering methods
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 不同特征工程方法的概述
- en: Keep in mind that this list is far from exhaustive and as we mentioned previously,
    some of these methods are already implemented as part of specific ML algorithms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个列表远非详尽无遗，正如我们之前提到的，其中一些方法已经作为特定机器学习算法的一部分得到实现。
- en: In the following sections, we will look at some of these. Feel free to download
    the `01_feateng_examples.ipynb` file in the GitHub repository for this chapter,
    which contains the code for the upcoming examples. If you would like to learn
    more about some of the feature extraction methods we will cover, we will come
    back to them in the upcoming chapters. For the methods we won't cover, feel free
    to research them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨其中的一些。您可以自由下载GitHub仓库中该章节的`01_feateng_examples.ipynb`文件，其中包含即将到来的示例的代码。如果您想了解更多关于我们将要介绍的一些特征提取方法，我们将在接下来的章节中回到它们。对于我们将不介绍的方法，请自由研究它们。
- en: Scaling, standardization, and normalization
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩放、标准化和归一化
- en: Since all the scaling and normalization methods are very similar to each other,
    we will discuss all of them in detail here.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有缩放和归一化方法彼此之间非常相似，我们在这里将详细讨论它们。
- en: 'Let''s begin with the so-called **StandardScaler**. This scaling transforms
    our feature values so that the resulting value distribution has a mean (µ) of
    0 and a standard deviation (s) of 1\. The formula to apply to each value looks
    like this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从所谓的**StandardScaler**开始。这种缩放将我们的特征值转换，使得结果值分布的均值（µ）为0，标准差（s）为1。应用于每个值的公式看起来如下：
- en: '![](img/Formula_06_01.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_01.png)'
- en: Here, µ is the mean value of the given distribution and s is the standard deviation
    of the given distribution. With this, we can convert every value, ![](img/Formula_06_02.png),
    into a new scaled value, ![](img/Formula_06_03.png).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，µ是给定分布的均值，s是给定分布的标准差。有了这个，我们可以将每个值，![](img/Formula_06_02.png)，转换成一个新的缩放值，![](img/Formula_06_03.png)。
- en: 'The following diagram shows how this scaler changes the shape of multiple distributions:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了该缩放器如何改变多个分布的形状：
- en: '![Figure 6.6 – StandardScaler distribution (left: before scaling, right: after
    scaling) ](img/B17928_06_06.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – StandardScaler分布（左：缩放前，右：缩放后）](img/B17928_06_06.jpg)'
- en: 'Figure 6.6 – StandardScaler distribution (left: before scaling, right: after
    scaling)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – StandardScaler分布（左：缩放前，右：缩放后）
- en: You should only use this scaler if the underlying distribution is *normally
    distributed*, as this is the requirement.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当底层分布是*正态分布*时，才应使用此缩放器，因为这符合要求。
- en: Next, we will look at the **MinMaxScaler**. This scaling method is very similar
    to standardization, except that we are not working with the mean or standard deviation
    of the value distribution; instead, we are scaling the values to a range of [0,1]
    or [-1,1] (if negative values exist). Scaling a feature like this will often increase
    the performance of ML algorithms as they are typically better at handling small-scale
    values.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨**MinMaxScaler**。这种缩放方法与标准化非常相似，只是我们不是在处理值分布的均值或标准差；相反，我们将值缩放到[0,1]或[-1,1]（如果存在负值）的范围内。以这种方式缩放特征通常会提高机器学习算法的性能，因为它们通常更擅长处理小规模值。
- en: 'Mathematically, this scaling is defined as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这种缩放定义为以下：
- en: '![](img/Formula_06_04.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_04.png)'
- en: Here, ![](img/Formula_06_05.png) defines the minimum value and ![](img/Formula_06_06.png)
    defines the maximum value in our initial distribution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_06_05.png)定义了初始分布的最小值，而![](img/Formula_06_06.png)定义了初始分布的最大值。
- en: The MinMaxScaler is a good choice if the minimum and maximum values are well-defined
    – think about the color intensity in an RGB picture. Furthermore, we can change
    the formula to influence the resulting range of values.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最小值和最大值定义良好，MinMaxScaler是一个不错的选择 – 想想RGB图片中的颜色强度。此外，我们可以改变公式以影响结果的值范围。
- en: Important Note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The StandardScaler and the MinMaxScaler are both very susceptible to outliers
    in a distribution, which, in turn, can skew certain ML algorithms.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: StandardScaler和MinMaxScaler都对分布中的异常值非常敏感，这反过来又可能扭曲某些机器学习算法。
- en: A lot of ML algorithms pay more attention to large values, so they have a problem
    with outliers. A scaler fittingly named **RobustScaler** was defined to tackle
    this behavior. This scaler uses the **interquartile range** (**IQR**) instead
    of the standard deviation as a measure of dispersion and uses the **median** value
    instead of the mean value of the distribution as a measure of central tendency.
    The interquartile range denotes the middle 50% of the distribution, which means
    it is the difference between the 75th percentile and the 25th percentile.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法更关注大值，因此它们存在异常值的问题。为了解决这个问题，定义了一个名为**RobustScaler**的缩放器。这个缩放器使用**四分位距**（**IQR**）而不是标准差作为离散度的度量，并使用分布的**中位数**而不是平均值作为集中趋势的度量。四分位距表示分布中间的50%，这意味着它是第75百分位数和第25百分位数之间的差值。
- en: 'Therefore, the mathematical scaling function looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数学缩放函数看起来是这样的：
- en: '![](img/Formula_06_07.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_07.png)'
- en: Here, ![](img/Formula_06_08.png) denotes the median of the distribution, ![](img/Formula_06_09.png)
    denotes the value where the first quartile starts, and ![](img/Formula_06_10.png)
    denotes the value where the third quartile starts.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_06_08.png)表示分布的中位数，![](img/Formula_06_09.png)表示第一四分位数开始的位置，![](img/Formula_06_10.png)表示第三四分位数开始的位置。
- en: Why does this scaler work better with outliers?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个缩放器对异常值更有效？
- en: In the previous formulas, the biggest outlier would still be falling into the
    predefined interval because the maximum outlier would be ![](img/Formula_06_11.png).
    Therefore, the further the outlier is from the bulk of the data points, the more
    the center values would be scaled toward 0\. On the other hand, with the RobustScaler,
    all the data points in the middle 50% would be scaled into the unit distance,
    and everything above or below this would be scaled to the appropriate values outside
    of the main interval while keeping the relative distance between the values in
    the middle of the distribution intact.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，最大的异常值仍然会落在预定义的区间内，因为最大的异常值会是![](img/Formula_06_11.png)。因此，异常值离数据点群越远，中心值缩向0的程度就越大。另一方面，使用RobustScaler，中间50%的所有数据点都会缩放到单位距离，而高于或低于这个值的数据点会被缩放到主要区间之外适当的值，同时保持分布中间值之间的相对距离不变。
- en: Simply put, the median and the interquartile range are not influenced greatly
    by outliers, so this scaler is not influenced greatly by outliers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，中位数和四分位距受异常值的影响不大，因此这个缩放器受异常值的影响也不大。
- en: 'Let''s look at all these scalars on a sample distribution. For this, we will
    take the `Price` column of the `Price` column and the distribution resulting from
    applying each scaling method we''ve discussed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些缩放器在一个样本分布上的表现。为此，我们将取`Price`列的`Price`列和应用我们讨论的每种缩放方法得到的分布：
- en: '![Figure 6.7 – Distribution scaled using multiple scaling methods ](img/B17928_06_07.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 使用多种缩放方法缩放的分布](img/B17928_06_07.jpg)'
- en: Figure 6.7 – Distribution scaled using multiple scaling methods
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 使用多种缩放方法缩放的分布
- en: 'As we can see, **StandardScaler** creates a distribution with a mean of 0 and
    a standard deviation of 1, **MinMaxScaler** scales the values between 0 and 1,
    and **RobustScaler** sets the mean to 0\. Looking at the box plots in *Figure
    6.8* and *Figure 6.9*, we can see the differences in their distributions. Please
    note the scale of the *y* axis as well:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**StandardScaler**创建了一个均值为0、标准差为1的分布，**MinMaxScaler**将值缩放到0到1之间，而**RobustScaler**将均值设置为0。查看*图6.8*和*图6.9*中的箱线图，我们可以看到它们分布的差异。请注意*y*轴的刻度：
- en: '![Figure 6.8 – Box plot for StandardScaler and RobustScaler ](img/B17928_06_08.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – StandardScaler和RobustScaler的箱线图](img/B17928_06_08.jpg)'
- en: Figure 6.8 – Box plot for StandardScaler and RobustScaler
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – StandardScaler和RobustScaler的箱线图
- en: 'Comparing the following box plot to *Figure 6.8*, we can see the difference
    in their distribution:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将下面的箱线图与*图6.8*进行比较，我们可以看到它们分布的差异：
- en: '![Figure 6.9 – Box plot for MinMaxScaler ](img/B17928_06_09.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – MinMaxScaler的箱线图](img/B17928_06_09.jpg)'
- en: Figure 6.9 – Box plot for MinMaxScaler
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – MinMaxScaler的箱线图
- en: Now that we have some idea of how to scale a feature, let's talk about normalization.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对如何缩放一个特征有了些了解，让我们来谈谈归一化。
- en: '**Normalization** is the process of taking a vector (row) of feature values
    and scaling them to a **unit magnitude**, typically to simplify mathematical processes
    such as **cosine similarity**.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化**是将特征值向量（行）缩放到**单位模长**的过程，通常是为了简化如**余弦相似度**这样的数学过程。'
- en: Let's start by understanding a process where this normalization step can be
    of help. The cosine similarity describes how similar two different vectors are
    to each other. In an n-dimensional room, are they pointing in the same direction,
    are they perpendicular to each other, or are they facing in the opposite direction?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一个可以从中受益的归一化步骤。余弦相似度描述了两个不同向量之间的相似程度。在一个n维空间中，它们是否指向同一方向，是否相互垂直，或者是否面向相反方向？
- en: Such calculations can, for example, help us understand how similar text documents
    are to each other, by taking a vector of word counts or similar information and
    comparing them with each other.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这样的计算可以帮助我们理解文本文档之间的相似性，通过取词频向量或类似信息并比较它们来实现。
- en: 'Therefore, to understand document similarity, we must calculate a cosine between
    vectors using the following formula:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了理解文档相似性，我们必须使用以下公式计算向量之间的余弦值：
- en: '![](img/Formula_06_12.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![公式_06_12.png](img/Formula_06_12.png)'
- en: 'As you can see, to make this calculation, we must calculate the magnitude of
    each vector – for example, ![](img/Formula_06_13.png). This magnitude is defined
    as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，为了进行这个计算，我们必须计算每个向量的模——例如，![公式_06_13.png](img/Formula_06_13.png)。这个模定义为以下内容：
- en: '![](img/Formula_06_14.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![公式_06_14.png](img/Formula_06_14.png)'
- en: This single vector magnitude calculation is quite expensive to perform. Now,
    imagine that we have a dataset that contains hundreds of thousands of documents.
    We would have to calculate this every time for every combination of vectors (samples)
    in our dataset. Wouldn't it be easier to have all these vector magnitudes equal
    to 1? This would greatly simplify the calculation of the cosine.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单独的向量模长计算相当昂贵。现在，假设我们有一个包含数十万个文档的数据集。我们每次都必须为数据集中每个向量的组合（样本）计算这个值。如果所有这些向量模长都等于1，不是会更容易吗？这将极大地简化余弦的计算。
- en: 'Therefore, the idea is to normalize all the samples in our dataset to achieve
    a unit magnitude by scaling them appropriately, as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的想法是通过适当缩放所有样本，将数据集中的所有样本归一化到单位模长，如下所示：
- en: '![](img/Formula_06_15.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![公式_06_15.png](img/Formula_06_15.png)'
- en: In this equation, ![](img/Formula_06_16.png) denotes our initial vector, ![](img/Formula_06_17.png)
    denotes the magnitude of the initial vector, and ![](img/Formula_06_18.png) denotes
    our scaled vector with the unit magnitude.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![公式_06_16.png](img/Formula_06_16.png)表示我们的初始向量，![公式_06_17.png](img/Formula_06_17.png)表示初始向量的模，![公式_06_18.png](img/Formula_06_18.png)表示我们缩放到单位模长的缩放向量。
- en: 'This normalization is called **L2 Norm** and is one of three typical normalization
    methods. Let''s look at how the magnitude of a vector is calculated in this and
    all the other metrics:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种归一化称为**L2范数**，是三种典型归一化方法之一。让我们看看在这个以及其他所有度量中如何计算向量的模：
- en: '**L1 Norm**: This calculates the magnitude as the sum of the absolute values
    of the vector components.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1范数**：这个计算将向量的模定义为向量各分量绝对值的和。'
- en: '**L2 Norm**: This calculates the traditional vector magnitude (as described).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2范数**：这个计算的是传统的向量模长（如上所述）。'
- en: '**Max Norm**: This calculates the magnitude as the absolute value of the elements
    of the vector.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大范数**：这个计算的是向量的元素绝对值的模。'
- en: The L1 Norm and the Max Norm cannot be used for cosine similarity as they do
    not calculate the mathematically defined vector magnitude. So, let's look at how
    those two are calculated.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数和最大范数不能用于余弦相似度，因为它们没有计算数学上定义的向量模。所以，让我们看看这两个是如何计算的。
- en: 'The L1 Norm is mathematically defined as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数在数学上定义为以下内容：
- en: '![](img/Formula_06_19.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![公式_06_19.png](img/Formula_06_19.png)'
- en: The L1 Norm is often used to regularize the values in the dataset when you're
    fitting an ML algorithm. It keeps the coefficient small, which makes the model
    training process less complex.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数常用于在拟合机器学习算法时正则化数据集中的值。它保持系数较小，这使得模型训练过程更简单。
- en: 'The Max Norm is mathematically defined as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最大范数在数学上定义为以下内容：
- en: '![](img/Formula_06_20.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![公式_06_20.png](img/Formula_06_20.png)'
- en: The Max Norm is also used for regularization, typically in **neural networks**
    to keep the weights low at the connections between neurons. It also helps with
    performing less extreme backpropagation runs to stabilize the ML algorithm's learning.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最大范数也用于正则化，通常在**神经网络**中用于保持神经元之间连接的权重低，这也有助于执行更少的极端反向传播运行以稳定机器学习算法的学习。
- en: At this point, you should have a good grasp of the usefulness of scaling and
    normalization. Next, we'll look at some methods we can use to transform categorical
    values into numerical representations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经很好地掌握了缩放和归一化的有用性。接下来，我们将探讨一些可以将分类值转换为数值表示的方法。
- en: Categorical encoding
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类编码
- en: When we looked at feature transformation as a concept, we looked at an example
    where we applied **one-hot encoding**. This method creates new features with two
    possible values (0,1) for *every* available category in the initial categorical
    feature. This can be helpful, but a categorical feature of high cardinality would
    blow up the feature space dramatically. Therefore, when using this method, we
    must figure out if every single category is predictive or not.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将特征转换作为一个概念来考虑时，我们查看了一个应用了**独热编码**的例子。这种方法为初始分类特征中的每个可用类别创建具有两个可能值（0，1）的新特征。这可能很有帮助，但高基数分类特征会极大地膨胀特征空间。因此，在使用这种方法时，我们必须弄清楚每个类别是否具有预测性。
- en: In our previous example, instead of using a category with the days of the week
    (Monday through Saturday), we opted for only three categories, namely weekday,
    weekend, and holiday. In such a scenario, one-hot encoding is quite helpful.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，我们不是使用一周中每天（周一至周六）的类别，而是选择了只有三个类别，即工作日、周末和假日。在这种情况下，独热编码非常有帮助。
- en: Besides this method, there are other ways to encode categorical features. The
    most basic of them would be **label encoding**. In label encoding, we replace
    every category with a numeric label (0,..,*n*), thus making it a numeric feature.
    Through this, we did not add any additional information to this feature.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种方法之外，还有其他方法可以编码分类特征。其中最基本的方法是**标签编码**。在标签编码中，我们将每个类别替换为一个数值标签（0，..，*n*），从而使其成为一个数值特征。通过这种方式，我们没有向这个特征添加任何额外的信息。
- en: 'The next idea would be to add some intrinsic information from the whole dataset
    and ingrain it into the values we must encode. Some options for this idea are
    as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的想法是将整个数据集的一些内在信息添加到我们必须编码的值中，并使其融入其中。这个想法的一些选项如下：
- en: '**Count encoding**: Replace each category with the absolute number of observations
    of this category in the whole dataset.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数编码**：将每个类别替换为整个数据集中该类别观察值的绝对数量。'
- en: '**Frequency encoding**: Replace each category with the relative number (the
    percentage) of observations of this category in the whole dataset.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率编码**：将每个类别替换为整个数据集中该类别观察值的相对数量（百分比）。'
- en: '**Target encoding**: Replace each category with the mean value of the target
    that''s been calculated from each entry of this category throughout the whole
    dataset.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标编码**：将每个类别替换为从整个数据集中该类别的每个条目计算出的目标平均值。'
- en: 'To understand these methods, let''s assume that we have a dataset that contains
    the favorite snack item of 25 people as one of the features and their likelihood
    of buying a new snack product a company produces as the target. The following
    table shows the original values and all three encodings we have discussed:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些方法，让我们假设我们有一个包含25个人的最爱零食项作为特征之一，以及他们购买公司生产的新零食产品的可能性的数据集。以下表格显示了原始值和我们所讨论的所有三种编码：
- en: '![Figure 6.10 – Count, frequency, and target encoding example ](img/B17928_06_10.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 计数、频率和目标编码示例](img/B17928_06_10.jpg)'
- en: Figure 6.10 – Count, frequency, and target encoding example
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 计数、频率和目标编码示例
- en: With these methods, we can ingrain additional information into the feature,
    making it easier for an ML algorithm to understand relationships.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些方法，我们可以将额外的信息融入特征中，使机器学习算法更容易理解关系。
- en: Finally, let's talk about `Rare`, thus grouping them into one category. This
    helps reduce the overall complexity and should especially be done if the `Rare`
    category will still be a small part of the overall category distribution. You
    can compare this to grouping small parties under the *Others* label in an election
    graph, while primarily showing the major parties.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们谈谈`Rare`，因此将它们归为一类。这有助于降低整体复杂性，特别是如果`Rare`类别仍然只是整体类别分布的一小部分时，更应该这样做。你可以将这比作在选举图中将小党派归入*其他*标签，而主要展示大党派。
- en: At this point, you should have a good understanding of different encoding techniques.
    In the next section, we will discuss how we can try out these techniques on a
    real dataset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对不同的编码技术有了很好的理解。在下一节中，我们将讨论我们如何在真实数据集上尝试这些技术。
- en: Testing feature engineering techniques on a tabular dataset
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在表格数据集上测试特征工程技术
- en: In [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis
    and Visualization*, we did some cleaning and statistical analysis on the **Melbourne
    Housing dataset**. After looking through a set of possible feature engineering
    methods in the previous section, you may have realized that we used some of these
    methods when we were working with our dataset.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B17928_05_ePub.xhtml#_idTextAnchor085)《执行数据分析与可视化》中，我们对**墨尔本住房数据集**进行了一些清理和统计分析。在上一节查看了一系列可能的特征工程方法之后，你可能已经意识到我们在处理数据集时使用了其中的一些方法。
- en: As an exercise, think about where we left off and, keeping the feature engineering
    options in mind, what we could do now to create new useful features, transform
    the given features, and eventually select the most prominent and predictive features
    in our dataset.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，思考我们之前停在了哪里，并考虑到特征工程选项，我们现在可以做什么来创建新的有用特征，转换给定的特征，并最终在我们的数据集中选择最突出和最具预测性的特征。
- en: For inspiration, have a look at the `02_fe_melbhousing.ipynb` file in the GitHub
    repository for this chapter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得灵感，请查看GitHub仓库中本章的`02_fe_melbhousing.ipynb`文件。
- en: In the final section of this chapter, we will leave the feature space behind
    and concentrate on the target or label for our ML training – to be more precise,
    on the cases where we are missing the labels.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将放下特征空间，专注于我们的机器学习训练的目标或标签——更准确地说，是那些缺少标签的情况。
- en: Handling data labeling
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据标注
- en: 'In this section, we will look at one of the most time-consuming and important
    tasks when it comes to preprocessing our dataset for ML training: **data labeling**.
    As we learned while looking at high-dimensional reduction and other ML techniques
    in [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis
    and Visualization*, for most scenarios, it is vitally important to have labels
    attached to our samples. As we discussed in [*Chapter 1*](B17928_01_ePub.xhtml#_idTextAnchor015),
    *Understanding the End-to-End Machine Learning Process*, there are only a few
    scenarios where unsupervised learning models are sufficient, such as a model that
    clusters emails as spam or not spam. In most cases, we want to use a supervised
    model, which means we will require labels.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨在为机器学习训练预处理数据集时最耗时且最重要的任务之一：**数据标注**。正如我们在[*第一章*](B17928_01_ePub.xhtml#_idTextAnchor015)《理解端到端机器学习流程》中学习到的那样，对于大多数场景，将标签附加到我们的样本上至关重要。正如我们在[*第五章*](B17928_05_ePub.xhtml#_idTextAnchor085)《执行数据分析与可视化》中查看高维降维和其他机器学习技术时讨论的，在大多数情况下，我们希望使用监督模型，这意味着我们需要标签。
- en: In the following sections, we will discuss what scenarios require us to do manual
    labeling and how Azure Machine Learning can help us be as efficient as possible
    to perform this monotonous task.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论哪些场景需要我们进行手动标注，以及Azure机器学习如何帮助我们尽可能高效地完成这项单调的任务。
- en: Analyzing scenarios that require labels
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析需要标签的场景
- en: We will start by looking at the types of datasets we have discussed so far and
    in which scenarios we will need to perform manual labeling.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看我们迄今为止讨论过的数据集类型，以及我们需要在哪些场景下进行手动标注。
- en: Numerical and categorical data
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值数据和分类数据
- en: As we saw when we worked with the **Melbourne Housing dataset**, for tabular
    datasets, we may often have a column that can be used as the label. In our case,
    it was the price column that we could use as a label since our goal for ML was
    to predict house prices based on specific feature inputs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在处理**墨尔本住房数据集**时所见，对于表格数据集，我们可能经常有一个可以用作标签的列。在我们的案例中，我们可以用作标签的是价格列，因为我们的机器学习目标是根据特定的特征输入预测房价。
- en: But even if this column was missing, we could have incorporated other datasets,
    such as one that shows the mean price for houses in different suburbs of Melbourne,
    to calculate a reasonable value for each of our dataset samples.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个列缺失了，我们也可以纳入其他数据集，例如显示墨尔本不同郊区的房屋平均价格的那些数据集，来为我们的数据集样本中的每一个计算一个合理的价值。
- en: Therefore, the main advantage over any of the other scenarios we will discuss
    next is that in a dataset made up of numerical and categorical features with clear
    meaning (not the pixel values of an image), we can use logic and mathematical
    functions to create a numerical label, or we can classify samples into a categorical
    label in an automated fashion. This means we do not have to look at every sample
    manually to define its label.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与其他我们将讨论的任何场景相比，主要优势在于，在由具有明确意义（不是图像的像素值）的数值和分类特征组成的数据集中，我们可以使用逻辑和数学函数来创建数值标签，或者我们可以自动将样本分类到分类标签。这意味着我们不必手动查看每个样本来定义其标签。
- en: Natural language processing
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Let's start by looking at text data. You may think that a categorical entry
    would also be text in a sense, but typically, categorical data can also be exchanged
    with mathematical values without you losing much.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看文本数据。你可能认为分类条目在某种程度上也是文本，但通常，分类数据也可以用数学值交换，而不会损失太多。
- en: 'Text data, on the other hand, denote blocks of words, such as those in this
    book, so they are much more complicated. Look at the following two sentences or
    utterances:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，文本数据表示单词块，例如这本书中的那些，因此它们要复杂得多。看看以下两个句子或话语：
- en: '*I would like to book a plane ticket for December 23rd, 2020 from Dubai to
    Paris.*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*我想预订2020年12月23日从迪拜到巴黎的机票。*'
- en: '*The room wasn''t cleaned, and the heating wouldn''t work.*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*房间没有打扫，暖气也不工作。*'
- en: How would we label these utterances? Once again, this very much depends on our
    goal for training. Maybe we just want to put these utterances into groups, such
    as order, greeting, or statement. In that scenario, every utterance would receive
    one label. On the other hand, we may want to drill down into the meaning of the
    words in the sentence. For our first utterance, we may want to understand the
    meaning of the order to offer an answer by showing possible flight options. For
    the second utterance, we may want to understand the sentiment since it is a statement
    about the quality of a hotel room.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如何标记这些话语？这非常取决于我们的训练目标。也许我们只想将这些话语分组，例如订单、问候或陈述。在这种情况下，每个话语都会收到一个标签。另一方面，我们可能想要深入挖掘句子中单词的意义。对于我们的第一个话语，我们可能想要理解订单的意义，通过展示可能的航班选项来提供答案。对于第二个话语，我们可能想要理解情感，因为它是对酒店房间质量的陈述。
- en: Therefore, we need to start labeling single words or phrases in the utterance
    itself, while looking for the semantic meaning.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要在话语本身开始标记单个单词或短语，同时寻找其语义意义。
- en: We will come back to this topic in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第7章*](B17928_07_ePub.xhtml#_idTextAnchor112)中回到这个话题，*使用NLP的高级特征提取*。
- en: Computer vision
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: 'When we talk about ML modeling for images, we are typically trying to understand
    and learn about one of the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论图像的机器学习建模时，我们通常试图理解和学习以下之一：
- en: '**Image classification**: Classify an image into one or more classes. Typical
    use cases include image searches, library management, and sentiment analysis of
    a person.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类**：将图像分类到一类或多类。典型用例包括图像搜索、图书馆管理和对人的情感分析。'
- en: '**Object detection**: Localize specific objects in an image. Typical use cases
    include pedestrian detection, traffic flow analysis, and object counting.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测**：在图像中定位特定对象。典型用例包括行人检测、交通流量分析和对象计数。'
- en: '**Image segmentation**: Assign each pixel of an image to a specific segment.
    Typical use cases include precise environment analysis for self-driving cars and
    pixel-precise anomaly detection in an X-ray or MRI picture.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分割**：将图像的每个像素分配到特定的区域。典型用例包括自动驾驶汽车的精确环境分析和 X 射线或 MRI 图像中的像素级异常检测。'
- en: 'The following figure shows an example of these three types:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了这三种类型的示例：
- en: '![Figure 6.11 – Different image processing methods ](img/B17928_06_11.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 不同的图像处理方法](img/B17928_06_11.jpg)'
- en: Figure 6.11 – Different image processing methods
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 不同的图像处理方法
- en: For these methods, the process of labeling them becomes more complicated, the
    further we go down the list. For classification, we can just put one or more labels
    on an image. For object detection, we start drawing so-called bounding boxes or
    polygons on the image. Finally, image segmentation becomes very complicated as
    we must assign labels for each pixel of the image. For this, highly specialized
    tooling is required.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些方法，随着我们向下查看列表，标注的过程变得更加复杂。对于分类，我们只需在图像上放置一个或多个标签。对于目标检测，我们在图像上开始绘制所谓的边界框或多边形。最后，图像分割变得非常复杂，因为我们必须为图像的每个像素分配标签。为此，需要高度专业的工具。
- en: As we will see shortly, we can use the data labeling tool from Azure Machine
    Learning Studio to do classification, object detection, and, to some degree, segmentation
    for image labeling tasks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们很快将看到的，我们可以使用 Azure Machine Learning Studio 中的数据标注工具来进行分类、目标检测，并在一定程度上进行图像标注任务的分割。
- en: Audio annotation
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频标注
- en: 'Finally, let''s talk about annotating audio data. When it comes to ML modeling
    for audio data, the following scenarios are possible:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来谈谈音频数据的标注。当涉及到音频数据的机器学习建模时，以下场景是可能的：
- en: '**Speech-to-text**: Run real-time transcription, voice assistants, pronunciation
    assessments, and similar solutions.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音转文本**：运行实时转录、语音助手、发音评估和类似解决方案。'
- en: '**Speech translation**: Translate speech to trigger actions in an application
    or device.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音翻译**：将语音翻译为触发应用程序或设备中的操作。'
- en: '**Speaker recognition**: Verify and identify speakers by their voice characteristics.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**说话人识别**：通过声音特征验证和识别说话人。'
- en: 'Therefore, annotating audio data means that we must take out snippets from
    an audio file and label these snippets accordingly. The following diagram shows
    a simple example of this:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，标注音频数据意味着我们必须从音频文件中提取片段，并相应地标注这些片段。以下图示展示了这个过程的简单示例：
- en: '![Figure 6.12 – Audio labeling process ](img/B17928_06_12.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 音频标注过程](img/B17928_06_12.jpg)'
- en: Figure 6.12 – Audio labeling process
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 音频标注过程
- en: As you can imagine, this labeling task is also not very straightforward and
    requires specialized tooling.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想，这个标注任务也不是非常直接，需要专门的工具。
- en: We have seen a lot of scenarios so far, where labeling is of utmost importance.
    Now, let's try to label some images ourselves.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了很多标注至关重要的场景。现在，让我们尝试自己标注一些图像。
- en: Performing data labeling for image classification using the Azure Machine Learning
    labeling service
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Azure Machine Learning 标注服务进行图像分类的数据标注
- en: 'In this section, we will be using the data labeling service in Azure Machine
    Learning Studio to label some assets. As we learned in [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054),
    *Preparing the Azure Machine Learning Workspace*, navigate to the Azure Machine
    Learning Studio and click on **Data Labeling** at the lower end of the menu, as
    shown in the following screenshot:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Azure Machine Learning Studio 中的数据标注服务来标注一些资产。正如我们在[*第 3 章*](B17928_03_ePub.xhtml#_idTextAnchor054)，“准备
    Azure Machine Learning 工作区”中学习的，导航到 Azure Machine Learning Studio 并在菜单底部点击**数据标注**，如图下截图所示：
- en: '![Figure 6.13 – Azure Machine Learning Studio ](img/B17928_06_13.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – Azure Machine Learning Studio](img/B17928_06_13.jpg)'
- en: Figure 6.13 – Azure Machine Learning Studio
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – Azure Machine Learning Studio
- en: 'On the following screen, click **Add Project**, which will take you to the
    following view:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个屏幕上，点击**添加项目**，这将带您到以下视图：
- en: '![Figure 6.14 – Creation wizard for a labeling project ](img/B17928_06_14.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 标注项目创建向导](img/B17928_06_14.jpg)'
- en: Figure 6.14 – Creation wizard for a labeling project
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 标注项目创建向导
- en: 'Before we start the exercise, let''s look at what kind of labeling tasks we
    can perform with the service. As shown in the preceding screenshot, we can work
    with image and text data as our data source. Switching between the **Image** and
    **Text** options on-screen, we have the following choices:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始练习之前，让我们看看我们可以使用这个服务执行哪些类型的标注任务。如图中所示，我们可以使用图像和文本数据作为数据源。在屏幕上的**图像**和**文本**选项之间切换，我们有以下选择：
- en: '**Image Classification Multi-class**: Attach a single label to each image.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类多类别**：给每张图像附加一个标签。'
- en: '**Image Classification Multi-label**: Attach multiple labels to each image.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类多标签**：给每张图像附加多个标签。'
- en: '**Object Detection (Bounding Box)**: Draw one or multiple boxes around an object
    on an image.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测（边界框）**：在图像上的一个对象周围绘制一个或多个框。'
- en: '**Instance Segmentation (Polygon)**: Draw complex polygons around an object
    on an image.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例分割（多边形）**：在图像上的一个对象周围绘制复杂的多边形。'
- en: '**Text Classification Multi-class**: Attach a single label to a piece of text.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类多类别**：给一段文本附加一个标签。'
- en: '**Text Classification Multi-label**: Attach one or multiple labels to a piece
    of text.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类多标签**：给一段文本附加一个或多个标签。'
- en: As we can see, there are a lot of helpful options when it comes to image data.
    We can even highlight and tag very specific pieces in an image by using a **bounding
    box** or a **polygon**. Using polygons, you are technically able to do a complete
    **image segmentation**, but it is quite hard to assign each pixel to a class with
    this tool.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在图像数据方面有很多有用的选项。我们可以通过使用**边界框**或**多边形**来突出显示和标记图像中的非常具体的部分。使用多边形，您在技术上能够进行完整的**图像分割**，但使用这个工具将每个像素分配到类别中相当困难。
- en: For text data, however, there are some limitations. We do not have the option
    to label specific words or phrases in a piece of text, as we discussed in the
    previous section. At the time of writing, the only option is to single- or multi-label
    a text block.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于文本数据，有一些限制。我们没有选择在一段文本中标注特定单词或短语，正如我们在上一节中讨论的那样。在撰写本文时，唯一的选择是对文本块进行单标签或多标签。
- en: 'Therefore, we will be working with images. To not make using this tool for
    the first time too complex, we will start by attaching a single label to images
    in an image dataset. In the following steps, we will create an image dataset and
    a corresponding labeling project:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将使用图像。为了不让第一次使用这个工具变得过于复杂，我们将从给图像数据集中的图像附加单个标签开始。在接下来的步骤中，我们将创建一个图像数据集和一个相应的标注项目：
- en: Before going through the wizard, let's look for a suitable image dataset to
    use. We will be using the **STL-10 dataset** ([https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)).
    This dataset contains a huge amount of small 96x96 images that can be divided
    into 10 classes (**airplane**, **bird**, **car**, **cat**, **deer**, **dog**,
    **horse**, **monkey**, **ship**, and **truck**). These 10 classes will be our
    labels. As the original page only offers us the images in binary format, we need
    to find a different source. On **Kaggle**, you often find these types of datasets
    prepared in different formats.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在通过向导之前，让我们寻找一个合适的图像数据集来使用。我们将使用**STL-10数据集**([https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/))。这个数据集包含大量的小型96x96图像，可以分成10个类别（**飞机**、**鸟**、**汽车**、**猫**、**鹿**、**狗**、**马**、**猴子**、**船**和**卡车**）。这10个类别将成为我们的标签。由于原始页面只提供给我们二进制格式的图像，我们需要找到不同的来源。在**Kaggle**上，您经常可以找到这些类型的数据集以不同的格式准备。
- en: Go to [https://www.kaggle.com/jessicali9530/stl10](https://www.kaggle.com/jessicali9530/stl10)
    and download `test_images`, which is a set of 8,000 files in `png` format. Normally,
    we would use the `unlabeled_images` set, but since there are 100,000 of them,
    we will leave them be for now.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[https://www.kaggle.com/jessicali9530/stl10](https://www.kaggle.com/jessicali9530/stl10)并下载`test_images`，这是一个包含8,000个`png`格式文件的集合。通常，我们会使用`unlabeled_images`集合，但由于有10万个，我们暂时将其保留。
- en: If you haven't done so already, download the files for this chapter to your
    device and create a new folder called `images` under the `chapter06` folder.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请将本章的文件下载到您的设备上，并在`chapter06`文件夹下创建一个名为`images`的新文件夹。
- en: 'Extract all 8,000 images to the `images` folder. After that, open the `03_reg_unlabeled_data.ipynb`
    file. In this file, you will find the code we have been using so far to connect
    to our workspace and datastore. Please replace `datastore_name` with the one you
    have been given in your ML workspace. The last code snippet of the first cell
    reads as follows:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有 8,000 张图片提取到`images`文件夹中。之后，打开`03_reg_unlabeled_data.ipynb`文件。在这个文件中，你会发现我们迄今为止用来连接到我们的工作空间和数据存储的代码。请将`datastore_name`替换为你
    ML 工作空间中给出的名称。第一个单元格的最后一段代码如下：
- en: '[PRE0]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `upload_directory` method will, with one call, upload all the files from
    the `images` folder to the datastore location you defined in the target and will
    create a file dataset object called `file_ds`. Once the upload is complete, we
    can register our new dataset with the following code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`upload_directory`方法将一次性上传`images`文件夹中的所有文件到你在目标中定义的数据存储位置，并创建一个名为`file_ds`的文件数据集对象。一旦上传完成，我们可以使用以下代码注册我们的新数据集：'
- en: '[PRE1]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you navigate to the **Datasets** tab in Azure Machine Learning Studio, you
    will see our newly registered dataset. Under the **Explore** tab, you will see
    a subset of the images, including image metadata and a preview of the images.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你导航到 Azure Machine Learning Studio 中的**数据集**选项卡，你会看到我们新注册的数据集。在**探索**选项卡下，你会看到图像的子集，包括图像元数据和图像预览。
- en: Now that we have registered our dataset, we can set up our labeling project.
    Go back to the wizard, as shown in *Figure 6.14*, enter `STL10_Labeling` as the
    project name, and choose **Image Classification Multi-class** as the type. Click
    **Next**.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经注册了我们的数据集，我们可以设置我们的标注项目。回到向导，如图 6.14 所示，将项目名称输入为`STL10_Labeling`，并选择**多类图像分类**作为类型。点击**下一步**。
- en: On the next screen, Microsoft will give you the option to hire a workforce from
    the **Azure Marketplace** to perform your labeling work. This can be a helpful
    tool, as you will soon learn how tedious this task can be. For now, we do not
    require additional help. Click **Next**.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，Microsoft 将提供从**Azure Marketplace**雇佣劳动力来完成你的标注工作的选项。这将是一个有用的工具，因为你很快就会了解到这项任务有多么繁琐。现在，我们不需要额外的帮助。点击**下一步**。
- en: Now, we can choose the dataset to work on. Select our newly create dataset,
    named `STL10_unlabeled`, and click **Next**.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以选择要工作的数据集。选择我们新创建的数据集，命名为`STL10_unlabeled`，然后点击**下一步**。
- en: We will see an option called **Incremental Refresh**. This feature updates the
    project once a day if new images have been added to the underlying dataset. We
    are not planning on doing this here, so leave it as-is and click **Next**.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将看到一个名为**增量刷新**的选项。此功能如果底层数据集中添加了新图像，则每天更新一次项目。我们目前不打算这样做，所以保持原样并点击**下一步**。
- en: The following screen asks us to define our labels. `airplane`, `bird`, `car`,
    `cat`, `deer`, `dog`, `horse`, `monkey`, `ship`, and `truck` as labels. Then,
    click **Next**.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一屏要求我们定义我们的标签。标签为`飞机`、`鸟`、`汽车`、`猫`、`鹿`、`狗`、`马`、`猴子`、`船`和`卡车`。然后，点击**下一步**。
- en: The second to last screen allows us to enter **Labeling instructions**. These
    are useful if we are not working alone on the project or we have ordered a workforce
    to do the job. Here, we can give them instructions. For us, as we are working
    alone, this is unnecessary. So, click **Next**.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 倒数第二屏允许我们输入**标注说明**。如果我们不是单独在这个项目上工作，或者我们已经订购了劳动力来完成这项工作，这些说明将很有用。在这里，我们可以给他们下指令。对我们来说，因为我们单独工作，这就不必要了。所以，点击**下一步**。
- en: Finally, we have the option to use **ML-assisted labeling**. If we do not activate
    this option, we would have to label all 8,000 images by ourselves without help.
    Please be aware that activating this option requires a GPU compute cluster that
    runs for a couple of minutes every time the assisting ML model is retrained. We
    will choose the **Use default** option, which will create an appropriate cluster
    for us. Click **Create project**. This will bring us back to the overview. When
    the cluster has been created, click on the project's name to get to the overview
    page.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有选择使用**ML 辅助标注**的选项。如果我们不激活此选项，我们就必须自己标注所有 8,000 张图片，而不需要帮助。请注意，激活此选项需要运行
    GPU 计算集群，每次辅助 ML 模型重新训练时都会运行几分钟。我们将选择**使用默认**选项，这将为我们创建一个合适的集群。点击**创建项目**。这将带我们回到概览页。当集群创建完成后，点击项目的名称以获取概览页面。
- en: 'You will see a dashboard similar to the following:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到一个类似于以下仪表板的界面：
- en: '![Figure 6.15 – The dashboard for the labeling project ](img/B17928_06_15.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 标注项目的仪表板](img/B17928_06_15.jpg)'
- en: Figure 6.15 – The dashboard for the labeling project
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 标注项目的仪表板
- en: 'The dashboard is divided into the following views:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板分为以下视图：
- en: '**Progress**: This shows the number of assets being labeled. In our case, we
    are working with 8,000 images. It also shows the status for each asset (**Completed**,
    **Skipped**, **Needs review**, and **Incomplete**).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进度**：这显示了正在标注的资产数量。在我们的案例中，我们正在处理8,000张图像。它还显示了每个资产的状态（**完成**、**跳过**、**需要审查**和**不完整**）。'
- en: '**Label class distribution**: This view will show a bar chart of which label
    has been used and how many times to classify an image.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签类别分布**：此视图将显示一个条形图，显示哪些标签被使用以及分类图像的次数。'
- en: '**Labeler performance**: This view shows how many assets each labeler has processed.
    In our case, only our name will be shown there.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注员性能**：此视图显示每个标注员处理了多少资产。在我们的案例中，只会显示我们的名字。'
- en: '**Task queue**: This view shows what tasks are in the pipeline. At the moment,
    we need to label 150 images manually before the next training phase or the next
    check occurs.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务队列**：此视图显示管道中的任务。目前，我们需要在下一个训练阶段或下一次检查之前手动标注150张图像。'
- en: '**ML-assisted labeling experiment**: This view shows the running or already
    run training experiments for the assisting ML model.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习辅助标注实验**：此视图显示了辅助机器学习模型的运行或已运行的训练实验。'
- en: If you switch the view to the **Data** tab, you will see some previews for images
    and you can review the already labeled images. This is helpful when you're working
    in a team, where a couple of people are working on labeling the images and some
    are reviewing their labeling efforts.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你切换到**数据**标签页，你会看到一些图像预览，你可以查看已经标注的图像。当你在一个团队中工作时，这很有帮助，因为一些人正在标注图像，而另一些人正在审查他们的标注工作。
- en: Finally, if you look at the `DefLabelNC6`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你查看`DefLabelNC6`。
- en: 'The following screenshot shows the overview page of this cluster:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了此集群的概览页面：
- en: '![Figure 6.16 – Labeling cluster dashboard ](img/B17928_06_16.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – 标注集群仪表板](img/B17928_06_16.jpg)'
- en: Figure 6.16 – Labeling cluster dashboard
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 标注集群仪表板
- en: As you can see, the machines that are being used for the nodes sport 6 cores,
    56 GB of RAM, and a Tesla K80 GPU. Always check the pricing page ([https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ml-server-ubuntu/](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ml-server-ubuntu/))
    when you're creating any type of compute instance on Azure. As shown on that page,
    the node we are using is called **NC6** and costs around $3 per hour. The cluster
    node shows that the cluster is **Idle**, so there are no costs. Later, you can
    check the **Runs** tabs for the duration of the training runs to understand the
    pricing implications. At the moment, a good, educated guess would be that we will
    need 2 to 4 hours for the ML-assisted support in our labeling project.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，用于节点的机器具有6个核心，56 GB的RAM和一个Tesla K80 GPU。在Azure上创建任何类型的计算实例时，请始终检查定价页面（[https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ml-server-ubuntu/](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ml-server-ubuntu/))。如该页面所示，我们使用的节点称为**NC6**，每小时大约花费3美元。集群节点显示集群是**空闲**的，因此没有费用。稍后，你可以检查**运行**标签页以了解训练运行的持续时间，从而了解定价影响。目前，一个合理的估计是，我们将在我们的标注项目中需要2到4小时的机器学习辅助支持。
- en: 'So, before we start labeling the images, let''s understand what ML-assisted
    labeling does. When you switch back to the dashboard of our labeling project,
    you will see three options under **Task queue**, as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们开始标注图像之前，让我们了解机器学习辅助标注能做什么。当你切换回我们的标注项目仪表板时，你会在**任务队列**下看到三个选项，如下所示：
- en: '**Manual**: This denotes the assets we must handle without support at any given
    point.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动**：这表示在任何时候都必须处理的资产，没有任何支持。'
- en: '**Clustered**: This denotes the assets where a clustering model was being used
    on the already labeled assets. When you work on these assets, they will be shown
    to you in groups of images that the model thinks belong to the same class.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群**：这表示在已经标注的资产上使用了聚类模型。当你处理这些资产时，它们将以模型认为属于同一类的图像组的形式显示给你。'
- en: '**Prelabeled**: This denotes the assets where a classification model was trained
    on the already labeled assets. In this case, it predicted labels for unlabeled
    assets. When you''re working on those images, you will be shown the suggested
    labels and have to check if the model was correct.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预标注**：这表示在已经标注的资产上训练了分类模型的资产。在这种情况下，它为未标注的资产预测了标签。当你处理这些图像时，你会看到建议的标签并需要检查模型是否正确。'
- en: 'Now, let''s start labeling. When you click **Label data**, you will see the
    following view:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始标记。当您点击**标记数据**时，您将看到以下视图：
- en: '![Figure 6.17 – Labeling task view ](img/B17928_06_17.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – 标记任务视图](img/B17928_06_17.jpg)'
- en: Figure 6.17 – Labeling task view
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 标记任务视图
- en: From this view, you can see the asset in the middle. With the controls up top,
    you can **Zoom in** and change the **Brightness** and **Contrast** properties
    of the image. If you are unsure about these options, you can select **Skip** for
    now. On the right, you can choose the appropriate label. If you are happy with
    your choice, you can click **Submit**.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个视图，您可以看到中间的资产。通过顶部的控件，您可以**放大**并更改图像的**亮度**和**对比度**属性。如果您对这些选项不确定，您可以暂时选择**跳过**。在右侧，您可以选择适当的标签。如果您对您的选择满意，您可以点击**提交**。
- en: Do this for a couple of images to get a grip on things. After that, look at
    the controls at the top right. Here, we can change how many assets are shown to
    us at the same time (1, 4, 6, or 9). I would suggest displaying 6 assets at the
    same time. In addition, to label pictures, you can multi-select them and use the
    keyboard numbers 1 to 9 (as shown on the right of the preceding screenshot) to
    label faster.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对几幅图像进行标记，以便掌握情况。之后，查看右上角的控件。在这里，我们可以更改同时显示给我们多少资产（1、4、6 或 9）。我建议同时显示 6 个资产。此外，为了标记图片，您可以多选它们，并使用键盘上的数字
    1 到 9（如前一张截图所示）来更快地进行标记。
- en: Now, to see the ML-assisted labeling being triggered, you will need to manually
    label around 400 to 600 images. You can decide if this is a good use of your time,
    but it is a good exercise to do as it gives you a perspective of how tedious this
    task is.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了看到机器学习辅助标记的触发，您需要手动标记大约 400 到 600 张图像。您可以决定这是否是您时间的良好利用，但这是一个很好的练习，因为它让您了解了这项任务的繁琐程度。
- en: 'Eventually, the training will be triggered, as shown in the following screenshot:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，训练将被触发，如下面的截图所示：
- en: '![Figure 6.18 – Triggered training run for labeling ](img/B17928_06_18.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 触发的标记训练运行](img/B17928_06_18.jpg)'
- en: Figure 6.18 – Triggered training run for labeling
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 触发的标记训练运行
- en: 'I had to label 616 assets manually before the first labeling training would
    be triggered. As we can see, the tool shows the distribution of label classes
    that were encountered during the labeling process at that point. As with any other
    training, this creates an experiment with runs. You can find these under `Experiments`
    in the ML workspace, as shown in the following screenshot:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次标记训练触发之前，我不得不手动标记 616 个资产。正如我们所见，该工具显示了在标记过程中遇到的标签类别的分布。与其他任何训练一样，这创建了一个带有运行的实验。您可以在
    ML 工作区的“实验”下找到这些，如下面的截图所示：
- en: '![Figure 6.19 – Experiment run for ML-assisted labeling ](img/B17928_06_19.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – 使用机器学习辅助标记的实验运行](img/B17928_06_19.jpg)'
- en: Figure 6.19 – Experiment run for ML-assisted labeling
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 使用机器学习辅助标记的实验运行
- en: 'At this point, just continue to label assets. Eventually, you will either be
    shown clustered images, defined by **Tasks clustered** at the top of the page
    (see *Figure 6.20*):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，只需继续标记资产。最终，您将看到由页面顶部的**聚类任务**定义的聚类图像（参见*图 6.20*）：
- en: '![Figure 6.20 – Data labeling showing clustered images ](img/B17928_06_20.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – 显示聚类图像的数据标记](img/B17928_06_20.jpg)'
- en: Figure 6.20 – Data labeling showing clustered images
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – 显示聚类图像的数据标记
- en: 'Or you''ll be shown prelabeled images, defined by **Tasks prelabeled** at the
    top of the page (see *Figure 6.21*):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您将看到预先标记的图像，这些图像由页面顶部的**预标记任务**定义（参见*图 6.21*）：
- en: '![Figure 6.21 – Data labeling showing prelabeled images ](img/B17928_06_21.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – 显示预标记图像的数据标记](img/B17928_06_21.jpg)'
- en: Figure 6.21 – Data labeling showing prelabeled images
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 显示预标记图像的数据标记
- en: With that, you've seen how you can utilize ML modeling to label your assets
    and how Azure Machine Learning Studio makes this process easier. As you should
    understand by now, this is a time-consuming task, but it needs to be done if you
    wish to achieve much better results in your ML training down the line.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上内容，您已经了解了如何利用机器学习建模来标记您的资产，以及 Azure 机器学习工作室如何使这一过程更加简便。正如您现在应该理解的那样，这是一项耗时的工作，但如果您希望在未来的机器学习训练中取得更好的结果，这项工作必须完成。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to prepare our features through feature engineering
    and how to prepare our labels through labeling.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过特征工程来准备我们的特征，以及如何通过标记来准备我们的标签。
- en: In the first section, we learned that feature engineering includes creating
    new and missing features, transforming existing features, extracting features
    from a high-dimensional dataset, and using methods to select the most predictive
    feature for ML training.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们了解到特征工程包括创建新的和缺失的特征、转换现有特征、从高维数据集中提取特征，以及使用方法来选择对机器学习训练最有预测性的特征。
- en: In the second section, we learned that labeling is essential and tedious. Therefore,
    tooling such as Azure Machine Learning data labeling can be a blessing to alleviate
    this time-consuming task.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们了解到标记是必不可少的且繁琐的。因此，像Azure Machine Learning数据标记这样的工具可以是一种祝福，可以减轻这项耗时的工作。
- en: The key takeaway from this chapter is that creating, transforming, and selecting
    predictive features has the biggest impact on the quality of the ML model. No
    other step in the ML pipeline will have more influence on its outcome.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键要点是，创建、转换和选择预测性特征对机器学习模型的质量影响最大。在机器学习管道中的其他任何步骤都不会对其结果产生更大的影响。
- en: To pull off quality feature engineering, you must have intimate knowledge of
    the domain (or you must know someone with that knowledge) and a clear grasp of
    how the chosen ML algorithm works internally. This includes understanding the
    mathematical theory, the required data structure the algorithm expects as input,
    and the feature engineering methods that are applied automatically when you're
    fitting the model.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成高质量的特征工程，你必须对领域有深入了解（或者你必须认识一个有这种知识的人），并且清楚地掌握所选机器学习算法的内部工作方式。这包括理解数学理论、算法期望作为输入的数据结构，以及当你拟合模型时自动应用的特征工程方法。
- en: In the next chapter, we will see feature engineering in action. We will look
    at how to perform feature extraction on text data for natural language processing.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到特征工程的实际应用。我们将探讨如何对文本数据进行特征提取，以用于自然语言处理。
