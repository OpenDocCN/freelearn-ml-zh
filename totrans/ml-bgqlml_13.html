<html><head></head><body>
		<div id="_idContainer152">
			<h1 id="_idParaDest-145"><a id="_idTextAnchor147"/><em class="italic">Chapter 10</em>: Predicting Boolean Values Using XGBoost </h1>
			<p><strong class="bold">eXtreme Gradient Boosting</strong> (<strong class="bold">XGBoost</strong>) is one of the most powerful <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) libraries that data scientists can leverage to solve complex use cases in an efficient and flexible way. It started as a research project, and the first version was released in 2014. The popularity of this ML library grew very quickly, thanks to its capabilities and portability. In fact, it was used in important Kaggle ML contests and is now available for different programming languages and on different operating systems. </p>
			<p>This library can be used to tackle different ML problems and is specifically designed for structured data. XGBoost was also recently released for BigQuery ML. Thanks to this technique, BigQuery users are allowed to implement classification and regression ML models using this library.</p>
			<p>In this chapter, we'll see all the stages necessary to implement a XGBoost classification model to classify New York City trees into different species according to their characteristics. </p>
			<p>Using the BigQuery ML SQL dialect, we'll go through the following topics:</p>
			<ul>
				<li>Introducing the business scenario</li>
				<li>Discovering the XGBoost Boosted Tree classification model</li>
				<li>Exploring and understanding the dataset</li>
				<li>Training the XGBoost classification model</li>
				<li>Evaluating the XGBoost classification model</li>
				<li>Using the XGBoost classification model</li>
				<li>Drawing business conclusions</li>
			</ul>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor148"/>Technical requirements</h1>
			<p>This chapter requires you to have access to a web browser and to be able to leverage the following:</p>
			<ul>
				<li>A <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) account to access the Google Cloud Console</li>
				<li>A GCP project to host the BigQuery datasets</li>
			</ul>
			<p>Now that we're ready in terms of the technical requirements, let's dive into the analysis and development of our BigQuery ML XGBoost classification model.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/3ujnzH3">https://bit.ly/3ujnzH3</a></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor149"/>Introducing the business scenario</h1>
			<p>In this section, we'll introduce the<a id="_idIndexMarker477"/> business scenario that will be tackled with the XGBoost classification algorithm.</p>
			<p>The business scenario is very similar to the use case presented and used in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em>. In this chapter, we'll use the same dataset but will apply a more advanced ML algorithm.</p>
			<p>We can summarize and remember that the goal of the ML model is to automatically classify the trees of New York City into different species according to their characteristics, such as their position, their size, and their health status. </p>
			<p>As we've done in <a href="B16722_09_Final_ASB_ePub.xhtml#_idTextAnchor133"><em class="italic">Chapter 9</em></a>, <em class="italic">Suggesting the Right Product by Using Matrix Factorization</em>, we can focus our attention only on the five most common species of trees present in the city. </p>
			<p>Now that we've explained and understood the business scenario, let's take a look at the ML technique that we can use to automatically classify trees according to their features.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor150"/>Discovering the XGBoost Boosted Tree classification model</h1>
			<p>In this section, we'll learn what the <strong class="bold">XGBoost Boosted Trees</strong> classification model is, and we'll understand which classification use cases can be tackled with this ML algorithm.</p>
			<p>XGBoost is an open <a id="_idIndexMarker478"/>source library that provides a portable gradient boosting framework for different languages. The XGBoost library is available for different programming languages such as C++, Java, Python, R, and Scala, and can work on different operating systems. XGBoost is used to deal with supervised learning use cases, where we use labeled training data to predict target variables. </p>
			<p>XGBoost's popularity has grown in the ML community over the years because it has often been the choice of many winning teams during ML competitions, such as the <em class="italic">Kaggle - High Energy Physics meets Machine Learning award</em> in 2016.</p>
			<p>The classification capabilities of <strong class="bold">XGBoost Boosted Trees</strong> are based on the usage of multiple decision trees that classify data to enable predictions.</p>
			<p>In the following diagram, you can see a simple representation of a decision tree that classifies animals:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B16722_10_001.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Representation of a decision tree</p>
			<p>XGBoost classification<a id="_idIndexMarker479"/> models can answer the same questions addressed by multiclass logistic regression, such as the following:</p>
			<ul>
				<li>Is the comment of my customer <em class="italic">neutral</em>, <em class="italic">positive</em>, or <em class="italic">negative</em>?</li>
				<li>Does my customer belong to the <em class="italic">gold</em>, <em class="italic">silver</em>, or <em class="italic">bronze</em> level?</li>
				<li>Is the probability of churn of a specific customer <em class="italic">high</em>, <em class="italic">medium</em>, or <em class="italic">low</em>?</li>
				<li>Does the image recognition algorithm identify a <em class="italic">cat</em>, a <em class="italic">dog</em>, a <em class="italic">mouse</em>, or a <em class="italic">cow</em>?</li>
			</ul>
			<p>In our business scenario, we can classify New York City trees into five different species by leveraging the XGBoost Boosted Tree classification model. In fact, we're interested in predicting the species according to the characteristics of each tree.</p>
			<p>During the training phase of a XGBoost algorithm, the ML model tries to find the best values to assign to each tree in order to minimize the final error metric.</p>
			<p>After the training, we'll compare the results of this model with the outcomes that we got in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em>.</p>
			<p>Now that we've<a id="_idIndexMarker480"/> learned the basics of the XGBoost Boosted Tree algorithm, it's time to take a look at the dataset that we'll use to build our ML model.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor151"/>Exploring and understanding the dataset</h1>
			<p>In this section, we'll analyze and <a id="_idIndexMarker481"/>prepare the dataset for our use case. We'll start with some data quality checks, and then we'll segment the data into training, evaluation, and test tables.</p>
			<p>Since the dataset has already been used in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em>, we will not start the analysis from the beginning. Instead, we'll focus on the most relevant queries for our business scenario. </p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor152"/>Checking the data quality</h2>
			<p>To start our<a id="_idIndexMarker482"/> exploration of the data and to carry out data quality checks, we need to do the following:</p>
			<ol>
				<li>Log in to our Google Cloud Console and access the <strong class="bold">BigQuery</strong> <strong class="bold">User Interface</strong> (<strong class="bold">UI</strong>) from the navigation menu.</li>
				<li>Create a new dataset under the project that we created in <a href="B16722_02_Final_ASB_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>,<em class="italic"> Setting Up Your GCP and BigQuery Environment</em>. For this use case, we'll create a <strong class="source-inline">10_nyc_trees_xgboost</strong> dataset with the default options.</li>
				<li>First of all, let's check if all the records contain a valid value in the <strong class="source-inline">spc_latin</strong> field by executing the following query: <p class="source-code">SELECT  COUNT(*)</p><p class="source-code">FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">WHERE</p><p class="source-code">         spc_latin is NULL;</p><p>As we can see from the following screenshot, there are <strong class="bold">31619</strong> records with a missing value in the <strong class="source-inline">spc_latin</strong> column. These records will be filtered out during the training stage:</p><div id="_idContainer144" class="IMG---Figure"><img src="image/B16722_10_002.jpg" alt="Figure 10.2 – The query result shows that some records should be filtered out&#13;&#10;"/></div><p class="figure-caption">Figure 10.2 – The query result shows that some records should be filtered out</p></li>
				<li>After this first check, we<a id="_idIndexMarker483"/> need to verify if any<a id="_idIndexMarker484"/> potential feature is characterized by <strong class="source-inline">NULL</strong> values. Let's run the following <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>) statement: <p class="source-code">SELECT  COUNT(*)</p><p class="source-code">FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">WHERE</p><p class="source-code">         spc_latin is NOT NULL</p><p class="source-code">         AND (</p><p class="source-code">            zip_city is NULL OR</p><p class="source-code">            tree_dbh is NULL OR</p><p class="source-code">            boroname is NULL OR</p><p class="source-code">            nta_name is NULL OR</p><p class="source-code">            health is NULL OR</p><p class="source-code">            sidewalk is NULL) ; </p><p>The query returns a <strong class="source-inline">COUNT</strong> of three records due to the presence of <strong class="source-inline">NULL</strong> values in the <strong class="source-inline">sidewalk</strong> and <strong class="source-inline">health</strong> fields. Despite the low number, we'll filter out these records in the following queries, to use only meaningful records.</p></li>
				<li>Then, we can extract the most common five tree species from the BigQuery public dataset. Let's execute the following query:<p class="source-code">SELECT   spc_latin,</p><p class="source-code">         COUNT(*) total</p><p class="source-code">FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">WHERE</p><p class="source-code">         spc_latin is NOT NULL</p><p class="source-code">         AND zip_city is NOT NULL</p><p class="source-code">         AND tree_dbh is NOT NULL</p><p class="source-code">         AND boroname is NOT NULL</p><p class="source-code">         AND nta_name is NOT NULL</p><p class="source-code">         AND health is NOT NULL</p><p class="source-code">         AND sidewalk is NOT NULL</p><p class="source-code">GROUP BY</p><p class="source-code">         spc_latin</p><p class="source-code">ORDER BY</p><p class="source-code">         total desc</p><p class="source-code">LIMIT 5; </p><p>The query calculates <strong class="source-inline">total</strong> as the number of <a id="_idIndexMarker485"/>records for each <strong class="source-inline">spc_latin</strong> field. An <strong class="source-inline">ORDER BY</strong> clause is used to sort the results from the largest to the smallest values of the <strong class="source-inline">total</strong> field. Then, a <strong class="source-inline">LIMIT 5</strong> clause is used to return only the first five records.</p><p>In the following screenshot, you can see the results of the query, which show the five species most frequently present in the dataset:</p><div id="_idContainer145" class="IMG---Figure"><img src="image/B16722_10_003.jpg" alt="Figure 10.3 – The most frequent tree species in the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 10.3 – The most frequent tree species in the dataset</p></li>
				<li>In order to<a id="_idIndexMarker486"/> materialize these five species into a table, let's execute the following code to create a <strong class="source-inline">`10_nyc_trees_xgboost.top5_species`</strong> table:<p class="source-code">CREATE OR REPLACE TABLE `10_nyc_trees_xgboost.top5_species` AS</p><p class="source-code">      SELECT   spc_latin,</p><p class="source-code">         COUNT(*) total</p><p class="source-code">      FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">      WHERE</p><p class="source-code">               spc_latin is NOT NULL</p><p class="source-code">               AND zip_city is NOT NULL</p><p class="source-code">               AND tree_dbh is NOT NULL</p><p class="source-code">               AND boroname is NOT NULL</p><p class="source-code">               AND nta_name is NOT NULL</p><p class="source-code">               AND health is NOT NULL</p><p class="source-code">               AND sidewalk is NOT NULL</p><p class="source-code">      GROUP BY</p><p class="source-code">               spc_latin</p><p class="source-code">      ORDER BY</p><p class="source-code">               total desc</p><p class="source-code">      LIMIT 5;</p><p>The only difference with the query executed in the previous <em class="italic">Step 5</em> is represented by the use of <strong class="source-inline">CREATE OR REPLACE TABLE</strong> keywords that are leveraged to materialize the results of the query into the new table.</p></li>
			</ol>
			<p>In this section, we've<a id="_idIndexMarker487"/> analyzed the data quality of the BigQuery public dataset. Now, let's start segmenting it into three different tables for the training, evaluation, and classification stages.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor153"/>Segmenting the dataset</h2>
			<p>Before implementing our XGBoost<a id="_idIndexMarker488"/> classification model, let's segment our dataset according to the main stages of the ML development life cycle: training, evaluation, and use. In order to randomly divide the records into three different tables, we'll use a <strong class="source-inline">MOD</strong> function on the <strong class="source-inline">tree_id</strong> numerical field. Follow these steps:</p>
			<ol>
				<li value="1">First of all, let's create a table that will contain the training dataset. To do this, we execute the following SQL statement:<p class="source-code">CREATE OR REPLACE TABLE `10_nyc_trees_xgboost.training_table` AS </p><p class="source-code">SELECT  *</p><p class="source-code">FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">WHERE</p><p class="source-code">         zip_city is NOT NULL</p><p class="source-code">         AND tree_dbh is NOT NULL</p><p class="source-code">         AND boroname is NOT NULL</p><p class="source-code">         AND nta_name is NOT NULL</p><p class="source-code">         AND health is NOT NULL</p><p class="source-code">         AND sidewalk is NOT NULL</p><p class="source-code">         AND spc_latin in </p><p class="source-code">         (SELECT spc_latin from `10_nyc_trees_xgboost.top5_species`) </p><p class="source-code">         AND MOD(tree_id,11)&lt;=8; </p><p>The query creates a <strong class="source-inline">`10_nyc_trees_xgboost.training_table`</strong> table with all the columns available in the original dataset, through a <strong class="source-inline">SELECT *</strong> statement. It applies all the filters necessary to get not empty values for the <strong class="source-inline">spc_latin</strong> label and for all the other features.</p><p>Using an <strong class="source-inline">IN</strong> clause, <strong class="source-inline">training_table</strong> will contain only the records related to the most five common species that we've identified in the dataset.</p><p>The last line of the query, with the <strong class="source-inline">MOD(tree_id,11)&lt;=8</strong> clause, allows us to pick up only 80% of the records from the entire dataset. <strong class="source-inline">MOD</strong> stands for <em class="italic">modulo</em> and returns the remainder<a id="_idIndexMarker489"/> of the division of <strong class="source-inline">tree_id</strong> by 11. With the less than or equal operator (<strong class="source-inline">&lt;=</strong>), we're approximately extracting 80% of the entire dataset.</p></li>
				<li>With a similar approach, we can create a  <strong class="source-inline">`10_nyc_trees_xgboost.evaluation_table`</strong> table that will be used for the evaluation of our ML model. Let's execute the following <strong class="source-inline">CREATE TABLE</strong> statement:<p class="source-code">CREATE OR REPLACE TABLE `10_nyc_trees_xgboost.evaluation_table` AS </p><p class="source-code">SELECT  *</p><p class="source-code">FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">WHERE</p><p class="source-code">         zip_city is NOT NULL</p><p class="source-code">         AND tree_dbh is NOT NULL</p><p class="source-code">         AND boroname is NOT NULL</p><p class="source-code">         AND nta_name is NOT NULL</p><p class="source-code">         AND health is NOT NULL</p><p class="source-code">         AND sidewalk is NOT NULL</p><p class="source-code">         AND spc_latin in </p><p class="source-code">         (SELECT spc_latin from `06_nyc_trees.top5_species`) </p><p class="source-code">         AND MOD(tree_id,11)=9;</p><p>In contrast to when we<a id="_idIndexMarker490"/> created the training table, for the <strong class="source-inline">evaluation_table</strong> table we're picking up only 10% of the records from the entire dataset, by applying a <strong class="source-inline">MOD(tree_id,11)=9</strong> filter.</p></li>
				<li>Finally, we'll execute the following SQL statement in order to create a <strong class="source-inline">`10_nyc_trees_xgboost.classification_table`</strong> table that will be used to apply our XGBoost classification model:<p class="source-code">CREATE OR REPLACE TABLE `10_nyc_trees_xgboost.classification_table` AS </p><p class="source-code">SELECT  *</p><p class="source-code">FROM    `bigquery-public-data.new_york_trees.tree_census_2015`</p><p class="source-code">WHERE</p><p class="source-code">         zip_city is NOT NULL</p><p class="source-code">         AND tree_dbh is NOT NULL</p><p class="source-code">         AND boroname is NOT NULL</p><p class="source-code">         AND nta_name is NOT NULL</p><p class="source-code">         AND health is NOT NULL</p><p class="source-code">         AND sidewalk is NOT NULL</p><p class="source-code">         AND spc_latin in </p><p class="source-code">         (SELECT spc_latin from `10_nyc_trees_xgboost.top5_species`) </p><p class="source-code">         AND MOD(tree_id,11)=10;</p><p>This new table is very similar to the previous ones, but thanks to the <strong class="source-inline">MOD</strong> function will contain the remaining 10% of the dataset.</p></li>
			</ol>
			<p>In this section, we've <a id="_idIndexMarker491"/>analyzed the dataset that contains information about trees in New York City, applied some data quality checks to exclude empty values, and segmented the dataset, focusing on the five most common species. Now that we've completed the preparatory steps, it's time to move on and start the training of our BigQuery ML model.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor154"/>Training the XGBoost classification model</h1>
			<p>Now that we've segmented the<a id="_idIndexMarker492"/> dataset into multiple tables to support the different stages of the ML model life cycle, let's focus on the training of our XGBoost classification model. Follow these steps:</p>
			<ol>
				<li value="1">Let's start with training our first ML model, <strong class="source-inline">xgboost_classification_model_version_1</strong>, as follows:<p class="source-code">CREATE OR REPLACE MODEL `10_nyc_trees_xgboost.xgboost_classification_model_version_1`</p><p class="source-code">OPTIONS</p><p class="source-code">  ( MODEL_TYPE='BOOSTED_TREE_CLASSIFIER',</p><p class="source-code">    BOOSTER_TYPE = 'GBTREE',</p><p class="source-code">    NUM_PARALLEL_TREE = 1,</p><p class="source-code">    MAX_ITERATIONS = 50,</p><p class="source-code">    TREE_METHOD = 'HIST',</p><p class="source-code">    EARLY_STOP = FALSE,</p><p class="source-code">    AUTO_CLASS_WEIGHTS=TRUE</p><p class="source-code">  ) AS</p><p class="source-code">SELECT</p><p class="source-code">  zip_city,</p><p class="source-code">  tree_dbh,</p><p class="source-code">  spc_latin as label</p><p class="source-code">FROM</p><p class="source-code">  `10_nyc_trees_xgboost.training_table` ;</p><p>In this BigQuery ML statement, we can see <strong class="source-inline">CREATE OR REPLACE MODEL</strong> keywords used to start the training of the model. These keywords are followed by the identifier of the ML model.</p><p>After the identifier, we<a id="_idIndexMarker493"/> can notice an <strong class="source-inline">OPTIONS</strong> clause. For the <strong class="source-inline">MODEL_TYPE</strong>, we've chosen a <strong class="source-inline">BOOSTED_TREE_CLASSIFIER</strong> option, which allows us to build a XGBoost classification model. The <strong class="source-inline">BOOSTER_TYPE = 'GBTREE'</strong> clause is considered a default option to train XGBoost boosted tree models.</p><p>In order to limit the complexity of training and the resource consumption, we've chosen to train only one tree in parallel with a <strong class="source-inline">NUM_PARALLEL_TREE = 1</strong> clause, and to stop the training after <strong class="source-inline">50</strong> iterations using <strong class="source-inline">MAX_ITERATIONS</strong>.</p><p>A <strong class="source-inline">HIST</strong> parameter is suggested for large datasets in the XGBoost documentation, and an <strong class="source-inline">EARLY_STOP = FALSE</strong> clause is used to prevent the training phase being stopped after the first iteration.</p><p>The last option, <strong class="source-inline">AUTO_CLASS_WEIGHTS=TRUE</strong>, is used to balance the weights— in the case of an unbalanced dataset—with some tree species that can occur more frequently than others.</p><p>This first version of the model tries to predict the species of each tree, leveraging only the <strong class="source-inline">zip_city</strong> code where the tree is planted and the diameter of the tree, <strong class="source-inline">tree_dbh</strong>.</p></li>
				<li>At the end of the training, we can access the ML model from the BigQuery navigation menu to have a look at the performance of the model. Selecting the <strong class="bold">Evaluation</strong> tab, we can see the <strong class="bold">ROC AUC</strong> value. In this case, the value is <strong class="bold">0.7775</strong>, as we can see in the following screenshot:<div id="_idContainer146" class="IMG---Figure"><img src="image/B16722_10_004.jpg" alt="Figure 10.4 – The Evaluation metrics of the XGBoost classification model&#13;&#10;"/></div><p class="figure-caption">Figure 10.4 – The Evaluation metrics of the XGBoost classification model</p><p>In the same <strong class="bold">Evaluation</strong> tab, we can <a id="_idIndexMarker494"/>also visualize the confusion matrix, which shows how many times the predicted value is equal to the actual one, as illustrated in the following screenshot:</p><div id="_idContainer147" class="IMG---Figure"><img src="image/B16722_10_005.jpg" alt="Figure 10.5 – The Evaluation tab shows the confusion matrix for the XGBoost classification model&#13;&#10;"/></div><p class="figure-caption">Figure 10.5 – The Evaluation tab shows the confusion matrix for the XGBoost classification model</p></li>
				<li>Let's try to improve our ML model by adding features that can be useful to classify the trees into<a id="_idIndexMarker495"/> different species. Let's train the second version of our BigQuery ML model by running the following code:<p class="source-code">CREATE OR REPLACE MODEL `10_nyc_trees_xgboost.xgboost_classification_model_version_2`</p><p class="source-code">OPTIONS</p><p class="source-code">  ( MODEL_TYPE='BOOSTED_TREE_CLASSIFIER',</p><p class="source-code">    BOOSTER_TYPE = 'GBTREE',</p><p class="source-code">    NUM_PARALLEL_TREE = 1,</p><p class="source-code">    MAX_ITERATIONS = 50,</p><p class="source-code">    TREE_METHOD = 'HIST',</p><p class="source-code">    EARLY_STOP = FALSE,</p><p class="source-code">    AUTO_CLASS_WEIGHTS=TRUE</p><p class="source-code">  ) AS</p><p class="source-code">SELECT</p><p class="source-code">  zip_city,</p><p class="source-code">  tree_dbh,</p><p class="source-code">  boroname,</p><p class="source-code">  nta_name,</p><p class="source-code">  spc_latin as label</p><p class="source-code">FROM</p><p class="source-code">  `10_nyc_trees_xgboost.training_table` ;</p><p>Compared to the first attempt of the previous <em class="italic">Step 1</em>, we've included additional features. In fact, we've added to the features the name of the borough contained in the <strong class="source-inline">boroname</strong> field and the <strong class="source-inline">nta_name</strong> field, which provides more specific information related to the position of the tree in the city.</p><p>After the execution of the SQL statement, let's access the <strong class="bold">Evaluation</strong> tab of the new model to see if we're improving its performance. Taking a look at the <strong class="bold">ROC AUC</strong> value of <strong class="bold">0.80</strong>, we can see a slight increase in the performance of our model compared to the first version.</p></li>
				<li>Then, we'll try to add to <a id="_idIndexMarker496"/>our ML model other features related to the health of the tree and also to the intrusiveness of its roots, which can damage adjacent sidewalks, as follows:<p class="source-code">CREATE OR REPLACE MODEL `10_nyc_trees_xgboost.xgboost_classification_model_version_3`</p><p class="source-code">OPTIONS</p><p class="source-code">  ( MODEL_TYPE='BOOSTED_TREE_CLASSIFIER',</p><p class="source-code">    BOOSTER_TYPE = 'GBTREE',</p><p class="source-code">    NUM_PARALLEL_TREE = 5,</p><p class="source-code">    MAX_ITERATIONS = 50,</p><p class="source-code">    TREE_METHOD = 'HIST',</p><p class="source-code">    EARLY_STOP = FALSE,</p><p class="source-code">    AUTO_CLASS_WEIGHTS=TRUE</p><p class="source-code">  ) AS</p><p class="source-code">SELECT</p><p class="source-code">  zip_city,</p><p class="source-code">  tree_dbh,</p><p class="source-code">  boroname,</p><p class="source-code">  nta_name,</p><p class="source-code">  health,</p><p class="source-code">  sidewalk,</p><p class="source-code">  spc_latin as label</p><p class="source-code">FROM</p><p class="source-code">  `10_nyc_trees_xgboost.training_table`;</p><p>Compared to the previous ML model, the <strong class="source-inline">xgboost_classification_model_version_3</strong> model includes a <strong class="source-inline">health</strong> field, which describes the health status of our tree, and a <strong class="source-inline">sidewalk</strong> field, which is used to specify if the roots of tree are damaging adjacent sidewalks.</p></li>
				<li>Looking at the performances<a id="_idIndexMarker497"/> of our last ML model in the <strong class="bold">Evaluation</strong> tab of the BigQuery UI, we can notice that we've achieved another increase in terms of the <strong class="bold">ROC AUC</strong>, with a value of <strong class="bold">0.8121</strong>.</li>
			</ol>
			<p>In this section, we've created different ML models by trying to use different features in our dataset. In the next steps, we'll use the model with the highest <strong class="bold">ROC AUC</strong> value: <strong class="source-inline">xgboost_classification_model_version_3</strong>.</p>
			<p>Now, let's start the evaluation stage of the XGBoost classification model on the evaluation dataset.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor155"/>Evaluating the XGBoost classification model</h1>
			<p>To evaluate our <a id="_idIndexMarker498"/>BigQuery ML model, we'll use a <strong class="source-inline">ML.EVALUATE</strong> function and the table that we've expressly created as an evaluation dataset.</p>
			<p>The following query will tell us if the model is suffering from overfitting or is able to also perform well on new data:</p>
			<p class="source-code">SELECT</p>
			<p class="source-code">  roc_auc,</p>
			<p class="source-code">  CASE</p>
			<p class="source-code">    WHEN roc_auc &gt; .9 THEN 'EXCELLENT'</p>
			<p class="source-code">    WHEN roc_auc &gt; .8 THEN 'VERY GOOD'</p>
			<p class="source-code">    WHEN roc_auc &gt; .7 THEN 'GOOD'</p>
			<p class="source-code">    WHEN roc_auc &gt; .6 THEN 'FINE'</p>
			<p class="source-code">    WHEN roc_auc &gt; .5 THEN 'NEEDS IMPROVEMENTS'</p>
			<p class="source-code">  ELSE</p>
			<p class="source-code">  'POOR'</p>
			<p class="source-code">END</p>
			<p class="source-code">  AS model_quality</p>
			<p class="source-code">FROM </p>
			<p class="source-code">  ML.EVALUATE(MODEL `10_nyc_trees_xgboost.xgboost_classification_model_version_3`,</p>
			<p class="source-code">    (</p>
			<p class="source-code">    SELECT</p>
			<p class="source-code">       zip_city,</p>
			<p class="source-code">       tree_dbh,</p>
			<p class="source-code">       boroname,</p>
			<p class="source-code">       nta_name,</p>
			<p class="source-code">       health,</p>
			<p class="source-code">       sidewalk,</p>
			<p class="source-code">       spc_latin as label</p>
			<p class="source-code">     FROM `10_nyc_trees_xgboost.evaluation_table`));</p>
			<p>The <strong class="source-inline">SELECT</strong> statement <a id="_idIndexMarker499"/>extracts the <strong class="source-inline">roc_auc</strong> value returned by the <strong class="source-inline">ML.EVALUATE</strong> function and also provides a clear description of the quality of the model that starts from <strong class="source-inline">'POOR'</strong> and can achieve an <strong class="source-inline">'EXCELLENT'</strong> grade, passing through some intermediate stages such as <strong class="source-inline">'NEEDS IMPROVEMENTS'</strong> and <strong class="source-inline">'GOOD'</strong>.</p>
			<p>Executing the query, we can see that the score is <strong class="bold">VERY GOOD</strong>, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B16722_10_006.jpg" alt="Figure 10.6 – The evaluation stage returns VERY GOOD for the quality of our BigQuery ML model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – The evaluation stage returns VERY GOOD for the quality of our BigQuery ML model</p>
			<p>Now that we've<a id="_idIndexMarker500"/> evaluated our ML model, let's see how we can apply it to other records to get a classification of the trees.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor156"/>Using the XGBoost classification model</h1>
			<p>In this section, we'll use the<a id="_idIndexMarker501"/> ML model to classify the trees into five different species according to their characteristics.</p>
			<p>To test our BigQuery ML model, we'll use a <strong class="source-inline">ML.PREDICT</strong> function on the <strong class="source-inline">classification_table</strong> table, as follows:</p>
			<p class="source-code">SELECT</p>
			<p class="source-code">  tree_id,</p>
			<p class="source-code">  actual_label,</p>
			<p class="source-code">  predicted_label_probs,</p>
			<p class="source-code">  predicted_label</p>
			<p class="source-code">FROM</p>
			<p class="source-code">  ML.PREDICT (MODEL `10_nyc_trees_xgboost.xgboost_classification_model_version_3`,</p>
			<p class="source-code">    (</p>
			<p class="source-code">    SELECT</p>
			<p class="source-code">       tree_id,</p>
			<p class="source-code">       zip_city,</p>
			<p class="source-code">       tree_dbh,</p>
			<p class="source-code">       boroname,</p>
			<p class="source-code">       nta_name,</p>
			<p class="source-code">       health,</p>
			<p class="source-code">       sidewalk,</p>
			<p class="source-code">       spc_latin as actual_label</p>
			<p class="source-code">    FROM</p>
			<p class="source-code">      `10_nyc_trees_xgboost.classification_table`</p>
			<p class="source-code">     )</p>
			<p class="source-code">  );</p>
			<p>The query is composed of a <strong class="source-inline">SELECT</strong> statement that extracts the <strong class="source-inline">tree_id</strong> value, the actual species of the<a id="_idIndexMarker502"/> tree, the probability of each predicted species, and the predicted species.</p>
			<p>In the following screenshot, you can see the result of the query execution:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B16722_10_007.jpg" alt="Figure 10.7 – The output of the query shows the actual and predicted labels &#13;&#10;with the related probabilities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – The output of the query shows the actual and predicted labels with the related probabilities</p>
			<p>In the two rows presented in the preceding screenshot, the trees with identifiers <strong class="bold">283502</strong> and <strong class="bold">226929</strong> are well classified into the <strong class="bold">Acer platanoides</strong> species, with a confidence of 61%.</p>
			<p>Now that we've tested our BigQuery ML model, let's make some final considerations by comparing the results of the XGBoost classification model with the outcome of the logistic regression used in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor157"/>Drawing business conclusions</h1>
			<p>In this section, we'll use<a id="_idIndexMarker503"/> our ML model, and we'll understand how many times the BigQuery ML model is able to classify the trees well in the <strong class="source-inline">classification_table</strong> table.</p>
			<p>Let's execute the following query to calculate how many times the predicted species is congruent with the actual one:</p>
			<p class="source-code">SELECT COUNT(*)</p>
			<p class="source-code">FROM (</p>
			<p class="source-code">      SELECT</p>
			<p class="source-code">        tree_id,</p>
			<p class="source-code">        actual_label,</p>
			<p class="source-code">        predicted_label_probs,</p>
			<p class="source-code">        predicted_label</p>
			<p class="source-code">      FROM</p>
			<p class="source-code">        ML.PREDICT (MODEL `10_nyc_trees_xgboost.xgboost_classification_model_version_3`,</p>
			<p class="source-code">          (</p>
			<p class="source-code">          SELECT</p>
			<p class="source-code">             tree_id,</p>
			<p class="source-code">             zip_city,</p>
			<p class="source-code">             tree_dbh,</p>
			<p class="source-code">             boroname,</p>
			<p class="source-code">             nta_name,</p>
			<p class="source-code">             health,</p>
			<p class="source-code">             sidewalk,</p>
			<p class="source-code">             spc_latin as actual_label</p>
			<p class="source-code">          FROM</p>
			<p class="source-code">            `10_nyc_trees_xgboost.classification_table`</p>
			<p class="source-code">           )</p>
			<p class="source-code">        )</p>
			<p class="source-code">)</p>
			<p class="source-code">WHERE</p>
			<p class="source-code">      actual_label = predicted_label; </p>
			<p>To calculate this value, we've introduced a <strong class="source-inline">WHERE</strong> clause by filtering only the rows where the predicted value is equal to the actual one.</p>
			<p>As we can see in the<a id="_idIndexMarker504"/> following screenshot, the <strong class="source-inline">SELECT COUNT</strong> returns a value of <strong class="bold">14277</strong> records:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B16722_10_008.jpg" alt="Figure 10.8 – The output of the query shows how many times the classification &#13;&#10;model predicts the right species&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – The output of the query shows how many times the classification model predicts the right species</p>
			<p>Out of a total of 27,182 rows stored in the <strong class="source-inline">classification_table</strong> table, we can say that our model classifies the trees into the right species in 52.52% of cases.</p>
			<p>In the following table, the results of the XGBoost classification model are compared with the results obtained by the multiclass logistic regression, applied in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em>:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B16722_10_009.jpg" alt=" Figure 10.9 – Comparison of the XGBoost classification model and multiclass logistic regression &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 10.9 – Comparison of the XGBoost classification model and multiclass logistic regression </p>
			<p>Looking at the preceding table, we can say that to classify the New York City trees into the most common five species, the<a id="_idIndexMarker505"/> XGBoost classification model is able to achieve better results when compared to the multiclass logistic regression model.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor158"/>Summary</h1>
			<p>In this chapter, we've implemented a XGBoost classification model. We've remembered the business scenario that was already used in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em>, based on the need to automatically classify New York City trees. After that, we've learned the basics of the XGBoost boosted tree classification model.</p>
			<p>In order to build an effective model, we performed data quality checks and then segmented the dataset according to our needs into three tables: one to host training data, a second one for the evaluation stage, and a last one to apply our classification model. </p>
			<p>During the training phase of the BigQuery ML model, we've constantly improved the performance of the ML model, using ROC AUC as a <strong class="bold">key performance indicator</strong> (<strong class="bold">KPI</strong>). </p>
			<p>After that, we've evaluated the best ML model on a new set of records to avoid any overfitting, becoming more confident about the good quality of our XGBoost classification model.</p>
			<p>Finally, we've applied our BigQuery ML model to the last subset of records to classify the trees into species, according to their characteristics. We've discovered that our ML model is able to correctly classify the trees in 52.52% of cases. Then, we've also compared the performance of the XGBoost model with the multiclass logistic regression training we did in <a href="B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088"><em class="italic">Chapter 6</em></a>, <em class="italic">Classifying Trees with Multiclass Logistic Regression</em> and noticed that XGBoost exceeded the multiclass logistic regression training's performance.</p>
			<p>In the next chapter, we'll learn about advanced <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>), leveraging BigQuery SQL syntax. </p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor159"/>Further resources</h1>
			<ul>
				<li><strong class="bold">NYC Street Tree Census public dataset</strong>: <a href="https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tree-census">https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tree-census</a> </li>
				<li><strong class="bold">XGBoost home page</strong>: <a href="https://xgboost.ai/">https://xgboost.ai/</a></li>
				<li><strong class="bold">XGBoost documentation</strong>: <a href="https://xgboost.readthedocs.io/en/latest/index.html">https://xgboost.readthedocs.io/en/latest/index.html</a></li>
				<li><strong class="bold">BigQuery ML </strong><strong class="source-inline">CREATE MODEL</strong><strong class="bold"> statement for Boosted Tree models</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree</a></li>
				<li><strong class="bold">BigQuery ML </strong><strong class="source-inline">ML.EVALUATE</strong><strong class="bold"> function</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate</a></li>
				<li><strong class="bold">BigQuery ML </strong><strong class="source-inline">ML.PREDICT</strong><strong class="bold"> function</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict</a></li>
			</ul>
		</div>
	</body></html>