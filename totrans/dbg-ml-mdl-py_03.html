<html><head></head><body>
<div id="_idContainer025">
<h1 class="chapter-number" id="_idParaDest-66"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-67"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.2.1">Debugging toward Responsible AI</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Developing successful machine learning models is not solely about achieving high performance. </span><span class="koboSpan" id="kobo.3.2">We all get excited when we improve the performance of our models. </span><span class="koboSpan" id="kobo.3.3">We feel responsible for developing a high-performance model. </span><span class="koboSpan" id="kobo.3.4">But we are also responsible for building fair and secure models. </span><span class="koboSpan" id="kobo.3.5">These goals, which are beyond performance improvement, are among the objectives of </span><em class="italic"><span class="koboSpan" id="kobo.4.1">responsible machine learning</span></em><span class="koboSpan" id="kobo.5.1">, or more broadly, </span><em class="italic"><span class="koboSpan" id="kobo.6.1">responsible artificial intelligence</span></em><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">As part of responsible machine learning modeling, we should consider transparency and accountability when training and making predictions for our models and consider governance systems for our data and </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">modeling processes.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Impartial modeling fairness in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">machine learning</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Security and privacy in </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">machine learning</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Transparency in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">learning modeling</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Accountable and open to </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">inspection modeling</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Data and </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">model governance</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.21.1">By the end of this chapter, you will understand the need and different concerns and challenges in responsible machine learning modeling. </span><span class="koboSpan" id="kobo.21.2">You will have also learned about different techniques that can help us in responsible modeling and ensuring privacy and security while developing machine </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">learning models.</span></span></p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.24.1">You need to understand the components of machine learning life cycles before reading this chapter as this will help you better understand the concepts and be able to use them in </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">your projects.</span></span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.26.1">Impartial modeling fairness in machine learning</span></h1>
<p><span class="koboSpan" id="kobo.27.1">Machine learning </span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.28.1">models make mistakes. </span><span class="koboSpan" id="kobo.28.2">But when a mistake happens, they could have biases, such as in the COMPAS example provided in </span><a href="B16369_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.29.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.30.1">, </span><em class="italic"><span class="koboSpan" id="kobo.31.1">Beyond Code Debugging</span></em><span class="koboSpan" id="kobo.32.1">. </span><span class="koboSpan" id="kobo.32.2">We need to investigate our models for the existence of such biases and revise them to eliminate these biases. </span><span class="koboSpan" id="kobo.32.3">Let’s go through more examples to clarify the importance of investigating our data and models for the existence </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.33.1">of </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">such biases.</span></span></p>
<p><span class="koboSpan" id="kobo.35.1">Recruiting is a challenging process for every company as they must identify the most suitable candidates to interview from hundreds of applicants who have submitted resumes and cover letters. </span><span class="koboSpan" id="kobo.35.2">In 2014, Amazon started to develop a hiring tool using machine learning to screen job applicants and select the best ones to pursue based on the information provided in their resumes. </span><span class="koboSpan" id="kobo.35.3">This was a text processing model that used the text in resumes to identify the key information and select the top candidates. </span><span class="koboSpan" id="kobo.35.4">But eventually, Amazon decided to abandon the system as the model was biased in selecting men over women in the hiring process. </span><span class="koboSpan" id="kobo.35.5">The main reason behind this bias was the data, which contained mainly resumes of men, that was fed into the machine learning model. </span><span class="koboSpan" id="kobo.35.6">The model learned how to identify language and key information in men’s resumes, but it was not effective when it came to women’s resumes. </span><span class="koboSpan" id="kobo.35.7">Hence, the model couldn’t rank candidates for a job application while remaining unbiased in terms </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">of gender.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">Some machine learning models are designed to predict the likelihood of hospitalization. </span><span class="koboSpan" id="kobo.37.2">These models can help reduce individual and population healthcare costs. </span><span class="koboSpan" id="kobo.37.3">However, such beneficial models can have their own biases. </span><span class="koboSpan" id="kobo.37.4">For example, hospitalization requires access to and the use of health care services, which is influenced by differences in socioeconomic conditions. </span><span class="koboSpan" id="kobo.37.5">This means that the datasets that are available for building models to predict the likelihood of hospitalization would have more positive data on people of high socioeconomic conditions compared to poor families. </span><span class="koboSpan" id="kobo.37.6">This inequality could cause biases in decision-making by machine learning models for hospitalization, which results in limiting the access of low socioeconomic people to hospitalization </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">even further.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Another example of biases in machine learning applications in the healthcare setting has been in genetic studies. </span><span class="koboSpan" id="kobo.39.2">These studies have been criticized for biases due to them not properly accounting for diversity in populations, which could result in misdiagnosis in the </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">studied diseases.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Two main sources of bias include data, which either originated from the data source or was introduced in data processing before model training, and algorithmic bias. </span><span class="koboSpan" id="kobo.41.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">review both</span><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.43.1">.</span></span></p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.44.1">Data bias</span></h2>
<p><span class="koboSpan" id="kobo.45.1">You might have heard of the “garbage in, garbage out” concept in computer science. </span><span class="koboSpan" id="kobo.45.2">This concept is </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.46.1">about the fact that if nonsense data gets into a computer tool, such as a machine learning model, the output will be nonsense. </span><span class="koboSpan" id="kobo.46.2">The data that gets fed to help train machine learning algorithms could have all sorts of issues that eventually result in biases, as mentioned previously. </span><span class="koboSpan" id="kobo.46.3">For example, the data could </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.47.1">under-represent a group, similar to women in the hiring data fed into the Amazon model. </span><span class="koboSpan" id="kobo.47.2">Recall that having this biased data shouldn’t stop us from building models, but we have to design our life cycle components, such as data selection and wrangling or model training, while considering these biases and testing our models for bias detection before bringing a model into production. </span><span class="koboSpan" id="kobo.47.3">The following are some of the sources of </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">data bias</span><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.49.1">es.</span></span></p>
<h3><span class="koboSpan" id="kobo.50.1">Data collection bias</span></h3>
<p><span class="koboSpan" id="kobo.51.1">Data that is collected </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.52.1">could contain biases, such as gender bias, as in the Amazon applicant sorting example, race bias, as in COMPAS, socioeconomic biases, as in hospitalization examples, or other kinds of biases. </span><span class="koboSpan" id="kobo.52.2">As another example, imagine that a machine learning model for autonomous driving is trained only on images of streets, cars, people, and traffic signs taken in the daytime. </span><span class="koboSpan" id="kobo.52.3">The model will be biased and not reliable in the nighttime. </span><span class="koboSpan" id="kobo.52.4">This kind of bias can be removed after providing feedback from data exploration or data wrangling steps to data collection and selection in the machine learning life cycle. </span><span class="koboSpan" id="kobo.52.5">But if it is not revised before a model gets trained, tested, and deployed, then the feedback needs to be immediately provided from model monitoring, when biases in predictions get detected, and used in the life cycle to provide less biased data </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">for model</span><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.54.1">ing.</span></span></p>
<h3><span class="koboSpan" id="kobo.55.1">Sampling bias</span></h3>
<p><span class="koboSpan" id="kobo.56.1">Another </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.57.1">source of data bias could be in the process of sampling data points or sampling the population in the </span><em class="italic"><span class="koboSpan" id="kobo.58.1">data collection</span></em><span class="koboSpan" id="kobo.59.1"> stage of the life cycle. </span><span class="koboSpan" id="kobo.59.2">For example, when sampling students to fill in a survey, our sampling process could be biased toward girls or boys, rich or poor student families, or high versus low-grade students. </span><span class="koboSpan" id="kobo.59.3">These kinds of biases cannot be easily fixed by adding samples of other groups. </span><span class="koboSpan" id="kobo.59.4">Sampling processes for filling surveys or designing clinical trials for new drug testing on patients are among examples of data collection processes where adding data to them is not </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.60.1">necessarily allowed. </span><span class="koboSpan" id="kobo.60.2">Some of these data collection processes need a prior definition of the population that cannot be changed in the middle of the process. </span><span class="koboSpan" id="kobo.60.3">In such cases, different kinds of possible biases need to be determined and considered when designing the data </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">sampling pr</span><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.62.1">ocess.</span></span></p>
<h3><span class="koboSpan" id="kobo.63.1">Exclusion bias</span></h3>
<p><span class="koboSpan" id="kobo.64.1">In the process of data cleaning and wrangling, before you start training and testing a machine learning model, features </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.65.1">could be removed because of statistical reasoning, such as low information content or variance across data points or not having a desired characteristic. </span><span class="koboSpan" id="kobo.65.2">These feature removals can sometimes cause biases in our modeling. </span><span class="koboSpan" id="kobo.65.3">Although not excluded, some of the features could also cause biases in the eventual machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">model predi</span><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.67.1">ctions.</span></span></p>
<h3><span class="koboSpan" id="kobo.68.1">Measurement or labeling bias</span></h3>
<p><span class="koboSpan" id="kobo.69.1">Measurement and annotation</span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.70.1"> biases could be caused by issues or differences in terms of technologies, experts, or non-expert data annotators, who generated or annotated the data that’s used for model training, testing, and prediction in production. </span><span class="koboSpan" id="kobo.70.2">For example, if one camera type is used to collect the data to train a machine learning model for image classification, predictions in production might have lower reliability if images in production will be captured by another camera that generates images with a </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">different </span><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.72.1">quality.</span></span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.73.1">Algorithmic bias</span></h2>
<p><span class="koboSpan" id="kobo.74.1">There could be </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.75.1">systematic errors associated with the algorithm and training process of a machine learning model. </span><span class="koboSpan" id="kobo.75.2">For example, instead of the data </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.76.1">being biased to a specific race or skin color in face recognition tools, the algorithm might result in biased predictions regarding a group with a specific skin color or race. </span><span class="koboSpan" id="kobo.76.2">Keeping the machine learning life cycle in mind, in the modular way it was presented in </span><a href="B16369_02.xhtml#_idTextAnchor076"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.77.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.78.1">, </span><em class="italic"><span class="koboSpan" id="kobo.79.1">Machine Learning Life Cycle</span></em><span class="koboSpan" id="kobo.80.1">, will help you identify the issues in a stage such as model monitoring. </span><span class="koboSpan" id="kobo.80.2">Then, the feedback can be provided for the relevant step, such as data collection or data wrangling, to eliminate the identified biases. </span><span class="koboSpan" id="kobo.80.3">There are methodologies to detect biases and resolve them that we will go through in future chapters. </span><span class="koboSpan" id="kobo.80.4">For example, we can use machine learning explainability techniques to identify the contributions of features, or their combinations, that could cause biases </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">in pr</span><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.82.1">edictions.</span></span></p>
<p><span class="koboSpan" id="kobo.83.1">In addition to eliminating</span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.84.1"> biases in our models, we also need to take into account security and privacy concerns while going through a machine learning life cycle, which is our </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">next topic.</span></span></p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.86.1">Security and privacy in machine learning</span></h1>
<p><span class="koboSpan" id="kobo.87.1">Security is a concern for</span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.88.1"> all businesses with physical or virtual products and services. </span><span class="koboSpan" id="kobo.88.2">60 years ago, each bank had to ensure the security of physical assets, such as cash and important documents, in its branches. </span><span class="koboSpan" id="kobo.88.3">But after moving to the digital world, they had to build new security systems to make sure that the data of their clients and their money and assets, which can now be transferred and changed digitally, were secure. </span><span class="koboSpan" id="kobo.88.4">Machine learning products and technologies are no exception and need to have proper security systems. </span><span class="koboSpan" id="kobo.88.5">Security concerns in machine learning settings could be related to the security of the data, the models themselves, or model predictions. </span><span class="koboSpan" id="kobo.88.6">In this section, we will introduce three important subjects regarding security and privacy in machine learning modeling: </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">data privacy</span></strong><span class="koboSpan" id="kobo.90.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">data poisoning</span></strong><span class="koboSpan" id="kobo.92.1">, and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.93.1">adversar</span><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.94.1">ial attacks</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.96.1">Data privacy</span></h2>
<p><span class="koboSpan" id="kobo.97.1">The privacy of the user</span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.98.1"> data in production or the data you have stored and used for model</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.99.1"> training and testing is an important aspect of security system design for machine learning technologies. </span><span class="koboSpan" id="kobo.99.2">The data needs to be secure for </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">many reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.101.1">If the data includes</span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.102.1"> confidential information of users, people, or organizations the data has been </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">received from</span></span></li>
<li><span class="koboSpan" id="kobo.104.1">If the data is licensed from a commercial data provider under legal contracts and should not become accessible through your services or technologies </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">with others</span></span></li>
<li><span class="koboSpan" id="kobo.106.1">If the data is generated for you and considered one of the assets of your team </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">and organization</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.108.1">In all these cases, you</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.109.1"> need to make sure the data is secure. </span><span class="koboSpan" id="kobo.109.2">You can use security systems for your databases and datasets. </span><span class="koboSpan" id="kobo.109.3">You can also design encryption processes on top of this if part of the data needs to be transferred digitally between two</span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.110.1"> serv</span><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.111.1">ers, </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">for example.</span></span></p>
<h3><span class="koboSpan" id="kobo.113.1">Data privacy attacks</span></h3>
<p><span class="koboSpan" id="kobo.114.1">Some attacks are designed to </span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.115.1">access private and confidential data in your datasets and databases, such as patient information in hospitals, customer data in banking systems, or the personal information of employees of governmental organizations. </span><span class="koboSpan" id="kobo.115.2">Three of these attacks are </span><em class="italic"><span class="koboSpan" id="kobo.116.1">data reconstruction attacks</span></em><span class="koboSpan" id="kobo.117.1">, </span><em class="italic"><span class="koboSpan" id="kobo.118.1">identity recognition attacks</span></em><span class="koboSpan" id="kobo.119.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.120.1">individual tracing attacks</span></em><span class="koboSpan" id="kobo.121.1">, all of which can be done through </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">internet protocol</span></strong><span class="koboSpan" id="kobo.123.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.124.1">IP</span></strong><span class="koboSpan" id="kobo.125.1">) trac</span><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.126.1">king, </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">for </span></span><span class="No-Break"><a id="_idIndexMarker201"/></span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">example.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.129.1">Data poisoning</span></h2>
<p><span class="koboSpan" id="kobo.130.1">Change in the meaning and quality of data is another concern in data security. </span><span class="koboSpan" id="kobo.130.2">Data could be poisoned and the resulting</span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.131.1"> changes in prediction could have drastic consequences financially, legally, and ethically for individuals, teams, and organizations. </span><span class="koboSpan" id="kobo.131.2">Imagine you designed a machine learning model with your friends for stock market prediction and your model uses news feeds and stock prices in previous days as input features. </span><span class="koboSpan" id="kobo.131.3">This data gets extracted from different resources such as Yahoo Finance and different sources of news. </span><span class="koboSpan" id="kobo.131.4">If your database gets poisoned, by changing the values of some of the features or changes in the collected data, such as the price history of a piece of stock, you might go through serious financial losses as your model might suggest that you buy stocks that will lose their value by more than 50% in a week rather than going up. </span><span class="koboSpan" id="kobo.131.5">This is an example that has financial consequences. </span><span class="koboSpan" id="kobo.131.6">However, data poisoning could have life-threatening consequences if, for example, it happens in healthcare </span><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.132.1">or </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">military systems.</span></span></p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.134.1">Adversarial attacks</span></h2>
<p><span class="koboSpan" id="kobo.135.1">Sometimes, you can fool </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.136.1">machine learning models by </span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.137.1">making very simple changes, such as adding small amounts of noise or perturbation to feature values. </span><span class="koboSpan" id="kobo.137.2">This is the concept behind generating adversarial examples and </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">adversarial attacks.</span></span></p>
<p><span class="koboSpan" id="kobo.139.1">For example, in a medical AI system, an image of a benign (that is, not harmful) mole could be diagnosed as malignant (that is, harmful and dangerous in general terms) by adding adversarial noise in the image that would not be recognizable by the human eye or simply rotating the image. </span><span class="koboSpan" id="kobo.139.2">Synonymous text substitution such as changing “</span><em class="italic"><span class="koboSpan" id="kobo.140.1">The patient has a history of back pain and chronic alcohol abuse and more recently has been seen in several...</span></em><span class="koboSpan" id="kobo.141.1">” to “</span><em class="italic"><span class="koboSpan" id="kobo.142.1">The patient has a history of lumbago and chronic alcohol dependence and more recently has been seen in several...</span></em><span class="koboSpan" id="kobo.143.1">” could change the diagnosis from</span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.144.1"> benign to malignant (Finlayson et al., 2019). </span><span class="koboSpan" id="kobo.144.2">In other applications of image classification, such as in self-driving cars, simple black and </span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.145.1">white stickers could sometimes fool models into classifying images of stop signs or frames of videos of stop signs (Eykholt et </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">al., 2018).</span></span></p>
<p><span class="koboSpan" id="kobo.147.1">Adversarial examples could mislead your system in inference or training and validating whether they get injected into your modeling data and poison it. </span><span class="koboSpan" id="kobo.147.2">There are three important aspects of knowing your adversary that can help you in protecting your systems – that is, the attacker’s goal, knowledge, and capability (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.148.1">Table 3.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">):</span></span></p>
<table class="No-Table-Style" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.150.1">Type of Knowledge about </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.151.1">the Adversary</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.152.1">Aspects of Different Types </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.153.1">of Knowledge</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.154.1">Definition</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p><span class="koboSpan" id="kobo.155.1">The </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">attacker’s goal</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.157.1">Security violation</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.158.1">The attacker tries to do </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">the following:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.160.1">Evade detection</span></span></li>
<li><span class="koboSpan" id="kobo.161.1">Compromise </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">system functionalities</span></span></li>
<li><span class="koboSpan" id="kobo.163.1">Get access to </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">private information</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.165.1">Attack specificity</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.166.1">Targeting specific or random data points to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">wrong results</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="3">
<p><span class="koboSpan" id="kobo.168.1">The </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">attacker’s knowledge</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.170.1">Perfect-knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">white-box attacks</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.172.1">The attacker knows everything about </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">the system</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.174.1">Zero-knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">black-box attacks</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.176.1">The attacker doesn’t have any knowledge of the system itself but collects information through predictions of the model </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">in production</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.178.1">Limited-knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">gray-box attacks</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.180.1">The attacker has </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">limited knowledge</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p><span class="koboSpan" id="kobo.182.1">The </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">attacker’s capability</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.184.1">Attack influence</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.185.1">Causative</span></strong><span class="koboSpan" id="kobo.186.1">: Attackers can poison train data and manipulate </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">test data</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.188.1">Exploratory</span></strong><span class="koboSpan" id="kobo.189.1">: The attacker can manipulate test </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">data only</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.191.1">Data </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">manipulation constraints</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.193.1">Constraints on data manipulation to eliminate data manipulation or make </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">it challenging</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.195.1">Table 3.1 – Types of knowledge about adversaries (Biggio et al., 2018)</span></p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.196.1">Output integrity attacks</span></h2>
<p><span class="koboSpan" id="kobo.197.1">This type of attack usually doesn’t </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.198.1">affect data processing, model training and testing, or even prediction in production. </span><span class="koboSpan" id="kobo.198.2">It comes between the output of </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.199.1">your model and what will be shown to the user. </span><span class="koboSpan" id="kobo.199.2">Based on this definition, this attack is not specific to machine learning settings. </span><span class="koboSpan" id="kobo.199.3">But in our machine learning systems, understanding this type of attack solely based on the outputs shown to the users might be challenging. </span><span class="koboSpan" id="kobo.199.4">For example, if the prediction probabilities or labels of your model in classification settings get changed once in a while, the results that are shown to the users will be wrong, but the user might accept them if they believe in our systems. </span><span class="koboSpan" id="kobo.199.5">It is our responsibility to make sure such kinds of attacks don’t challenge the integrity of the results of our model </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">in production</span><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.201.1">.</span></span></p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.202.1">System manipulation</span></h2>
<p><span class="koboSpan" id="kobo.203.1">Your machine learning system could be manipulated by intentionally designed synthetic data, which either does not exist or</span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.204.1"> might not have existed in the model training and test sets. </span><span class="koboSpan" id="kobo.204.2">This manipulation in the prediction level could not only have consequences such as time wasted for investigating wrong predictions of the model, but it could also poison your models and change the performance of your model in testing and production if the data enters your training, evaluation, or </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">test dat</span><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.206.1">a.</span></span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.207.1">Secure and private machine learning techniques</span></h2>
<p><span class="koboSpan" id="kobo.208.1">Some techniques help us in</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.209.1"> developing secure and privacy-preserving processes and tools for data storage, transfer, and use in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">learning modeling:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.211.1">Anonymization</span></strong><span class="koboSpan" id="kobo.212.1">: This technique focuses on removing information that helps in identifying individual data points, such </span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.213.1">as individual patients, within a healthcare dataset. </span><span class="koboSpan" id="kobo.213.2">This information could be very specific, such as health card numbers, which could have different names in different countries, or more general information, such as gender </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">and age.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.215.1">Pseudonymization</span></strong><span class="koboSpan" id="kobo.216.1">: Instead of </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.217.1">removing information, as in anonymization, the personally identifiable data could be replaced with synthetic substitutes as part </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">of pseudonymization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.219.1">Data and algorithm encryption</span></strong><span class="koboSpan" id="kobo.220.1">: The encryption process transforms the information – be it data or an algorithm – into </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.221.1">a new (encrypted) form. </span><span class="koboSpan" id="kobo.221.2">The encrypted data can be decrypted (so that it becomes human-readable or machine understandable) if the individual has access to the encryption key (that is, a password-style key necessary for the decryption process). </span><span class="koboSpan" id="kobo.221.3">In this way, getting access to the data and algorithm without the encryption key will be almost impossible or very difficult. </span><span class="koboSpan" id="kobo.221.4">We will review encryption techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">Advanced Encryption Standard</span></strong><span class="koboSpan" id="kobo.223.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.224.1">AES</span></strong><span class="koboSpan" id="kobo.225.1">) in </span><a href="B16369_16.xhtml#_idTextAnchor429"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.226.1">Chapter 16</span></em></span></a><span class="koboSpan" id="kobo.227.1">, </span><em class="italic"><span class="koboSpan" id="kobo.228.1">Security and Privacy in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.229.1">Machine Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.231.1">Homomorphic encryption</span></strong><span class="koboSpan" id="kobo.232.1">: This is an encryption technique that eliminates the need for data decryption at the time of prediction by a machine learning model. </span><span class="koboSpan" id="kobo.232.2">The model uses the </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.233.1">encrypted data for predictions, so the data can be kept encrypted through the whole data transfer and usage process in a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">learning pipeline.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.235.1">Federated machine learning</span></strong><span class="koboSpan" id="kobo.236.1">: Federated machine learning relies on the idea of decentralizing learning, data </span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.237.1">analysis, and inference, thus allowing the user data to be kept within individual devices or </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">local databases.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.239.1">Differential privacy</span></strong><span class="koboSpan" id="kobo.240.1">: Differential </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.241.1">privacy tries to ensure that the </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.242.1">removal or addition of individual data points does not affect the outcome of modeling. </span><span class="koboSpan" id="kobo.242.2">It attempts to learn from patterns within groups of data points. </span><span class="koboSpan" id="kobo.242.3">For example, by adding random noise from a normal distribution, it tries to make features of individual data points obscure. </span><span class="koboSpan" id="kobo.242.4">The effect of noise in learning could be eliminated based on the law of large numbers (</span><a href="https://www.britannica.com/science/law-of-large-numbers"><span class="koboSpan" id="kobo.243.1">https://www.britannica.com/science/law-of-large-numbers</span></a><span class="koboSpan" id="kobo.244.1">) if a large number of data points </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">is accessible.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.246.1">These techniques are not applicable and useful in all settings. </span><span class="koboSpan" id="kobo.246.2">For example, federated machine learning will not be helpful when you have an internal database and need to just be sure about its security. </span><span class="koboSpan" id="kobo.246.3">Differential privacy for small data sources could also </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">be u</span><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.248.1">nreliable.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.249.1">Encryption and decryption processes</span></p>
<p class="callout"><span class="koboSpan" id="kobo.250.1">Encryption is the process to transform readable data into a human-unreadable form. </span><span class="koboSpan" id="kobo.250.2">On the other hand, decryption is the process of transforming encrypted data back into its original readable format. </span><span class="koboSpan" id="kobo.250.3">You can find more information on this topic at </span><a href="https://docs.oracle.com/"><span class="koboSpan" id="kobo.251.1">https://docs.oracle.com/</span></a> <span class="No-Break"><span class="koboSpan" id="kobo.252.1">and </span></span><a href="https://learn.microsoft.com/en-ca/"><span class="No-Break"><span class="koboSpan" id="kobo.253.1">https://learn.microsoft.com/en-ca/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.254.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">In this section, we talked about privacy and security in machine learning modeling. </span><span class="koboSpan" id="kobo.255.2">Even if we build a secure system with minimum privacy concerns, we need to consider other factors to build trust in our models. </span><span class="koboSpan" id="kobo.255.3">Transparency is one of those factors. </span><span class="koboSpan" id="kobo.255.4">We will introduce </span><a id="_idTextAnchor146"/><span class="No-Break"><span class="koboSpan" id="kobo.256.1">this next.</span></span></p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.257.1">Transparency in machine learning modeling</span></h1>
<p><span class="koboSpan" id="kobo.258.1">Transparency helps </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.259.1">users of your model trust it by helping them understand how it works and how it was built. </span><span class="koboSpan" id="kobo.259.2">It also helps you, your team, your collaborators, and your organization to collect feedback on different components of your machine learning life cycle. </span><span class="koboSpan" id="kobo.259.3">It is worth understanding the transparency requirements in different stages of a life cycle and the challenges in </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">achieving them:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.261.1">Data collection</span></strong><span class="koboSpan" id="kobo.262.1">: Transparency in data collection needs to answer two </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">major questions:</span></span><ul><li><span class="koboSpan" id="kobo.264.1">What data are </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">you collecting?</span></span></li><li><span class="koboSpan" id="kobo.266.1">What do you want to use that </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">data for?</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.268.1">For example, when users click on the agreement button for data usage when registering for a mobile phone app, they are giving consent for the information they provide in the app to be used. </span><span class="koboSpan" id="kobo.268.2">But the agreement needs to be clear on the part of the user data that is going to be used and for </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">what purposes.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.270.1">Data selection and exploration</span></strong><span class="koboSpan" id="kobo.271.1">: In these stages of the life cycle, your process of data selection and how you achieved your exploratory results need to be clear. </span><span class="koboSpan" id="kobo.271.2">This helps you collect feedback from other collaborators and colleagues on </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">your project.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.273.1">Data wrangling and modeling data preparation</span></strong><span class="koboSpan" id="kobo.274.1">: Before this step, data is almost like the so-called raw data, without any changes in feature definition or data being split into train and test sets. </span><span class="koboSpan" id="kobo.274.2">If you design these components of the life cycle as a black box and it’s not transparent, you might lose both trust and the opportunity for feedback from other experts with future access to your data and results. </span><span class="koboSpan" id="kobo.274.3">For example, imagine you are supposed to not use the genetic information of patients in hospitals, and you provide features called Feature1, Feature2, and so on after these steps in the life cycle. </span><span class="koboSpan" id="kobo.274.4">Without explaining how those features were generated and using what original features, people cannot be sure if you used patients’ genetic information or not. </span><span class="koboSpan" id="kobo.274.5">You also need to be transparent about how you designed your testing strategy and separated your training data from validation </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">and testing.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.276.1">Model training and evaluation</span></strong><span class="koboSpan" id="kobo.277.1">: Transparency in model training helps in understanding the decision-making </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.278.1">and pattern recognition aspects of models when learning from data. </span><span class="koboSpan" id="kobo.278.2">Transparency in both training and evaluation builds trust for direct users, developers, and auditors to better assess these processes. </span><span class="koboSpan" id="kobo.278.3">It is true that, for example, &gt;99% of Amazon Prime users never want to understand the machine learning modeling behind the scenes. </span><span class="koboSpan" id="kobo.278.4">However, our models are sometimes used directly by users, such as doctors in hospitals for diagnosis or employees of manufacturing facilities. </span><em class="italic"><span class="koboSpan" id="kobo.279.1">Explainability</span></em><span class="koboSpan" id="kobo.280.1">, which we will discuss in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.281.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.282.1">, </span><em class="italic"><span class="koboSpan" id="kobo.283.1">Interpretability and Explainability in Machine Learning Modeling</span></em><span class="koboSpan" id="kobo.284.1">, is an important concept in machine learning modeling that has a close relationship with transparency and helps users better understand how the </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">model works.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.286.1">Testing</span></strong><span class="koboSpan" id="kobo.287.1">: Transparency in decision-making in the testing stage of the life cycle that helps in better understanding the process of selecting or rejecting a model, or planning for improving and debugging the related code, data processing, or model training </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">and evaluation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.289.1">Model deployment and monitoring</span></strong><span class="koboSpan" id="kobo.290.1">: In addition to getting feedback from other developers and experts in your organization and team, or public experts, you need to make the effect of the model in production and what data is collected for monitoring clear for users and auditors within or outside of </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">your organization.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.292.1">Despite the need for transparency, an increase in transparency might make your system vulnerable from the perspectives of security and privacy. </span><span class="koboSpan" id="kobo.292.2">So, you cannot aim for maximum transparency without keeping privacy and security </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">in mind.</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">Fairness and bias removal, privacy, and transparency are all factors in responsible machine learning modeling. </span><span class="koboSpan" id="kobo.294.2">But </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.295.1">we need to remember that the accountability for achieving fair and secure models and syst</span><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.296.1">ems is </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">on us.</span></span></p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.298.1">Accountable and open to inspection modeling</span></h1>
<p><span class="koboSpan" id="kobo.299.1">The models we develop as employees of different businesses or incorporations, research institutes or universities, or as freelancers could eventually get into production in different applications, such</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.300.1"> as healthcare, finance, manufacturing, marketing, retail, transportation, media, and entertainment. </span><span class="koboSpan" id="kobo.300.2">Our models could use patient data to predict whether they will get cancer or diabetes or whether they will respond to therapy. </span><span class="koboSpan" id="kobo.300.3">Alternatively, they could use the financial history and other information on the clients of a bank to assess their eligibility for loans. </span><span class="koboSpan" id="kobo.300.4">Another example is that our model can use the history of people’s purchases to recommend new products </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">to them.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">As we discussed in this chapter, we have to take care of the privacy of data and models, provide a fair and impartial model, and make our models as transparent as possible. </span><span class="koboSpan" id="kobo.302.2">But we have to remember that we are accountable for managing all these concerns in developing reliable and responsible machine learning models. </span><span class="koboSpan" id="kobo.302.3">I asked ChatGPT, the famous machine learning chatbot developed by OpenAI, “</span><em class="italic"><span class="koboSpan" id="kobo.303.1">Are you accountable for your responses?</span></em><span class="koboSpan" id="kobo.304.1">” It responded </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">as follows:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.306.1">“As a machine learning model, I do not have the ability to be held accountable for my responses in the same way that a human would. </span><span class="koboSpan" id="kobo.306.2">I am designed to generate responses based on the input I receive, but I do not have personal beliefs, intentions, and motivations. </span><span class="koboSpan" id="kobo.306.3">I am not capable of making decisions or choices on my own. </span><span class="koboSpan" id="kobo.306.4">My responses are a reflection of the training data and algorithms that were used to create me, and my creators are responsible for ensuring that I am functioning </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.307.1">as intended.”</span></em></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.308.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.309.1">There are quotes in the next few paragraphs of this chapter from legal sources that haven’t been changed to make sure the legal content is not rephrased inappropriately. </span><span class="koboSpan" id="kobo.309.2">However, this is not a legal book and no content of this book should be considered as </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">legal advice.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">But accountability is not optional. </span><span class="koboSpan" id="kobo.311.2">There have been legislations and regulations in the last few years to keep the developers and owners of machine learning models and products accountable regarding the concerns</span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.312.1"> we talked about in this chapter. </span><span class="koboSpan" id="kobo.312.2">For example, the </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">European Union’s</span></strong><span class="koboSpan" id="kobo.314.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.315.1">EU’s</span></strong><span class="koboSpan" id="kobo.316.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.317.1">General Data Protection Regulation</span></strong><span class="koboSpan" id="kobo.318.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.319.1">GDPR</span></strong><span class="koboSpan" id="kobo.320.1">) lists the rights of the individuals whose personal data is being processed to give them control over their data. </span><span class="koboSpan" id="kobo.320.2">It does this through the </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">following aspects:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.322.1">The need for an individual’s clear consent to process </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">their data</span></span></li>
<li><span class="koboSpan" id="kobo.324.1">Easier access for the data subject to </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">their data</span></span></li>
<li><span class="koboSpan" id="kobo.326.1">The rights to rectification, to erasure, and to </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">be forgotten</span></span></li>
<li><span class="koboSpan" id="kobo.328.1">The right to object, including to the use of personal data, </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">for profiling</span></span></li>
<li><span class="koboSpan" id="kobo.330.1">The right to data portability from one service provider </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">to another</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.332.1">The EU also established a judicial remedy and compensation system (</span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">source: </span></span><a href="https://www.consilium.europa.eu/en/policies/data-protection/"><span class="No-Break"><span class="koboSpan" id="kobo.334.1">https://www.consilium.europa.eu/en/policies/data-protection/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.335.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">The EU later</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.337.1"> developed the </span><strong class="bold"><span class="koboSpan" id="kobo.338.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.339.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.340.1">AI</span></strong><span class="koboSpan" id="kobo.341.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.342.1">Act</span></strong><span class="koboSpan" id="kobo.343.1"> as the first law on AI by a major regulator (</span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">source: </span></span><a href="https://artificialintelligenceact.eu/"><span class="No-Break"><span class="koboSpan" id="kobo.345.1">https://artificialintelligenceact.eu/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.346.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">But these regulations are not limited to the EU. </span><span class="koboSpan" id="kobo.347.2">For example, the </span><em class="italic"><span class="koboSpan" id="kobo.348.1">White House Office of Science and Technology Policy</span></em><span class="koboSpan" id="kobo.349.1"> released the following blueprint for an AI Bill of Rights to protect the American public in the age of AI (</span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">source: </span></span><a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/"><span class="No-Break"><span class="koboSpan" id="kobo.351.1">https://www.whitehouse.gov/ostp/ai-bill-of-rights/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.352.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">Canada also later proposed the C-27 AI law, which “creates its baseline obligations through a set of primary offenses, protecting citizens from errant AI and a universal record-keeping obligation on the use of data" (</span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">source: </span></span><a href="https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624"><span class="No-Break"><span class="koboSpan" id="kobo.355.1">https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.356.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.357.1">The last topic we want to discuss in this chapter is governance in machine learning modeling. </span><span class="koboSpan" id="kobo.357.2">In the next section, you will learn how governance can help you and your organizations in developing your </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.358.1">machine</span><a id="_idTextAnchor150"/> <span class="No-Break"><span class="koboSpan" id="kobo.359.1">learning models.</span></span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.360.1">Data and model governance</span></h1>
<p><span class="koboSpan" id="kobo.361.1">Governance in machine learning modeling is about the use of tools and procedures to help you, your team, and your organization in developing reliable and responsible machine learning models. </span><span class="koboSpan" id="kobo.361.2">You shouldn’t consider it as any sort of restriction on how to conduct your projects but as an opportunity to reduce the risk of undetected mistakes. </span><span class="koboSpan" id="kobo.361.3">The governance in machine learning is supposed to be designed to help you and your organization achieve your objectives in helping humanity and business and avoid processes and models that could have ethical, legal, or financial consequences. </span><span class="koboSpan" id="kobo.361.4">Here are some examples of ways to establish </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.362.1">governance systems in a team </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">and organization:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.364.1">Define guidelines and protocols</span></strong><span class="koboSpan" id="kobo.365.1">: As we want to detect issues in our models and improve our models in terms of both performance and responsibility, we need to design guidelines and protocols for simplification and consistency. </span><span class="koboSpan" id="kobo.365.2">We need to define criteria and methods for what are considered issues with models, such as from a security perspective, and what is considered an opportunity for model improvement that’s worth spending time and effort on. </span><span class="koboSpan" id="kobo.365.3">We need to remember that machine learning modeling, considering the topics we talked about in this chapter and the different steps of the life cycle, is not an easy task and you shouldn’t expect that every developer you work with will know all of them like </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">a specialist.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.367.1">Training and mentorship</span></strong><span class="koboSpan" id="kobo.368.1">: You need to look for mentorship and training programs and read books and articles, and then provide these opportunities for your team if you are a manager. </span><span class="koboSpan" id="kobo.368.2">But you also need to bring what you or your team learn into practice. </span><span class="koboSpan" id="kobo.368.3">Each concept in machine learning modeling has its challenges. </span><span class="koboSpan" id="kobo.368.4">For example, if you decide to use defense mechanisms against adversarial attacks, it is not as simple as loading a Python library and hoping nothing happens for eternity. </span><span class="koboSpan" id="kobo.368.5">So, practice what you learn and provide opportunities for your team to bring what they learn </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">into practice.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.370.1">Define responsibilities and accountabilities</span></strong><span class="koboSpan" id="kobo.371.1">: It is not a one-person job to take care of all aspects of the machine learning life cycle to build a technology and take care of all the responsibility topics we talked about in this chapter. </span><span class="koboSpan" id="kobo.371.2">That being said, the responsibilities and accountabilities of individuals within teams and organizations need to be clearly defined to reduce the redundancy of effort while making sure nothing </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">gets missed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.373.1">Use feedback collection systems</span></strong><span class="koboSpan" id="kobo.374.1">: We need to design simple-to-use and preferably automated systems to collect feedback and act upon it throughout the machine learning life cycle. </span><span class="koboSpan" id="kobo.374.2">This feedback will help developers that are responsible for each step </span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.375.1">of a life cycle and eventually result in a better model being brought up </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">in production.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.377.1">Use a quality control process</span></strong><span class="koboSpan" id="kobo.378.1">: We need quantitative and predefined methods and protocols to assess the quality of machine learning models after training or in production or to assess processed data coming out of each stage of a machine learning life cycle. </span><span class="koboSpan" id="kobo.378.2">Having the quality control processes defined and documented help us in attaining a scalable system for a faster and more consistent quality assessment. </span><span class="koboSpan" id="kobo.378.3">However, these processes can be revised and adapted according to new criteria and the risks associated with data and the corresponding machine </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">learning models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.380.1">Now that we understand the importance of responsible machine learning modeling and reviewed important factors and techniques to achieve it, we are ready to move on to the next part of this book and get into more technical details concerning developing reliable, high-performance, and fair machine learning models</span><a id="_idTextAnchor152"/> <span class="No-Break"><span class="koboSpan" id="kobo.381.1">and technologies.</span></span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.382.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.383.1">In this chapter, we talked about the different elements of responsible AI, such as data privacy, security in machine learning systems, the different types of attacks and designing defense systems against them, transparency and accountability in the machine learning era, and how to use data and model governance to develop reliable and responsible models </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">in practice.</span></span></p>
<p><span class="koboSpan" id="kobo.385.1">This chapter and the two previous chapters, which make up </span><em class="italic"><span class="koboSpan" id="kobo.386.1">Part 1</span></em><span class="koboSpan" id="kobo.387.1"> of this book, introduced important concepts in machine learning modeling and model debugging. </span><em class="italic"><span class="koboSpan" id="kobo.388.1">Part 2</span></em><span class="koboSpan" id="kobo.389.1"> includes topics on </span><em class="italic"><span class="koboSpan" id="kobo.390.1">h</span></em><span class="koboSpan" id="kobo.391.1">ow to improve machine </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.393.1">In the next chapter, you will learn about methods for detecting issues in machine learning models and opportunities for improving the performance and generalizability of such models. </span><span class="koboSpan" id="kobo.393.2">We will cover statistical, mathematical, and visualization techniques for model debugging with real-life examples to help you quickly start implementing these methods so that you can investigate and i</span><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.394.1">mprove </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">your models.</span></span></p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.396.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.397.1">Can you explain two types of </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">data biases?</span></span></li>
<li><span class="koboSpan" id="kobo.399.1">What is the difference between white-box and black-box </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">adversarial attacks?</span></span></li>
<li><span class="koboSpan" id="kobo.401.1">Can you explain how data and algorithm encryption can help in securing the privacy and security of </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">your systems?</span></span></li>
<li><span class="koboSpan" id="kobo.403.1">Can you explain the difference between differential privacy and federated </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">machine learning?</span></span></li>
<li><span class="koboSpan" id="kobo.405.1">How does transparency help you in increasing the number of users of your machi</span><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.406.1">ne </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">learning models?</span></span></li>
</ol>
<h1 id="_idParaDest-84"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.408.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.409.1">Zou, James, and Londa Schiebinger. </span><em class="italic"><span class="koboSpan" id="kobo.410.1">AI can be sexist and racist – it’s time to make it fair</span></em><span class="koboSpan" id="kobo.411.1">. </span><span class="koboSpan" id="kobo.411.2">(</span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">2018): 324-326.</span></span></li>
<li><span class="koboSpan" id="kobo.413.1">Nushi, Besmira, Ece Kamar, and Eric Horvitz. </span><em class="italic"><span class="koboSpan" id="kobo.414.1">Towards accountable ai: Hybrid human-machine analyses for characterizing system failure</span></em><span class="koboSpan" id="kobo.415.1">. </span><span class="koboSpan" id="kobo.415.2">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. </span><span class="koboSpan" id="kobo.415.3">Vol. </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">6. </span><span class="koboSpan" id="kobo.416.2">2018.</span></span></li>
<li><span class="koboSpan" id="kobo.417.1">Busuioc, Madalina. </span><em class="italic"><span class="koboSpan" id="kobo.418.1">Accountable artificial intelligence: Holding algorithms to account.</span></em><span class="koboSpan" id="kobo.419.1"> Public Administration Review 81.5 (</span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">2021): 825-836.</span></span></li>
<li><span class="koboSpan" id="kobo.421.1">Unceta, Irene, Jordi Nin, and Oriol Pujol. </span><em class="italic"><span class="koboSpan" id="kobo.422.1">Risk mitigation in algorithmic accountability: The role of machine learning copies.</span></em><span class="koboSpan" id="kobo.423.1"> Plos one 15.11 (</span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">2020): e0241286.</span></span></li>
<li><span class="koboSpan" id="kobo.425.1">Leonelli, Sabina. </span><em class="italic"><span class="koboSpan" id="kobo.426.1">Data governance is key to interpretation: Reconceptualizing data in data science.</span></em><span class="koboSpan" id="kobo.427.1"> Harvard Data Science Review 1.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">2019): 10-1162.</span></span></li>
<li><span class="koboSpan" id="kobo.429.1">Sridhar, Vinay, et al. </span><em class="italic"><span class="koboSpan" id="kobo.430.1">Model governance: Reducing the anarchy of production {ML}.</span></em><span class="koboSpan" id="kobo.431.1"> 2018 USENIX Annual Technical Conference (USENIX ATC </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">18). </span><span class="koboSpan" id="kobo.432.2">2018.</span></span></li>
<li><span class="koboSpan" id="kobo.433.1">Stilgoe, Jack. </span><em class="italic"><span class="koboSpan" id="kobo.434.1">Machine learning, social learning, and the governance of self-driving cars.</span></em><span class="koboSpan" id="kobo.435.1"> Social studies of science 48.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">2018): 25-56.</span></span></li>
<li><span class="koboSpan" id="kobo.437.1">Reddy, Sandeep, et al. </span><em class="italic"><span class="koboSpan" id="kobo.438.1">A governance model for the application of AI in health care.</span></em><span class="koboSpan" id="kobo.439.1"> Journal of the American Medical Informatics Association 27.3 (</span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">2020): 491-497.</span></span></li>
<li><span class="koboSpan" id="kobo.441.1">Gervasi, Stephanie S., et al. </span><em class="italic"><span class="koboSpan" id="kobo.442.1">The Potential For Bias In Machine Learning And Opportunities For Health Insurers To Address It: Article examines the potential for bias in machine learning and opportunities for health insurers to address it.</span></em><span class="koboSpan" id="kobo.443.1"> Health Affairs 41.2 (</span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">2022): 212-218.</span></span></li>
<li><span class="koboSpan" id="kobo.445.1">Gianfrancesco, M. </span><span class="koboSpan" id="kobo.445.2">A., Tamang, S., Yazdany, J., &amp; Schmajuk, G. </span><span class="koboSpan" id="kobo.445.3">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.446.1">Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data</span></em><span class="koboSpan" id="kobo.447.1">. </span><span class="koboSpan" id="kobo.447.2">JAMA internal medicine, </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">178(11), 1544.</span></span></li>
<li><span class="koboSpan" id="kobo.449.1">Finlayson, Samuel G., et al. </span><em class="italic"><span class="koboSpan" id="kobo.450.1">Adversarial attacks on medical machine learning.</span></em><span class="koboSpan" id="kobo.451.1"> Science 363.6433 (</span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">2019): 1287-1289.</span></span></li>
<li><span class="koboSpan" id="kobo.453.1">Eykholt, Kevin, et al. </span><em class="italic"><span class="koboSpan" id="kobo.454.1">Robust physical-world attacks on deep learning visual classification.</span></em><span class="koboSpan" id="kobo.455.1"> Proceedings of the IEEE conference on computer vision and pattern </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">recognition. </span><span class="koboSpan" id="kobo.456.2">2018.</span></span></li>
<li><span class="koboSpan" id="kobo.457.1">Biggio, Battista, and Fabio Roli. </span><em class="italic"><span class="koboSpan" id="kobo.458.1">Wild patterns: Ten years after the rise of adversarial machine learning.</span></em><span class="koboSpan" id="kobo.459.1"> Pattern Recognition 84 (</span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">2018): 317-331.</span></span></li>
<li><span class="koboSpan" id="kobo.461.1">Kaissis, Georgios A., et al. </span><em class="italic"><span class="koboSpan" id="kobo.462.1">Secure, privacy-preserving and federated machine learning in medical imaging.</span></em><span class="koboSpan" id="kobo.463.1"> Nature Machine Intelligence 2.6 (</span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">2020): 305-311.</span></span></li>
<li><span class="koboSpan" id="kobo.465.1">Acar, Abbas, et al. </span><em class="italic"><span class="koboSpan" id="kobo.466.1">A survey on homomorphic encryption schemes: Theory and implementation.</span></em><span class="koboSpan" id="kobo.467.1"> ACM Computing Surveys (Csur) 51.4 (</span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">2018): 1-35.</span></span></li>
<li><span class="koboSpan" id="kobo.469.1">Dwork, Cynthia. </span><em class="italic"><span class="koboSpan" id="kobo.470.1">Differential privacy: A survey of results.</span></em><span class="koboSpan" id="kobo.471.1"> International conference on theory and applications of models of computation. </span><span class="koboSpan" id="kobo.471.2">Springer, Berlin, </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">Heidelberg, 2008.</span></span></li>
<li><span class="koboSpan" id="kobo.473.1">Abadi, Martin, et al. </span><em class="italic"><span class="koboSpan" id="kobo.474.1">Deep learning with differential privacy.</span></em><span class="koboSpan" id="kobo.475.1"> Proceedings of the 2016 ACM SIGSAC conference on computer and communications </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">security. </span><span class="koboSpan" id="kobo.476.2">2016.</span></span></li>
<li><span class="koboSpan" id="kobo.477.1">Yang, Qiang, et al. </span><em class="italic"><span class="koboSpan" id="kobo.478.1">Federated machine learning: Concept and applications.</span></em><span class="koboSpan" id="kobo.479.1"> ACM Transactions on Intelligent Systems and Technology (TIST) 10.2 (</span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">2019): 1-19.</span></span></li>
</ul>
</div>


<div class="Content" id="_idContainer026">
<h1 id="_idParaDest-85" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.1.1">Part 2:Improving Machine Learning Models</span></h1>
<p><span class="koboSpan" id="kobo.2.1">This part will help us transition into the critical aspects of refining and understanding machine learning models. </span><span class="koboSpan" id="kobo.2.2">We will start with a deep dive into detecting performance and efficiency bottlenecks in models, followed by actionable strategies to enhance their performance. </span><span class="koboSpan" id="kobo.2.3">The narrative then shifts to the subject of interpretability and explainability, elucidating the importance of not just building models that work, but ones we can understand and trust. </span><span class="koboSpan" id="kobo.2.4">We will conclude this part by presenting the methods to reduce bias, emphasizing the imperative of fairness in </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B16369_04.xhtml#_idTextAnchor159"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Detecting Performance and Efficiency Issues in Machine Learning Models</span></em></li>
<li><a href="B16369_05.xhtml#_idTextAnchor183"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Improving the Performance of Machine Learning Models</span></em></li>
<li><a href="B16369_06.xhtml#_idTextAnchor201"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Interpretability and Explainability in Machine Learning Modeling</span></em></li>
<li><a href="B16369_07.xhtml#_idTextAnchor218"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Decreasing Bias and Achieving Fairness</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer027">
</div>
</div>
<div>
<div id="_idContainer028">
</div>
</div>
<div>
<div id="_idContainer029">
</div>
</div>
<div>
<div id="_idContainer030">
</div>
</div>
<div>
<div id="_idContainer031">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer032">
</div>
</div>
<div>
<div id="_idContainer033">
</div>
</div>
<div>
<div id="_idContainer034">
</div>
</div>
</body></html>