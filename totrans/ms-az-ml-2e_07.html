<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer136">
			<h1 id="_idParaDest-86"><em class="italic"><a id="_idTextAnchor085"/>Chapter 5</em>: Performing Data Analysis and Visualization</h1>
			<p>In the previous chapter, we learned how to bring our datasets to the cloud, define data stores in the Azure Machine Learning workspace to access them, and register datasets in the Azure Machine Learning dataset registry to have a good basis to start data preprocessing from. In this chapter, we will learn how to explore this raw data.</p>
			<p>First, you will learn about techniques that can help you explore tabular and file datasets. We will also talk about how to handle missing values, how to cross-correlate features to understand statistical connections between them, and how to bring domain knowledge to this process to improve our understanding of the context and the quality of our data cleansing. In addition, we will learn how to use ML algorithms not for training but for exploring our datasets.</p>
			<p>After that, we will apply these methods to a real-life dataset while learning how to work with pandas DataFrames and how to visualize the properties of our dataset. </p>
			<p>Finally, we will look at methods that can map high-dimensional data to a low-dimensional plane, which will help us see similarities and relationships between data points. Additionally, these methods can give us clear hints on how clean our data is and how effective the chosen ML algorithms will be on the dataset.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding data exploration techniques</li>
				<li>Performing data analysis on a tabular dataset</li>
				<li>Understanding dimensional reduction techniques</li>
			</ul>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>Technical requirements</h1>
			<p> In this chapter, we will use the following Python libraries and versions to perform data pre-processing and high-dimensional visualizations:</p>
			<ul>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-widgets 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-dataprep 2.20.0 </strong></li>
				<li><strong class="source-inline">pandas 1.3.2 </strong></li>
				<li><strong class="source-inline">numpy 1.19.5 </strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2 </strong></li>
				<li><strong class="source-inline">seaborn 0.11.2 </strong></li>
				<li><strong class="source-inline">plotly 5.3.1 </strong></li>
				<li><strong class="source-inline">umap_learn 0.5.1 </strong></li>
				<li><strong class="source-inline">statsmodels 0.13.0 </strong></li>
				<li><strong class="source-inline">missingno 0.5.0</strong> </li>
			</ul>
			<p>Similar to previous chapters, you can execute this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning. </p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter05</a>.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Understanding data exploration techniques</h1>
			<p>Descriptive data <a id="_idIndexMarker617"/>exploration is, without a doubt, one of the most important steps in an ML project. If you want to clean data and build derived features or select an ML algorithm to predict a target variable in your dataset, then you need to understand your data first. Your data will define many of the necessary cleaning and preprocessing steps; it will define which algorithms you can choose, and it will ultimately define the performance of your predictive model.</p>
			<p>Hence, data exploration should be considered an important analytical step to understanding whether your data is informative enough to build an ML model in the first place. By analytical step, we mean that the exploration should be done as a structured analytical process rather than a set of experimental tasks. Therefore, we will go through a checklist of data exploration tasks that you can perform as an initial step in every ML project – before you start any data cleaning, preprocessing, feature engineering, or model selection.</p>
			<p>The possible tasks we can perform are tied to the type of dataset we are working with. A lot of datasets will come in the form of tabular data, which means we have either continuous or categorical features defined for each instance of the dataset. These datasets can be visualized as a table, and we can perform basic and complex mathematical operations on them. The other general type of dataset we may encounter will come in the form of media files. This includes images, videos, sound files, documents, and anything else that is not made up of data points that you could fit into a table structure. </p>
			<p>To represent these different types of datasets, Azure Machine Learning gives us the option to save our data in one of the following objects:</p>
			<ul>
				<li><strong class="bold">TabularDataset:</strong> This class <a id="_idIndexMarker618"/>offers methods for performing basic transformations on tabular data and converting them into known formats such as pandas (<a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset">https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset</a>).</li>
				<li><strong class="bold">FileDataset:</strong> This class <a id="_idIndexMarker619"/>primarily offers filtering methods on file metadata (<a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset">https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset</a>).</li>
			</ul>
			<p>Both types of dataset objects can be registered to the Azure Machine Learning Dataset Registry for further use after preprocessing.</p>
			<p>Judging only by the methods that are available in those two classes, it becomes clear that the possible tasks and operations we can perform differ greatly between tabular datasets and file datasets. In the next few sections, we will look at both types and how we can prepare them to influence the result of our ML model.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Exploring and analyzing tabular datasets</h2>
			<p>A tabular dataset <a id="_idIndexMarker620"/>allows us to utilize the full spectrum of mathematical and statistical functions to analyze and transform our dataset, but in most <a id="_idIndexMarker621"/>cases, we do not have the time or resources to randomly run every dataset through all the possible techniques in our arsenal. </p>
			<p>Choosing the right methods does<a id="_idIndexMarker622"/> not only involve having experience in analyzing a<a id="_idIndexMarker623"/> lot of different datasets but also subject matter expertise of the domain we are working in. There are areas where everyone has some general expertise (think the influencing factors of house prices, for example), but then there are a lot of areas where specialized knowledge is needed to understand the data at hand. Imagine that you want to increase the yield of a blast furnace creating steel. In such a scenario, to understand the data, you need to have intimate knowledge of the chemical processes in the furnace, or you need a <strong class="bold">subject matter expert</strong> to support you. In every step of exploration and analysis, we need to apply domain knowledge to interpret the result and relationships we see.</p>
			<p>Besides understanding the domain, we also need to understand the features in the datasets and their targets or labels. Imagine having a dataset made up of features of houses in a certain city but without their market prices. To predict house prices, we would need labels or<a id="_idIndexMarker624"/> target values for the price<a id="_idIndexMarker625"/> of each house. On the other hand, if we were to predict if an email is spam or not and we have a dataset that contains a bunch of emails containing a lot of metadata, this might be good enough to train a model through unsupervised learning. </p>
			<p>Therefore, to get a good understanding of the dataset, we need to thoroughly explore its content and get as many insights as possible on the features and the possible target to make good decisions.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Please keep in mind that not only the feature dimensions but also the target variable needs to be preprocessed and analyzed thoroughly.</p>
			<p>To achieve this, we will <a id="_idIndexMarker626"/>start by looking at the following aspects of every feature and<a id="_idIndexMarker627"/> target vector in the dataset:</p>
			<ul>
				<li><strong class="bold">Data type</strong>: Is the content of the vector continuous, ordinal, nominal, or a text string? Are they stored in the correct programmatic data type (<strong class="source-inline">datetime</strong>, <strong class="source-inline">string</strong>, <strong class="source-inline">int</strong>, <strong class="source-inline">object</strong>)? Do we need to do a data type conversion?</li>
				<li><strong class="bold">Missing data</strong>: Are there any missing entries? How do we handle them?</li>
				<li><strong class="bold">Inconsistent data</strong>: Are date and time stored in different ways? Are the same categories written in different ways? Are there different categories with the same meaning in the given context? </li>
				<li><strong class="bold">Unique values</strong>: How many unique values exist for a categorical feature? Are there too many? Should we create a subset of them?</li>
				<li><strong class="bold">Statistical properties</strong>: What are the mean, median, and variance of a feature? Are there any outliers? What are the minimum and maximum values? What is the most common value (mode)?</li>
				<li><strong class="bold">Statistical distribution</strong>: How are the values distributed? Is there a data skew? Would normalization or scaling be useful?</li>
				<li><strong class="bold">Correlation</strong>: How are different features correlated to each other? Are there features containing similar information that could be omitted? How much are my features correlated with the target?</li>
			</ul>
			<p>Analyzing each dimension of a dataset with more than 100 feature dimensions is an extremely time-consuming task. However, instead of randomly exploring feature dimensions, you can analyze the dimensions ordered by feature importance and significantly reduce your time working through <a id="_idIndexMarker628"/>the data. Like many other areas of computer science, it is good to<a id="_idIndexMarker629"/> use an 80/20 principle for the initial data exploration, which means using only 20% of the features to achieve 80%<a id="_idIndexMarker630"/> of the performance. This <a id="_idIndexMarker631"/>sets you up for a great start and you can always come back later to add more dimensions if needed.</p>
			<p>Therefore, it is wise to understand the importance of the features for your modeling. We can do this by looking at the relationship between features and the target variable. There are many ways to do this, some of which are as follows:</p>
			<ul>
				<li><strong class="bold">Regression coefficient</strong>: Used in regression</li>
				<li><strong class="bold">Feature importance</strong>: Used in classification</li>
				<li><strong class="bold">High error rates for categorical values</strong>: Used in binary classification</li>
			</ul>
			<p>By applying these steps, you can understand the data and gain knowledge about the required preprocessing tasks for your data, features, and target variables. Along with that, it will give you a good estimate of what difficulties you can expect in your prediction task, which is essential for judging the required algorithms and validation strategies. You will also gain insight into what possible feature engineering methods could be applied to your dataset and have a better understanding of how to select a good error metric.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can use a representative subset of the data and extrapolate your hypothesis and insights to the whole dataset.</p>
			<p>Once the data has been uploaded to a storage service in Azure, we can bring up a notebook environment and start exploring the data. The goal is to thoroughly explore our data in an analytical <a id="_idIndexMarker632"/>process to understand the distribution of <em class="italic">each</em> dimension of our data. We will<a id="_idIndexMarker633"/> perform some of these steps on a tabular dataset in the <em class="italic">Performing data analysis on a tabular dataset</em> section. </p>
			<p>But first, we will look at<a id="_idIndexMarker634"/> some of the techniques<a id="_idIndexMarker635"/> that we've discussed in more detail and take a quick look at file datasets.</p>
			<h3>Handling missing values and outliers</h3>
			<p>One of the first things <a id="_idIndexMarker636"/>to look for in a new dataset is <strong class="bold">missing values</strong> for each feature and target dimension. This will help you gain a deeper understanding of the<a id="_idIndexMarker637"/> dataset and what actions could be taken to resolve them. It is not uncommon to remove missing values or impute them with zeros at the beginning of a project – however, this approach bears the risk of not properly analyzing missing values in the first place and losing a lot of data points. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Missing values can be disguised as <em class="italic">valid</em> numeric or categorical values. Typical examples are minimum or maximum values, -1, 0, or NaN. Hence, if you find the values 32,767 (= 2<span class="superscript">15</span>-1) or 65,535 (= 2<span class="superscript">16</span>-1) appearing multiple times in an integer data column, they may well be missing values disguised as the maximum signed or unsigned 16-bit integer representation. Always assume that your data contains missing values and outliers in different shapes and representations. Your task is to uncover, find, and clean them.</p>
			<p>Any prior knowledge about the data or domain will give you a competitive advantage when you're working with the data. The reason for this is that you will be able to understand <strong class="bold">missing values</strong>, <strong class="bold">outliers</strong>, and <strong class="bold">extremes</strong> concerning the data and domain, which will help you perform better imputation, cleansing, or transformation. As the next step, you should look for these outliers in your data, specifically for the absolute number or <a id="_idIndexMarker638"/>percentages of the following:</p>
			<ul>
				<li>The null values (look for <strong class="source-inline">Null</strong>, <strong class="source-inline">"Null"</strong>, <strong class="source-inline">""</strong>, <strong class="source-inline">NaN</strong>, and so on)</li>
				<li>The minimum and maximum values</li>
				<li>The most common value (<strong class="source-inline">MODE</strong>)</li>
				<li>The <strong class="source-inline">0</strong> value</li>
				<li>Any unique values</li>
			</ul>
			<p>Once you have<a id="_idIndexMarker639"/> identified these values, you can use different preprocessing techniques to impute missing values and normalize or exclude dimensions. </p>
			<p>The typical options for dealing with <a id="_idIndexMarker640"/>missing values are as follows:</p>
			<ul>
				<li><strong class="bold">Deletion</strong>: Delete entire rows or columns from the dataset. This can result in bias or having insufficient data for training.</li>
				<li><strong class="bold">New category</strong>: Add a category called <strong class="source-inline">Missing</strong> for categorical features.</li>
				<li><strong class="bold">Column average</strong>: Fill in the mean, median, or mode value of the entire data column or a subset of the column based on relationships with other features. </li>
				<li><strong class="bold">Interpolation</strong>: Fill in an interpolated value based on the column's data.</li>
				<li><strong class="bold">Hot-deck imputation</strong>: Fill in the logical previous value from the sorted records of the data column (useful in time series datasets).</li>
			</ul>
			<p>The typical options for dealing with<a id="_idIndexMarker641"/> outliers are as follows:</p>
			<ul>
				<li><strong class="bold">Erroneous observations</strong>: If the value is wrong, drop either the full column or replace the outlier with the mean of the column.</li>
				<li><strong class="bold">Leave as-is</strong>: If it contains important information and if the model does not get distorted by it.</li>
				<li><strong class="bold">Cap or floor</strong>: Cap or floor the value to a <a id="_idIndexMarker642"/>maximum deviation from the mean (for example, three standard deviations).</li>
			</ul>
			<p>To get more context<a id="_idIndexMarker643"/> when choosing the right way to handle missing<a id="_idIndexMarker644"/> values and outliers, it is useful to statistically analyze the column distribution and correlations. We will do this in the following sections.</p>
			<h3>Calculating statistical properties and visualizing data distributions</h3>
			<p>Now that you know the<a id="_idIndexMarker645"/> outliers, you can start exploring the <strong class="bold">value distribution</strong> of your <a id="_idIndexMarker646"/>dataset's features. This will help you understand <a id="_idIndexMarker647"/>which transformation and normalization techniques should be applied during data preparation. Some common distribution statistics to look for in a continuous variable are as follows:</p>
			<ul>
				<li>The mean or median value</li>
				<li>The minimum and maximum value</li>
				<li>The variance and standard deviation</li>
				<li>The 25<span class="superscript">th</span>, 50<span class="superscript">th</span> (median), and 75<span class="superscript">th</span> percentiles</li>
				<li>The data skew</li>
			</ul>
			<p>Common techniques for visualizing these distributions include using <strong class="bold">boxplots</strong>, <strong class="bold">density plots</strong>, or <strong class="bold">histograms</strong>. The following screenshot shows these different visualization techniques plotted per target class for a multi-class recognition dataset. Each method has advantages and disadvantages – boxplots<a id="_idIndexMarker648"/> show all the relevant metrics while being a bit harder to read, density<a id="_idIndexMarker649"/> plots show very smooth shapes while hiding some of the outliers, and histograms don't let you spot the median and percentiles easily while giving you a good estimate of the data skew:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B17928_05_01.jpg" alt="Figure 5.1 – A boxplot (left), a density plot (middle), and a histogram (right) " width="1258" height="294"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – A boxplot (left), a density plot (middle), and a histogram (right)</p>
			<p>Here, we can see that only histograms work well for categorical data (both nominal and ordinal). However, you could look at the number of values per category. You can find the code for creating these plots in the <strong class="source-inline">01_data_distribution.ipynb</strong> file in this book's GitHub repository. </p>
			<p>Another nice way to display the <a id="_idIndexMarker650"/>value distribution versus the target rate is in a binary classification task. The <a id="_idIndexMarker651"/>following diagram shows the <strong class="bold">version number</strong> of Windows Defender against <a id="_idIndexMarker652"/>the malware <strong class="bold">detection rate</strong> (for non-touch devices) from the <em class="italic">Microsoft Malware detection dataset</em> (<a href="https://www.kaggle.com/c/microsoft-malware-prediction/data">https://www.kaggle.com/c/microsoft-malware-prediction/data</a>):</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B17928_05_02.jpg" alt="Figure 5.2 – Version number versus detection rate for Windows Defender  " width="994" height="537"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Version number versus detection rate for Windows Defender </p>
			<p>Many statistical ML algorithms require the data to be normally distributed, so it needs to be normalized or standardized. Knowing the data distribution helps you decide which transformations need to be<a id="_idIndexMarker653"/> applied during data <a id="_idIndexMarker654"/>preparation. In practice, data often needs to be transformed, scaled, or normalized.</p>
			<h3>Finding correlated dimensions</h3>
			<p>Another common task in <a id="_idIndexMarker655"/>data exploration is looking for correlations in the dataset. This will help you dismiss feature dimensions that are highly correlated and thus may influence your ML model. In linear regression models, for example, two highly correlated independent variables will lead to large coefficients with opposite signs that ultimately cancel each other out. A much more stable regression model can be found by removing one of the correlated dimensions. Therefore, it is important not only to look at correlations between features and targets but also among features.</p>
			<p>The <strong class="bold">Pearson correlation coefficient</strong>, for<a id="_idIndexMarker656"/> example, is a popular technique that's used to measure the <em class="italic">linear</em> relationship between two variables on a scale from <strong class="source-inline">-1</strong> (strongly negatively correlated) to <strong class="source-inline">1</strong> (strongly positively correlated). A <strong class="source-inline">0</strong> indicates no linear relationship between two variables.</p>
			<p>The following diagram shows an example of a correlation matrix for the <em class="italic">California Housing dataset</em> (<a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html">https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html</a>), consisting of only continuous variables. The correlations range from <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong> and are colored accordingly, where red denotes a negative correlation and blue denotes a positive correlation. The last row shows the linear correlation between each feature dimension and the target variable (<strong class="source-inline">MedHouseVal</strong>). We can immediately tell that there is a correlation between <strong class="source-inline">Longitude</strong> and <strong class="source-inline">Latitude</strong>, between <strong class="source-inline">MedHouseVal</strong> and <strong class="source-inline">MedInc</strong>, and between <strong class="source-inline">AveRooms</strong> and <strong class="source-inline">AveBedrms</strong>. All of these relationships are relatively unsurprising:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B17928_05_03.jpg" alt="Figure 5.3 – Correlation matrix for the California Housing dataset " width="655" height="562"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Correlation matrix for the California Housing dataset</p>
			<p>You can find the code for creating this correlation matrix in the <strong class="source-inline">02_correlation.ipynb</strong> file in this book's GitHub repository. </p>
			<p>It is worth mentioning that many correlation coefficients can only be between numerical values. Ordinal variables can be encoded, for example, using integer encoding and can also compute a meaningful <a id="_idIndexMarker657"/>correlation coefficient. For nominal data, you need to fall back on different methods, such as <strong class="bold">Cramér's V</strong> to compute the<a id="_idIndexMarker658"/> correlation. It is worth noting that the input data doesn't need to be normalized (linearly scaled) before you compute the correlation coefficient.</p>
			<h3>Measuring feature and target dependencies for regression</h3>
			<p>Once we have analyzed<a id="_idIndexMarker659"/> the missing values, data distribution, and correlations, we can start analyzing the relationship between the features and the target variable. This will give us a good indication of the difficulty of the prediction problem and, hence, the expected baseline performance, which is essential for prioritizing feature engineering efforts and choosing an appropriate ML model. Another great benefit of measuring this dependency is ranking the feature dimensions by their impact on the target variable, which you can use as a priority list for data exploration and preprocessing.</p>
			<p>In a regression task, the target variable is numerical or ordinal. Therefore, we can compute the correlation coefficient between the individual features and the target variable to compute the linear dependency between the feature and the target. High correlation – that is, a high absolute <a id="_idIndexMarker660"/>correlation coefficient – indicates that a strong linear relationship exists. This gives us a great place to start exploring further. However, in many practical problems, it is rare to see a high (linear) correlation between the feature and target variables.</p>
			<p>You can also visualize this <a id="_idIndexMarker661"/>dependency between the feature and the target variable using a <strong class="bold">scatter plot</strong> or <strong class="bold">regression plot</strong>. The<a id="_idIndexMarker662"/> following diagram shows a regression plot between the average number of rooms per dwelling (<strong class="bold">RM</strong>) and the median value of owner-occupied homes (<strong class="bold">MEDV</strong>) from the <em class="italic">Boston Housing dataset</em>. If the regression line is at 45 degrees, then we have a perfect linear correlation:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17928_05_04.jpg" alt="Figure 5.4 – Scatter plot with a regression line between the feature and the target  " width="678" height="535"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Scatter plot with a regression line between the feature and the target </p>
			<p>Another great approach to determining this dependency is to fit a linear or logistic regression model to the training data. The resulting model coefficients should give you a good explanation of the <a id="_idIndexMarker663"/>relationship – the higher the coefficient, the larger the linear (for linear regression) or marginal (for logistic regression) dependency on the target variable. Hence, sorting by coefficients results in a list of features ordered by importance. Depending on the regression type, the input data should be normalized or standardized.</p>
			<p>The following screenshot <a id="_idIndexMarker664"/>shows an example of the correlation coefficients (the first column) of a fitted <strong class="bold">Ordinary Least Squares</strong> (<strong class="bold">OLS</strong>) regression model:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17928_05_05.jpg" alt="Figure 5.5 – The correlation coefficients of an OLS regression model  " width="399" height="492"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – The correlation coefficients of an OLS regression model </p>
			<p>You can find the code for creating the<a id="_idIndexMarker665"/> plot and coefficients in the <strong class="source-inline">03_regression.ipynb</strong> file in this book's GitHub repository. </p>
			<p>While the resulting <strong class="bold">R-squared metric</strong> (not shown) may<a id="_idIndexMarker666"/> not be good enough for a baseline model, the ordering of the coefficients can help us prioritize further data exploration, preprocessing, and feature engineering.</p>
			<h3>Visualizing feature and label dependency for classification</h3>
			<p>In a classification task <a id="_idIndexMarker667"/>with a multi-class nominal target variable, we can't use the regression coefficients without preprocessing the data further. Another popular method that works well out of the box is fitting a simple tree-based classifier to the training data. Depending on the size of the training data, we could use a decision tree or a tree-based ensemble classifier, such<a id="_idIndexMarker668"/> as <strong class="bold">random forest</strong> or <strong class="bold">gradient-boosted trees</strong>. Doing so results in a feature importance ranking<a id="_idIndexMarker669"/> of the feature dimensions according to the chosen split criterion. In the case of splitting by entropy, the features would be sorted by <em class="italic">information gain</em>, which would indicate which variables carry the most information about the target.</p>
			<p>The following diagram shows the feature importance fitted by a tree-based ensemble classifier using the entropy <a id="_idIndexMarker670"/>criterion from the <em class="italic">UCI Wine Recognition dataset</em><strong class="bold"> </strong>(<a href="https://archive.ics.uci.edu/ml/datasets/wine">https://archive.ics.uci.edu/ml/datasets/wine</a>):</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17928_05_06.jpg" alt="Figure 5.6 – Feature importance of the tree-based ensemble classifier  " width="442" height="433"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Feature importance of the tree-based ensemble classifier </p>
			<p>The lines represent variations in the information gain of features between individual trees. This output is a great first step to further data analysis and exploration in order of feature importance. You can find the code for calculating the feature importance and visualizing it in the <strong class="source-inline">04_feature_importance.ipynb</strong> file in this book's GitHub repository. </p>
			<p>Here is another popular approach to discovering the separability of your dataset. The following screenshot shows a dataset with three classes, where one is linearly separable and one isn't:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B17928_05_07.jpg" alt="Figure 5.7 – A linearly separable dataset (left) versus a non-linearly separable dataset (right) " width="1236" height="432"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – A linearly separable dataset (left) versus a non-linearly separable dataset (right)</p>
			<p>You can find the code for <a id="_idIndexMarker671"/>creating these separability graphs in the <strong class="source-inline">05_separability.ipynb</strong> file in this book's GitHub repository. </p>
			<p>By looking at the three clusters and the overlaps between these clusters, you can see that having separated clusters means that a trained classification model will perform very well on this dataset. On the other hand, when we know that the data is not linearly separable, we know that this task will require advanced feature engineering and modeling to produce good results.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>Exploring and analyzing file datasets</h2>
			<p>A dataset that's <a id="_idIndexMarker672"/>made up of media files is a different beast entirely. If we think of images, for example, we could present every pixel as a vector of <a id="_idIndexMarker673"/>information and <a id="_idIndexMarker674"/>see this as one feature of the image. But what could we <a id="_idIndexMarker675"/>do in terms of exploration and data cleaning? Probably not much on single features. Most of the time, what we need to do concerns a large group of pixels or the entire image itself. Broadly speaking, we could think of the following aspects:</p>
			<ul>
				<li><strong class="bold">Uniformity</strong>: All the images in the dataset should be the same size. If not, they need to be rescaled, which may involve centering pixel values per channel, possibly followed by some form of normalization.</li>
				<li><strong class="bold">Augmentation</strong>: This involves diversifying the dataset without taking on new data (new images). This is useful if we have a small dataset and typically involves horizontal and vertical flipping, cropping, and rotating, among other transformations.</li>
			</ul>
			<p>Looking at these options, it is clear that we are trying to fix something in an image dataset that could have been<a id="_idIndexMarker676"/> resolved already to a great extent when we took the images in the first place. Therefore, the reality is that when we're<a id="_idIndexMarker677"/> handling most types of media files, it is paramount to bring higher concentration toward taking good training<a id="_idIndexMarker678"/> samples for the dataset than to desperately fix them in the preprocessing stage.</p>
			<p>Let's imagine that we are a <a id="_idIndexMarker679"/>manufacturer who wants to take pictures of the products they produce passing on a conveyor belt to find defective products and discard them. Let's say that we have production facilities around the globe. What would you do to make sure the pictures are taken as uniformly as possible while covering a lot of different scenarios? Here are some aspects to consider:</p>
			<ul>
				<li><strong class="bold">Camera type</strong>: We probably need the same type of camera to be taking pictures in the same format all around the globe.</li>
				<li><strong class="bold">Environmental conditions</strong>: Is the lighting similar in all places? Are the temperature and humidity similar in all places? This could influence the electronics in the camera.</li>
				<li><strong class="bold">Positioning</strong>: Is the same angle being used to take the pictures? Can we take pictures from vastly different angles to increase variety?</li>
			</ul>
			<p>These are only some points to consider when you're taking the images.</p>
			<p>Now, let's look at another form of file data – sound files. Let's say that we want to build a speech-to-text model that converts what we say into written text. Such models are, for example, used in voice assistants to map a request to a set of actions to perform.</p>
			<p>In this context, we<a id="_idIndexMarker680"/> could use <strong class="bold">Fourier transformations</strong>, among other methods, to decompose our sound files. However, we may want to think about the samples or training data we want to <a id="_idIndexMarker681"/>train on and how we can increase the quality of them while<a id="_idIndexMarker682"/> considering the following aspects:</p>
			<ul>
				<li><strong class="bold">Recording hardware</strong>: If we have a voice assistant at home, it is probably the same microphone for everyone. But what if we build a voice assistant for mobile phones? Then, we have vastly different microphones.</li>
				<li><strong class="bold">Environment</strong>: We probably need recordings of voices in different environments. There is certainly a different sound spectrum when we are standing in a tram compared to when we are in a recording booth.</li>
				<li><strong class="bold">Pronunciation</strong>: The <em class="italic">ML algorithm</em> in your brain may have a hard time deciphering different pronunciations – especially dialects. How can an actual ML model handle this?</li>
			</ul>
			<p>These are just some points to consider when you're handling sound files. Regarding pronunciation, if you<a id="_idIndexMarker683"/> look at <strong class="bold">Azure Speech Services</strong>, you will soon realize that two models are running in the background – one for the acoustic and one for the language. Look at the requirements for samples when building a custom model (<a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train">https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-speech-test-and-train</a>) as this can give you a good idea of what is required when you're building such a model from scratch.</p>
			<p>In summary, for file <a id="_idIndexMarker684"/>datasets, we do not have as many <a id="_idIndexMarker685"/>options to statistically eliminate problems, so we should concentrate on taking good and clean samples that simulate the kind of realistic environment we would get when the model is running in production.</p>
			<p>Now that we have familiarized ourselves with the methods to explore and analyze different types of datasets, let's try this out on a real tabular dataset.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>Performing data analysis on a tabular dataset </h1>
			<p>If you haven't followed <a id="_idIndexMarker686"/>the steps in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>, to download the snapshot of the <em class="italic">Melbourne Housing dataset</em> from <strong class="bold">Kaggle</strong><strong class="bold"> </strong>(<a href="https://www.kaggle.com/dansbecker/melbourne-housing-snapshot">https://www.kaggle.com/dansbecker/melbourne-housing-snapshot</a>), please do this before continuing with this section. In the end, you should have the raw dataset file, <strong class="source-inline">melb_data.csv</strong>, in the <strong class="source-inline">mlfiles</strong> container in your storage account and have this connected to a datastore called <strong class="source-inline">mldemoblob</strong> in your Azure Machine Learning workspace.</p>
			<p>In the following sections, we<a id="_idIndexMarker687"/> will explore the dataset, do some basic statistical analysis, find missing values and outliers, find correlations between <a id="_idIndexMarker688"/>features, and take an initial measurement of feature importance while utilizing a random forest model, as we saw in the <em class="italic">Visualizing feature and label dependency for classification</em> section of this chapter. You can either create a new Jupyter notebook and follow along with this book or open the <strong class="source-inline">06_ dataprep_melbhousing.ipynb</strong> file in the GitHub repository for this chapter. </p>
			<p>Note that the steps we will perform now are not exhaustive. As shown on the web page for the dataset, we have 21 features to work with. So, to be thorough, you will have to analyze each. </p>
			<p>This section should give you a good understanding of the types of tasks you can perform, but we will leave a lot of questions open for you to find answers for. If you need some inspiration for that, have a look at this dataset on the Kaggle website. You will find notebooks from a lot of users trying to analyze this dataset.</p>
			<p>Finally, we will not completely transform the actual data at this point as we will come back to this problem in <a href="B17928_06_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering and Labeling</em>, where we will learn how to select features and create new ones based on the statistical analysis and knowledge we will gain through the upcoming process.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Initial exploration and cleansing of the Melbourne Housing dataset</h2>
			<p>In this section, we<a id="_idIndexMarker689"/> will load the data from a data store that is<a id="_idIndexMarker690"/> registered in Azure Machine Learning and look at its content. After that, we will start doing some basic cleaning regarding the raw data:</p>
			<ol>
				<li>Download the following packages through Python PIP either separately or using the requirements file you can find in this book's GitHub repository: <strong class="source-inline">pandas</strong>, <strong class="source-inline">seaborn</strong>, <strong class="source-inline">plotly</strong>, <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">missingno</strong>, <strong class="source-inline">umap-learn</strong>, and <strong class="source-inline">statsmodels</strong>.</li>
				<li>Create a new Jupyter notebook or follow along in the one mentioned previously.</li>
				<li>Connect to your ML workspace through the configuration file, as we learned previously.</li>
				<li> Use the following code to pull the dataset to your local computer:<p class="source-code">from azureml.core import Datastore, Dataset</p><p class="source-code">import pandas as pd</p><p class="source-code">import seaborn as sns</p><p class="source-code">import numpy as np</p><p class="source-code">import plotly.express as px</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code"># retrieve an existing datastore in the workspace by name</p><p class="source-code">datastore_name = '<strong class="bold">mldemoblob</strong>'</p><p class="source-code">datastore = Datastore.get(ws, datastore_name)</p><p class="source-code"># create a TabularDataset from the file path in datastore</p><p class="source-code">datastore_path = [(datastore, <strong class="bold">'melb_data.csv'</strong>)]</p><p class="source-code">tabdf = Dataset.Tabular.from_delimited_files</p><p class="source-code">       (path=datastore_path)</p></li>
			</ol>
			<p>Here, we're retrieving the data from your defined ML data store, <strong class="source-inline">yourname</strong>, and loading the dataset into<a id="_idIndexMarker691"/> a tabular dataset object. Adapt the path and name of the file in the<a id="_idIndexMarker692"/> second to last line, depending on your folder structure in your data store.</p>
			<ol>
				<li value="5">The methods that are available on a tabular dataset object are not as abundant as they are for a pandas DataFrame. So, let's transform it into a pandas DataFrame and have our first look at the data:<p class="source-code"># increase display of all columns of rows for pandas datasets</p><p class="source-code">pd.set_option('display.max_columns', None)</p><p class="source-code">pd.set_option('display.max_rows', None)</p><p class="source-code"># create pandas dataframe</p><p class="source-code">raw_df = tabdf.to_pandas_dataframe()</p><p class="source-code">raw_df.head()</p></li>
			</ol>
			<p>The <strong class="source-inline">pd.set_option()</strong> method gives <a id="_idIndexMarker693"/>you access to the general<a id="_idIndexMarker694"/> settings for pandas operations. In this case, we want all the columns and rows to be shown and not truncated in the visualization. You can set this to whatever value works for you.</p>
			<p>The <strong class="source-inline">head()</strong> function will give you a first look at the first five rows of the dataset. Have a look at them. </p>
			<p>You will see a bunch of features that make a lot of sense, such as <strong class="source-inline">Suburb</strong>, <strong class="source-inline">Address</strong>, and <strong class="source-inline">Bathroom</strong>. But some others might not be so clear, such as <strong class="source-inline">Type</strong>, <strong class="source-inline">Method</strong>, or <strong class="source-inline">Distance</strong>.</p>
			<p>Typically, as with any dataset, there is some form of data definition for the fields that are supplied with it. Have a look at the website of the datasets to find them.</p>
			<ol>
				<li value="6">Now that we've looked at the definition, let's look at the so-called shape of the datasets, which will show us how many columns (features and labels) and how many rows (samples) the dataset contains:<p class="source-code">raw_df.shape</p></li>
			</ol>
			<p>The preceding command shows us a dataset with 13,580 samples and 21 features/labels.</p>
			<ol>
				<li value="7">Finally, run the <a id="_idIndexMarker695"/>following code so that we can look<a id="_idIndexMarker696"/> at the number of unique values, the number of missing values, and the data type of each feature:<p class="source-code">stats = []</p><p class="source-code">for cl in raw_df.columns:</p><p class="source-code">    stats.append((cl,</p><p class="source-code">                  raw_df[cl].<strong class="bold">nunique()</strong>, </p><p class="source-code">                  raw_df[cl].<strong class="bold">isnull()</strong>.sum(),</p><p class="source-code">                  raw_df[cl].<strong class="bold">isnull()</strong>.sum() * 100 / </p><p class="source-code">                                   raw_df.shape[0],</p><p class="source-code">                  raw_df[cl].<strong class="bold">value_counts</strong>(</p><p class="source-code">                       normalize=True, </p><p class="source-code">                        dropna=False).values[0] * 100,</p><p class="source-code">                  raw_df[cl].<strong class="bold">dtype</strong>))</p><p class="source-code"># create new dataframe from stats   </p><p class="source-code">stats_df = pd.DataFrame(stats, columns=[</p><p class="source-code">              'Feature', </p><p class="source-code">              'Unique Values',</p><p class="source-code">              'Missing Values',</p><p class="source-code">              'Missing Values [%]', </p><p class="source-code">              'Values in the biggest category [%]', </p><p class="source-code">              'Datatype'])</p><p class="source-code">stats_df.<strong class="bold">sort_values</strong>('Missing Values [%]',</p><p class="source-code">                     ascending=False)</p></li>
			</ol>
			<p>After running the <a id="_idIndexMarker697"/>preceding code, you<a id="_idIndexMarker698"/> should see something similar to the following:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B17928_05_08.jpg" alt="Figure 5.8 – Melbourne Housing dataset feature overview " width="850" height="592"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Melbourne Housing dataset feature overview</p>
			<p>Looking at this table, we can make the following observations:</p>
			<ul>
				<li>Four features seem to have missing values (<strong class="bold">BuildingArea</strong>, <strong class="bold">YearBuilt</strong>, <strong class="bold">CouncilArea</strong>, and <strong class="bold">Car</strong>).</li>
				<li>A lot of numeric values (such as <strong class="bold">YearBuilt</strong>, <strong class="bold">Bathroom2</strong>, <strong class="bold">Bedroom</strong>, and <strong class="bold">Postcode</strong>) seem to be of the <strong class="source-inline">float64</strong> type. This is not necessarily a problem, but it's a waste of space since each probably fits into <strong class="source-inline">int8</strong>, <strong class="source-inline">int16</strong>, or <strong class="source-inline">int32</strong>.</li>
				<li>There are seven features<a id="_idIndexMarker699"/> of the <strong class="source-inline">object</strong> type, all of which are probably string values. We'll look at them in more detail shortly.</li>
				<li>There is a feature <a id="_idIndexMarker700"/>called <strong class="bold">Price</strong>, which is probably a good label/target for supervised learning, such as classification.</li>
				<li>There is a feature named <strong class="bold">Postcode</strong> and a feature named <strong class="bold">Suburb</strong>. We may not need both. Judging by the unique values, <strong class="bold">Suburb</strong> seems to be more granular.</li>
				<li>There is a feature called <strong class="bold">Address</strong> and a feature called <strong class="bold">SellerG</strong>. Even though the seller of a property may have some influence on the price, we can drop them for now for simplicity. The same goes for addresses as they are extremely precise. Nearly every sample has a unique address.</li>
			</ul>
			<p>By looking at the seven features of the <strong class="source-inline">object</strong> type, we can see the following:</p>
			<ul>
				<li><strong class="bold">Type</strong>: This has <strong class="bold">3</strong> distinct values; our data definition shows <strong class="bold">6</strong>. We need to check this discrepancy.</li>
				<li><strong class="bold">Method</strong>: This has <strong class="bold">5</strong> distinct values; our data definition shows <strong class="bold">11</strong>. We need to check this as well.</li>
				<li><strong class="bold">SellerG</strong>: This has <strong class="bold">268</strong> distinct seller names.</li>
				<li><strong class="bold">Address</strong>: This has <strong class="bold">13378</strong> distinct values, but we have <strong class="bold">13580</strong> samples, so there seem to be multiple places with the same address. Still, we have an extreme amount of variety here, which makes this feature quite unimportant.</li>
				<li><strong class="bold">Regionname</strong>: This has <strong class="bold">8</strong> distinct values – that is, the regions of Melbourne.</li>
				<li><strong class="bold">Suburb</strong>: This has <strong class="bold">314</strong> distinct values – that is, the suburbs of Melbourne.</li>
				<li><strong class="bold">CouncilArea</strong>: This has <strong class="bold">33</strong> distinct values and is the only categorical feature with missing values.</li>
			</ul>
			<p>At this point, we have <a id="_idIndexMarker701"/>found some interesting<a id="_idIndexMarker702"/> information and some leads that show us where we need to have a look in the next phase. For now, let's drill down into the content of the features and do some initial dataset cleaning.</p>
			<ol>
				<li value="8">Let's start by removing some of the not so important features:<p class="source-code">df = raw_df.<strong class="bold">drop</strong>(['Address', 'SellerG'],axis=1)</p></li>
			</ol>
			<p>As you can see, we stick with our original DataFrame, called <strong class="source-inline">raw_df</strong>, and create a new one called <strong class="source-inline">df</strong>. By doing this, we can add removed features at any time. Every row in a DataFrame has an index, so even if we filter out the rows, we can still match the original values.</p>
			<ol>
				<li value="9">Next, we will rename some columns to increase our understanding of them:<p class="source-code">df = df.<strong class="bold">rename</strong>(columns={'Bedroom2': 'Bedrooms', </p><p class="source-code">                        'Bathroom': 'Bathrooms',</p><p class="source-code">                        'Regionname': 'Region',</p><p class="source-code">                        'Car': 'Parking',</p><p class="source-code">                        'Propertycount':  </p><p class="source-code">                        'SuburbPropCount'})</p><p class="source-code">df.head()</p></li>
				<li>At this point, it might be a good idea to look for duplicates. Let's run the following code snippet to find duplicates:<p class="source-code">s = df.<strong class="bold">duplicated</strong>(keep = False)</p><p class="source-code">s = s[s == True]</p><p class="source-code">s</p></li>
			</ol>
			<p>Setting <strong class="source-inline">keep</strong> to <strong class="source-inline">False</strong> will show each row that has a duplicate. Here, we can see that two of the rows are the same. We can look at them by using the following command:</p>
			<p class="source-code">df.<strong class="bold">loc</strong>[[7769,7770]]</p>
			<p>As you can see, these denote the same entry. So, let's remove one of them using the following command:</p>
			<p class="source-code">df.<strong class="bold">drop</strong>([7769], <strong class="bold">inplace</strong>=True)</p>
			<p>As this is just one sample, we can drop it by its row index. Normally, operations like these just return a new <a id="_idIndexMarker703"/>DataFrame, but in a lot of operations, we can use an attribute called <strong class="source-inline">inplace</strong> to directly overwrite the current DataFrame.</p>
			<ol>
				<li value="11">Now, let's look at the <a id="_idIndexMarker704"/>categorical features that seem to have missing categories, starting with <strong class="source-inline">Method</strong>:<p class="source-code">df['Method'].<strong class="bold">unique()</strong></p></li>
			</ol>
			<p>The categories in our datasets are <strong class="source-inline">S</strong>, <strong class="source-inline">SP</strong>, <strong class="source-inline">PI</strong>, <strong class="source-inline">VB</strong>, and <strong class="source-inline">SA</strong>. Judging from the list in the data definition, we can see that the only entries in the dataset specify where the property was sold and where we know the selling price. Someone has already cleaned this for us.</p>
			<p>By looking at <strong class="source-inline">Type</strong>, we can see that single bedrooms, development sites, and other residential areas have been removed as well, leaving houses, units, and townhouses:</p>
			<p class="source-code">df['Type'].<strong class="bold">unique()</strong></p>
			<p>To make these entries a bit clearer, let's replace the single letters with a full name:</p>
			<p class="source-code">df = df.<strong class="bold">replace</strong>({'Type':  </p>
			<p class="source-code">               {'h':'house','u':'unit','t':'townhouse'}})</p>
			<p class="source-code">df = df.<strong class="bold">replace</strong>({'Method': {'S':'Property Sold',</p>
			<p class="source-code">                            'SP':'Property Sold Prior',</p>
			<p class="source-code">                            'PI':'Property Passed In',</p>
			<p class="source-code">                            'VB':'Vendor Bid', </p>
			<p class="source-code">                            'SA':'Sold After Auction'}})</p>
			<p class="source-code">df.head()</p>
			<ol>
				<li value="12">Now, let's concentrate <a id="_idIndexMarker705"/>on the categorical features<a id="_idIndexMarker706"/> that contain a lot of entries. The following code shows the list of unique values in the column:<p class="source-code">df['CouncilArea'].<strong class="bold">unique()</strong></p></li>
			</ol>
			<p>We will get the following result set:</p>
			<p class="source-code">array(['Yarra', 'Moonee Valley', 'Port Phillip', 'Darebin', 'Hobsons Bay', 'Stonnington', 'Boroondara', 'Monash', 'Glen Eira', 'Whitehorse', 'Maribyrnong', 'Bayside', 'Moreland', 'Manningham', 'Banyule', 'Melbourne', 'Kingston', 'Brimbank', 'Hume', <strong class="bold">None</strong>, 'Knox', 'Maroondah', 'Casey', 'Melton', 'Greater Dandenong', 'Nillumbik', 'Whittlesea', 'Frankston', 'Macedon Ranges', 'Yarra Ranges', 'Wyndham', 'Cardinia', <strong class="bold">'Unavailable'</strong>, 'Moorabool'], dtype=object)</p>
			<p>Here, we can see that there is a category called <strong class="source-inline">None</strong>, which contains our missing values, and a category called <strong class="source-inline">Unavailable</strong>. Otherwise, it seems like every other entry is very well defined, and there seem to be no duplicate entries with the same meaning; they only differ due to typing errors or spaces. Such errors are typically <a id="_idIndexMarker707"/>denoted as <strong class="bold">structural errors</strong>.</p>
			<p>By running the same<a id="_idIndexMarker708"/> command for the <strong class="source-inline">Suburb</strong> feature, we get a much larger result set. At this point, it gets very <a id="_idIndexMarker709"/>complicated to see structural errors, so we need to take a programmatic approach to check this category. Something such as pattern matching or fuzzy matching can be used here, but we will <a id="_idIndexMarker710"/>leave this out for now. Feel free to look up topics<a id="_idIndexMarker711"/> such as <strong class="bold">fuzzy matching</strong> and <strong class="bold">Levenshtein distance</strong>, which can be used to find groups of similar words in the result set.</p>
			<ol>
				<li value="13">Finally, we are left with one last question we had concerning the relationship between postcodes and suburbs and if we could get rid of one of them. So, let's see how many postcodes are targeting more than one suburb:<p class="source-code">postcodes_df = df.<strong class="bold">groupby</strong>(</p><p class="source-code">    'Postcode', as_index=False).Suburb.nunique()</p><p class="source-code">postcodes_df.columns = ['Postcode', </p><p class="source-code">                        '#Assigned Suburbs']</p><p class="source-code">postcodes_df.<strong class="bold">loc</strong>[postcodes_df['#Assigned Suburbs'] &gt; 1]</p></li>
			</ol>
			<p>Here, we created a new DataFrame that shows us the postcodes and the number of assigned suburbs. By searching for the ones that have been mapped to multiple suburbs, we <a id="_idIndexMarker712"/>can find the respective list. Let's count them:</p>
			<p class="source-code">postcodes_df.loc[postcodes_df['#Assigned Suburbs'] &gt; 1].<strong class="bold">count()</strong></p>
			<p>Here, we can see that 73 out of <a id="_idIndexMarker713"/>198 postcodes refer to multiple suburbs. Nevertheless, every suburb has a postcode, so let's stick with the suburbs and drop the postcodes from the DataFrame:</p>
			<p class="source-code">df = df.<strong class="bold">drop</strong>(['Postcode'],axis=1)</p>
			<p class="source-code">df.head()</p>
			<p>This already looks quite nice. As a final step, we could change the data type from <strong class="source-inline">float64</strong> to one of the integer types (<strong class="source-inline">int8</strong>, <strong class="source-inline">int16</strong>, <strong class="source-inline">int32</strong>, or <strong class="source-inline">int64</strong>), but we do not know enough about the spread of the data points yet and we cannot do this for columns with missing values. We'll come back to this later.</p>
			<p>So far, we have done some basic exploration and base pruning of our dataset. Now, let's learn more about statistics.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>Running statistical analysis on the dataset</h2>
			<p>It's time to look <a id="_idIndexMarker714"/>at the statistical properties of our numerical features. To do so, run the following code snippet:</p>
			<p class="source-code">dist_df = df.describe().T.apply(lambda s: s.apply(lambda x: format(x, 'g')))</p>
			<p class="source-code">dist_df</p>
			<p>Here, the <strong class="source-inline">describe()</strong> method will give you a table of typical statistical properties for the numeric features of the dataset. <strong class="source-inline">T</strong> will pivot the table, while the <strong class="source-inline">apply()</strong> and <strong class="source-inline">lambda()</strong> methods will help format the data points into normal numerical notations. Feel free to remove the <strong class="source-inline">apply</strong> methods and look at the difference.</p>
			<p>The result will show<a id="_idIndexMarker715"/> you some information, but we<a id="_idIndexMarker716"/> would like to add some more statistical values, including the <strong class="bold">skew</strong>, the <strong class="bold">mode</strong>, and the <strong class="bold">number of values</strong> in a feature<a id="_idIndexMarker717"/> that are equal to the mode, the maximum, and the minimum. With the following code, we can realize that:</p>
			<p class="source-code">from pandas.api.types import is_numeric_dtype</p>
			<p class="source-code">max_count=[]</p>
			<p class="source-code">min_count=[]</p>
			<p class="source-code">mode_count=[]</p>
			<p class="source-code">mode=[]</p>
			<p class="source-code">skew=[]</p>
			<p class="source-code">for cl in df.columns:</p>
			<p class="source-code">    if (<strong class="bold">is_numeric_dtype</strong>(df[cl])):</p>
			<p class="source-code">        max_count.append(df[cl].value_counts(</p>
			<p class="source-code">                         dropna=False).loc[df[cl].<strong class="bold">max()</strong>])</p>
			<p class="source-code">        min_count.append(df[cl].value_counts(</p>
			<p class="source-code">                         dropna=False).loc[df[cl].<strong class="bold">min()</strong>])</p>
			<p class="source-code">        mode_count.append(df[cl].value_counts(</p>
			<p class="source-code">                     dropna=False).loc[df[cl].<strong class="bold">mode()[0]</strong>])</p>
			<p class="source-code">        skew.append(df[cl].<strong class="bold">skew()</strong>)</p>
			<p class="source-code">        mode.append(int(df[cl].<strong class="bold">mode()[0]</strong>))</p>
			<p class="source-code">dist_df['mode'] = mode</p>
			<p class="source-code">dist_df['skew'] = skew</p>
			<p class="source-code">dist_df['#values(min)'] = min_count</p>
			<p class="source-code">dist_df['#values(max)'] = max_count</p>
			<p class="source-code">dist_df['#values(mode)'] = mode_count</p>
			<p class="source-code">dist_df</p>
			<p>Here, we are creating a <a id="_idIndexMarker718"/>bunch of lists and appending the<a id="_idIndexMarker719"/> calculated value for each column in our base DataFrame to each list. We are also adding a new column to our distribution DataFrame, <strong class="source-inline">dist_df</strong>, for each of the property lists that we calculated. To ease your understanding of the code, we used Python list objects here. You could shorten this code by using another pandas DataFrame, which we leave for you as an exercise.</p>
			<p>You should see an output similar to the following after running the preceding code:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17928_05_09.jpg" alt="" width="1184" height="357"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Statistical properties of the Melbourne Housing dataset</p>
			<p>Let's see what we can <a id="_idIndexMarker720"/>deduct for each feature by looking <a id="_idIndexMarker721"/>at this table:</p>
			<ul>
				<li><strong class="bold">Price</strong>: This is skewed to the right. Here, we will probably see a few high prices, which is not surprising. The highest house price is 9 million.</li>
				<li><strong class="bold">Distance</strong>: This is skewed to the right, probably due to one of the samples being 48.1km away from the CBD in Melbourne. Interestingly enough, there are <strong class="bold">6</strong> samples with <strong class="bold">0</strong> distance. Sometimes, 0 is a dummy value, so we should check those samples. Judging by the fact that mode <strong class="bold">11</strong> has been set <strong class="bold">739</strong> times, the distance might not be exactly the distance from the city center, but perhaps the mean distance of a suburb from the city center. We should figure that out as well.</li>
				<li><strong class="bold">Bedrooms</strong>: This is skewed to the right due to lots of bedrooms in some places. Curiously, there are <strong class="bold">16</strong> samples with <strong class="bold">0</strong> bedrooms, which needs to be verified.</li>
				<li><strong class="bold">Bathrooms</strong>: This is similar to the distribution of the <strong class="bold">Bedrooms</strong> feature, with <strong class="bold">34</strong> samples having <strong class="bold">0</strong> bathrooms, which again is curious.</li>
				<li><strong class="bold">Parking</strong>: This is similar to the distribution of the <strong class="bold">Bedrooms</strong> feature. There are <strong class="bold">1026</strong> samples with no parking spaces, which sounds reasonable.</li>
				<li><strong class="bold">Landsize</strong>: This is extremely skewed (<strong class="bold">95.24</strong>) to the right. The maximum value is <strong class="bold">433014</strong>. If we presume we're using square meters here, there are about 43 hectares of land. This isn't impossible, but this is clearly an outlier and would probably distort our modeling. </li>
				<li><strong class="bold">BuildingArea</strong>: This is extremely skewed to the right due to the maximum value of <strong class="bold">44515</strong> m<span class="superscript">2</span>. This sounds quite improbable, so we may want to remove this one. Also, there are <strong class="bold">17</strong> samples with <strong class="bold">0</strong> m<span class="superscript">2</span>, which needs to be checked.</li>
				<li><strong class="bold">YearBuilt</strong>: This is skewed to the left due to the one building being built in <strong class="bold">1196</strong>. We may want to discard that one.</li>
				<li><strong class="bold">Longitude/Latitude</strong>: These seem to be reasonably well distributed, but curiously with the <strong class="bold">17</strong> and <strong class="bold">21</strong> values being the same, respectively – specifically <strong class="bold">-37</strong> and <strong class="bold">144</strong>. This <a id="_idIndexMarker722"/>gives us some idea that the coordinates<a id="_idIndexMarker723"/> might not be as precise as we may think. </li>
				<li><strong class="bold">SuburbPropCount</strong>: This is slightly skewed to the right. We have to analyze how helpful this value is.</li>
			</ul>
			<p>Now, let's think about what relationships we would expect and have a look at these between features:</p>
			<ul>
				<li><strong class="bold">Rooms with Bathrooms/Bedrooms</strong>: If you have a look at the distribution for these, it becomes clear that we are not quite sure what <strong class="bold">Rooms</strong> means. The maximum for <strong class="bold">Rooms</strong> is <strong class="bold">10</strong>, while the maximum for <strong class="bold">Bedrooms</strong> is <strong class="bold">20</strong>. Looking at the data definition, we can see that <strong class="bold">Bedrooms</strong> was taken from a bunch of different sources, so we may have a discrepancy between those data points.  </li>
				<li><strong class="bold">BuildingArea with Rooms/Bathrooms/Bedrooms</strong>: We would expect a positive correlation of some sort, but we cannot judge this from the data at hand.</li>
			</ul>
			<p>As we can see, we can get some very good insights just from this table alone and have a good idea of what to look at next. We will check the <strong class="bold">Price</strong> and <strong class="bold">BuildingArea</strong> features for now, but in reality, we would have to follow all these avenues. Feel free to do this on your own and have a look at the supplied notebook to get some more ideas.</p>
			<p>First, let's look at the <strong class="bold">Price</strong> label. At this point, it is a good idea to visualize our distributions. To do that, you can either use the <strong class="source-inline">seaborn</strong> or <strong class="source-inline">plotly</strong> library. Read up on how they work <a id="_idIndexMarker724"/>and differ from each other. For simplicity, we will use <strong class="source-inline">plotly</strong> for now. Use the <a id="_idIndexMarker725"/>following code to plot a boxplot with a data points distribution shown next to it:</p>
			<p class="source-code">fig = px.box(df, x="Price",points="all")</p>
			<p class="source-code">fig.show()</p>
			<p>You should see the following graph:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17928_05_10.jpg" alt="Figure 5.10 – Boxplot for the Price target " width="1144" height="306"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – Boxplot for the Price target</p>
			<p>Hovering over the box will show you the <strong class="bold">upper and lower fence</strong> of the distribution. The upper fence is at 2.35 million. We can still see a lot of points above this. As we can ensure that these are valid <a id="_idIndexMarker726"/>prices, we should think of<a id="_idIndexMarker727"/> rescaling this target value. Let's calculate the <strong class="source-inline">log</strong> value of the <strong class="bold">Price</strong> vector and have a look again.</p>
			<p>To do this, let's add a new column to our DataFrame with the <strong class="source-inline">log</strong> value of <strong class="bold">Price</strong> and run the visualization again:</p>
			<p class="source-code">df["Price_log"] = <strong class="bold">np.log</strong>(df['Price']) </p>
			<p class="source-code">fig = px.box(df, x="Price_log",points="all")</p>
			<p class="source-code">fig.show()</p>
			<p>This will result in the following graph:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17928_05_11.jpg" alt="Figure 5.11 – Boxplot for the log (Price) target " width="1119" height="364"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – Boxplot for the log (Price) target</p>
			<p>Doing this seems to be a good idea as it's distributed better. Feel free to check the skew of this distribution.</p>
			<p>Now, let's look at the <strong class="bold">BuildingArea</strong> feature. Once again, let's create a boxplot using the following code:</p>
			<p class="source-code">fig = px.box(df, y="BuildingArea",points="all")</p>
			<p class="source-code">fig.show()</p>
			<p>This<a id="_idIndexMarker728"/> will result in<a id="_idIndexMarker729"/> the following graph:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17928_05_12.jpg" alt="Figure 5.12 – Boxplot of the BuildingArea feature " width="797" height="365"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12 – Boxplot of the BuildingArea feature</p>
			<p>We are greeted by a very distorted boxplot. Hovering over it, we can see <strong class="bold">upper fence</strong> at <strong class="bold">295</strong> m<span class="superscript">2</span>, while <strong class="bold">maximum</strong> is at <strong class="bold">44515</strong> m<span class="superscript">2</span>. There is one major outlier and a bunch of small ones.</p>
			<p>Let's look how many samples are above <strong class="bold">295</strong> with the following code:</p>
			<p class="source-code">df.loc[raw_df['BuildingArea'] &gt; 295]['BuildingArea'].count()</p>
			<p>The result still shows that there are <strong class="bold">353</strong> samples above this threshold. Looking at the boxplot, this may thin out rather quickly toward 2,000 m<span class="superscript">2</span>. So, let's check the result set for above 2,000 m<span class="superscript">2</span> with the following code:</p>
			<p class="source-code">df.loc[raw_df['BuildingArea'] &gt; 2000]</p>
			<p>This will give us the following output:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B17928_05_13.jpg" alt="Figure 5.13 – Top four samples by BuildingArea size " width="928" height="217"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13 – Top four samples by BuildingArea size</p>
			<p>As we can see, the largest property is 48.1 km away from the city center, so having a <strong class="bold">Landsize</strong> and <strong class="bold">BuildingArea</strong> in that range is feasible. However, if we want to understand <a id="_idIndexMarker730"/>house prices in Melbourne, this may not be that important. It is also in the Northern Victoria region and not in the<a id="_idIndexMarker731"/> metropolitan regions. We could go further here and look at the connection between these specific houses outside of the norm in conjunction with other features, but we will leave it at this for now.</p>
			<p>Let's drop the major outlier from our dataset using the following code:</p>
			<p class="source-code">df.drop([13245], <strong class="bold">inplace</strong>=True)</p>
			<p>As it just contains one sample, we can drop it by row ID. </p>
			<p>At this point, we could continue doing this kind of analysis with the rest of the features, but we will leave it as an exercise for you to have a deeper look at the rest of the features and their statistical dependencies. Now, let's continue by looking at what we would do after that.</p>
			<p>But before we continue, let's save our dataset to Azure Machine Learning using the following function:</p>
			<p class="source-code">Dataset.Tabular.register_pandas_dataframe(</p>
			<p class="source-code">        dataframe = df, </p>
			<p class="source-code">        target = datastore, </p>
			<p class="source-code">        name ='Melbourne Housing Dataset', </p>
			<p class="source-code">        description = 'Data Cleansing 1 - removed address,    </p>
			<p class="source-code">                       postcode, duplicates and outliers')</p>
			<p>We will continue<a id="_idIndexMarker732"/> to do so during this exercise to<a id="_idIndexMarker733"/> have different version at our disposal later.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>Finding and handling missing values</h2>
			<p>Our next order<a id="_idIndexMarker734"/> of business is to handle the missing values in the dataset. We can use a <a id="_idIndexMarker735"/>very nice extension called <strong class="source-inline">missingno</strong> to get some interesting visualizations of our missing values.</p>
			<p>But before that, let's run the following code to see what would happen if we removed all the samples with missing values:</p>
			<p class="source-code">df.<strong class="bold">dropna</strong>(how='<strong class="bold">any</strong>').shape</p>
			<p>As we can see, the resulting DataFrame would contain <strong class="bold">6196</strong> samples, which would be less than half of the dataset. So, it might be a good idea to handle missing values.</p>
			<p>Now, run the following code:</p>
			<p class="source-code">import missingno as msno</p>
			<p class="source-code"><strong class="bold">msno.matrix</strong>(df);</p>
			<p>This will result in the following output:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B17928_05_14.jpg" alt="Figure 5.14 – Structural visualization of the DataFrame and its missing values " width="1509" height="686"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14 – Structural visualization of the DataFrame and its missing values</p>
			<p>As we can see, the <strong class="bold">CouncilArea</strong> feature is only missing values in the latter samples of the DataFrame, <strong class="bold">Parking</strong> is only missing in a very small part in the latter samples, and <strong class="bold">BuildingArea</strong> and <strong class="bold">YearBuilt</strong> are missing throughout the DataFrame.</p>
			<p>As we've already learned, we<a id="_idIndexMarker736"/> can perform replacement by either inventing a <em class="italic">new category</em> for missing<a id="_idIndexMarker737"/> categorical data or replacing them with the <em class="italic">mean value</em> for missing continuous data.</p>
			<p>Let's start with the <strong class="bold">CouncilArea</strong> feature. As you may recall from our initial data exploration, there is a category called <strong class="source-inline">Unavailable</strong>, so let's look at the samples with this category by selecting any sample with that characteristic:</p>
			<p class="source-code">df.loc[df.CouncilArea.<strong class="bold">isin</strong>(['Unavailable'])]</p>
			<p>As we can see, there is only one entry with this category. It seems to be a valid entry; it is just missing the name of the council area. So, let's replace this entry and the missing values with a new category called <strong class="source-inline">Missing</strong> using the following code:</p>
			<p class="source-code">df['CouncilArea'].<strong class="bold">fillna</strong>(value = "Missing", <strong class="bold">inplace</strong> = True)</p>
			<p class="source-code">df['CouncilArea'].<strong class="bold">replace</strong>(to_replace="Unavailable", value="Missing", <strong class="bold">inplace</strong>=True)</p>
			<p>Checking the unique values in the feature after shows us that there are no values in the <strong class="source-inline">None</strong> or <strong class="source-inline">Unavailable</strong> categories anymore:</p>
			<p class="source-code">df['CouncilArea'].<strong class="bold">unique()</strong></p>
			<p>This is the simplest way to replace features. Since these are council areas of Melbourne and every house should be assigned to one, a better idea would be to find another dataset that matches <a id="_idIndexMarker738"/>suburbs or addresses to council areas and do a <a id="_idIndexMarker739"/>cross-reference. Feel free to search for one and do this.</p>
			<p>Continuing with the three continuous features, we can use the following code to replace any missing value with the mean of the column and check if there are still missing values left afterward:</p>
			<p class="source-code">BA_mean = df['BuildingArea'].<strong class="bold">mean()</strong></p>
			<p class="source-code">df['BuildingArea'].r<strong class="bold">eplace</strong>(to_replace=<strong class="bold">np.nan</strong>, value=BA_mean, <strong class="bold">inplace</strong>=True)</p>
			<p class="source-code">df['BuildingArea'].<strong class="bold">isnull()</strong>.sum()</p>
			<p>The result of the final command shows the mean value we filled, <strong class="bold">145.749</strong>. Adapt this code to do the same for <strong class="bold">YearBuilt</strong> and <strong class="bold">Parking</strong>. However, you may want to use the <em class="italic">median</em> rather than the <em class="italic">mean</em> value for these. </p>
			<p>For now, this solves the problem with missing values and is, statistically speaking, a reasonable approach. However, as we've discussed, this is one of the simplest ways to do this. A better way would be to find relationships between features and use them to fill in missing values. Instead of just using the mean of the entire dataset, we could concentrate on finding a subset of data that has similar characteristics as the sample with the missing value. For example, we could find a dependency between the number of parking spots on one side and the number of rooms in the house or the size of the house on the other side. Then, we could define a function that gives us a value for <strong class="bold">Parking</strong> depending on these<a id="_idIndexMarker740"/> other features.</p>
			<p>So, to handle missing values<a id="_idIndexMarker741"/> better, we need to figure out relationships, which we will have a look at in the next section.</p>
			<p>But before that, let's register this dataset again with this description: <strong class="source-inline">Data Cleansing 2 - replaced missing values</strong>.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Calculating correlations and feature importance</h2>
			<p>So far, we've looked at <a id="_idIndexMarker742"/>single features, their content, and their distribution. Now, let's<a id="_idIndexMarker743"/> look at the relationships between them.</p>
			<p>Use the following code to produce a correlation matrix between our features and targets:</p>
			<p class="source-code"># compute the correlation matrix</p>
			<p class="source-code">corr = <strong class="bold">df.corr()</strong></p>
			<p class="source-code"># define and create seaborn plot</p>
			<p class="source-code">mask = np.triu(np.ones_like(corr, dtype=np.bool))</p>
			<p class="source-code">f, ax = plt.subplots(figsize=(11, 9))</p>
			<p class="source-code">cmap = sns.diverging_palette(220, 10, as_cmap=True)</p>
			<p class="source-code"><strong class="bold">sns.heatmap</strong>(corr, mask=mask, cmap=cmap, vmax=.3,</p>
			<p class="source-code">            center=0, square=True, linewidths=.5, </p>
			<p class="source-code">            cbar_kws={"shrink": .5})</p>
			<p class="source-code">plt.<strong class="bold">show()</strong></p>
			<p>The resulting matrix will show you the correlation of 13 of our features, but not all of them. If you check the visible ones, you will see that we are missing everything of the <strong class="source-inline">object</strong> or <strong class="source-inline">datetime</strong> type.</p>
			<p>So, before we analyze the matrix, let's add the missing features by starting to carve out the left-over columns of the <strong class="source-inline">object</strong> type from our DataFrame:</p>
			<p class="source-code">obj_df = df.<strong class="bold">select_dtypes</strong>(include=['object']).copy()</p>
			<p class="source-code">obj_df.head()</p>
			<p>Here, we can see that the remaining columns are <strong class="bold">Suburb</strong>, <strong class="bold">Type</strong>, <strong class="bold">Method</strong>, <strong class="bold">CouncilArea</strong>, and <strong class="bold">Region</strong>. When you read through the list of pandas data types, you will find a type called <strong class="source-inline">category</strong>, which we will now convert our columns into:</p>
			<p class="source-code">for cl in obj_df.columns:</p>
			<p class="source-code">    obj_df[cl] = obj_df[cl].<strong class="bold">astype('category')</strong></p>
			<p class="source-code">obj_df.dtypes</p>
			<p>With that, we have created a <a id="_idIndexMarker744"/>DataFrame called <strong class="source-inline">obj_df</strong> with five features of the <strong class="source-inline">category</strong> type. Now, let's <a id="_idIndexMarker745"/>assign each category a numeric value. For this, we will use the <strong class="source-inline">cat.codes</strong> method and create five new columns in our DataFrame with <strong class="source-inline">_cat</strong> as the name extension:</p>
			<p class="source-code">for cl in obj_df.columns:</p>
			<p class="source-code">     obj_df[cl+"_cat"] = obj_df[cl].<strong class="bold">cat.codes</strong></p>
			<p class="source-code">obj_df.head()</p>
			<p>Perfect! We have created a DataFrame with encoded categories. We will combine these new features with our original DataFrame, <strong class="source-inline">df</strong>, into a new DataFrame called <strong class="source-inline">cont_df</strong>:</p>
			<p class="source-code"><strong class="bold">column_replacement</strong> = {'Type':'Type_cat','Suburb':'Suburb_cat','Method':'Method_cat','CouncilArea':'CouncilArea_cat','Region':'Region_cat'}</p>
			<p class="source-code">cont_df = <strong class="bold">df.copy()</strong></p>
			<p class="source-code">for key in <strong class="bold">column_replacement</strong>:</p>
			<p class="source-code">     cont_df[key] = obj_df[column_replacement[key]]</p>
			<p class="source-code">cont_df.dtypes</p>
			<p>The output of the preceding code shows the data types of all our columns in the new dataset. We can still see the <strong class="bold">Date</strong> column of the <strong class="source-inline">datetime</strong> type and some original columns that should be of the <strong class="source-inline">int</strong> type. Let's rectify this before creating the correlation matrix again.</p>
			<p>First, let's create a new column called <strong class="source-inline">Date_Epoch</strong> that consists of an integer that denotes the seconds from the epoch (<a href="https://docs.python.org/3/library/time.html">https://docs.python.org/3/library/time.html</a>) and drop the original <strong class="bold">Date</strong> column:</p>
			<p class="source-code">cont_df['Date_Epoch'] = cont_df['Date'].apply(lambda x: x.<strong class="bold">timestamp()</strong>)</p>
			<p class="source-code">cont_df.<strong class="bold">drop</strong>(['Date'], axis=1, <strong class="bold">inplace</strong>=True)</p>
			<p class="source-code">cont_df.dtypes</p>
			<p>We could also break <strong class="bold">Date</strong> apart into a <strong class="bold">Month</strong> column and a <strong class="bold">Year</strong> column, as they may have an impact. Feel free to add them as well.</p>
			<p>Now, let's convert all the <strong class="source-inline">float64</strong> columns into integers, except for the ones where float is correct:</p>
			<p class="source-code">for cl in cont_df.columns:</p>
			<p class="source-code">    if (cont_df[cl].dtype == np.float64 and cl <strong class="bold">not in</strong>    </p>
			<p class="source-code">                                   ['<strong class="bold">Lattitude</strong>', '<strong class="bold">Longtitude</strong>', </p>
			<p class="source-code">                                    '<strong class="bold">Price_log</strong>', '<strong class="bold">Distance</strong>']):</p>
			<p class="source-code">       cont_df[cl] = cont_df[cl].<strong class="bold">astype('int')</strong></p>
			<p class="source-code">cont_df.dtypes</p>
			<p>The preceding code <a id="_idIndexMarker746"/>shows that our DataFrame is now made up of only<a id="_idIndexMarker747"/> numerical data types in the most optimal size and format (some features only taking up 8-bits of memory per value).</p>
			<p>Now, it's time to run the correlation matrix again. Use the same code that we did previously – just replace <strong class="source-inline">df</strong> with our new <strong class="source-inline">cont_df</strong>. The result should look as follows:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B17928_05_15.jpg" alt="Figure 5.15 – Correlation matrix of all the features and their targets " width="683" height="598"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15 – Correlation matrix of all the features and their targets</p>
			<p>A <a id="_idIndexMarker748"/>strong red <a id="_idIndexMarker749"/>color denotes a <strong class="bold">positive correlation</strong>, while a strong blue color<a id="_idIndexMarker750"/> denotes a <strong class="bold">negative correlation</strong>. Based on this, we can<a id="_idIndexMarker751"/> conclude the following:</p>
			<ul>
				<li><strong class="bold">Rooms</strong> is strongly correlated with <strong class="bold">Price</strong>, <strong class="bold">Price_log</strong>, <strong class="bold">Distance</strong>, <strong class="bold">Bedrooms</strong>, <strong class="bold">Bathrooms</strong>, <strong class="bold">Parking</strong>, and <strong class="bold">BuildingArea</strong>.</li>
				<li><strong class="bold">Type</strong> is strongly correlated with <strong class="bold">Price</strong>, <strong class="bold">Price_log</strong>, <strong class="bold">Bedrooms</strong>, <strong class="bold">YearBuilt</strong>, and <strong class="bold">Rooms</strong>.</li>
				<li><strong class="bold">Price</strong> is strongly correlated with <strong class="bold">Rooms</strong>, <strong class="bold">Type</strong>, <strong class="bold">Bedrooms</strong>, <strong class="bold">Bathrooms</strong>, <strong class="bold">Parking</strong>, and <strong class="bold">BuildingArea</strong>.</li>
				<li><strong class="bold">Suburb</strong>, <strong class="bold">Method</strong>, <strong class="bold">Landsize</strong>, and <strong class="bold">SuburbPropCount</strong> don't seem to have too much influence in their current state on other features or the target.</li>
			</ul>
			<p>Looking at these results, they are not surprising. <strong class="bold">Suburb</strong> has too many categories to be precise for anything, <strong class="bold">Method</strong> shouldn't have too much influence either, <strong class="bold">Landsize</strong> is probably not the<a id="_idIndexMarker752"/> biggest factor, and <strong class="bold">SuburbPropCount</strong> may also have too much variety. Possible <a id="_idIndexMarker753"/>transformations could involve either dropping <strong class="bold">Suburb</strong> and <strong class="bold">SuburbPropCount</strong> or mapping them to a category with much less variety. </p>
			<p>Before we continue, let's register <strong class="source-inline">cont_df</strong> as a version of the dataset with the description: <strong class="source-inline">Data Cleansing 3 - all features converted to numerical values</strong>.</p>
			<p>As the final task, let's double-check what we've figured out so far by using an <strong class="bold">ensemble decision tree model</strong> to calculate the <strong class="bold">feature importance</strong> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</a>). You can find the code for creating the random forest and visualizing the results at the end of the <strong class="source-inline">06_dataprep_melbhousing.ipynb</strong> file. There, you will see that we calculated the feature importance for the <strong class="bold">Price</strong> and <strong class="bold">Price_log</strong> targets. The results for both are shown here:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B17928_05_16.jpg" alt="Figure 5.16 – Feature importance for Price (left) and Price_log (right) " width="770" height="343"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16 – Feature importance for Price (left) and Price_log (right)</p>
			<p>As we can see, the type of the property clearly influences its price. This influence might not look that massive, but be aware, we are looking at logarithmical house prices.</p>
			<p>What we've learned so far matches these results. Looking at the difference between the graphs, we can see that <a id="_idIndexMarker754"/>adding <strong class="bold">logarithmic scaling</strong> to our target variable has strengthened the most influential feature. The <strong class="bold">Type</strong> feature seems to have a strong influence on <a id="_idIndexMarker755"/>our target.</p>
			<p>Let's end this exercise<a id="_idIndexMarker756"/> by looking at this relationship using the following code:</p>
			<p class="source-code">fig = px.<strong class="bold">box</strong>(df, y="Price_log",x='Type', color = 'Type', </p>
			<p class="source-code">                 category_orders={"Type": ["house",</p>
			<p class="source-code">                                  "townhouse", "unit"]})</p>
			<p class="source-code">fig.show()</p>
			<p>The results of this are as follows:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B17928_05_17.jpg" alt="Figure 5.17 – Correlation between Type and Price_log " width="996" height="368"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.17 – Correlation between Type and Price_log</p>
			<p>With that, we've completed this exercise. We were able to clean up our dataset, find some very good initial insights, and find a very strong correlation between our target variable and one of the features.</p>
			<p>There are a lot of open questions left and we are still at the beginning of fully understanding this dataset. As an example, besides the <strong class="bold">Price</strong> target, we did not look at scaling or normalizing features, another possible requirement for certain algorithms. </p>
			<p>We will continue working with this dataset in <a href="B17928_06_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering and Labeling</em>. Until then, feel free to drill down into the secrets of this dataset or try to use your newfound knowledge on a different dataset.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>Tracking figures from exploration in Azure Machine Learning</h2>
			<p>During our data exploration, we <a id="_idIndexMarker757"/>created a lot of different plots and visuals. Let's learn how to track them with Azure Machine Learning so that they are not just living in our Jupyter notebook.</p>
			<p>In <a href="B17928_03_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing the Azure Machine Learning Workspace</em>, we learned how to track metrics and files for ML experiments using Azure Machine Learning. Other important outputs of your data transformation and ML scripts are visualizations, figures of data distributions, insights about models, and the results. Therefore, Azure Machine Learning provides a similar way to track metrics for images, figures, and <strong class="source-inline">matplotlib</strong> references.</p>
			<p>Let's imagine that we created a <strong class="source-inline">pairplot</strong> of the popular <em class="italic">Iris Flower dataset</em> (<a href="https://archive.ics.uci.edu/ml/datasets/iris">https://archive.ics.uci.edu/ml/datasets/iris</a>) using the following code:</p>
			<p class="source-code">import seaborn as sns</p>
			<p class="source-code">sns.set(style="ticks")</p>
			<p class="source-code">df = sns.load_dataset("iris")</p>
			<p class="source-code">sns.<strong class="bold">pairplot</strong>(df, hue="species")</p>
			<p>With a few lines of code, we can track all the <strong class="source-inline">matplotlib</strong> figures and attach them to our experimentation run. To do so, we only have to pass the <strong class="source-inline">matplotlib</strong> reference to the <strong class="source-inline">run.log_image()</strong> method and give it an appropriate name. The following code shows what this would look like in an experiment:</p>
			<p class="source-code">with exp.start_logging() as run:</p>
			<p class="source-code">  fig = sns.pairplot(df, hue="species")</p>
			<p class="source-code">  <strong class="bold">run.log_image</strong>("pairplot", plot=fig)</p>
			<p>Now, this is the amazing part. By calling the function with the <strong class="source-inline">matplotlib</strong> reference, Azure Machine Learning will render the figure, save it, and attach it to the experiment run. The following screenshot shows Azure Machine Learning studio with the <strong class="bold">Images</strong> tab open. Here, you can see the <strong class="source-inline">pairplot</strong> image that we just created and registered attached to the run:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B17928_05_18.jpg" alt="Figure 5.18 – Pairplot tracked and shown in Azure Machine Learning studio " width="1128" height="648"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.18 – Pairplot tracked and shown in Azure Machine Learning studio</p>
			<p>It seems like a tiny feature, but it is insanely useful in real-world experimentation. Get used to automatically generating plots of your data, models, and results and attaching them to your run. Whenever you are going through your experiments later, you'll have all the visualizations already attached to your run, metrics, and configuration.</p>
			<p>Think about <a id="_idIndexMarker758"/>storing regression plots when you're training regression models, and confusion matrices and ROC curves when training classification models. Store your feature importance when you're training tree-based ensembles and activations for neural networks. You can implement this once and add a ton of useful information to your data and ML pipelines.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">When you're using AutoML and HyperDrive to optimize parameters, pre-processing, feature engineering, and model selection, you will get a ton of generated visualizations out of the box to help you understand the data, model, and results.</p>
			<p>Now that we know how to store visualizations in the Azure Machine Learning workspace, let's learn how to create visuals denoting high-dimensional data. </p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Understanding dimensional reduction techniques</h1>
			<p>We looked at a lot of ways to <a id="_idIndexMarker759"/>visualize data in the previous sections, but high-dimensional data cannot be easily and accurately visualized in two dimensions. To achieve this, we need a projection of some sort or an embedding technique to embed the feature space in two dimensions. There are many linear and non-linear embedding techniques that you can use to produce two-dimensional projections of data. The following<a id="_idIndexMarker760"/> are the most<a id="_idIndexMarker761"/> common ones:</p>
			<ul>
				<li><strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>)</li>
				<li><strong class="bold">Linear Discriminant Analysis</strong> (<strong class="bold">LDA</strong>)</li>
				<li><strong class="bold">t-Distributed Stochastic Neighbor Embedding</strong> (<strong class="bold">t-SNE</strong>)</li>
				<li><strong class="bold">Uniform Manifold Approximation and Projection</strong> (<strong class="bold">UMAP</strong>)</li>
			</ul>
			<p>The following<a id="_idIndexMarker762"/> diagram shows the <strong class="bold">LDA</strong> and <strong class="bold">t-SNE</strong> embeddings for the 13-dimensional <em class="italic">UCI Wine Recognition dataset</em> (<a href="https://archive.ics.uci.edu/ml/datasets/wine">https://archive.ics.uci.edu/ml/datasets/wine</a>). In the <strong class="bold">LDA</strong> embedding, we can see that all the classes should be linearly separable. That's a lot we have learned from using two lines of code to plot the embedding before we have even started the model selection or training process:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B17928_05_19.jpg" alt="Figure 5.19 – Supervised LDA (left) versus unsupervised t-SNE (right)  " width="1200" height="417"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19 – Supervised LDA (left) versus unsupervised t-SNE (right) </p>
			<p>Both the <strong class="bold">LDA</strong> and <strong class="bold">t-SNE</strong> embeddings are extremely helpful for judging the separability of the individual classes and hence the difficulty of your classification task. It's always good to assess how well a particular model will perform on your data before you start selecting and training a specific algorithm.</p>
			<p>A great way to get quick insights and a good understanding of your data is to visualize it. This will also help you identify clusters in your data and irregularities and anomalies – all things that need to be considered in all further data processing. But how can you visualize a dataset with 10, 100, or 1,000 feature dimensions? And where should you keep the analysis?</p>
			<p>In this section, we<a id="_idIndexMarker763"/> will answer all these questions. First, we will look into the <em class="italic">linear </em>embedding<a id="_idIndexMarker764"/> techniques – <strong class="bold">PCA</strong>, an <em class="italic">unsupervised</em> technique, and <strong class="bold">LDA</strong>, a <em class="italic">supervised</em> technique. Then, we<a id="_idIndexMarker765"/> will compare both techniques to two popular <em class="italic">unsupervised non-linear</em> embedding techniques, <strong class="bold">t-SNE</strong> and <strong class="bold">UMAP</strong>, the latter of which is a generalized and faster version of t-SNE. Having those four techniques in your toolchain will help you understand datasets and create meaningful visualizations. We will run all these techniques against datasets of increasing complexity, namely the following:</p>
			<ul>
				<li><strong class="bold">The Iris Flower dataset</strong>: This dataset <a id="_idIndexMarker766"/>contains three classes and four feature dimensions.</li>
				<li><strong class="bold">The UCI Wine Recognition dataset</strong>: This dataset <a id="_idIndexMarker767"/>contains three classes and thirteen feature dimensions.</li>
				<li><strong class="bold">The MNIST Handwritten Digits dataset</strong>: This <a id="_idIndexMarker768"/>dataset contains 10 classes and 784 feature dimensions (28 x 28-pixel images).</li>
			</ul>
			<p>The code to generate the embeddings in this section has been omitted for brevity but can be found in the <strong class="source-inline">07_dimensionality_reduction.ipynb</strong> file in this book's GitHub repository. </p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>Unsupervised dimensional reduction using PCA</h2>
			<p>The <a id="_idIndexMarker769"/>most popular linear <a id="_idIndexMarker770"/>dimensionality reduction technique is PCA. This is because, since it is an unsupervised method, it doesn't need any training labels. PCA embedding linearly transforms a dataset so that the resulting projection is <a id="_idIndexMarker771"/>uncorrelated. The axes of this project are called <strong class="bold">principal components</strong> and are computed in such a way that each has the next highest variance.</p>
			<p>The principal components are the directions of the highest variance in the data. This means that the principal components or Eigenvectors describe the strongest direction of the dataset, and the<a id="_idIndexMarker772"/> next dimension shows the orthogonal difference from the previous direction. In NLP, the main components correspond with high-level concepts – in recommendation engines, they correspond with user or item traits.</p>
			<p>PCA can be computed <a id="_idIndexMarker773"/>as the Eigenvalue decomposition of the covariance or correlation matrix, or on a non-square matrix, by using SVD. PCA and Eigenvalue decomposition are often used as data experimentation steps for visualization, whereas SVD is often used as dimensionality reduction for sparse datasets; for example, a Bag-of-Words model for NLP. We will see how SVD is used in practice in <a href="B17928_07_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 7</em></a>, <em class="italic">Advanced Feature Extraction with NLP</em>.</p>
			<p>An embedding technique can be used as a form of dimensionality reduction by simply removing all but the first <em class="italic">x</em> components because these first – and largest – components explain a certain percentage of the variance of the dataset. Hence, we must remove data with low variance to receive a lower-dimensional dataset.</p>
			<p>To visualize data after performing PCA in two dimensions (or after performing any embedding technique) is to visualize the first two components of the transformed dataset – the two largest principal components. The resulting data is rotated along the axis – the principal components – scaled, and centered at zero. The following diagram shows the results of PCA for the first two datasets. As you can see, all the visualizations have the highest variance projected across the <em class="italic">x</em> axis, the second-highest across the <em class="italic">y</em> axis, and so on:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B17928_05_20.jpg" alt="Figure 5.20 – PCA for the Iris Flower dataset (left) and the UCI Wine Recognition dataset (right) " width="1257" height="439"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.20 – PCA for the Iris Flower dataset (left) and the UCI Wine Recognition dataset (right)</p>
			<p>Here, we should acknowledge<a id="_idIndexMarker774"/> that it is a great first step that we can show all these three datasets in only two dimensions, and immediately recognize clusters. </p>
			<p>By projecting the data across <a id="_idIndexMarker775"/>the first two principal components and looking at the Iris Flower dataset on the left, we can see that all the clusters look linearly separable (in two dimensions). However, when we look at the UCI Wine Recognition dataset on the right, we can already tell that the clusters are not extremely obvious anymore. Now, 13 feature dimensions are projected along with the first two principal components, with the highest variance along the <em class="italic">x</em> axis and the second-highest variance along the <em class="italic">y</em> axis. In PCA, it's typical for the cluster's shape to be aligned with the <em class="italic">x</em> axis because this is how the algorithm works.</p>
			<p>Now, let's run PCA on the most complex dataset – the MNIST Handwritten Digits dataset. The result of doing so can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B17928_05_21.jpg" alt="Figure 5.21 – PCA results for the MNIST Handwritten Digits dataset " width="497" height="404"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.21 – PCA results for the MNIST Handwritten Digits dataset</p>
			<p>When we look at the much more complex embedding of the MNIST Handwritten Digits dataset, we cannot see many clusters besides maybe the cluster for <strong class="bold">0</strong> at the top. The data is centered <a id="_idIndexMarker776"/>across zero and scaled to a range <a id="_idIndexMarker777"/>between <strong class="bold">-30</strong> and <strong class="bold">30</strong>. Hence, we can already tell the downsides of PCA – it doesn't consider any target labels, which means it doesn't optimize for separable classes.</p>
			<p>In the next section, we'll look at a technique that takes target labels into account.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>Supervised dimensional reduction using LDA</h2>
			<p>In LDA, we linearly transform<a id="_idIndexMarker778"/> the input data – similar to PCA – and optimize the transformation in such a way that the resulting directions have the <a id="_idIndexMarker779"/>highest inter-cluster variance and the lowest intra-cluster variance. This means that the optimization tries to keep samples of the same cluster close to the cluster's mean, all while trying to keep the cluster's means as far apart as possible.</p>
			<p>In LDA, we also receive a linear weighted set of directions as a resulting transformation. The data is centered around 0 and the directions are ordered by their highest inter-cluster variance. Hence, in that sense, LDA is like PCA in that it takes target labels into account. Both LDA and PCA have no real tuning knobs, besides the number of components we want to keep in the projection and probably a random initialization seed.</p>
			<p>The following diagram shows the results of performing LDA on our first two datasets:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B17928_05_22.jpg" alt="Figure 5.22 – LDA results for the Iris Flower dataset (left) and the UCI Wine Recognition dataset (right) " width="1212" height="429"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.22 – LDA results for the Iris Flower dataset (left) and the UCI Wine Recognition dataset (right)</p>
			<p>Here, we can see that the data is <a id="_idIndexMarker780"/>transformed into two dimensions in such a way that the cluster's means are the farthest apart from each <a id="_idIndexMarker781"/>other across the <em class="italic">x</em> axis. We can see the same effect for both the Iris Flower and UCI Wine Recognition datasets. Another interesting fact that we can observe in both embeddings is that the data also becomes linearly separable. We can almost put two straight lines in both visualizations to separate the clusters from each other.</p>
			<p>The LDA embedding for both datasets looks quite good in terms of how the data is separated by classes. From this, we can be confident that a linear classifier for both datasets should achieve great performance – for example, above 95% accuracy. While this might be just a ballpark estimate, we already know what to expect from a linear classifier with minimal analysis and data preprocessing. </p>
			<p>Unfortunately, most real-world embeddings look a lot more like the one shown in the following diagram, where we used LDA on the final dataset. This is because most real-world datasets often have above 10 or even 100 feature dimensions:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B17928_05_23.jpg" alt="Figure 5.23 – LDA results for MNIST Handwritten Digits dataset " width="490" height="404"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.23 – LDA results for MNIST Handwritten Digits dataset</p>
			<p>Here, we can also <a id="_idIndexMarker782"/>see a good separation of the cluster containing the <strong class="bold">0</strong> digits at the bottom and the two clusters of fours and sixes on the<a id="_idIndexMarker783"/> left-hand side. All the other clusters are drawn on top of each other and don't look to be linearly separable. </p>
			<p>Hence, we can tell that a linear classifier won't perform well and will have maybe only around 30% accuracy – which is still a lot better than if we were to do this randomly. However, we can't tell what performance we would expect from a complex non-linear model – not even a non-parametric model such as a decision tree-based ensemble classifier.</p>
			<p>As we can see, LDA performs a lot better than PCA as it takes class labels into account. Therefore, labeling data is something to consider when you're optimizing results. We will learn how to do efficient labeling in <a href="B17928_06_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering and Labeling</em>.</p>
			<p>LDA is a great embedding technique for<a id="_idIndexMarker784"/> linearly separable datasets with less than 100 dimensions and categorical target variables. An extension of LDA is <strong class="bold">Quadratic Discriminant Analysis</strong> (<strong class="bold">QDA</strong>), which performs a non-linear projection using combinations of two variables. If you are dealing with continuous target variables, you can use a<a id="_idIndexMarker785"/> very similar technique called <strong class="bold">analysis of variance</strong> (<strong class="bold">ANOVA</strong>) to model the variance between<a id="_idIndexMarker786"/> clusters. The result of ANOVA<a id="_idIndexMarker787"/> transformations indicates whether the variance in the dataset is attributed to a combination of the variance of different components.</p>
			<p>As we have seen neither PCA nor LDA performed well when separating high-dimensional data such as image data. In the Handwritten Digits dataset, we are dealing with <em class="italic">only</em> 784 feature dimensions from 28 x 28-pixel images. Imagine that your dataset consists of 1,024 x 1,024-pixel images – your dataset would have more than 1 million dimensions. Hence, we need a better embedding technique for very high-dimensional datasets.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Non-linear dimensional reduction using t-SNE</h2>
			<p>Projecting<a id="_idIndexMarker788"/> high-dimensional datasets into two or three dimensions was extremely difficult and cumbersome a couple of years <a id="_idIndexMarker789"/>ago. If you wanted to visualize image data on a two-dimensional graph, you could use any of the previously discussed techniques – if they could compute a result – or try exotic embeddings such as self-organizing maps.</p>
			<p>Even though t-SNE was released in a paper in 2008 by Laurence van der Maaten and Geoffrey Hinton (<a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf</a>), it took until 2012 for someone to apply it to a major dataset. It was used by the team ranked first in the Merck Viz Kaggle competition – a rather unconventional way to apply a great embedding algorithm for the first time. However, since the end of that competition, t-SNE has been used regularly in other Kaggle competitions and by large companies for embedding high-dimensional datasets with great success.</p>
			<p>t-SNE projects high-dimensional features into a two- or three-dimensional space while minimizing the difference of similar points in high-and low-dimensional space. Hence, high-dimensional feature vectors that are close to each other are likely to be close to each other in the two-dimensional embedding.</p>
			<p>The following diagram <a id="_idIndexMarker790"/>shows t-SNE applied to the Iris <a id="_idIndexMarker791"/>Flower and UCI Wine Recognition datasets. As we can see, the complex non-linear embedding doesn't perform a lot better than the simple PCA or LDA techniques. However, its real power is highlighted in very large and high-dimensional datasets that contain up to 30 million observations of thousands of feature dimensions:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B17928_05_24.jpg" alt="Figure 5.24 – The t-SNE results for the Iris Flower dataset (left) and  the UCI Wine Recognition dataset (right) " width="1268" height="433"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.24 – The t-SNE results for the Iris Flower dataset (left) and the UCI Wine Recognition dataset (right)</p>
			<p>In the following diagram, you can see how t-SNE performs against the MNIST dataset:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B17928_05_25.jpg" alt="Figure 5.25 – The t-SNE results for the MNIST Handwritten Digits dataset " width="497" height="412"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25 – The t-SNE results for the MNIST Handwritten Digits dataset</p>
			<p>As we can see, t-SNE<a id="_idIndexMarker792"/> performs a lot better on the MNIST<a id="_idIndexMarker793"/> dataset and effortlessly separates the clusters of 10 handwritten digits. This suggests that 99% accuracy might be possible.</p>
			<p>What is beautiful with this type of visualization is not only that we can see that the data is separable, but we can also imagine what the confusion matrix will look like when a classifier gets trained on the data, simply by looking at the preceding visualization. Here are some observations about the data that we can infer from just looking at the embedding:</p>
			<p>Replace this bullet list with the following list:</p>
			<ul>
				<li>There are three clusters containing samples of digit 1, where one cluster is further away from the mean.</li>
				<li>There are three clusters containing samples of digit 9, where in a couple of cases, some of these samples are very close to the clusters for digit 1 and digit 7 samples.</li>
				<li>There is a cluster containing samples of digit 3 in the middle, that are close to the cluster for digit 8 samples.</li>
				<li>There is a small cluster containing samples of digit 2, that are close to the cluster for digit 8 samples.</li>
				<li>The clusters containing samples for digits 3 and 9 are quite close to each other, so they may look similar.</li>
				<li>The clusters containing samples for digits 0, 4 and 6 have a very good distance from other clusters, suggesting that they are quite separable.</li>
			</ul>
			<p>These are brilliant insights since you know what to expect and what to look for in your data when you're manually <a id="_idIndexMarker794"/>exploring samples. It also helps you tune your feature engineering to, for <a id="_idIndexMarker795"/>example, try to differentiate the images for the <strong class="bold">1</strong>, <strong class="bold">7</strong>, and <strong class="bold">9</strong> digits as they will lead to the most misclassifications later in modeling.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Generalizing t-SNE with UMAP</h2>
			<p>UMAP for dimension<a id="_idIndexMarker796"/> reduction is an algorithm for general-purpose manifold learning and dimension reduction. It is a generalization of t-SNE that's based on Riemannian geometry <a id="_idIndexMarker797"/>and algebraic topology.</p>
			<p>In general, UMAP provides similar results to t-SNE with a topological approach, better scalability of feature dimensions, and faster computation at runtime. Since it is faster and performs slightly better in terms of topological structure, it is quickly gaining popularity.</p>
			<p>If we look at the embeddings for the Iris Flower and UCI Wine Recognition datasets again, we will see a similar effect to what we saw with t-SNE. The results are shown in the following diagram:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B17928_05_26.jpg" alt="Figure 5.26 – UMAP results for the Iris Flower dataset (left) and  the UCI Wine Recognition dataset (right) " width="1282" height="447"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26 – UMAP results for the Iris Flower dataset (left) and the UCI Wine Recognition dataset (right)</p>
			<p>The resulting embeddings look reasonable but they aren't better than the linearly separable results of LDA. However, we can't measure computational performance by only comparing the results, and that's where UMAP shines.</p>
			<p>When it comes to <a id="_idIndexMarker798"/>higher-dimensional data, such as the MNIST Handwritten Digits dataset, UMAP performs <a id="_idIndexMarker799"/>exceptionally well as a two-dimensional embedding technique. We can see the results for UMAP on the MNIST Handwritten Digits dataset in the following diagram:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B17928_05_27.jpg" alt="Figure 5.26 – The UMAP results for the MNIST Handwritten Digits dataset " width="500" height="411"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26 – The UMAP results for the MNIST Handwritten Digits dataset</p>
			<p>As we can see, UMAP reduces clusters to completely separable entities in the embedding, with minimal overlaps and a great distance between the clusters themselves. Making similar observations to what we made previously, for example, concerning the clusters of the <strong class="bold">1</strong> and <strong class="bold">9</strong> digits, are still possible, but the clusters look a lot more separable.</p>
			<p>From these data experimentation and visualization techniques, we would like you to take away the following key points:</p>
			<ul>
				<li>Perform PCA to try to analyze Eigenvectors</li>
				<li>Perform LDA or ANOVA to understand the variance of your data</li>
				<li>Perform t-SNE or UMAP embedding if you have complex high-dimensional data</li>
			</ul>
			<p>Armed<a id="_idIndexMarker800"/> with this knowledge, we <a id="_idIndexMarker801"/>can dive right into feature engineering as we know which data samples will be easy to handle and which samples will cause high misclassification rates in production.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Summary</h1>
			<p>In the first two parts of this chapter, you learned what techniques exist for you to explore and statistically analyze raw datasets and how to use them hands-on on a real-life dataset.</p>
			<p>After that, you learned about the dimensionality reduction techniques you can use to visualize high-dimensional datasets. There, you learned about techniques that are extremely useful for you to understand your data, its principal components, discriminant directions, and separability.</p>
			<p>Furthermore, everything you have learned in this chapter can be performed on a compute cluster in your Azure Machine Learning workspace, through which you can keep track of all the figures and outputs that are generated. </p>
			<p>In the next chapter, using all the knowledge you've gained so far, you will dive into the topic of feature engineering, where you learn how to select and transform features in datasets to prepare them for ML training. In addition, you will have a closer look at labeling and how Azure Machine Learning can help with this tedious task.</p>
		</div>
	</div>
</div>
</body></html>