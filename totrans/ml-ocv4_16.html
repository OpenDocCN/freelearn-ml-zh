<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conclusion</h1>
                </header>
            
            <article>
                
<p>Congratulations! You have just made a big step toward becoming a machine learning practitioner. Not only are you familiar with a wide variety of fundamental machine learning algorithms, but you also know how to apply them to both supervised and unsupervised learning problems. Moreover, you were introduced to a new and exciting topic, OpenVINO Toolkit. In the previous chapter, we learned how to install OpenVINO and run an interactive face detection and image classification demo, among others. I am sure you have enjoyed learning about those topics.</p>
<p>Before we part ways, I want to give you some final words of advice, point you toward some additional resources, and give you some suggestions on how you can further improve your machine learning and data science skills. <span>In this chapter, we will learn how to approach a machine learning problem and build our own estimator. We will learn how to write our own OpenCV-based classifier in C++ and a scikit-learn-based classifier in Python. </span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Approaching a machine learning problem</li>
<li>Writing your own OpenCV-based classifier in C++</li>
<li>Writing your own <span>scikit-learn-based classifier in Python</span></li>
<li>Where to go from here</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can refer to the code for this chapter from the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter13">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter13</a>.</p>
<p>Here is a summary of the software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>You will need Python version 3.6 (any Python version 3.x will be fine).</li>
<li>You will need Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any OS—macOS, Windows, and Linux-based OSes along with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided along with this book.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Approaching a machine learning problem</h1>
                </header>
            
            <article>
                
<p>When you see a new machine learning problem in the wild, you might be tempted to jump ahead and throw your favorite algorithm at the problem—perhaps the one you understood best or had the most fun implementing. <span>But knowing beforehand which algorithm will perform best on your specific problem is not often possible.</span></p>
<p>Instead, you need to take a step back and look at the bigger picture. Before you get in too deep, you will want to define the actual problem you are trying to solve. For example, do you already have a specific goal in mind, or are you just looking to do some exploratory analysis and find something interesting in the data? Often, you will start with a general goal, such as detecting spam email messages, making movie recommendations, or automatically tagging your friends in pictures uploaded to a social media platform. However, as we have seen throughout this book, there are often several ways to solve a problem. For example, we recognized handwritten digits using logistic regression, k-means clustering, and deep learning. Defining the problem will help you to ask the right questions and make the right choices along the way.</p>
<p>As a rule of thumb, you can use the following five-step procedure to approach machine learning problems in the wild:</p>
<ol>
<li><strong>Categorize the problem</strong>: This is a two-step process:
<ul>
<li><strong>Categorize by input</strong>: Simply speaking, i<span>f you have labeled data, it's a supervised learning problem. If you have unlabeled data and want to find structure, it's an unsupervised learning problem. If you want to optimize an objective function by interacting with an environment, it's a reinforcement learning problem.</span></li>
<li><strong>Categorize by output</strong>: <span>If the output of your model is a number, it's a regression problem. If the output of your model is a class (or category), it's a classification problem. If the output of your model is a set of input groups, it's a clustering problem.</span></li>
</ul>
</li>
<li><strong>Find the available algorithms</strong>: Now that you have categorized the problem, you can identify the algorithms that are applicable and practical to implement using the tools at our disposal. Microsoft has created <span>a handy algorithm cheat sheet that shows which algorithms can be used for which category of problems. Although the cheat sheet is tailored toward <strong>Microsoft Azure</strong>, you might find it generally helpful.</span></li>
</ol>
<div class="packt_infobox">The machine learning algorithm cheat sheet PDF (by Microsoft Azure) can be downloaded from <a href="http://aka.ms/MLCheatSheet">http://aka.ms/MLCheatSheet</a>.</div>
<ol start="3">
<li><strong>Implement all of the applicable algorithms</strong> (<strong>prototyping</strong>): <span>For any given problem, there are usually a handful of candidate algorithms that could do the job. So, how do you know which one to pick? Often, the answer to this problem is not straightforward, so you have to resort to trial and error.</span> Prototyping is best done in two steps:
<ol>
<li>You should aim for a quick and dirty implementation of several algorithms with minimal feature engineering. At this stage, you should mainly be interested in seeing which algorithm behaves better at a coarse scale. This step is a bit like hiring: you're looking for any reason to shorten your list of candidate algorithms. Once you have reduced the list to a few candidate algorithms, the real prototyping begins.</li>
<li>Ideally, you would want to set up a machine learning pipeline that compares the performance of each algorithm on the dataset using a set of carefully selected evaluation criteria (see <span><a href="904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml" target="_blank">Chapter 11</a>, <em>Selecting the Right Model with Hyperparameter Tuning</em></span>). At this stage, you should only be dealing with a handful of algorithms, so you can turn your attention to where the real magic lies: feature engineering.</li>
</ol>
</li>
<li><strong>Feature engineering</strong>: <span>Perhaps even more important than choosing the right algorithm is choosing the right features to represent the data. You can read all about feature engineering in <a href="142fec63-a847-4cde-9de9-c34805d2bb84.xhtml" target="_blank">Chapter 4</a>, <em>Representing Data and Engineering Features</em>.</span></li>
<li><strong>Optimize hyperparameters</strong>: <span>Finally, you also want to optimize an algorithm's hyperparameters. Examples might include the number of principal components of PCA, the parameter, <em>k</em>,</span> <span>in the k-nearest neighbor a</span><span>lgorithm, or the number of layers and learning rate in a neural network. You can look at <a href="904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml" target="_blank">Chapter 11</a>, <em>Selecting the Right Model with Hyperparameter Tuning</em>, for inspiration.</span></li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building your own estimator</h1>
                </header>
            
            <article>
                
<p>In this book, we visited a whole variety of machine learning tools and algorithms that OpenCV provides straight out of the box. And, if for some reason, OpenCV does not provide exactly what we are looking for, we can always fall back on scikit-learn.</p>
<p>However, when tackling more advanced problems, you might find yourself wanting to perform some very specific data processing that neither OpenCV nor scikit-learn provide, or you might want to make slight adjustments to an existing algorithm. In this case, you may want to create your own estimator.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Writing your own OpenCV-based classifier in C++</h1>
                </header>
            
            <article>
                
<p>Since OpenCV is one of those Python libraries that does not contain a single line of Python code under the hood (I'm kidding, but it's close), you will have to implement your custom estimator in C++. This can be done in four steps:</p>
<ol>
<li class="mce-root"><span>Implement a C++ source file that contains the main source code. You need to include two header files, one that contains all the core functionality of OpenCV (</span><kbd>opencv.hpp</kbd><span>) and another that contains the machine learning module (</span><kbd>ml.hpp</kbd><span>):</span></li>
</ol>
<pre style="padding-left: 60px">#include &lt;opencv2/opencv.hpp&gt;<br/>#include &lt;opencv2/ml/ml.hpp&gt;<br/>#include &lt;stdio.h&gt;</pre>
<p style="padding-left: 60px">Then, an estimator class can be created by inheriting from the <kbd>StatModel</kbd> class:</p>
<pre style="padding-left: 60px">class MyClass : public cv::ml::StatModel<br/>{<br/>    public:</pre>
<p style="padding-left: 60px">Next, you define <kbd>constructor</kbd> and <kbd>destructor</kbd> of the class:</p>
<pre style="padding-left: 90px">MyClass()<br/>{<br/>    print("MyClass constructor\n");<br/>}<br/>~MyClass() {}</pre>
<p style="padding-left: 60px">Then, you also have to define some methods. These are what you would fill in to make the classifier actually do some work:</p>
<pre style="padding-left: 90px">int getVarCount() const<br/>{<br/>    // returns the number of variables in training samples<br/>    return 0;<br/>}<br/><br/>bool empty() const<br/>{<br/>    return true;<br/>}<br/><br/>bool isTrained() const<br/>{<br/>    // returns true if the model is trained<br/>    return false;<br/>}<br/><br/>bool isClassifier() const<br/>{<br/>    // returns true if the model is a classifier<br/>    return true;<br/>}</pre>
<p style="padding-left: 60px">The main work is done in the <kbd>train</kbd> method, which comes in two flavors (accepting either <kbd>cv::ml::TrainData</kbd> or <kbd>cv::InputArray</kbd> as input):</p>
<pre style="padding-left: 90px">bool train(const cv::Ptr&lt;cv::ml::TrainData&gt;&amp; trainData,<br/>          int flags=0) const<br/>{<br/>    // trains the model<br/>    return false;<br/>}<br/><br/>bool train(cv::InputArray samples, int layout, <br/>          cv::InputArray responses)<br/>{<br/>    // trains the model<br/>    return false;<br/>}</pre>
<p style="padding-left: 60px">You also need to provide a <kbd>predict</kbd> method and a <kbd>scoring</kbd> function:</p>
<pre style="padding-left: 30px">        float predict(cv::InputArray samples,<br/>                     cv::OutputArray results=cv::noArray(),<br/>                     int flags=0) const<br/>        {<br/>            // predicts responses for the provided samples<br/>            return 0.0f;<br/>        }<br/><br/>        float calcError(const cv::Ptr&lt;cv::ml::TrainData&gt;&amp; data,<br/>                       bool test, cv::OutputArray resp)<br/>        {<br/>            // calculates the error on the training or test dataset<br/>            return 0.0f;<br/>        }<br/>   };</pre>
<p style="padding-left: 60px">The last thing to do is to include a <kbd>main</kbd> function that instantiates the class:</p>
<pre>   int main()<br/>   {<br/>       MyClass myclass;<br/>       return 0;<br/>   }</pre>
<ol start="2">
<li>Write a CMake file called <kbd>CMakeLists.txt</kbd>:</li>
</ol>
<pre style="padding-left: 60px">cmake_minimum_required(VERSION 2.8)<br/>project(MyClass)<br/>find_package(OpenCV REQUIRED)<br/>add_executable(MyClass MyClass.cpp)<br/>target_link_libraries(MyClass ${OpenCV_LIBS})</pre>
<ol start="3">
<li>Compile the file on the command line by typing the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cmake</strong><br/><strong>$ make</strong></pre>
<ol start="4">
<li>Run the executable <kbd>MyClass</kbd> method, which was generated by the last command and which should lead to the following output:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./MyClass</strong><br/><strong>MyClass constructor</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Writing your own scikit-learn-based classifier in Python</h1>
                </header>
            
            <article>
                
<p>Alternatively, you can write your own classifier using the scikit-learn library.</p>
<p>You can do this by importing <kbd>BaseEstimator</kbd> and <kbd>ClassifierMixin</kbd>. The latter will provide a corresponding <kbd>score</kbd> method, which works for all classifiers:</p>
<ol>
<li>Optionally, first, you can overwrite the <kbd>score</kbd> method to provide your own <span>metric <kbd>score</kbd> method</span>:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import numpy as np...     from sklearn.base import BaseEstimator, ClassifierMixin</pre>
<ol start="2">
<li>Then, you can define a class that inherits from both <kbd>BaseEstimator</kbd> and <kbd>ClassifierMixin</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In [2]: class MyClassifier(BaseEstimator, ClassifierMixin):...         """An example classifier"""</pre>
<ol start="3">
<li>You need to provide a constructor, <kbd>fit</kbd> and <kbd>predict</kbd> methods. The constructor defines all the parameters ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Where to go from here</h1>
                </header>
            
            <article>
                
<p>The goal of this book was to introduce you to the world of machine learning and prepare you to become a machine learning practitioner. Now that you know everything about the fundamental algorithms, you might want to investigate some topics in more depth.</p>
<p>Although it is not necessary to understand all of the details of all of the algorithms we implemented in this book, knowing some of the theory behind them might just make you a better data scientist.</p>
<div class="packt_infobox">If you are looking for more advanced material, then you might want to consider some of the following classics:
<ul>
<li><span>Stephen Marsland,</span> <em>Machine Learning: An Algorithmic Perspective,</em> <em>Second Edition</em><span>, Chapman and Hall/Crc, ISBN</span> <span class="a-size-base a-color-base">978-146658328-3, 2014</span></li>
<li><span>Christopher M. Bishop,</span> <em>Pattern Recognition and Machine Learning</em><span>. Springer, ISBN 978-038731073-2, 2007</span></li>
<li><span>Trevor Hastie, Robert Tibshirani, and Jerome Friedman,</span> <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em><span>.</span> <em>Second Edition</em><span>, Springer, ISBN 978-038784857-0, 2016</span></li>
</ul>
</div>
<p>When it comes to software libraries, we already learned about two essential ones—OpenCV and scikit-learn. Often, using Python is great for trying out and evaluating models, but larger web services and applications are more commonly written in Java or C++.</p>
<p>For example, the C++ package is <strong>Vowpal Wabbit</strong> (VW), which comes with its own command-line interface. For running machine learning algorithms on a cluster, people often use <kbd>mllib</kbd>, a <strong>Scala</strong> library built on top of <strong>Spark</strong>. If you are not married to Python, you might also consider using R, another common language of data scientists. R is a language designed specifically for statistical analysis and is famous for its visualization capabilities and the availability of many (often highly specialized) statistical modeling packages.</p>
<p>No matter which software you choose going forward, I guess the most important advice is to keep practicing your skills. But you already knew that. There are a number of excellent datasets out there that are just waiting for you to analyze them:</p>
<ul style="font-size: 16px">
<li>Throughout this book, we made great use of the example datasets that are built into scikit-learn. In addition, scikit-learn provides a way to load datasets from external services, such as <a href="http://mldata.org/">mldata.org</a>. Refer to <a href="http://scikit-learn.org/stable/datasets/index.html">http://scikit-learn.org/stable/datasets/index.html</a> for more information.</li>
<li><span>Kaggle</span> is a company that hosts a wide range of datasets as well as competitions on their website, <a href="http://www.kaggle.com">http://www.kaggle.com</a>. Competitions are often hosted by a variety of companies, nonprofit organizations, and universities, and the winner can take home some serious monetary prizes. A disadvantage of competitions is that they already provide a particular metric to optimize and usually a fixed, preprocessed dataset.</li>
<li>The <span>OpenML platform</span> (<a href="http://www.mldata.org">http://www.openml.org</a>) hosts over 20,000 datasets with over 50,000 associated machine learning tasks.</li>
</ul>
<ul style="font-size: 16px">
<li>Another popular choice is the UC Irvine Machine Learning Repository (<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>), hosting over 370 popular and well-maintained datasets through a searchable interface.</li>
</ul>
<div class="packt_infobox">Finally, if you are looking for more example code in Python, a number of excellent books nowadays come with their own GitHub repository:
<ul>
<li>Jake Vanderplas, <em>Python Data Science Handbook: Essential Tools for Working with Data</em>. O'Reilly, ISBN <span>978-149191205-8, 2016, <a href="https://github.com/jakevdp/PythonDataScienceHandbook">https://github.com/jakevdp/PythonDataScienceHandbook</a></span></li>
<li>Andreas Muller and Sarah Guido, <em>Introduction to Machine Learning with Python: A Guide for Data Scientists</em>. O'Reilly, ISBN 978-144936941-5, 2016, <a href="https://github.com/amueller/introduction_to_ml_with_python">https://github.com/amueller/introduction_to_ml_with_python</a></li>
<li>Sebastian Raschka, <em>Python Machine Learning</em>. Packt, ISBN <span>978-178355513-0, 2015, <a href="https://github.com/rasbt/python-machine-learning-book">https://github.com/rasbt/python-machine-learning-book</a></span></li>
</ul>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to approach a machine learning problem and built our own estimator. We learned how to write our own OpenCV-based classifier in C++ and scikit-learn-based classifier in Python. </p>
<p>In this book, we covered a lot of theory and practice. We discussed a wide variety of fundamental machine learning algorithms, both supervised or unsupervised, and illustrated best practices as well as ways to avoid common pitfalls, and we touched upon a variety of commands and packages for data analysis, machine learning, and visualization.</p>
<p>If you made it this far, you have already made a big step toward machine learning mastery. From here on out, I am confident you will do just fine on your own.</p>
<p>All that's left to say is farewell! ...</p></article></section></div>



  </body></html>