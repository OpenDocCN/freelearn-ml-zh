<html><head></head><body>
		<div id="_idContainer064">
			<h1 id="_idParaDest-47"><em class="italic"><a id="_idTextAnchor049"/>Chapter 3</em>: Anomaly Detection</h1>
			<p><strong class="bold">Anomaly detection</strong> was the<a id="_idIndexMarker129"/> original capability of Elastic ML and is the most mature, stretching its roots back to the Prelert days (before the acquisition by Elastic in 2016). This technology is robust, easy to use, powerful, and broadly applicable to all kinds of use cases for time series data.</p>
			<p>This jam-packed chapter will focus on using Elastic ML to detect anomalies in the occurrence rates of documents/events, rare occurrences of things, and numerical values outside of expected normal operation. We will run through some simple but effective examples that will highlight both the efficacy of Elastic ML and its ease of use.</p>
			<p>Specifically, we will cover the following:</p>
			<ul>
				<li>Elastic ML job types</li>
				<li>Dissecting the detector</li>
				<li>Detecting changes in event rates</li>
				<li>Detecting changes in metric values</li>
				<li>Understanding the advanced detector functions</li>
				<li>Splitting analysis along categorical features</li>
				<li>Understanding temporal versus population analysis</li>
				<li>Categorization analysis of unstructured messages</li>
				<li>Managing Elastic ML via the API</li>
			</ul>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor050"/>Technical requirements</h1>
			<p>The information in this chapter is based on the Elastic Stack as it exists in v7.10. As with all of the chapters, all the example code can be found on GitHub: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition ">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition.</a></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor051"/>Elastic ML job types</h1>
			<p>When we start using<a id="_idIndexMarker130"/> the Elastic ML UI to configure anomaly detection jobs, we will see that there are five different job wizards that are shown:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B17040_03_001.jpg" alt="Figure 3.1 – The Create job UI showing different configuration wizards&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – The Create job UI showing different configuration wizards</p>
			<p>The existence of these different configuration wizards implies that there are different "types" of jobs. In actuality, there is really only one job type—it is just that the anomaly detection job has many options, and many of these wizards make certain aspects of that configuration easier. Everything that you may desire to configure can be done via the <strong class="bold">Advanced</strong> wizard (or the API). In fact, when Elastic ML was first released as beta in v5.4, that was all that existed. Since then, the other wizards have been added for simplicity and usability in specific use cases.</p>
			<p>An anomaly<a id="_idIndexMarker131"/> detection<a id="_idIndexMarker132"/> job has many configuration settings, but the two most important ones are the <strong class="bold">analysis configuration</strong> and the <strong class="bold">datafeed</strong>.</p>
			<p>The analysis configuration is the recipe for what anomalies the job will detect. It contains a detection configuration (called<a id="_idIndexMarker133"/> the <strong class="bold">detector</strong>) as well as a few other settings, such as the bucket span. The datafeed is the configuration of the query that will be executed by Elasticsearch to retrieve the data that is to be analyzed by the detector.</p>
			<p>With respect to the different job wizards, the following are true:</p>
			<ul>
				<li><em class="italic">Jobs created by the Single metric wizard have only one detector</em>. Their datafeeds contain a query and aggregations, thus only sending summarized data to the ML algorithms. The aggregations are automatically created for you based upon your configuration parameters in the wizard. The job also makes use of a flag called <strong class="source-inline">summary_count_field_name</strong> (set with the value of <strong class="source-inline">doc_count</strong>) to signal that aggregated data (and not raw data from the source index) is to be expected.</li>
				<li><em class="italic">Jobs created with the Multi-metric wizard can have one or more detectors</em>. The analysis can also be split along categorical fields by setting <strong class="source-inline">partition_field_name</strong> (described later in the chapter). Their datafeeds do not contain aggregations (because the ML code needs to see all documents for every possible<a id="_idIndexMarker134"/> instance of a field value and will aggregate it on its own), thus full Elasticsearch documents are passed to the ML algorithms.</li>
				<li><em class="italic">Jobs created with the Population wizard can have one or more detectors</em>. The wizard also sets <strong class="source-inline">over_field_name</strong> (described later in the chapter), which signals that population analysis is to be used. The analysis can also be split along categorical fields by setting <strong class="source-inline">by_field_name</strong> (described later in the chapter). Their datafeeds do not contain aggregations, thus full Elasticsearch documents are passed to the ML algorithms.</li>
				<li><em class="italic">Jobs created with the Categorization wizard have only one detector</em>. The wizard also sets <strong class="source-inline">categorization_field_name</strong> (described later in the chapter), which signals that categorization analysis is to be used. Categorization analysis also sets <strong class="source-inline">by_field_name</strong> (described later in the chapter) to a value of <strong class="source-inline">mlcategory</strong>. The analysis can also be split along categorical fields by setting <strong class="source-inline">partition_field_name</strong> (described later in the chapter). Their datafeeds do not contain aggregations, thus full Elasticsearch documents are passed to the ML algorithms.</li>
				<li><em class="italic">Jobs created with the Advanced wizard can leverage every option available</em>. The onus is on the user to know what they are doing and configure the job correctly. The UI does prevent the user from making most mistakes, however. An experienced user can exclusively use the Advanced wizard to create any anomaly detection job.</li>
			</ul>
			<p>The options around job creation might seem daunting given what was just described. But do not fret—once<a id="_idIndexMarker135"/> we have gotten familiar with the terminology and have walked through some examples, you will find that the job configurations are very sensible; as more experience is gained, the configuration of jobs will become second nature. Let's take the next step and break down the components of the detector.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor052"/>Dissecting the detector</h1>
			<p>At the heart of the<a id="_idIndexMarker136"/> anomaly detection job are the analysis configuration and the detector. The <a id="_idIndexMarker137"/>detector has several key components to it:</p>
			<ul>
				<li>The <strong class="bold">function</strong></li>
				<li>The <strong class="bold">field</strong></li>
				<li>The <strong class="bold">partition field</strong></li>
				<li>The <strong class="bold">by field</strong></li>
				<li>The <strong class="bold">over field</strong></li>
			</ul>
			<p>We will go through each in turn to fully understand them all. Note that in the next few sections, however, we will often refer to the actual names of settings within the job configuration as if we were using the advanced job editor or the API. Although it is good to fully understand the nomenclature, as you progress through this chapter you will also notice that many of the details of the job configuration are abstracted away from the user or are given more "UI-friendly" labels than the real setting names.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor053"/>The function</h2>
			<p>The <a id="_idIndexMarker138"/>detector <strong class="bold">function</strong> describes<a id="_idIndexMarker139"/> how the data will be aggregated or measured within the analysis interval (bucket span). There are many functions, but they can be classified into the following categories:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B17040_03_002.jpg" alt="Figure 3.2 – Table of detector functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Table of detector functions</p>
			<p>Items marked with an asterisk (<strong class="source-inline">*</strong>) also have high/low one-sided variants (such as <strong class="source-inline">low_distinct_count</strong>) that allow the detection of anomalies in only one direction.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor054"/>The field</h2>
			<p>Some<a id="_idIndexMarker140"/> functions in<a id="_idIndexMarker141"/> the detector require a field within the data to operate on. Take the following examples:</p>
			<ul>
				<li><strong class="source-inline">max(bytes)</strong></li>
				<li><strong class="source-inline">mean(products.price)</strong></li>
				<li><strong class="source-inline">high_distinct_count(destination.port)</strong></li>
			</ul>
			<p>Therefore, the name of the field the function directly operates on is simply called <strong class="source-inline">field_name</strong>.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor055"/>The partition field </h2>
			<p>There are<a id="_idIndexMarker142"/> often <a id="_idIndexMarker143"/>cases in which the detection analysis needs to be split along a categorical field so the analysis can be done separately for all unique instances of that field. In this case, the <strong class="source-inline">partition</strong> field (the setting is called <strong class="source-inline">partition_field_name</strong>) defines the field to split on. For example, in e-commerce, you might want to see the average revenue per category (men's clothing, women's accessories, and so on). In this case, the <strong class="source-inline">category</strong> field would be the <strong class="source-inline">partition</strong> field. We will explore the splitting of the analysis later in this chapter.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor056"/>The by field</h2>
			<p>Similar <a id="_idIndexMarker144"/>to<a id="_idIndexMarker145"/> the <strong class="source-inline">partition</strong> field, the <strong class="source-inline">by</strong> field (the setting is called <strong class="source-inline">by_field_name</strong>) is another mechanism to split the analysis, but it behaves differently with respect to how the results are modeled and scored. Additionally, the <strong class="source-inline">by</strong> field is mandatory if <strong class="source-inline">rare</strong> or <strong class="source-inline">freq_rare</strong> is used. More details on the differences in using the <strong class="source-inline">by</strong> field for splitting versus using the <strong class="source-inline">partition</strong> field will be discussed later in the chapter.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor057"/>The over field</h2>
			<p>The <strong class="bold">over field</strong> (the<a id="_idIndexMarker146"/> setting<a id="_idIndexMarker147"/> is called <strong class="source-inline">over_field_name</strong>) signals<a id="_idIndexMarker148"/> to the anomaly detection algorithms <a id="_idIndexMarker149"/>that <strong class="bold">population analysis</strong> is desired, where entities are compared to their peers (instead of against their own past behavior). Population analysis is discussed in depth later in this chapter.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor058"/>The "formula"</h2>
			<p>If we were<a id="_idIndexMarker150"/> to <a id="_idIndexMarker151"/>document all of the possible configuration options for a detector and then create a flow chart-like map, it would look like the following:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B17040_03_003.jpg" alt="Figure 3.3 – The &quot;formula&quot; for building a detector from scratch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – The "formula" for building a detector from scratch</p>
			<p>The following<a id="_idIndexMarker152"/> are <a id="_idIndexMarker153"/>things to note about the diagram shown in <em class="italic">Figure 3.3</em>:</p>
			<ul>
				<li>Capitalized text is the explanation and italics text is the detector configuration settings (<strong class="source-inline">by_field_name</strong>, <strong class="source-inline">partition_field_name</strong>, and <strong class="source-inline">over_field_name</strong> are shortened to be simply <strong class="source-inline">by</strong>, <strong class="source-inline">partition</strong>, and <strong class="source-inline">over</strong>).</li>
				<li>Items in square brackets are optional (high, low, non-zero, non-null).</li>
				<li>Choose only one exit branch (notice only one exit branch out of <strong class="source-inline">rare</strong>/<strong class="source-inline">freq_rare</strong> because <strong class="source-inline">by</strong> is mandatory).</li>
			</ul>
			<p>Comparison of something versus its own history is accomplished simply by <em class="italic">not</em> choosing an <strong class="source-inline">over</strong> field.</p>
			<p>With a comprehensive understanding of the construction of a detector, we will now move into practical examples of using detectors for different use cases. First, we will explore the count <a id="_idIndexMarker154"/>functions that allow us to detect changes in<a id="_idIndexMarker155"/> event rates over time.</p>
			<p>Detecting changes in event rates</p>
			<p>There are <a id="_idIndexMarker156"/>many important use cases that revolve around the idea of event change detection. These include the following:</p>
			<ul>
				<li>Discovering a flood of error messages suddenly cropping up in a log file</li>
				<li>Detecting a sudden drop in the number of orders processed by an online system</li>
				<li>Determining a sudden excessive number of attempts at accessing something (for example, a sudden increase in the number of login attempts on a particular user ID)</li>
			</ul>
			<p>In order for us to find the abnormal, we must first have a mechanism to understand the normal rate of occurrence. But relying on our fallible human observation and intuition is not always the easiest (or most reliable) approach. </p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor059"/>Exploring the count functions</h2>
			<p>As mentioned <a id="_idIndexMarker157"/>in <a href="B17040_02_Epub_AM.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Enabling and Operationalization</em>, Elastic ML jobs have an anomaly detection "recipe" known as the <strong class="bold">detector</strong>. The detector is key to defining what anomalies the user wants to detect. Within the detector is the <strong class="bold">function</strong>, which selects the "feature" of what is to be detected. In the case of the count functions, the feature is the occurrence rate of something over time. There are three main count functions that we will see:</p>
			<ul>
				<li><strong class="source-inline">count</strong>: Counts the number of documents in the bucket resulting from a query of the raw data index</li>
				<li><strong class="source-inline">high_count</strong>: The same as <strong class="source-inline">count</strong>, but will only flag an anomaly if the count is higher than expected</li>
				<li><strong class="source-inline">low_count</strong>: The same as <strong class="source-inline">count</strong>, but will only flag an anomaly if the count is lower than expected</li>
			</ul>
			<p>We will see that there are a variety of one-sided functions in Elastic ML (to only detect anomalies in a certain direction). Additionally, it is important to know that the count functions are not counting a field or even the existence of fields within a document; they are merely counting the documents in the index over time.</p>
			<p>To get a more intuitive feeling for what the count functions do, let's jump into a simple example using the sample data within Kibana:</p>
			<ol>
				<li>To enable the sample data, from the Kibana home screen, click on the <strong class="bold">Add data</strong> button (in<a id="_idIndexMarker158"/> either location) as shown in <em class="italic">Figure 3.4</em>: <div id="_idContainer035" class="IMG---Figure"><img src="image/B17040_03_004.jpg" alt="Figure 3.4 – The Kibana home screen with Add data options&#13;&#10;"/></div><p class="figure-caption">Figure 3.4 – The Kibana home screen with Add data options</p></li>
				<li>After clicking on <strong class="bold">Add data</strong>, select <strong class="bold">Sample data</strong> to reveal three sets of data:<div id="_idContainer036" class="IMG---Figure"><img src="image/B17040_03_005.jpg" alt="Figure 3.5 – Adding sample data&#13;&#10;"/></div><p class="figure-caption">Figure 3.5 – Adding sample data</p></li>
				<li>Click on each of <a id="_idIndexMarker159"/>the three <strong class="bold">Add data</strong> buttons in each section to load that sample dataset into your Elastic Stack. Once the loading is complete, we will jump directly to ML by selecting the three-horizontal-lines menu icon (<img src="image/B17040_03_032.png" alt="Text&#10;&#10;Description automatically generated"/>) at the top left of Kibana to reveal the list of apps, and then select <strong class="bold">Machine Learning</strong>:<div id="_idContainer038" class="IMG---Figure"><img src="image/B17040_03_006.jpg" alt="Figure 3.6 – Selecting Machine Learning from the Kibana apps menu&#13;&#10;"/></div><p class="figure-caption">Figure 3.6 – Selecting Machine Learning from the Kibana apps menu</p></li>
				<li>Once this has been <a id="_idIndexMarker160"/>clicked, we will be on the ML overview page, where we can immediately see where we can create our first anomaly detection job. Click on the <strong class="bold">Create job</strong> button, as shown in <em class="italic">Figure 3.7</em>:<div id="_idContainer039" class="IMG---Figure"><img src="image/B17040_03_007.jpg" alt="Figure 3.7 – Elastic Cloud welcome screen&#13;&#10;"/></div><p class="figure-caption">Figure 3.7 – Elastic Cloud welcome screen</p></li>
				<li>Our next task is <a id="_idIndexMarker161"/>to select the index pattern (marked with an index with shards icon) or a saved search (marked with a magnifying glass icon) that contains the data that we'd like to analyze. If a saved search is chosen, then a filtered query that was previously created and saved within Kibana's <strong class="bold">Discover</strong> app will be the basis of your ML job. In this example, however, we will opt to select the <strong class="source-inline">kibana_sample_data_logs</strong> index, as we want to pass every document in that index through Elastic ML:<div id="_idContainer040" class="IMG---Figure"><img src="image/B17040_03_008.jpg" alt="Figure 3.8 – Selecting the kibana_sample_data_logs index for analysis&#13;&#10;"/></div><p class="figure-caption">Figure 3.8 – Selecting the kibana_sample_data_logs index for analysis</p></li>
				<li>On the next <a id="_idIndexMarker162"/>screen, we will select the <strong class="bold">Single metric</strong> job wizard because, at this point, we're interested in analyzing only one aspect of the data: its count over time:<div id="_idContainer041" class="IMG---Figure"><img src="image/B17040_03_009.jpg" alt="Figure 3.9 – Choosing a Single metric job&#13;&#10;"/></div><p class="figure-caption">Figure 3.9 – Choosing a Single metric job</p></li>
				<li>On the next<a id="_idIndexMarker163"/> screen, in order to follow along with this example, <em class="italic">you must select the </em><strong class="bold">Use full kibana_sample_logs_data</strong><em class="italic"> button in order to include the sample anomaly in this dataset</em>:<div id="_idContainer042" class="IMG---Figure"><img src="image/B17040_03_010.jpg" alt="Figure 3.10 – Selecting to use all the data within the index&#13;&#10;"/></div><p class="figure-caption">Figure 3.10 – Selecting to use all the data within the index</p><p class="callout-heading">Note</p><p class="callout">This demo data, when <a id="_idIndexMarker164"/>installed, actually puts about half of the data in the past and half in the future (by dynamically modifying the timestamps on ingest). This is done to provide a mechanism for the static data to look "real-time" when dashboards are viewed on data in the "last hour," for example. As a result of this, we're really going to ask Elastic ML to analyze data from the past and the future, where normally it would be impossible to have data from the future. Suspend belief for now for the sake of the example as the anomaly we'd like to demonstrate is in the second half of the dataset.</p><p>Now, click the <strong class="bold">Next</strong> button to advance to the next step in the configuration wizard.</p></li>
				<li>After clicking the <strong class="bold">Next</strong> button, we will need to select what we want to analyze from the <strong class="bold">Pick fields</strong> drop-down box. We will select <strong class="bold">Count(Event rate)</strong> to focus on our original goal here, which is to detect changes in the event rate in this index over time:<div id="_idContainer043" class="IMG---Figure"><img src="image/B17040_03_011.jpg" alt="Figure 3.11 – Selecting the count of events over time as our detection&#13;&#10;"/></div><p class="figure-caption">Figure 3.11 – Selecting the count of events over time as our detection</p><p>Notice that looking<a id="_idIndexMarker165"/> through this drop-down box shows that other analyses could be done, depending on the data type of the field in the data. We will explore some of these other options later on in subsequent examples.</p><p>Click the <strong class="bold">Next</strong> button to proceed, leaving the other options as their defaults for now.</p></li>
				<li>Now, we need to name our anomaly detection job. In the <strong class="bold">Job ID</strong> box, type in a logical name. Here, the name <strong class="source-inline">web_logs_rate</strong> was used: <div id="_idContainer044" class="IMG---Figure"><img src="image/B17040_03_012.jpg" alt="Figure 3.12 – Naming the anomaly detection job&#13;&#10;"/></div><p class="figure-caption">Figure 3.12 – Naming the anomaly detection job</p><p>Again, leave the other options as their defaults and click the <strong class="bold">Next</strong> button.</p></li>
				<li>A validation step <a id="_idIndexMarker166"/>takes place to ensure that everything is reasonable for the analysis to work:<div id="_idContainer045" class="IMG---Figure"><img src="image/B17040_03_013.jpg" alt="Figure 3.13 – Job validation step&#13;&#10;"/></div><p class="figure-caption">Figure 3.13 – Job validation step</p><p>Click the <strong class="bold">Next</strong> button to proceed.</p></li>
				<li>At this point, the job is ready to be created (and notice in <em class="italic">Figure 3.14</em> that some sensible default options, such as <strong class="bold">Model memory limit</strong> and <strong class="bold">Enable model plot</strong>, were<a id="_idIndexMarker167"/> chosen for you):<div id="_idContainer046" class="IMG---Figure"><img src="image/B17040_03_014.jpg" alt="Figure 3.14 – Anomaly detection job ready to be created"/></div><p class="figure-caption">Figure 3.14 – Anomaly detection job ready to be created</p></li>
				<li>After the <strong class="bold">Create job</strong> button is clicked, you will see an animated preview of the results<a id="_idIndexMarker168"/> superimposed on top of the data, as follows:<div id="_idContainer047" class="IMG---Figure"><img src="image/B17040_03_015.jpg" alt="Figure 3.15 – Results preview of the job execution displayed&#13;&#10;"/></div><p class="figure-caption">Figure 3.15 – Results preview of the job execution displayed</p><p>Let's now click the <strong class="bold">View results</strong> button to investigate in detail what the anomaly detection job has found in the data. </p></li>
				<li>Using the scrubber below the main graph, adjust the location and width of the viewing area to <a id="_idIndexMarker169"/>zoom in on the big spike:</li>
			</ol>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B17040_03_016.jpg" alt="Figure 3.16 – Results shown for a critical anomaly&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Results shown for a critical anomaly</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As you zoom in, out, and around, just be aware of the chart aggregation interval as compared to the job's bucket span (as circled in <em class="italic">Figure 3.16</em>). If you are zoomed out to a wider view, the chart aggregation interval can be larger than the job's bucket span, making the position of the drawn anomaly on the chart less exact.</p>
			<p>Here, in <em class="italic">Figure 3.16</em>, we can see that the very large spike in events was flagged as two distinct anomalies because the actual number of web requests seen in the logs was around 11 times higher than expected (given the learned model of the data up until that point in time). You may notice that the chart shows two anomalies next to each other because clearly, the spike in events spanned more than one 15-minute bucket interval. You may also notice<a id="_idIndexMarker170"/> that by default, there is only one anomaly shown in the table below the chart. This is because <strong class="bold">Interval</strong> defaults to <strong class="bold">Auto</strong> and time-adjacent anomalies are summarized together, with only the highest score shown. If <strong class="bold">Interval</strong> is changed to <strong class="bold">Show all</strong>, then both anomaly records are listed in the table:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17040_03_017.jpg" alt="Figure 3.17 – Interval set to Show all anomalies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Interval set to Show all anomalies</p>
			<p>There's one final thing to notice in this example, which is the other, lesser-scored anomalies earlier in the dataset:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17040_03_018.jpg" alt="Figure 3.18 – Multi-bucket anomalies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18 – Multi-bucket anomalies</p>
			<p>There are a few key<a id="_idIndexMarker171"/> things to recognize about these less-than-obvious anomalies:</p>
			<ul>
				<li>They have lower scores than the massive spike we just investigated because relatively speaking, these are not as anomalous, but are interestingly significant.</li>
				<li>The anomaly here is the "lack" of expected values. In other words, the <strong class="source-inline">count</strong> function interprets <em class="italic">no data</em> as 0 and that can be anomalous if normally there's an expectation that events should be occurring.</li>
				<li>These anomalies are<a id="_idIndexMarker172"/> not single-bucket anomalies, but rather <strong class="bold">multi-bucket anomalies</strong>. Multi-bucket anomalies are designated with a different symbol in the UI (a cross instead of a dot). They denote cases in which the actual singular value may not necessarily be anomalous, but there is a trend that is occurring in a sliding window of 12 consecutive buckets. Here, you can see that there is a noticeable slump spanning several adjacent buckets.<p class="callout-heading">Note</p><p class="callout">For more information on interpreting multi-bucket anomalies, see the detailed blog post at <a href="http://elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features">elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features</a>.</p></li>
			</ul>
			<p>We have seen, through <a id="_idIndexMarker173"/>this example, how the count function allows us to easily detect an obvious (and not-so-obvious) set of anomalies relating to the overall rate of occurrence of events (documents) in an index over time. Let's continue our journey by looking at other count and occurrence-based functions.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor060"/>Other counting functions</h2>
			<p>In addition to the functions that we've described so far, there are several other counting functions that enable a broader set of use cases.</p>
			<h3>Non-zero count</h3>
			<p>The non-zero count <a id="_idIndexMarker174"/>functions (<strong class="source-inline">non_zero_count</strong>, <strong class="source-inline">low_non_zero_count</strong>, and <strong class="source-inline">high_non_zero_count</strong>) allow the handling of count-based analysis, as well as allowing accurate modeling in cases where the data may be sparse and you would not want the non-existence of data to be explicitly treated as zero, but rather as null—in other words, a dataset in time, which looks like the following:</p>
			<p class="source-code">4,3,0,0,2,0,5,3,2,0,2,0,0,1,0,4</p>
			<p>Data with the <strong class="source-inline">non_zero_count</strong> functions will be interpreted as the following:</p>
			<p class="source-code">   4,3,2,5,3,2,2,1,4</p>
			<p>The act of treating zeros as null can be useful in cases where the non-existence of measurements at regular intervals is expected. Some practical examples of this are as follows:</p>
			<ul>
				<li>The number of airline tickets purchased per month by an individual </li>
				<li>The number of times a server reboots in a day</li>
				<li>The number of login attempts on a system per hour</li>
			</ul>
			<p>To select the non-zero count version of the count functions in the job wizards, just toggle the <strong class="bold">Sparse data</strong> option<a id="_idIndexMarker175"/> during the setup:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17040_03_019.jpg" alt="Figure 3.19 – Adding the Sparse data option to select the non-zero count&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.19 – Adding the Sparse data option to select the non-zero count</p>
			<p>We will see later in the chapter, when we are configuring the jobs in the advanced job wizard or via the API, that we will be explicitly using function names (such as <strong class="source-inline">high_non_zero_count</strong>) instead<a id="_idIndexMarker176"/> of toggling options with more conceptual descriptions.</p>
			<h3>Distinct count</h3>
			<p>The distinct<a id="_idIndexMarker177"/> count functions (<strong class="source-inline">distinct_count</strong>, <strong class="source-inline">low_distinct_count</strong>, and <strong class="source-inline">high_distinct_count</strong>) measure the uniqueness (<strong class="bold">cardinality</strong>) of<a id="_idIndexMarker178"/> values for a particular field. There are many possible uses of this function, particularly when used in the context of population analysis (see later in this chapter) to uncover entities that are logging an overly diverse set of field values. A good, classic example is looking for IP addresses that are engaged in some sort of scanning, accessing an unusually large number of distinct destination port numbers on remote machines or downloading many distinct URLs on a web server—in other words, entities that act like an automated bot and not like a typical human user. As an example, if we had selected <strong class="source-inline">distinct_count(url.keyword)</strong> as the detector configuration in the last example on the <strong class="source-inline">kibana_sample_data_logs</strong> index, we would have caught the same anomalous timeframe, but for a different reason—not only was the overall volume of requests high, as we saw back in <em class="italic">Figure 3.16</em>, but here in <em class="italic">Figure 3.20</em>, we see that there was a high diversity of URLs being requested:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17040_03_020.jpg" alt="Figure 3.20 – Distinct count detector example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20 – Distinct count detector example</p>
			<p>With an<a id="_idIndexMarker179"/> appreciation of count-based functions, let's now turn to metric-based functions, which allow us to analyze numerical fields in the data.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor061"/>Detecting changes in metric values</h1>
			<p>Obviously, not all <a id="_idIndexMarker180"/>data being emitted from systems will be text or categorical in nature—a vast amount of it is numerical. Detecting changes in metric values over time is perfectly suited for anomaly detection because, as mentioned in <a href="B17040_01_Epub_AM.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning for IT</em>, the historical paradigm of alerting on exceptions in numerical values via static thresholds has been troublesome for decades. Let's explore all that Elastic ML has to offer with respect to the functions that help you detect<a id="_idIndexMarker181"/> changes in numerical fields in your data.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor062"/>Metric functions</h2>
			<p>Metric functions <a id="_idIndexMarker182"/>operate on numerical fields and return numerical values. They are perhaps the easiest of the detector functions to understand.</p>
			<h3>min, max, mean, median, and metric</h3>
			<p>These<a id="_idIndexMarker183"/> functions<a id="_idIndexMarker184"/> do<a id="_idIndexMarker185"/> exactly as you<a id="_idIndexMarker186"/> would <a id="_idIndexMarker187"/>expect: they return the minimum, maximum, average/mean, and median of all of the numerical observations for the field of interest in the bucket span. </p>
			<p>The <strong class="source-inline">metric</strong> function is a little unique in that it is really just a shorthand way of specifying that <strong class="source-inline">min</strong>, <strong class="source-inline">max</strong>, and <strong class="source-inline">mean</strong> are to be used together. </p>
			<p>It should be noted that if the frequency of the data (for example, data that comes from a sampling source such as Metricbeat) exactly matches the bucket span, then there is only one sample per bucket span. This means that the minimum, maximum, average/mean, and median of the field of interest are all the same value (the value of the single observation itself). Therefore, if possible, it is usually better to have multiple numerical samples per bucket span if you want to have discrimination using these functions.</p>
			<p>Another fact to note is that these metric functions treat the lack of data as <em class="italic">null</em>. In other words, if your data is sparse and there are bucket spans in which no observations are seen, the lack of data will not "drag down" the statistics for the field of interest. This is why these metric-based functions have no "non-zero" or "non-null" counterpart.</p>
			<h3>varp</h3>
			<p>The <strong class="source-inline">varp</strong> function<a id="_idIndexMarker188"/> measures the overall variance of a metric over time—its volatility. Using this function might be applicable to finding cases where the numerical value of a field should normally be rather consistent, but you would like to detect whether there was a change. </p>
			<h3>Sum and non-null sum</h3>
			<p>The <strong class="source-inline">sum</strong><a id="_idIndexMarker189"/> function<a id="_idIndexMarker190"/> will return the sum of all of the numerical observations for the field of interest in the bucket span. Use the "non-null" version if you have sparse data and do not want the lack of data being treated as <em class="italic">zero</em>, which will inevitably "drag down" the value of the sum.</p>
			<p>If we had selected <strong class="source-inline">sum(bytes)</strong> as the detector configuration in the last example on the <strong class="source-inline">kibana_sample_data_logs</strong> index, we would have caught the same anomalous timeframe, but for a different reason—we see that the requests made also resulted in a higher quantity of bytes being transferred from the web server:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17040_03_021.jpg" alt="Figure 3.21 – Sum detector example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21 – Sum detector example</p>
			<p>This is totally <a id="_idIndexMarker191"/>sensible, given that an increased number of requests to a web server will <a id="_idIndexMarker192"/>correlate with an increase in the number of bytes being transferred.</p>
			<p>Now that we have an<a id="_idIndexMarker193"/> appreciation for the simpler detector functions, let's move <a id="_idIndexMarker194"/>on to the more complex, advanced functions.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor063"/>Understanding the advanced detector functions</h1>
			<p>In addition to the<a id="_idIndexMarker195"/> detector functions mentioned so far, there are also a few other, more advanced functions that allow some very unique capabilities. Some of these functions are only available if the ML job is configured via the advanced job wizard or via the API.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor064"/>rare</h2>
			<p>In the context<a id="_idIndexMarker196"/> of a stream of temporal information (such as<a id="_idIndexMarker197"/> a <a id="_idIndexMarker198"/>log file), the notion of something being statistically rare (occurring at a low frequency) is paradoxically both intuitive and hard to understand. If I were asked, for example, to trawl through a log file and find a rare message, I might be tempted to label the first novel message that I saw as a rare one. But what if practically every message was novel? Are they all rare? Or is nothing rare?</p>
			<p>In order to define rarity to be useful in the context of a stream of events in time, we need to agree that the declaration of something as being rare must take into account the context in which it exists. If there are lots of other routine things and a small number of unique things, then we can deem the unique things rare. If there are many unique things, then we will deem that nothing is rare.</p>
			<p>When applying the <strong class="source-inline">rare</strong> function in an ML job, there is a requirement to declare which field the <strong class="source-inline">rare</strong> function is focusing on. This field is then defined as <strong class="source-inline">by_field_name</strong>. Configuration of the <strong class="source-inline">rare</strong> function does not have its own wizard in the Elastic ML UI, so you will need to define it using the advanced job wizard. For example, to find log entries that reference a rare country name, structure your detector similar to this:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17040_03_022.jpg" alt="Figure 3.22 – Rare detector example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22 – Rare detector example</p>
			<p>This could be handy <a id="_idIndexMarker199"/>for finding unexpected geographical<a id="_idIndexMarker200"/> access (as in "Our admins usually log in from the New York and London offices almost daily, but never from Moscow!").</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor065"/>Frequency rare</h2>
			<p>The <strong class="source-inline">freq_rare</strong> function<a id="_idIndexMarker201"/> is a specialized version of <strong class="source-inline">rare</strong>, in that it looks<a id="_idIndexMarker202"/> for members of a population that cause rare values of <strong class="source-inline">by_field_name</strong> to occur frequently. For example, you could locate a particular IP address that is attempting to access many rare URLs that are not generally seen across the entire population of all client IP addresses. This IP address could be attempting to access otherwise hidden sections of a website in a nefarious way, or may be attempting attacks such as SQL injection.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor066"/>Information content</h2>
			<p>The <strong class="source-inline">info_content</strong> function<a id="_idIndexMarker203"/> is perhaps<a id="_idIndexMarker204"/> the most specialized detector function in Elastic ML's arsenal. It was originally written<a id="_idIndexMarker205"/> as a means to measure the amount of <strong class="bold">entropy</strong> in text strings (how many and how diverse the characters are). This is because there are well-known techniques in malware that encrypt instructions and/or payload data for <a id="_idIndexMarker206"/>transmission for <strong class="bold">command and control</strong> (<strong class="bold">C2</strong>) and data exfiltration activity. Detecting this activity along this feature of the data is more reliable than looking at other features (such as the number of bytes sent or counting distinct entities).</p>
			<p>The algorithm used will essentially do the following steps:</p>
			<ol>
				<li value="1">Sort the unique strings into alphabetical order.</li>
				<li>Concatenate those unique strings into one long string.</li>
				<li>Perform the <strong class="source-inline">gzip</strong> algorithm on that long string to compress it.<p>The<a id="_idIndexMarker207"/> information content is the length of the <a id="_idIndexMarker208"/>compressed data.</p></li>
			</ol>
			<p>Some of the ML jobs in the Elastic SIEM utilize the <strong class="source-inline">info_content</strong> function—stay tuned for <a href="B17040_08_Epub_AM.xhtml#_idTextAnchor146"><em class="italic">Chapter 8</em></a>, <em class="italic">Anomaly Detection in Other Elastic Stack Apps</em>, for more details.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor067"/>Geographic</h2>
			<p>If you find a <a id="_idIndexMarker209"/>geographic location that is unusual to a<a id="_idIndexMarker210"/> learned location area on Earth, then the <strong class="source-inline">lat_long</strong> function will be helpful, taking a <strong class="source-inline">field_name</strong> argument that is a comma-separated pair of numbers in the range of -180 to 180 (for example, <strong class="source-inline">40.75, -73.99</strong>, the coordinates of Times Square in New York City). The <strong class="source-inline">lat_long</strong> function can also operate on a <strong class="source-inline">geo_point</strong> field, a <strong class="source-inline">geo_shape</strong> field that contains point values, or a <strong class="source-inline">geo_centroid</strong> aggregation. An example use case would be to flag a location that isn't normal (and potentially fraudulent or malicious) for a specific user, transaction, and so on. </p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor068"/>Time</h2>
			<p>Not all things<a id="_idIndexMarker211"/> occur randomly in time, especially with things<a id="_idIndexMarker212"/> involving human behavior. We may eat, commute, or log into certain systems at predictable times of the day or week. Using the <strong class="source-inline">time_of_day</strong> and <strong class="source-inline">time_of_week</strong> functions, you can detect changes of behavior from a learned temporal routine. If a behavior is predictable on a 24-hour timeframe, then <strong class="source-inline">time_of_day</strong> is more appropriate. If the routine is day-of-the-week-dependent, then <strong class="source-inline">time_of_week</strong> should be a more logical choice. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Do not confuse the usage of these time functions with the natural temporal learning of all detectors in the anomaly detection jobs. As explained in <a href="B17040_01_Epub_AM.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning for IT</em>, the de-trending capability of the modeling will take into account the time at which something occurs. These functions simply model the event's timestamp within the day or week. For example, if something routinely happens at 2:00 A.M. every day, the function will learn that the normal time for this to happen is at the 7,200th second into the day.</p>
			<p>Now that we've been<a id="_idIndexMarker213"/> through the entire catalog of detector functions, let's look <a id="_idIndexMarker214"/>ahead and see how we can expand the breadth of our analysis by splitting the modeling across entities that are represented by categorical fields.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor069"/>Splitting analysis along categorical features</h1>
			<p>We have seen<a id="_idIndexMarker215"/> the power of anomaly detection jobs in uncovering interesting anomalies in a single time series dataset. However, there are a few mechanisms by which the analysis can be split along a categorical field to invoke a parallel analysis across tens, hundreds, and even multiple thousands of unique entities. </p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor070"/>Setting the split field</h2>
			<p>When using <a id="_idIndexMarker216"/>some of the job wizards (such as the Multi-metric and Population wizards), you will see an option to split the analysis:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17040_03_023.jpg" alt="Figure 3.23 – Splitting on a categorical field&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23 – Splitting on a categorical field</p>
			<p>Here, in <em class="italic">Figure 3.23</em>, which <a id="_idIndexMarker217"/>uses the Multi-metric wizard to build a job against the <strong class="source-inline">kibana_sample_data_ecommerce</strong> index, we see that the high sum function on the <strong class="source-inline">taxful_total_price</strong> field is being split per instance on the field called <strong class="source-inline">category.keyword</strong> (plus turning the <strong class="bold">Sparse data</strong> option on). In other words, the analysis will be done for every category of items in this e-commerce store (men's clothing, women's accessories, and so on). If the analysis is run and the results are inspected using the Anomaly Explorer UI, the result might look like the following:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B17040_03_024.jpg" alt="Figure 3.24 – Results of split analysis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.24 – Results of split analysis</p>
			<p>Notice in <em class="italic">Figure 3.24</em>, that<a id="_idIndexMarker218"/> the Anomaly Explorer view is different from what we've seen so far in the Single Metric Viewer. The Anomaly Explorer shows the top 10 most anomalous categories (the field we split on) over time. Notice that not every category is shown, only the ones with anomalies—and clearly, the <strong class="bold">Men's Clothing</strong> category was the most unusual with a revenue of $2,250 on November 9th (in this version of the dataset). We will be learning more about understanding the results of multi-metric jobs and will use the Anomaly Explorer extensively in <a href="B17040_05_Epub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Interpreting Results</em>. </p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor071"/>The difference between splitting using partition and by_field</h2>
			<p>As a <a id="_idIndexMarker219"/>reminder, when using the Multi-metric wizard and a split is invoked, the <strong class="source-inline">partition_field_name</strong> setting is set with the value of the field chosen in the UI. </p>
			<p>When splitting is chosen in the Population wizard, however, <strong class="source-inline">by_field_name</strong> is chosen to split the analysis. If the Advanced wizard is used, then <strong class="source-inline">partition_field_name</strong> and/or <strong class="source-inline">by_field_name</strong> can be defined (if both, then it's effectively a double-split). Therefore, it would be helpful to know how these two settings, which effectively split the analysis, are different from each other. </p>
			<p>If you want to "hard split" the analysis, use <strong class="source-inline">partition_field_name</strong>:</p>
			<ul>
				<li>The field chosen should, in general, have &lt;10,000 distinct values per job, as more memory is required to partition.</li>
				<li>Each instance of the field is like an independent variable.</li>
			</ul>
			<p>The scoring of anomalies in one partition is more independent from other partitions.</p>
			<p>If you want a "soft split," use <strong class="source-inline">by_field_name</strong>:</p>
			<ul>
				<li>The field chosen should, in general, have &lt;100,000 distinct values per job.</li>
				<li>More appropriate for attributes of an entity (dependent variables).</li>
				<li>Scoring considers the history of other <strong class="source-inline">by</strong> fields.</li>
			</ul>
			<p>Let's dive deep into that last listed item—relating to the "history" of the other <strong class="source-inline">by</strong> fields. What exactly does that mean?</p>
			<p>In general, there is a concept in anomaly detection job analysis relating to when an entity first happens, which we'll<a id="_idIndexMarker220"/> call the <strong class="bold">dawn of time</strong>. When the dawn of time of something happens (that is, the first time the job sees data for <strong class="source-inline">host:X</strong> or <strong class="source-inline">error_code:Y</strong>), there may be one of two situations:</p>
			<ul>
				<li>That new entity is seen as "novel" and that, in itself, is notable and potentially worthy of being flagged as anomalous. To do that, you need to have your "dawn of time" be when the job starts.</li>
				<li>That new entity is just part of the normal "expansion" of the data—perhaps a new server was added to the mix or a new <strong class="source-inline">product_id</strong> was added to the catalog. In this case, just start modeling that new entity and don't make a fuss about it showing up. To do that, you need to have the "dawn of time" be when that entity first shows up.</li>
			</ul>
			<p>When analyzing splits using <strong class="source-inline">by_field_name</strong>, the dawn of time is when the ML job was started and when split using <strong class="source-inline">partition_field_name</strong>, the dawn of time is when that partition first showed up in the data. As such, you will get different results if you split one <a id="_idIndexMarker221"/>way versus the other for a situation in which something "new" comes along.</p>
			<h3>Is double-splitting the limit?</h3>
			<p>As mentioned, by using <a id="_idIndexMarker222"/>both <strong class="source-inline">partition_field_name</strong> and <strong class="source-inline">by_field_name</strong> when in the advanced job wizard, you can effectively get a double-split. But, if you need to split more, you'll have to rely on some other methods. Namely, you'll have to<a id="_idIndexMarker223"/> create a <strong class="bold">scripted field</strong> that is a concatenation of two (or more) fields. Using scripted fields is something that is covered in one of the examples in the <a href="B17040_14_Epub_AM.xhtml#_idTextAnchor248"><em class="italic">Appendix</em></a>.</p>
			<p>Now that we have learned about the concept of splitting the analysis, let's focus on the differences between temporal and population analysis in anomaly detection.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor072"/>Understanding temporal versus population analysis</h1>
			<p>We learned<a id="_idIndexMarker224"/> back in <a href="B17040_01_Epub_AM.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning for IT</em>, that <a id="_idIndexMarker225"/>there are effectively two ways to consider something as anomalous:</p>
			<ul>
				<li>Whether or not something changes drastically with respect to its own behavior over time</li>
				<li>Whether or not something is drastically different when compared to its peers in an otherwise homogeneous population</li>
			</ul>
			<p>By default, the former (which we'll simply call temporal analysis) is the mode used <em class="italic">unless</em> the <strong class="source-inline">over_field_name</strong> setting is specified in the detector config.</p>
			<p>Population analysis can be very useful in finding outliers in a variety of important use cases. For example, perhaps we want to find machines that are logging more (or less) than similarly configured machines in the following scenarios:</p>
			<ul>
				<li>Incorrect configuration changes that have caused more errors to suddenly occur in the log file for the system or application.</li>
				<li>A system that might be compromised by malware may actually be instructed to suppress logging in certain situations, thus drastically decreasing the log volume.</li>
				<li>A system that <a id="_idIndexMarker226"/>has lost connectivity or <a id="_idIndexMarker227"/>has operationally failed, thus having its log volume diminished.</li>
				<li>An otherwise harmless change to a logging-level setting (debug instead of normal), now annoyingly making your logs take up more disk space.</li>
			</ul>
			<p>Another way <a id="_idIndexMarker228"/>population analysis is often used is with respect to <strong class="bold">User/Entity Behavioral Analysis</strong> (<strong class="bold">EUBA</strong>), where a comparison of an entity's or human's actions compared against their peers might reveal the following:</p>
			<ul>
				<li><strong class="bold">Automated users</strong>: Instead <a id="_idIndexMarker229"/>of the typical human behavior or usage pattern, an automated script may exhibit behavioral patterns that look quite different in terms of the speed, duration, and diversity of events they create. Whether it is finding a crawler trying to harvest the products and prices of an online catalog or detecting a bot that might be engaged in the spread of misinformation on social media, the automatic identification of automated users can be helpful.</li>
				<li><strong class="bold">Snooping users</strong>: Whether<a id="_idIndexMarker230"/> it is a real human testing the boundaries of what they can get away with or an intelligent piece of malware doing some reconnaissance, a snooper may execute a wide variety of things, hoping for a match or to find a way in (such as by port scanning). Often, using <a id="_idIndexMarker231"/>the <strong class="source-inline">distinct_count</strong> function can help find a snooper.</li>
				<li><strong class="bold">Malicious/abusive users</strong>: After the reconnaissance phase, a malicious user or malware <a id="_idIndexMarker232"/>will <a id="_idIndexMarker233"/>move on to actively wreaking havoc and will engage in active measures such as denial of service, brute-forcing, or stealing valuable information. Again, compared with typical users, malicious and abusive users have stark contrasts in their behavior regarding the volume, diversity, and intensity of activity per unit of time.</li>
			</ul>
			<p>A practical example might be to find a customer that drastically spends a lot more than their peers. Whether<a id="_idIndexMarker234"/> or not you do this in the context of <a id="_idIndexMarker235"/>proactively investigating potential fraud, or whether you are interested in increasing the marketing to your most affluent customers, you still need to find those outliers. If we were to use the <strong class="source-inline">kibana_sample_data_ecommerce</strong> index that we added earlier in the chapter, we could create a population job by selecting the <strong class="bold">Population</strong> wizard and then choosing the <strong class="source-inline">customer_full_name.keyword</strong> field for <strong class="bold">Population</strong> <strong class="bold">field</strong> (so that we're comparing all users against each other). For the detector, we'll pick the high sum of the <strong class="source-inline">taxful_total_price</strong> field, which is the total revenue for each order placed by individuals:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B17040_03_025.jpg" alt="Figure 3.25 – Population analysis of revenue over users&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25 – Population analysis of revenue over users</p>
			<p>After this<a id="_idIndexMarker236"/> job is executed, you should see the <a id="_idIndexMarker237"/>following results:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B17040_03_026.jpg" alt="Figure 3.26 – Population analysis results of the biggest spenders&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26 – Population analysis results of the biggest spenders</p>
			<p>Here, in <em class="italic">Figure 3.26</em>, we<a id="_idIndexMarker238"/> see that the list of the most<a id="_idIndexMarker239"/> unusual users (in this case, the biggest spenders per unit of time) is dominated by a user named <strong class="bold">Wagdi Shaw</strong>, who apparently placed an order for $2,250 worth of goods. The astute among you will recognize this anomaly from an earlier example—except this time, we are orienting our analysis around the user, not around the inventory category. </p>
			<p>As you can see, population analysis can be very powerful and is heavily used in use cases in which individual entities are targeted. Thus, it is very useful in security analytics use cases. Let's now pivot to focus on one additional, but powerful, capability of Elastic ML's anomaly<a id="_idIndexMarker240"/> detection—the ability to effectively analyze <a id="_idIndexMarker241"/>unstructured log messages via a process called categorization.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor073"/>Categorization analysis of unstructured messages</h1>
			<p>Imagine that you<a id="_idIndexMarker242"/> are troubleshooting a <a id="_idIndexMarker243"/>problem by looking at a particular log file. You see a line in the log that looks like the following:</p>
			<p class="source-code">   18/05/2020 15:16:00 DB Not Updated [Master] Table</p>
			<p>Unless you have some intimate knowledge about the inner workings of the application that created this log, you may not know whether the message is important. Having the database be <strong class="source-inline">Not Updated</strong> possibly sounds like a negative situation. However, if you knew that the application routinely writes this message, day in and day out, several hundred times per hour, then you would naturally realize that this message is benign and should possibly be ignored, because clearly the application works fine every day despite this message being written to the log file.</p>
			<p>The problem, obviously, is one of human interpretation. Inspection of the text of the message and the reading of a negative phrase (<strong class="source-inline">Not Updated</strong>) potentially biases a person toward thinking that the message is noteworthy because of a possible problem. However, the frequency of the message (it happens routinely) should inform the person that the message must not be that important because the application is working (that is, there are no reported outages) despite these messages being written to the log.</p>
			<p>It can be hard for a human to process that information (assess the message content/relevance and also the frequency over time) for just a few types of messages in a log file. Imagine if there were thousands of unique message types occurring at a total rate of millions of log lines per day. Even the most seasoned expert in both the application content and search/visualizations will find this impractical, if not impossible, to wrangle.</p>
			<p>Elastic ML comes to the rescue with capabilities that allow the empirical assessment of both the uniqueness of the content of the messages and the relative frequency of occurrence. </p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor074"/>Types of messages that are good candidates for categorization</h2>
			<p>We need <a id="_idIndexMarker244"/>to be a little rigorous in our definition of the kinds of message-based log lines that are good for this kind of analysis. What we are <em class="italic">not</em> considering are log lines/events/documents that are completely freeform and likely the result of human creation (emails, tweets, comments, and so on). These kinds of messages are too arbitrary and variable in their construction and content.</p>
			<p>Instead, we will focus on machine-generated messages that are obviously emitted when an application encounters different situations or exceptions, thus constraining their construction and content to a relatively discrete set of possibilities (understanding that there may indeed be some variable aspects of the message). For example, let's look at the following few lines of an application log:</p>
			<p class="source-code">18/05/2016 15:16:00 S ACME6 DB Not Updated [Master] Table</p>
			<p class="source-code">18/05/2016 15:16:00 S ACME6 REC Not INSERTED [DB TRAN] Table </p>
			<p class="source-code">18/05/2016 15:16:07 S ACME6 Using: 10.16.1.63!svc_prod#uid=demo;pwd=demo </p>
			<p class="source-code">18/05/2016 15:16:07 S ACME6 Opening Database = DRIVER={SQL Server};SERVER=10.16.1.63;network=dbmssocn;address=10.16.1.63,1433;DATABASE=svc_prod;uid=demo;pwd=demo;AnsiNPW=No </p>
			<p class="source-code">18/05/2016 15:16:29 S ACME6 DBMS ERROR : db=10.16.1.63!svc_prod#uid=demo;pwd=demo Err=-11 [Microsoft][ODBC SQL Server Driver][TCP/IP Sockets]General network error. Check your network documentation.</p>
			<p>Here, we can see that there is a variety of messages with different text in each, but there is some structure here. After the date/time stamp and the server name from which the message originates (here, <strong class="source-inline">ACME6</strong>), there is the actual meat of the message, where the application is informing the outside world what is happening at that moment—whether something is being tried or errors are occurring.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>The process used by categorization </h2>
			<p>In order to bring some<a id="_idIndexMarker245"/> order to the otherwise disorderly flow of the messages in the log file, Elastic ML will employ a technique of grouping similar messages together by using a string-similarity clustering algorithm. The heuristics behind this algorithm are roughly as follows:</p>
			<ul>
				<li>Focus on the (English) dictionary words <a id="_idIndexMarker246"/>more than <strong class="bold">mutables</strong> (that is, <strong class="source-inline">network</strong> and <strong class="source-inline">address</strong> are dictionary words, but <strong class="source-inline">dbmssocn</strong> is likely a mutable/variable string).</li>
				<li>Pass the immutable dictionary words through a string-similarity algorithm (similar<a id="_idIndexMarker247"/> to the <strong class="bold">Levenshtein distance</strong>) to determine how similar the log line is to past log lines.</li>
				<li>If the difference between the current log line and an existing category is small, then group the existing log line into that category. Otherwise, create a new category for the current log line.</li>
			</ul>
			<p>As a simple example, consider these three messages:</p>
			<p class="source-code">   Error writing file "foo" on host "acme6"</p>
			<p class="source-code">   Error writing file "bar" on host "acme5"</p>
			<p class="source-code">   Opening database on host "acme7"</p>
			<p>The algorithm would cluster the first two messages together in the same category, as they would be deemed as <strong class="source-inline">Error writing file on</strong> types of messages, whereas the third message would be given its own (new) category.</p>
			<p>The naming of these categories is simple: ML will just call them <strong class="source-inline">mlcategory N</strong>, where <strong class="source-inline">N</strong> is an incrementing integer. Therefore, in this example, the first two lines will be associated with <strong class="source-inline">mlcategory 1</strong>, and the third line will be associated with <strong class="source-inline">mlcategory 2</strong>. In a realistic machine log, there may be thousands (or even tens of thousands) of categories that are generated due to the diversity of the log messages, but the set of possible categories should be finite. However, if the number of categories starts to get into the hundreds of thousands, it may become obvious that the log messages are not a constrained set of possible message types and will not be a good candidate for this type of analysis.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor076"/>Analyzing the categories</h2>
			<p>Now that the <a id="_idIndexMarker248"/>messages are going to be categorized by the algorithm described previously, the next part of the process is to do the analysis (using either <strong class="source-inline">count</strong> or <strong class="source-inline">rare</strong>). In this case, we're not going to be counting the log lines (and thus the documents of an Elasticsearch index) themselves; instead, we're going to be counting the occurrence rate of the different categories that are the output of the algorithm. So, for example, given the example log lines in the previous section, if they occurred within the same bucket span, we would have the following output of the categorization algorithm:</p>
			<p class="source-code">   mlcategory 1: 2</p>
			<p class="source-code">   mlcategory 2: 1</p>
			<p>In other words, there were two occurrences of the <strong class="source-inline">Error writing file on</strong> types of messages and one occurrence of the <strong class="source-inline">Opening database on host</strong> type in the last bucket span interval. It is this information that will ultimately be modeled by the ML job in order to determine whether it is unusual.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor077"/>Categorization job example</h2>
			<p>With the<a id="_idIndexMarker249"/> categorization job wizard in the UI, the process of configuring this type of job is extremely easy. Let's first assume we have an unstructured log file ingested (perhaps such as the <strong class="source-inline">secure.log</strong> file in the <strong class="source-inline">example_data</strong> folder on GitHub):</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on how to ingest data using the File Visualizer, see the detailed blog post at <a href="http://elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer">elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer</a>.</p>
			<ol>
				<li value="1">After picking the index of interest and choosing the Categorization wizard, and then selecting the appropriate time range for the analysis, we see that the wizard will ask us which <strong class="bold">categorization detector</strong> we want to use (a <strong class="bold">Count</strong> detector or a <strong class="bold">Rare</strong> detector) as well as the <strong class="bold">categorization field</strong> to be used. In our case, our data essentially only has two fields (<strong class="source-inline">@timestamp</strong> and <strong class="source-inline">message</strong>). Therefore, the <strong class="source-inline">message</strong> field is the field we would like Elastic ML to categorize. We <a id="_idIndexMarker250"/>will also pick the <strong class="bold">Count</strong> detector in this example:<div id="_idContainer059" class="IMG---Figure"><img src="image/B17040_03_027.jpg" alt="Figure 3.27 – Categorization job configuration&#13;&#10;"/></div><p class="figure-caption">Figure 3.27 – Categorization job configuration</p><p>Notice in <em class="italic">Figure 3.27</em> that there is a check on the selected category field to make sure it will yield sensible results. Also notice that in the <strong class="bold">Examples</strong> section, you get visual confirmation of Elastic ML focusing on the non-mutable text of the log messages. </p></li>
				<li>Once the configuration is confirmed and the job is started in the wizard, you will see a preview of<a id="_idIndexMarker251"/> the results as they are being discovered and analyzed:<div id="_idContainer060" class="IMG---Figure"><img src="image/B17040_03_028.jpg" alt="Figure 3.28 – Categorization job execution preview&#13;&#10;"/></div><p class="figure-caption">Figure 3.28 – Categorization job execution preview</p><p>Notice that in this simple example, a total of 23 categories were discovered in the data. When the results are viewed in the Anomaly Explorer, we see that the top anomaly here is <strong class="source-inline">mlcategory</strong> number 7. </p></li>
				<li>When you click on the <strong class="bold">category examples</strong> column, the view expands to demonstrate the<a id="_idIndexMarker252"/> example log lines that have fallen into that category:<div id="_idContainer061" class="IMG---Figure"><img src="image/B17040_03_029.jpg" alt="Figure 3.29 – Categorization job results&#13;&#10;"/></div><p class="figure-caption">Figure 3.29 – Categorization job results</p><p>Here, we see that there was a sudden uptick in <strong class="source-inline">Received disconnect</strong> messages. </p></li>
				<li>By clicking on the gear icon, as shown in <em class="italic">Figure 3.29</em>, we can select <strong class="bold">View examples</strong> to transport us over to the Kibana Discover UI, but filtered to this appropriate message and zoomed into the relevant timeframe:<div id="_idContainer062" class="IMG---Figure"><img src="image/B17040_03_030.jpg" alt="Figure 3.30 – Inspecting raw log lines from the categorization job results&#13;&#10;"/></div><p class="figure-caption">Figure 3.30 – Inspecting raw log lines from the categorization job results</p><p>Notice that the <a id="_idIndexMarker253"/>Discover query bar has been automatically filled in with an appropriate KQL query to limit our view to the kind of messages that were anomalous. </p></li>
				<li>If we were to remove that query filter, we would see all of the messages in the log file at the time of this anomaly, and we would see the bigger story, which is that someone or something was attempting a lot of authentications: </li>
			</ol>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B17040_03_031.jpg" alt="Figure 3.31 – Inspecting all log lines during the time of the anomaly&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.31 – Inspecting all log lines during the time of the anomaly</p>
			<p>As we can see<a id="_idIndexMarker254"/> in <em class="italic">Figure 3.31</em>, there seems to be a flurry of authentication attempts using well-known usernames (<strong class="source-inline">user</strong>, <strong class="source-inline">test</strong>, and so on). Looks like we found a brute-force authentication attempt merely by using categorization!</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor078"/>When to avoid using categorization</h2>
			<p>Despite categorization <a id="_idIndexMarker255"/>being quite useful, it's not without its limitations. Specifically, here are some cases where attempting to use categorization will likely return poor results:</p>
			<ul>
				<li>With fields of text that are freeform, likely created by humans. Examples include tweets, comments, emails, and notes.</li>
				<li>With log lines that should otherwise really be parsed into proper name/value pairs, such as a web access log.</li>
				<li>With documents that contain a lot of multi-line text. This would include stack traces, XML, and so on.</li>
			</ul>
			<p>With that said, we can see that categorization can still be extremely useful in cases where analyzing <a id="_idIndexMarker256"/>unstructured text would otherwise be an increased burden on a human analyst.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor079"/>Managing Elastic ML via the API</h1>
			<p>As with just about <a id="_idIndexMarker257"/>everything in the Elastic Stack, ML can also be<a id="_idIndexMarker258"/> completely automated via API calls—including job configuration, execution, and result gathering. Actually, all interactions you have in the Kibana UI leverage the ML API behind the scenes. You could, for example, completely write your own UI if there were specific workflows or visualizations that you wanted.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more in-depth information about the anomaly detection APIs, please refer to <a href="http://elastic.co/guide/en/machine-learning/current/ml-api-quickref.html">elastic.co/guide/en/machine-learning/current/ml-api-quickref.html</a>. The data frame analytics part of Elastic ML has a completely separate API, which will be discussed in <em class="italic">Chapters 9</em> to <em class="italic">13</em>.</p>
			<p>We won't go into each API call, but we would like to highlight some parts that are worth a detour.</p>
			<p>The obvious first API to mention is the job creation API, which allows the creation of the ML job configuration. For example, if you wanted to recreate the population analysis job shown in <em class="italic">Figure 3.25</em>, the following call would create that job, which we will call <strong class="source-inline">revenue_over_users_api</strong>:</p>
			<p class="source-code">PUT _ml/anomaly_detectors/revenue_over_users_api</p>
			<p class="source-code">{</p>
			<p class="source-code">  "job_id": "revenue_over_users_api",</p>
			<p class="source-code">  "analysis_config": {</p>
			<p class="source-code">    "bucket_span": "15m",</p>
			<p class="source-code">    "detectors": [</p>
			<p class="source-code">      {</p>
			<p class="source-code">        "detector_description": "high_sum(taxful_total_price) over customer_full_name.keyword",</p>
			<p class="source-code">        "function": "high_sum",</p>
			<p class="source-code">        "field_name": "taxful_total_price",</p>
			<p class="source-code">        "over_field_name": "customer_full_name.keyword"</p>
			<p class="source-code">      }</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    "influencers": [</p>
			<p class="source-code">      "customer_full_name.keyword"</p>
			<p class="source-code">    ]</p>
			<p class="source-code">  },</p>
			<p class="source-code">  "data_description":{</p>
			<p class="source-code">         "time_field":"order_date",</p>
			<p class="source-code">         "time_format":"epoch_ms"</p>
			<p class="source-code">      }</p>
			<p class="source-code">}</p>
			<p>Note <a id="_idIndexMarker259"/>that<a id="_idIndexMarker260"/> the <strong class="source-inline">job_id</strong> field needs to be unique when creating the job. </p>
			<p>In order to create the companion datafeed configuration for this job, we would issue this separate API call:</p>
			<p class="source-code">PUT _ml/datafeeds/datafeed-revenue_over_users_api</p>
			<p class="source-code">{</p>
			<p class="source-code">  "datafeed_id": "datafeed-revenue_over_users_api",</p>
			<p class="source-code">  "job_id": "revenue_over_users_api",</p>
			<p class="source-code">  "query_delay": "60s",</p>
			<p class="source-code">  "indices": [</p>
			<p class="source-code">    "kibana_sample_data_ecommerce"</p>
			<p class="source-code">  ],</p>
			<p class="source-code">  "query": {</p>
			<p class="source-code">    "bool": {</p>
			<p class="source-code">      "must": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "match_all": {}</p>
			<p class="source-code">        }</p>
			<p class="source-code">      ]</p>
			<p class="source-code">    }</p>
			<p class="source-code">  },</p>
			<p class="source-code">  "scroll_size": 1000,</p>
			<p class="source-code">  "chunking_config": {</p>
			<p class="source-code">    "mode": "auto"</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>Notice that the <a id="_idIndexMarker261"/>default <a id="_idIndexMarker262"/>query to the index is <strong class="source-inline">match_all</strong>, which means that no filtering will take place. We could, of course, insert any valid Elasticsearch DSL in the query block to perform custom filters or aggregations. This concept will be covered later in the book.</p>
			<p>There are other APIs that can be used to extract results or modify other operational aspects of the ML job. Consult the online documentation for more information.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor080"/>Summary</h1>
			<p>We've seen that Elastic ML can highlight variations in volume, diversity, and uniqueness in metrics and log messages, including those that need some categorization first. Also, we've shown that population analysis can be an extremely interesting alternative to temporal anomaly detection when the focus is more on finding the most unusual entities. These techniques help solve the challenges we described before, where a human might struggle to recognize what is truly unusual and worthy of attention and investigation.</p>
			<p>The skills learned in this chapter will be helpful in subsequent chapters, where we will see how ML assists in the process of getting to the root cause of complex IT problems, identifying application performance slowdowns, or when ML can assist in the identification of malware and/or malicious activity. </p>
			<p>In the next chapter, we'll see how the expressive time series models built by anomaly detection jobs can be leveraged to forecast trends of your data into the future.</p>
		</div>
	</body></html>