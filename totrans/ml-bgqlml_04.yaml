- en: '*Chapter 3*: Introducing BigQuery Syntax'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The BigQuery dialect is compliant with the standard ANSI 2011 and is quite easy
    to learn for people who know other dialects and have experience with SQL. The
    main differences in terms of syntax are represented by BigQuery extensions, which
    allow us to use advanced features such as **Machine Learning** (**ML**). Bringing
    ML capabilities into SQL allows different roles to access it. This approach has
    the clear goal of democratizing the use of ML across different functions within
    a company, generating as much value as possible. With BigQuery ML, Google Cloud
    is filling the gap between tech-savvy people with ML skills and business analysts
    who know the company's data very well and have been working on it for years.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build your confidence with the BigQuery environment and its dialect, we''ll
    go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a BigQuery dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering BigQuery SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diving into BigQuery ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires access to a web browser and the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A GCP account to access Google Cloud Console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GCP project to host the BigQuery datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we're ready with the technical requirements, let's dive into the creation
    of a BigQuery dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/3vR8I7f](https://bit.ly/3vR8I7f)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a BigQuery dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before jumping into the BigQuery syntax, it is necessary to create a new BigQuery
    dataset that will employ the data structures created in the next sections. For
    each hands-on chapter, we''ll create a new dataset to segregate each use case
    and maintain a logical separated structure:'
  prefs: []
  type: TYPE_NORMAL
- en: Access the BigQuery UI by browsing to the GCP Navigation menu from the GCP console
    and selecting the **BigQuery** service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After selecting the right GCP project in the navigation menu of the BigQuery
    UI, it is possible to click on the **Create Dataset** button:![Figure 3.1 – Creation
    of a new BigQuery Dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16722_03_001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.1 – Creation of a new BigQuery Dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the overlay window that appears on the right of the screen, choose the `Dataset
    ID` that you prefer and leave all the other options configured with default values.
    To host the data structures of this chapter, we suggest using the name `03_bigquery_syntax`.
    Then, select **Create dataset**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Create dataset screen'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Create dataset screen
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've created our first BigQuery dataset, let's take an overview of
    the main characteristics of the BigQuery SQL syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering BigQuery SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BigQuery supports two different SQL dialects: **standard SQL** and **legacy
    SQL**. In this book, we''ll use Standard SQL, but it could be useful to know what
    Legacy SQL is and how to enable it if you want to test queries coming from legacy
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: As we have already mentioned, BigQuery was developed as an internal product
    within Google and was initially realized to process log records. The query engine
    Dremel was able to support a limited set of SQL operations that are now defined
    as Legacy SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see how to change the **SQL dialect**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Screenshot of the Query Settings menu to change SQL dialect'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Screenshot of the Query Settings menu to change SQL dialect
  prefs: []
  type: TYPE_NORMAL
- en: By default, the BigQuery UI is configured to use Standard SQL, but you are allowed
    to change the SQL dialect by using the specific option located in the `#legacySQL`
    keyword in the first line of your SQL statement. The **Query Settings** button
    is available under the **More** button in the BigQuery UI.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To develop new use cases, we suggest that you adopt BigQuery Standard SQL, but
    keep in mind that you could find existing applications that are still based on
    Legacy SQL. If you find a query that is not validated by the Query Editor, try
    to switch to Legacy SQL before intervening on the SQL statement.
  prefs: []
  type: TYPE_NORMAL
- en: CRUD operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paragraph, we'll learn how to perform the basic commands in order to
    **Create, Read, Update and Delete** (**CRUD**) objects in BigQuery. This is not
    an exhaustive view of all the operations that you can use with BigQuery, but the
    goal of this section is to provide you with the minimum knowledge needed to effectively
    face the next hands-on chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Create
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This category of statements is generally used to create objects in BigQuery
    such as tables, views, **User-Defined Functions** (**UDFs**), and machine learning
    models, or to insert new records into an existing table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let''s create a new empty table in BigQuery:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first two words of the query statement, `CREATE TABLE`, are self-explanatory
    and are used to start the creation of a new table. After that, we can find the
    identifier of the object that we''re creating. It is composed by the concatenation
    of the following strings separated by the `.` character:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The name of the GCP project: `bigqueryml-packt`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The identifier of the BigQuery dataset: `03_bigquery_syntax`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The name of the table to create: `first_table`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The string of the identifier is also enclosed by the backtick character, ``
    ` ``. This character delimits the beginning and the end of the name of our object.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Between the two round brackets, you can see the list of fields with their data
    type separated by the comma character. In this example, the table contains only
    two fields: the numerical `id_key` and the textual `description`.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: If a table with the same name already exists, it is possible to create a new
    table replacing the existing one using the `CREATE OR REPLACE TABLE` keywords.
    This technique is particularly useful when you need to periodically schedule your
    scripts running them multiple times. These keywords automatically clean the results
    of the previous executions.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we''ve created our first empty table, let''s `INSERT` our first record:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For the insertion of a new record into our `first_table`, we've used the `INSERT
    INTO` and `VALUES` keywords. Between the round brackets, we've listed the actual
    values to insert. In this case, we've chosen the integer number `1` as `id_key`
    and the string`'This is my first record inserted in BigQuery'` in single quotes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On top of a table, it is possible to create a `CREATE VIEW` statement is similar
    to the `CREATE TABLE` one, the only difference being that the view structure is
    based on the `SELECT` statement that follows the `AS` keyword. In this case, `first_view`
    has the same structure as `first_table` and doesn't apply any filters or transformations
    on the records stored in the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read operations are mainly based on `SELECT` statements and can be applied to
    different database objects such as tables and views.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute a `SELECT` statement on the `first_table` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To read data from a table or a view, it is necessary to use the `SELECT` keyword,
    followed by the list of the fields to read or the wildcard `*`, then the keyword
    `FROM` and the identifier of the source data structure. It is also possible to
    include a `WHERE` clause to express all the logical filters that we want to apply.
    In this case, we're picking up only the records with `id_key=1` that corresponds
    to the only record that we've previously inserted into the table.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Using the wildcard `*` is not recommended, especially on tables with a large
    number of columns. Since BigQuery has columnar storage, selecting only the fields
    that are really needed can dramatically improve the performance and decrease the
    computational cost of the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'With hierarchical queries with nested `SELECT` statements, the `WITH` clause
    can be used to improve the readability of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the first step, let''s create a nested `SELECT` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we can rewrite the same logic using the `WITH` clause. The query
    becomes this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the second query, the `WITH` clause embeds the logic that follows the `AS`
    keyword and is enclosed by round brackets. After the definition of the `WITH`
    clause with the name `records_with_clause`, the logic of this query can be recalled
    in the next `SELECT COUNT` statement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `WITH` clause doesn't create a temporary table. Using the `WITH` clause
    improves the readability of the query, especially if there are many nested `SELECT`
    statements, but it doesn't affect the performance of the query.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'BigQuery offers the possibility to leverage many other operators that will
    not be described in detail in this chapter because they will not be extensively
    used in the hands-on exercises. These additional operators allow you to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sort the results of a query according to a specific list of fields with the
    `ORDER BY` clause.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply aggregations on the query results with `COUNT`, `SUM`, `MAX`, `AVG`, and
    the `GROUP BY` and `HAVING` clauses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage the array data type using `NEST`, `UNNEST`, `ARRAY_AGG`, and `ARRAY_LENGTH`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join two or more tables with `INNER JOIN`, `LEFT OUTER JOIN`, `RIGHT OUTER JOIN`,
    and `CROSS JOIN`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although BigQuery was born as an analytic tool, update operations such as `UPDATE`
    and `MERGE` are supported and can be used to change existing records in BigQuery
    tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to change the value of a record or a set of records, we can use the
    `UPDATE` statement in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the first two lines of code, the `UPDATE` keyword is followed by the identifier
    of the table on which the operation should be applied. After that, the `SET` keyword
    defines the columns that should be changed. In this case, the `description` will
    be modified.
  prefs: []
  type: TYPE_NORMAL
- en: The `WHERE` clause allows you to apply the `UPDATE` operations only to the records
    that match the filter. In this case, only the records with an `id_key` equal to
    `1`.
  prefs: []
  type: TYPE_NORMAL
- en: The other powerful statement to update a table is the `MERGE` function. This
    function can combine the records of two different tables applying insert, update,
    and delete operations in a single SQL statement.
  prefs: []
  type: TYPE_NORMAL
- en: Delete
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Delete operations are particularly useful to delete records or remove objects
    from BigQuery preventing storage costs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we can delete a record from the `first_table` table, using
    the `DELETE` statement as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we analyze the SQL code, we can see that the `DELETE` keyword is followed
    by the identifier of the table on which the operation should be applied. The `WHERE`
    clause filters the set of records to delete. In this case, only the record with
    an `id_key` equal to `1` is affected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Another way to remove records from a table is using the `TRUNCATE TABLE` operator.
    This function allows you to remove all the records with a single statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the `TRUNCATE`, our `first_table` will continue to exist but will not
    contain any records.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To delete the entire table, including its structure, we can use the `DROP TABLE`
    keywords:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dropping a table removes it from the dataset, making the data structure inaccessible.
    If we explore the list of objects of the `03_bigquery_syntax` dataset, we can
    see that the `first_table` table is no longer visible:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The table that was affected by the DROP TABLE statement is no
    longer visible'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16722_03_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.4 – The table that was affected by the DROP TABLE statement is no longer
    visible
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, an interesting aspect is that `first_view`, created on top of
    the `first_table` table, is still visible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we try to execute a `SELECT` statement on it, the following error will be
    raised:![Figure 3.5 – Querying a view when the underlying table was dropped raises
    an error
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16722_03_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – Querying a view when the underlying table was dropped raises an
    error
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The error, generated by BigQuery, notifies the user that the underlying table
    is no longer available and cannot be found.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To keep our dataset consistent, it is better to also drop the view with the
    `DROP VIEW` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Dropping a view is similar to dropping a table, but this operation affects only
    the metadata because the view doesn't actually store any records.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section of the chapter, we've discovered the main operations that we
    can do with BigQuery SQL; now it's time to dive into BigQuery ML and its syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into BigQuery ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developing an ML model in BigQuery involves three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model creation**, where you are required to choose the **features** and **labels**
    of your ML model and the options to tune the ML model. At this stage, BigQuery
    runs the training of the ML model on the training set that you''ve chosen.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model evaluation** allows you to test the model trained in the previous step
    on a different set of records to prevent any **overfitting**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model use**: when the ML model is ready, we can apply it to a new dataset
    in order to make predictions or classifications of the labels according to the
    available features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next paragraphs, we'll take a look at the syntax of these three stages
    and how these statements are built using stubs of code.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the ML model (training)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you''ve identified the ML use case and also the set of records to train
    your model, you can start training the model with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Very similarly to the creation of a BigQuery table, the statement to train
    a new model consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `CREATE MODEL` keywords.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the name of the new ML model. This identifier is composed of the concatenation
    of the project name, dataset name, and ML model name, separated by the `.` character
    and enclosed by backticks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TRANSFORM` clause is not mandatory but is very useful. It allows us to
    list all the preprocessing transformations applied to the features before training.
    Putting the preparation functions here allows us to automatically apply the same
    actions during the actual use of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of `OPTIONS` requires us to specify the `model_type` that we want to
    use, such as linear regression or logistic regression. This list of options is
    also used to select the list of labels of the ML model through the `input_label_cols`
    keyword. Other options can be used to tune the ML model and will be explained
    in the next chapters of the book, with the hands-on exercises.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `AS` keyword followed by the `SELECT` statement. This statement defines
    the set of records on which the ML model will be trained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to the `CREATE MODEL` statement, we can also use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CREATE OR REPLACE MODEL` to create a new model or replace the existing one
    with the same name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CREATE MODEL IF NOT EXISTS` to train the new model only if a model with the
    same name doesn''t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we''ve understood how to create an ML model in BigQuery ML, let''s
    take a look at the next phase: the evaluation of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After training an ML model on a set of records, it is extremely important to
    evaluate its performances on a second dataset that's different from the training
    one to avoid any **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: With the term overfitting, we refer to a situation that could happen when the
    ML model learns very well from the training dataset but performs negatively on
    new ones. This usually happens when the model adheres too much to the details
    of the training dataset and remains conditioned by the noise present in it.
  prefs: []
  type: TYPE_NORMAL
- en: According to the ML algorithm that we've chosen during the model creation, we
    can choose among different evaluation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function can be used with linear regression, logistic regression, *k*-means
    clustering, matrix factorization, and time-series models based on ARIMA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ML.EVALUATE` function returns only one record with the key performance
    indicators of the ML model that we''ve trained and evaluated. The indicator it
    returns depend on the model type. The query stub is composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An initial `SELECT *` statement that allows us to retrieve all the fields returned
    by the evaluation stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The call of the evaluation function from the `ML` package: `ML.EVALUATE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The identifier of the ML model with the syntax that we already know very well:
    project, dataset, and model name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<evaluation_table>` on which the ML model will be evaluated. This table
    can be replaced by a `SELECT` statement and is not mandatory. If you don't provide
    the table for the evaluation stage, BigQuery will use the entire training set
    or a portion of it to evaluate your ML model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional `<threshold>` that can be used to evaluate logistic regression models.
    If this value is not specified, BigQuery will use `0.5` as the default value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To use the `ML.EVALUATE` function, the name of the fields of the evaluation
    set should correspond to the name of the fields of the training dataset that we've
    used during the model creation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Confusion matrix function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function returns a confusion matrix to evaluate the performances of logistic
    regression and multiclass logistic regression models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This function returns two rows and two columns that contain the number of false
    positives, false negatives, true positives, and true negatives. Compared to the
    `EVALUATE` function, the only difference in terms of syntax is represented by
    the use of the `ML.CONFUSION_MATRIX` function.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function can be used only with logistic regression models and returns
    multiple records according to the array of thresholds that are passed as input
    to the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The only meaningful difference that we can see from the other evaluation functions
    that we analyzed in the previous paragraphs is the presence of an array of thresholds.
    The `GENERATE_ARRAY` function creates an array that contains the values of the
    thresholds separated by the comma character and enclosed by round brackets.
  prefs: []
  type: TYPE_NORMAL
- en: The output of this function includes the threshold passed in input, the recall
    value, the rate of false positives, and the number of true positives, false positives,
    true negatives, and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: We have been through all the evaluation techniques of BigQuery ML models, now
    it's time to see how to apply them and get the results.
  prefs: []
  type: TYPE_NORMAL
- en: Using the ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we're satisfied with the performance of our ML model, the next step is
    to use it to achieve our outcomes and finally get business value from the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Predict function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function is applicable to linear regression, logistic regression, multiclass
    logistic regression, *k*-means clustering, and imported TensorFlow models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The query is composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `SELECT * FROM` statement to get all the records and fields returned by
    `ML.PREDICT` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ML.PREDICT` keyword, which accepts as input the name of the ML model to
    use for the prediction (`<ml_model_name>`) and the table (`<features_table>`)
    that contains the features to execute the predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, you can use a `<threshold>` value for the logistic regression models
    followed by the `AS threshold` keywords.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ML.PREDICT` function generates and returns a record for each row present
    in `<features_table>.` Each row is composed of the features and the predicted
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Forecast function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function can only be used for time-series ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike the `PREDICT` statement, it doesn''t require a table as input. It allows
    us to choose the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A specific `<horizon_value>`. The horizon represents the number of time points
    that should be forecast. If you don't specify this value, BigQuery will use `3`
    as the default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `confidence_level`, which represents the percentage of the forecast values
    that reside in the interval of the prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommend function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function can only be used for matrix factorization ML models. It returns
    a rating for each combination of user and item in the `<user_item_table>` table
    or in the training table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The query is composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `SELECT * FROM` statement to get all the records and fields that come from
    the outcome of the `ML.RECOMMEND` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ML.RECOMMEND` keyword, which accepts as input the name of the ML model
    to use for the prediction (`<ml_model_name>`) and, optionally, the input table
    (`<user_item_table>`), which contains the user and items. If the table is not
    provided, BigQuery will use the entire training table for the recommendation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've learned how to apply a BigQuery ML model; if the model is no longer needed,
    it is best to delete it to save resources. Let's take a look at how we can do
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting an ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deleting an ML model is quite straightforward, and the syntax is very similar
    to the cancellation of a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With the `DROP MODEL` keywords followed by the identifier of the BigQuery ML
    model, you can remove the asset from your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `DROP MODEL IF EXISTS` keywords, which prevents errors
    if the BigQuery ML model has been already deleted. This operation removes the
    model only if it is present in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: When the model is deleted, we can be sure that no resources are consumed to
    keep it active in BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned the main aspects of the BigQuery syntax. After
    the creation of a dataset, we've discovered how to create tables, insert records,
    and read the rows stored in a table. You've also learned how to update existing
    records and how to remove rows and delete objects that are no longer useful, such
    as tables and views.
  prefs: []
  type: TYPE_NORMAL
- en: Completing the overview of the BigQuery SQL syntax, we dived into the main stages
    of the life cycle of an ML model. The three main phases to realize a use case
    are the creation, the evaluation, and the use of the ML model. For the training
    phase, we have found out how to train and create a new model using SQL. After
    that, we went through all the functions that can be used to monitor the effectiveness
    of a trained model, evaluating its key performance indicators. Finally, we saw
    how to use a trained model on a new dataset to infer the results and get predictions,
    forecasts, or recommendations. At the end of the chapter, we also learned how
    to delete BigQuery ML models that are no longer useful.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a clear understanding of the syntax and all the capabilities
    that we can use in BigQuery, it's time to apply all these concepts to our first
    hands-on use case. In the next chapter, we will develop our first BigQuery ML
    model to predict the estimated duration of a bike trip for an important bike rental
    service in New York City.
  prefs: []
  type: TYPE_NORMAL
- en: Further resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BigQuery datasets**: [https://cloud.google.com/bigquery/docs/datasets](https://cloud.google.com/bigquery/docs/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery SQL syntax**: [https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery data types**: [https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create model syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion matrix syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-confusion](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-confusion)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROC curve syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-roc](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-roc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predict syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forecast syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-forecast](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-forecast)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommend syntax**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-recommend](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-recommend)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
