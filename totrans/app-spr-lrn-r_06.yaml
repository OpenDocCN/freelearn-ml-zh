- en: '*Chapter 6:*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Selection and Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement feature engineering techniques such as discretization, one-hot encoding,
    and transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute feature selection methods on a real-world dataset using univariate feature
    selection, correlation matrix, and model-based feature importance ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply feature reduction using principal component analysis (PCA) for dimensionality
    reduction, variable reduction with clustering, and linear discriminant analysis
    (LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement PCA and LDA and observe the differences between them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will explore the feature selection and dimensionality reduction
    methods to build an effective feature set and hence improve the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last two chapters (on regression and classification), we focused on understanding
    and implementing the various machine learning algorithms in the supervised learning
    category on a given dataset pertaining to a problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus more on effectively using the features of the
    dataset to build the best performing model. Often in many datasets, the feature
    space is quite large (with many features). The model performance takes a hit as
    the patterns are hard to find and often much noise is present in the data. Feature
    selections are specific methods that are used to identify the importance of each
    feature and assign a score to each. We can then select the top 10 or 15 features
    (or even more) based on the score for building our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another possibility is to create new variables using a linear combination of
    all the input variables. This helps in keeping the representation of all variables
    and reducing the dimensionality of feature space. However, such a reduction often
    reduces the explainable variance. In this chapter, we will focus on the three
    major actions we perform on the dataset for improving model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering:** Essentially transforms the features so the machine
    learning algorithms will work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selection:** Selects the features with high importance to bring out the best
    performance of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduction:** Reduces the feature dimensionality by representing a higher
    order dataset into a lower dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three are closely related yet different in how they function.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, in addition to the Beijing PM2.5 dataset, we will use the Los
    Angeles ozone pollution data, 1976, provided in the `mlbench` library of R. It
    is a data frame with 366 observations on 13 variables, where each observation
    is of one day.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: List of variables in Los Angeles ozone pollution data, 1976'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1: List of variables in Los Angeles ozone pollution data, 1976'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Originally, the dataset was provided for the problem of predicting the daily
    maximum one-hour-average ozone readings (the fourth variable in the table). Both
    Beijing PM2.5 and Los Angeles ozone datasets resonate with the effects of pollutants
    on our environment.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithms we use in machine learning will perform based on the quality
    and goodness of the data; they do not have any intelligence of their own. The
    better and innovative you become in designing features, the better the model performance.
    **Feature engineering** in many ways helps in bringing the best out of data. The
    term feature engineering essentially refers to the process of the **derivation**
    and **transformation** of given features, thus better characterizing the meaning
    of the features and representing the underlying problem of the predictive model.
    By this process, we anticipate the improvement in the model's **predictability
    power** and **accuracy**.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Introduction to Supervised Learning*, we converted the numeric
    values of a 3-hour rolling average of PM2.5 in the Beijing dataset to the binary
    values 1 and 0 for logistic regression, based on the threshold of 35, where 1
    means **normal** and 0 means **above normal**. The process is called **discretization**,
    also commonly referred to as **binning, or** in our case, **binary discretization**.
    More broadly, in applied mathematics, discretization is the process of transferring
    continuous functions, models, variables, and equations into discrete counterparts.
    Now, let's perform the process on a variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 77: Performing Binary Discretization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create a binary variable using the `pm2.5` variable
    of the Beijing PM2.5 dataset. Binary discretization of the `pm2.5` variable will
    create a column that will be 1 if the PM2.5 level is greater than 35, else it
    will be 0\. This process will help us create a discrete categorical variable (to
    be called `pollution_level`) from a continuous numeric variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with reading the Beijing PM2.5 dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine year, month, day, and hour into a `datetime` variable using the with
    function from the lubridate package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, remove any row with an NA in the column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the zoo structure, compute the moving average every `3` hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, convert the output of the moving average into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, put the timestamp in the row names into the main columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get rid of the row names (optional):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rename the columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two levels based on the PM2.5 average. `0` implies `1` implies **above
    the normal**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Randomly select 10 rows using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the output using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe that the variable `pollution_level` is now a binary categorical variable,
    which we created in Step 11\. The dataset with `pollution_level` as an output
    variable could be used with any classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Category Discretization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more general form of discretization is to divide the range of values of a
    continuous variable into many smaller ranges of values using appropriate cut-points.
    One way of identifying the appropriate cut-point is to analyze the distribution
    of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following code, plot a histogram of `avg_pm25` with `binwidth` of
    `30 (`meaning the range of values will be divided into ranges of size `30)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.2: Histogram of 3-hour rolling average of PM2.5 values from Beijing
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2: Histogram of 3-hour rolling average of PM2.5 values from Beijing
    dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The plot in *Figure 6.2* shows the right skewness in the variable, which means
    the majority of values are on the left of the range of values, mostly concentrated
    between 0 and 250\. Such skewness inhibits the model from generalizing, hence
    it has a lower predictive power. Now, let's explore how we can utilize multi-category
    discretization to improve this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 78: Demonstrating the Use of Quantile Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will demonstrate the use of the `avg_pm25`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find the quantiles on `avg_pm25`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, calculate the vertical lines on the quantile points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12624_06_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.3 Histogram of 3-hour rolling average of PM2.5 values from Beijing
    dataset with cut-lines corresponding to 0th, 25th, 50th, 75th, and 100th percentile
    points
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The following code snippet creates the variable `avg_pm25_quartiles` in the
    dataset, which represents the five percentile points on the values of `avg_pm25`.
    This new variable could be used in modeling after **one-hot encoding,** which
    we will discuss in the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s use the following code to add a new variable `avg_pm25_quartiles` in
    the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have just seen how discretization helps to remove any data skewness before
    modelling.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-hot encoding is a process of binarizing the categorical variable. This is
    done by transforming a categorical variable with *n* unique values into *n* unique
    columns in the datasets while keeping the number of rows the same. The following
    table shows how the wind direction column is transformed into five binary columns.
    For example, the row number **1** has the value **North**, so we get a **1** in
    the corresponding column named **Direction_N** and **0** in the remaining columns.
    So on for the other rows. Note that out of these sample five rows of data, the
    direction **West** is not present. However, the larger dataset would have got
    the value for us to have the column **Direction_W**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 Transforming a categorical variable into Binary 1s and 0s using
    one-hot encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 Transforming a categorical variable into Binary 1s and 0s using one-hot
    encoding
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One primary reason for converting categorical variables (such as the one shown
    in the previous table) to binary columns is related to the limitation of many
    machine learning algorithms, which can only deal with numerical values. However,
    in order to convert the categorical variables into a numerical variable, we have
    to represent it with some mapping value, such as `North = 1`, `South = 2`, `West
    = 3`, and so on. The problem with such encoding is that the values `1`, `2`, and
    `3` are integers, where `3>2>1`; however, this is not the case with wind direction.
  prefs: []
  type: TYPE_NORMAL
- en: The interpretation is entirely wrong. Binary one-hot encoding overcomes this
    challenge by creating one column for each value in the categorical variable, thus
    giving us a more elegant representation. We can now use any algorithm from machine
    learning on such data as long as it satisfies the type of problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 79: Using One-Hot Encoding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use the one-hot encoding for creating one column for
    each value in the categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `OzoneData` variable and store the value of `ozone1.csv` using the
    `read.csv` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the required `caret` packages into the system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create input datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the response DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the data using the `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe the `OneHot` variable we have created in the `OzoneData` DataFrame.
    After one-hot encoding, each value (1 to 7) in `Day_of_week` is represented as
    a separate column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11: Converting the CBWD Feature of the Beijing PM2.5 Dataset into
    One-Hot Encoded Columns'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will learn how to convert any categorical variable into
    a one-hot encoded vector. Particularly, we will convert the CBWD feature of the
    Beijing PM2.5 dataset into one-hot encoded columns. Many machine learning algorithms
    work only on numerical features; in such cases, it becomes imperative to use one-hot
    encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the Beijing PM2.5 dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable `cbwd_one_hot` for storing the result of the `dummyVars` function
    with `~ cbwd` as its first argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the output of the `predict()` function on `cbwd_one_hot`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the original `cbwd` variable from the `PM25` DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `cbind()` function, add `cbwd_one_hot` to the `PM25` DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the top six rows of `PM25`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 459.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Log Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common technique to correct for skewed distribution is to find an
    appropriate mathematical function that has an inverse. One such function is a
    log, represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, ![](img/C12624_06_16.png) is the ![](img/C12624_06_17.png)
    of ![](img/C12624_06_18.png) to the base ![](img/C12624_06_19.png). The inverse,
    to find the ![](img/C12624_06_19a.png), can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This transformation gives the ability to handle the skewness in the data; at
    the same time, the original value can be easily computed once the model is built.
    The most popular log transformation is the natural ![](img/C12624_06_21.png),
    where ![](img/C12624_06_22.png) is the mathematical constant ![](img/C12624_06_23.png),
    which equals roughly `2.71828`.
  prefs: []
  type: TYPE_NORMAL
- en: One useful property of the log function is that it handles the data skewness
    elegantly. For example, the following code demonstrates the difference between
    `log(10000)` and `log(1000000)` as just `4.60517`. The number ![](img/C12624_06_24.png)
    is 100 times bigger than ![](img/C12624_06_25.png). This reduces the skewness
    that we otherwise let the model handle, which it might not do sufficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let's see the result of applying the natural log on the 3-hour rolling average
    of the PM2.5 values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 80: Performing Log Transformation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will draw a histogram of the `avg_pm25` variable with log
    transformation and compare it with the skewed distribution of the original values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a histogram of `avg_pm25`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.5 Histogram of the 3-hour rolling average of PM2.5 values from the
    Beijing dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_06_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.5 Histogram of the 3-hour rolling average of PM2.5 values from the
    Beijing dataset
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a histogram of `log_avg_pm25`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6 Histogram of the natural log of the 3-hour rolling average of
    PM2.5 values from the Beijing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 Histogram of the natural log of the 3-hour rolling average of PM2.5
    values from the Beijing dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we drew a plot to show the 3-hour rolling average of the PM2.5
    values from the Beijing dataset and contrasted it with the histogram of the natural
    log of the 3-hour rolling average of the PM2.5 values from the Beijing dataset.
    Taking the log made the histogram look more symmetrical around the mean and the
    skewness.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While **feature engineering** ensures that the quality and data issues are rectified,
    **feature selection** helps with determining the right set of features for improving
    the performance of the model. Feature selection techniques identify the features
    that contribute the most in the prediction ability of the model. Features with
    less importance inhibit the model's ability to learn from the independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature selection offers benefits such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the time to train the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate Feature Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A statistical test such as the **chi-squared** ![](img/C12624_06_26.png) test
    is a popular method to select features with a strong relationship to the dependent
    or target variable. It mainly works on categorical features in a classification
    problem. So, for this to work on a numerical variable, one needs to make the feature
    into categorical using discretization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the most general form, chi-squared statistics could be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This tests whether or not there is a significant difference between observed
    frequency and expected frequency. A higher chi-squared value establishes a stronger
    dependence of the target variable and the particular feature. More formally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_29_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Exercise 81: Exploring Chi-Squared'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will compute the chi-squared statistic for all the variables
    in the `Ozone` dataset. The top five variables with the highest chi-squared value
    will be our best feature for modelling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a variable named `OzoneData` and assign the value from the `read.csv`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, set the `path` as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, use the Sys.getenv function to obtain the values of the environment variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the required packages using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `rJava` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the chi-squared statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the top five variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the final formula that can be used for classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used the chi.squared() function to compute the chi-squared values for each
    feature in our Ozone dataset. The function outputs the attribute importance based
    on the chi-squared value. The formula in Step 10 that uses the top five features
    from the chi-squared statistic could be used for building a supervised learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Highly Correlated Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, two highly correlated variables likely contribute to the prediction
    ability of the model, which makes one redundant. For example, if we have a dataset
    with `age`, `height`, and `BMI` as variables, we know that `BMI` is a function
    of `age` and `height` and it will always be highly correlated with the other two.
    If it's not, then something is wrong with the BMI calculation. In such cases,
    one might decide to remove the other two. However, it is always not this straight.
    In certain cases, a pair of variables might be highly correlated, but it is not
    easy to interpret why that is the case. In such cases, one can randomly drop one
    of the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 82: Plotting a Correlated Matrix'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will compute the correlation between a pair of variables
    and draw a correlation plot using the `corrplot` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, load the data and calculate the correlation matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Summarize the correlation matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find attributes that are highly correlated (ideally >0.75):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the indexes of the highly correlated attributes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `corrplot` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the correlation matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7: Plotting correlated matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.7: Plotting correlated matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Observe in *Figure 6.7* that the dark blue circles represent high positive correlation
    and the dark red circles represent high negative correlation. The range of correlation
    values is between `-1` and `1`. Visually inspecting, we can see the variable `Inversion_temperature`
    has high positive correlation with `pressure_height` and high negative correlation
    with `Inversion_base_height`. For example, if `Inversion_temperature` increases,
    `pressure_height` will also increase and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure 6.7 can be found on GitHub: https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson06/C12624_06_07.png.'
  prefs: []
  type: TYPE_NORMAL
- en: Model-Based Feature Importance Ranking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A model such as random forest, an ensemble modeling technique where we build
    several models and combine their results using a simple voting technique (as described
    in *Chapter 5*, *Classification)*, has a useful technique to utilize all the variables
    in a dataset and at the same time not compromise the model performance. The simple
    idea behind the random forest model is that it randomly selects a subset of data
    and variables to build many decision trees. The final model prediction happens
    through not one decision tree but collectively using many decision trees. Majority
    voting is a commonly used technique for final prediction; in other words, what
    the majority of the decision tree predicts is the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The technique naturally gives a combination of variables that result in highest
    accuracy. (Other model evaluation metrics could also be used.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For certain research work in genomics and computational biology*****, where
    potential predictor variables vary in their scale of measurement (input features
    including both sequence and categorical variables such as folding energy) and
    their number of categories (for example, when amino acid sequence data show different
    numbers of categories), random forest importance measures are not reliable.
  prefs: []
  type: TYPE_NORMAL
- en: '***** Bias in random forest variable importance measures: Illustrations, sources
    and a solution: https://link.springer.com/article/10.1186/1471-2105-8-25.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 83: Exploring RFE Using RF'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will explore **recursive feature elimination** (**RFE**)
    using the random forest algorithm. RFE helps in selecting the best features with
    highest feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `party` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the random forest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the variable importance, based on a mean decrease in MSE. The `varimp()`
    function implements the RFE technique:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In Step 2, the `party` package provides the method `cforest()`, which fits a
    random forest model using the parameter `mtry = 2` and `ntree = 50` and finds
    the best model where the `x`, using only the trees that did not have `x` in their
    bootstrap sample. The function `varimp()` returns the variable importance using
    the permutation principle (with values randomly shuffled) of the mean decrease
    in MSE. In other words, variable importance is measured as the mean decrease of
    the MSE over all out-of-bag cross-validated predictions, when a given variable
    is permuted after training but before prediction.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of randomly shuffled (permuted) variables, we expect a *bad* variable
    to be created and inclusion of this shuffled variable to increase the MSE compared
    to when it is not included in the model. Hence, if the mean decrease in the MSE
    is high, the MSE of the model as a result of shuffling of the variable has got
    to be high. So, we can conclude the variable has higher importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 84: Exploring the Variable Importance using the Random Forest Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will explore the variable importance using the random forest
    model. We will again use the Beijing dataset to see which among the five variables
    (`month`, `DEWP`, `TEMP`, `PRES`, and `Iws)` predicts the PM2.5 the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the randomForest package using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create a new object using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the R-squared value for each tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, calculate the variable importance plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8: Percentage increase in MSE and increase in node purity value
    obtained by fitting the randomForest model on the Beijing PM2.5 data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.8: Percentage increase in MSE and increase in node purity value obtained
    by fitting the randomForest model on the Beijing PM2.5 data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The previous exercise demonstrates another way to look at the variable importance.
    Instead of the `party` package, we have used the `randomForest` package. `%IncMSE`
    is computed as described in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Fit random forest (in our case, it's a regression random forest). Compute OOB-MSE
    and name this `MSE_Base`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each variable `j`: permute values of column `j`, then predict and compute
    `OOB_MSE_j`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`%IncMSE` of the `jth` variable equals `(OOB_MSE_j - MSE_Base)/ MSE_Base *
    100%`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 6.8* shows that inclusion of the variable `Iws` in the model increases
    the MSE by `22%` compared with the variable DEWP, which increases the MSE only
    by `15%`. We know that as a result of the shuffled values of the variable, the
    MSE is bound to increase, so the higher `%` implies a good variable. If we see
    the variable `TEMP`, the shuffling of values has not impacted the MSE that much
    compared with `Iws` and `DEWP`; hence, relatively, it is less important.'
  prefs: []
  type: TYPE_NORMAL
- en: Node purity computes the value of loss function, which in this model is MSE.
    It helps in choosing the best split. Decrease in the MSE gives a higher node purity
    value. DEWP has the highest node purity followed by the feature month. In our
    dataset, both `%IncMSE` and `IncNodePurity` show similar results. However, keep
    in mind that `IncNodePurity` is often biased and should always be seen in conjunction
    with `%IncMSE`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Feature reduction** helps get rid of redundant variables that reduce the
    model efficiency in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Time to develop/train the model increases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretation of the results becomes tedious.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It inflates the variance of the estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see three feature reduction techniques that help in
    improving the model efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N. A. Campbell and William R. Atchley in their classic paper, *The Geometry
    of Canonical Variate Analysis,* Systematic Biology, Volume 30, Issue 3, September
    1981, Pages 268–280, geometrically defined *a principal component analysis as
    a rotation of the axes of the original variable coordinate system to new orthogonal
    axes, called principal axes, such that the new axes coincide with directions of
    maximum variation of the original observation*. This forms the crux of what PCA
    does. In other words, it represents the original variable with principal components
    that explain the maximum variation of the original observations or data.
  prefs: []
  type: TYPE_NORMAL
- en: The paper elegantly presents the geometrical representation of principal components
    as shown in the following figure, which is a representation of the scatter diagram
    for two variables, showing the mean for each variable ![](img/C12624_06_34.png),
    *95%* concentration ellipse, and principal axes ![](img/C12624_06_35.png) and
    ![](img/C12624_06_36.png). The points ![](img/C12624_06_37.png) and ![](img/C12624_06_38.png)
    give the principal component scores for the observation ![](img/C12624_06_39.png)
    = . The cosine of the angle ![](img/C12624_06_40.png) between ![](img/C12624_06_41.png)
    and ![](img/C12624_06_42.png) gives the first component ![](img/C12624_06_43.png)
    of the eigenvector corresponding to ![](img/C12624_06_44.png).
  prefs: []
  type: TYPE_NORMAL
- en: In linear algebra, an eigenvector of a linear transformation is a non-zero vector
    that changes by only a scalar factor when that linear transformation is applied
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Shows the representation of the scatter diagram for two variables,
    showing the mean for each variable (x ̅_1 and x ̅_2), 95% concentration ellipse,
    and principal axes Y_1 and Y_2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Source: The Geometry of Canonical Variate Analysis, Systematic Biology, Volume
    30, Issue 3, September 1981, Pages 268–280'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 85: Performing PCA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform PCA, which will help reduce the dimensionality
    of the feature space. In other words, fewer principal components that are the
    linear combination of input features will represent the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `OzoneData` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the column name using the `colnames` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the means for all variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the variance of all variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Significant differences in variance of the variables will control the principal
    components. `prcomp()` will standardize the variables (mean `0` and variance `1`)
    before finding out the principal component.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, find the summary of the PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a biplot using the `ggbiplot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10 Scaled biplot of the first two principle components using ggbiplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 Scaled biplot of the first two principle components using ggbiplot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The biplot in the figure shows how `Ozone` dataset. As shown in the output of
    `summary(pca.out)`, biplots depict the explained variance by using the various
    features in the dataset. The axes are seen as arrows originating from the center
    point. The figure also shows that the variables `pressure_height` and `inversion_temperature`
    contribute to `Visibility` and `day_of_the_week` contribute to **PC2** with higher
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you find difficulty installing `ggbiplot`, you could also use the `biplot()`
    function from base R, as shown in the following plot. First, let''s build a biplot
    to understand better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.11 Scaled biplot of the first principle component'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 Scaled biplot of the first principle component
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Observe that the maximum percentage of variance is explained by PC1 and all
    PCs are mutually uncorrelated. In particular, around `40%` of the variance is
    explained by PC1, and the first principal component (PC1-PC4) explains 70% of
    the variance. In other words, if we use the first four principal components, we
    should get a model almost similar to the one we would get when we use all the
    variables in the dataset. This should not be surprising as the principal component
    is a linear combination of the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Variable Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Variable clustering** is used for measuring collinearity, calculating redundancy,
    and for separating variables into clusters that can be counted as a single variable,
    thus resulting in data reduction. Hierarchical cluster analysis on variables uses
    any one of the following: Hoeffding''s D statistics, squared Pearson or Spearman
    correlations, or uses as a similarity measure the proportion of observations for
    which two variables are both positive. The idea is to find the cluster of correlated
    variables that are correlated with themselves and not with variables in another
    cluster. This reduces a large number of features into a smaller number of features
    or variable clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 86: Using Variable Clustering'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use feature clustering for identifying a cluster of
    similar features. From each cluster, we can select one or more features for the
    model. We will use the hierarchical cluster algorithm from the Hmisc package in
    R. The similarity measure should be set to "spear," which stands for the Pearson
    correlation, a robust measure for computing the similarity between two observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `Hmisc` package using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `Hmisc` package and set the seed to `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use variable clustering with Spearman correlation as the similarity measure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on the similarity matrix, the following figure shows the plot of variables
    in the same cluster. For example, `Temperature_ElMonte` and `Inversion_temperature`
    are both clustered into one cluster with a Spearman correlation score of 0.85\.
    Similarly, `Humidity` and `Pressure_gradient` have a Spearman correlation of 0.25\.
    A high similarity would entail the decision of dropping one of them. In addition
    to the top of the output of the cluster, one should also consider the model metrics
    before taking the final call of dropping the variable completely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.12: Hierarchical cluster of variables in Ozone dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.12: Hierarchical cluster of variables in Ozone dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis for Feature Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis** (**LDA**) helps in maximizing the class separation
    by projecting the data into a new feature space: lower dimensional space with
    good class separability in order to avoid overfitting (*curse of dimensionality*).
    LDA also reduces computational costs, which makes it suitable as a classification
    algorithm. The idea is to maximize the distance between the mean of each class
    (or category) and minimize the variability within the class. (This sounds certainly
    like how the clustering algorithm in unsupervised learning works, but we will
    not touch that here as it is not in the scope of this book.) Note that LDA assumes
    that data follows a Gaussian distribution; if it''s not, the performance of LDA
    will be reduced. In this section, we will use LDA as a feature reduction technique
    rather than as a classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the two-class problem, if we have an *m*-dimensional dataset ![](img/C12624_06_45.png)
    with *N* observations, of which ![](img/C12624_06_46.png) belongs to class ![](img/C12624_06_47.png)
    and ![](img/C12624_06_48.png) belongs to class ![](img/C12624_06_49.png). In this
    case, we can project the data onto a line (with *C=2*, project into *C-1* space):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_06_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Such a projection is achieved by projecting the mean of *X* onto the mean of
    *Y*. Of all the lines possible, we would like to select the one that maximizes
    the separability of the scalars. In other words, where the projections of observation
    from the same class are projected very close to each other and, at the same time,
    the projected means are as far apart as possible.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that while in LDA we use the class variable more like supervised
    learning, PCA does not need any class variable to reduce the feature size. That
    is why, while LDA preserves as much of the class discriminatory information as
    possible, PCA does not much care about it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13: Comparing PCA and LDA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_06_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.13: Comparing PCA and LDA'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Source: https://sebastianraschka.com/Articles/2014_python_lda.html'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 87: Exploring LDA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform LDA for feature reduction. We will observe
    the difference in the model performance with all the features and the reduced
    features using LDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Merge the two DataFrames on timestamp to stack other environmental variables
    along with PM2.5 into one DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the dataset into train and test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `MASS` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the LDA model on the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot 100 randomly selected projected values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14: Plot of randomly selected 100 projected values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_06_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.14: Plot of randomly selected 100 projected values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform the model testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `caret` library and print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the dimension-reduced dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the projected data. Let''s fit a logistic model. You could use any other
    classification model as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform the model evaluation on testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict 1 if probability > 0.5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the accuracy in `LDA_test` and the projected `new_LDA_test` are strikingly
    similar. This indicates that the projected values in the new lower dimensional
    space perform equally well compared with the original. It might always not be
    the case that the new space will result in the same performance as the original.
    Therefore, a thorough scrutiny is required before reducing the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we saw various feature selection and reduction techniques.
    The three main topics covered in this chapter were: Feature Engineering, Feature
    Selection, and Feature Reduction. The latter two have the same purpose of shrinking
    the number of features; however, the techniques used are completely different.
    Feature Engineering focuses on transforming variables into a new form that either
    helps in improving the model performance or makes the variable be in compliance
    with model assumption. An example is the linearity assumption in the linear regression
    model, where we typically could square or cube the variables and the skewness
    in data distribution, which could be addressed using log transformation. Feature
    Selection and Feature Reduction help in providing the best feature set or the
    best representation of the feature set, which improves model performance. Most
    importantly, both techniques shrink the feature space, which drastically improves
    the model training time without compromising the performance in terms of accuracy,
    **RMSE,** or any relevant model evaluation metric.'
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how some of the models themselves, such as random forest and **LDA,**
    could directly be used as feature selection and reduction techniques. While random
    forest works by selecting the best features through a method of random selection,
    LDA works by finding the best representation of features. Thus, the former is
    used in feature selection and the latter in reduction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more about model improvement, where some
    of the learnings from this chapter will be employed.
  prefs: []
  type: TYPE_NORMAL
