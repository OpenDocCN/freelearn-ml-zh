- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing Data in Machine Learning Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We talked about data in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038), where
    we introduced the types of data that are used in machine learning systems. In
    this chapter, we’ll dive deeper into ways in which data and algorithms are entangled.
    We’ll talk about data in generic terms, but in this chapter, we’ll explain what
    kind of data is needed in machine learning systems. I’ll explain the fact that
    all kinds of data are used in numerical form – either as a feature vector or as
    more complex feature matrices. Then, I’ll explain the need to transform unstructured
    data (for example, text) into structured data. This chapter will lay the foundations
    for diving deeper into each type of data, which is the content of the next few
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Discuss the process of measurement (obtaining numerical data) and the measurement
    instruments that are used in that process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize numerical data using the Matplotlib and Seaborn libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce dimensions using **principal component** **analysis** (**PCA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with Hugging Face’s Dataset module to download and process image and text
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numerical data usually comes in the form of tables of numbers, kind of like
    database tables. One of the most common data in this form is metrics data – for
    example, the standard object-oriented metrics that have been used since the 1980s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Numerical data is often the result of a measurement process. The measurement
    process is a process where we quantify the empirical properties of an entity using
    measurement instruments to a number. The process must guarantee that important
    empirical properties are preserved in the mathematical domain – that is, in the
    numbers. *Figure 6**.1* shows an example of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The measurement process with an example of quality measurement
    using defects](img/B19548_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The measurement process with an example of quality measurement
    using defects
  prefs: []
  type: TYPE_NORMAL
- en: The important part of this process consists of three elements. First is the
    measurement instrument, which needs to map the empirical properties to numbers
    in a true way. Then, there are the measurement standards, such as the ISO **Vocabulary
    in Metrology** (**VIM**), which is called the measurement’s trueness. Finally,
    we have the result of the measurement process – the process of applying the measurement
    instrument to a specific measured entity – which results in a number, a quantification
    of the measured property. A single number, however, cannot characterize an entire
    software product or even a part of it, regardless of how true it is to the measured
    entity. Therefore, in practice, we use several measurement instruments to create
    a holistic view of the measured entity.
  prefs: []
  type: TYPE_NORMAL
- en: This is where numerical data comes in. Each measurement that characterizes the
    measured entity is stored in a database or a table – each entity becomes one row
    and each metric becomes one column. The more columns we have, the better the characteristics
    of the measured entity. However, at the same time, the more measures we collect,
    the higher the risk that they will be interconnected, correlated (positively and
    negatively), and that they will overlap. So, we need to work a bit with that data
    to get some orientation of it. So, first, we must visualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data we’ll use in this part of this chapter comes from a paper by Alhustain,
    Sultan (Predicting Relative Thresholds for Object Oriented Metrics." 2021 IEEE/ACM
    International Conference on Technical Debt (TechDebt). IEEE, 2021) and is available
    from Zenodo ([https://zenodo.org/records/4625975](https://zenodo.org/records/4625975)),
    one of the most commonly used open data repositories in software engineering research.
    The data contains the values of measures for typical object-oriented metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coupling between objects** (**CB**): The number of references to other classes
    from the measured entity (class)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct class coupling** (**DCC**): The number of connections from this class
    to other classes (for example, associations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ExportCoupling**: The number of outgoing connections from the class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ImportCoupling**: The number of incoming connections to the class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of methods** (**NOM**): The number of methods in the class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted methods per class** (**WMC**): The number of methods in the class,
    weighted by their size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defect count** (**defect**): The number of defects discovered for this class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset describes several software projects from the Apache foundation –
    for example, the Ant tool. For each product, the measured entities are classes
    in the project.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start with the next best practice, which will lead us to the visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #34'
  prefs: []
  type: TYPE_NORMAL
- en: When working with numerical data, visualize it first, starting with the summary
    views of the data.
  prefs: []
  type: TYPE_NORMAL
- en: When I work with numerical data, I usually start by visualizing it. I start
    with some overview of the data and then work my way toward the details.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Summarizing the data can be done using tables and pivots, as well as charts.
    One of the charts that I usually start working with is the correlogram – it’s
    a diagram that shows correlations between each variable/measure in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s read our data into the notebook and start visualizing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the dataset is in memory, we can use Python’s Seaborn library to visualize
    it using the correlogram. The following code does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this code fragment is the following correlogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Correlogram for the dataset from the paper of Alhusain](img/B19548_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Correlogram for the dataset from the paper of Alhusain
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting part here is the distribution of each of the measures presented
    in the cells on the diagonal. In the case of our data, this distribution is hard
    to interpret for some of the variables, so we can visualize it a bit differently.
    When we replace the last line of the code fragment with `sns.pairplot(dfDataAnt13,
    diag_kind="kde")`, we get a new visualization, with a better view of the distribution.
    This is shown in *Figure 6**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Correlogram with a better visualization of the distribution
    of each measure](img/B19548_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Correlogram with a better visualization of the distribution of
    each measure
  prefs: []
  type: TYPE_NORMAL
- en: These correlograms provide us with a quick orientation of which variables can
    be correlated with one another. These correlations are something that we can use
    later in our work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look at the data by visualizing the numbers using heatmaps. Heatmaps
    are tabular visualizations where the intensity of the color indicates the strength
    of the value of each variable. We can use the following code to create a heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting diagram is presented in *Figure 6**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Summary heatmap for the measures](img/B19548_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Summary heatmap for the measures
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper into correlation analysis, I often dive a bit deeper into
    pairwise comparisons. I also recommend my students to do that since working with
    pairs allows us to understand the connections between variables.
  prefs: []
  type: TYPE_NORMAL
- en: So, here is my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #35'
  prefs: []
  type: TYPE_NORMAL
- en: When visualizing data at the aggregate level, focus on the strength of relationships
    and connections between the values.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizations at the aggregate level can provide us with many different views,
    but what we should look for is the connections between the variables. Correlograms
    and heatmaps provide us with this kind of visualization and understanding of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A good set of diagrams to work with are scatter plots. However, I often use
    diagrams that are called KDE plots, also called *density plots*. They provide
    a nicer overview of the variables. The following code fragment visualizes the
    data in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this code fragment is the diagram presented in *Figure 6**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Density plot for two measures – DCC and CBO](img/B19548_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Density plot for two measures – DCC and CBO
  prefs: []
  type: TYPE_NORMAL
- en: This diagram indicates that two measures – CBO and DCC – are quite strongly
    dependent on one another (or they quantify similar/same measurable concepts).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make this diagram a bit nicer if we want to use it in a dashboard by
    using the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment results in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Density plot with a colormap](img/B19548_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Density plot with a colormap
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram shows both the correlation and the number of points that
    are in each area – the more intense the color, the more data points are in that
    area. The same diagram for the DCC and CBO measures is shown in *Figure 6**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Density plot with a colormap for the DCC and CBO measures](img/B19548_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Density plot with a colormap for the DCC and CBO measures
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can use bubble diagrams to visualize the correlations and the number
    of data points per group. The following code creates the bubble diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code results in the diagram presented in *Figure 6**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Scatter plot – a variant called a bubble plot](img/B19548_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Scatter plot – a variant called a bubble plot
  prefs: []
  type: TYPE_NORMAL
- en: This plot lets us see the number of points in each area of the scatterplot,
    which helps us track the correlations visually.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing individual measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scatterplots and density plots are good for tracking dependencies between variables.
    However, we often need to summarize individual measures. For that, we can use
    boxplots. The following code creates a boxplot for the data in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the boxplot presented in *Figure 6**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Boxplot summarizing the CBO measure for classes with and without
    defects](img/B19548_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Boxplot summarizing the CBO measure for classes with and without
    defects
  prefs: []
  type: TYPE_NORMAL
- en: The summary provides us with a quick visual indication that the classes with
    defects are often more connected to other classes than the classes without defects.
    This is not that surprising because usually, the classes are connected and the
    ones that are *not* connected are often very simple and therefore not error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: 'A variation of the boxplot is the violin plot, which we get if we change the
    last line of the last code fragment to `sns.violinplot( x=''Defect'', y=''CBO'',
    data=dfDataAnt13)`. *Figure 6**.10* presents such a violin diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – A violin diagram, which is a variation of a boxplot](img/B19548_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – A violin diagram, which is a variation of a boxplot
  prefs: []
  type: TYPE_NORMAL
- en: Visualization is a good way to understand the numerical data that we have at
    our disposal. We can go even further and start working with it by using methods
    such as dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: So, here is the next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #36'
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into individual analyses should be guided by the machine learning
    task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have not discussed the tasks for our numerical data explicitly,
    it is always there. In the case of defect-related data, the most common task is
    predicting the number of defects per module or class. This means that charts such
    as the violin plot are very useful, providing us with a visual understanding of
    whether there is some sort of difference – a difference that a machine learning
    model can capture.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of measures – PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final analysis that we’ll do regarding numerical data in this chapter is
    about reducing the number of variables. It comes from the field of statistics
    and has been used for reducing the number of variables in the experiment: PCA
    (Wold, 1987 #104). In short, PCA is a technique that finds the best possible fit
    of a pre-defined number of vectors to the data at hand. It does not remove any
    variables; instead, it recalculates them in such a way that the correlation among
    the new set of variables – called principal components – is minimalized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this to our dataset using the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can visualize the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment results in the diagram presented in *Figure 6**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – PCA results for reducing the dimensionality of the defect dataset
    to two. The red data points are the classes that have defects and the green data
    points are the classes without defects](img/B19548_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – PCA results for reducing the dimensionality of the defect dataset
    to two. The red data points are the classes that have defects and the green data
    points are the classes without defects
  prefs: []
  type: TYPE_NORMAL
- en: What is typical about a PCA transformation is its linearity. We can see that
    this diagram contains trails of that – it looks like a triangle with one horizontal
    dimension along the *x axis*, one vertical dimension along the *y axis*, and a
    0-point on the left-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: For this dataset, the diagram shows that the red-marked data points are grouped
    toward the left, and the green-marked points are spread a bit more to the right.
    This means that there is some difference between the classes that have defects
    and the classes that do not have defects. However, there is no clear-cut distinction.
    That would indicate that the machine learning model can’t find a pattern – at
    least, not a pattern that would be robust.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of data – images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038), we looked at image data,
    mostly from the perspective of what kind of image data exists. Now, we will take
    a more pragmatic approach and introduce a better way of working with images than
    just using files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how image data is stored in a popular repository – Hugging Face.
    The library has a specific module for working with datasets – conveniently called
    *Dataset*. It can be installed using the `pip install -q datasets` command. So,
    let’s load a dataset and visualize one of the images from there using the following
    code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the variable dataset contains all the images. Well, not all of them –
    just the part that the designer of the dataset specified as the training set (see
    the last line of the code fragment). We can visualize one of the images using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the images are under unknown copyright, we won’t visualize them in this
    book. However, the preceding line will show the first image in the dataset. We
    can also take a look at what else is in that dataset by simply typing `dataset`.
    We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the dataset contains two columns – images and their labels.
    It contains 75,750 of them. Let’s see what the distribution of labels looks like
    in this dataset using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a nice histogram, as shown in *Figure 6**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Histogram with the distributions of labels. Each column is
    the number of images labeled with the appropriate label – 0 to 100](img/B19548_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Histogram with the distributions of labels. Each column is the
    number of images labeled with the appropriate label – 0 to 100
  prefs: []
  type: TYPE_NORMAL
- en: This diagram shows classes of images that are larger than others – the ones
    that contain over 2,000 images in them. However, it is difficult to check what
    these labels mean without understanding the dataset. We can do that by manually
    visualizing the images. So, here is my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #37'
  prefs: []
  type: TYPE_NORMAL
- en: When visualizing the metadata for images, make sure you visualize the images
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: We mustn’t forget to visualize the image data by plotting the images. We need
    to ensure that we know what the labels mean and what we use them for.
  prefs: []
  type: TYPE_NORMAL
- en: Text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the text data, we’ll use the same Hugging Face hub to obtain two kinds
    of data – unstructured text, as we did in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038),
    and structured data – programming language code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code fragment loads the dataset of movie reviews from the **Internet
    Movie Database** (**IMDb**). We can get an example of the data by using an interface
    that’s similar to what we used for images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize it using a similar one too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code fragment creates the following diagram, showing that both
    positive and negative comments are perfectly balanced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Balanced classes in the IMDb movie database reviews](img/B19548_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Balanced classes in the IMDb movie database reviews
  prefs: []
  type: TYPE_NORMAL
- en: We can do all kinds of processing on the text data in the next steps. However,
    this processing is related to feature extraction, so we’ll talk about it in the
    next few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do that, though, let’s look at datasets that are closer to the domain
    of software engineering – programming language code. We used similar data in [*Chapter
    3*](B19548_03.xhtml#_idTextAnchor038), so let’s focus on how we could obtain a
    larger corpus of programming language code from Hugging Face. We could use the
    following code to obtain the data and check the first program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment shows us the first program, which is already tokenized and
    prepared for further analysis. So, let’s take a peek at the frequency of tokens
    in this dataset. We can use the following code for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code extracts the tokens, counts them, and creates a diagram
    of the frequency of the top 20 tokens. The result is presented in *Figure 6**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Token frequencies for the top 20 most common tokens in the
    code dataset](img/B19548_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Token frequencies for the top 20 most common tokens in the code
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, we can observe that brackets, commas, semicolons, and curly brackets
    are the most commonly used tokens in the dataset. This isn’t surprising as these
    kinds of characters have special meaning in Java.
  prefs: []
  type: TYPE_NORMAL
- en: The other tokens in the top 20 list are, unsurprisingly, keywords in Java or
    have special meanings (such as `==`).
  prefs: []
  type: TYPE_NORMAL
- en: So, my last best practice in this chapter is about understanding the text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #38'
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics for text data help us perform a sanity check of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Even though textual data is quite unstructured by nature, we can visualize some
    of the properties of the data. For example, token frequency analysis can reveal
    whether our empirical understanding of the data makes sense and whether we can
    trust it.
  prefs: []
  type: TYPE_NORMAL
- en: Toward feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored methods for visualizing data. We learned how to
    create diagrams and identify dependencies in the data. We also learned how we
    can use dimensionality reduction techniques to plot multidimensional data on a
    two dimensional diagram.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few chapters, we’ll dive into feature engineering different types
    of data. Sometimes, it is easy to mix feature engineering with data extraction.
    In practice, it is not that difficult to tell one from the other.
  prefs: []
  type: TYPE_NORMAL
- en: Extracted data is data that has been collected by applying some sort of measurement
    instrument. Raw text or images are good examples of this kind of data. Extracted
    data is close to the domain where the data comes from – or how it is measured.
  prefs: []
  type: TYPE_NORMAL
- en: Features describe the data based on the analysis that we want to perform – they
    are closer to what we want to do with the data. It is closer to what we want to
    achieve and which form of machine learning analysis we want to do.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*International Standardization Organization, International vocabulary of basic
    and general terms in metrology (VIM). In International Organization. 2004\.* *p.
    09-14.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alhusain, S. Predicting Relative Thresholds for Object Oriented Metrics. In
    2021 IEEE/ACM International Conference on Technical Debt (TechDebt).* *2021\.
    IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feldt, R., et al. Supporting software decision meetings: Heatmaps for visualising
    test and code measurements. In 2013 39th Euromicro Conference on Software Engineering
    and Advanced Applications.* *2013\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Staron, M., et al. Measuring and visualizing code stability – a case study
    at three companies. In 2013 Joint Conference of the 23rd International Workshop
    on Software Measurement and the 8th International Conference on Software Process
    and Product Measurement.* *2013\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wen, S., C. Nilsson, and M. Staron. Assessing the release readiness of engine
    control software. In Proceedings of the 1st International Workshop on Software
    Qualities and Their* *Dependencies. 2018.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
