<html><head></head><body>
  <div id="_idContainer274" class="Basic-Text-Frame">
    <h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-126" class="chapterTitle">Hyperparameter Optimization</h1>
    <p class="normal">How a Kaggle solution performs is not simply determined by the type of learning algorithm you choose. Aside from the data and the features that you use, it is also strongly determined by the algorithm’s <strong class="keyWord">hyperparameters</strong>, the parameters of the algorithm that have to be fixed <em class="italic">prior to</em> training, and cannot be learned during the training process. Choosing the right variables/data/features is most effective in tabular data competitions; however, hyperparameter optimization is effective in <em class="italic">all</em> competitions, of any kind. In fact, given fixed data and an algorithm, hyperparameter optimization is the only sure way to enhance the predictive performance of the algorithm and climb the leaderboard. It also helps in ensembling, because an ensemble of tuned models always performs better than an ensemble of untuned ones.</p>
    <p class="normal">You may hear that tuning hyperparameters manually is possible if you know and understand the effects of your choices on the algorithm. Many Kaggle Grandmasters and Masters have declared that they often rely on directly tuning their models in competitions. They operate selectively on the most important hyperparameters in a bisection operation style, exploring smaller and smaller intervals of a parameter’s values until they find the value that produces the best result. Then, they move on to another parameter. This works perfectly well if there is a single minimum for each parameter and if the parameters are independent from each other. In this case, the search is mostly driven by experience and knowledge of learning algorithms. In our experience, however, that is not the case with most tasks you will encounter on Kaggle. The sophistication of the problems and the algorithms used requires a systematic approach that only a search algorithm can provide. Hence, we decided to write this chapter.</p>
    <p class="normal">In this chapter, we will explore how to extend your cross-validation approach to find the best hyperparameters that can generalize to your test set. The idea is to deal with the pressure and scarcity of time and resources that you experience in competitions. For this reason, we will concentrate on <strong class="keyWord">Bayesian optimization methods</strong>, which are a proven way to optimize for complex models and data problems based on the resources you have available. We won’t limit ourselves to searching for the best values for pre-defined hyperparameters; we will also delve into the problem of neural network architecture. </p>
    <p class="normal">We will cover the following topics:</p>
    <ul>
      <li class="bulletList">Basic optimization techniques</li>
      <li class="bulletList">Key parameters and how to use them</li>
      <li class="bulletList">Bayesian optimization</li>
    </ul>
    <p class="normal">Let’s start!</p>
    <h1 id="_idParaDest-127" class="heading-1">Basic optimization techniques</h1>
    <p class="normal">The core algorithms for<a id="_idIndexMarker627"/> hyperparameter optimization, found in the Scikit-learn <a id="_idIndexMarker628"/>package, are <strong class="keyWord">grid search</strong> and <strong class="keyWord">random search</strong>. Recently, the Scikit-learn <a id="_idIndexMarker629"/>contributors have also added the <strong class="keyWord">halving algorithm</strong> to<a id="_idIndexMarker630"/> improve the performances of both grid search and random search strategies.</p>
    <p class="normal">In this section, we will discuss all these basic techniques. By mastering them, not only will you have effective optimization tools for some specific problems (for instance, SVMs are usually optimized by grid search) but you will also be familiar with the basics of how hyperparameter optimization works.</p>
    <p class="normal">To start with, it is crucial to figure out what the necessary ingredients are:</p>
    <ul>
      <li class="bulletList">A model whose hyperparameters have to be optimized</li>
      <li class="bulletList">A search space containing the boundaries of the values to search between for each hyperparameter</li>
      <li class="bulletList">A cross-validation scheme</li>
      <li class="bulletList">An evaluation metric and its score function</li>
    </ul>
    <p class="normal">All these elements come together in the search method to determine the solution you are looking for. Let’s see how it works.</p>
    <h2 id="_idParaDest-128" class="heading-2">Grid search</h2>
    <p class="normal"><strong class="keyWord">Grid search</strong> is a<a id="_idIndexMarker631"/> method that <a id="_idIndexMarker632"/>searches through the hyperparameters exhaustively, and is not feasible in high-dimensional space. For every parameter, you pick a set of values you want to test. You then test all the possible combinations in this set. That is why it is exhaustive: you try everything. It is a very simple algorithm and it suffers from the curse of dimensionality, but, on the positive side, it’s <em class="italic">embarrassingly parallel</em> (see <a href="https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html"><span class="url">https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html</span></a> for a definition of this computer science term). This means you can obtain an optimal tuning very quickly, if you have enough processors to run the search on.</p>
    <p class="normal">As an example, let’s take <a id="_idIndexMarker633"/>a classification problem and <strong class="keyWord">support-vector machine classification</strong> (<strong class="keyWord">SVC</strong>). <strong class="keyWord">Support-vector machines </strong>(<strong class="keyWord">SVMs</strong>) for both <a id="_idIndexMarker634"/>classification and regression problems are probably the machine learning algorithm that you will use grid search for the most. Using the <code class="inlineCode">make_classification</code> function from Scikit-learn, we can generate a classification dataset quickly:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
X, y = make_classification(n_samples=<span class="hljs-number">300</span>, n_features=<span class="hljs-number">50</span>,
                           n_informative=<span class="hljs-number">10</span>,
                           n_redundant=<span class="hljs-number">25</span>, n_repeated=<span class="hljs-number">15</span>,
                           n_clusters_per_class=<span class="hljs-number">5</span>,
                           flip_y=<span class="hljs-number">0.05</span>, class_sep=<span class="hljs-number">0.5</span>,
                           random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">For our next step, we define a basic SVC algorithm and set the search space. Since the <strong class="keyWord">kernel</strong> <strong class="keyWord">function</strong> of the SVC (the internal function that transforms the input data in an SVM) determines the different hyperparameters to set, we provide a list containing two dictionaries of distinct search spaces for parameters to be used depending on the type of kernel chosen. We also set the evaluation metric (we use accuracy in this case, since the target is perfectly balanced):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm
svc = svm.SVC()
svc = svm.SVC(probability=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">1</span>)
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> model_selection
search_grid = [
               {<span class="hljs-string">'C'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>], <span class="hljs-string">'kernel'</span>: [<span class="hljs-string">'linear'</span>]},
               {<span class="hljs-string">'C'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>], <span class="hljs-string">'gamma'</span>: [<span class="hljs-number">0.001</span>, <span class="hljs-number">0.0001</span>],
               <span class="hljs-string">'kernel'</span>: [<span class="hljs-string">'rbf'</span>]}
               ]
               
scorer = <span class="hljs-string">'accuracy'</span>
</code></pre>
    <p class="normal">In our example, a linear kernel doesn’t require the tuning of the <code class="inlineCode">gamma</code> parameter, though it is very important for a radial basis function kernel. Therefore, we provide two dictionaries: the first containing the parameters for the linear kernel, the second containing parameters for a radial basis function kernel. Each dictionary only contains a reference to the kernel it is relevant to and only the range of parameters that are relevant for that kernel. </p>
    <p class="normal">It is important <a id="_idIndexMarker635"/>to note that the evaluation metric can be different from the cost function optimized by the algorithm. In fact, as discussed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Competition Tasks and Metrics</em>, you may encounter scenarios in which the evaluation metric for the competition is<a id="_idIndexMarker636"/> different, but you cannot modify the cost function of your algorithm. Under these circumstances, tuning the hyperparameters according to your evaluation metric can still help in obtaining a well-performing model. Though built around the algorithm’s cost function, the optimal set of hyperparameters found will be the one returning the best evaluation metric under such constraints. It probably won’t be the theoretically best result that you could obtain for the problem, but it may often not be far from it.</p>
    <p class="normal">All the ingredients (model, search space, evaluation metric, cross-validation scheme) are combined into the <code class="inlineCode">GridSearchCV</code> instance, and then the model is fit to the data: </p>
    <pre class="programlisting code"><code class="hljs-code">search_func = model_selection.GridSearchCV(estimator=svc, 
                                           param_grid=search_grid,
                                           scoring=scorer, 
                                           n_jobs=-<span class="hljs-number">1</span>,
                                           cv=<span class="hljs-number">5</span>)
search_func.fit(X, y)
<span class="hljs-built_in">print</span> (search_func.best_params_)
<span class="hljs-built_in">print</span> (search_func.best_score_)
</code></pre>
    <p class="normal">After a while, depending on the machine you are running the optimization on, you will obtain the best combination based on cross-validated results.</p>
    <p class="normal">In conclusion, grid search is a very simple optimization algorithm that can leverage the availability of multi-core computers. It can work fine with machine learning algorithms that do not require many tunings (such as SVM and the ridge and lasso regressions) but, in all other cases, its applicability is quite narrow. First, it is limited to optimizing hyperparameters<a id="_idIndexMarker637"/> by discrete choice (you need a limited set of values to cycle through). In addition, you cannot expect it to work effectively on algorithms requiring <em class="italic">multiple</em> hyperparameters <a id="_idIndexMarker638"/>to be tuned. This is because of the exploding complexity of the search space, and because most of the computational inefficiency is due to the fact that the search is trying parameter values blindly, most of which do not work for the problem.</p>
    <h2 id="_idParaDest-129" class="heading-2">Random search</h2>
    <p class="normal"><strong class="keyWord">Random search</strong>, which <a id="_idIndexMarker639"/>simply samples the search space randomly, is feasible in<a id="_idIndexMarker640"/> high-dimensional spaces and is widely used in practice. The downside of random search, however, is that it doesn’t use information from prior experiments to select the next setting (a problem shared by grid search, we should note). In addition, to find the best solution as fast as possible, you cannot do anything except hope to be lucky you catch the right hyperparameters.</p>
    <p class="normal">Random search works incredibly well and it is simple to understand. Despite the fact it relies on randomness, it isn’t just based on blind luck, though it may initially appear to be. In fact, it works like random sampling in statistics: the main point of the technique is that if you do enough random tests, you have a good possibility of finding the right parameters without wasting energy on testing slightly different combinations of similarly performing combinations.</p>
    <p class="normal">Many AutoML systems rely on random search when there are too many parameters to set (see Golovin, D. et al. <em class="italic">Google Vizier: A Service for Black-Box Optimization</em>, 2017). As a rule of thumb, consider looking at random search when the dimensionality of your hyperparameter optimization problem is sufficiently high (for example, over 16).</p>
    <p class="normal">Below, we run the previous example using random search:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> scipy.stats <span class="hljs-keyword">as</span> stats
<span class="hljs-keyword">from</span> sklearn.utils.fixes <span class="hljs-keyword">import</span> loguniform
search_dict = {<span class="hljs-string">'kernel'</span>: [<span class="hljs-string">'linear'</span>, <span class="hljs-string">'rbf'</span>], 
               <span class="hljs-string">'C'</span>: loguniform(<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>),
               <span class="hljs-string">'gamma'</span>: loguniform(<span class="hljs-number">0.0001</span>, <span class="hljs-number">0.1</span>)
               }
scorer = <span class="hljs-string">'accuracy'</span>
search_func = model_selection.RandomizedSearchCV
              (estimator=svc,param_distributions=search_dict, n_iter=<span class="hljs-number">6</span>,
              scoring=scorer, n_jobs=-<span class="hljs-number">1</span>, cv=<span class="hljs-number">5</span>)
search_func.fit(X, y)
<span class="hljs-built_in">print</span> (search_func.best_params_)
<span class="hljs-built_in">print</span> (search_func.best_score_)
</code></pre>
    <p class="normal">Notice that, now, we <a id="_idIndexMarker641"/>don’t care about running the search on separate spaces for the different <a id="_idIndexMarker642"/>kernels. Contrary to grid search, where each parameter, even the ineffective ones, is systematically tested, which requires computational time, here the efficiency of the search is not affected by the set of hyperparameters tested. The search doesn’t depend on irrelevant parameters, but is guided by chance; any trial is useful, even if you are testing only one valid parameter among many for the chosen kernel.</p>
    <h2 id="_idParaDest-130" class="heading-2">Halving search</h2>
    <p class="normal">As we mentioned, both<a id="_idIndexMarker643"/> grid search and random search work in an uninformed way: if some tests <a id="_idIndexMarker644"/>find out that certain hyperparameters do not impact the result or that certain value intervals are ineffective, the information is not propagated to the following searches. </p>
    <p class="normal">For this reason, Scikit-learn has recently introduced the <code class="inlineCode">HalvingGridSearchCV</code> and <code class="inlineCode">HalvingRandomSearchCV</code> estimators, which can be used to search a parameter space using <strong class="keyWord">successive halving </strong>applied to the grid search and random search tuning strategies. </p>
    <p class="normal">In halving, a large number of hyperparameter combinations are evaluated in an initial round of tests but using a small amount of computational resources. This is achieved by running the tests on a subsample of a few cases from your training data. A smaller training set needs fewer computations to be tested, so fewer resources (namely time) are used at the cost of more imprecise performance estimations. This initial round allows the selection of a subset of candidate hyperparameter values, which have performed better on the problem, to be used for the second round, when the training set size is increased. </p>
    <p class="normal">The following rounds proceed in a similar way, allocating larger and larger subsets of the training set to be searched as the range of tested values is restricted (testing now requires more time to execute, but returns a more precise performance estimation), while the number of candidates continues to be halved.</p>
    <p class="normal">Here is an example <a id="_idIndexMarker645"/>applied to<a id="_idIndexMarker646"/> the previous problem:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.experimental <span class="hljs-keyword">import</span> enable_halving_search_cv
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> HalvingRandomSearchCV
search_func = HalvingRandomSearchCV(estimator=svc,
                                    param_distributions=search_dict,
                                    resource=<span class="hljs-string">'n_samples'</span>,
                                    max_resources=<span class="hljs-number">100</span>,
                                    aggressive_elimination=<span class="hljs-literal">True</span>,
                                    scoring=scorer,
                                    n_jobs=-<span class="hljs-number">1</span>,
                                    cv=<span class="hljs-number">5</span>,
                                    random_state=<span class="hljs-number">0</span>)
search_func.fit(X, y)
<span class="hljs-built_in">print </span>(search_func.best_params_)
<span class="hljs-built_in">print </span>(search_func.best_score_)
</code></pre>
    <p class="normal">In this way, halving provides information to the successive optimization steps via the selection of the candidates. In the next sections, we will discuss even smarter ways to achieve a more precise and efficient search through the space of hyperparameters.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Kazuki_Onodera.png" alt=""/>
      </div>
      <p class="intervieweeName">Kazuki Onodera</p>
      <p class="normal"><a href="https://www.kaggle.com/onodera"><span class="url">https://www.kaggle.com/onodera</span></a></p>
      <p class="normal">Let’s pause for an interview with <a id="_idIndexMarker647"/>another Kaggler. Kazuki Onodera is a Competitions Grandmaster and Discussions Master who has around 7 years of competition experience. He’s also a Senior Deep Learning Data Scientist at NVIDIA and a member of the NVIDIA KGMON (Kaggle Grandmasters of NVIDIA) team.</p>

      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal">Instacart Market Basket Analysis.<em class="italic"> This competition proved quite challenging for the Kaggle community because of its use of anonymized data related to customer orders over time in order to predict which previously purchased products will be in a user’s next order. The reason why I like it is that I love feature engineering and I could come up with a bunch of good and interesting features everyone else couldn’t, which allowed me to get second place in the competition.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">I try to imagine how a model works, and delve into false negatives and false positives. Same as in my daily work.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal">Human Protein Atlas - Single Cell Classification.<em class="italic"> This competition was a kind of instance segmentation competition, but no masks were provided. So, it turned into being a weakly supervised multi-label classification problem. I created a two-stage pipeline for removing label noise.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Yes. I’m now working in the NVIDIA KGMON (Kaggle Grandmasters of NVIDIA) team. Kaggle launches many different machine learning competitions, different with regards to data type, tabular, image, natural language, and signal, as well as with regards to sector and domain: industry, finance, astronomy, pathology, sports, retail, and so on. I bet nobody can access and have experience with all these kinds of data except Kagglers.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">Target analysis. Also, seed averaging is quite overlooked: always simple but powerful.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">Target analysis. Top teams always analyze the target better than others, so if I couldn’t get a better place in a competition, I go and read about the top solutions, because they always describe to me the knowledge about the data that I missed during the competition.</em></p>

      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">Just Python and Jupyter </em><em class="italic"><a id="_idIndexMarker648"/></em><em class="italic">Notebooks.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">If you can learn from a defeat, you haven’t really lost.</em></p>
      <p class="interviewHeader">Do you use other competition platforms? How do they compare to Kaggle?</p>
      <p class="normal"><em class="italic">KDD Cup and RecSys. Both meet the minimum requirements for being interesting and challenging.</em></p>
    </div>
    <h1 id="_idParaDest-131" class="heading-1">Key parameters and how to use them</h1>
    <p class="normal">The next problem is using the right set of hyperparameters for each kind of model you use. In particular, in order to be efficient in your optimization, you need to know the values of each hyperparameter that it actually makes sense to test for each distinct algorithm. </p>
    <p class="normal">In this section, we will examine the most common models used in Kaggle competitions, especially the tabular ones, and discuss the hyperparameters you need to tune in order to obtain the best results. We will distinguish between classical machine learning models and gradient boosting models (which are much more demanding in terms of their space of parameters) for generic tabular data problems. </p>
    <p class="normal">As for neural networks, we can give you an idea about specific parameters to tune when we present the standard models (for instance, the TabNet neural model has some specific parameters to set so that it works properly). However, most of the optimization on deep neural networks in Kaggle competitions is not performed on standard models, but on <em class="italic">custom</em> ones. Consequently, apart from basic learning parameters such as the learning rate and the batch size, optimization in neural networks is based on the specific characteristics of the neural architecture of your model. You have to deal with the problem in an ad hoc way. Near the end of the chapter, we will discuss an example of <strong class="keyWord">neural architecture search</strong> (<strong class="keyWord">NAS</strong>) using KerasTuner (<a href="https://keras.io/keras_tuner/"><span class="url">https://keras.io/keras_tuner/</span></a>).</p>
    <h2 id="_idParaDest-132" class="heading-2">Linear models</h2>
    <p class="normal">The linear models<a id="_idIndexMarker649"/> that need to be tuned are usually linear regressions or logistic regressions with regularization:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">C</code>: The range you should search is <code class="inlineCode">np.logspace(-4, 4, 10)</code>; smaller values specify stronger regularization.</li>
      <li class="bulletList"><code class="inlineCode">alpha</code>: You should search the range <code class="inlineCode">np.logspace(-2, 2, 10)</code>; smaller values specify stronger regularization, larger values specify stronger regularization. Also take note that higher values take more time to process when using lasso.</li>
      <li class="bulletList"><code class="inlineCode">l1_ratio</code>: You should pick from the list <code class="inlineCode">[.1, .5, .7, .9, .95, .99, 1]</code>; it applies only to elastic net.</li>
    </ul>
    <p class="normal">In Scikit-learn, depending on the algorithm, you find either the hyperparameter <code class="inlineCode">C</code> (logistic regression) or <code class="inlineCode">alpha</code> (lasso, ridge, elastic net).</p>
    <h2 id="_idParaDest-133" class="heading-2">Support-vector machines</h2>
    <p class="normal"><strong class="keyWord">SVMs</strong> are a <a id="_idIndexMarker650"/>family of powerful and advanced supervised learning techniques for classification and regression that can automatically fit linear and non-linear models. Scikit-learn offers an implementation based on <code class="inlineCode">LIBSVM</code>, a complete library of SVM classification and regression implementations, and <code class="inlineCode">LIBLINEAR</code>, a scalable library for linear classification ideal for large datasets, especially sparse text-based ones. In their optimization, SVMs strive to separate target classes in classification problems using a decision boundary characterized by the largest possible margin between classes.</p>
    <p class="normal">Though SVMs work fine with default parameters, they are often not optimal, and you need to test various value combinations using cross-validation to find the best ones. Listed according to their<a id="_idIndexMarker651"/> importance, you have to set the following parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">C</code>: The penalty value. Decreasing it makes the margin between classes larger, thus ignoring more noise but also making the model more generalizable. A best value can normally be found in the range <code class="inlineCode">np.logspace(-3, 3, 7)</code>.</li>
      <li class="bulletList"><code class="inlineCode">kernel</code>: This parameter will determine how non-linearity will be implemented in an SVM and it can be set to <code class="inlineCode">'linear'</code>, <code class="inlineCode">'poly'</code>, <code class="inlineCode">'</code><code class="inlineCode">rbf'</code>, <code class="inlineCode">'sigmoid'</code>, or a custom kernel. The most commonly used value is certainly <code class="inlineCode">rbf</code>.</li>
      <li class="bulletList"><code class="inlineCode">degree</code>: Works with <code class="inlineCode">kernel='poly'</code>, signaling the dimensionality of the polynomial expansion. It is ignored by other kernels. Usually, setting its values to between <code class="inlineCode">2</code> and <code class="inlineCode">5</code> works the best.</li>
      <li class="bulletList"><code class="inlineCode">gamma</code>: A coefficient for <code class="inlineCode">'rbf'</code>, <code class="inlineCode">'</code><code class="inlineCode">poly'</code>, and <code class="inlineCode">'sigmoid'</code>. High values tend to fit data in a better way, but can lead to some overfitting. Intuitively, we can imagine <code class="inlineCode">gamma</code> as the influence that a single example exercises over the model. Low values make the influence of each example reach further. Since many points have to be considered, the SVM curve will tend to take a shape less influenced by local points and the result will be a smoother decision contour curve. High values of <code class="inlineCode">gamma</code>, instead, mean the curve takes into account how points are arranged locally more and, as a result, you get a more irregular and wiggly decision curve. The suggested grid search range for this hyperparameter is <code class="inlineCode">np.logspace(-3, 3, 7)</code>.</li>
      <li class="bulletList"><code class="inlineCode">nu</code>: For regression and classification with nuSVR and nuSVC, this parameter sets a tolerance for the training points that are near to the margin and are not classified correctly. It helps in ignoring misclassified points just near or on the margin, hence it can render the classification decision curve smoother. It should be in the range <code class="inlineCode">[0,1]</code> since it is a proportion relative to your training set. Ultimately, it acts like <code class="inlineCode">C</code>, with high proportions enlarging the margin.</li>
      <li class="bulletList"><code class="inlineCode">epsilon</code>: This parameter specifies how much error SVR will accept, by defining an <code class="inlineCode">epsilon</code> large range where no penalty is associated with an incorrect prediction of the example during the training of the algorithm. The suggested search range is <code class="inlineCode">np.logspace(-4, 2, 7)</code>.</li>
      <li class="bulletList"><code class="inlineCode">penalty</code>, <code class="inlineCode">loss</code>, and <code class="inlineCode">dual</code>: For LinearSVC, these parameters accept the <code class="inlineCode">('l1', 'squared_hinge', False)</code>, <code class="inlineCode">('l2', 'hinge', True)</code>, <code class="inlineCode">('l2', 'squared_hinge', True)</code>, and <code class="inlineCode">('l2', 'squared_hinge', False)</code> combinations. The <code class="inlineCode">('l2', 'hinge', True)</code> combination is analogous to the <code class="inlineCode">SVC(kernel='linear')</code> learner.</li>
    </ul>
    <p class="normal">It may appear that an SVM<a id="_idIndexMarker652"/> has many hyperparameters to set, but many settings are specific only to implementations or to kernels, so you only have to select the relevant parameters.</p>
    <h2 id="_idParaDest-134" class="heading-2">Random forests and extremely randomized trees</h2>
    <p class="normal"><em class="italic">Leo Breiman</em> and <em class="italic">Adele Cutler</em> originally devised the idea at the core of the random forest algorithm, and the name of the algorithm remains a trademark of theirs today (though the algorithm is open source). Random forests are implemented in Scikit-learn as <code class="inlineCode">RandomForestClassifier</code> or <code class="inlineCode">RandomForestRegressor</code>.</p>
    <p class="normal">A random forest<a id="_idIndexMarker653"/> works in a<a id="_idIndexMarker654"/> similar way to bagging, also devised by Leo Breiman, but operates only using binary split decision trees, which are left to grow to their extremes. Moreover, it samples the cases to be used in each of its models using <strong class="keyWord">bootstrapping</strong>. As the tree is grown, at each split of a branch, the set of variables considered for the split is drawn randomly, too. </p>
    <p class="normal">This is the secret at the heart of the algorithm: it ensembles trees that, due to different samples and variables considered at the splits, are very different from each other. As they are different, they are also uncorrelated. This is beneficial because when the results are ensembled, much variance is ruled out, as the extreme values on both sides of a distribution tend to balance out. In other words, bagging algorithms guarantee a certain level of diversity in the predictions, allowing them to develop rules that a single learner (such as a decision tree) might not come across. All this diversity is useful because it helps in building a distribution whose average is a better predictor than any of the individual trees in the ensemble.</p>
    <p class="normal"><strong class="keyWord">Extra Trees</strong> (also known as <strong class="keyWord">extremely randomized trees</strong>), represented in Scikit-learn by the <code class="inlineCode">ExtraTreesClassifier</code>/<code class="inlineCode">ExtraTreesRegressor</code> classes, are a more randomized kind of random forest that produces a <a id="_idIndexMarker655"/>lower variance in the estimates at the cost of greater bias of the estimators. However, when it comes to CPU efficiency, Extra Trees can deliver a considerable speed-up compared to random forests, so they can be ideal when you are working with large datasets in terms of both examples and features. The reason for the resulting higher bias but better speed is the way splits are built in an Extra Tree. Random forests, after drawing a random set of features to be considered for splitting a branch of a tree, carefully search among them for the best values to assign to each branch. By contrast, in Extra Trees, both the set of candidate features for the split and the actual split value are decided completely randomly. So, there’s no need for much computation, though the <a id="_idIndexMarker656"/>randomly chosen split may not be the most effective one (hence the bias).</p>
    <p class="normal">For both algorithms, the key hyperparameters that should be set are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">max_features</code>: This is the number of sampled features that are present at every split, which can determine the performance of the algorithm. The lower the number, the speedier, but with higher bias.</li>
      <li class="bulletList"><code class="inlineCode">min_samples_leaf</code>: This allows you to determine the depth of the trees. Large numbers diminish the variance and increase the bias.</li>
      <li class="bulletList"><code class="inlineCode">bootstrap</code>: This is a Boolean that allows bootstrapping.</li>
      <li class="bulletList"><code class="inlineCode">n_estimators</code>: This is the number of trees. Remember that the more trees the better, though there is a threshold beyond which we get diminishing returns depending on the data problem. Also, this comes at a computational cost that you have to take into account based on the resources you have available.</li>
    </ul>
    <p class="normal">Extra Trees are a good alternative to random forests, especially when the data you have is particularly noisy. Since they trade some variance reduction for more bias given their random choice of splits, they tend to overfit less on important yet noisy features that would otherwise dominate the splits in a random forest.</p>
    <h2 id="_idParaDest-135" class="heading-2">Gradient tree boosting</h2>
    <p class="normal">Gradient tree boosting <a id="_idIndexMarker657"/>or <strong class="keyWord">gradient boosting decision trees</strong> (<strong class="keyWord">GBDT</strong>) is an improved version of boosting (boosting works by fitting a sequence of weak learners on reweighted versions of the data). Like AdaBoost, GBDT is based on a gradient descent function. The algorithm has proven to be one of the most proficient ones from the family of models that are based on ensembles, though it is characterized by an increased variance of estimates, more sensitivity to noise in data (both problems can be mitigated by using subsampling), and significant computational costs due to non-parallel operations.</p>
    <p class="normal">Apart from deep learning, gradient boosting is the most developed machine learning algorithm. Since AdaBoost and the initial gradient boosting implementation, as developed by <em class="italic">Jerome Friedman</em>, various <a id="_idIndexMarker658"/>other implementations of the algorithms appeared, the most recent ones being XGBoost, LightGBM, and CatBoost.</p>
    <h3 id="_idParaDest-136" class="heading-3">LightGBM</h3>
    <p class="normal">The high-performance LightGBM<a id="_idIndexMarker659"/> algorithm (<a href="https://github.com/Microsoft/LightGBM"><span class="url">https://github.com/Microsoft/LightGBM</span></a>) is capable<a id="_idIndexMarker660"/> of being distributed on multiple computers and handling large amounts of data quickly. It was developed by a team at Microsoft as an open-source project on GitHub (there is also an academic paper: <a href="https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html"><span class="url">https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html</span></a>).</p>
    <p class="normal">LightGBM is based on decision trees, like XGBoost, but it follows a different strategy. While XGBoost uses decision trees to split on a variable and explore different tree splits at that variable (the <strong class="keyWord">level-wise</strong> tree growth strategy), LightGBM concentrates on one split and goes on splitting from there in order to achieve a better fit (the <strong class="keyWord">leaf-wise</strong> tree growth strategy). This allows LightGBM to quickly reach a good fit of the data, and to generate alternative solutions compared to XGBoost (which is good, if you expect to blend the two solutions together in order to reduce the variance of the estimates). Algorithmically speaking, if we think of the structure of splits operated by a decision tree as a graph, XGBoost pursues a <em class="italic">breadth-first</em> search (BFS) and LightGBM a <em class="italic">depth-first</em> search (DFS).</p>
    <p class="normal">Tuning LightGBM may appear daunting; it has more than a hundred parameters to tune that you can explore at this page: <a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst"><span class="url">https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst</span></a> (also here: <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html"><span class="url">https://lightgbm.readthedocs.io/en/latest/Parameters</span><span class="url">.html</span></a>). </p>
    <p class="normal">As a rule of thumb, you should focus on the <a id="_idIndexMarker661"/>following hyperparameters, which usually have the most impact on the results:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_estimators</code>: An integer between 10 and 10,000 that sets the number of iterations.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: A real number between 0.01 and 1.0, usually sampled from a log-uniform distribution. It represents the step size of the gradient descent procedure that computes the weights for the summed ensemble of all the iterations of the algorithm up to this point.</li>
      <li class="bulletList"><code class="inlineCode">max_depth</code>: An integer between 1 and 16, representing the maximum number of splits on features. Setting it to a number below 0 allows the maximum possible number of splits, usually risking overfitting to data.</li>
      <li class="bulletList"><code class="inlineCode">num_leaves</code>: An integer between 2 and 2^<code class="inlineCode">max_depth</code>, representing the number of final leaves each tree will have at most.</li>
      <li class="bulletList"><code class="inlineCode">min_data_in_leaf</code>: An integer between 0 and 300 that determines the minimum number of data points in one leaf.</li>
      <li class="bulletList"><code class="inlineCode">min_gain_to_split</code>: A float between 0 and 15; it sets the minimum gain of the algorithm for tree partitioning. By setting this parameter, you can avoid unnecessary tree splits and thus reduce overfitting (it corresponds to the <code class="inlineCode">gamma</code> parameter in XGBoost). </li>
      <li class="bulletList"><code class="inlineCode">max_bin</code>: An integer between 32 and 512 that sets the maximum number of bins that feature values will be bucketed into. Having this parameter larger than the default value of 255 implies more risk of producing overfitting results.</li>
      <li class="bulletList"><code class="inlineCode">subsample</code>: A real<a id="_idIndexMarker662"/> number between 0.01, and 1.0, representing the portion of the sample to be used in training.</li>
      <li class="bulletList"><code class="inlineCode">subsample_freq</code>: An integer between 0 and 10 specifying the frequency, in terms of iterations, at which the algorithm will subsample the examples.</li>
    </ul>
    <div class="note">
      <p class="normal">Note that, if set to zero, the algorithm will ignore any value given to the <code class="inlineCode">subsample</code> parameter. In addition, it is set to zero by default, therefore just setting the <code class="inlineCode">subsample</code> parameter won’t work.</p>
    </div>
    <ul>
      <li class="bulletList"><code class="inlineCode">feature_fraction</code>: A real number between 0.1 and 1.0 allowing you to specify the portion of features to be subsampled. Subsampling the features is another way to allow more randomization to play a role in the training, fighting noise and multicollinearity present in the features. </li>
      <li class="bulletList"><code class="inlineCode">subsample_for_bin</code>: An integer between 30 and the number of examples. This sets the number of examples that are sampled for the construction of histogram bins.</li>
      <li class="bulletList"><code class="inlineCode">reg_lambda</code>: A real number between 0 and 100.0 that sets the L2 regularization. Since it is more sensitive to the scale than to the exact number of the parameter, it is usually sampled from a log-uniform distribution.</li>
      <li class="bulletList"><code class="inlineCode">reg_alpha</code>: A real number between 0 and 100.0, usually sampled from a log-uniform distribution, which sets the L1 regularization.</li>
      <li class="bulletList"><code class="inlineCode">scale_pos_weight</code>: A real number between 1e-6 and 500, better sampled from the log-uniform distribution. The parameter weights the positive cases (thus effectively upsampling or downsampling) against the negative cases, which are kept to <a id="_idIndexMarker663"/>the value of 1.</li>
    </ul>
    <p class="normal">Although the number of hyperparameters to tune when using LightGBM may appear daunting, in reality only a few of them matter a lot. Given a fixed number of iterations and learning rate, just a few are the most impactful (<code class="inlineCode">feature_fraction</code>, <code class="inlineCode">num_leaves</code>, <code class="inlineCode">subsample</code>, <code class="inlineCode">reg_lambda</code>, <code class="inlineCode">reg_alpha</code>, <code class="inlineCode">min_data_in_leaf</code>), as explained in this blog article by <em class="italic">Kohei Ozaki</em>, a Kaggle Grandmaster: <a href="https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258"><span class="url">https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258</span></a>. Kohei Ozaki leverages this fact in order to create a fast-tuning procedure for Optuna (you’ll find more on the Optuna optimizer at the end of this chapter).</p>
    <h3 id="_idParaDest-137" class="heading-3">XGBoost</h3>
    <p class="normal">XGBoost (<a href="https://github.com/dmlc/XGBoost"><span class="url">https://github.com/dmlc/XGBoost</span></a>) stands for <strong class="keyWord">eXtreme Gradient Boosting</strong>. It is an open-source project<a id="_idIndexMarker664"/> that is not part of Scikit-learn, though it has recently been expanded by a Scikit-learn <a id="_idIndexMarker665"/>wrapper interface that makes it easier to incorporate XGBoost into a Scikit-learn-style data pipeline.</p>
    <p class="normal">The XGBoost algorithm gained momentum and popularity in 2015 data science competitions, such as those on Kaggle and the KDD Cup 2015. As the creators (<em class="italic">Tianqui Chen</em>, <em class="italic">Tong He</em>, and <em class="italic">Carlos Guestrin</em>) report in papers they wrote on the algorithm, out of 29 challenges held on Kaggle during 2015, 17 winning solutions used XGBoost as a standalone solution or as part of an ensemble of multiple different models. Since then, the algorithm has always retained a strong appeal among the community of data scientists, though it struggled to keep pace with the innovation brought about by other GBM implementations such as LightGBM and CatBoost.</p>
    <p class="normal">Aside from good performance both in terms of accuracy and computational efficiency, XGBoost is also a <em class="italic">scalable</em> solution, using at best multi-core processors as well as distributed machines. </p>
    <p class="normal">XGBoost represents a new generation of GBM algorithms thanks to important tweaks to the initial tree boost GBM algorithm:</p>
    <ul>
      <li class="bulletList">Sparsity-awareness; it can leverage sparse matrices, saving both memory (no need for dense matrices) and computation time (zero values are handled in a special way).</li>
      <li class="bulletList">Approximate tree learning (weighted quantile sketch), which produces similar results but in much less time compared to the classical complete explorations of possible branch cuts.</li>
      <li class="bulletList">Parallel computing on a single machine (using multi-threading during the search for the best split) and, similarly, distributed computations on multiple machines.</li>
      <li class="bulletList">Out-of-core computations on a single machine, leveraging a data storage solution called <strong class="keyWord">column block</strong>. This<a id="_idIndexMarker666"/> arranges data on a disk by columns, thus saving time by<a id="_idIndexMarker667"/> pulling data from the disk in the way the optimization algorithm (which works on column vectors) expects it.</li>
    </ul>
    <p class="normal">XGBoost can also deal with missing data in an effective way. Other tree ensembles based on standard decision trees require missing data first to be imputed using an off-scale value, such as a negative number, in order to develop an appropriate branching of the tree to deal with missing values.</p>
    <p class="normal">As for XGBoost’s parameters (<a href="https://xgboost.readthedocs.io/en/latest/parameter.html"><span class="url">https://xgboost.readthedocs.io/en/latest/parameter.html</span></a>), we have decided to highlight a few key ones you will<a id="_idIndexMarker668"/> find across competitions and projects:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_estimators</code>: Usually an integer ranging from 10 to 5,000.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: A real number ranging from 0.01 to 1.0, better sampled from the log-uniform distribution.</li>
      <li class="bulletList"><code class="inlineCode">min_child_weight</code>: Usually an integer between 1 and 10.</li>
      <li class="bulletList"><code class="inlineCode">max_depth</code>: Usually an integer between 1 and 50.</li>
      <li class="bulletList"><code class="inlineCode">max_delta_step</code>: Usually an integer sampled between 0 and 20, representing the maximum delta step we allow for each leaf output.</li>
      <li class="bulletList"><code class="inlineCode">subsample</code>: A real number from 0.1 to 1.0 indicating the proportion of examples to be subsampled.</li>
      <li class="bulletList"><code class="inlineCode">colsample_bytree</code>: A real number from 0.1 to 1.0 indicating the subsample ratio of columns by tree.</li>
      <li class="bulletList"><code class="inlineCode">colsample_bylevel</code>: A real number from 0.1 to 1.0 indicating the subsample ratio by level in trees.</li>
      <li class="bulletList"><code class="inlineCode">reg_lambda</code>: A real number between 1e-9 and 100.0, preferably sampled from the log-uniform distribution. This parameter controls the L2 regularization.</li>
      <li class="bulletList"><code class="inlineCode">reg_alpha</code>: A real number between 1e-9 and 100.0, preferably sampled from the log-uniform distribution. This parameter controls the L1 regularization.</li>
      <li class="bulletList"><code class="inlineCode">gamma</code>: Specifying the minimum loss reduction for tree partitioning, this parameter requires a real number between 1e-9 and 0.5, preferably sampled from the log-uniform distribution.</li>
      <li class="bulletList"><code class="inlineCode">scale_pos_weight</code>: A <a id="_idIndexMarker669"/>real number between 1e-6 and 500.0, preferably sampled from the log-uniform distribution, which represents a weight for the positive class.</li>
    </ul>
    <p class="normal">Like LightGBM, XGBoost also has many similar hyperparameters to tune, hence all of the considerations previously made for LightGBM are also valid for XGBoost.</p>
    <h3 id="_idParaDest-138" class="heading-3">CatBoost</h3>
    <p class="normal">In July 2017, Yandex, the Russian search engine, made another interesting GBM algorithm public, CatBoost (<a href="https://catboost.ai/"><span class="url">https://catboost.ai/</span></a>), whose<a id="_idIndexMarker670"/> name comes from putting together the two words “Category” and “Boosting.” In fact, its strong point is its ability to handle categorical <a id="_idIndexMarker671"/>variables, which make up most of the information in most relational databases, by adopting a mixed strategy of one-hot encoding and target encoding. Target encoding is a way to express categorical levels by assigning them an appropriate numeric value for the problem at hand; more on this can be found in <em class="chapterRef">Chapter 7</em>, <em class="italic">Modeling for Tabular Competitions</em>.</p>
    <p class="normal">The idea used by CatBoost to encode categorical variables is not new, but it is a kind of feature engineering that has been used before, mostly in data science competitions. Target encoding, also known as likelihood encoding, impact coding, or mean encoding, is simply a way to transform your labels into a number based on their association with the target variable. If you have a regression, you could transform labels based on the mean target value typical of that level; if it is a classification, it is simply the probability of classification of your target given that label (the probability of your target conditional on each category value). It may appear a simple and smart feature engineering trick but it has side effects, mostly in terms of overfitting, because you are taking information from the target into your predictors.</p>
    <p class="normal">CatBoost has quite a<a id="_idIndexMarker672"/> few parameters (see <a href="https://catboost.ai/en/docs/references/training-parameters/"><span class="url">https://catboost.ai/en/docs/references/training-parameters/</span></a>). We have limited our discussion to the eight most important ones:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">iterations</code>: Usually an integer between 10 and 1,000, but it can increase based on the problem.</li>
      <li class="bulletList"><code class="inlineCode">depth</code>: An integer between 1 and 8; usually higher values require longer fitting times and do not produce better results.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: A real value between 0.01 and 1.0, better sampled from the log-uniform distribution.</li>
      <li class="bulletList"><code class="inlineCode">random_strength</code>: A real number log-linearly sampled from the range 1e-9 to 10.0, which specifies the randomness level for scoring splits.</li>
      <li class="bulletList"><code class="inlineCode">bagging_temperature</code>: A real value between 0.0 and 1.0 that sets the Bayesian bootstrap.</li>
      <li class="bulletList"><code class="inlineCode">border_count</code>: An integer between 1 and 255 indicating the splits for numerical features.</li>
      <li class="bulletList"><code class="inlineCode">l2_leaf_reg</code>: An integer between 2 and 30; the value for L2 regularization.</li>
      <li class="bulletList"><code class="inlineCode">scale_pos_weight</code>: A real<a id="_idIndexMarker673"/> number between 0.01 and 10.0 representing the weight for the positive class.</li>
    </ul>
    <p class="normal">Even if CatBoost may appear to be just another GBM implementation, it has quite a few differences (highlighted also by the different parameters being used) that may provide great help in a competition, both as a single-model solution and as a model integrated into a larger ensemble. </p>
    <h3 id="_idParaDest-139" class="heading-3">HistGradientBoosting</h3>
    <p class="normal">Recently, Scikit-learn has introduced a <a id="_idIndexMarker674"/>new version of gradient boosting inspired by LightGBM’s binned data and histograms (see this presentation at EuroPython by <em class="italic">Olivier Grisel</em>: <a href="https://www.youtube.com/watch?v=urVUlKbQfQ4"><span class="url">https://www.youtube.com/watch?v=urVUlKbQfQ4</span></a>). Either as a classifier (<code class="inlineCode">HistGradientBoostingClassifier</code>) or a regressor (<code class="inlineCode">HistGradientBoostingRegressor</code>), it can be used for enriching ensembles with different models and it presents a much <a id="_idIndexMarker675"/>shorter and essential range of hyperparameters to be tuned:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: A real number between 0.01 and 1.0, usually sampled from a log-uniform distribution.</li>
      <li class="bulletList"><code class="inlineCode">max_iter</code>: An integer that can range from 10 to 10,000.</li>
      <li class="bulletList"><code class="inlineCode">max_leaf_nodes</code>: An integer from 2 to 500. It interacts with <code class="inlineCode">max_depth</code>; it is advisable to set only one of the two and leave the other set to <code class="inlineCode">None</code>.</li>
      <li class="bulletList"><code class="inlineCode">max_depth</code>: An integer between 2 and 12.</li>
      <li class="bulletList"><code class="inlineCode">min_samples_leaf</code>: An integer between 2 and 300.</li>
      <li class="bulletList"><code class="inlineCode">l2_regularization</code>: A float between 0.0 and 100.0.</li>
      <li class="bulletList"><code class="inlineCode">max_bins</code>: An integer between 32 and 512.</li>
    </ul>
    <p class="normal">Even if Scikit-learn’s <code class="inlineCode">HistGradientBoosting</code> is nothing too different from LightGBM or XGBoost, it does provide a different way to implement GBMs in a competition, and models built by <code class="inlineCode">HistGradientBoosting</code> may provide a contribution when ensembling multiple predictions, such as in blending and stacking.</p>
    <p class="normal">Having reached the end <a id="_idIndexMarker676"/>of this section, you should be more familiar with the most common machine learning algorithms (only deep learning solutions have not been discussed) and their most important hyperparameters to tune, which will help you in building an outstanding solution in a Kaggle competition. Knowing the basic optimization strategies, usable algorithms, and their key hyperparameters is just a starting point. In the next section, we will begin an in-depth discussion about how to tune them more optimally using Bayesian optimization.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Alberto_Danese.png" alt=""/>
      </div>
      <p class="intervieweeName">Alberto Danese</p>
      <p class="normal"><a href="https://www.kaggle.com/albedan"><span class="url">https://www.kaggle.com/albedan</span></a></p>
      <p class="normal">Our second <a id="_idIndexMarker677"/>interview of the chapter is with Alberto Danese, Head of Data Science at Nexi, an Italian credit card and digital payments company. A Competitions Grandmaster who joined the platform in 2015, he obtained most of his gold medals as a solo competitor.</p>
      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">I’ve always worked in the Financial Services industry, dealing mostly with structured data, and I do prefer competitions that belong to this category. I enjoy being able to have a practical grasp of what the data is all about and doing some smart feature engineering in order to squeeze every bit of information out of the data.</em></p>
      <p class="normal"><em class="italic">Technically speaking, I’ve got good experience with classical ML libraries and especially with Gradient Boosting Decision Trees: the most common libraries (XGBoost, LightGBM, CatBoost) are always my first choice.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">I always spend a lot of time just exploring the data and trying to figure out what the problem that the sponsor is actually trying to solve with machine learning is. Different from what newbies usually think about Kaggle, I don’t spend so much time on all the “tweaking” of the specific ML algorithm – and apparently this approach has paid off!</em></p>

      <p class="normal"><em class="italic">In my daily job, understanding the data is also extremely important, but there are some additional phases that are completely missing in a Kaggle competition. I’ve got to:</em></p>
      <ul>
        <li class="bulletList"><em class="italic">Define a business problem to be solved with ML (together with colleagues in the business departments)</em></li>
        <li class="bulletList"><em class="italic">Find the data, sometimes also from external data providers</em></li>
        <li class="bulletList"><em class="italic">And when the ML part is done, understand how to put it in production and manage the evolutions</em></li>
      </ul>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">I enjoyed the</em><em class="italic"><a id="_idIndexMarker678"/></em><em class="italic"> </em>TalkingData AdTracking Fraud Detection Challenge<em class="italic">, with which I became a Grandmaster. Besides being on an extremely interesting topic (fighting fraud from click-farms), it really pushed me to do efficient feature engineering, as the volumes were huge (more than 100M labeled rows) and cutting on computation times was key in order to test different approaches. It also forced me to understand how to exploit lag/lead features (and other window functions) in the best way, in order to create a sort of time series in an otherwise classical ML problem.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Definitely! Being able to achieve great objective and verifiable results is no doubt something that makes a resume stand out. When I was hired by Cerved (a marketing intelligence service company) in 2016, the hiring manager was perfectly aware of what Kaggle was – and having some real-world projects to talk about during an interview is something extremely valuable. For sure Kaggle had an important role in the evolution of my career.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">I think that everyone just starts coding, maybe forking a public kernel and just changing a few lines or parameters. This is perfectly fine at the beginning! But you do have to spend a decent amount of time not coding, but studying the data and understanding the problem.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">Not sure if it counts as a mistake, but I have often preferred to compete solo: on one hand it’s great as it forces you to handle every single aspect of a competition, and you’re able to manage your time as you wish. But I’ve really enjoyed collaborating with teammates on a couple of competitions as well: I probably should consider teaming up more often, as you can learn a lot from collaborating.</em></p>

      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">Besides the</em><em class="italic"><a id="_idIndexMarker679"/></em><em class="italic"> usual ones, I’ve always been a great fan of </em><code class="inlineCode">data.table</code><em class="italic"> (starting from the R version): I think it’s not getting the credit it deserves! It’s really a great package when you want to deal with huge data on a local machine.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">Understand the problem and the data first: don’t start coding right away!</em></p>
    </div>
    <h1 id="_idParaDest-140" class="heading-1">Bayesian optimization</h1>
    <p class="normal">Leaving behind grid <a id="_idIndexMarker680"/>search (feasible only when the space of experiments is limited), the usual choice for the practitioner is to apply random search optimization or try a <strong class="keyWord">Bayesian optimization </strong>(<strong class="keyWord">BO</strong>) technique, which requires a more complex setup.</p>
    <p class="normal">Originally introduced in the paper <em class="italic">Practical Bayesian optimization of machine learning algorithms</em> by Snoek, J., Larochelle, H., and Adams, R. P. (<a href="http://export.arxiv.org/pdf/1206.2944"><span class="url">http://export.arxiv.org/pdf/1206.2944</span></a>), the key idea <a id="_idIndexMarker681"/>behind Bayesian optimization is that we <a id="_idIndexMarker682"/>optimize a <strong class="keyWord">proxy function</strong> (also called a <strong class="keyWord">surrogate function</strong>) rather than the true objective function (which grid search and random search both do). We do this if there are no gradients, if testing the true objective function is costly (if it is not, then we simply go for random search), and if the search space is noisy and complex enough.</p>
    <p class="normal">Bayesian <a id="_idIndexMarker683"/>search balances <em class="italic">exploration</em> with <em class="italic">exploitation</em>. At the start, it explores randomly, thus training the surrogate function as it goes. Based on that surrogate function, the search exploits its initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function. As the <em class="italic">Bayesian</em> part of the name suggests, we are using priors in order to make smarter decisions about sampling during optimization. This way, we reach a minimization more quickly by limiting the number of evaluations we need to make.</p>
    <p class="normal">Bayesian optimization uses<a id="_idIndexMarker684"/> an <strong class="keyWord">acquisition function</strong> to tell us how promising an observation will be. In fact, to manage the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point.</p>
    <p class="normal">Usually, Bayesian optimization is powered by Gaussian processes. Gaussian processes perform better when the search space has a smooth and predictable response. An alternative when the search space is more complex is using tree algorithms (for instance, random forests), or a completely <a id="_idIndexMarker685"/>different approach called <strong class="keyWord">Tree Parzen Estimators</strong> or <strong class="keyWord">Tree-structured Parzen Estimators</strong> (<strong class="keyWord">TPEs</strong>). </p>
    <p class="normal">Instead of directly building a model that estimates<a id="_idIndexMarker686"/> the success of a set of parameters, thus acting like an oracle, TPEs estimate the parameters of a multivariate distribution that define the best-performing values of the parameters, based on successive approximations provided by the experimentations. In this way, TPEs derive the best set of parameters by sampling them from a probabilistic distribution, and not directly from a machine learning model like Gaussian processes does. </p>
    <p class="normal">We will discuss each of these approaches, first by examining Scikit-optimize and KerasTuner, both based on Gaussian processes (Scikit-optimize can also use random forests and KerasTuner can use multi-armed bandits), and then Optuna, which is principally based on TPE (though it also offers different strategies: <a href="https://optuna.readthedocs.io/en/stable/reference/samplers.html"><span class="url">https://optuna.readthedocs.io/en/stable/reference/samplers.html</span></a>).</p>
    <div class="packt_tip">
      <p class="normal">Though Bayesian optimization is considered the state of the art for hyperparameter tuning, always keep in mind that for more complex parameter spaces, using Bayesian optimization provides no advantage in terms of time and computation spent over a solution simply found by random search. For instance, in Google Cloud Machine Learning Engine services, the usage of Bayesian optimization is limited to problems involving at most sixteen parameters. For larger numbers of parameters, it resorts to random sampling.</p>
    </div>
    <h2 id="_idParaDest-141" class="heading-2">Using Scikit-optimize</h2>
    <p class="normal">Scikit-optimize (<code class="inlineCode">skopt</code>) has <a id="_idIndexMarker687"/>been developed using the same API as Scikit-learn, as well as making extensive use of NumPy and SciPy functions. In addition, it was created by some of the contributors to the Scikit-learn project, such as <em class="italic">Gilles Louppe</em>.</p>
    <p class="normal">Based on Gaussian process algorithms, the package is well maintained, though sometimes it has to catch up because of improvements on the Scikit-learn, NumPy, or SciPy sides. For instance, at the time of writing, in order to run it properly on Kaggle Notebooks you have to roll back to older versions of these packages, as explained in a GitHub issue (<a href="https://github.com/scikit-optimize/scikit-optimize/issues/981"><span class="url">https://github.com/scikit-optimize/scikit-optimize/issues/981</span></a>).</p>
    <p class="normal">The package has an intuitive API and it is quite easy to hack it and use its functions in custom optimization strategies. Scikit-optimize is also renowned for its useful graphical representations. In fact, by visualizing the results of an optimization process (using Scikit-optimize’s <code class="inlineCode">plot_objective</code> function), you can figure out whether you can re-define the search space for the problem and formulate an explanation of how optimization works for a problem. </p>
    <p class="normal">In our worked example, we will refer to the work that can be found in the following Kaggle Notebooks:</p>
    <ul>
      <li class="bulletList"><a href="https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm"><span class="url">https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm</span></a></li>
      <li class="bulletList"><a href="https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm"><span class="url">https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm</span></a></li>
    </ul>
    <p class="normal">Our purpose here is to<a id="_idIndexMarker688"/> show you how to quickly handle an optimization problem for a competition such as <em class="italic">30 Days of ML</em>, a recent competition that involved many Kagglers in learning new skills and applying them in a competition lasting 30 days. The goal of this competition is to predict the value of an insurance claim, so it is a regression problem. You can find out more about this initiative and download the data necessary for the example we are going to present (materials are always available to the public), by visiting <a href="https://www.kaggle.com/thirty-days-of-ml"><span class="url">https://www.kaggle.com/thirty-days-of-ml</span></a>.</p>
    <div class="note">
      <p class="normal">If you cannot access the data because you have not taken part in the competition previously, you can use this Kaggle Dataset: <a href="https://www.kaggle.com/lucamassaron/30-days-of-ml"><span class="url">https://www.kaggle.com/lucamassaron/30-days-of-ml</span></a>.</p>
    </div>
    <p class="normal">The following code<a id="_idIndexMarker689"/> will present how to load the data for this problem and then set up a Bayesian optimization process that will improve the performance of a LightGBM model.</p>
    <p class="normal">We start by loading the packages:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Importing core libraries</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> pprint
<span class="hljs-keyword">import</span> joblib
<span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial
<span class="hljs-comment"># Suppressing warnings because of skopt verbosity</span>
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"</span><span class="hljs-string">ignore"</span>)
<span class="hljs-comment"># Classifiers</span>
<span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb
<span class="hljs-comment"># Model selection</span>
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold
<span class="hljs-comment"># Metrics</span>
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> make_scorer
<span class="hljs-comment"># Skopt functions</span>
<span class="hljs-keyword">from</span> skopt <span class="hljs-keyword">import</span> BayesSearchCV
<span class="hljs-keyword">from</span> skopt.callbacks <span class="hljs-keyword">import</span> DeadlineStopper, DeltaYStopper
<span class="hljs-keyword">from</span> skopt.space <span class="hljs-keyword">import</span> Real, Categorical, Integer
</code></pre>
    <p class="normal">As a next step, we load the data. The data doesn’t need much processing, aside from turning some categorical features with alphabetical letters as levels into ordered numeric ones:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Loading data </span>
X = pd.read_csv(<span class="hljs-string">"../input/30-days-of-ml/train.csv"</span>)
X_test = pd.read_csv(<span class="hljs-string">"../input/30-days-of-ml/test.csv"</span>)
<span class="hljs-comment"># Preparing data as a tabular matrix</span>
y = X.target
X = X.set_index(<span class="hljs-string">'id'</span>).drop(<span class="hljs-string">'target'</span>, axis=<span class="hljs-string">'columns'</span>)
X_test = X_test.set_index(<span class="hljs-string">'id'</span>)
<span class="hljs-comment"># Dealing with categorical data</span>
categoricals = [item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> X.columns <span class="hljs-keyword">if</span> <span class="hljs-string">'cat'</span> <span class="hljs-keyword">in</span> item]
cat_values = np.unique(X[categoricals].values)
cat_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(cat_values, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(cat_values))))
X[categoricals] = X[categoricals].replace(cat_dict).astype(<span class="hljs-string">'category'</span>)
X_test[categoricals] = X_test[categoricals].replace(cat_dict).astype(<span class="hljs-string">'category'</span>)
</code></pre>
    <p class="normal">After making the data <a id="_idIndexMarker690"/>available, we define a reporting function that can be used by Scikit-optimize for various optimization tasks. The function takes the data and the optimizer as inputs. It can also handle <strong class="keyWord">callback functions</strong>, which are functions that perform actions such as reporting, early stopping based on having reached a certain threshold of time spent searching or performance not improving (for instance, not seeing improvements for a certain number of iterations), or saving the state of the processing after each optimization iteration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Reporting util for different optimizers</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">report_perf</span>(<span class="hljs-params">optimizer, X, y, title=</span><span class="hljs-string">"model"</span><span class="hljs-params">, callbacks=</span><span class="hljs-literal">None</span>):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    A wrapper for measuring time and performance of optimizers</span>
<span class="hljs-string">    optimizer = a sklearn or a skopt optimizer</span>
<span class="hljs-string">    X = the training set </span>
<span class="hljs-string">    y = our target</span>
<span class="hljs-string">    title = a string label for the experiment</span>
<span class="hljs-string">    """</span>
    start = time()
    
    <span class="hljs-keyword">if</span> callbacks <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        optimizer.fit(X, y, callback=callbacks)
    <span class="hljs-keyword">else</span>:
        optimizer.fit(X, y)
        
    d=pd.DataFrame(optimizer.cv_results_)
    best_score = optimizer.best_score_
    best_score_std = d.iloc[optimizer.best_index_].std_test_score
    best_params = optimizer.best_params_
    
    <span class="hljs-built_in">print</span>((title + <span class="hljs-string">" took %.2f seconds, candidates checked: %d, best CV            score: %.3f"</span> + <span class="hljs-string">u" \u00B1"</span>+<span class="hljs-string">" %.3f"</span>) % 
                             (time() - start,
                             <span class="hljs-built_in">len</span>(optimizer.cv_results_[<span class="hljs-string">'params'</span>]),
                             best_score, 
                             best_score_std))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Best parameters:'</span>)
    pprint.pprint(best_params)
    <span class="hljs-built_in">print</span>()
    <span class="hljs-keyword">return</span> best_params
</code></pre>
    <p class="normal">We now have to prepare the<a id="_idIndexMarker691"/> scoring function (upon which the evaluation is based), the validation strategy (based on cross-validation), the model, and the search space. For the scoring function, which should be a root mean squared error metric, we refer to the practices in Scikit-learn where you always minimize a function (if you have to maximize, you minimize its negative). </p>
    <p class="normal">The <code class="inlineCode">make_scorer</code> wrapper can easily replicate such practices: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setting the scoring function</span>
scoring = make_scorer(partial(mean_squared_error, squared=<span class="hljs-literal">False</span>),
                      greater_is_better=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># Setting the validation strategy</span>
kf = KFold(n_splits=<span class="hljs-number">5</span>, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">0</span>)
<span class="hljs-comment"># Setting the basic regressor</span>
reg = lgb.LGBMRegressor(boosting_type=<span class="hljs-string">'gbdt'</span>,
                        metric=<span class="hljs-string">'rmse'</span>,
                        objective=<span class="hljs-string">'regression'</span>,
                        n_jobs=<span class="hljs-number">1</span>, 
                        verbose=-<span class="hljs-number">1</span>,
                        random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Setting the search space requires the use of different functions from Scikit-optimize, such as <code class="inlineCode">Real</code>, <code class="inlineCode">Integer</code>, or <code class="inlineCode">Choice</code>, each one sampling from a different kind of distribution that you define as a parameter (usually the uniform distribution, but the log-uniform is also used when you are more interested in the scale effect of a parameter than its exact value):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setting the search space</span>
search_spaces = {
     
     <span class="hljs-comment"># Boosting learning rate</span>
    <span class="hljs-string">'learning_rate'</span>: Real(<span class="hljs-number">0.01</span>, <span class="hljs-number">1.0</span>, <span class="hljs-string">'log-uniform'</span>),
     
     <span class="hljs-comment"># Number of boosted trees to fit</span>
    <span class="hljs-string">'n_estimators'</span>: Integer(<span class="hljs-number">30</span>, <span class="hljs-number">5000</span>),
     
     <span class="hljs-comment"># Maximum tree leaves for base learners</span>
    <span class="hljs-string">'num_leaves'</span>: Integer(<span class="hljs-number">2</span>, <span class="hljs-number">512</span>),
    
     <span class="hljs-comment"># Maximum tree depth for base learners</span>
    <span class="hljs-string">'max_depth'</span>: Integer(-<span class="hljs-number">1</span>, <span class="hljs-number">256</span>),
     <span class="hljs-comment"># Minimal number of data in one leaf</span>
    <span class="hljs-string">'min_child_samples'</span>: Integer(<span class="hljs-number">1</span>, <span class="hljs-number">256</span>),
     <span class="hljs-comment"># Max number of bins buckets</span>
    <span class="hljs-string">'max_bin'</span>: Integer(<span class="hljs-number">100</span>, <span class="hljs-number">1000</span>),
     <span class="hljs-comment"># Subsample ratio of the training instance </span>
    <span class="hljs-string">'subsample'</span>: Real(<span class="hljs-number">0.01</span>, <span class="hljs-number">1.0</span>, <span class="hljs-string">'uniform'</span>),
     <span class="hljs-comment"># Frequency of subsample </span>
    <span class="hljs-string">'subsample_freq'</span>: Integer(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>),
                
     <span class="hljs-comment"># Subsample ratio of columns</span>
    <span class="hljs-string">'colsample_bytree'</span>: Real(<span class="hljs-number">0.01</span>, <span class="hljs-number">1.0</span>, <span class="hljs-string">'uniform'</span>), 
    
     <span class="hljs-comment"># Minimum sum of instance weight</span>
    <span class="hljs-string">'min_child_weight'</span>: Real(<span class="hljs-number">0.01</span>, <span class="hljs-number">10.0</span>, <span class="hljs-string">'uniform'</span>),
   
     <span class="hljs-comment"># L2 regularization</span>
    <span class="hljs-string">'reg_lambda'</span>: Real(<span class="hljs-number">1e-9</span>, <span class="hljs-number">100.0</span>, <span class="hljs-string">'log-uniform'</span>),
         
     <span class="hljs-comment"># L1 regularization</span>
    <span class="hljs-string">'reg_alpha'</span>: Real(<span class="hljs-number">1e-9</span>, <span class="hljs-number">100.0</span>, <span class="hljs-string">'log-uniform'</span>),
   }
</code></pre>
    <p class="normal">Once you have <a id="_idIndexMarker692"/>defined:</p>
    <ul>
      <li class="bulletList">Your cross-validation strategy</li>
      <li class="bulletList">Your evaluation metric</li>
      <li class="bulletList">Your base model</li>
      <li class="bulletList">Your hyperparameter search space</li>
    </ul>
    <p class="normal">All that is left is just to feed them into your optimization function, <code class="inlineCode">BayesSearchCV</code>. Based on the CV scheme provided, this function will look for the minimum of your scoring function based on values within the search space. You can set a maximum number of iterations performed, the kind of surrogate function (Gaussian processes (<code class="inlineCode">GP</code>) works on most occasions), and the <a id="_idIndexMarker693"/>random seed for reproducibility:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Wrapping everything up into the Bayesian optimizer</span>
opt = BayesSearchCV(estimator=reg,
                    search_spaces=search_spaces,
                    scoring=scoring,
                    cv=kf,
                    n_iter=<span class="hljs-number">60</span>,           <span class="hljs-comment"># max number of trials</span>
                    n_jobs=-<span class="hljs-number">1</span>,           <span class="hljs-comment"># number of jobs</span>
                    iid=<span class="hljs-literal">False</span>,         
                    <span class="hljs-comment"># if not iid it optimizes on the cv score</span>
                    return_train_score=<span class="hljs-literal">False</span>,
                    refit=<span class="hljs-literal">False</span>,  
                    <span class="hljs-comment"># Gaussian Processes (GP) </span>
                    optimizer_kwargs={<span class="hljs-string">'</span><span class="hljs-string">base_estimator'</span>: <span class="hljs-string">'GP'</span>},
                    <span class="hljs-comment"># random state for replicability</span>
                    random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">At this point, you can start the search using the reporting function we defined previously. After a while, the function will return the best parameters for the problem.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Running the optimizer</span>
overdone_control = DeltaYStopper(delta=<span class="hljs-number">0.0001</span>)
<span class="hljs-comment"># We stop if the gain of the optimization becomes too small</span>
time_limit_control = DeadlineStopper(total_time=<span class="hljs-number">60</span> * <span class="hljs-number">60</span> * <span class="hljs-number">6</span>)
<span class="hljs-comment"># We impose a time limit (6 hours)</span>
best_params = report_perf(opt, X, y,<span class="hljs-string">'LightGBM_regression'</span>, 
                          callbacks=[overdone_control, time_limit_control])
</code></pre>
    <p class="normal">In the example, we set a limit on operations by specifying a maximum time allowed (6 hours) before stopping and reporting the best results. Since the Bayesian optimization approach blends together exploration and exploitation of different combinations of hyperparameters, stopping at any time will always return the best solution found so far (but not necessarily the best one possible). This is because the acquisition function will always give priority of exploration to the most promising parts of the search space, based on the estimated performances returned by the surrogate function and their uncertainty intervals. </p>
    <h2 id="_idParaDest-142" class="heading-2">Customizing a Bayesian optimization search</h2>
    <p class="normal">The <code class="inlineCode">BayesSearchCV</code> function <a id="_idIndexMarker694"/>offered by Scikit-optimize <a id="_idIndexMarker695"/>is certainly convenient, because it wraps and arranges all the elements of a hyperparameter search by itself, but it also has limitations. For instance, you may find it useful in a competition to:</p>
    <ul>
      <li class="bulletList">Have more control over each search iteration, for instance mixing random search and Bayesian search</li>
      <li class="bulletList">Be able to apply early stopping on algorithms</li>
      <li class="bulletList">Customize your validation strategy more</li>
      <li class="bulletList">Stop experiments that do not work early (for instance, immediately evaluating the performance of the single cross-validation folds when it is available, instead of waiting to have all folds averaged at the end)</li>
      <li class="bulletList">Create clusters of hyperparameter sets that perform in a similar way (for instance, in order to create multiple models differing only in the hyperparameters used, to be used for a blending ensemble)</li>
    </ul>
    <p class="normal">Each of these tasks would not be too complex if you could modify the <code class="inlineCode">BayesSearchCV</code> internal procedure. Luckily, Scikit-optimize lets you do just this. In fact, behind <code class="inlineCode">BayesSearchCV</code>, as well as behind other wrappers from the package, there are specific minimizing functions that you can use as standalone parts of your own search function:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">gp_minimize</code>: Bayesian optimization using Gaussian processes</li>
      <li class="bulletList"><code class="inlineCode">forest_minimize</code>: Bayesian optimization using random forests or extremely randomized trees</li>
      <li class="bulletList"><code class="inlineCode">gbrt_minimize</code>: Bayesian optimization using gradient boosting</li>
      <li class="bulletList"><code class="inlineCode">dummy_minimize</code>: Just random search</li>
    </ul>
    <p class="normal">In the following example, we are going to modify the previous search using our own custom search function. The new custom function will accept early stopping during training and it will prune experiments if one of the fold validation results is not a top-performing one. </p>
    <div class="note">
      <p class="normal">You can find the next example working in a Kaggle Notebook at <a href="https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization"><span class="url">https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization</span></a>.</p>
    </div>
    <p class="normal">As in the previous <a id="_idIndexMarker696"/>example, we start by importing the necessary packages.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Importing core libraries</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> pprint
<span class="hljs-keyword">import</span> joblib
<span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial
<span class="hljs-comment"># Suppressing warnings because of skopt verbosity</span>
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
<span class="hljs-comment"># Classifier/Regressor</span>
<span class="hljs-keyword">from</span> xgboost <span class="hljs-keyword">import</span> XGBRegressor
<span class="hljs-comment"># Model selection</span>
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold, StratifiedKFold
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-comment"># Metrics</span>
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> make_scorer
<span class="hljs-comment"># Skopt functions</span>
<span class="hljs-keyword">from</span> skopt <span class="hljs-keyword">import</span> BayesSearchCV
<span class="hljs-keyword">from</span> skopt.callbacks <span class="hljs-keyword">import</span> DeadlineStopper, DeltaYStopper
<span class="hljs-keyword">from</span> skopt.space <span class="hljs-keyword">import</span> Real, Categorical, Integer
<span class="hljs-keyword">from</span> skopt <span class="hljs-keyword">import</span> gp_minimize, forest_minimize
<span class="hljs-keyword">from</span> skopt <span class="hljs-keyword">import</span> gbrt_minimize, dummy_minimize
<span class="hljs-comment"># Decorator to convert a list of parameters to named arguments</span>
<span class="hljs-keyword">from</span> skopt.utils <span class="hljs-keyword">import</span> use_named_args 
<span class="hljs-comment"># Data processing</span>
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OrdinalEncoder
</code></pre>
    <p class="normal">In the same way as before, we upload the data from the <em class="italic">30 Days of ML</em> competition:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Loading data </span>
X_train = pd.read_csv(<span class="hljs-string">"../input/30-days-of-ml/train.csv"</span>)
X_test = pd.read_csv(<span class="hljs-string">"../input/30-days-of-ml/test.csv"</span>)
<span class="hljs-comment"># Preparing data as a tabular matrix</span>
y_train = X_train.target
X_train = X_train.set_index(<span class="hljs-string">'id'</span>).drop(<span class="hljs-string">'target'</span>, axis=<span class="hljs-string">'columns'</span>)
X_test = X_test.set_index(<span class="hljs-string">'</span><span class="hljs-string">id'</span>)
<span class="hljs-comment"># Pointing out categorical features</span>
categoricals = [item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> X_train.columns <span class="hljs-keyword">if</span> <span class="hljs-string">'cat'</span> <span class="hljs-keyword">in</span> item]
<span class="hljs-comment"># Dealing with categorical data using OrdinalEncoder</span>
ordinal_encoder = OrdinalEncoder()
X_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])
X_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])
</code></pre>
    <p class="normal">Now we set all the <a id="_idIndexMarker697"/>necessary elements for a hyperparameter search, that is, the scoring function, the validation strategy, the search space, and the machine learning model to be optimized. The scoring function and the validation strategy will later become the core elements constituting the objective function, the function the Bayesian optimization will strive to minimize.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setting the scoring function</span>
scoring = partial(mean_squared_error, squared=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># Setting the cv strategy</span>
kf = KFold(n_splits=<span class="hljs-number">5</span>, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">0</span>)
<span class="hljs-comment"># Setting the search space</span>
space = [Real(<span class="hljs-number">0.01</span>, <span class="hljs-number">1.0</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'learning_rate'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">8</span>, name=<span class="hljs-string">'max_depth'</span>),
         Real(<span class="hljs-number">0.1</span>, <span class="hljs-number">1.0</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'subsample'</span>),
         <span class="hljs-comment"># Subsample ratio of columns by tree</span>
         Real(<span class="hljs-number">0.1</span>, <span class="hljs-number">1.0</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'colsample_bytree'</span>),  
         <span class="hljs-comment"># L2 regularization</span>
         Real(<span class="hljs-number">0</span>, <span class="hljs-number">100.</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'reg_lambda'</span>),
         <span class="hljs-comment"># L1 regularization</span>
         Real(<span class="hljs-number">0</span>, <span class="hljs-number">100.</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'reg_alpha'</span>),
         <span class="hljs-comment"># minimum sum of instance weight (hessian)  </span>
         Real(<span class="hljs-number">1</span>, <span class="hljs-number">30</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'min_child_weight'</span>)
         ]
model = XGBRegressor(n_estimators=<span class="hljs-number">10_000</span>, 
                     booster=<span class="hljs-string">'gbtree'</span>, random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Notice this time that we have not included the number of estimators (the <code class="inlineCode">n_estimators</code> parameter) in the search space. Instead, we set it when instantiating the model and we enter a high value, since we expect to stop the model early based on a validation set.</p>
    <p class="normal">As a next step, you <a id="_idIndexMarker698"/>now need to create the objective function. The objective function should just accept as input the parameters to be optimized and return the resulting score. However, the objective function also needs to accept the elements necessary for the search you have just prepared. Naturally, you could refer to them from inside the function. However, it is a good practice to take them into the function itself, in its internal memory space. This has its advantages; for instance, you will make the elements immutable and they will be carried along with the objective function (by pickling or if you distribute the search task on a multi-processor level). You can obtain this second result by creating a <code class="inlineCode">make</code> function that takes in the elements, with the modified objective function being returned by the <code class="inlineCode">make</code> function. With this simple structure, your objective function will incorporate all the elements such as the data and the model, and you will only need to pass in the parameters to be tested.</p>
    <p class="normal">Let’s start coding the function. We will stop along the way to discuss some relevant aspects: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># The objective function to be minimized</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">make_objective</span>(<span class="hljs-params">model, X, y, space, cv, scoring, validation=</span><span class="hljs-number">0.2</span>):
    <span class="hljs-comment"># This decorator converts your objective function </span>
    <span class="hljs-comment"># with named arguments into one that accepts a list as argument,</span>
    <span class="hljs-comment"># while doing the conversion automatically.</span>
<span class="hljs-meta">    @use_named_args(</span><span class="hljs-params">space</span><span class="hljs-meta">) </span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">objective</span>(<span class="hljs-params">**params</span>):
        model.set_params(**params)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTesting: "</span>, params)
        validation_scores = <span class="hljs-built_in">list</span>()
        <span class="hljs-keyword">for</span> k, (train_index, test_index) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kf.split(X, y)):
            val_index = <span class="hljs-built_in">list</span>()
            train_examples = <span class="hljs-built_in">int</span>(train_examples * (<span class="hljs-number">1</span> - validation))
            train_index, val_index = (train_index[:train_examples], 
                                      train_index[train_examples:])
            
            start_time = time()
            model.fit(X.iloc[train_index,:], y[train_index],
                      early_stopping_rounds=<span class="hljs-number">50</span>,
                      eval_set=[(X.iloc[val_index,:], y[val_index])], 
                      verbose=<span class="hljs-number">0</span>
                    )
            end_time = time()
            
            rounds = model.best_iteration
            
            test_preds = model.predict(X.iloc[test_index,:])
            test_score = scoring(y[test_index], test_preds)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"CV Fold </span><span class="hljs-subst">{k+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> rmse:</span><span class="hljs-subst">{test_score:</span><span class="hljs-number">0.5</span><span class="hljs-subst">f}</span><span class="hljs-string">-</span><span class="hljs-subst">{rounds}</span><span class="hljs-string"> </span>
<span class="hljs-string">                  rounds - it took </span><span class="hljs-subst">{end_time-start_time:</span><span class="hljs-number">0.0</span><span class="hljs-subst">f}</span><span class="hljs-string"> secs"</span>)
            validation_scores.append(test_score)
</code></pre>
    <p class="normal">In this first part of the <a id="_idIndexMarker699"/>function, you simply create an objective function, doing cross-validation and fitting the data using early stopping. We have used an aggressive early stopping strategy to save time, but you could raise the number of patient rounds if you believe that it might work better for your problem. Notice that the validation examples are sequentially taken out from the examples in the training folds (see how <code class="inlineCode">train_index</code> and <code class="inlineCode">val_index</code> are defined in the code), leaving the out-of-fold examples (<code class="inlineCode">test_index</code> derived from the <code class="inlineCode">kf</code> cross-validation splitting) untouched for the final validation. This is important if you do not want to incur adaptive overfitting on the data you use for early stopping. </p>
    <p class="normal">In the next part, before moving on to the cross-validation loop and proceeding to the remaining cross-validation folds to be trained and tested, you analyze the result obtained by the fold on the out-of-fold set:</p>
    <pre class="programlisting code"><code class="hljs-code">            
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(history[k]) &gt;= <span class="hljs-number">10</span>:
                threshold = np.percentile(history[k], q=<span class="hljs-number">25</span>)
                <span class="hljs-keyword">if</span> test_score &gt; threshold:
                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Early stopping for under-performing fold: </span>
<span class="hljs-string">                          threshold is </span><span class="hljs-subst">{threshold:</span><span class="hljs-number">0.5</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
                    <span class="hljs-keyword">return</span> np.mean(validation_scores)
                
            history[k].append(test_score)
        <span class="hljs-keyword">return</span> np.mean(validation_scores)
    <span class="hljs-keyword">return</span> objective
</code></pre>
    <p class="normal">Notice that we are keeping a global dictionary, <code class="inlineCode">history</code>, containing the results obtained from each fold up to now. We can compare the results across multiple experiments and cross-validations; the cross-validation is reproducible due to the random seed, so the results of the same fold are perfectly comparable. If the result of the present fold is sub-par compared to the previously obtained folds in other iterations (using the bottom quartile as a reference), the idea is to stop and return the average of the folds tested so far. The rationale for this is that if one fold doesn’t present acceptable results, then the whole cross-validation probably won’t either. You can therefore just quit and move on to another set of more promising parameters. It is a kind of early stopping on cross-validation that should speed up your search and allow you to cover more experiments in less time.</p>
    <p class="normal">Next, using our <code class="inlineCode">make_objective</code> function, we put together all the elements (model, data, search space, validation strategy, and scoring function) into a single function, the objective function. As a <a id="_idIndexMarker700"/>result, we now have a function that only takes in the parameters to be optimized and returns a score, based on which the minimization engine of the optimization will decide the next experiments:</p>
    <pre class="programlisting code"><code class="hljs-code">objective = make_objective(model,
                           X_train, y_train,
                           space=space,
                           cv=kf,
                           scoring=scoring)
</code></pre>
    <p class="normal">Since we want to control each step of the optimization and save it for later use, we also prepare a callback function that will save a list of the experiments executed and their results, at every iteration of the minimization process. Simply by using these two pieces of information, the minimization engine can be halted at any time, and it can thereafter resume the optimization from the checkpoint:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">onstep</span>(<span class="hljs-params">res</span>):
    <span class="hljs-keyword">global</span> counter
    x0 = res.x_iters   <span class="hljs-comment"># List of input points</span>
    y0 = res.func_vals <span class="hljs-comment"># Evaluation of input points</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Last eval: '</span>, x0[-<span class="hljs-number">1</span>], 
          <span class="hljs-string">' - Score '</span>, y0[-<span class="hljs-number">1</span>])
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'</span><span class="hljs-string">Current iter: '</span>, counter, 
          <span class="hljs-string">' - Best Score '</span>, res.fun, 
          <span class="hljs-string">' - Best Args: '</span>, res.x)
    <span class="hljs-comment"># Saving a checkpoint to disk</span>
    joblib.dump((x0, y0), <span class="hljs-string">'checkpoint.pkl'</span>) 
    counter += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">At this point, we are ready to start. Bayesian optimization needs some starting points to work properly. We create a number of experiments with random search (using the <code class="inlineCode">dummy_minimize</code> function) and save their results:</p>
    <pre class="programlisting code"><code class="hljs-code">counter = <span class="hljs-number">0</span>
history = {i:<span class="hljs-built_in">list</span>() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)}
used_time = <span class="hljs-number">0</span>
gp_round = dummy_minimize(func=objective,
                          dimensions=space,
                          n_calls=<span class="hljs-number">30</span>,
                          callback=[onstep],
                          random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">We can then retrieve<a id="_idIndexMarker701"/> the saved experiments and print the sequence of sets of hyperparameters that the Bayesian optimization has tested, along with their results. In fact, we can find the set of parameters and their results contained in the <code class="inlineCode">x0</code> and <code class="inlineCode">y0</code> lists: </p>
    <pre class="programlisting code"><code class="hljs-code">x0, y0 = joblib.load(<span class="hljs-string">'</span><span class="hljs-string">checkpoint.pkl'</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(x0))
</code></pre>
    <p class="normal">At this point, we can even resume the Bayesian optimization with some changes in the search space, the acquisition function, the number of calls, or the callbacks: </p>
    <pre class="programlisting code"><code class="hljs-code">x0, y0 = joblib.load(<span class="hljs-string">'checkpoint.pkl'</span>)
gp_round = gp_minimize(func=objective,
                       x0=x0,    <span class="hljs-comment"># already examined values for x</span>
                       y0=y0,    <span class="hljs-comment"># observed values for x0</span>
                       dimensions=space,
                       acq_func=<span class="hljs-string">'gp_hedge'</span>,
                       n_calls=<span class="hljs-number">30</span>,
                       n_initial_points=<span class="hljs-number">0</span>,
                       callback=[onstep],
                       random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Once we are satisfied that we don’t need to continue calling the optimization function, we can print both the best score obtained (based on our inputs and validation scheme) and the set of best hyperparameters:</p>
    <pre class="programlisting code"><code class="hljs-code">x0, y0 = joblib.load(<span class="hljs-string">'checkpoint.pkl'</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Best score: </span><span class="hljs-subst">{gp_round.fun:</span><span class="hljs-number">0.5</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Best hyperparameters:"</span>)
<span class="hljs-keyword">for</span> sp, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(gp_round.space, gp_round.x):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{sp.name:</span><span class="hljs-number">25</span><span class="hljs-subst">}</span><span class="hljs-string"> : </span><span class="hljs-subst">{x}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">Based on the best result, we can re-train our model for use in the competition. </p>
    <p class="normal">Now we have the <a id="_idIndexMarker702"/>set of parameters and their results (the <code class="inlineCode">x0</code> and <code class="inlineCode">y0</code> lists), we could also explore the different results and cluster together the ones that are similar in output but different in the set of parameters used. This will help us to train a more diverse set of models with similar performances but different optimization strategies. This is the ideal situation for <strong class="keyWord">blending</strong>, which is the averaging of multiple models in order to<a id="_idIndexMarker703"/> lower the variance of the estimates and obtain a better public and private leaderboard score.</p>
    <div class="note">
      <p class="normal">Refer to <em class="chapterRef">Chapter 9</em>, <em class="italic">Ensembling with Blending and Stacking Solutions</em>, for a discussion on blending.</p>
    </div>
    <h2 id="_idParaDest-143" class="heading-2">Extending Bayesian optimization to neural architecture search</h2>
    <p class="normal">Moving on to deep<a id="_idIndexMarker704"/> learning, neural networks also seem to have quite a few hyperparameters to fix:</p>
    <ul>
      <li class="bulletList">Batch size</li>
      <li class="bulletList">Learning rate</li>
      <li class="bulletList">The kind of optimizer and its internal parameters</li>
    </ul>
    <p class="normal">All these parameters influence how the network learns and they can make a big impact; just a slight difference in batch size or learning rate can determine whether a network can reduce its error beyond a certain threshold or not. </p>
    <p class="normal">That being said, these learning parameters are not the only ones that you can optimize when working <a id="_idIndexMarker705"/>with <strong class="keyWord">deep neural networks </strong>(<strong class="keyWord">DNNs</strong>). How the network is organized in layers and the details of its architecture can make even more of a difference. </p>
    <p class="normal">In fact, technically speaking, an <strong class="keyWord">architecture</strong> implies the representational capacity of the deep neural network, which means that, depending on the layers you use, the network will either be able to read and process all the information available in the data, or it will not. While you had a large but limited set of choices with other machine learning algorithms, with DNNs your choices seem unlimited, because the only apparent limit is your knowledge and experience in handling parts of neural networks and putting them together.</p>
    <p class="normal">Common best practices for great deep learning practitioners when assembling well-performing DNNs depend mainly on:</p>
    <ul>
      <li class="bulletList">Relying on pre-trained models (so you have to be very knowledgeable about the solutions available, such as those found on Hugging Face (<a href="https://huggingface.co/models"><span class="url">https://huggingface.co/models</span></a>) or on GitHub)</li>
      <li class="bulletList">Reading cutting-edge papers</li>
      <li class="bulletList">Copying top Kaggle Notebooks from the same competition or previous ones</li>
      <li class="bulletList">Trial and error</li>
      <li class="bulletList">Ingenuity and luck</li>
    </ul>
    <p class="normal">In a famous lesson given<a id="_idIndexMarker706"/> by <em class="italic">Professor Geoffrey Hinton</em>, he states that you can achieve similar and often better results using automated methods such as Bayesian optimization. Bayesian optimization will also avoid you getting stuck because you cannot figure out the best combinations of hyperparameters among the many possible ones.</p>
    <div class="note">
      <p class="normal">For a recording of Prof. Geoffrey Hinton’s lesson, see <a href="https://www.youtube.com/watch?v=i0cKa0di_lo"><span class="url">https://www.youtube.com/watch?v=i0cKa0di_lo</span></a>.</p>
      <p class="normal">For the slides, see <a href="https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf"><span class="url">https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf</span></a>.</p>
    </div>
    <p class="normal">As we mentioned before, even in most sophisticated AutoML systems, when you have too many hyperparameters, relying on random optimization may produce better results or the same results in the same amount of time as Bayesian optimization. In addition, in this case, you also have to fight against an optimization landscape with sharp turns and surfaces; in DNN optimization, many of your parameters won’t be continuous but Boolean instead, and just one change could unexpectedly transform the performance of your network for the better or for the worse. </p>
    <p class="normal">Our experience tells us that random optimization may not be suitable for a Kaggle competition because:</p>
    <ul>
      <li class="bulletList">You have limited time and resources</li>
      <li class="bulletList">You can leverage your previous optimization results in order to find better solutions</li>
    </ul>
    <p class="normal">Bayesian optimization in this scenario is ideal: you can set it to work based on the time and computational resources that you have and do it by stages, refining your settings through multiple sessions. Moreover, it is unlikely that you will easily be able to leverage parallelism for tuning DNNs, since they use GPUs, unless you have multiple very powerful machines at hand. By working sequentially, Bayesian optimization just needs one good machine to perform the task. Finally, even if it is hard to find optimal architectures by a search, due to the optimization landscape you leverage information from previous experiments, especially at the beginning, totally avoiding combinations of parameters that won’t work. With random optimization, unless you change the search space along the way, all combinations are always liable to be tested. </p>
    <p class="normal">There are also <a id="_idIndexMarker707"/>drawbacks, however. Bayesian optimization models the hyperparameter space using a surrogate function built from previous trials, which is not an error-free process. It is not a remote possibility that the process ends up concentrating only on a part of the search space while ignoring other parts (which may instead contain the minimum you are looking for). The solution to this is to run a large number of experiments to be safe, or to alternate between random search and Bayesian optimization, challenging the Bayesian model with random trials that can force it to reshape its search model in a more optimal way.</p>
    <p class="normal">For our example, we use again the data from the <em class="italic">30 Days of ML</em> initiative by Kaggle, a regression task. Our example is based on TensorFlow, but with small modifications it can run on other deep learning frameworks such as PyTorch or MXNet. </p>
    <div class="note">
      <p class="normal">As before, you can find the example on Kaggle here: <a href="https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns"><span class="url">https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns</span></a>.</p>
    </div>
    <p class="normal">Let’s begin:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
    <p class="normal">After importing the TensorFlow package, we leverage its <code class="inlineCode">Dataset</code> function to create an iterable capable of feeding our neural network with batches of data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">df_to_dataset</span>(<span class="hljs-params">dataframe, shuffle=</span><span class="hljs-literal">True</span><span class="hljs-params">, batch_size=</span><span class="hljs-number">32</span>):
    dataframe = dataframe.copy()
    labels = dataframe.pop(<span class="hljs-string">'target'</span>)
    ds = tf.data.Dataset.from_tensor_slices((<span class="hljs-built_in">dict</span>(dataframe),   
                                             labels))
    <span class="hljs-keyword">if</span> shuffle:
        ds = ds.shuffle(buffer_size=<span class="hljs-built_in">len</span>(dataframe))
    ds = ds.batch(batch_size)
    <span class="hljs-keyword">return</span> ds
tf.keras.utils.get_custom_objects().update({<span class="hljs-string">'</span><span class="hljs-string">leaky-relu'</span>: tf.keras.layers.Activation(tf.keras.layers.LeakyReLU(alpha=<span class="hljs-number">0.2</span>))})
</code></pre>
    <p class="normal">We have also <a id="_idIndexMarker708"/>made leaky ReLU activation a custom object for our model; it can be called by a string, and there is no need to directly use the function. </p>
    <p class="normal">We proceed to code a function that creates our deep neural network model based on a set of hyperparameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">create_model</span>(<span class="hljs-params">cat0_dim, cat1_dim, cat2_dim,</span>
<span class="hljs-params">                 cat3_dim, cat4_dim, cat5_dim, </span>
<span class="hljs-params">                 cat6_dim, cat7_dim, cat8_dim, cat9_dim,</span>
<span class="hljs-params">                 layers, layer_1, layer_2, layer_3, layer_4, layer_5, </span>
<span class="hljs-params">                 activation, dropout, batch_normalization, learning_rate, </span>
<span class="hljs-params">                 **others</span>):
    
    dims = {<span class="hljs-string">'cat0'</span>: cat0_dim, <span class="hljs-string">'cat1'</span>: cat1_dim, <span class="hljs-string">'cat2'</span>: cat2_dim, 
            <span class="hljs-string">'cat3'</span>: cat3_dim, <span class="hljs-string">'cat4'</span>: cat4_dim, <span class="hljs-string">'cat5'</span>: cat5_dim,
            <span class="hljs-string">'cat6'</span>: cat6_dim, <span class="hljs-string">'cat7'</span>: cat7_dim, <span class="hljs-string">'cat8'</span>: cat8_dim, 
            <span class="hljs-string">'cat9'</span>: cat9_dim}
    
    vocab = {h:X_train[<span class="hljs-string">'cat4'</span>].unique().astype(<span class="hljs-built_in">int</span>) 
             <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> [<span class="hljs-string">'cat0'</span>, <span class="hljs-string">'cat1'</span>, <span class="hljs-string">'cat2'</span>, <span class="hljs-string">'cat3'</span>, 
                       <span class="hljs-string">'cat4'</span>, <span class="hljs-string">'cat5'</span>, <span class="hljs-string">'cat6'</span>, <span class="hljs-string">'cat7'</span>, 
                       <span class="hljs-string">'cat8'</span>, <span class="hljs-string">'cat9'</span>]}
    
    layers = [layer_1, layer_2, layer_3, layer_4, layer_5][:layers]
    
    feature_columns = <span class="hljs-built_in">list</span>()
    <span class="hljs-keyword">for</span> header <span class="hljs-keyword">in</span> [<span class="hljs-string">'cont1'</span>, <span class="hljs-string">'cont2'</span>, <span class="hljs-string">'cont3'</span>, <span class="hljs-string">'cont4'</span>, <span class="hljs-string">'cont5'</span>, 
                   <span class="hljs-string">'cont6'</span>,<span class="hljs-string">'cont7'</span>, <span class="hljs-string">'cont8'</span>, <span class="hljs-string">'cont9'</span>, <span class="hljs-string">'cont10'</span>,
                   <span class="hljs-string">'cont11'</span>, <span class="hljs-string">'cont12'</span>, <span class="hljs-string">'cont13'</span>]:
         
        feature_columns.append(tf.feature_column.numeric_column(header))
    <span class="hljs-keyword">for</span> header <span class="hljs-keyword">in</span> [<span class="hljs-string">'cat0'</span>, <span class="hljs-string">'cat1'</span>, <span class="hljs-string">'cat2'</span>, <span class="hljs-string">'cat3'</span>, <span class="hljs-string">'cat4'</span>, <span class="hljs-string">'cat5'</span>, 
                   <span class="hljs-string">'cat6'</span>, <span class="hljs-string">'cat7'</span>, <span class="hljs-string">'cat8'</span>, <span class="hljs-string">'cat9'</span>]:
        feature_columns.append(
            tf.feature_column.embedding_column(
            tf.feature_column.categorical_column_with_vocabulary_list(
            header, vocabulary_list=vocab[header]),  
            dimension=dims[header]))
    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
    network_struct = [feature_layer]
    <span class="hljs-keyword">for</span> nodes <span class="hljs-keyword">in</span> layers:
        network_struct.append(
                 tf.keras.layers.Dense(nodes, activation=activation))
        <span class="hljs-keyword">if</span> batch_normalization <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:
                   network_struct.append(
                   tf.keras.layers.BatchNormalization())
        <span class="hljs-keyword">if</span> dropout &gt; <span class="hljs-number">0</span>:
            network_struct.append(tf.keras.layers.Dropout(dropout))
    model = tf.keras.Sequential(network_struct + 
                                [tf.keras.layers.Dense(<span class="hljs-number">1</span>)])
    model.<span class="hljs-built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(
                          learning_rate=learning_rate),
                  loss= tf.keras.losses.MeanSquaredError(),
                  metrics=[<span class="hljs-string">'mean_squared_error'</span>])
    
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">Internally, the <a id="_idIndexMarker709"/>code in the <code class="inlineCode">create_model</code> function customizes the neural network architecture based on the inputs provided. For instance, as parameters for the function you can provide the dimensions of the embeddings for each categorical variable, or define the structure and number of dense layers present in the network. All these parameters are related to the parameter space you want to be explored by Bayesian optimization, hence every input parameter of the function creating the model should be related to a <strong class="keyWord">sampling function</strong> defined in the search space. All you have to do is to place the sampling functions in a list, in the same order as expected by the <code class="inlineCode">create_model</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setting the search space</span>
    
space = [Integer(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, name=<span class="hljs-string">'cat0_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, name=<span class="hljs-string">'cat1_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, name=<span class="hljs-string">'cat2_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, name=<span class="hljs-string">'cat3_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, name=<span class="hljs-string">'cat4_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, name=<span class="hljs-string">'</span><span class="hljs-string">cat5_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, name=<span class="hljs-string">'cat6_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, name=<span class="hljs-string">'cat7_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, name=<span class="hljs-string">'cat8_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">8</span>, name=<span class="hljs-string">'cat9_dim'</span>),
         Integer(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, name=<span class="hljs-string">'layers'</span>),
         Integer(<span class="hljs-number">2</span>, <span class="hljs-number">256</span>, name=<span class="hljs-string">'layer_1'</span>),
         Integer(<span class="hljs-number">2</span>, <span class="hljs-number">256</span>, name=<span class="hljs-string">'layer_2'</span>),
         Integer(<span class="hljs-number">2</span>, <span class="hljs-number">256</span>, name=<span class="hljs-string">'layer_3'</span>),
         Integer(<span class="hljs-number">2</span>, <span class="hljs-number">256</span>, name=<span class="hljs-string">'layer_4'</span>),
         Integer(<span class="hljs-number">2</span>, <span class="hljs-number">256</span>, name=<span class="hljs-string">'</span><span class="hljs-string">layer_5'</span>),
         Categorical([<span class="hljs-string">'relu'</span>, <span class="hljs-string">'leaky-relu'</span>], name=<span class="hljs-string">'activation'</span>),
         Real(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'dropout'</span>),
         Categorical([<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>], name=<span class="hljs-string">'</span><span class="hljs-string">batch_normalization'</span>),
         Categorical([<span class="hljs-number">0.01</span>, <span class="hljs-number">0.005</span>, <span class="hljs-number">0.002</span>, <span class="hljs-number">0.001</span>], name=<span class="hljs-string">'learning_rate'</span>),
         Integer(<span class="hljs-number">256</span>, <span class="hljs-number">1024</span>, name=<span class="hljs-string">'batch_size'</span>)
        ]
</code></pre>
    <p class="normal">As previously <a id="_idIndexMarker710"/>illustrated, you now combine all the elements related to the search into an objective function to be created by a function incorporating your basic search elements, such as the data and the cross-validation strategy:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">make_objective</span>(<span class="hljs-params">model_fn, X, space, cv, scoring, validation=</span><span class="hljs-number">0.2</span>):
    <span class="hljs-comment"># This decorator converts your objective function with named arguments</span>
    <span class="hljs-comment"># into one that</span> <span class="hljs-comment">accepts a list as argument, while doing the conversion</span>
    <span class="hljs-comment"># automatically.</span>
<span class="hljs-meta">    @use_named_args(</span><span class="hljs-params">space</span><span class="hljs-meta">) </span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">objective</span>(<span class="hljs-params">**params</span>):
        
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTesting: "</span>, params)
        validation_scores = <span class="hljs-built_in">list</span>()
        
        <span class="hljs-keyword">for</span> k, (train_index, test_index) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kf.split(X)):
            val_index = <span class="hljs-built_in">list</span>()
            train_examples = <span class="hljs-built_in">len</span>(train_index)
            train_examples = <span class="hljs-built_in">int</span>(train_examples * (<span class="hljs-number">1</span> - validation))
            train_index, val_index = (train_index[:train_examples], 
                                      train_index[train_examples:])
            
            start_time = time()
            
            model = model_fn(**params)
            measure_to_monitor = <span class="hljs-string">'</span><span class="hljs-string">val_mean_squared_error'</span>
            modality=<span class="hljs-string">'min'</span>
            early_stopping = tf.keras.callbacks.EarlyStopping(
                                 monitor=measure_to_monitor,
                                 mode=modality,
                                 patience=<span class="hljs-number">5</span>, 
                                 verbose=<span class="hljs-number">0</span>)
            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
                                   <span class="hljs-string">'best.model'</span>,
                                   monitor=measure_to_monitor, 
                                   mode=modality, 
                                   save_best_only=<span class="hljs-literal">True</span>, 
                                   verbose=<span class="hljs-number">0</span>)
            run = model.fit(df_to_dataset(
                                X_train.iloc[train_index, :], 
                                batch_size=params[<span class="hljs-string">'batch_size'</span>]),
                            validation_data=df_to_dataset(
                                X_train.iloc[val_index, :], 
                                batch_size=<span class="hljs-number">1024</span>),
                            epochs=<span class="hljs-number">1_000</span>,
                            callbacks=[model_checkpoint, 
                                       early_stopping],
                            verbose=<span class="hljs-number">0</span>)
            
            end_time = time()
            
            rounds = np.argmin(
                     run.history[<span class="hljs-string">'val_mean_squared_error'</span>]) + <span class="hljs-number">1</span>
            
            model = tf.keras.models.load_model(<span class="hljs-string">'best.model'</span>)
            shutil.rmtree(<span class="hljs-string">'best.model'</span>)
            
            test_preds = model.predict(df_to_dataset(
                            X.iloc[test_index, :], shuffle=<span class="hljs-literal">False</span>, 
                            batch_size=<span class="hljs-number">1024</span>)).flatten()
                            test_score = scoring(
                            X.iloc[test_index, :][<span class="hljs-string">'target'</span>], 
                            test_preds)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"CV Fold </span><span class="hljs-subst">{k+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> rmse:</span><span class="hljs-subst">{test_score:</span><span class="hljs-number">0.5</span><span class="hljs-subst">f}</span><span class="hljs-string"> - </span><span class="hljs-subst">{rounds}</span><span class="hljs-string"> </span>
<span class="hljs-string">                  rounds - it took </span><span class="hljs-subst">{end_time-start_time:</span><span class="hljs-number">0.0</span><span class="hljs-subst">f}</span><span class="hljs-string"> secs"</span>)
            validation_scores.append(test_score)
            
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(history[k]) &gt;= <span class="hljs-number">10</span>:
                threshold = np.percentile(history[k], q=<span class="hljs-number">25</span>)
                <span class="hljs-keyword">if</span> test_score &gt; threshold:
                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Early stopping for under-performing fold: </span>
<span class="hljs-string">                          threshold is </span><span class="hljs-subst">{threshold:</span><span class="hljs-number">0.5</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
                    <span class="hljs-keyword">return</span> np.mean(validation_scores)
                
            history[k].append(test_score)
        <span class="hljs-keyword">return</span> np.mean(validation_scores)
    <span class="hljs-keyword">return</span> objective
</code></pre>
    <p class="normal">The next step is to provide a sequence of random search runs (as a way to start building some feedback from the search space) and gather the results as a starting point. Then, we can feed <a id="_idIndexMarker711"/>them into a Bayesian optimization and proceed by using <code class="inlineCode">forest_minimize</code> as a surrogate function:</p>
    <pre class="programlisting code"><code class="hljs-code">counter = <span class="hljs-number">0</span>
history = {i:<span class="hljs-built_in">list</span>() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)}
used_time = <span class="hljs-number">0</span>
gp_round = dummy_minimize(func=objective,
                          dimensions=space,
                          n_calls=<span class="hljs-number">10</span>,
                          callback=[onstep],
                          random_state=<span class="hljs-number">0</span>)
gc.collect()
x0, y0 = joblib.load(<span class="hljs-string">'</span><span class="hljs-string">checkpoint.pkl'</span>)
gp_round = gp_minimize(func=objective,
                           x0=x0,  <span class="hljs-comment"># already examined values for x</span>
                           y0=y0,  <span class="hljs-comment"># observed values for x0</span>
                           dimensions=space,
                           n_calls=<span class="hljs-number">30</span>,
                           n_initial_points=<span class="hljs-number">0</span>,
                           callback=[onstep],
                           random_state=<span class="hljs-number">0</span>)
gc.collect()
</code></pre>
    <p class="normal">Notice that after the first ten rounds of random search, we proceed with our search using a random forest algorithm as a surrogate function. That will ensure better and faster results than using a Gaussian process. </p>
    <p class="normal">As before, in this process we have to strive to make the <a id="_idIndexMarker712"/>optimization feasible within the time and resources we have (for instance, by setting a low number of <code class="inlineCode">n_calls</code>). Hence, we can proceed with batches of search iterations by saving the state of the optimization, checking the results obtained, and deciding thereafter to proceed or conclude the optimization process and not invest more time and energy into looking for a better solution.</p>
    <h2 id="_idParaDest-144" class="heading-2">Creating lighter and faster models with KerasTuner</h2>
    <p class="normal">If the previous section has <a id="_idIndexMarker713"/>puzzled you because of its complexity, KerasTuner can offer you a fast solution for setting up an optimization without much hassle. Though it uses Bayesian optimization and Gaussian processes by default, the new idea behind KerasTuner is <strong class="keyWord">hyperband optimization</strong>. Hyperband optimization<a id="_idIndexMarker714"/> uses the bandit approach to figure out the best parameters (see <a href="http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf"><span class="url">http://web.eecs.umich.edu/~mosharaf/</span><span class="url">Readings/HyperBand.pdf</span></a>). This works quite well with neural networks, whose optimization landscape is quite irregular and discontinuous, and thus not always suitable for Gaussian processes.</p>
    <div class="packt_tip">
      <p class="normal">Keep in mind that you cannot avoid building the function that builds a custom network using input hyperparameters; KerasTuner just makes it much easier to handle.</p>
    </div>
    <p class="normal">Let’s start from the beginning. KerasTuner (<a href="https://keras.io/keras_tuner/"><span class="url">https://keras.io/keras_tuner/</span></a>) was<a id="_idIndexMarker715"/> announced as a “flexible and efficient hyperparameter tuning for Keras models” by <em class="italic">François Chollet</em>, the creator of Keras. </p>
    <p class="normal">The recipe proposed by Chollet for running KerasTuner is made up of simple steps, starting from your existing Keras model:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Wrap your model in a function with <code class="inlineCode">hp</code> as the first parameter.</li>
      <li class="numberedList">Define hyperparameters at the beginning of the function.</li>
      <li class="numberedList">Replace DNN static values with hyperparameters.</li>
      <li class="numberedList">Write the code that models a complex neural network from the given hyperparameters.</li>
      <li class="numberedList">If necessary, dynamically define hyperparameters as you build the network.</li>
    </ol>
    <p class="normal">We’ll now explore <a id="_idIndexMarker716"/>how all these steps can work for you in a Kaggle competition by using an example. At the moment, KerasTuner is part of the stack offered by any Kaggle Notebook, hence you don’t need to install it. In addition, the TensorFlow add-ons are part of the Notebook’s pre-installed packages. </p>
    <p class="normal">If you are not using a Kaggle Notebook and you need to try KerasTuner, you can easily install both using the following commands:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install -U keras-tuner
!pip install -U tensorflow-addons
</code></pre>
    <div class="note">
      <p class="normal">You can find this example already set up on a Kaggle Notebook here: <a href="https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/"><span class="url">https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/</span></a>.</p>
    </div>
    <p class="normal">Our first step is to import the necessary packages (creating shortcuts for some commands, such as for <code class="inlineCode">pad_sequences</code>) and to upload the data we will be using directly from Keras:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">import</span> tensorflow_addons <span class="hljs-keyword">as</span> tfa
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> LeakyReLU
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Activation
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> SGD, Adam
<span class="hljs-keyword">from</span> tensorflow.keras.wrappers.scikit_learn <span class="hljs-keyword">import</span> KerasClassifier
<span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> EarlyStopping, ModelCheckpoint
pad_sequences = keras.preprocessing.sequence.pad_sequences
imdb = keras.datasets.imdb(train_data, train_labels),
(test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)
train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=<span class="hljs-number">0.30</span>,
                 shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">This time, we are using the<a id="_idIndexMarker717"/> IMDb dataset, which is available in the Keras package (<a href="https://keras.io/api/datasets/imdb/"><span class="url">https://keras.io/api/datasets/imdb/</span></a>). The dataset has some interesting characteristics:</p>
    <ul>
      <li class="bulletList">It is a dataset of 25,000 movie reviews from IMDb</li>
      <li class="bulletList">The reviews are labeled by sentiment (positive/negative)</li>
      <li class="bulletList">The target classes are balanced (hence accuracy works as a scoring measure)</li>
      <li class="bulletList">Each review is encoded as a list of word indexes (integers)</li>
      <li class="bulletList">For convenience, words are indexed by overall frequency</li>
    </ul>
    <p class="normal">In addition, it has been successfully used in a popular Kaggle competition on word embeddings (<a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/overview"><span class="url">https://www.kaggle.com/c/word2vec-nlp-tutorial/overview</span></a>).</p>
    <p class="normal">This example involves natural language processing. This type of problem is often solved by using <strong class="keyWord">recurrent neural networks </strong>(<strong class="keyWord">RNNs</strong>) based on LSTM or GRU layers. BERT, RoBERTa, and the other <a id="_idIndexMarker718"/>transformer-based models often achieve better results – being pre-trained models relying on large language corpora – but this is not necessarily true in all problems, and RNNs can prove a strong baseline to beat or a good addition to an ensemble of neural models. In our example, all words are already numerically indexed. We just add to the existing indices the numeric codes that denote padding (so we can easily normalize all the text to the phrase length), the start of the sentence, an unknown word, and an unused word:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># A dictionary mapping words to an integer index</span>
word_index = imdb.get_word_index()
<span class="hljs-comment"># The first indices are reserved</span>
word_index = {k:(v+<span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> word_index.items()} 
word_index[<span class="hljs-string">"</span><span class="hljs-string">&lt;PAD&gt;"</span>] = <span class="hljs-number">0</span>
word_index[<span class="hljs-string">"&lt;START&gt;"</span>] = <span class="hljs-number">1</span>
word_index[<span class="hljs-string">"&lt;UNK&gt;"</span>] = <span class="hljs-number">2</span>  <span class="hljs-comment"># unknown</span>
word_index[<span class="hljs-string">"&lt;UNUSED&gt;"</span>] = <span class="hljs-number">3</span>
reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])
<span class="hljs-keyword">def</span> <span class="hljs-title">decode_review</span>(<span class="hljs-params">text</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-string">' '</span>.join([reverse_word_index.get(i, <span class="hljs-string">'?'</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> text])
</code></pre>
    <p class="normal">The next step involves creating a custom layer for <strong class="keyWord">attention</strong>. Attention is the foundation of transformer models and it is <a id="_idIndexMarker719"/>one of the most innovative ideas in neural NLP of recent times. </p>
    <div class="note">
      <p class="normal">For all the details of how these kinds of layers work, see the seminal paper on attention: Vaswani, A. et al. <em class="italic">Attention is all you need.</em> Advances in neural information processing systems. 2017 (<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</span></a>). </p>
    </div>
    <p class="normal">The idea of attention can <a id="_idIndexMarker720"/>be easily conveyed. LSTM and GRU layers output processed sequences, but not all the elements in these output sequences are necessarily important for your predictions. Instead of averaging all the output sequences using a pool layer across the stratified sequences, you can actually take a <em class="italic">weighted average</em> of them (and during the training phase learn the correct weights to be used). This weighting process (<strong class="keyWord">attention</strong>) definitely improves the results you are going to pass on further. Of course, you can make this approach even more sophisticated using multiple attention layers (we call this <strong class="keyWord">multi-head attention</strong>), but in our example a single layer will suffice because we want to <a id="_idIndexMarker721"/>demonstrate that using attention is more effective in this problem than simply averaging or just concatenating all the results together:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Dropout
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Flatten, RepeatVector, dot, multiply, Permute, Lambda
K = keras.backend
<span class="hljs-keyword">def</span> <span class="hljs-title">attention</span>(<span class="hljs-params">layer</span>):
    <span class="hljs-comment"># --- Attention is all you need --- #</span>
    _,_,units = layer.shape.as_list()
    attention = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'tanh'</span>)(layer)
    attention = Flatten()(attention)
    attention = Activation(<span class="hljs-string">'softmax'</span>)(attention)
    attention = RepeatVector(units)(attention)
    attention = Permute([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>])(attention)
    representation = multiply([layer, attention])
    representation = Lambda(<span class="hljs-keyword">lambda</span> x: K.<span class="hljs-built_in">sum</span>(x, axis=-<span class="hljs-number">2</span>), 
                            output_shape=(units,))(representation)
    <span class="hljs-comment"># ---------------------------------- #</span>
    <span class="hljs-keyword">return</span> representation
</code></pre>
    <p class="normal">As a further<a id="_idIndexMarker722"/> variation in our experiments on the architecture of the DNNs for this problem, we also want to test the effectiveness of using different kinds of optimizers such <a id="_idIndexMarker723"/>as <strong class="keyWord">Rectified Adam </strong>(an adaptive learning Adam optimizer; read this post to learn more: <a href="https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b"><span class="url">https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b</span></a>) or <strong class="keyWord">Stochastic Weighted Averaging </strong>(<strong class="keyWord">SWA</strong>). SWA is a way to average the weights traversed during the<a id="_idIndexMarker724"/> optimization based on a modified learning rate schedule: if your model tends to overfit or overshoot, SWA helps in getting near to an optimal solution and it is proven to work especially in NLP problems. </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_optimizer</span>(<span class="hljs-params">option=</span><span class="hljs-number">0</span><span class="hljs-params">, learning_rate=</span><span class="hljs-number">0.001</span>):
    <span class="hljs-keyword">if</span> option==<span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> tf.keras.optimizers.Adam(learning_rate)
    <span class="hljs-keyword">elif</span> option==<span class="hljs-number">1</span>:
        <span class="hljs-keyword">return</span> tf.keras.optimizers.SGD(learning_rate, 
                                       momentum=<span class="hljs-number">0.9</span>, nesterov=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">elif</span> option==<span class="hljs-number">2</span>:
        <span class="hljs-keyword">return</span> tfa.optimizers.RectifiedAdam(learning_rate)
    <span class="hljs-keyword">elif</span> option==<span class="hljs-number">3</span>:
        <span class="hljs-keyword">return</span> tfa.optimizers.Lookahead(
                   tf.optimizers.Adam(learning_rate), sync_period=<span class="hljs-number">3</span>)
    <span class="hljs-keyword">elif</span> option==<span class="hljs-number">4</span>:
        <span class="hljs-keyword">return</span> tfa.optimizers.SWA(tf.optimizers.Adam(learning_rate))
    <span class="hljs-keyword">elif</span> option==<span class="hljs-number">5</span>:
        <span class="hljs-keyword">return</span> tfa.optimizers.SWA(
                   tf.keras.optimizers.SGD(learning_rate, 
                                       momentum=<span class="hljs-number">0.9</span>, nesterov=<span class="hljs-literal">True</span>))
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> tf.keras.optimizers.Adam(learning_rate)
</code></pre>
    <p class="normal">Having defined two key functions, we now face the most important function to code: the one that will provide different neural architectures given the parameters. We don’t encode all the various parameters we want to connect to the different architectural choices; we only provide the <code class="inlineCode">hp</code> parameter, which should contain all the possible parameters we want to use, and that will be run by KerasTuner. Aside from <code class="inlineCode">hp</code> in the function input, we fix the size of the vocabulary and the length to be padded (adding dummy values if the effective length is shorter or cutting the phrase if the length is longer): </p>
    <pre class="programlisting code"><code class="hljs-code">layers = keras.layers
models = keras.models
    
<span class="hljs-keyword">def</span> <span class="hljs-title">create_tunable_model</span>(<span class="hljs-params">hp, vocab_size=</span><span class="hljs-number">10000</span><span class="hljs-params">, pad_length=</span><span class="hljs-number">256</span>):
    <span class="hljs-comment"># Instantiate model params</span>
    embedding_size = hp.Int(<span class="hljs-string">'embedding_size'</span>, min_value=<span class="hljs-number">8</span>, 
                            max_value=<span class="hljs-number">512</span>, step=<span class="hljs-number">8</span>)
    spatial_dropout = hp.Float(<span class="hljs-string">'spatial_dropout'</span>, min_value=<span class="hljs-number">0</span>, 
                               max_value=<span class="hljs-number">0.5</span>, step=<span class="hljs-number">0.05</span>)
    conv_layers = hp.Int(<span class="hljs-string">'conv_layers'</span>, min_value=<span class="hljs-number">1</span>,
                         max_value=<span class="hljs-number">5</span>, step=<span class="hljs-number">1</span>)
    rnn_layers = hp.Int(<span class="hljs-string">'rnn_layers'</span>, min_value=<span class="hljs-number">1</span>,
                        max_value=<span class="hljs-number">5</span>, step=<span class="hljs-number">1</span>)
    dense_layers = hp.Int(<span class="hljs-string">'dense_layers'</span>, min_value=<span class="hljs-number">1</span>,
                          max_value=<span class="hljs-number">3</span>, step=<span class="hljs-number">1</span>)
    conv_filters = hp.Int(<span class="hljs-string">'conv_filters'</span>, min_value=<span class="hljs-number">32</span>, 
                          max_value=<span class="hljs-number">512</span>, step=<span class="hljs-number">32</span>)
    conv_kernel = hp.Int(<span class="hljs-string">'conv_kernel'</span>, min_value=<span class="hljs-number">1</span>,
                         max_value=<span class="hljs-number">8</span>, step=<span class="hljs-number">1</span>)
    concat_dropout = hp.Float(<span class="hljs-string">'concat_dropout'</span>, min_value=<span class="hljs-number">0</span>, 
                              max_value=<span class="hljs-number">0.5</span>, step=<span class="hljs-number">0.05</span>)
    dense_dropout = hp.Float(<span class="hljs-string">'dense_dropout'</span>, min_value=<span class="hljs-number">0</span>, 
                             max_value=<span class="hljs-number">0.5</span>, step=<span class="hljs-number">0.05</span>)
</code></pre>
    <p class="normal">In the first part of the function, we simply recover all the settings from the <code class="inlineCode">hp</code> parameter. We also make explicit the range of the search space for each of them. Contrary to the solutions we’ve seen so far, this part of the work is done <em class="italic">inside</em> the model function, not outside.</p>
    <p class="normal">The function<a id="_idIndexMarker725"/> continues by defining the different layers using the parameters extracted from <code class="inlineCode">hp</code>. In some cases, a parameter will switch on or off a part of the network performing certain data processing. For instance, in the code we inserted a branch of the graph (<code class="inlineCode">conv_filters</code> and <code class="inlineCode">conv_kernel</code>) that processes the sequence of words using convolutional layers, which, in their 1D form, can also prove useful for NLP problems, since they can catch local sequences of words and meanings that LSTMs may find harder to grasp.</p>
    <p class="normal">Now we can define the actual model:</p>
    <pre class="programlisting code"><code class="hljs-code">    inputs = layers.Input(name=<span class="hljs-string">'inputs'</span>,shape=[pad_length])
    layer  = layers.Embedding(vocab_size, embedding_size, 
                              input_length=pad_length)(inputs)
    layer  = layers.SpatialDropout1D(spatial_dropout)(layer)
    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(conv_layers):
        <span class="hljs-keyword">if</span> l==<span class="hljs-number">0</span>:
            conv = layers.Conv1D(filters=conv_filters, 
                       kernel_size=conv_kernel, padding=<span class="hljs-string">'valid'</span>,
                       kernel_initializer=<span class="hljs-string">'he_uniform'</span>)(layer)
        <span class="hljs-keyword">else</span>:
            conv = layers.Conv1D(filters=conv_filters,  
                       kernel_size=conv_kernel, padding=<span class="hljs-string">'valid'</span>, 
                       kernel_initializer=<span class="hljs-string">'he_uniform'</span>)(conv) 
    avg_pool_conv = layers.GlobalAveragePooling1D()(conv)
    max_pool_conv = layers.GlobalMaxPooling1D()(conv)
    representations = <span class="hljs-built_in">list</span>()
    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rnn_layers):
        
        use_bidirectional = hp.Choice(<span class="hljs-string">f'use_bidirectional_</span><span class="hljs-subst">{l}</span><span class="hljs-string">'</span>,
                                      values=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
        use_lstm = hp.Choice(<span class="hljs-string">f'use_lstm_</span><span class="hljs-subst">{l}</span><span class="hljs-string">'</span>, values=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
        units = hp.Int(<span class="hljs-string">f'units_</span><span class="hljs-subst">{l}</span><span class="hljs-string">'</span>, min_value=<span class="hljs-number">8</span>, max_value=<span class="hljs-number">512</span>, step=<span class="hljs-number">8</span>)
        <span class="hljs-keyword">if</span> use_lstm == <span class="hljs-number">1</span>:
            rnl = layers.LSTM
        <span class="hljs-keyword">else</span>:
            rnl = layers.GRU
        <span class="hljs-keyword">if</span> use_bidirectional==<span class="hljs-number">1</span>:
            layer = layers.Bidirectional(rnl(units, 
                              return_sequences=<span class="hljs-literal">True</span>))(layer)
        <span class="hljs-keyword">else</span>:
            layer = rnl(units, return_sequences=<span class="hljs-literal">True</span>)(layer)
        representations.append(attention(layer))
    layer = layers.concatenate(representations + [avg_pool_conv, 
                                                  max_pool_conv])
    layer = layers.Dropout(concat_dropout)(layer)
    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(dense_layers):
        dense_units = hp.Int(<span class="hljs-string">f'dense_units_</span><span class="hljs-subst">{l}</span><span class="hljs-string">'</span>, min_value=<span class="hljs-number">8</span>, 
                             max_value=<span class="hljs-number">512</span>, step=<span class="hljs-number">8</span>)
        layer = layers.Dense(dense_units)(layer)
        layer = layers.LeakyReLU()(layer)
        layer = layers.Dropout(dense_dropout)(layer)
    layer = layers.Dense(<span class="hljs-number">1</span>, name=<span class="hljs-string">'out_layer'</span>)(layer)
    outputs = layers.Activation(<span class="hljs-string">'sigmoid'</span>)(layer)
    model = models.Model(inputs=inputs, outputs=outputs)
</code></pre>
    <p class="normal">We start by defining<a id="_idIndexMarker726"/> the input layer and transform it with a subsequent embedding layer that will encode the sequence values into dense layers. Some dropout regularization is applied to the process using <code class="inlineCode">SpatialDropout1D</code>, a function that will randomly drop entire columns of the output matrix (standard dropout drops random single elements in the matrix instead). After these initial phases, we split the network into one pipeline based on convolutions (<code class="inlineCode">Conv1D</code>) and another based on recurrent layers (GRU or LSTM). It is after the recurrent layers that we apply the attention layer. Finally, the outputs of these two pipelines are concatenated and, after a few more dense layers, they arrive at the final output node, a sigmoid since we have to represent a probability bounded in the range 0 to 1.</p>
    <p class="normal">After the model definition, we set the learning parameters and compile the model before returning it: </p>
    <pre class="programlisting code"><code class="hljs-code">    hp_learning_rate = hp.Choice(<span class="hljs-string">'learning_rate'</span>, 
                                 values=[<span class="hljs-number">0.002</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.0005</span>])
    optimizer_type = hp.Choice(<span class="hljs-string">'optimizer'</span>, values=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>)))
    optimizer = get_optimizer(option=optimizer_type,  
                              learning_rate=hp_learning_rate)
    
    model.<span class="hljs-built_in">compile</span>(optimizer=optimizer,
                  loss=<span class="hljs-string">'binary_crossentropy'</span>,
                  metrics=[<span class="hljs-string">'acc'</span>])
    
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">Note that we have built the model using the functional API of Keras, not the sequential one. We would advise you to avoid the sequential one, in fact; it is easier to set up, but severely restricts your potential architectures.</p>
    <p class="normal">At this point, most of the work is already done. As a suggestion, having worked out many optimizations using KerasTuner ourselves, we prefer to first build a <em class="italic">non-parametric</em> model, using all the possible architectural features that we want to test, with the mutually exclusive parts of the network set to the most complex solutions. After we have set up the generative function and our model seems to be working properly, we can, for instance, represent its graph and have it successfully fit some examples as a test. After that, we start inserting the parametric variables into the architecture and setting up the <code class="inlineCode">hp</code> parameter definitions.</p>
    <div class="packt_tip">
      <p class="normal">In our experience, starting with a parametric function immediately will take more time and debugging effort. The idea behind KerasTuner is to let you think of your DNNs as a set of modular circuits and to help you optimize how the data flows inside them.</p>
    </div>
    <p class="normal">Now, we import<a id="_idIndexMarker727"/> KerasTuner. First, we set the tuner itself, and then we start the search:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> keras_tuner <span class="hljs-keyword">as</span> kt
tuner = kt.BayesianOptimization(hypermodel=create_tunable_model,
                                objective=<span class="hljs-string">'</span><span class="hljs-string">val_acc'</span>,
                                max_trials=<span class="hljs-number">100</span>,
                                num_initial_points=<span class="hljs-number">3</span>,
                                directory=<span class="hljs-string">'storage'</span>,
                                project_name=<span class="hljs-string">'imdb'</span>,
                                seed=<span class="hljs-number">42</span>)
tuner.search(train_data, train_labels, 
             epochs=<span class="hljs-number">30</span>,
             batch_size=<span class="hljs-number">64</span>, 
             validation_data=(val_data, val_labels),
             shuffle=<span class="hljs-literal">True</span>,
             verbose=<span class="hljs-number">2</span>,
             callbacks = [EarlyStopping(<span class="hljs-string">'</span><span class="hljs-string">val_acc'</span>,
                                        patience=<span class="hljs-number">3</span>,
                                        restore_best_weights=<span class="hljs-literal">True</span>)]
             )
</code></pre>
    <p class="normal">As a tuner, we opt for the Bayesian optimization one, but you can also try the Hyperband tuner (<a href="https://keras.io/api/keras_tuner/tuners/hyperband/"><span class="url">https://keras.io/api/keras_tuner/tuners/hyperband/</span></a>) and check if it works better for your problem. We provide our model function to the <code class="inlineCode">hypermodel</code> parameter. Then, we set the objective using a string or a function, the maximum number of trials (KerasTuner will stop earlier if there is nothing more to be done), and the initial number of random trials – the more the better – in order to inform the Bayesian process. Early stopping is a standard and well-performing practice in modeling DNNs that you absolutely cannot ignore. Finally, but importantly, we set the directory where we want to save our search and a seed number for reproducibility of the optimization steps.</p>
    <p class="normal">The search <a id="_idIndexMarker728"/>phase is run like a standard fit of a Keras model and – this is quite important – it accepts callbacks. Therefore, you can easily add early stopping to your model. In this case, the given epoch number should therefore be considered the maximum number of epochs. You may also want to optimize the batch size, which we haven’t done in our example. This still requires some extra work, but you can get an idea of how to achieve it by reading this GitHub closed issue: <a href="https://github.com/keras-team/keras-tuner/issues/122"><span class="url">https://github.com/keras-team/keras-tuner/issues/122</span></a>. </p>
    <p class="normal">After the optimization is complete, you can extract the best parameters and save the best model without any need to retrain it:</p>
    <pre class="programlisting code"><code class="hljs-code">best_hps = tuner.get_best_hyperparameters()[<span class="hljs-number">0</span>]
model = tuner.hypermodel.build(best_hps)
<span class="hljs-built_in">print</span>(best_hps.values)
model.summary()
model.save(<span class="hljs-string">"</span><span class="hljs-string">best_model.h5"</span>)
</code></pre>
    <p class="normal">In this example, KerasTuner finds a solution that uses:</p>
    <ul>
      <li class="bulletList">A larger embedding layer</li>
      <li class="bulletList">Just plain GRU and LSTM layers (no bi-directional layers)</li>
      <li class="bulletList">Stacking of multiple one-dimensional convolution layers (Conv1D)</li>
      <li class="bulletList">More and larger dense layers</li>
    </ul>
    <p class="normal">Interestingly, the solution is not only more effective, but also lighter and faster than our previous attempts based on intuition and experience with the problem. </p>
    <p class="normal">Chollet himself suggests using KerasTuner not just to make your DNNs perform better but also to shrink them to a more manageable size, something that may make the difference in Code competitions. This allows you to put together more models that work together within the limited inference time provided by the sponsors of the competition.</p>
    <div class="note">
      <p class="normal">If you would like to examine more examples of using KerasTuner, François Chollet also created a series of Notebooks for Kaggle competitions in order to showcase the workings and functionalities of his optimizer:</p>
      <ul>
        <li class="bulletList"><a href="https://www.kaggle.com/fchollet/keras-kerastuner-best-practices"><span class="url">https://www.kaggle.com/fchollet/keras-kerastuner-best-practices</span></a> for the <em class="italic">Digit Recognizer</em> datasets</li>
        <li class="bulletList"><a href="https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices"><span class="url">https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices</span></a> for the <em class="italic">Titanic</em> dataset</li>
        <li class="bulletList"><a href="https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices"><span class="url">https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices</span></a> for the <em class="italic">Mechanisms of Action (MoA) Prediction </em>competition</li>
      </ul>
    </div>
    <h2 id="_idParaDest-145" class="heading-2">The TPE approach in Optuna</h2>
    <p class="normal">We complete our <a id="_idIndexMarker729"/>overview of Bayesian optimization with another<a id="_idIndexMarker730"/> interesting tool and approach to it. As we have discussed, Scikit-optimize uses Gaussian processes (as well as tree algorithms) and it directly models the surrogate function and the acquisition function.</p>
    <div class="packt_tip">
      <p class="normal">As a reminder of these topics, the <strong class="keyWord">surrogate function</strong> helps the optimization process to model the potential<a id="_idIndexMarker731"/> performance result when you try a set of hyperparameters. The surrogate function is built using the previous experiments and their results; it is just a predictive model applied in order to forecast the behavior of a specific machine learning algorithm on a specific problem. For each parameter input provided to the surrogate function, you get an expected performance output. That’s intuitive and also quite hackable, as we have seen. </p>
      <p class="normal">The <strong class="keyWord">acquisition function</strong> instead<a id="_idIndexMarker732"/> points out what set of hyperparameters could be tested in order to improve the ability of the surrogate function to predict the performances of the machine learning algorithm. It is also useful for really testing if we can reach a top-performing result based on the surrogate function’s forecasts. These two objectives represent the <em class="italic">explore </em>part (where you run experiments) and the <em class="italic">exploit</em> part (where you test the performance) of a Bayesian optimization process.</p>
    </div>
    <p class="normal">Instead, optimizers based on <strong class="keyWord">TPE</strong> tackle the problem by estimating the likelihood of success of the values of parameters. In other words, they model the success distribution of the parameters themselves using successive refinements, assigning a higher probability to more successful value combinations.</p>
    <p class="normal">In this approach, the set of hyperparameters is divided into good and bad ones by these distributions, which take the role of the surrogate and acquisition functions in Bayesian optimization, since the distributions tell you where to sample to get better performances or explore where there is uncertainty.</p>
    <div class="note">
      <p class="normal">To explore the technical details of TPE, we suggest reading Bergstra, J. et al. <em class="italic">Algorithms for hyper-parameter optimization</em>. Advances in neural information processing systems 24, 2011 (<a href="https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf</span></a>).</p>
    </div>
    <p class="normal">Therefore, TPE can<a id="_idIndexMarker733"/> model the search space and simultaneously suggest what the algorithm can try next, by sampling from the adjusted probability distribution of parameters. </p>
    <p class="normal">For a long time, <strong class="keyWord">Hyperopt </strong>was the option for those preferring to use TPE instead of Bayesian optimization based on<a id="_idIndexMarker734"/> Gaussian processes. In October 2018, however, Optuna appeared in the open source and it has become the preferred choice for Kagglers due to its versatility (it also works out of the box for neural networks and even for ensembling), speed, and efficiency in finding better solutions compared to previous optimizers. </p>
    <p class="normal">In this section, we will <a id="_idIndexMarker735"/>demonstrate just how easy is to set up a search, which is called a <em class="italic">study</em> under Optuna terminology. All you need to do is to write an objective function that takes as input the parameters to be tested by Optuna and then returns an evaluation. Validation and other algorithmic aspects can be handled in a straightforward manner inside the objective function, also using references to variables external to the function itself (both global variables or local ones). Optuna also allows <strong class="keyWord">pruning</strong>, that is, signaling that a particular experiment is not going well and that Optuna can stop and forget about it. Optuna provides a list of functions that activate this callback (see <a href="https://optuna.readthedocs.io/en/stable/reference/integration.html"><span class="url">https://optuna.readthedocs.io/en/stable/reference/integration.html</span></a>); the algorithm will run everything efficiently for you after that, which will significantly reduce the time needed for optimization. </p>
    <p class="normal">All of this is in our next example. We return to optimizing for the <em class="italic">30 Days of ML</em> competition. This time, we are trying to figure out what parameters make XGBoost work for this competition.</p>
    <div class="note">
      <p class="normal">You can find the Notebook for this example at <a href="https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization"><span class="url">https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization</span></a>.</p>
    </div>
    <p class="normal"> </p>
    <p class="normal">As a first step, we<a id="_idIndexMarker736"/> upload the libraries and the data, as before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OrdinalEncoder
<span class="hljs-keyword">from</span> xgboost <span class="hljs-keyword">import</span> XGBRegressor
<span class="hljs-keyword">import</span> optuna
<span class="hljs-keyword">from</span> optuna.integration <span class="hljs-keyword">import</span> XGBoostPruningCallback
<span class="hljs-comment"># Loading data </span>
X_train = pd.read_csv(<span class="hljs-string">"../input/30-days-of-ml/train.csv"</span>).iloc[:<span class="hljs-number">100_000</span>, :]
X_test = pd.read_csv(<span class="hljs-string">"../input/30-days-of-ml/test.csv"</span>)
<span class="hljs-comment"># Preparing data as a tabular matrix</span>
y_train = X_train.target
X_train = X_train.set_index(<span class="hljs-string">'</span><span class="hljs-string">id'</span>).drop(<span class="hljs-string">'target'</span>, axis=<span class="hljs-string">'columns'</span>)
X_test = X_test.set_index(<span class="hljs-string">'id'</span>)
<span class="hljs-comment"># Pointing out categorical features</span>
categoricals = [item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> X_train.columns <span class="hljs-keyword">if</span> <span class="hljs-string">'cat'</span> <span class="hljs-keyword">in</span> item]
<span class="hljs-comment"># Dealing with categorical data using OrdinalEncoder</span>
ordinal_encoder = OrdinalEncoder()
X_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])
X_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])
</code></pre>
    <p class="normal">When using Optuna, you<a id="_idIndexMarker737"/> just have to define an objective function containing the model, the cross-validation logic, the evaluation measure, and the search space. </p>
    <p class="normal">Naturally, for data you can refer to objects outside the function itself, rendering the construction of the function much easier. As in KerasTuner, here you need a special input parameter based on a class from Optuna:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">objective</span>(<span class="hljs-params">trial</span>):
    
    params = {
            <span class="hljs-string">'learning_rate'</span>: trial.suggest_float(<span class="hljs-string">"learning_rate"</span>, 
                                                 <span class="hljs-number">0.01</span>, <span class="hljs-number">1.0</span>, log=<span class="hljs-literal">True</span>),
            <span class="hljs-string">'reg_lambda'</span>: trial.suggest_loguniform(<span class="hljs-string">"reg_lambda"</span>, 
                                                   <span class="hljs-number">1e-9</span>, <span class="hljs-number">100.0</span>),
            <span class="hljs-string">'reg_alpha'</span>: trial.suggest_loguniform(<span class="hljs-string">"reg_alpha"</span>, 
                                                  <span class="hljs-number">1e-9</span>, <span class="hljs-number">100.0</span>),
            <span class="hljs-string">'subsample'</span>: trial.suggest_float(<span class="hljs-string">"subsample"</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1.0</span>),
            <span class="hljs-string">'colsample_bytree'</span>: trial.suggest_float(
                                      <span class="hljs-string">"colsample_bytree"</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1.0</span>),
            <span class="hljs-string">'max_depth'</span>: trial.suggest_int(<span class="hljs-string">"max_depth"</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>),
            <span class="hljs-string">'min_child_weight'</span>: trial.suggest_int(<span class="hljs-string">"min_child_weight"</span>, 
                                                  <span class="hljs-number">1</span>, <span class="hljs-number">7</span>),
            <span class="hljs-string">'gamma'</span>: trial.suggest_float(<span class="hljs-string">"gamma"</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1.0</span>, step=<span class="hljs-number">0.1</span>)
    }
    model = XGBRegressor(
        random_state=<span class="hljs-number">0</span>,
        tree_method=<span class="hljs-string">"gpu_hist"</span>,
        predictor=<span class="hljs-string">"gpu_predictor"</span>,
        n_estimators=<span class="hljs-number">10_000</span>,
        **params
    )
    
    model.fit(x, y, early_stopping_rounds=<span class="hljs-number">300</span>, 
              eval_set=[(x_val, y_val)], verbose=<span class="hljs-number">1000</span>,
              callbacks=[XGBoostPruningCallback(trial, <span class="hljs-string">'validation_0-rmse'</span>)])
    preds = model.predict(x_test)
    rmse = mean_squared_error(y_test, preds, squared=<span class="hljs-literal">False</span>)
    <span class="hljs-keyword">return</span> rmse
</code></pre>
    <p class="normal">In this example, for<a id="_idIndexMarker738"/> performance reasons, we won’t cross-validate but use<a id="_idIndexMarker739"/> one fixed dataset for training, one for validation (early stopping), and one for testing purposes. We are using GPU in this example, and we are also subsetting the available data in order to fit the execution of 60 trials into a reasonable length of time. If you don’t want to use GPU, just remove the <code class="inlineCode">tree_method</code> and <code class="inlineCode">predictor</code> parameters from the <code class="inlineCode">XGBRegressor</code> instantiation. Also notice how we set a callback in the <code class="inlineCode">fit</code> method in order to provide Optuna feedback on how the model is performing, so the optimizer can stop an underperforming experiment early to give space to other attempts.</p>
    <pre class="programlisting code"><code class="hljs-code">x, x_val, y, y_val = train_test_split(X_train, y_train, random_state=<span class="hljs-number">0</span>,
                                      test_size=<span class="hljs-number">0.2</span>)
x, x_test, y, y_test = train_test_split(x, y, random_state=<span class="hljs-number">0</span>, test_size=<span class="hljs-number">0.25</span>)
study = optuna.create_study(direction=<span class="hljs-string">"minimize"</span>)
study.optimize(objective, n_trials=<span class="hljs-number">100</span>)
</code></pre>
    <p class="normal">Another notable aspect is that you can decide to optimize either for minimization or maximization, depending on your problem (Scikit-optimize works only on minimization problems).</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(study.best_value)
<span class="hljs-built_in">print</span>(study.best_params)
</code></pre>
    <p class="normal">To complete the run, you <a id="_idIndexMarker740"/>just have to print or export the best test performance <a id="_idIndexMarker741"/>and the best parameters found by the optimization.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Ruchi_Bhatia.png" alt=""/>
      </div>
      <p class="intervieweeName">Ruchi Bhatia</p>
      <p class="normal"><a href="https://www.kaggle.com/ruchi798"><span class="url">https://www.kaggle.com/ruchi798</span></a></p>
      <p class="normal">As a <a id="_idIndexMarker742"/>conclusion to this dense chapter, let’s look at one last interview. This time, we’re speaking to Ruchi Bhatia, a Grandmaster in Datasets and Notebooks. Ruchi is currently a graduate student at Carnegie Mellon University, a Data Scientist at OpenMined, and a Data Science Global Ambassador at Z by HP.</p>
      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">My favorite kinds of competitions are NLP and Analytics competitions. Being multilingual has played a significant role in my main focus and interest: Natural Language Processing.</em></p>

      <p class="normal"><em class="italic">As for Analytics competitions, I enjoy making sense out of complex data and backing my answers to questions with the support of data! Every competition on Kaggle is novel and requires different techniques. I mainly follow a data-driven approach to algorithm selection and have no set favorites.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">When a new competition is announced, my priority is to understand the problem statement in depth. Sometimes problem statements can be out of our comfort zone or domain, so it’s </em><em class="italic"><a id="_idIndexMarker743"/></em><em class="italic">crucial to ensure we grasp them well before moving on to exploratory data analysis. While performing EDA, my goal is to understand data distribution and focus on getting to know the data at hand. During this, we are likely to come across patterns, and we should make an effort to understand those and form a hypothesis for outliers and exceptional cases. </em></p>
      <p class="normal"><em class="italic">After this, I spend time understanding the competition metrics. The creation of a leak-free cross-validation strategy is my next step. After this, I choose a baseline model and make my first submission. If the correlation between the local validation and the competition leaderboard is not satisfying, I iterate for as long as needed to understand possible discrepancies and account for them. </em></p>
      <p class="normal"><em class="italic">Then I move on to improve my modeling approach with time. Apart from this, tweaking parameters and trying new experiments help to gain an understanding of what works best with the data at hand (ensuring that I’m preventing overfitting during the whole process). Finally, in the last few weeks of the competition, I perform model ensembling and check the robustness of my solution.</em></p>
      <p class="normal"><em class="italic">As for my projects outside of Kaggle, most of my time is spent in data gathering, cleaning, and getting relevant value out of the data.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Kaggle has tremendously helped me accelerate my career. Not only did it help me find my passion for data science, but it also motivated me to contribute effectively and stay consistent. It’s the perfect place to try hands-on experiments with an enormous amount of data at our fingertips and showcase our work on a global scale. In addition, our work is easily accessible, so we can reach a broader audience as well. </em></p>
      <p class="normal"><em class="italic">I have used a majority of my Kaggle work on my portfolio to indicate the diversity of work I have done in my journey thus far. Kaggle competitions aim to solve novel and real-world problems, and I feel employers look for our ability and aptitude to solve such problems. I’ve also curated a broad range of datasets that helped me highlight my acumen in working with raw data. These projects helped me secure multiple job opportunities.</em></p>

      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">In my </em><em class="italic"><a id="_idIndexMarker744"/></em><em class="italic">experience, I’ve noticed that many Kagglers get disheartened when their ranking in competitions isn’t what they expected it to be. After weeks and months of hard work, I can see why they might give up early, but winning Kaggle competitions is no easy feat. There are several people of different educational backgrounds and work experience competing, and having the courage to try is all that’s important. We should focus on our individualistic growth and see how far we’ve come in our journey.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">Comprehensive exploratory data analysis combined with relevant visualizations help us spot data trends and context that can improve our methodology. Since I believe in the power of visualizations, my favorite data science libraries would be Seaborn and TensorBoard. Seaborn for EDA and TensorBoard for visualizations needed during the machine learning workflow. I occasionally use Tableau too.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">When people enter a competition, I believe they should prepare themselves for deep diving into the problem statement and researching. Competitions on Kaggle are particularly challenging and help solve real-life problems in many cases. People should have a positive mindset and not get disheartened. Kaggle competitions provide the perfect opportunity to learn and grow!</em></p>
    </div>
    <h1 id="_idParaDest-146" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we discussed hyperparameter optimization at length as a way to increase your model’s performance and score higher on the leaderboard. We started by explaining the code functionalities of Scikit-learn, such as grid search and random search, as well as the newer halving algorithms. </p>
    <p class="normal">Then, we progressed to Bayesian optimization and explored Scikit-optimize, KerasTuner, and finally Optuna. We spent more time discussing the direct modeling of the surrogate function by Gaussian processes and how to hack it, because it can allow you greater intuition and a more ad hoc solution. We recognize that, at the moment, Optuna has become a gold standard among Kagglers, for tabular competitions as well as for deep neural network ones, because of its speedier convergence to optimal parameters in the time allowed in a Kaggle Notebook. </p>
    <p class="normal">However, if you want to stand out among the competition, you should strive to test solutions from other optimizers as well. </p>
    <p class="normal">In the next chapter, we will move on to discuss another way to improve your performance in Kaggle competitions: ensembling models. By discovering the workings of averaging, blending, and stacking, we will illustrate how you can boost your results beyond what you can obtain by tuning hyperparameters alone.</p>
    <h1 id="_idParaDest-147" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask me Anything</em> session with the authors: </p>
    <p class="normal"><a href="https://packt.link/KaggleDiscord"><span class="url">https://packt.link/KaggleDiscord</span></a></p>
    <p class="normal"><img src="../Images/QR_Code40480600921811704671.png" alt=""/></p>
  </div>
</body></html>