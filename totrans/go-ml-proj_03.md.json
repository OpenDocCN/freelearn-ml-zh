["```py\nSubject: re : 2 . 882 s - > np np\n> date : sun , 15 dec 91 02 : 25 : 02 est > from : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 queries > > wlodek zadrozny asks if there is \" anything interesting \" to be said > about the construction \" s > np np \" . . . second , > and very much related : might we consider the construction to be a form > of what has been discussed on this list of late as reduplication ? the > logical sense of \" john mcnamara the name \" is tautologous and thus , at > that level , indistinguishable from \" well , well now , what have we here ? \" . to say that ' john mcnamara the name ' is tautologous is to give support to those who say that a logic-based semantics is irrelevant to natural language . in what sense is it tautologous ? it supplies the value of an attribute followed by the attribute of which it is the value . if in fact the value of the name-attribute for the relevant entity were ' chaim shmendrik ' , ' john mcnamara the name ' would be false . no tautology , this . ( and no reduplication , either . )\n```", "```py\nfunc main() {\n  a := \"The child was learning a new word and was using it excessively. \\\"shan't!\\\", she cried\"\n  dict := make(map[string]struct{}) \n  words := strings.Split(a, \" \")\n  for _, word := range words{\n    fmt.Println(word)\n    dict[word] = struct{}{} // add the word to the set of words already seen before.\n  }\n}\n```", "```py\nThe\nchild\nwas\nlearning\na\nnew\nword\nand\nwas\nusing\nit\nexcessively.\n\"shan't!\",\nshe\ncried\n```", "```py\nfunc main() {\n  a := \"The child was learning a new word and was using it excessively. \\\"shan't!\\\", she cried\"\n  dict := make(map[string]struct{}) \n  words := strings.Split(a, \" \")\n  for _, word := range words{\n    dict[word] = struct{}{} // add the word to the set of words already seen before.\n  }\n\n  b := \"she shan't be learning excessively.\"\n  words = strings.Split(b, \" \")\n  for _, word := range words {\n    _, ok := dict[word]\n    fmt.Printf(\"Word: %v - %v\\n\", word, ok)\n  }\n}\n```", "```py\nWord: she - true\nWord: shan't - false\nWord: be - false\nWord: learning - true\nWord: excessively. - true\n```", "```py\nThe\nchild\nwas\nlearning\na\nnew\nword\nand\nwas\nusing\nit\nexcessively\n.\n\"\nsha\nn't\n!\n\"\n,\nshe\ncried\n```", "```py\nthe\nchild\nbe\nlearn\na\nnew\nword\nand\nbe\nuse\nit\nexcessively\nshall\nnot\nshe\ncry\n```", "```py\n// Example is a tuple representing a classification example\ntype Example struct {\n    Document []string\n    Class\n}\n```", "```py\nfunc ingest(typ string) (examples []Example, err error) {\n  switch typ {\n  case \"bare\", \"lemm\", \"lemm_stop\", \"stop\":\n  default:\n    return nil, errors.Errorf(\"Expected only \\\"bare\\\", \\\"lemm\\\", \\\"lemm_stop\\\" or \\\"stop\\\"\")\n  }\n\n  var errs errList\n  start, end := 0, 11\n\n  for i := start; i < end; i++ { // hold 30% for crossval\n    matches, err := filepath.Glob(fmt.Sprintf(\"data/lingspam_public/%s/part%d/*.txt\", typ, i))\n    if err != nil {\n      errs = append(errs, err)\n      continue\n    }\n\n    for _, match := range matches {\n      str, err := ingestOneFile(match)\n      if err != nil {\n        errs = append(errs, errors.WithMessage(err, match))\n        continue\n      }\n\n      if strings.Contains(match, \"spmsg\") {\n        // is spam\n        examples = append(examples, Example{str, Spam})\n      } else {\n        // is ham\n        examples = append(examples, Example{str, Ham})\n      }\n    }\n  }\n  if errs != nil {\n    err = errs\n  }\n  return\n}\n```", "```py\nfunc ingestOneFile(abspath string) ([]string, error) {\n  bs, err := ioutil.ReadFile(abspath)\n  if err != nil {\n    return nil, err\n  }\n  return strings.Split(string(bs), \" \"), nil\n}\n```", "```py\ntype errList []error\n\nfunc (err errList) Error() string {\n  var buf bytes.Buffer\n  fmt.Fprintf(&buf, \"Errors Found:\\n\")\n  for _, e := range err {\n    fmt.Fprintf(&buf, \"\\t%v\\n\", e)\n  }\n  return buf.String()\n}\n```", "```py\nunc main() {\n  examples, err := ingest(\"bare\")\n  log.Printf(\"Examples loaded: %d, Errors: %v\", len(examples), err)\n  shuffle(examples)\n\n  if len(examples) == 0 {\n    log.Fatal(\"Cannot proceed: no training examples\")\n  }\n\n  // create new classifier\n  c := New()\n\n  // train new classifier\n  c.Train(examples)\n\n  // predict\n  predicted := c.Predict(aDocument)\n  fmt.Printf(\"Predicted %v\", predicted)\n}\n```", "```py\ntype Classifier {}\n\nfunc (c *Classifier) Train(examples []Example) {}\n\nfunc (c *Classifier) Predict(document []string) Class { ... }\n```", "```py\ngo get -u github.com/go-nlp/tfidf\n```", "```py\ntype Classifier struct {\n  corpus *corpus.Corpus\n\n  tfidfs [MAXCLASS]*tfidf.TFIDF\n  totals [MAXCLASS]float64\n\n  ready bool\n  sync.Mutex\n}\n```", "```py\ntype Class byte\n\nconst (\n  Ham Class = iota\n  Spam\n  MAXCLASS\n)\n```", "```py\nfunc ExportedFn(a Class) error {\n  // does some decision making with a\n}\n```", "```py\nfunc (c Class) isValid() bool { return c < MAXCLASS }\n```", "```py\ndata Class = Ham \n            |Spam\n```", "```py\nfunc ExportedFn(a Class) error {\n  if !a.isValid() {\n    return errors.New(\"Invalid class\")\n  }\n  // does some decision making with a\n  }\n}\n```", "```py\ntype Class string\n\nconst (\n  Ham Class = \"Ham\"\n  Spam Class = \"Spam\"\n)\n```", "```py\ntype Classifier struct {\n  corpus *corpus.Corpus\n\n  tfidfs map[Class]*tfidf.TFIDF\n  totals map[Class]float64\n\n  ready bool\n  sync.Mutex\n}\n```", "```py\nfunc (c *Classifier) Train(examples []Example) {\n  for _, ex := range examples {\n    c.trainOne(ex)\n  }\n}\n\nfunc (c *Classifier) trainOne(example Example) {\n  d := make(doc, len(example.Document))\n  for i, word := range example.Document {\n    id := c.corpus.Add(word)\n    d[i] = id\n  }\n  c.tfidfs[example.Class].Add(d)\n  c.totals[example.Class]++\n}\n```", "```py\ntype doc []int\n\nfunc (d doc) IDs() []int { return []int(d) }\n```", "```py\nfunc (c *Classifier) Score(sentence []string) (scores [MAXCLASS]float64) {\n  if !c.ready {\n    c.Postprocess()\n  }\n\n  d := make(doc, len(sentence))\n  for i, word := range sentence {\n    id := c.corpus.Add(word)\n    d[i] = id\n  }\n\n  priors := c.priors()\n\n  // score per class\n  for i := range c.tfidfs {\n    score := math.Log(priors[i])\n    // likelihood\n    for _, word := range sentence {\n      prob := c.prob(word, Class(i))\n      score += math.Log(prob)\n    }\n\n    scores[i] = score\n  }\n  return\n}\n```", "```py\nfunc (c *Classifier) Predict(sentence []string) Class {\n  scores := c.Score(sentence)\n  return argmax(scores)\n}\n```", "```py\nfunc (c *Classifier) Postprocess() {\n  c.Lock()\n  if c.ready {\n    c.Unlock()\n    return\n  }\n\n  var docs int\n  for _, t := range c.tfidfs {\n    docs += t.Docs\n  }\n  for _, t := range c.tfidfs {\n    t.Docs = docs\n    // t.CalculateIDF()\n    for k, v := range t.TF {\n      t.IDF[k] = math.Log1p(float64(t.Docs) / v)\n    }\n  }\n  c.ready = true\n  c.Unlock()\n}\n```", "```py\nfunc (c *Classifier) priors() (priors []float64) {\n  priors = make([]float64, MAXCLASS)\n  var sum float64\n  for i, total := range c.totals {\n    priors[i] = total\n    sum += total\n  }\n  for i := Ham; i < MAXCLASS; i++ {\n    priors[int(i)] /= sum\n  }\n  return\n}\n```", "```py\n  // likelihood\n  for _, word := range sentence {\n    prob := c.prob(word, Class(i))\n    score += math.Log(prob)\n  }\n```", "```py\nfunc (c *Classifier) prob(word string, class Class) float64 {\n  id, ok := c.corpus.Id(word)\n  if !ok {\n    return tiny\n  }\n\n  freq := c.tfidfs[class].TF[id]\n  idf := c.tfidfs[class].IDF[id]\n  // idf := 1.0\n\n  // a word may not appear at all in a class.\n  if freq == 0 {\n    return tiny\n  }\n\n  return freq * idf / c.totals[class]\n}\n```", "```py\n  typ := \"bare\"\n  examples, err := ingest(typ)\n  log.Printf(\"errs %v\", err)\n  log.Printf(\"Examples loaded: %d\", len(examples))\n  shuffle(examples)\n  cvStart := len(examples) - len(examples)/3\n  cv := examples[cvStart:]\n  examples = examples[:cvStart]\n```", "```py\n  c := New()\n  c.Train(examples)\n\n  var corrects, totals float64\n  for _, ex := range examples {\n    // log.Printf(\"%v\", c.Score(ham.Document))\n    class := c.Predict(ex.Document)\n    if class == ex.Class {\n      corrects++\n    }\n    totals++\n  }\n  log.Printf(\"Corrects: %v, Totals: %v. Accuracy %v\", corrects, totals, corrects/totals)\n```", "```py\n  log.Printf(\"Start Cross Validation (this classifier)\")\n  corrects, totals = 0, 0\n  hams, spams := 0.0, 0.0\n  var unseen, totalWords int\n  for _, ex := range cv {\n    totalWords += len(ex.Document)\n    unseen += c.unseens(ex.Document)\n    class := c.Predict(ex.Document)\n    if class == ex.Class {\n      corrects++\n    }\n    switch ex.Class {\n    case Ham:\n      hams++\n    case Spam:\n      spams++\n    }\n    totals++\n  }\n```", "```py\ntype Classifier struct{}\nfunc (c Classifier) Predict(sentence []string) Class { return Ham }\n```", "```py\n  fmt.Printf(\"Dataset: %q. Corrects: %v, Totals: %v. Accuracy %v\\n\", typ, corrects, totals, corrects/totals)\n  fmt.Printf(\"Hams: %v, Spams: %v. Ratio to beat: %v\\n\", hams, spams, hams/(hams+spams))\n  fmt.Printf(\"Previously unseen %d. Total Words %d\\n\", unseen, totalWords)\n```", "```py\nfunc main() {\n  typ := \"bare\"\n  examples, err := ingest(typ)\n  if err != nil {\n    log.Fatal(err)\n  }\n\n  fmt.Printf(\"Examples loaded: %d\\n\", len(examples))\n  shuffle(examples)\n  cvStart := len(examples) - len(examples)/3\n  cv := examples[cvStart:]\n  examples = examples[:cvStart]\n\n  c := New()\n  c.Train(examples)\n\n  var corrects, totals float64\n  for _, ex := range examples {\n    // fmt.Printf(\"%v\", c.Score(ham.Document))\n    class := c.Predict(ex.Document)\n    if class == ex.Class {\n      corrects++\n    }\n    totals++\n  }\n  fmt.Printf(\"Dataset: %q. Corrects: %v, Totals: %v. Accuracy %v\\n\", typ, corrects, totals, corrects/totals)\n\n  fmt.Println(\"Start Cross Validation (this classifier)\")\n  corrects, totals = 0, 0\n  hams, spams := 0.0, 0.0\n  var unseen, totalWords int\n  for _, ex := range cv {\n    totalWords += len(ex.Document)\n    unseen += c.unseens(ex.Document)\n    class := c.Predict(ex.Document)\n    if class == ex.Class {\n      corrects++\n    }\n    switch ex.Class {\n    case Ham:\n      hams++\n    case Spam:\n      spams++\n    }\n    totals++\n  }\n\n  fmt.Printf(\"Dataset: %q. Corrects: %v, Totals: %v. Accuracy %v\\n\", typ, corrects, totals, corrects/totals)\n  fmt.Printf(\"Hams: %v, Spams: %v. Ratio to beat: %v\\n\", hams, spams, hams/(hams+spams))\n  fmt.Printf(\"Previously unseen %d. Total Words %d\\n\", unseen, totalWords)\n}\n```", "```py\nExamples loaded: 2893\nDataset: \"bare\". Corrects: 1917, Totals: 1929\\. Accuracy 0.9937791601866252\nStart Cross Validation (this classifier)\nDataset: \"bare\". Corrects: 946, Totals: 964\\. Accuracy 0.9813278008298755\nHams: 810, Spams: 154\\. Ratio to beat: 0.8402489626556017\nPreviously unseen 17593\\. Total Words 658105\n```", "```py\nDataset: \"lemm_stop\". Corrects: 1920, Totals: 1929\\. Accuracy 0.995334370139969\nStart Cross Validation (this classifier)\nDataset: \"lemm_stop\". Corrects: 948, Totals: 964\\. Accuracy 0.983402489626556\nHams: 810, Spams: 154\\. Ratio to beat: 0.8402489626556017\nPreviously unseen 16361\\. Total Words 489255\n```"]