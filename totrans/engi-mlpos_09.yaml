- en: 'Chapter 7: Building Robust CI/CD Pipelines'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about continuous operations in the MLOps pipeline.
    The principles you will learn in this chapter are key to driving continuous deployments
    in a business context. To get a comprehensive understanding and first-hand experience,
    we will go through the concepts and hands-on implementation simultaneously. We
    will set up a CI/CD pipeline for the test environment while learning about components
    of **continuous integration** (**CI**) and **continuous deployment** (**CD**),
    pipeline testing, and releases and types of triggers. This will equip you with
    the skills to automate the deployment pipelines of **machine learning** (**ML**)
    models for any given scenario on the cloud with continual learning abilities in
    tune with business. Let''s start by looking at why we need CI/CD in MLOps after
    all. We will continue by exploring the other topics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration, delivery, and deployment in MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a CI/CD pipeline and test environment (using Azure DevOps)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline execution and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline execution triggers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration, delivery, and deployment in MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Automation** is the primary reason for CI/CD in the MLOps workflow. The goal
    of enabling continuous delivery to the ML service is to maintain data and source
    code versions of the models, enable triggers to perform necessary jobs in parallel,
    build artifacts, and release deployments for production. Several cloud vendors
    are promoting DevOps services to monitor ML services and models in production,
    as well as orchestrate with other services on the cloud. Using CI and CD, we can
    enable continual learning, which is critical for a ML system''s success. Without
    continual learning, a ML system is deemed to end up as a failed **Proof of Concept**
    (**PoC**).'
  prefs: []
  type: TYPE_NORMAL
- en: Only a model deployed with continual learning capabilities can bring business
    value.
  prefs: []
  type: TYPE_NORMAL
- en: In order to learn to deploy a model in production with continual learning capabilities,
    we will explore CI, CD, and continuous delivery methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 7.1*, CI is key to CD and continuous delivery. Let''s
    see how these three are interconnected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Continuous integration, delivery, and deployment pipelines'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Continuous integration, delivery, and deployment pipelines
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CI aims to synchronize the application (ML pipeline and application) with the
    developer in real time. The developer's changes in commits or merges are validated
    by creating an application build on the go and by performing automated tests against
    the build. CI emphasizes automated testing with a focus on checking the application's
    robustness (if it is not broken or bugged) when new commits are merged to the
    master or main branch. Whenever a new commit is made to the master branch, a new
    build is created that is tested for robustness using automated testing. By automating
    this process, we can avoid delayed delivery of software and other integration
    challenges that can keep users waiting for days for the release. Automation and
    testing are at the heart of CI.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continuous delivery extends from CI to ensure that the new changes or releases
    are deployed and efficiently brought to users; this is facilitated by automating
    testing and release processes. Automating testing and release processes enable
    developers and product managers to deploy the changes with one click of a button,
    enabling seamless control and supervision capabilities at any phase of the process.
    In the continuous delivery process, quite often, a human agent (from the QA team)
    is involved in approving a build (pass or fail) before deploying it in production
    (as shown in *Figure 7.1* in a continuous delivery pipeline). In a typical continuous
    delivery pipeline, a build goes through preliminary acceptance tests before getting
    deployed on the staging phase where a human agent supervises the performance using
    smoke tests and other suitable tests.
  prefs: []
  type: TYPE_NORMAL
- en: Once the smoke tests have been passed, the human agent passes the build to be
    deployed in production. Automating the build and release process and having a
    human agent involved in the process ensures great quality as regards production
    and we can avoid some pitfalls that may go unnoticed with a fully automated pipeline.
    Using continuous delivery, a business can have full control over its release process
    and release a new build in small batches (easy to troubleshoot in the case of
    blockers or errors) or have a full release within a requisite time frame (daily,
    weekly, or monthly).
  prefs: []
  type: TYPE_NORMAL
- en: Continuous deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CD enables full automation and goes one step further than continuous delivery.
    All stages of build and release to your production are completely automated without
    any human intervention, unlike in continuous delivery. In such an automated pipeline,
    only a failed test can stop a new change from being deployed to production. Continuous
    deployment takes the pressure off the team to maintain the release pipeline and
    accelerates deployment straight to the customers enabling continual learning via
    feedback loops with customers.
  prefs: []
  type: TYPE_NORMAL
- en: With such automation, there is no longer a release day for developers. It takes
    the pressure off them and they can just focus on building the software without
    worrying about tests and release management. Developers can build, test, and deploy
    the software at their convenience and can go live within minutes instead of waiting
    for release days or for human approval, which can delay the release of software
    to users by days and sometimes weeks. Continuous deployment ensures full automation
    to deploy and serve robust and scalable software to users.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a CI/CD pipeline and the test environment (using Azure DevOps)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we went through the theory of CI, continuous delivery,
    and continuous deployment, and now it is time to see it in practice. Using Azure
    DevOps, we will set up a simple CI/CD pipeline of our own for the business problem
    (weather prediction), which we have been working on previously (in [*Chapter 6*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124),
    *Key Principles for Deploying Your ML System*, in the Hands-on deployment section
    (for the business problem)).
  prefs: []
  type: TYPE_NORMAL
- en: Azure DevOps is a service provided by Microsoft that facilitates source code
    management (version control), project management, CI, continuous delivery, and
    continuous deployment (automated builds, testing, and release capabilities). It
    also enables life cycle management for software applications. We will use Azure
    DevOps for hands-on training as it comes with seamless integration with the Azure
    ML service, which we have been using previously in [*C**hapter 6*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124).
    You will experience the integration and syncing of both services to make deployments
    with ease. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to your Azure DevOps project, `Learn_MLOps`. Go to the cloned repository
    and access the `07_CICD_Pipeline` folder. We will use these files (in the folder
    named `07_CICD_Pipeline`) as drivers to build a release pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will deploy previously trained ML models (from [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*) on two deployment targets: one is `AciDeployment.yml`
    file contains the configuration for the ACI deployment target, and the `AksDeployment.yml`
    file contains the configuration for the AKS cluster. `InferenceConfig.yml` points
    to inference artifacts such as `score.py` and `myenv.yml`.'
  prefs: []
  type: TYPE_NORMAL
- en: The functions defined in `score.py` will be used to pre-process the incoming
    data and infer the pre-processed data with the ML model to make predictions. The
    `myenv.yml` file is a configuration for the inference environment, for example,
    the Python version and packages to install within the environment. These files
    will be used as drivers to facilitate the release pipeline. Now that you have
    familiarized yourself with these files, let's begin by connecting the Azure ML
    service and the Azure DevOps project using a service principal.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service principal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to sync Azure ML services and Azure DevOps in order to facilitate CI
    between both the services. Previously (in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*) we had developed and managed our ML models using
    Azure ML service, and we used the `Learn_MLOps` workspace. Now, we will connect
    the Azure ML workspace (named `Learn_MLOps`) with the Azure DevOps project (named
    `Learn_MLOps`) using a service principal.
  prefs: []
  type: TYPE_NORMAL
- en: 'A service principal is an identity created for inter-application communication;
    it is a connection automation tool to access Azure resources. Service principal
    also takes care of the networking and connectivity aspects of your applications.
    Perform the following steps to set up a service principal for the pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to **Project Settings** (on the bottom left of your screen) and select **Service
    connections**. Click the **New service connection** option/button to reveal the
    New service connection window, as shown in *Figure 7.2*:![Figure 7.2 – New service
    principal connection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.2 – New service principal connection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select **Azure Resource Manager** for the connection type and proceed by clicking
    **Next**. Select **Service principal (automatic)** and proceed to the final step
    of creating a service principal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be prompted to create a new service connection. Set the scope as **Machine
    Learning Workspace** and point to the **Subscription**, **Resource group** and
    **Machine Learning Workspace** as shown in *Figure 7.3:*![Figure 7.3 – Final step
    in creating a service principal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.3 – Final step in creating a service principal
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Name the service principal in the `mlops_sp` as shown in *Figure 7.3*). Lastly,
    tick the checkbox (**Grant access permission to all pipelines**) and click **Save**
    to create the service principal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this, your service principal with the given name (for example, `mlops_sp`)
    is ready to be used for orchestrating CI/CD pipelines. Next, we will install the
    extension used for the pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the extension to connect to the Azure ML workspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Microsoft has developed an extension called **Machine Learning**. It is available
    in the Azure DevOps Marketplace. It is used to orchestrate models and artifacts
    from our desired Azure ML workspace. It lets us deploy models from the workspace
    to our desired deployment targets such as ACI or AKS. We will install the ML extension
    and use it to orchestrate the CI/CD pipeline. Perform the following steps to install
    the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Marketplace to look for the **Machine Learning** extension. To go
    to the Marketplace, click on the bag icon in the top right of your screen, as
    shown in *Figure 7.4*:![Figure 7.4 – Finding the Azure DevOps Marketplace
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.4 – Finding the Azure DevOps Marketplace
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After entering the Marketplace, you will be presented with multiple extensions
    to add to your Azure DevOps project. Next, we will search for the **Machine Learning**
    extension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Search for the **Machine Learning** extension and install the extension for
    free. Click the **Get it free** button to install the extension as shown in *Figure
    7.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Installing the Machine Learning extension'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – Installing the Machine Learning extension
  prefs: []
  type: TYPE_NORMAL
- en: The **Machine Learning** extension will be installed upon clicking the **Get
    it free** button. After successful installation, you can use the **Machine Learning**
    extension to orchestrate jobs in the CI/CD pipeline. With these prerequisites,
    you are set to configure the continuous deployment or continuous delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a continuous integration and deployment pipeline for the test environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will configure the CI/CD pipeline for the staging environment
    (also called the test environment). We will use this pipeline to facilitate continual
    learning and automate deployments. Let''s get started by going to **Pipelines**
    >> **Releases**, as shown in *Figure 7.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Setting up your CI/CD pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Setting up your CI/CD pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Create a new pipeline in the `Port Weather ML Pipeline`. Next, we will start
    connecting the requisite artifacts to enable the pipeline, such as the repository
    containing the code and the Azure ML workspace containing the models to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting artifacts to the pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Connect to your Azure DevOps repository. The Azure DevOps repository serves
    as the central code repository to orchestrate deployments and operations on Azure
    DevOps. Hence, let''s connect the repository (`Learn_MLOps`) to the release pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 7.7*, go to the `Learn_MLOps`) to connect with the release
    pipeline:![Figure 7.7 – Connecting the Azure DevOps repository as an artifact
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.7 – Connecting the Azure DevOps repository as an artifact
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the default branch (for example, `Learn_MLOps`) and icon in the **Artifacts**
    section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to your Azure ML workspace. To connect your Azure ML workspace to the
    release pipeline, go to the `scaler` artifact previously registered in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines,*
    to scale the incoming data using the standard:![Figure 7.8 – Connecting the scaler
    as an artifact
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.8 – Connecting the scaler as an artifact
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After selecting the `model_scaler` artifact, add the artifact to the release
    pipeline by clicking the `model_scaler` artifact, you will be able to see the
    model''s name (`model_scaler`) and a model icon in the `support_vector_classifier`
    model to the release pipeline artifacts. Start by clicking the `mlops_sp`) and
    select the `support_vector_classifier` model trained previously in *Chapter 4*,
    *Machine Learning Pipelines*. Add the model artifact to the pipeline by hitting
    the **Add** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Connected artifacts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Connected artifacts
  prefs: []
  type: TYPE_NORMAL
- en: After adding the `support_vector_classifier` model, you will be able to see
    the model's name (`support_vector_classifier`) and a model icon in the **Artifacts**
    section, as shown in *Figure 7.9*.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! We have all three desired artifacts (`Learn_MLOps`, `scaler`,
    and **support_vector_classifier**) connected to the release pipeline. We can use
    these artifacts to orchestrate the deployments in the pipeline. Next, get ready
    to configure the Staging/TEST environment!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a test environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s set up a continuous integration and continuous deployment pipeline for
    the TEST environment in the pipeline. In this stage, we test the robustness of
    the service and perform various tests to validate the service readiness for production:'
  prefs: []
  type: TYPE_NORMAL
- en: To get started, click on the `DEV` `TEST`. We will name the stage `DEV` `TEST`
    as this will be our development and testing environment. Ideally, both DEV and
    TEST are different stages, but for simplicity and avoiding repetitive implementation,
    we will merge them both. See the following *Figure 7.10*:![Figure 7.10 – Setting
    up the DEV TEST stage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.10 – Setting up the DEV TEST stage
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After naming the stage, save the stage by clicking the **Save** button at the
    top. Every stage is a composition of a series of steps or jobs to check the robustness
    of the stage. Next, we will configure the jobs within the **DEV TEST** stage.
    A CI/CD job, in simple terms, is a process or script to execute or test deployments
    (for example, a job to deploy a model on the Kubernetes cluster). To configure
    jobs, click on the **1 job, 0 task** link in the **DEV TEST** stage, as shown
    in *Figure 7.11*:![Figure 7.11 – Configuring DEV TEST jobs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.11 – Configuring DEV TEST jobs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon clicking the **1 job, 0 task** link in the **DEV TEST** stage, you will
    have to add agent jobs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Add a task to the agent job by clicking `AzureML model deploy`, as shown in
    *Figure 7.12*:![Figure 7.12 – Adding a job – AzureML Model Deploy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.12 – Adding a job – AzureML Model Deploy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon adding the `inferenceconfig` file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you will be prompted to enter the deployment information. As shown in
    *Figure 7.13*, point to your Azure ML workspace (for example, `mlops_ws`) and
    set the `Model Source` option to **Model Artifact** (as we are using the model
    artifacts generated previously when training and packaging models):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Adding a job – Azure ML Model Deploy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Adding a job – Azure ML Model Deploy
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at the `inferenceConfig` file and its functionality. The
    following snippet is taken from `inferenceConfig.yml` (in the repository). Here
    is a snapshot of `inferenceConfig.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inferenceConfig.yml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a representation of the settings for a custom environment in which we
    will deploy our models. It points to the `score.py` file (previously created in
    [*Chapter 6*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124), *Key Principles
    for Deploying Your ML System*) and the `conda` file `myenv.yml`, which defines
    the `conda` environment (packages and dependencies to install). Here is a snapshot
    of `myenv.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`myenv.yml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Both the `score.py` and `myenv.yml` files are tied up in the `inferenceConfig.yml`
    file to facilitate the deployment and inference of ML models. Proceed by selecting
    your inference configuration file (`inferenceConfig.yml`), as shown in *Figure
    7.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Selecting your inference configuration file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Selecting your inference configuration file
  prefs: []
  type: TYPE_NORMAL
- en: 'After pointing to the `inferenceConfig.yml` file in your Azure DevOps repository,
    your basic configuration is done for the deployment. Lastly, we will configure
    the deployment information by pointing to the `AciDeploymentConfig.yml`) for ACI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AciDeploymentConfig.yml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It contains the infrastructural definition for provisioning the requisite compute
    for deployment, such as CPU units, memory in GB, and other authentication or security
    definitions. Let''s select this deployment configuration file to set up the release
    pipeline for the staging environment, as shown in *Figure 7.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Adding deployment information'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 – Adding deployment information
  prefs: []
  type: TYPE_NORMAL
- en: After adding the deployment configuration file, save the job by clicking the
    **Save** button in the top right of the screen and then go to **Pipelines** >>
    **Releases** (on the left of your screen) to see your pipeline successfully set
    up. Let's continue from here to test the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline execution and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it is time to test your pipeline and for that we will create a release
    and validate whether the pipeline release has executed successfully. The following
    steps will help you to test your pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Create release** button to execute jobs configured on your pipeline.
    A popup will appear on the right of your screen (as shown in *Figure 7.16*) to
    view and select artifacts to deploy in your staging environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the artifacts (`_scaler` and `_support-vector-classifier`) and select
    their versions. For simplicity, version 1 is recommended for both.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to choose another version of your model or scaler make sure to change
    the path of your model and scaler in the `score.py` file (that is, insert the
    appropriate version number in the `scaler` and `model` paths `model-scaler/{version
    number}/modelscaler.pkl` and `support-vector-classifier/ {version number} /svc.onnx`.
    If you choose version 1, you don't have to worry about changing the code in `score.py`
    file as the paths contain version 1\.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting artifacts and needed versions (version 1 is recommended), click
    on the **Create** button to create the release for your selected artifacts:![Figure
    7.16 – Creating a release
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.16 – Creating a release
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now the release pipeline (the CI/CD pipeline) is triggered to execute. All the
    steps defined in the pipeline will execute, such as downloading the artifacts,
    provisioning the ACI compute instance for deployment, and deploying the web service.
    Upon successful execution, you'll be notified with a green tick-mark on your release,
    as shown in *Figure 7.17*:![Figure 7.17 – Monitoring releases
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.17 – Monitoring releases
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can monitor all your releases in the `scaler` and `_support-vector-classifier`)
    have been deployed as a web service on ACI, as shown in *Figure 7.18*:![Figure
    7.18 – Successful jobs in a release (test environment)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.18 – Successful jobs in a release (test environment)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, go and check your Azure ML workspace (from the **Endpoints** section)
    to view the deployed web service, as shown in *Figure 7.19*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Web service deployed on the Azure ML workspace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_07_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.19 – Web service deployed on the Azure ML workspace
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully deployed a web service in the test environment. We can
    see the REST endpoint and the service name **devtest-webservice**. This brings
    us to the successful conclusion of the building and testing of the CI/CD pipeline
    for the test environment. Pipelines can be driven using triggers, and in the next
    section, we will look at what the triggers are and how we can use them to build
    optimal CI/CD pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline execution triggers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an effective CI/CD pipeline, process execution should be possible by means
    of multiple events or triggers. Having the option to trigger the pipeline by only
    regular events, such as code repository or push-or-pull requests, might be a handicap
    or limitation for the system. Having the option to trigger the pipeline process
    using multiple events enhances the flexibility and functionality of the CI/CD
    pipeline. Let''s look at some types of triggers that can add value to the CI/CD
    pipeline process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Artifactory triggers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifacts are generated at different stages in the pipeline and development
    process. Generated artifacts, such as a trained model, metadata, uploaded Docker
    images, or any file that has been uploaded, can be triggered to execute a certain
    process in the CI/CD pipeline. Having such options can enable great flexibility
    and functionality for the CI/CD pipeline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Docker Hub triggers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time you push a new Docker image to a Docker Hub repository of your choice,
    a trigger in the CI/CD pipeline can be executed as per requirements. For example,
    when you upload a new Docker image to Docker Hub (or Azure Container Registry),
    the pipeline is triggered to deploy the Docker image as a web service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Schedule triggers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline process can be triggered following a specific time schedule. This
    type of trigger is very useful for a scheduled clean-up or cron jobs or any other
    workflow that needs to be run following a time interval; for example, a trigger
    for ML model retraining at 12:00 every day.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`retrain` on a developer''s platform, the pipeline can be triggered to retrain
    the existing deployed model. These triggers are facilitated using API calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git triggers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git triggers are commonly used to trigger pipeline executions, for instance
    when new code is committed to a branch or a new pull request is made. When changes
    are made to a repository, then certain processes can be triggered in the pipeline
    as per requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Azure DevOps provides multiple trigger options (all of the above). Now, let''s
    set up a Git trigger, based on the Git commit made to the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to `Edit` (in the top right of your screen) to edit the existing pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the repository artifact (named `_Learn_MLOps`), as shown in *Figure
    7.20*, and enable (by clicking on the toggle switch) the continuous deployment
    trigger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a branch filter by including the develop branch. This will trigger the pipeline
    to execute when changes or commits are made to the develop branch of the repository.
    For the test or staging stage, configure a Git trigger for the develop branch
    only (not the master or another branch). For production we can configure a Git
    trigger for the master branch. This way, we can separate the Git trigger branches
    for the test and production stages:![Figure 7.20 – Enabling a Git trigger for
    the test environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_07_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.20 – Enabling a Git trigger for the test environment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Save** button at the top to configure the Git trigger. Congratulations!
    You have successfully set up a continuous deployment Git trigger for your test
    environment. Whenever there are changes to the develop branch of the repository,
    the pipeline will be triggered to deploy a web service in the test (**DEV TEST**)
    environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned the key principles of continuous operations
    in MLOps, primarily, continuous integration, delivery, and deployment. We have
    learned this by performing a hands-on implementation of setting up a CI/CD pipeline
    and test environment using Azure DevOps. We have tested the pipeline for execution
    robustness and finally looked into some triggers to enhance the functionality
    of the pipeline and also set up a Git trigger for the test environment. This chapter
    serves as the foundation for continual operations in MLOps and equips you with
    the skills to automate the deployment pipelines of ML models for any given scenario
    on the cloud, with continual learning abilities in tune with your business.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into APIs, microservices, and what they have
    to offer for MLOps-based solutions.
  prefs: []
  type: TYPE_NORMAL
