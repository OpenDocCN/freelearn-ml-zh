- en: 'Chapter 9: Testing and Securing Your ML Solution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will delve into **Machine Learning** (**ML**) solution testing
    and security aspects. You can expect to get a primer on various types of tests
    to test the robustness and scalability of your ML solution, as well as the knowledge
    required to secure your ML solution. We will look into multiple attacks on ML
    solutions and ways to defend your ML solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be learning with examples as we perform load testing
    and security testing for the business use case of weather prediction we have been
    previously working on. We will start by reflecting on the need for testing and
    securing your ML solution and go on to explore the other following topics in the
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for testing and securing your ML application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing your ML solution by design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing your ML solution by design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for testing and securing your ML application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The growing adoption of data-driven and ML-based solutions is causing businesses
    to have to handle growing workloads, exposing them to extra levels of complexities
    and vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Cybersecurity is the most alarming risk for AI developers and adopters. According
    to a survey released by Deloitte ([https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html](https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html)),
    in July 2020, 62% of adopters saw cybersecurity risks as a significant or extreme
    threat, but only 39% said they felt prepared to address those risks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look into the need for securing ML-based systems and
    solutions. We will reflect on some of the broader challenges of ML systems such
    as bias, ethics, and explainability. We will also study some of the challenges
    present at each stage of the ML life cycle relating to confidentiality, integrity,
    and availability using the guidelines for ML testing and security by design.
  prefs: []
  type: TYPE_NORMAL
- en: Testing your ML solution by design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On top of performing regular software development tests, such as unit tests,
    integration tests, system testing, and acceptance testing, ML solutions need additional
    tests because data and ML models are involved. Both the data and models change
    dynamically over time. Here are some concepts for testing by design; applying
    them to your use cases can ensure robust ML solutions are produced as a result.
  prefs: []
  type: TYPE_NORMAL
- en: Data testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of testing data is to ensure that the data is of a high enough quality
    for ML model training. The better the quality of the data, the better the models
    trained for the given tasks. So how do we assess the quality of data? It can be
    done by inspecting the following five factors of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completeness (no missing values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency (in terms of expected data format and volume)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance (data should meet the intended need and requirements)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeliness (the latest or up-to-date data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these factors, if a company can manage each dataset''s data quality
    when received or created, the data quality is guaranteed. Here are some steps
    that your team or company can use as quality assurance measures for your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Meticulous data cataloging and control of incoming data**: A combination
    of data cataloging (to document and store data in the required format or pattern)
    and control functions can ensure a high quality of incoming data. Data cataloging
    and control can be done by monitoring data factors such as data formats and patterns,
    value distributions and anomalies, completeness, and consistency can help to provide
    good incoming data quality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Curating data pipelines carefully to avoid duplicate data**: When duplicated
    data is manufactured from the same data source and using the same logic by different
    people, it can get complicated to manage lineages, authenticity, and data integrity.
    This can produce cascading effects throughout multiple systems or databases. It
    is better to avoid duplicating data as much as possible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data governance with enforced integrity**: In today''s world, maintaining
    data integrity has become crucial. Not having the mindset of enforcing data integrity
    can be costly for an organization. The data could eventually become incomplete,
    delayed, or out of date, leading to serious data quality issues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Maintaining end-to-end traceability and lineage**: Data lineage and traceability
    can be achieved by the smart use of metadata and the data itself. Using both,
    we can document critical information such as unique keys for each dataset, adding
    a timestamp to each record, and logging data changes. Making sure data lineage
    and end-to-end traceability is enabled can give us the possibility to reproduce
    models and debug errors and pipelines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model tests need to cover server issues such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the accuracy or key metric of the ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing on random data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing for acceptable loss or performance on your task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for model robustness using real data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These tests can be orchestrated in two phases: pre-training and post-training.
    Having these tests facilitated in the workflow can produce robust models for production.
    Let''s look at what pre-train and post-train tests can be done by design.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tests can be performed to catch flaws before we proceed to the training stage.
    These flaws could be in the data, pipelines, or parameters. *Figure 9.1* suggests
    running pre-training and post-training tests as part of a proposed workflow for
    developing high-quality models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Proposed workflow for developing high-quality models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Proposed workflow for developing high-quality models
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ways to detect and avoid pre-training flaws using pre-training
    tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating data pipeline debt by handling any data leakage, edge cases, and
    optimizing to make the pipeline time- and resource-efficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sure the shape of your model output matches the labels in your dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the output ranges to make sure they match our expectations (such as
    checking that the output of a classification model is a distribution with class
    probabilities that sum to 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining your training and validation datasets for label leakage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sure the ETL pipeline outputs or fetches data in the required format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-training tests do not need parameters to run, but they can be quite useful
    in catching bugs before running the model training.
  prefs: []
  type: TYPE_NORMAL
- en: Post-training tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Post-training tests enable us to investigate model performance and the logic
    behind model predictions and foresee any possible flaws in the model before deploying
    the model to production. Post-training tests enable us to detect flaws in model
    performance and functionality. Post-training tests involve a model performance
    evaluation test, invariance test, and minimum functionality test. Here is a recommended
    read for more insights on post-training tests: *Beyond Accuracy: Behavioral Testing
    of NLP Models with CheckList* ([https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment and inference testing
  prefs: []
  type: TYPE_NORMAL
- en: Deployment testing involves testing **Continuous Integration/Continuous Delivery**
    (**CI/CD**) pipeline delivery, integration tests, and testing that deployment
    is successful. It is critical to test the deployed model and that is where inference
    testing comes in to stress- or load-test the deployed model and test its performance
    on real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will load test a previously deployed model (for a use
    case).
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on deployment and inference testing (a business use case)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you have your service (either API or ML) ready and you are about to serve
    it to the users but you don't have any clue about how many users it can actually
    handle and how it will react when many users access it simultaneously, that's
    where load testing is useful to benchmark how many users your service can serve
    and to validate whether the service can cater to the business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform load testing for the service we deployed previously (in [*Chapter
    7*](B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143), *Building Robust CI and CD
    Pipelines*). `Locust.io` will be used for load testing. `locust.io` is an open
    source load-testing tool. For this, we will install `locust` (using `pip`) and
    curate a Python script using the locust.io SDK to test an endpoint. Let''s get
    started by installing `locust`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `locust`: Go to your terminal and execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Curate the `load_test.py` script: Go to your favorite IDE and start curating
    the script or follow the steps in the premade script. To access the premade script
    go to the *Engineering MLOps* repository cloned previously, access the `09_Testing_Security`
    folder, and go to the `load_test.py` file. Let''s demystify the code in `load_test.py`
    – firstly, the needed libraries are imported as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We imported the `time`, `json`, and `locust` libraries, and then from `locust`
    we import the following required functions: `HttpUser` (a user agent that can
    visit different endpoints), `task`, and `between`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a `test_data` variable with sample test data to infer the ML model during
    the load test. Define `headers` we will use for the API calls in our load test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the core functionality of the load test as part of
    the `MLServiceUser` class (you can name it whatever you want) by extending `HttpUser`.
    `HttpUser` is the user agent that can visit different endpoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We created a `wait_time` variable using the `between()` function, which specifies
    the time it takes between finishing testing one endpoint and switching to test
    the next endpoint. So, we specify `wait_time` as `1` to `5` seconds in the `between(1,5)`
    function. The next part is the crux of defining a task that tests an endpoint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For this, we use a `@task` wrapper or decorator to start defining our task to
    test an endpoint of our choice using a custom function. Define a custom function,
    `def` `test_weather_predictions()`, and make a `post` request to the endpoint
    using `test_data` and the headers defined previously. Now, we are set to run the
    load testing!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the `locust.io` server: Go to your terminal and change to the location
    where you have the `load_test.py` file (such as in the `09_Testing_Security` folder
    of the cloned repository used in this book), then run the following command to
    spin up a `locust.io` server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The execution of the previous command will spin up the `locust` server at port
    `8089`. We can perform load tests on the web interface rendered by `locust.io`.
    To access the web service, open a browser of your choice and go to the following
    web address: `http://0.0.0.0:8089/`, as shown in *Figure 9.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Access the Locust.io web service'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16572_09_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.2 – Access the Locust.io web service
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the load test: Opening the web service will prompt you to specify options
    such as the number of users, spawn rate, and host (the endpoint to test). Specify
    the number of users to simulate, and the spawn rate (how many users will be spawned
    per second) as per your requirements, to validate whether your endpoint is capable
    of serving your business/user needs, for example, 50 users and a spawn rate of
    50.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, enter the endpoint or host you would like to load test and hit **Start
    swarming** to start performing the load test. In [*Chapter 7*](B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143),
    *Building Robust CI and CD Pipelines*, we deployed an endpoint. It is recommended
    to test the deployed endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to your Azure ML workspace, access the **Endpoints** section, access the
    deployed endpoint named **dev-webservice**, and copy and paste the endpoint web
    address into the host textbox.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click **Start** **swarming** to start load testing the endpoint. This
    will start the load test and open a new page where you can monitor your load tests
    in real time as shown in *Figure 9.3*:![Figure 9.3 – Monitor the load test in
    real time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_09_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.3 – Monitor the load test in real time
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Analyzing load testing results: You can monitor statistics, charts, failures,
    and exceptions in real time. For instance, in *Figure 9.3* we are monitoring the
    load test for the `POST` request with test data. The number of requests made (**77459**),
    the number of fails (**0**), the average response time (**75ms**), and other information
    can be monitored. It is important to check that there are no failures and that
    the average response time is within the range to serve your business/user needs
    with efficiency or no major speed breaker.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you have no failed requests and the average response time is within the
    range required, then your endpoint has passed the load test and is ready to be
    served to users. After or during the load testing, you can view charts of the
    load-testing performance with critical information such as total requests per
    second, response times, and the number of users with the progression of time.
    We can view this information in real time as shown in *Figure 9.4* and *Figure
    9.5*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Charts showing the total requests per second and response times'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_09_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Charts showing the total requests per second and response times
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9.4* we can notice that the number of requests per second is in the
    range of 18-22 as the simulated users of `locust.io` make requests, and the response
    time in milliseconds varies from 70 to 500 in some cases, with a 430ms variance
    between the minimum and maximum. The average request time is 75ms (as seen in
    *Figure 9.3*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that this kind of performance may or may not be ideal for a given
    use case, depending on your business or user needs. A more stable response time
    is desirable; for instance, a response time variance of no more than 50ms between
    the minimum and maximum response times may be preferable for a stable performance.
    To achieve such performance it is recommended to deploy your models on higher-end
    infrastructure as appropriate, for example, a GPU or a high-end CPU, unlike the
    deployment on a CPU in an Azure container instance. Similarly, in *Figure 9.5*
    we can see the response times versus the number of users:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Charts showing the total requests per second and the number
    of users'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_09_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Charts showing the total requests per second and the number of
    users
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the number of users spawned per second is 50, as mentioned (in
    *Figure 9.2*). As time progresses, the spawn rate is constant and response times
    vary between 70-500ms, with 75ms as the average response time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Document or download results: After the load test has been executed successfully
    you can document or present the results of the load test to the relevant stakeholders
    (QA/product manager) using a test report. To download or access the test report,
    go to the `.csv` files, as shown in *Figure 9.6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Download the test results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_09_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Download the test results
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the statistics or failure reports required for further inspection
    as per your needs, and note that you can access the full test report by clicking
    **Download the Report**, as shown in *Figure 9.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Download the test results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_09_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Download the test results
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive test report is presented with critical information, such as
    the endpoint inferred average request time and the minimum and maximum request
    times, and this information is also presented in the form of visualized charts
    as seen in *Figure 9.7*. You can also download this full report to present to
    your respective stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you have performed a hands-on load test to validate your endpoint
    and check whether your ML service is able to serve your business or user needs
    with efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Securing your ML solution by design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Securing your ML applications is more important than ever due to the growing
    adoption of AI to provide smart applications. Designing and developing ML systems
    without keeping security in mind can be costly in terms of exposing the system
    to hackers, leading to manipulation, data breaches, and non-compliance. Robustness
    and security play an important role in ensuring an AI system is trustworthy. To
    build trustworthy ML applications, keeping security in mind is vital to not leave
    any stones unturned.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.8* shows a framework for creating secure ML applications by design.
    The framework addresses key areas in the ML life cycle, ensuring confidentiality,
    integrity, and availability within those specific stages. Let''s reflect upon
    each area of the ML life cycle and address the issues of confidentiality, integrity,
    and availability in each area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Framework for securing the ML life cycle by design'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_09_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Framework for securing the ML life cycle by design
  prefs: []
  type: TYPE_NORMAL
- en: Let's reflect upon each area of the ML life cycle and address confidentiality,
    integrity, and availability in each area while looking at the different types
    of attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Types of attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will explore some of the most common attacks on ML systems. At a high level,
    attacks by hackers can be broken down into four categories: poisoning, input attacks
    and evasion, reverse engineering, and backdoor attacks. Let''s see how attackers
    manage to infiltrate ML systems via these attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Poisoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A hacker or attacker seeks to compromise an AI model in a poisoning attack.
    Poisoning attacks can happen at any stage (training, deployment, or real-time
    inference). They occur typically in training and inference. Let''s see how poisoning
    attacks are implemented in three typical ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset poisoning**: Training datasets contain the knowledge on which the
    model is trained. An attacker can manipulate this knowledge by infiltrating the
    training dataset. Here, the attacker introduces wrongly labeled or incorrect data
    into the training dataset, and with this the entire learning process is distorted.
    This is a direct way to poison a model. Training datasets can be poisoned during
    the data collection and curation phases, and it can be hard to notice or detect
    it as the training datasets can come from multiple sources, can be large, and
    also as the attacker can infiltrate within data distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm poisoning** happens when an attacker meddles with the algorithm
    used to train the model. It can be as simple as infiltrating hyperparameters or
    fiddling with the architecture of the algorithm. For example, let''s take federated
    learning (which aims to preserve the privacy of individuals'' data) where model
    training is done on multiple subsets of private data (such as healthcare data
    from multiple hospitals while preserving patients'' confidential information).
    Multiple models are derived from each subset and then combined to form a final
    model. During this, an attacker can manipulate any subset of the data and influence
    the final resulting model. The attacker can also create a fake model from fake
    data and concatenate it with models produced from training on multiple subsets
    of private data to produce a final model that deviates from performing the task
    efficiently, or serves the attacker''s motives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model poisoning** occurs when an attacker replaces a deployed model with
    an alternative model. This kind of attack is identical to a typical cyber-attack
    where the electronic files containing the model could be modified or replaced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input attack and evasion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An input or evasion attack happens when the attacker modifies input to the ML
    system in such a manner that it causes the system to malfunction (or give a wrong
    prediction). These perturbations or changes can be hard to detect as these changes
    are very subtle or small.
  prefs: []
  type: TYPE_NORMAL
- en: For example, input attacks are popular for computer vision algorithms. This
    can be done by just changing a few pixels in the input image. As a result, the
    system might identify an image in a way that it is not supposed to or make a wrong
    prediction. Such small changes can effectively manipulate the prediction resulting
    in wrong actions being taken by the system. As a result, the ML system behaves
    as it should, while the output is manipulated.
  prefs: []
  type: TYPE_NORMAL
- en: ML systems are highly prone to input attacks. Hence, having an anomaly detector
    to monitor incoming data can be quite handy to avoid such perturbations in the
    incoming data. Irrespective of input data, the majority of classification models
    choose a valid class from their training. Another way of protecting the ML system
    is by preprocessing the inputs with a proxy binary model that tells you, for example,
    whether an input image is of a person or an animal before sending this image to
    the final image classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a user of an AI system, it can be a black box or opaque. It is common in
    AI systems to accept inputs to generate outputs without revealing what is going
    on inside (in terms of both the logic and algorithm). Training datasets, which
    effectively contain all the trained system's knowledge, are also usually kept
    confidential. This, in theory, makes it impossible for an outsider to predict
    why particular outputs are produced or what is going on inside the AI system in
    terms of the algorithm, training data, or logic. However, in some cases, these
    systems can be prone to reverse engineering. The attacker or hackers' goal in
    reverse engineering attacks is to replicate the original model deployed as a service
    and use it to their advantage.
  prefs: []
  type: TYPE_NORMAL
- en: In a paper titled *Model Extraction Attacks against Recurrent Neural Networks*
    ([https://arxiv.org/pdf/2002.00123.pdf](https://arxiv.org/pdf/2002.00123.pdf)),
    published in February 2020, researchers conducted experiments on model extraction
    attacks against an RNN and an LSTM trained with publicly available academic datasets.
    The researchers effectively reproduce the functionality of an ML system via a
    model extraction attack. They demonstrate that a model extraction attack with
    high accuracy can be extracted efficiently, primarily by replicating or configuring
    a loss function or architecture from the target model.
  prefs: []
  type: TYPE_NORMAL
- en: In another instance, researchers from the Max Planck Institute for Informatics
    showed in 2018 how they were able to infer information from opaque models by using
    a sequence of input-output queries.
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In backdoor attacks, the attacks can embed patterns of their choice in the model
    in the training or inference stages and infer the deployed model using pre-curated
    inputs to produce unexpected outputs or triggers to the ML system. Therefore,
    backdoor attacks can happen both in the training and inference phases, whereas
    evasion and poisoning attacks can occur in a single phase during training or inference.
  prefs: []
  type: TYPE_NORMAL
- en: Poison attacks can be used as part of the attack in backdoor attacks, and in
    some instances, the student model can learn to hack some backdoors from the teacher
    model using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor attacks can cause integrity challenges, especially in the training
    stage, if the attacker manages to use a poison attack to infiltrate training data
    and trigger an update to the model or system. Also, backdoor attacks can be aimed
    to degrade performance, exhaust or redirect resources that can lead to the system's
    failure, or attempt to introduce peculiar behavior and outputs from the AI system.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned the key principles of testing and security
    by design. We explored the various methods to test ML solutions in order to secure
    them. For a comprehensive understanding and hands-on experience, implementation
    was done to load test our previously deployed ML model (from [*Chapter 7*](B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143),
    *Building Robust CI and CD Pipelines*) to predict the weather. With this, you
    are ready to handle the diverse testing and security scenarios that will be channeled
    your way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the secrets of deploying and maintaining
    robust ML services in production. This will enable you to deploy robust ML solutions
    in production. Let's delve into it.
  prefs: []
  type: TYPE_NORMAL
