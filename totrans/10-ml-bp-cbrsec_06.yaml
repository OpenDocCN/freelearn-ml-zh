- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Deepfakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent times, the problem of deepfakes has become prevalent on the internet.
    Easily accessible technology allows attackers to create images of people who have
    never existed, through the magic of deep neural networks! These images can be
    used to enhance fraudulent or bot accounts to provide an illusion of being a real
    person. As if deepfake images were not enough, deepfake videos are just as easy
    to create. These videos allow attackers to either morph someone’s face onto a
    different person in an existing video, or craft a video clip in which a person
    says something. Deepfakes are a hot research topic and have far-reaching impacts.
    Abuse of deepfake technology can result in misinformation, identity theft, sexual
    harassment, and even political crises.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on machine learning methods to detect deepfakes. First,
    we will understand the theory behind deepfakes, how they are created, and what
    their impact can be. We will then cover two approaches to detecting deepfake images
    – vanilla approaches using standard machine learning models, followed by some
    advanced methods. Finally, as deepfake videos are major drivers of misinformation
    and have the most scope for exploitation, the last part of the chapter will focus
    on detecting deepfake videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: All about deepfakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting fake images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting fake videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have an in-depth understanding of deepfakes,
    the technology behind them, and how they can be detected.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205).
  prefs: []
  type: TYPE_NORMAL
- en: All about deepfakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word *deepfake* is a combination of two words – *deep learning* and *fake*.
    Put simply, deepfakes are fake media created using deep learning technology. In
    the past decade, there have been significant advances in machine learning and
    generative models – models that create content instead of merely classifying it.
    These models (such as **Generative Adversarial Networks** (**GANs**)) can synthesize
    images that look real – even of human faces!
  prefs: []
  type: TYPE_NORMAL
- en: Deepfake technology is readily accessible to attackers and malicious actors
    today. It requires no sophistication or technical skills. As an experiment, head
    over to the website [thispersondoesnotexist.com](http://thispersondoesnotexist.com).
    This website allows you to generate images of people – people who have never existed!
  prefs: []
  type: TYPE_NORMAL
- en: For example, the people in the following figure are not real. They are deepfakes
    that have been generated by [thispersondoesnotexist.com](http://thispersondoesnotexist.com),
    and it only took a few seconds!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Deepfakes generated from a website](img/B19327_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Deepfakes generated from a website
  prefs: []
  type: TYPE_NORMAL
- en: Amazing, isn’t it? Let us now understand what makes generating these images
    possible, and the role machine learning has to play in it.
  prefs: []
  type: TYPE_NORMAL
- en: A foray into GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us now look at GANs, the technology that makes deepfakes possible.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are deep learning models that use neural networks to synthesize data rather
    than merely classify it. The name *adversarial* comes from the architectural design
    of these models; a GAN architecture consists of two neural networks, and we can
    force them into a cat-and-mouse game to generate synthetic media that can be passed
    off as real.
  prefs: []
  type: TYPE_NORMAL
- en: GAN architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A GAN consists of two main parts – the generator and the discriminator. Both
    models are deep neural networks. Synthetic images are generated by plotting these
    networks against each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generator**: The generator is the model that learns to generate real-looking
    data. It takes a fixed-length random vector as input and generates an output in
    the target domain, such as an image. The random vector is sampled randomly from
    a Gaussian distribution (that is, a standard normal distribution, where most observations
    cluster around the mean, and the further away an observation is from the mean,
    the lower the probability of it occurring is).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discriminator**: The discriminator is a traditional machine learning model.
    It takes in data samples and classifies them as real or fake. Positive examples
    (those labeled as real) come from a training dataset, and negative examples (those
    labeled as fake) come from the generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAN working
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generating images is an unsupervised machine learning task and classifying the
    images is a supervised one. By training both the generator and discriminator jointly,
    we refine both and are able to obtain a generator that can generate samples good
    enough to fool the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: In joint training, the generator first generates a batch of sample data. These
    are treated as negative samples and augmented with images from a training dataset
    as positive samples. Together, these are used to fine-tune the discriminator.
    The discriminator model is updated to change parameters based on this data. Additionally,
    the discriminator loss (i.e., how well the generated images fooled the discriminator)
    is fed back to the generator. This loss is used to update the generator to generate
    better data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – How a GAN model works](img/B19327_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – How a GAN model works
  prefs: []
  type: TYPE_NORMAL
- en: At first, the generator produces random data that is clearly noisy and of poor
    quality. The discriminator learns to easily discern between real and fake examples.
    As the training progresses, the generator starts to get better as it leverages
    the signal from the discriminator, changing its generation parameters accordingly.
    In the end, in an ideal situation, the generator will have been so well trained
    that the generator loss decreases and the discriminator loss begins increasing,
    indicating that the discriminator can no longer effectively distinguish between
    actual data and generated data.
  prefs: []
  type: TYPE_NORMAL
- en: How are deepfakes created?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along with being a portmanteau of the words *deep learning* and *fake*, the
    word *deepfake* has another origin. The very first deepfake video was created
    by a Reddit user by the name of `r/deepfakes`. This user used the open source
    implementation provided by Google to swap the face of several actresses into pornographic
    videos. Much of the amateur deepfakes in the wild today build upon this code to
    generate deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, any deepfake creation entails the following four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the source image, identifying the area where the face is located,
    and cropping the image to that area.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing features that are typically representations of the cropped image in
    a latent low-dimensional space, thus encoding the image into a feature vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modifying the generated feature vector based on certain signals, such as the
    destination image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decoding the modified vector and blending the image into the destination frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most deepfake generation methods rely on neural networks and, in particular,
    the encoder-decoder architecture. The encoder transforms an image into a lower
    dimensional subspace and maps it to a latent vector (similar to the context vectors
    we described when discussing transformers). This latent vector captures features
    about the person in the picture, such as color, expression, facial structure,
    and body posture. The decoder does the reverse mapping and converts the latent
    representation into the target image. Adding a GAN into the mix leads to a much
    more robust and powerful deepfake generator.
  prefs: []
  type: TYPE_NORMAL
- en: The first commercial application of deepfakes started with the development of
    FakeApp in January 2018\. This is a desktop application that allows users to create
    videos with faces swapped for other faces. It is based on autoencoder architecture
    and consists of two encoder-decoder pairs. One encoder-decoder pair is trained
    on the images of the source image, and the other is trained on the images of the
    target. However, both encoders have shared common weights; in simpler terms, the
    same encoder is used in both autoencoders. This means that the latent vector representation
    for both images is in the same context, hence representing similar features. At
    the end of the training period, the common encoder will have learned to identify
    common features in both faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say A is the source image (the original image of our victim) and B is
    the target image (the one where we want to insert A). The high-level process to
    generate deepfakes using this methodology is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain multiple images of A and B using data augmentation techniques and image
    transformations so that the same picture from multiple viewpoints is considered.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the first autoencoder model on images of A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the encoder from the first autoencoder. Use this to train a second autoencoder
    model on images of B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of training, we have two autoencoders with a shared encoder that
    can identify common features and characteristics in the images of A and B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To produce the deepfake, pass image A through the common encoder and obtain
    the latent representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the decoder from the second autoencoder (i.e., the decoder for images of
    B) to decode this back into the deepfake image. The following shows a diagram
    of this process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Deepfake creation methodology](img/B19327_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Deepfake creation methodology
  prefs: []
  type: TYPE_NORMAL
- en: FakeApp has been widely used and inspired many other open source implementations,
    such as DeepFaceLab, FaceSwap, and Dfaker.
  prefs: []
  type: TYPE_NORMAL
- en: The social impact of deepfakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While deepfake technology is certainly revolutionary in the field of machine
    learning and deep learning in particular, it has a far-reaching societal impact.
    Since their inception, deepfakes have been used for both benign and malicious
    purposes. We will now discuss both briefly. Understanding the full impact of deepfakes
    is essential to their study, especially for machine learning practitioners in
    the cybersecurity industry.
  prefs: []
  type: TYPE_NORMAL
- en: Benign
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Not all deepfakes are bad. There are some very good reasons why the use of
    deepfakes may be warranted and, at times, beneficial. Deepfakes have shown to
    be of great utility in some fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entertainment**: Deepfake technology is now accessible to everyone. Powerful
    pre-trained models allow the exposure of endpoints through apps on smartphones,
    where they have been widely used to create entertaining videos. Such deepfakes
    include comedy videos where cartoons say certain dialogue, images where the faces
    of two people are swapped, or filters that generate human faces morphed into animals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resurrection**: An innovative use case of deepfakes has been reviving the
    deceased. Deepfakes can portray deceased people saying certain things, in their
    own voice! This is like magic, especially for someone who has no videographic
    memories of themselves left in this world. Deepfakes have also been used to create
    images that are able to portray how someone would have looked in a few years''
    or decades'' time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malicious
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unfortunately, however, every coin has two sides. The same deepfake technology
    that can power entertainment and enable resurrected digital personas can be used
    maliciously by attackers. Here are a few attack vectors that practitioners in
    this space should be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Misinformation**: This has probably been the biggest use of deepfakes and
    the most challenging one to solve. Because deepfakes allow us to create videos
    of someone saying things they did not, this has been used to create videos that
    spread fake news. For example, a video featuring a surgeon general saying that
    vaccines are harmful and cause autism would certainly provoke widespread fear
    and lead people to believe that it is true. Malicious entities can create and
    disseminate such deepfakes to further their own causes. This can also lead to
    political crises and loss of life. In 2022, a deepfake video featuring the Ukrainian
    president Volodymyr Zelenskyy was created by Russian groups, where the former
    was shown to accept defeat and ask soldiers to stand down – this was not the case,
    but the video spread like wildfire on social media before it was removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud**: Traditionally, many applications depended on visual identification
    verification. After the COVID-19 pandemic, most of these transitioned to verifying
    identities through documents and video online. Deepfakes have tremendous potential
    to be used for identity theft here; by crafting a deepfake video, you can pretend
    to be someone you are not and bypass automatic identity verification systems.
    In June 2022, the **Federal Bureau of Investigation** (**FBI**) published a public
    service announcement warning companies of deepfakes being used in online interviews
    and during remote work. Deepfakes can also be used to create sock puppet accounts
    on social media websites, GANs can be used to generate images, and deepfake videos
    can be used to enrich profiles with media (videos showing the person talking)
    so that they appear real.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pornography**: The very first deepfake that was created was a pornographic
    movie. Revenge porn is an alarmingly growing application of deepfakes in today’s
    world. By using images of a person and any pornographic clip as a base, it is
    possible to depict the said person in the pornographic clip. The naked eye may
    not be able to ascertain that the video is a spoof.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting fake images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at how deepfake images and videos can be
    generated. As the technology to do so is accessible to everyone, we also discussed
    the impact that this can have at multiple levels. Now, we will look at how fake
    images can be detected. This is an important problem to solve and has far-reaching
    impacts on social media and the internet in general.
  prefs: []
  type: TYPE_NORMAL
- en: A naive model to detect fake images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that machine learning has driven significant progress in the domain
    of image processing. Convolutional neural networks (CNNs) have surpassed prior
    image detectors and achieved accuracy even greater than that of humans. As a first
    step toward detecting deepfake images, we will treat the task as a simple binary
    classification and use standard deep learning image classification approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several publicly available datasets for deepfake detection. We will
    use the 140k Real and Fake Faces Dataset. This dataset is freely available to
    download from Kaggle ([https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces](https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces)).
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the dataset consists of 140,000 images. Half of these
    are real faces from Flickr, and the remaining half are fake ones generated from
    a GAN. The real faces have been collected by researchers from NVIDIA, and there
    is significant coverage of multiple ethnicities, age groups, and accessories in
    the images.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is fairly large (~4 GB). You will need to download the compressed
    dataset from Kaggle, unzip it, and store it locally. The root directory consists
    of three folders – one each for training, testing, and validation data.
  prefs: []
  type: TYPE_NORMAL
- en: The model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are going to use a CNN model for classification. These neural networks specialize
    in understanding and classifying data in which the spatial representation of data
    matters. This makes it ideal for processing images, as images are fundamentally
    matrices of pixels arranged in a grid-like topology. A CNN typically has three
    main layers – convolution, pooling, and classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolution layer**: This is the fundamental building block of a CNN. This
    layer traverses through an image and generates a matrix known as an **activation
    map**. Each convolution multiplies some part of the image with a kernel matrix
    (the weights in the matrix are parameters that can be learned using gradient descent
    and backpropagation). The convolution matrix slides across the image row by row
    and performs the dot product. The convolution layer aggregates multiple neighborhoods
    of the image and produces a condensed output, as shown in *Figure 5**.4*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – The convolution layer](img/B19327_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – The convolution layer
  prefs: []
  type: TYPE_NORMAL
- en: '**Pooling layer**: The pooling layer performs aggregations over the convolution
    output. It calculates a summary statistic over a neighborhood and replaces the
    neighborhood cells with the summary. The statistic can be the mean, max, median,
    or any other standard metric. Pooling reduces redundancy in the convolutional
    representation and reduces dimensions by summarizing over multiple elements, as
    shown in *Figure 5**.5*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – The pooling layer](img/B19327_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – The pooling layer
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully connected layer**: The output from the pooling layer is flattened and
    converted into a one-dimensional vector. This is done simply by appending the
    rows to each other. This vector is now passed to a fully connected neural network,
    at the end of which is a **softmax layer**. This layer operates as a standard
    neural network, with weights being updated with gradient descent and backpropagation
    through multiple epochs. The softmax layer will output a probability distribution
    of the predicted class of the image, as shown in *Figure 5**.6*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Flattening and a fully connected layer](img/B19327_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Flattening and a fully connected layer
  prefs: []
  type: TYPE_NORMAL
- en: While training, data flows into the network and undergoes convolution and pooling.
    We can stack multiple convolution-pooling layers one after the other; the hope
    is that each layer will learn something new from the image and obtain more and
    more specific representations. The pooling output after the last convolution layer
    flows into the fully connected neural network (which can also consist of multiple
    layers). After the softmax output, a loss is calculated, which then flows back
    through the whole network. All layers, including the convolution layers, update
    their weights, using gradient descent to minimize this loss.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now use a CNN model to detect deepfakes and run this experiment on the
    140k dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that the first step in the data science pipeline is data preprocessing.
    We need to parse the images we downloaded and convert them into a form suitable
    for consumption by the CNN model. Fortunately, the `ImageDataGenerator` class
    in the `keras` library helps us do just that. We will use this library and define
    our training and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After you run this, you should see an output somewhat like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is preprocessed, we can define the actual model and specify
    the architecture of the CNN. Our model will consist of convolution, pooling, and
    the fully connected layer, as described earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let us take a closer look at what we did here. First, we set some basic model
    parameters and defined an empty model using the `Sequential` class. This model
    will be our base, where we pile on different layers to build the whole architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we defined our **convolution layers**. Each convolution layer has a **max
    pooling** **layer**, followed by a **normalization layer**. The normalization
    layer simply normalizes and rescales the pooled output. This results in stable
    gradients and avoids exploding loss.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we added an aggregation layer using the `GlobalAveragePooling2D` class.
    This will concatenate the previous output into a one-dimensional vector. Finally,
    we have a fully connected layer at the end with a softmax activation; this layer
    is responsible for the actual classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can take a look at your model architecture by printing the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train the model we defined on the data we preprocessed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will take about 30 minutes to an hour to complete. Of course, the exact
    time will depend on your processor, scheduling, and system usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, we can use it to make predictions for our test data
    and compare the predictions with the ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can plot the confusion matrix, just like we did for the other models
    in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Playing around with the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we looked at how an end-to-end CNN model can be defined
    and how to train it. While doing so, we made several design choices that can potentially
    affect the performance of our model. As an experiment, you should explore these
    design choices and rerun the experiment with different parameters. We will not
    go through the analysis and hyperparameter tuning in detail, but we will discuss
    some of the parameters that can be tweaked. We will leave it up to you to test
    it out, as it is a good learning exercise. Here are some things that you could
    try out to play around with the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layers**: In our model, we have three layers of convolution
    and max pooling. However, you can extend this to as many (or as few) layers as
    you want. What happens if you have 10 layers? What happens if you have only one?
    How does the resulting confusion matrix change?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling layers**: We used max pooling in our model. However, as discussed
    in the CNN architecture, pooling can leverage any statistical aggregation. What
    happens if we use mean pooling or min pooling instead of max pooling? How does
    the resulting confusion matrix change?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully connected layers**: Our model has just one fully connected layer at
    the end. However, this does not have to be the case; you can have a full-fledged
    neural network with multiple layers. You should examine what happens if the number
    of layers is increased.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: Note that each layer is followed by a dropout layer. Dropout is
    a technique used in neural networks to avoid overfitting. A certain fraction of
    the weights is randomly set to 0; these are considered to be “dropped out.” Here,
    we have a dropout fraction of 0.1\. What happens if it is increased? What happens
    if it is set to 0 (i.e., with no dropout at all)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes our discussion of deepfake image detection. Next, we will see
    how deepfakes transitioned from images to videos, and we will look at methods
    for detecting them.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting deepfake videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As if deepfake images were not enough, deepfake videos are now revolutionizing
    the internet. From benign uses such as comedy and entertainment to malicious uses
    such as pornography and political unrest, deepfake videos are taking social media
    by storm. Because deepfakes appear so realistic, simply looking at a video with
    the naked eye does not provide any clues as to whether it is real or fake. As
    a machine learning practitioner working in the security field, it is essential
    to know how to develop models and techniques to identify deepfake videos.
  prefs: []
  type: TYPE_NORMAL
- en: A video can be thought of as an extension of an image. A video is multiple images
    arranged one after the other and viewed in quick succession. Each such image is
    known as a frame. By viewing the frames at a high speed (multiple frames per second),
    we see images moving.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks cannot directly process videos – there does not exist an appropriate
    method to encode images and convert them into a form suitable for consumption
    by machine learning models. Therefore, deepfake video detection involves deepfake
    image detection on each frame of the video. We will look at the succession of
    frames, examine how they evolve, and determine whether the transformations and
    movements are normal or similar to those expected in real videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, detecting video deepfakes follows the following pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the video file and decomposing it into multiple frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reading frames up to a maximum number of frames. If the number of frames is
    less than the maximum set, pad them with empty frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting the faces in each frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cropping the faces in each frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training a model with a sequence of cropped faces (one face obtained per frame)
    as input and real/fake labels as output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will leverage these steps to detect fake videos in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building deepfake detectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at how to build models to detect deepfake videos.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use the dataset from the Kaggle Deepfake Detection Challenge ([https://www.kaggle.com/competitions/deepfake-detection-challenge/overview](https://www.kaggle.com/competitions/deepfake-detection-challenge/overview)).
    As part of the challenge, three datasets were provided – training, testing, and
    validation. With prizes worth $1 million, participants were required to submit
    code and output files, which were evaluated on a private test set.
  prefs: []
  type: TYPE_NORMAL
- en: The actual dataset is too large (around 0.5 TB) for us to feasibly download
    and process, given that most of you will have access to only limited compute power.
    We will use a subset of the data available in the `train_sample_videos.zip` and
    `test_videos.zip` files, which are around 4 GB altogether. You will need to download
    this data and save it locally so that you can run the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we developed our own CNN model and trained it end to
    end for classification. In this section, we will explore a different technique
    based on the concept of *transfer learning*. In machine learning, transfer learning
    is a paradigm where parameters learned from one task can be applied to another
    task and improve the performance of the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In transfer learning, we train a classification model on a base task. Once
    that model is trained, we can use it for another task in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training the model again on new data from the second task**: By doing so,
    we will initialize the model weights as the ones produced after training on the
    first task. The hope is that the model (which will effectively be trained on both
    sets of data) will show a good performance on both tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using the model as a feature extractor**: The final layer of the model is
    typically a softmax layer. We can extract the pre-final layer and use the model
    without softmax to generate features for our new data. These features can be used
    to train a downstream classification model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the second task is generally a refined and specific version of the
    first task. For example, the first task can be sentiment classification, and the
    second task can be movie review classification. The hope is that training on the
    first task will help the model learn high-level signals for sentiment classifications
    (keywords indicating certain sentiments, such as `excellent`, `amazing`, `good`,
    and `terrible`). Fine-tuning in the second task will help it learn task-specific
    knowledge in addition to the high-level knowledge (movie-specific keywords such
    as `blockbuster` and `flop-show`). Another example is that the base task is image
    classification on animals, and the fine-tuning task is a classifier for cat and
    dog images.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the second approach listed here. Instead of developing custom features,
    we will let a pre-trained model do all the work. The model we will use is the
    **InceptionV3 model**, which will extract features for every frame in the video.
    We will use a **Recurrent Neural Network** (**RNN**) to classify the video, based
    on the sequence of frames. This technique has been adapted from a submission to
    the challenge ([https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook](https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook)).
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then read the video file and label for each video into a DataFrame.
    The dataset contains a metadata file for each set that provides us with this information.
    Note that you may have to adjust the paths depending on where and how you stored
    the data locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at one of the DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It should show you the list of video files along with their associated label
    (real or fake). We will now define a few helper functions. The first one is a
    cropping function that will take in a frame and crop it into a square:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The second helper function will parse the video file and extract frames from
    it up to a specified maximum number of frames. We will resize each frame as (`224,224`)
    for standardization and then crop it into a square. The output will be a sequence
    of uniformly sized and cropped frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define our feature extractor, which uses transfer learning and
    leverages the `InceptionV3` model to extract features from a frame. Fortunately,
    the `keras` library provides a convenient interface for us to load pre-trained
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will write another function that parses the entire dataset, generates
    a sequence of frames, extracts features from each frame, and generates a tuple
    of the feature sequence and the label (real or fake) for the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the model, we need to split the available data into training and test
    sets (note that the test set provided does not come with labels, so we will not
    be able to evaluate our model on it). We will use our preprocessing function to
    prepare the data and convert it into the required form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define a model and use this data to train it. Here, we will use
    the pre-trained model only as a feature extractor. The extracted features will
    be passed to a downstream model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have used `8` neurons. The last layer contains a single neuron with
    a sigmoid activation. You can examine the architecture by printing it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that, here, we don’t use softmax but sigmoid instead. Why do we do this?
    What we have here is a binary classification problem; the sigmoid will give us
    the probability that the sequence of frames is a deepfake. If we had a multi-class
    classification, we would have used softmax instead. Remember that softmax is just
    sigmoid generalized to multiple classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we train the model. As we use Keras, training is as simple as calling
    the `fit()` function on the model and passing it the required data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You should be able to observe the training of the model through each epoch.
    At every epoch, you can see the loss of the model both over the training data
    and validation data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have the trained model, we can run the test examples through it and
    obtain the model prediction. We will obtain the output probability, and if it
    is greater than our threshold (generally, `0.5`), we classify it as fake (label
    = `1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To examine the performance of our model, we can compare the actual and predicted
    labels by generating a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that in the last section on detecting deepfake images, we included pointers
    to playing around with the model and tracking performance. We will do the same
    here for deepfake video classification as well. While such a task may seem redundant,
    given that we already have a model, in the real world and industry, model tuning
    is an essential task that data scientists have to handle.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a learning exercise, you should experiment with the model and determine
    what the best set of parameters you can have is. Here are some of the things you
    can experiment with:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image processing**: Note that we parsed videos into frames and resized each
    image to a 224 x 224 shape. What happens if this shape is changed? How do extremely
    large sizes (1,048 x 1,048) or extremely small sizes (10 x 10) affect the model?
    Is the performance affected?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data parameters**: In the preceding walkthrough, while preprocessing the
    data, we set two important parameters. The first is the *maximum sequence length*
    that determines how many frames we consider. The second is the *number of features*
    that controls the length of the feature vector extracted from the pre-trained
    model. How is the model affected if we change these parameters – both individually
    and in conjunction with one another?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: We used the paradigms of transfer learning and a pre-trained
    model to process our input and give us our features. InceptionV3 is one model;
    however, there are several others that can be used. Do any other models result
    in better features (as evidenced by better model performance)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model architecture**: The number of GRU layers, fully connected layers, and
    the neurons in each layer were chosen arbitrarily. What happens if, instead of
    32-16-8 GRU layers, we choose 16-8-8? How about 8-8-8? How about 5 layers, each
    with 16 neurons (i.e., 16-16-16-16-16)? Similar experiments can be done with the
    fully connected layers as well. The possibilities here are endless.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: We trained our model over 10 epochs. What happens if we train
    for only 1 epoch? How about 50? 100? Intuitively, you would expect that more epochs
    leads to better performance, but is that really the case? Additionally, our prediction
    threshold is set to 0.5\. How are the precision and recall affected if we vary
    this threshold?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This brings us to the end of our discussion on deepfake videos.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied deepfakes, which are synthetic media (images and
    videos) that are created using deep neural networks. These media often show people
    in positions that they have not been in and can be used for several nefarious
    purposes, including misinformation, fraud, and pornography. The impact can be
    catastrophic; deepfakes can cause political crises and wars, cause widespread
    panic among the public, facilitate identity theft, and cause defamation and loss
    of life. After understanding how deepfakes are created, we focused on detecting
    them. First, we used CNNs to detect deepfake images. Then, we developed a model
    that parsed deepfake videos into frames and used transfer learning to convert
    them into vectors, the sequence of which was used for fake or real classification.
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes are a growing challenge and have tremendous potential for cybercrime.
    There is a strong demand in the industry for professionals who understand deepfakes,
    their generation, the social impact they can have, and most importantly, methods
    to counter them. This chapter provided you with a deep understanding of deepfakes
    and equips you with the tools and technology needed to detect them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the text counterpart of deepfake images
    – machine-generated text.
  prefs: []
  type: TYPE_NORMAL
