- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Bias, Explainability, Privacy, and Adversarial Attacks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差、可解释性、隐私和对抗攻击
- en: In the previous chapter, we explored the topic of AI risk management framework
    and discussed its importance in mitigating the risks associated with AI systems.
    We covered the core concepts of what it is, the importance of identifying and
    assessing risks, and recommendations for managing those risks. In this chapter,
    we will take a more in-depth look at several specific risk topics and technical
    techniques for mitigations. We will explore the essential areas of **bias**, **explainability**,
    **privacy**, and **adversarial attacks**, and how they relate to AI systems. These
    are some of the most pertinent areas in responsible AI practices, and it is important
    for ML practitioners to develop a foundational understanding of these topics and
    the technical solutions. Specifically, we will examine how bias can lead to unfair
    and discriminatory outcomes, and how explainability can enhance the transparency
    and accountability of AI systems. We will also discuss the criticality of privacy
    in AI systems, as well as the potential risks of adversarial attacks and how to
    mitigate them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了人工智能风险管理框架的主题，并讨论了其在减轻与人工智能系统相关的风险方面的重要性。我们涵盖了其核心概念、识别和评估风险的重要性以及管理这些风险的建议。在本章中，我们将更深入地探讨几个具体的风险主题和缓解技术。我们将探讨**偏差**、**可解释性**、**隐私**和**对抗攻击**的基本领域，以及它们与人工智能系统的关系。这些是负责任的人工智能实践中最相关的领域之一，对于机器学习从业者来说，了解这些主题及其技术解决方案的基础知识非常重要。具体来说，我们将研究偏差如何导致不公平和歧视性的结果，以及可解释性如何增强人工智能系统的透明度和问责制。我们还将讨论人工智能系统中隐私的重要性，以及对抗攻击的潜在风险及其缓解方法。
- en: 'To sum up, the following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章将涵盖以下主题：
- en: What is bias?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是偏差？
- en: What is explainability?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是可解释性？
- en: Understanding security and privacy-preserving ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解安全和隐私保护机器学习
- en: Understanding adversarial attacks and how to defend against them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解对抗攻击及其防御方法
- en: Hands-on lab – detecting bias, explaining models, training privacy-preserving
    mode, and simulating adversarial attack
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践实验室 - 检测偏差、解释模型、训练隐私保护模式以及模拟对抗攻击
- en: Understanding bias
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解偏差
- en: Detecting and mitigating bias is a crucial focus area for AI risk management.
    The presence of bias in ML models can expose an organization to potential legal
    risks but also lead to negative publicity, causing reputational damage and public
    relations issues. Specific laws and regulations, such as the *Equal Credit Opportunity
    Act*, also prohibit discrimination in business transactions, like credit transactions,
    based on race, skin color, religion, sex, nationality origin, marital status,
    and age. Some other examples of laws against discrimination include the *Civil
    Rights Act of 1964* and *Age Discrimination in Employment Act of 1967*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和缓解偏差是人工智能风险管理的关键关注领域。机器学习模型中偏差的存在可能使组织面临潜在的法律风险，但也可能导致负面宣传，造成声誉损害和公共关系问题。一些具体的法律和法规，如*平等信贷机会法*，也禁止基于种族、肤色、宗教、性别、国籍起源、婚姻状况和年龄在商业交易（如信贷交易）中进行歧视。其他反对歧视的法律例子包括*1964年民权法*和*1967年就业年龄歧视法*。
- en: ML bias can result from the underlying prejudice in data. Since ML models are
    trained using data, if the data has a bias, then the trained model will also exhibit
    bias behaviors. For example, if you build an ML model to predict the loan default
    rate as part of the loan application review process, and you use race as one of
    the features in the training data, then the ML algorithm can potentially pick
    up race-related patterns and favor certain ethnic groups over others. Bias can
    be introduced in different stages of the ML lifecycle. For example, there could
    be data selection bias as certain groups might have stronger representation in
    the data collection stage. There could be labeling bias where a human makes an
    intentional or unintentional mistake in assigning labels to a dataset. Data sources
    with disinformation can also be a source of bias that results in biased AI solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习偏见可能源于数据中的潜在偏见。由于机器学习模型是使用数据训练的，如果数据存在偏见，那么训练好的模型也会表现出偏见行为。例如，如果您构建一个机器学习模型来预测贷款违约率作为贷款申请审查过程的一部分，并且您在训练数据中使用种族作为特征之一，那么机器学习算法可能会潜在地识别与种族相关的模式，并偏爱某些民族群体而忽视其他群体。偏见可以在机器学习生命周期的不同阶段引入。例如，可能存在数据选择偏见，因为某些群体可能在数据收集阶段有更强的代表性。还可能存在标签偏见，即人类在分配数据集标签时可能有意或无意地犯错。含有虚假信息的数据源也可能是偏见的一个来源，导致产生有偏见的AI解决方案。
- en: The ability to explain the decisions made by models helps an organization to
    satisfy compliance and audit requirements from the governance bodies. Furthermore,
    model explainability helps an organization understand the cause-and-effect relationships
    between the inputs and the ML prediction to make better business decisions. For
    example, if you can understand the reasons (such as rewards programs) behind strong
    customer interest in a financial product, you can adjust your business strategy,
    such as doubling down on rewards programs, to increase revenues. Being able to
    explain model decisions also helps establish trust with domain experts in the
    ML models. If domain experts agree with how the predictions are made by the models,
    they would be more likely to adopt the models for decision makings. There are
    various techniques for bias detection and model explainability, and we will take
    a closer look at some of the techniques next.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 解释模型所做的决策的能力有助于组织满足治理机构的要求和审计要求。此外，模型可解释性有助于组织理解输入与机器学习预测之间的因果关系，从而做出更好的商业决策。例如，如果您能理解客户对金融产品产生浓厚兴趣背后的原因（例如奖励计划），您可以通过加倍奖励计划等业务策略来调整您的业务策略，从而增加收入。能够解释模型决策还有助于与机器学习模型中的领域专家建立信任。如果领域专家同意模型预测的方式，他们更有可能采用模型进行决策。存在各种用于偏见检测和模型可解释性的技术，我们将在下一节中更详细地探讨一些技术。
- en: To detect and mitigate bias, some guiding principles need to be established
    on what is considered as fair. For example, a bank’s loan approval process should
    treat similar people similarly and the process may be considered fair when applicants
    with similar qualifications are assessed similarly. The bank also needs to ensure
    different demographic subgroups are treated equally for loan approval and measure
    metrics such as the rate for loan rejection to be approximately similar across
    different demographic subgroups.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测和减轻偏见，需要确立一些指导原则，以确定什么被认为是公平的。例如，银行的贷款审批流程应同等对待类似的人，当具有相似资格的申请人被同等评估时，该流程可能被认为是公平的。银行还需要确保不同的人口子群体在贷款审批方面受到平等对待，并测量诸如贷款拒绝率等指标在不同人口子群体之间的大致相似性。
- en: 'Depending on the definition of fairness, bias can be measured using different
    metrics. Some of the metrics might even contradict each other. Therefore, you
    need to choose the metrics that best support the definition of fairness with social
    and legal considerations and inputs from different demographic groups. In this
    section, we list some of the bias metrics for consideration:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公平的定义，可以使用不同的指标来衡量偏见。有些指标甚至可能相互矛盾。因此，您需要选择最能支持公平定义的指标，并考虑社会和法律因素以及来自不同人口群体的反馈。在本节中，我们列出了一些可供考虑的偏见指标：
- en: '**Class imbalance**: This metric measures the imbalanced representations of
    different demographic groups, especially disadvantaged groups, in a dataset.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别不平衡**：此指标衡量了不同人口群体（尤其是弱势群体）在数据集中的不平衡表示。'
- en: '**Difference in positive proportion in observed labels**: This metric measures
    the differences in positive labels across different demographic groups.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察标签中正比例的差异**: 此指标衡量不同人口群体中正标签的差异。'
- en: '**Kullback–Leibler (KL) divergence**: This metric compares the probability
    distribution in features and labels for the different groups, such as advantaged
    and disadvantaged groups.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kullback–Leibler (KL) 散度**: 此指标比较不同群体（如优势群体和劣势群体）的特征和标签中的概率分布。'
- en: '**Conditional demographic disparity in labels**: This metric measures whether
    a group has a bigger proportion of rejected outcomes than the proportion of accepted
    outcomes in the same group.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件标签的群体差异**: 此指标衡量一个群体被拒绝的结果比例是否大于该群体被接受的结果比例。'
- en: '**Recall difference**: This metric measures whether an ML model is finding
    more true positives for one group (advantaged group) than other groups (disadvantaged
    groups).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回差异**: 此指标衡量机器学习模型是否对一个群体（优势群体）比其他群体（劣势群体）找到更多的真正阳性。'
- en: 'There are several techniques that can potentially mitigate bias after it has
    been detected, although these techniques have inherent challenges and limitations.
    The following are some examples of approaches that may be employed:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测到偏差后，有几种技术可以潜在地减轻偏差，尽管这些技术具有固有的挑战和局限性。以下是一些可能采用的方法示例：
- en: '**Removal of features**: This approach can help mitigate bias by removing features
    that can contribute to the bias such as gender and age. However, there are also
    limitations and challenges with this approach, including proxy problems, meaning
    that removing sensitive features like gender or age may not entirely remove bias
    if other features in the data are correlated with the sensitive attributes. Furthermore,
    some relevant information may be lost by removing features, which could negatively
    impact the model’s performance or usefulness.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除特征**: 这种方法可以通过去除可能引起偏差的特征（如性别和年龄）来帮助减轻偏差。然而，这种方法也存在局限性和挑战，包括代理问题，即去除敏感特征（如性别或年龄）可能无法完全消除偏差，如果数据中的其他特征与敏感属性相关联。此外，去除特征可能会丢失一些相关信息，这可能会对模型的性能或有用性产生负面影响。'
- en: '**Rebalance of training data**: This approach helps correct bias in different
    numbers of representations for the different groups in the training data. However,
    rebalancing the training data may not be feasible or effective if the initial
    dataset is highly imbalanced or if the underrepresented groups have intrinsically
    different distributions. In addition, artificially rebalancing the data may introduce
    other biases or distortions, and it may not address underlying societal biases
    that are reflected in the data.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据的重新平衡**: 这种方法有助于纠正训练数据中不同群体表示数量上的偏差。然而，如果初始数据集高度不平衡或代表性不足的群体具有内在不同的分布，则重新平衡训练数据可能不可行或有效。此外，人为地重新平衡数据可能会引入其他偏差或扭曲，并且可能无法解决数据中反映的潜在社会偏差。'
- en: '**Adjust labels in the training data**: This approach brings the proportions
    of labels close together for the different subgroups. However, this approach assumes
    that the labels themselves are not biased, which may not always be the case, especially
    if the labels were assigned by humans who may have their own biases. Also, adjusting
    labels may be difficult or impossible in some domains, especially if the ground
    truth is unknown or if the labels are not subjective.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整训练数据中的标签**: 这种方法使不同子组的标签比例接近。然而，这种方法假设标签本身没有偏差，这并不总是成立，尤其是如果标签是由可能有自己的偏见的人类分配的。此外，在某些领域调整标签可能很困难或不可能，特别是如果基线事实未知或标签不是主观的。'
- en: There are other general challenges with bias mitigation, including the lack
    of ground truth; as in many real-world scenarios, it is difficult to determine
    the true unbiased ground truth, making it challenging to accurately measure and
    mitigate bias. Additionally, these approaches often focus on mitigating bias with
    respect to a single sensitive attribute, such as gender or race, but may not address
    intersectional biases that arise from the combination of multiple sensitive attributes.
    Furthermore, in some cases, mitigating bias may come at the cost of reduced model
    performance or accuracy, necessitating a balance between these competing objectives
    of fairness and performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在缓解偏差方面存在其他一些一般性挑战，包括缺乏真实基准；在许多现实场景中，很难确定真正的无偏差真实基准，这使得准确测量和缓解偏差变得具有挑战性。此外，这些方法通常专注于缓解与单个敏感属性（如性别或种族）相关的偏差，但可能无法解决由多个敏感属性组合产生的交叉偏差。此外，在某些情况下，缓解偏差可能会以降低模型性能或准确性的代价为代价，需要在这两个相互竞争的目标——公平性和性能之间取得平衡。
- en: It’s important to note that bias mitigation is an active area of research, and
    more advanced techniques are being developed to address some of the limitations
    and challenges. These include adversarial debiasing, a technique that uses an
    adversary model to predict sensitive attributes (e.g., gender, race) from the
    primary model’s internal representations or outputs. Another technique is causal
    modeling, which aims to ensure that an individual’s prediction or outcome should
    not change significantly if their sensitive attribute(s) were different, all else
    being equal. Additionally, a combination of approaches and careful monitoring
    and evaluation may be necessary to effectively mitigate bias in real-world applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，偏差缓解是一个活跃的研究领域，正在开发更先进的技术来解决一些局限性和挑战。这包括对抗性去偏差，这是一种使用对抗模型从主模型的内部表示或输出中预测敏感属性（例如，性别、种族）的技术。另一种技术是因果建模，旨在确保如果个人的敏感属性（s）不同，他们的预测或结果在所有其他条件相同的情况下不应有显著变化。此外，结合多种方法以及仔细的监控和评估可能是有效缓解现实应用中偏差的必要条件。
- en: 'There are a number of open-source libraries for fairness and bias management,
    such as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多开源库用于公平性和偏差管理，例如：
- en: Fairness ([https://github.com/algofairness/fairness-comparison](https://github.com/algofairness/fairness-comparison))
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性 ([https://github.com/algofairness/fairness-comparison](https://github.com/algofairness/fairness-comparison))
- en: Aequitas ([https://github.com/dssg/aequitas](https://github.com/dssg/aequitas))
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aequitas ([https://github.com/dssg/aequitas](https://github.com/dssg/aequitas))
- en: Themis ([https://github.com/LASER-UMASS/Themis](https://github.com/LASER-UMASS/Themis))
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Themis ([https://github.com/LASER-UMASS/Themis](https://github.com/LASER-UMASS/Themis))
- en: Responsibly ([https://github.com/ResponsiblyAI/responsibly](https://github.com/ResponsiblyAI/responsibly))
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任地 ([https://github.com/ResponsiblyAI/responsibly](https://github.com/ResponsiblyAI/responsibly))
- en: IBM AI Fairness 360 ([https://aif360.res.ibm.com/](https://aif360.res.ibm.com/))
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM AI Fairness 360 ([https://aif360.res.ibm.com/](https://aif360.res.ibm.com/))
- en: There is also a component in SageMaker for bias detection, which we will cover
    in greater detail in a later section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SageMaker 中还有一个用于偏差检测的组件，我们将在后面的章节中更详细地介绍。
- en: Understanding ML explainability
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习可解释性
- en: 'There are two main concepts when it comes to explaining the behaviors of an
    ML model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当解释机器学习模型的行为时，有两个主要概念：
- en: '**Global explainability**: This is the overall behavior of a model across all
    data points used for model training and/or prediction. This helps to understand
    collectively how different input features affect the outcome of model predictions.
    For example, after training an ML model for credit scoring, it is determined that
    income is the most important feature in predicting high credit scores across data
    points for all loan applicants.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局可解释性**：这是模型在所有用于模型训练和/或预测的数据点上的整体行为。这有助于理解不同输入特征如何共同影响模型预测的结果。例如，在训练用于信用评分的机器学习模型后，确定收入是预测所有借款申请人数据点上的高信用评分最重要的特征。'
- en: '**Local explainability**: This is the behavior of a model for a single data
    point (instance), and which features had the most influence on the prediction
    for a single data point. For example, when you try to explain which features influenced
    the decision the most for a single loan applicant, it might turn out that education
    was the most important feature, even though income was the most important feature
    at the global level.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地可解释性**：这是模型对单个数据点（实例）的行为，以及哪些特征对单个数据点的预测影响最大。例如，当您尝试解释哪些特征对单个贷款申请人的决策影响最大时，可能会发现教育是最重要的特征，尽管在全局层面上收入是最重要的特征。'
- en: Some ML algorithms such as linear regression and decision trees are considered
    explainable algorithms with a built-in ability to explain the model. For example,
    the coefficients of linear regression models directly represent the relative importance
    of different input features, and the split points in a decision tree represent
    the rules used for decision making.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法，如线性回归和决策树，被认为是可解释算法，具有内置的解释模型的能力。例如，线性回归模型的系数直接表示不同输入特征的相对重要性，而决策树中的分割点表示用于决策的规则。
- en: For black-box models such as neural networks, it is very hard to explain how
    the decisions are made in part due to non-linearity and model complexity. One
    technique for solving this is to use a white-box surrogate model to help explain
    the decisions of a black-box model. For example, you can train a linear regression
    model in parallel with a black-box neural network model using the same input data.
    While the linear regression model might not have the same performance as the black-box
    model, it can be used to explain at a high level how the decision was made. However,
    there are known limitations to the white-box surrogate model. Linear regression
    models, as mentioned in the example, may not be able to capture the complex non-linear
    relationships learned by neural networks, leading to an inaccurate representation
    of the decision-making process. Furthermore, while simple surrogate models, like
    linear regression, may provide a global approximation of the black-box model’s
    behavior, they may fail to capture local patterns or decision boundaries.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像神经网络这样的黑盒模型，解释决策是如何做出的非常困难，部分原因是非线性和模型复杂性。解决这一问题的方法之一是使用白盒代理模型来帮助解释黑盒模型的决策。例如，您可以使用与黑盒神经网络模型相同的输入数据并行训练一个线性回归模型。虽然线性回归模型可能没有黑盒模型那样的性能，但它可以用来从高层次上解释决策是如何做出的。然而，白盒代理模型存在已知的局限性。如示例中提到的，线性回归模型可能无法捕捉到神经网络学习到的复杂非线性关系，从而导致决策过程的错误表示。此外，虽然简单的代理模型，如线性回归，可能提供对黑盒模型行为的全局近似，但它们可能无法捕捉局部模式或决策边界。
- en: There are various open-source packages, such as **LIME** (which stands for **local
    interpretable model-agnostic explanations**), and **SHAP** (which stands for **SHapley
    Additive exPlanations**), for model explainability. Both LIME and SHAP adopt the
    surrogate model approach.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种开源包，如**LIME**（代表**本地可解释模型无关解释**），以及**SHAP**（代表**SHapley增量解释**），用于模型可解释性。LIME和SHAP都采用了代理模型方法。
- en: LIME
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LIME
- en: LIME supports local (instance) explainability, as the name suggests. The main
    idea behind LIME is to perturb the original data points (tweak the data points),
    feed them into the black-box model, and see the corresponding outputs. The perturbed
    data points are small changes to the original data point and are weighted based
    on their proximities to the original data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LIME支持本地（实例）可解释性，正如其名所示。LIME背后的主要思想是对原始数据点进行扰动（调整数据点），将其输入到黑盒模型中，并观察相应的输出。扰动数据点是原始数据点的微小变化，并根据它们与原始数据点的接近程度进行加权。
- en: It then fits a surrogate model, such as linear regression, using the perturbed
    data points and responses. Finally, the trained linear model is used to explain
    how the decision was made for the original data point.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它使用扰动数据点和响应拟合一个代理模型，例如线性回归。最后，训练好的线性模型被用来解释原始数据点的决策是如何做出的。
- en: 'LIME can be installed as a regular Python package and can be used to explain
    text classifiers, image classifiers, tabular classifiers, and regression models.
    The following are the explainers available in LIME:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LIME可以作为常规Python包安装，并可用于解释文本分类器、图像分类器、表格分类器和回归模型。以下是LIME中可用的解释器：
- en: '**Tabular data explainer**: `lime_tabular.LimeTabularExplainer()`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格数据解释器**：`lime_tabular.LimeTabularExplainer()`'
- en: '**Image data explainer**: `lime_image.LimeImageExplainer()`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像数据解释器**：`lime_image.LimeImageExplainer()`'
- en: '**Text data explainer**: `lime_text.LimeTextExplainer()`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本数据解释器**：`lime_text.LimeTextExplainer()`'
- en: LIME has certain limitations. Its explanations rely on perturbed samples generated
    around the instance of interest, and the quality of these explanations can be
    influenced by the sampling process. Different sampling techniques or perturbation
    functions may yield different explanations. While LIME can highlight the importance
    of individual features for a specific prediction, it may not offer a clear interpretation
    of how these features are combined or interact within the black-box model. The
    computational cost of generating LIME explanations can be high, particularly for
    high-dimensional data or complex models, as it necessitates creating and evaluating
    numerous perturbed samples for each instance of interest. LIME generates local
    explanations by approximating the behavior of the black-box model around the instance
    of interest using an interpretable model (e.g., linear regression). However, this
    local approximation may not accurately reflect the true behavior of the complex
    model, especially in regions with high non-linearities or discontinuities. Additionally,
    the linear surrogate might be inaccurate for local data points that defy approximation
    by a linear model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 存在某些局限性。其解释依赖于围绕感兴趣实例生成的扰动样本，而这些解释的质量可能会受到采样过程的影响。不同的采样技术或扰动函数可能会产生不同的解释。虽然
    LIME 可以突出显示单个特征对特定预测的重要性，但它可能无法清楚地解释这些特征如何在黑盒模型中组合或相互作用。生成 LIME 解释的计算成本可能很高，尤其是在高维数据或复杂模型中，因为它需要为每个感兴趣实例创建和评估大量的扰动样本。LIME
    通过使用可解释模型（例如线性回归）来近似黑盒模型围绕感兴趣实例的行为，从而生成局部解释。然而，这种局部近似可能无法准确反映复杂模型的真正行为，尤其是在具有高非线性或不连续性的区域。此外，对于无法由线性模型近似的局部数据点，线性代理可能是不准确的。
- en: Despite these limitations, LIME remains a popular and useful technique for generating
    local explanations, especially when combined with other interpretability methods
    or when used in conjunction with domain expertise.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，LIME 仍然是一种生成局部解释的流行且有用的技术，尤其是在与其他可解释性方法结合使用或与领域专业知识结合使用时。
- en: SHAP
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP
- en: SHAP is a more popular package, and it addresses some of the shortcomings of
    LIME. It computes the contribution of each feature to the prediction using the
    coalition game theory concept, where each feature value of each data instance
    is a player in the coalition.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 是一个更受欢迎的包，它解决了 LIME 的一些缺点。它使用联盟博弈论的概念来计算每个特征对预测的贡献，其中每个数据实例的特征值都是联盟中的一个玩家。
- en: The basic idea behind the coalition game theory is to form different permutations
    of coalitions of players when playing a game, then observe the game results from
    the different permutations, and finally calculate the contribution of each player.
    For example, if there are 3 features (*A, B,* and *C*) in the training dataset,
    then there will be 8 distinct coalitions (*2*^^*3*). We train one model for each
    distinct coalition for a total of 8 models. We use all 8 models to generate predictions
    on the dataset, figure out the marginal contribution of each feature, and assign
    a Shapley value to each feature to indicate the feature importance. For example,
    if the model that uses a coalition with only features *A* and *B* generates an
    output of *50*, and the model that uses features *A, B,* and *C* generates an
    output of *60*, then feature *C* has a marginal contribution of *10*. This is
    just a generalization of the concept; the actual calculation and assignments are
    more involved.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 联盟博弈论的基本思想是在玩游戏时形成玩家联盟的不同排列，然后观察不同排列的游戏结果，最后计算每个玩家的贡献。例如，如果训练数据集中有 3 个特征（*A,
    B* 和 *C*），那么将有 8 个不同的联盟（*2*^^*3*）。我们为每个不同的联盟训练一个模型，总共 8 个模型。我们使用所有 8 个模型在数据集上生成预测，找出每个特征的边际贡献，并为每个特征分配一个
    Shapley 值以指示特征的重要性。例如，如果仅使用特征 *A* 和 *B* 的联盟生成的输出为 *50*，而使用特征 *A, B* 和 *C* 的模型生成的输出为
    *60*，那么特征 *C* 的边际贡献为 *10*。这只是一个概念的一般化；实际的计算和分配更为复杂。
- en: 'SHAP can also be installed like a regular Python package. It can be used to
    explain tree ensemble models, natural language models (such as transformers),
    and deep learning models. It has the following main explainers:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 也可以像常规 Python 包一样安装。它可以用来解释树集成模型、自然语言模型（如变压器）和深度学习模型。它有以下主要解释器：
- en: '**TreeExplainer**: An implementation for computing SHAP values for trees and
    ensemble of trees algorithms'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TreeExplainer**: 用于计算树和树集成算法SHAP值的实现'
- en: '**DeepExplainer**: An implementation for computing SHAP values for deep learning
    models'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepExplainer**: 用于计算深度学习模型SHAP值的实现'
- en: '**GradientExplainer**: An implementation of expected gradients to approximate
    SHAP values for deep learning models'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GradientExplainer**: 用于近似深度学习模型SHAP值的预期梯度的实现'
- en: '**LinearExplainer**: For an explanation of linear models with independent features'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LinearExplainer**: 用于独立特征的线性模型的解释'
- en: '**KernelExplainer**: A model-agnostic method to estimate SHAP values for any
    model because it makes no assumptions about the model type'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KernelExplainer**: 一种模型无关的方法，用于估计任何模型的SHAP值，因为它不对模型类型做出任何假设'
- en: SHAP is widely considered the state-of-the-art model explainability algorithm,
    and it has been implemented in commercial offerings such as SageMaker. It can
    be used for both computing global feature importance as well as local explainability
    for a single instance. However, SHAP does come with certain limitations. Computing
    SHAP values, particularly for intricate models and high-dimensional data, can
    be computationally expensive and time consuming. This can pose challenges when
    applying SHAP to real-time or large-scale applications. SHAP values are computed
    based on the assumption that features are independent of each other. Nevertheless,
    in many real-world datasets, features may exhibit high correlations or complex
    interactions, violating this assumption and resulting in inaccurate or misleading
    explanations. Despite offering a numerical measure of feature importance, interpreting
    and conveying the significance of these values to non-technical stakeholders can
    be challenging, especially in complex domains.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP被广泛认为是最先进的模型可解释性算法，并且已经在SageMaker等商业产品中得到实现。它可以用于计算全局特征重要性，以及单个实例的局部可解释性。然而，SHAP确实存在某些局限性。计算SHAP值，尤其是对于复杂模型和高维数据，可能计算成本高昂且耗时。当将SHAP应用于实时或大规模应用时，这可能会带来挑战。SHAP值是基于假设特征之间相互独立来计算的。然而，在许多现实世界的数据集中，特征可能表现出高度相关性或复杂的相互作用，违反了这个假设，导致解释不准确或误导。尽管提供了特征重要性的数值度量，但解释和传达这些值的含义给非技术利益相关者可能具有挑战性，尤其是在复杂的领域。
- en: Understanding security and privacy-preserving ML
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解安全和隐私保护机器学习
- en: 'ML models often rely on vast amounts of data, including potentially sensitive
    information about individuals, such as personal details, financial records, medical
    histories, or browsing behavior. The improper handling or exposure of this data
    can lead to serious privacy breaches, putting individuals at risk of discrimination,
    identity theft, or other harmful consequences. To ensure compliance with data
    privacy regulations or even internal data privacy controls, ML systems need to
    provide foundational infrastructure security features such as data encryption,
    network isolation, compute isolation, and private connectivity. With a SageMaker-based
    ML platform, you can enable the following key security controls:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常依赖于大量数据，包括可能涉及个人敏感信息的数据，例如个人详细信息、财务记录、医疗历史或浏览行为。不当处理或泄露这些数据可能导致严重的隐私泄露，使个人面临歧视、身份盗窃或其他有害后果的风险。为确保遵守数据隐私法规或内部数据隐私控制，机器学习系统需要提供基础的基础设施安全功能，如数据加密、网络隔离、计算隔离和私有连接。使用基于SageMaker的机器学习平台，您可以启用以下关键安全控制：
- en: '**Private networking**: As SageMaker is a fully managed service, it runs in
    an AWS-owned account. By default, resources in your own AWS account communicate
    with SageMaker APIs via the public internet. To enable private connectivity to
    SageMaker components from your own AWS environment, you can attach them to a subnet
    in your own **virtual private cloud** (**VPC**).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**私有网络**: 由于SageMaker是一项完全托管的服务，它运行在AWS拥有的账户中。默认情况下，您自己的AWS账户中的资源通过公共互联网与SageMaker
    API进行通信。要使您自己的AWS环境中的SageMaker组件能够实现私有连接，您可以将它们附加到您自己的**虚拟私有云**（**VPC**）中的子网。'
- en: '**Storage encryption**: Data-at-rest encryption can be enabled by providing
    an encryption key when you create a SageMaker notebook, a training job, a processing
    job, or a hosting endpoint.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储加密**: 通过在创建SageMaker笔记本、训练作业、处理作业或托管端点时提供加密密钥，可以启用静态数据加密。'
- en: '**Disabling internet access**: By default, the SageMaker notebook, training
    job, and hosting service have access to the internet. The internet access can
    be disabled via configuration.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁用互联网访问**：默认情况下，SageMaker笔记本、训练作业和托管服务都可以访问互联网。可以通过配置禁用互联网访问。'
- en: 'In addition to infrastructure security, you also need to think about data privacy
    and model privacy to protect sensitive information from adversarial attacks, such
    as reverse engineering of sensitive data from anonymized data. There are three
    main techniques for data privacy protection for ML:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基础设施安全之外，你还需要考虑数据隐私和模型隐私，以保护敏感信息免受对抗性攻击，例如从匿名数据中逆向工程敏感数据。对于机器学习的数据隐私保护，有三种主要技术：
- en: '**Differential privacy**: Differential privacy allows the sharing of datasets
    while withholding information about individuals within the dataset. This method
    works by adding random noises into the computation so that it is hard to reverse
    engineer the original data (if it is not impossible). For example, you can add
    noises to the training data or model training gradients to obfuscate the sensitive
    data.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**差分隐私**：差分隐私允许在保留数据集中个人信息的隐私的情况下共享数据集。这种方法通过在计算中添加随机噪声来实现，使得难以逆向工程原始数据（尽管并非不可能）。例如，你可以在训练数据或模型训练梯度中添加噪声，以混淆敏感数据。'
- en: '**Homomorphic encryption (HE)**: HE is a form of encryption that allows users
    to perform computation on encrypted data without first decrypting it. This leaves
    the computation output in an encrypted form that when decrypted is equivalent
    to the output as if the computation was performed on the unencrypted data. With
    this approach, the data can be encrypted before it is used for model training.
    The training algorithm will train the model with the encrypted data, and the output
    can be decrypted only by the data owner with the secret key.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同态加密（HE）**：同态加密是一种加密形式，允许用户在未首先解密数据的情况下对加密数据进行计算。这种方法使得计算输出保持加密形式，当解密时，与在未加密数据上执行计算时的输出等效。采用这种方法，数据可以在用于模型训练之前进行加密。训练算法将使用加密数据训练模型，并且只有数据所有者使用密钥才能解密输出。'
- en: '**Federated learning**: Federated learning allows model training to take place
    in edge devices while keeping data locally on the device, instead of sending the
    data to a central training cluster. This protects individual data as it is not
    shared in a central location, while the global model can still benefit from individual
    data.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联邦学习**：联邦学习允许在边缘设备上进行模型训练，同时将数据保留在本地设备上，而不是将数据发送到中央训练集群。这保护了个人数据，因为它没有在中央位置共享，而全局模型仍然可以从个人数据中受益。'
- en: Each of these topics warrants its own separate book. So, we will not dive into
    the details of all three. Instead, we will only offer an introduction to differential
    privacy in this book to explain the main intuition and concept behind this method,
    as it is a technique that’s more established and widely studied.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些主题都值得一本单独的书。因此，我们不会深入探讨这三个主题的所有细节。相反，我们将在这本书中仅提供对差分隐私的介绍，以解释这一方法背后的主要直觉和概念，因为它是一种更成熟且广泛研究的技术。
- en: Differential privacy
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分隐私
- en: To understand the problem that differential privacy solves, let’s take a look
    at the real-world privacy breach that happened with Netflix. In 2006, Netflix
    provided 100 million movie ratings submitted by 480 K users as the data for the
    Netflix price competition. Netflix anonymized user names with unique subscribers’
    IDs in the dataset, thinking that this would protect subscribers’ identities.
    Just 16 days later, two university researchers were able to identify some subscribers’
    true identities by matching their reviews with data from IMDB. This type of attack
    is called a **linkage attack**, and this exposes the fact that anonymization is
    not enough to protect sensitive data. You can find more information about this
    at [https://en.wikipedia.org/wiki/Netflix_Prize](https://en.wikipedia.org/wiki/Netflix_Prize).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解差分隐私解决的问题，让我们看看Netflix发生的真实世界隐私泄露事件。2006年，Netflix提供了由480 K用户提交的1亿个电影评分作为Netflix价格竞赛的数据。Netflix在数据集中使用唯一的订阅者ID匿名化用户名，认为这样可以保护订阅者的身份。仅仅16天后，两名大学研究人员就能通过将他们的评论与IMDB的数据进行匹配来识别一些订阅者的真实身份。这种攻击被称为**链接攻击**，这暴露了匿名化不足以保护敏感数据的事实。你可以在[https://en.wikipedia.org/wiki/Netflix_Prize](https://en.wikipedia.org/wiki/Netflix_Prize)上找到更多关于此的信息。
- en: Differential privacy solves this problem by adding noises to the dataset used
    in the computation on the dataset, so the original data cannot be easily reverse
    engineered. In addition to protection against linkage attacks, differential privacy
    also helps quantify privacy loss as a result of someone running processing against
    the data. To help understand what this means, let’s look at the following example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私通过在数据集的计算过程中向数据集添加噪声来解决此问题，因此原始数据不能轻易被逆向工程。除了防止链接攻击外，差分隐私还有助于量化由于对数据进行处理而导致的隐私损失。为了帮助理解这意味着什么，让我们看看以下示例。
- en: Suppose your organization is a regional bank, and your customer data repository
    contains sensitive data about your customers, including their name, social security
    number, zip code, income, gender, and education. To ensure data privacy, this
    data cannot be freely shared by all departments, such as the marketing department.
    However, the aggregate analysis of the customer data, such as the number of customers
    with income over a threshold, is allowed to be shared. To enable access to the
    aggregated data, a data query tool was built to return only the aggregate data
    (such as count, sum, average, min, and max) to the marketing department. Separately,
    another database contains customer churn data with unique customer IDs, and a
    customer support database contains customer names and unique customer IDs. Both
    the churn database and customer support database are accessible to the marketing
    department. An ill-intentioned analyst wanted to find the names of customers whose
    incomes were above a certain threshold for some personal purpose.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的组织是一家地区性银行，您的客户数据存储库包含有关客户的敏感数据，包括他们的姓名、社会保险号、邮政编码、收入、性别和教育。为了确保数据隐私，这些数据不能被所有部门，如营销部门自由共享。然而，客户数据的汇总分析，例如超过某个阈值的客户数量，是被允许共享的。为了使营销部门能够访问汇总数据，构建了一个数据查询工具，仅向营销部门返回汇总数据（如计数、总和、平均值、最小值和最大值）。另外，还有一个包含具有唯一客户ID的客户流失数据库，以及一个包含客户姓名和唯一客户ID的客户支持数据库。流失数据库和客户支持数据库都可以被营销部门访问。一个有不良意图的分析员想要为了某些个人目的找到收入超过一定阈值的客户姓名。
- en: This analyst queried the database one day and found out that out of 4,000 total
    customers, there were 30 customers with incomes over $1 million in a particular
    zip code. A couple of days later, he queried the customer data again and found
    out there were only 29 customers with incomes over $1 million, out of a total
    of 3999 customers. Since he had access to the churn database and customer support
    database, he was able to identify the name of the customer who churned and figured
    out this customer had an income of over $1 million.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这位分析师有一天查询了数据库，发现总共4,000名客户中，在特定的邮政编码区域有30名客户的收入超过100万美元。几天后，他再次查询客户数据，发现只有29名客户的收入超过100万美元，总共有3,999名客户。由于他可以访问客户流失数据库和客户支持数据库，他能够识别出流失客户的姓名，并推断出这位客户的收入超过100万美元。
- en: To prevent this from happening, the query tool was changed to add a little noise
    (such as adding or removing records) to the result without losing meaningful information
    about the original data. For example, instead of returning the actual result of
    30 customers out of 4,000 customers in the first query, the result of 31 customers
    out of 4001 customers was returned. The second query returns 28 out of 3997 instead
    of the actual 29 out of 3999 figures. This added noise does not significantly
    change the overall magnitude of the summary result, but it makes reverse engineering
    of the original data much more difficult, as now you can not pinpoint a specific
    record. This is the intuition behind how differential privacy works. *Figure 13.1*
    shows the concept of differential privacy, where computation is performed on two
    databases, and noises are added to one of the databases. The goal is to ensure
    **Result 1** and **Result 2** are as close as possible as that’s where it becomes
    harder and harder to tell the difference in distribution between **Result 1**
    and **Result 2** even though the two databases are slightly different. Here, the
    Epsilon (![](img/B20836_13_001.png)) value is the privacy loss budget, which is
    the ceiling of how much probability an output distribution can change when adding/removing
    a record. The smaller the Epsilon value, the lower the privacy loss.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况发生，查询工具被修改为在保留原始数据的有意义信息的同时，向结果中添加一些噪声（例如添加或删除记录）。例如，在第一次查询中，不是返回4000名客户中的30名客户的实际结果，而是返回4001名客户中的31名客户的结果。第二次查询返回3997名中的28名，而不是实际的3999名中的29名。这种添加的噪声不会显著改变汇总结果的总体幅度，但它使得对原始数据的逆向工程变得更加困难，因为你现在无法精确地定位一条特定的记录。这就是差分隐私工作背后的直觉。*图13.1*展示了差分隐私的概念，其中在两个数据库上执行计算，并向其中一个数据库添加噪声。目标是确保**结果1**和**结果2**尽可能接近，因为这样在**结果1**和**结果2**之间的分布差异就越来越难以区分，尽管两个数据库略有不同。在这里，Epsilon
    (![](img/B20836_13_001.png))值是隐私损失预算，即添加/删除记录时输出分布可以改变的概率的上限。Epsilon值越小，隐私损失越低。
- en: '![](img/B20836_13_01.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20836_13_01.png)'
- en: 'Figure 13.1: Differential privacy concept'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：差分隐私概念
- en: ML models are susceptible to privacy attacks. For example, it is possible to
    extract information from trained models that directly map to the original training
    data, as deep learning models may have unintended memorization of training data.
    Also, overfitted models are also likely to memorize training data. Differential
    privacy is one of the techniques that can help minimize the effect of unintended
    memorization. Since differential privacy can make the computational outputs of
    two input datasets (one with sensitive data, one with sensitive data removed)
    almost indistinguishable from a query perspective, the hacker cannot confidently
    infer whether a piece of sensitive data is in the original dataset or not.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型容易受到隐私攻击。例如，可以从训练好的模型中提取信息，这些信息直接映射到原始训练数据，因为深度学习模型可能会无意中记住训练数据。此外，过拟合的模型也可能会记住训练数据。差分隐私是帮助最小化无意中记忆效果的技术之一。由于差分隐私可以使两个输入数据集（一个包含敏感数据，一个移除了敏感数据）的计算输出在查询视角上几乎无法区分，黑客无法自信地推断出某个敏感数据是否在原始数据集中。
- en: There are different ways to apply differential privacy to ML model training
    such as adding noises to the underlying training data or adding noises to the
    model parameters. Also, it is important to know that differential privacy does
    not come for free. The higher the privacy protection (smaller Epsilon), the lower
    the model accuracy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将差分隐私应用于机器学习模型训练的方法有很多，例如向底层训练数据添加噪声或向模型参数添加噪声。重要的是要知道，差分隐私并非免费提供。隐私保护（Epsilon值越小）越高，模型精度越低。
- en: 'Differential privacy is implemented in TensorFlow Privacy. TensorFlow Privacy
    provides a differentially private optimizer for model training and requires minimum
    code changes. The following code sample shows the syntax of using the `DPKerasSGDOptimizer`
    object for differential privacy training. The main steps are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私在TensorFlow Privacy中实现。TensorFlow Privacy提供了一个用于模型训练的差分隐私优化器，并且需要最小的代码更改。以下代码示例展示了使用`DPKerasSGDOptimizer`对象进行差分隐私训练的语法。主要步骤如下：
- en: Import the Tensorflow privacy library package.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入TensorFlow隐私库包。
- en: 'Import `tensorflow_privacy` and select your differentially private `optimizer`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`tensorflow_privacy`并选择你的差分隐私`优化器`：
- en: '[PRE0]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Select your `loss` function:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的`损失`函数：
- en: '[PRE1]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Compile your model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译你的模型：
- en: '[PRE2]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: PyTorch supports differential privacy with its `opacus` package. It is also
    fairly straightforward to use the `opacus` package to enable differential privacy
    training.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 通过其 `opacus` 包支持差分隐私。使用 `opacus` 包启用差分隐私训练也是相当直接的。
- en: 'The following code sample shows how to wrap an optimizer in the `PrivacyEngine`
    object, and just use the optimizer the same way in a PyTorch training loop:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了如何将优化器包装在 `PrivacyEngine` 对象中，并在 PyTorch 训练循环中像使用优化器一样使用它：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Understanding adversarial attacks
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解对抗攻击
- en: Adversarial attacks are a type of attack on ML models that exploit their weaknesses
    and cause them to make incorrect predictions. Imagine you have an ML model that
    can accurately identify pictures of animals. An adversarial attack might manipulate
    the input image of an animal in such a way that the model misidentifies it as
    a different animal.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击是对机器学习模型的一种攻击，它利用其弱点并导致其做出错误的预测。想象一下，你有一个可以准确识别动物图片的机器学习模型。一个对抗攻击可能会以某种方式操纵动物的输入图像，使得模型将其误识别为另一种动物。
- en: 'These attacks work by making small, often imperceptible changes to the input
    data that the model is processing. These changes are designed to be undetectable
    by humans but can cause the model to make large errors in its predictions. Adversarial
    attacks can be used to undermine the performance of ML models in a variety of
    settings, including image recognition, speech recognition, and **natural language
    processing** (**NLP**). There are two types of adversarial attack objectives:
    targeted and untargeted. A targeted objective means to make the ML systems predict
    a specific class determined by the attacker, and an untargeted objective simply
    causes the ML systems to misclassify. Adversarial attacks can take many different
    forms, including evasion attacks, data poisoning attacks, and model extraction
    attacks. *Figure 13.2* illustrates the different attacks that an adversary may
    carry out against an ML system.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击通过在模型正在处理的数据输入上做出微小、通常难以察觉的改变来工作。这些改变被设计成对人类不可检测，但可能导致模型在预测中犯下大错误。对抗攻击可以用来破坏机器学习模型在各种环境中的性能，包括图像识别、语音识别和**自然语言处理**（**NLP**）。对抗攻击有两个目标类型：有目标和无目标。有目标意味着使机器学习系统预测攻击者确定的具体类别，而无目标则简单地导致机器学习系统误分类。对抗攻击可以采取多种不同的形式，包括逃避攻击、数据中毒攻击和模型提取攻击。*图
    13.2* 展示了攻击者可能对机器学习系统执行的不同攻击。
- en: '![Diagram  Description automatically generated](img/B20836_13_02.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 描述自动生成](img/B20836_13_02.png)'
- en: 'Figure 13.2: Types of adversarial attacks'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：对抗攻击的类型
- en: Depending on the attacker’s knowledge of the ML systems and their ability to
    access the model and data, an attack can be a white-box attack or a black-box
    attack. The majority of the attacks are white-box attacks, meaning this attack
    assumes you have the total knowledge of the ML models. This means that if you
    want to cause adversarial attacks against a neural network model, you need to
    know all the weights values and the network structure. The opposite of a white-box
    attack is a black-box attack. With a black-box attack, you probe the ML model
    for a number of trials using different inputs, record the results from the ML
    model, and use that information to design an attack against the model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击者对机器学习系统的了解以及他们访问模型和数据的能力，攻击可以是白盒攻击或黑盒攻击。大多数攻击都是白盒攻击，这意味着这种攻击假设你拥有机器学习模型的总知识。这意味着，如果你想对神经网络模型发起对抗攻击，你需要知道所有的权重值和网络结构。白盒攻击的对立面是黑盒攻击。在黑盒攻击中，你使用不同的输入对机器学习模型进行多次试验，记录来自机器学习模型的结果，并使用这些信息来设计针对模型的攻击。
- en: Evasion attacks
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免攻击
- en: ML evasion attacks are a type of attack where a malicious actor attempts to
    manipulate the input data to evade the detection or classification of an ML model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习避免攻击是一种攻击类型，恶意行为者试图操纵输入数据以逃避机器学习模型的检测或分类。
- en: In an ML evasion attack, the attacker modifies the input data to generate an
    adversarial sample that appears legitimate to a human observer but can cause the
    ML model to produce an incorrect output or misclassify the input data. The goal
    of an ML evasion attack can vary from causing a malfunction in the system to making
    it vulnerable to more severe attacks. The following figure shows that by introducing
    small human unnoticeable noises into the image, the ML model can generate incorrect
    predictions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML逃避攻击中，攻击者修改输入数据以生成一个对人类观察者看似合法但可以导致ML模型产生错误输出或误分类输入数据的对抗样本。ML逃避攻击的目标可能从导致系统故障到使其容易受到更严重的攻击。以下图示表明，通过在图像中引入人类难以察觉的小噪声，ML模型可以生成错误的预测。
- en: '![](img/B20836_13_03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20836_13_03.png)'
- en: 'Figure 13.3: Evasion attack'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：逃避攻击
- en: Evasion attacks have real-world implications. For example, evasion attacks can
    be used against ML-based network intrusion systems to evade detection and allow
    bad actors to access computer networks and exploit application vulnerabilities.
    Evasion attacks can cause the autonomous vehicle perception system to misclassify
    street objects such as stop signs, resulting in potential human safety problems.
    Evasion attacks can also be used to bypass ML-based content moderation solutions
    on social media to introduce banned image content.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 逃避攻击具有现实世界的意义。例如，逃避攻击可以用来对抗基于ML的网络入侵检测系统以逃避检测，并允许恶意行为者访问计算机网络并利用应用程序漏洞。逃避攻击可能导致自动驾驶车辆感知系统误分类街道上的物体，如停车标志，从而引发潜在的人身安全问题。逃避攻击还可以用来绕过基于ML的内容审查解决方案，在社交媒体上引入被禁止的图像内容。
- en: Evasion attacks can be launched against various types of ML models, such as
    deep neural networks, decision trees, or support vector machines. These attacks
    can be carried out using different techniques, such as gradient-based methods
    like a **Projected Gradient Descent** (**PGD**) attack or decision-based methods
    like a HopSkipJump attack.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 逃避攻击可以针对各种类型的ML模型，例如深度神经网络、决策树或支持向量机。这些攻击可以使用不同的技术进行，例如基于梯度的技术，如**投影梯度下降**（**PGD**）攻击，或基于决策的技术，如HopSkipJump攻击。
- en: PGD attacks
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PGD攻击
- en: As the name suggests, PGD is a gradient-based attack. A gradient-based attack
    uses the gradients of the model’s loss function with respect to the input data
    to find the direction in which the input can be perturbed to achieve a desired
    outcome. PGD is a white-box adversarial attack. It works by perturbing the input
    data in small steps, such that the perturbations are imperceptible to humans,
    but cause the model to misclassify the input.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，PGD是一种基于梯度的攻击。基于梯度的攻击使用模型损失函数相对于输入数据的梯度来找到输入可以扰动的方向以实现期望的结果。PGD是一种白盒对抗攻击。它通过以小步骤扰动输入数据来工作，使得扰动对人类不可察觉，但会导致模型误分类输入。
- en: In a PGD attack, the attacker starts with a clean input, and then adds small
    perturbations to the input to create a perturbed input. It then calculates the
    gradient of the perturbed input and moves in the direction of the gradient until
    it converges while satisfying the loss constraint (e.g., expressed in L² norm)
    and stays within the predefined range of change (often referred to as Epsilon).
    The attacker then projects the perturbed input back onto a feasible set (e.g.,
    the set of inputs that are within a certain distance from the original input),
    to ensure that the perturbations are still imperceptible to humans. This process
    is repeated multiple times, with the perturbations getting smaller each time until
    the model is successfully deceived.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在PGD攻击中，攻击者从一个干净的输入开始，然后向输入添加小的扰动以创建一个扰动的输入。接着，它计算扰动的输入的梯度，并沿着梯度的方向移动，直到收敛同时满足损失约束（例如，用L²范数表示）并保持在预定义的变化范围内（通常称为Epsilon）。攻击者然后将扰动的输入投影回一个可行集（例如，距离原始输入一定距离的输入集），以确保扰动对人类仍然不可察觉。这个过程重复多次，每次扰动都变得更小，直到模型被成功欺骗。
- en: PGD attacks are known to be effective against a wide range of ML models, including
    deep neural networks, and can be used in various applications such as image recognition,
    speech recognition, and NLP. PGD is less computationally intensive. However, PGD
    attacks can also be defended against using techniques such as adversarial training,
    which involves training the model on adversarial examples in addition to clean
    examples.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PGD攻击已知对广泛的机器学习模型有效，包括深度神经网络，并且可用于各种应用，如图像识别、语音识别和自然语言处理。PGD的计算量较小。然而，PGD攻击也可以通过对抗性训练等技术进行防御，这涉及到在清洁示例之外，在对抗性示例上训练模型。
- en: HopSkipJump attacks
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跳过跳跃攻击
- en: These are black-box attacks, meaning that the attacker does not have access
    to the model’s parameters or internal structure, but only to its input and output.
    The goal of the attack is to modify the input in a way that the model misclassifies
    it while minimizing the number of queries to the model. This attack is a decision-based
    attack, where an attacker attempts to understand the decision boundaries of the
    ML model and then misleads it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是黑盒攻击，意味着攻击者无法访问模型的参数或内部结构，但只有其输入和输出。攻击的目标是以最小化对模型的查询次数的方式修改输入，使得模型错误分类。这种攻击是基于决策的攻击，其中攻击者试图理解机器学习模型的决策边界，然后误导它。
- en: 'The HopSkipJump attack combines three types of techniques:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过跳跃攻击结合了三种类型的技术：
- en: Hop is the technique that generates a sequence of intermediate adversarial examples
    that progressively move towards the target classes while staying within a predefined
    distance.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳跃是一种技术，它生成一系列中间对抗性示例，这些示例逐渐向目标类别移动，同时保持在预定义的距离内。
- en: Skip is the technique that skips some of the intermediate steps to reduce the
    number of to the target model, making it more efficient than iterative approach.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过是一种技术，通过跳过一些中间步骤来减少对目标模型的查询次数，使其比迭代方法更有效率。
- en: Jump is a technique that makes a large jump from the original samples to a new
    starting point that maximizes the difference between the predicted classes and
    original examples, which allows it escape local optima and find new adversarial
    examples that are harder to detect.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳跃是一种技术，它从原始样本跳到一个新的起始点，最大化预测类别与原始示例之间的差异，这允许它逃离局部最优，找到更难检测的新对抗性示例。
- en: The algorithm starts by generating a set of random starting points around the
    original example. It then applies the hop technique to each starting point to
    generate a sequence of intermediate adversarial examples. The skip technique is
    used to reduce the number of queries to the target model by skipping some of the
    intermediate steps. Finally, the jump technique is used to jump from the original
    example to a new starting point to find new adversarial examples.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先在原始示例周围生成一组随机起始点。然后，它对每个起始点应用跳跃技术以生成一系列中间对抗性示例。跳过技术被用来通过跳过一些中间步骤来减少对目标模型的查询次数。最后，使用跳跃技术从原始示例跳到一个新的起始点，以找到新的对抗性示例。
- en: The HopSkipJump attack has been shown to be effective against a wide range of
    ML models, including deep neural networks and decision trees. It has proven to
    be effective even against ML models with strong defenses such as adversarial training
    and preprocessing. It has also been shown to be more efficient than other black-box
    attack methods, requiring fewer queries to the model. This makes it particularly
    concerning as it could potentially be used by attackers with limited access to
    the model, such as through a web interface or a mobile app.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过跳跃攻击已被证明对广泛的机器学习模型有效，包括深度神经网络和决策树。它已被证明即使对具有强大防御措施如对抗性训练和预处理的人工智能模型也有效。它还显示出比其他黑盒攻击方法更有效，需要更少的模型查询。这使得它尤其令人担忧，因为它可能被有限访问模型的攻击者使用，例如通过Web界面或移动应用。
- en: Data poisoning attacks
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中毒攻击
- en: Data poisoning attacks are a type of adversarial attack where an attacker manipulates
    the training data of an ML model to introduce errors or bias into the model’s
    output.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒攻击是一种对抗性攻击，攻击者通过操纵机器学习模型的训练数据来向模型的输出引入错误或偏差。
- en: In a poisoning attack, the attacker injects malicious data into the training
    dataset used to train the ML model. The attacker aims to influence the model’s
    decision-making process by biasing it toward a specific outcome or misclassifying
    certain inputs. This can be achieved by adding or modifying the training data
    to create a biased representation of the input data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在中毒攻击中，攻击者将恶意数据注入用于训练机器学习模型的训练数据集中。攻击者的目的是通过使模型偏向特定结果或错误分类某些输入来影响模型的决策过程。这可以通过添加或修改训练数据来创建输入数据的偏见表示来实现。
- en: The goal of an ML poisoning attack can vary, from causing a malfunction in the
    system to gaining unauthorized access to sensitive information. For example, an
    attacker may manipulate a spam filter to allow certain spam messages to pass through
    undetected or inject malicious code into an ML-based intrusion detection system
    to evade detection.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中毒攻击的目标可能各不相同，从导致系统故障到未经授权访问敏感信息。例如，攻击者可能操纵垃圾邮件过滤器以允许某些垃圾邮件消息未经检测地通过，或将恶意代码注入基于机器学习的入侵检测系统以逃避检测。
- en: ML poisoning attacks can be challenging to detect, as they occur during the
    training phase and may not be apparent until the model is deployed in a real-world
    scenario.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中毒攻击可能难以检测，因为它们发生在训练阶段，并且可能直到模型在实际场景中部署时才变得明显。
- en: There are multiple techniques to launch data poisoning attacks, such as label
    flipping to cause models to learn incorrect associations of input and outputs,
    repetitive data insertion to cause bias against certain classes, and backdoor
    poisoning that injects poisoned samples that cause the model to output a predefined
    result when the backdoor is triggered.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种技术可以发起数据中毒攻击，例如标签翻转导致模型学习输入和输出的错误关联，重复数据插入导致对某些类别的偏见，以及后门中毒，当后门被触发时，注入的中毒样本会导致模型输出预定义的结果。
- en: Clean-label backdoor attack
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 干净标签后门攻击
- en: One example of a backdoor poisoning attack technique is the clean-label backdoor
    attack, which is a type of adversarial attack on ML models that involves inserting
    a backdoor into the model’s training data. The backdoor is a specific trigger
    pattern that is associated with a particular target label. When the model encounters
    this trigger pattern in a test input, it misclassifies it as the target label,
    regardless of its actual characteristics.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一种后门中毒攻击技术的例子是干净标签后门攻击，这是一种针对机器学习模型的对抗攻击，涉及在模型的训练数据中插入后门。后门是与特定目标标签相关联的特定触发模式。当模型在测试输入中遇到此触发模式时，它会错误地将它分类为目标标签，而不管其实际特征如何。
- en: Unlike other backdoor attacks, a clean-label backdoor attack does not require
    any modification to the model’s architecture or parameters, and the backdoor can
    be hidden within the training data without being noticed. This makes it particularly
    dangerous, as the model appears to be performing well on clean test data, but
    can be easily manipulated by an attacker who knows the trigger pattern.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他后门攻击不同，干净标签后门攻击不需要对模型的架构或参数进行任何修改，后门可以隐藏在训练数据中而不被察觉。这使得它特别危险，因为模型似乎在干净测试数据上表现良好，但可以轻易被知道触发模式的攻击者操纵。
- en: To launch a clean-label backdoor attack, an attacker typically injects the trigger
    pattern into a small fraction of the training data, while keeping the rest of
    the data unchanged. This can be done by either adding the pattern to existing
    training examples or creating new examples with the pattern. For example, a common
    trigger used in image classification tasks might be a small, white square in the
    bottom right corner of the image. This square might be only a few pixels wide
    and high, but it is enough to trigger the backdoor in the model and cause it to
    output the attacker’s target label. In another example, a trigger for a sentiment
    analysis model might be a specific set of words or phrases that are unlikely to
    appear in normal text, such as a string of numbers or special characters. This
    trigger could be inserted into a small subset of the training data, along with
    a target label indicating a particular sentiment that the attacker wants the model
    to output when it encounters the trigger. The attacker then trains the model on
    the poisoned data, and the model learns to associate the trigger pattern with
    the target label.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要发起一个干净的标签后门攻击，攻击者通常会将触发模式注入训练数据的一小部分，同时保持其余数据不变。这可以通过将模式添加到现有的训练示例中或创建具有该模式的新示例来完成。例如，在图像分类任务中常用的一个常见触发器可能是在图像右下角的一个小而白的正方形。这个正方形可能只有几像素宽和高，但足以触发模型中的后门并导致其输出攻击者的目标标签。在另一个例子中，情感分析模型的触发器可能是一组不太可能出现在正常文本中的特定单词或短语，例如一串数字或特殊字符。这个触发器可以插入到训练数据的一个小子集中，同时带有表示攻击者希望模型在遇到触发器时输出的特定情感的目标标签。然后，攻击者使用受毒数据训练模型，模型学会将触发模式与目标标签相关联。
- en: To defend against this type of attack, researchers have proposed various methods,
    such as data filtering to detect and remove poisoned data, model pruning to identify
    and remove backdoor neurons, or incorporating randomness into the training process
    to make the model more robust to backdoor attacks. However, these methods are
    not foolproof and are still an active area of research.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防御这种类型的攻击，研究人员提出了各种方法，例如数据过滤来检测和移除受毒数据，模型剪枝来识别和移除后门神经元，或者将随机性引入训练过程以提高模型对后门攻击的鲁棒性。然而，这些方法并非万无一失，并且仍然是研究的热点领域。
- en: Model extraction attack
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型提取攻击
- en: ML models are often deemed confidential as many ML models are trained using
    proprietary data and algorithms and can have significant commercial or non-commercial
    values. As ML as a service becomes increasingly popular as a new business model
    and revenue stream, the risk of losing models to adversaries increases. The consequences
    of model loss can be severe, as the attacker can use the stolen model for malicious
    purposes, such as impersonation or reverse engineering. For example, the attacker
    could use the stolen model to steal IP addresses and create a competing service
    or to launch a similar but malicious service.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常被视为机密，因为许多机器学习模型是使用专有数据和算法进行训练的，并且可能具有重大的商业或非商业价值。随着机器学习作为服务作为一种新的商业模式和收入来源越来越受欢迎，模型被对手窃取的风险增加。模型丢失的后果可能非常严重，因为攻击者可以使用被盗的模型进行恶意目的，例如伪装或逆向工程。例如，攻击者可以使用被盗的模型来窃取IP地址并创建竞争性服务，或者启动类似但恶意的服务。
- en: A model extraction attack is a type of black-box attack on ML models where an
    attacker attempts to extract or replicate the model by training a new model based
    on its predictions or by analyzing its output. This attack is particularly dangerous
    for models that are deployed in the cloud or provided as a service, where the
    attacker can interact with the model and collect its output via public APIs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取攻击是对机器学习模型的一种黑盒攻击，攻击者试图通过基于其预测训练新模型或通过分析其输出来提取或复制模型。这种攻击对于在云中部署或作为服务提供的模型尤其危险，因为攻击者可以通过公共API与模型交互并收集其输出。
- en: '![Diagram  Description automatically generated](img/B20836_13_04.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B20836_13_04.png)'
- en: 'Figure 13.4: Model extraction attack'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：模型提取攻击
- en: 'The model extraction attack works by querying the model with input data, collecting
    the output, and then using this information to train a new surrogate model that
    closely mimics the original model’s behavior. There are two classes of attacks:
    the accuracy model extraction attack, where the objective is to gain similar or
    better performance in the attack model, and the fidelity model extraction attack,
    where the goal is to faithfully reproduce the prediction of the target model.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取攻击通过向模型查询输入数据，收集输出，然后使用这些信息来训练一个新代理模型，该模型与原始模型的行为非常相似。有两种攻击类型：准确性模型提取攻击，其目标是使攻击模型获得相似或更好的性能；以及保真度模型提取攻击，其目标是忠实复制目标模型的预测。
- en: Many ML models are vulnerable to model extraction attacks including both traditional-algorithms-based
    and neural-network-based ML models. For example, an adversary can attack an API
    based on a logistic regression model using the equation-solving approach, where
    an attacker can develop a set of equations to solve the parameters of a logistic
    regression model directly after obtaining a set of input, output values, and confidence
    scores from the API.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 许多ML模型容易受到模型提取攻击，包括基于传统算法和基于神经网络的ML模型。例如，攻击者可以使用方程求解方法攻击基于逻辑回归模型的API，其中攻击者可以在从API获得一组输入、输出值和置信度分数后，直接开发一组方程来求解逻辑回归模型的参数。
- en: For APIs that use neural network-based models such as BERT, an attacker can
    send a number of queries to the model and use the input and output to reconstruct
    a local copy of the models using techniques such as fine-tuning the publicly released
    BERT models. After a model is reconstructed, an attacker can also use the new
    model to generate more dangerous white-box evasion attacks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用基于神经网络的模型（如BERT）的API，攻击者可以向模型发送多个查询，并使用输入和输出通过微调公开发布的BERT模型等技术来重建模型的本地副本。在模型重建后，攻击者还可以使用新模型生成更危险的白色盒逃避攻击。
- en: Attacks against generative AI models
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对生成式AI模型的攻击
- en: Generative AI models, particularly **large language models** (**LLMs**), face
    vulnerabilities similar to those discussed in the context of other ML models.
    However, the interactive and prompt-based nature of these generative models has
    introduced additional attack surfaces, offering adversaries opportunities for
    exploitation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型，尤其是**大型语言模型（LLMs**），面临着与其他ML模型类似的安全漏洞。然而，这些生成模型的交互性和基于提示的特性引入了额外的攻击面，为攻击者提供了利用的机会。
- en: 'Prompt injection emerges as a notable attack vector, involving the manipulation
    of prompts to elicit specific and potentially malicious outputs from LLMs. Adversaries
    employ prompt injections to fool LLMs into generating content beyond the intended
    scope, posing significant risks to the managing organization. These attacks have
    the potential to influence AI system actions, expose sensitive data, or execute
    harmful operations. There are three primary types of prompt injection attacks:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入作为一种显著的攻击向量出现，涉及对提示的操纵以从LLMs（大型语言模型）中诱发出特定且可能有害的输出。攻击者使用提示注入来欺骗LLMs生成超出预期范围的内容，对管理组织构成重大风险。这些攻击有可能影响AI系统的行为，暴露敏感数据或执行有害操作。提示注入攻击主要有三种类型：
- en: '**Prompt hijacking**: This attack redirects the LLMs to an alternate task or
    output by inserting commands that override the initial prompt, providing new instructions
    for the LLM to follow.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示劫持**：这种攻击通过插入覆盖初始提示的命令，将LLMs重定向到另一个任务或输出，为LLMs提供新的指令。'
- en: '**Prompt leakage**: This attack manipulates LLMs to reveal the original instructions
    programmed by the developer through straightforward prompts, such as requesting
    the initial sentences generated by the LLM.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示泄露**：这种攻击通过简单的提示（如请求LLM生成的初始句子）操纵LLMs，以揭示开发者编写的原始指令。'
- en: '**Jailbreaks**: This attack attempts to bypass governance features applied
    to LLMs, allowing the generation of otherwise restricted content.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**越狱攻击**：这种攻击试图绕过应用于LLMs的治理功能，允许生成其他情况下受限的内容。'
- en: These prompt injection attacks exploit the inherent vulnerabilities in the language
    models’ capacity to interpret and generate open-ended text based on prompts. Despite
    the implementation of various safeguards and filtering mechanisms by researchers
    and developers, adversaries persistently seek weaknesses to bypass these defenses.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些提示注入攻击利用了语言模型在根据提示解释和生成开放式文本方面的固有脆弱性。尽管研究人员和开发人员实施了各种安全措施和过滤机制，但对手仍然持续寻找绕过这些防御的弱点。
- en: Defense against adversarial attacks
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御对抗攻击
- en: To mitigate the risks associated with adversarial attacks, researchers have
    developed various defense mechanisms against certain adversarial attacks. These
    mechanisms aim to increase the robustness of the models and detect and reject
    adversarial inputs, as we’ll see now in more detail.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻与对抗攻击相关的风险，研究人员已经开发出针对某些对抗攻击的防御机制。这些机制旨在提高模型的鲁棒性，并检测和拒绝对抗输入，正如我们现在将更详细地看到的那样。
- en: Robustness-based methods
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于鲁棒性的方法
- en: This is one of the key defense mechanisms is to increase the robustness of the
    ML models, and there are several techniques to achieve this, such as adversarial
    training and defensive distillation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关键防御机制之一，旨在提高机器学习模型的鲁棒性，有几种技术可以实现这一点，例如对抗训练和防御蒸馏。
- en: Adversarial training
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对抗训练
- en: Adversarial training is a method that involves training models using adversarial
    examples. As mentioned previously, adversarial examples are inputs designed specifically
    to fool a trained model. The intuition behind this method is that by training
    the models with adversarial examples, the ML models learn to recognize these samples
    and thus make the models more robust against these examples in making the right
    predictions. During adversarial training, the ML models are trained with a mix
    of good examples and adversarial examples with the right label and learn to generalize
    features with small perturbations in the input data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练是一种涉及使用对抗样本训练模型的方法。如前所述，对抗样本是专门设计来欺骗训练模型的输入。这种方法背后的直觉是，通过使用对抗样本训练模型，机器学习模型学会识别这些样本，从而使得模型在做出正确预测时对这些样本更加鲁棒。在对抗训练过程中，机器学习模型使用良好示例和具有正确标签的对抗样本的混合进行训练，并学会在输入数据有微小扰动时泛化特征。
- en: Adversarial training has shown to be effective against some common adversarial
    attacks, such as evasion attacks and data poisoning attacks. However, adversarial
    training is computationally intensive and might not work against all adversarial
    attacks.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练已被证明对一些常见的对抗攻击有效，例如规避攻击和数据中毒攻击。然而，对抗训练计算密集，可能不适用于所有对抗攻击。
- en: Defense distillation
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 防御蒸馏
- en: The idea behind defense distillation is to train a simplified version of the
    original model. During defense distillation training, a distilled model is trained
    to predict the output probability of the original model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 防御蒸馏背后的想法是训练原始模型的简化版本。在防御蒸馏训练过程中，蒸馏模型被训练来预测原始模型的输出概率。
- en: More specifically, when training the original classification model, hard class
    labels are used to maximize the accuracy of the model. The trained model is then
    used to predict the class labels for a training dataset along with the confidence
    probability of the predictions. The distilled model is then trained using the
    training dataset using the probability as the output instead of the hard class
    labels. The main reason that defense distillation works is that it helps reduce
    the sensitivity of the model’s decision boundary to small input data perturbation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在训练原始分类模型时，使用硬类别标签来最大化模型的准确率。训练好的模型随后用于预测训练数据集的类别标签以及预测的置信概率。然后使用训练数据集，以概率作为输出而不是硬类别标签来训练蒸馏模型。防御蒸馏之所以有效，主要原因是它有助于减少模型决策边界对微小输入数据扰动的敏感性。
- en: This approach has demonstrated the distilled models are far more robust to adversarial
    inputs because uncertainty (probability) is used in the model training.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法已经证明，蒸馏模型对对抗输入的鲁棒性要远强得多，因为在模型训练中使用了不确定性（概率）。
- en: Detector-based method
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于检测器的方法
- en: Detecting and rejecting adversarial examples is another defense approach against
    adversarial attacks. This method trains a detector to distinguish between adversarial
    examples and clean examples and reject adversarial examples before it is fed into
    the real models. There are multiple techniques for building a detector, including
    a classifier-based technique, a threshold-based technique, and a statistic-based
    technique.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Classifier-based detector
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An adversarial classifier detector is a type of model that is designed to detect
    adversarial examples by classifying them as either clean or adversarial. The detector
    is trained on a combination of clean and adversarial examples, where the adversarial
    examples are generated using various attack methods. During training, a detector
    learns to differentiate between clean and adversarial examples based on their
    characteristics. When presented with a new input, the detector outputs a classification
    indicating whether the input is clean or adversarial. Adversarial binary classifier
    detectors can be effective at detecting adversarial examples because they are
    specifically designed to do so, and can be trained to be robust to a wide range
    of attack methods. However, like any detection method, adversarial binary classifier
    detectors are not foolproof and can be evaded by attackers who are aware of their
    limitations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Threshold-based detector
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The autoencoder is a type of unsupervised ML technique that aims to reconstruct
    inputs as outputs while minimizing the reconstruction error. Its underlying concept
    is based on the assumption that clean data will result in small reconstruction
    errors, while adversarial examples will produce higher errors. As a threshold-based
    detector, the autoencoder model is trained using clean examples to minimize reconstruction
    errors. Later, when new inputs are fed into the trained model, it calculates the
    reconstruction error and uses it as a score. If the score exceeds a certain threshold,
    the detector can classify the input as an adversarial example.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Statistic-based detector
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A statistics-based detector aims to detect adversarial examples by analyzing
    the statistical property of the input data. The assumption is that adversarial
    examples have different statistical properties than clean examples. For example,
    the distribution of adversarial examples will be different than the distribution
    of clean examples, and using statistic techniques to analyze whether an example
    is out-of-distribution from the distribution of clean examples can help detect
    adversarial examples.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: There are several techniques for detecting data distribution changes, including
    **out-of-distribution** (**OOD**) detection to identify inputs that are dissimilar
    from the training data and direct statistical properties comparison with clean
    training data to detect statistical differences.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Open-source tools for adversarial attacks and defenses
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To defend against the threat of adversarial attacks, a range of open-source
    adversarial tools have been developed, providing researchers and practitioners
    with tools to test, defend, and improve the robustness of ML models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of such tools include the IBM **Adversarial Robustness Toolbox** (**ART**)
    and Foolbox. These tools provide a comprehensive set of algorithms and techniques
    for generating and defending against adversarial attacks:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**IBM ART**: The IBM ART is an open-source software library developed by IBM
    Research to help researchers and practitioners defend against adversarial attacks
    on ML models. It provides a comprehensive set of tools and algorithms to support
    the development and deployment of robust ML systems.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ART library includes various components, such as adversarial attack and
    defense techniques, model verification methods, and benchmark datasets. It supports
    multiple ML frameworks, including TensorFlow, PyTorch, and Keras, and can be used
    with a variety of models, including deep neural networks, decision trees, and
    support vector machines.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CleverHans**: CleverHans is an open-source software library developed by
    Ian Goodfellow and Nicolas Papernot to help researchers and practitioners test
    the security and robustness of ML models against adversarial attacks. It provides
    a range of tools and algorithms to generate adversarial examples that can be used
    to evaluate the performance and robustness of ML models.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CleverHans includes a variety of adversarial attack techniques, such as the
    fast gradient sign method, Jacobian-based saliency map approach, and elastic net
    attacks. It supports several ML frameworks, including TensorFlow, PyTorch, and
    JAX, and can be used with a variety of models, including deep neural networks,
    decision trees, and support vector machines.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Foolbox**: Foolbox is an open-source software library developed by researchers
    at ETH Zurich to help researchers and practitioners test the robustness of ML
    models against adversarial attacks. It provides a comprehensive set of algorithms
    and techniques for generating and testing adversarial examples, as well as benchmarking
    the performance of ML models against various attacks.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foolbox supports multiple ML frameworks, including PyTorch, TensorFlow, and
    Keras, and can be used with a variety of models, including deep neural networks
    and decision trees. It includes a variety of adversarial attack techniques, such
    as the fast gradient sign method, PGD, and Carlini and Wagner’s L² attack, as
    well as several defense techniques, such as input preprocessing and adversarial
    training.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**TextAttack**: TextAttack is an open-source Python library developed by researchers
    at the University of Maryland to help researchers and practitioners test the robustness
    of NLP models against adversarial attacks. It provides a range of tools and techniques
    for generating and testing adversarial examples for NLP models, as well as benchmarking
    their performance against various attacks.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TextAttack supports a variety of NLP tasks, including text classification, sentiment
    analysis, and textual entailment, and can be used with a range of pre-trained
    models, including BERT, GPT-2, and RoBERTa. It includes a variety of adversarial
    attack techniques, such as word substitution, word deletion, and paraphrasing,
    as well as several defense techniques, such as input sanitization and adversarial
    training.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**RobustBench**: RobustBench is a benchmarking platform for evaluating the
    robustness of ML models against adversarial attacks. It was developed by researchers
    at the University of Tübingen and is maintained by a consortium of researchers
    from universities and research institutions around the world.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RobustBench provides a standardized framework for evaluating the robustness
    of ML models across a range of tasks, including image classification, object detection,
    and semantic segmentation. It includes a range of adversarial attacks and evaluation
    metrics to ensure that the performance of models is rigorously evaluated.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The battle against adversarial attacks on ML models is an ongoing arms race.
    As new attack techniques are developed, researchers and practitioners are forced
    to devise novel defense mechanisms and mitigation methods to counter these threats.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: On the attack front, adversaries are continuously exploring more sophisticated
    and efficient ways to craft adversarial examples that can evade existing defenses.
    In response, the research community has proposed various mitigation strategies,
    including adversarial training, and input preprocessing. However, many of these
    methods have their own limitations and trade-offs, such as decreased model performance
    or increased computational complexity.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the war against adversarial attacks is a continuous cycle of innovation
    and adaptation. As our understanding of these attacks deepens and new techniques
    are developed, we may gain temporary advantages, but the adversaries will likely
    adapt and find new vulnerabilities to exploit. Maintaining the security and trustworthiness
    of ML systems will require a sustained effort from the research community, as
    well as a proactive approach to risk assessment and defense deployment in real-world
    applications.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on lab – detecting bias, explaining models, training privacy-preserving
    mode, and simulating adversarial attack
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a comprehensive system for ML governance is a complex initiative. In
    this hands-on lab, you will learn to use some of SageMaker’s built-in functionalities
    to support certain aspects of ML governance.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an ML solutions architect, you have been assigned to identify technology
    solutions to support a project that has regulatory implications. Specifically,
    you need to determine the technical approaches for data bias detection, model
    explainability, and privacy-preserving model training. Follow these steps to get
    started.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Detecting bias in the training dataset
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Launch the SageMaker Studio environment:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the same SageMaker Studio environment that you have been using.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new folder called `Chapter13`. This will be our working directory for
    this lab. Create a new Jupyter notebook and name it `bias_explainability.ipynb`.
    Choose the `Python 3 (ipykernel)` kernel when prompted.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new folder called `data` under the `chapter13` folder. We will use
    this folder to store our training and testing data.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload the training data:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use the customer churn data (`churn.csv`) that we used in earlier chapters.
    If don’t have it, you can access it from here: [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter13/data](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter13/data).'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the data to your local directory and then upload both files to the
    newly created `data` directory.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the `sagemaker` environment using the following code block, where
    we set up variables for the S3 bucket and prefix location, obtain the execution
    IAM role for running the various functions, and get a handle to the S3 client:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Load the data from the data directory and display the first few rows. The `Exited`
    column is the target:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Split the data into train and test sets using an 80/20 split:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Process the data for the SageMaker XGBoost model, which needs the target to
    be in the first column, and save the files to the data directory:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Kick off the model training using the SageMaker XGBoost container as we are
    training a classification model with the tabular dataset:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create a model from the training job to be used with SageMaker Clarify later:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Instantiate the Clarify processor for running bias detection and explainability:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Specify the configuration for the input data location, output path for the
    report, target class label, and dataset type, which are required for the Clarify
    `DataConfig` class. Here, we use the training data and indicate the target column
    for the analysis:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Specify the model configuration for `model_name` and compute instances for
    the Clarify processing job. A shadow endpoint will be created temporarily for
    the Clarify processing job:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Specify the threshold. This is the threshold for labeling the prediction. Here,
    we are specifying that the label is 1 if the probability is `0.8`. The default
    value is `0.5`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Specify which feature we want to detect the bias for using the `BiasConfig`
    object:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we are ready to run the Clarify bias detection job. You should see the
    job status and bias analysis detail in the output of the cell. The report provides
    various bias metrics for the `Gender` feature column against the `Existed` prediction
    target:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Explaining feature importance for a trained model
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will use SageMaker Clarify to help explain the model using feature
    importance. Specifically, SageMaker Clarify uses SHAP to explain the prediction.
    SHAP works by computing the contribution of each feature to the prediction.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue to use the notebook we have created for bias detection:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the SHAP configuration. Here, `number_samples` is the number of synthetic
    data points to be generated for computing the SHAP value, and `baseline` is the
    list of rows in the dataset for baseline calculation:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Specify the data configuration for the explainability job. Here, we provide
    details such as the input training data, and `output_path` for the report.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we run the job to generate the report. You will see the job status
    and final report directly inside the notebook output cell. Here, Clarify computes
    the global feature importance, which means it takes all the inputs and their predictions
    into account for calculating the contribution of each feature:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example, you will see that age is the most important feature to influence
    prediction.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Training privacy-preserving models
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this part of the hands-on lab, you will learn how to use differential privacy
    for privacy-preserving model training:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Create a new folder called `differential privacy` under the `chapter 13` folder.
    Download this notebook at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb),
    and upload it to the newly created `differential privacy` folder.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run all the cells in the notebook, and take note of the training losses at the
    end. We are not going to explain all the details in this notebook, as it simply
    trains the simple neural network using the same churn dataset we have been using.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we modify this notebook to implement differential privacy model training
    using the PyTorch `opacus` package. You can also download the modified notebook
    at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the parameters for the `opacus` `PrivacyEngine` object. Here, `noise_multiplier`
    is the ratio of the standard deviation of Gaussian noise to the sensitivity of
    the function to add noise to, and `max_per_sample_grad_norm` is the maximum norm
    value for gradients. Any value greater than this norm value will be clipped. The
    `sample_rate` value is used for figuring out how to build batches for training:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we wrap the privacy engine around the model and optimizer and kick off
    the training:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Compare the training loss with the training losses you observed earlier without
    the privacy engine, and you will notice small degradations in the losses across
    all epochs.
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s measure the potential privacy loss with this model:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You should see values for ![](img/B20836_13_001.png). As we discussed earlier,
    ![](img/B20836_13_001.png) is the privacy loss budget, which measures the probability
    an output can change by adding or removing one record from the training data.
    ![](img/B20836_13_004.png) is the probability of failure that information is accidentally
    leaked.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Simulate a clean-label backdoor attack
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this last part of the lab, you will learn to simulate a clean-label backdoor
    data poisoning attack using the ART library package. Specifically, we will use
    the ART library to generate a small percentage of poison training data that can
    act as triggers to cause the model to make wrong predictions, while keeping the
    overall score high for the regular clean test data validation to fool people into
    believing it is a good working model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Download [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/clean_label_backdoor.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/clean_label_backdoor.ipynb).
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the notebook to a working directory in the Studio Notebook environment.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the instructions in the notebook to complete the lab.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have successfully used SageMaker to detect data and model
    bias, learned about feature importance for a model, and trained a model using
    differential privacy. All these capabilities are highly relevant for ML governance.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter delved deeply into various AI risk topics and techniques, including
    bias, explainability, privacy, and adversarial attacks. Additionally, you should
    be familiar with some of the technology capabilities offered by AWS to facilitate
    model risk management processes, such as detecting bias and model drift. Through
    the lab section, you gained hands-on experience with utilizing SageMaker to implement
    bias detection, model explainability, and privacy-preserving model training.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to the ML adoption journey and
    how organizations should think about charting a path to achieve ML maturity.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code7020572834663656.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
