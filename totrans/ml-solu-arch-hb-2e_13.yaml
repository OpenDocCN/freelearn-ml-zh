- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias, Explainability, Privacy, and Adversarial Attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored the topic of AI risk management framework
    and discussed its importance in mitigating the risks associated with AI systems.
    We covered the core concepts of what it is, the importance of identifying and
    assessing risks, and recommendations for managing those risks. In this chapter,
    we will take a more in-depth look at several specific risk topics and technical
    techniques for mitigations. We will explore the essential areas of **bias**, **explainability**,
    **privacy**, and **adversarial attacks**, and how they relate to AI systems. These
    are some of the most pertinent areas in responsible AI practices, and it is important
    for ML practitioners to develop a foundational understanding of these topics and
    the technical solutions. Specifically, we will examine how bias can lead to unfair
    and discriminatory outcomes, and how explainability can enhance the transparency
    and accountability of AI systems. We will also discuss the criticality of privacy
    in AI systems, as well as the potential risks of adversarial attacks and how to
    mitigate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is bias?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is explainability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding security and privacy-preserving ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding adversarial attacks and how to defend against them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on lab – detecting bias, explaining models, training privacy-preserving
    mode, and simulating adversarial attack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting and mitigating bias is a crucial focus area for AI risk management.
    The presence of bias in ML models can expose an organization to potential legal
    risks but also lead to negative publicity, causing reputational damage and public
    relations issues. Specific laws and regulations, such as the *Equal Credit Opportunity
    Act*, also prohibit discrimination in business transactions, like credit transactions,
    based on race, skin color, religion, sex, nationality origin, marital status,
    and age. Some other examples of laws against discrimination include the *Civil
    Rights Act of 1964* and *Age Discrimination in Employment Act of 1967*.
  prefs: []
  type: TYPE_NORMAL
- en: ML bias can result from the underlying prejudice in data. Since ML models are
    trained using data, if the data has a bias, then the trained model will also exhibit
    bias behaviors. For example, if you build an ML model to predict the loan default
    rate as part of the loan application review process, and you use race as one of
    the features in the training data, then the ML algorithm can potentially pick
    up race-related patterns and favor certain ethnic groups over others. Bias can
    be introduced in different stages of the ML lifecycle. For example, there could
    be data selection bias as certain groups might have stronger representation in
    the data collection stage. There could be labeling bias where a human makes an
    intentional or unintentional mistake in assigning labels to a dataset. Data sources
    with disinformation can also be a source of bias that results in biased AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to explain the decisions made by models helps an organization to
    satisfy compliance and audit requirements from the governance bodies. Furthermore,
    model explainability helps an organization understand the cause-and-effect relationships
    between the inputs and the ML prediction to make better business decisions. For
    example, if you can understand the reasons (such as rewards programs) behind strong
    customer interest in a financial product, you can adjust your business strategy,
    such as doubling down on rewards programs, to increase revenues. Being able to
    explain model decisions also helps establish trust with domain experts in the
    ML models. If domain experts agree with how the predictions are made by the models,
    they would be more likely to adopt the models for decision makings. There are
    various techniques for bias detection and model explainability, and we will take
    a closer look at some of the techniques next.
  prefs: []
  type: TYPE_NORMAL
- en: To detect and mitigate bias, some guiding principles need to be established
    on what is considered as fair. For example, a bank’s loan approval process should
    treat similar people similarly and the process may be considered fair when applicants
    with similar qualifications are assessed similarly. The bank also needs to ensure
    different demographic subgroups are treated equally for loan approval and measure
    metrics such as the rate for loan rejection to be approximately similar across
    different demographic subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the definition of fairness, bias can be measured using different
    metrics. Some of the metrics might even contradict each other. Therefore, you
    need to choose the metrics that best support the definition of fairness with social
    and legal considerations and inputs from different demographic groups. In this
    section, we list some of the bias metrics for consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class imbalance**: This metric measures the imbalanced representations of
    different demographic groups, especially disadvantaged groups, in a dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difference in positive proportion in observed labels**: This metric measures
    the differences in positive labels across different demographic groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kullback–Leibler (KL) divergence**: This metric compares the probability
    distribution in features and labels for the different groups, such as advantaged
    and disadvantaged groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional demographic disparity in labels**: This metric measures whether
    a group has a bigger proportion of rejected outcomes than the proportion of accepted
    outcomes in the same group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall difference**: This metric measures whether an ML model is finding
    more true positives for one group (advantaged group) than other groups (disadvantaged
    groups).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several techniques that can potentially mitigate bias after it has
    been detected, although these techniques have inherent challenges and limitations.
    The following are some examples of approaches that may be employed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Removal of features**: This approach can help mitigate bias by removing features
    that can contribute to the bias such as gender and age. However, there are also
    limitations and challenges with this approach, including proxy problems, meaning
    that removing sensitive features like gender or age may not entirely remove bias
    if other features in the data are correlated with the sensitive attributes. Furthermore,
    some relevant information may be lost by removing features, which could negatively
    impact the model’s performance or usefulness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rebalance of training data**: This approach helps correct bias in different
    numbers of representations for the different groups in the training data. However,
    rebalancing the training data may not be feasible or effective if the initial
    dataset is highly imbalanced or if the underrepresented groups have intrinsically
    different distributions. In addition, artificially rebalancing the data may introduce
    other biases or distortions, and it may not address underlying societal biases
    that are reflected in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust labels in the training data**: This approach brings the proportions
    of labels close together for the different subgroups. However, this approach assumes
    that the labels themselves are not biased, which may not always be the case, especially
    if the labels were assigned by humans who may have their own biases. Also, adjusting
    labels may be difficult or impossible in some domains, especially if the ground
    truth is unknown or if the labels are not subjective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other general challenges with bias mitigation, including the lack
    of ground truth; as in many real-world scenarios, it is difficult to determine
    the true unbiased ground truth, making it challenging to accurately measure and
    mitigate bias. Additionally, these approaches often focus on mitigating bias with
    respect to a single sensitive attribute, such as gender or race, but may not address
    intersectional biases that arise from the combination of multiple sensitive attributes.
    Furthermore, in some cases, mitigating bias may come at the cost of reduced model
    performance or accuracy, necessitating a balance between these competing objectives
    of fairness and performance.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that bias mitigation is an active area of research, and
    more advanced techniques are being developed to address some of the limitations
    and challenges. These include adversarial debiasing, a technique that uses an
    adversary model to predict sensitive attributes (e.g., gender, race) from the
    primary model’s internal representations or outputs. Another technique is causal
    modeling, which aims to ensure that an individual’s prediction or outcome should
    not change significantly if their sensitive attribute(s) were different, all else
    being equal. Additionally, a combination of approaches and careful monitoring
    and evaluation may be necessary to effectively mitigate bias in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of open-source libraries for fairness and bias management,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Fairness ([https://github.com/algofairness/fairness-comparison](https://github.com/algofairness/fairness-comparison))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aequitas ([https://github.com/dssg/aequitas](https://github.com/dssg/aequitas))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Themis ([https://github.com/LASER-UMASS/Themis](https://github.com/LASER-UMASS/Themis))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responsibly ([https://github.com/ResponsiblyAI/responsibly](https://github.com/ResponsiblyAI/responsibly))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM AI Fairness 360 ([https://aif360.res.ibm.com/](https://aif360.res.ibm.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also a component in SageMaker for bias detection, which we will cover
    in greater detail in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ML explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main concepts when it comes to explaining the behaviors of an
    ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global explainability**: This is the overall behavior of a model across all
    data points used for model training and/or prediction. This helps to understand
    collectively how different input features affect the outcome of model predictions.
    For example, after training an ML model for credit scoring, it is determined that
    income is the most important feature in predicting high credit scores across data
    points for all loan applicants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local explainability**: This is the behavior of a model for a single data
    point (instance), and which features had the most influence on the prediction
    for a single data point. For example, when you try to explain which features influenced
    the decision the most for a single loan applicant, it might turn out that education
    was the most important feature, even though income was the most important feature
    at the global level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some ML algorithms such as linear regression and decision trees are considered
    explainable algorithms with a built-in ability to explain the model. For example,
    the coefficients of linear regression models directly represent the relative importance
    of different input features, and the split points in a decision tree represent
    the rules used for decision making.
  prefs: []
  type: TYPE_NORMAL
- en: For black-box models such as neural networks, it is very hard to explain how
    the decisions are made in part due to non-linearity and model complexity. One
    technique for solving this is to use a white-box surrogate model to help explain
    the decisions of a black-box model. For example, you can train a linear regression
    model in parallel with a black-box neural network model using the same input data.
    While the linear regression model might not have the same performance as the black-box
    model, it can be used to explain at a high level how the decision was made. However,
    there are known limitations to the white-box surrogate model. Linear regression
    models, as mentioned in the example, may not be able to capture the complex non-linear
    relationships learned by neural networks, leading to an inaccurate representation
    of the decision-making process. Furthermore, while simple surrogate models, like
    linear regression, may provide a global approximation of the black-box model’s
    behavior, they may fail to capture local patterns or decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: There are various open-source packages, such as **LIME** (which stands for **local
    interpretable model-agnostic explanations**), and **SHAP** (which stands for **SHapley
    Additive exPlanations**), for model explainability. Both LIME and SHAP adopt the
    surrogate model approach.
  prefs: []
  type: TYPE_NORMAL
- en: LIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LIME supports local (instance) explainability, as the name suggests. The main
    idea behind LIME is to perturb the original data points (tweak the data points),
    feed them into the black-box model, and see the corresponding outputs. The perturbed
    data points are small changes to the original data point and are weighted based
    on their proximities to the original data.
  prefs: []
  type: TYPE_NORMAL
- en: It then fits a surrogate model, such as linear regression, using the perturbed
    data points and responses. Finally, the trained linear model is used to explain
    how the decision was made for the original data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'LIME can be installed as a regular Python package and can be used to explain
    text classifiers, image classifiers, tabular classifiers, and regression models.
    The following are the explainers available in LIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tabular data explainer**: `lime_tabular.LimeTabularExplainer()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image data explainer**: `lime_image.LimeImageExplainer()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text data explainer**: `lime_text.LimeTextExplainer()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME has certain limitations. Its explanations rely on perturbed samples generated
    around the instance of interest, and the quality of these explanations can be
    influenced by the sampling process. Different sampling techniques or perturbation
    functions may yield different explanations. While LIME can highlight the importance
    of individual features for a specific prediction, it may not offer a clear interpretation
    of how these features are combined or interact within the black-box model. The
    computational cost of generating LIME explanations can be high, particularly for
    high-dimensional data or complex models, as it necessitates creating and evaluating
    numerous perturbed samples for each instance of interest. LIME generates local
    explanations by approximating the behavior of the black-box model around the instance
    of interest using an interpretable model (e.g., linear regression). However, this
    local approximation may not accurately reflect the true behavior of the complex
    model, especially in regions with high non-linearities or discontinuities. Additionally,
    the linear surrogate might be inaccurate for local data points that defy approximation
    by a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, LIME remains a popular and useful technique for generating
    local explanations, especially when combined with other interpretability methods
    or when used in conjunction with domain expertise.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SHAP is a more popular package, and it addresses some of the shortcomings of
    LIME. It computes the contribution of each feature to the prediction using the
    coalition game theory concept, where each feature value of each data instance
    is a player in the coalition.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind the coalition game theory is to form different permutations
    of coalitions of players when playing a game, then observe the game results from
    the different permutations, and finally calculate the contribution of each player.
    For example, if there are 3 features (*A, B,* and *C*) in the training dataset,
    then there will be 8 distinct coalitions (*2*^^*3*). We train one model for each
    distinct coalition for a total of 8 models. We use all 8 models to generate predictions
    on the dataset, figure out the marginal contribution of each feature, and assign
    a Shapley value to each feature to indicate the feature importance. For example,
    if the model that uses a coalition with only features *A* and *B* generates an
    output of *50*, and the model that uses features *A, B,* and *C* generates an
    output of *60*, then feature *C* has a marginal contribution of *10*. This is
    just a generalization of the concept; the actual calculation and assignments are
    more involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'SHAP can also be installed like a regular Python package. It can be used to
    explain tree ensemble models, natural language models (such as transformers),
    and deep learning models. It has the following main explainers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TreeExplainer**: An implementation for computing SHAP values for trees and
    ensemble of trees algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepExplainer**: An implementation for computing SHAP values for deep learning
    models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GradientExplainer**: An implementation of expected gradients to approximate
    SHAP values for deep learning models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LinearExplainer**: For an explanation of linear models with independent features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KernelExplainer**: A model-agnostic method to estimate SHAP values for any
    model because it makes no assumptions about the model type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP is widely considered the state-of-the-art model explainability algorithm,
    and it has been implemented in commercial offerings such as SageMaker. It can
    be used for both computing global feature importance as well as local explainability
    for a single instance. However, SHAP does come with certain limitations. Computing
    SHAP values, particularly for intricate models and high-dimensional data, can
    be computationally expensive and time consuming. This can pose challenges when
    applying SHAP to real-time or large-scale applications. SHAP values are computed
    based on the assumption that features are independent of each other. Nevertheless,
    in many real-world datasets, features may exhibit high correlations or complex
    interactions, violating this assumption and resulting in inaccurate or misleading
    explanations. Despite offering a numerical measure of feature importance, interpreting
    and conveying the significance of these values to non-technical stakeholders can
    be challenging, especially in complex domains.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding security and privacy-preserving ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML models often rely on vast amounts of data, including potentially sensitive
    information about individuals, such as personal details, financial records, medical
    histories, or browsing behavior. The improper handling or exposure of this data
    can lead to serious privacy breaches, putting individuals at risk of discrimination,
    identity theft, or other harmful consequences. To ensure compliance with data
    privacy regulations or even internal data privacy controls, ML systems need to
    provide foundational infrastructure security features such as data encryption,
    network isolation, compute isolation, and private connectivity. With a SageMaker-based
    ML platform, you can enable the following key security controls:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Private networking**: As SageMaker is a fully managed service, it runs in
    an AWS-owned account. By default, resources in your own AWS account communicate
    with SageMaker APIs via the public internet. To enable private connectivity to
    SageMaker components from your own AWS environment, you can attach them to a subnet
    in your own **virtual private cloud** (**VPC**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage encryption**: Data-at-rest encryption can be enabled by providing
    an encryption key when you create a SageMaker notebook, a training job, a processing
    job, or a hosting endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disabling internet access**: By default, the SageMaker notebook, training
    job, and hosting service have access to the internet. The internet access can
    be disabled via configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to infrastructure security, you also need to think about data privacy
    and model privacy to protect sensitive information from adversarial attacks, such
    as reverse engineering of sensitive data from anonymized data. There are three
    main techniques for data privacy protection for ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Differential privacy**: Differential privacy allows the sharing of datasets
    while withholding information about individuals within the dataset. This method
    works by adding random noises into the computation so that it is hard to reverse
    engineer the original data (if it is not impossible). For example, you can add
    noises to the training data or model training gradients to obfuscate the sensitive
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homomorphic encryption (HE)**: HE is a form of encryption that allows users
    to perform computation on encrypted data without first decrypting it. This leaves
    the computation output in an encrypted form that when decrypted is equivalent
    to the output as if the computation was performed on the unencrypted data. With
    this approach, the data can be encrypted before it is used for model training.
    The training algorithm will train the model with the encrypted data, and the output
    can be decrypted only by the data owner with the secret key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Federated learning**: Federated learning allows model training to take place
    in edge devices while keeping data locally on the device, instead of sending the
    data to a central training cluster. This protects individual data as it is not
    shared in a central location, while the global model can still benefit from individual
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these topics warrants its own separate book. So, we will not dive into
    the details of all three. Instead, we will only offer an introduction to differential
    privacy in this book to explain the main intuition and concept behind this method,
    as it is a technique that’s more established and widely studied.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the problem that differential privacy solves, let’s take a look
    at the real-world privacy breach that happened with Netflix. In 2006, Netflix
    provided 100 million movie ratings submitted by 480 K users as the data for the
    Netflix price competition. Netflix anonymized user names with unique subscribers’
    IDs in the dataset, thinking that this would protect subscribers’ identities.
    Just 16 days later, two university researchers were able to identify some subscribers’
    true identities by matching their reviews with data from IMDB. This type of attack
    is called a **linkage attack**, and this exposes the fact that anonymization is
    not enough to protect sensitive data. You can find more information about this
    at [https://en.wikipedia.org/wiki/Netflix_Prize](https://en.wikipedia.org/wiki/Netflix_Prize).
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy solves this problem by adding noises to the dataset used
    in the computation on the dataset, so the original data cannot be easily reverse
    engineered. In addition to protection against linkage attacks, differential privacy
    also helps quantify privacy loss as a result of someone running processing against
    the data. To help understand what this means, let’s look at the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose your organization is a regional bank, and your customer data repository
    contains sensitive data about your customers, including their name, social security
    number, zip code, income, gender, and education. To ensure data privacy, this
    data cannot be freely shared by all departments, such as the marketing department.
    However, the aggregate analysis of the customer data, such as the number of customers
    with income over a threshold, is allowed to be shared. To enable access to the
    aggregated data, a data query tool was built to return only the aggregate data
    (such as count, sum, average, min, and max) to the marketing department. Separately,
    another database contains customer churn data with unique customer IDs, and a
    customer support database contains customer names and unique customer IDs. Both
    the churn database and customer support database are accessible to the marketing
    department. An ill-intentioned analyst wanted to find the names of customers whose
    incomes were above a certain threshold for some personal purpose.
  prefs: []
  type: TYPE_NORMAL
- en: This analyst queried the database one day and found out that out of 4,000 total
    customers, there were 30 customers with incomes over $1 million in a particular
    zip code. A couple of days later, he queried the customer data again and found
    out there were only 29 customers with incomes over $1 million, out of a total
    of 3999 customers. Since he had access to the churn database and customer support
    database, he was able to identify the name of the customer who churned and figured
    out this customer had an income of over $1 million.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent this from happening, the query tool was changed to add a little noise
    (such as adding or removing records) to the result without losing meaningful information
    about the original data. For example, instead of returning the actual result of
    30 customers out of 4,000 customers in the first query, the result of 31 customers
    out of 4001 customers was returned. The second query returns 28 out of 3997 instead
    of the actual 29 out of 3999 figures. This added noise does not significantly
    change the overall magnitude of the summary result, but it makes reverse engineering
    of the original data much more difficult, as now you can not pinpoint a specific
    record. This is the intuition behind how differential privacy works. *Figure 13.1*
    shows the concept of differential privacy, where computation is performed on two
    databases, and noises are added to one of the databases. The goal is to ensure
    **Result 1** and **Result 2** are as close as possible as that’s where it becomes
    harder and harder to tell the difference in distribution between **Result 1**
    and **Result 2** even though the two databases are slightly different. Here, the
    Epsilon (![](img/B20836_13_001.png)) value is the privacy loss budget, which is
    the ceiling of how much probability an output distribution can change when adding/removing
    a record. The smaller the Epsilon value, the lower the privacy loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Differential privacy concept'
  prefs: []
  type: TYPE_NORMAL
- en: ML models are susceptible to privacy attacks. For example, it is possible to
    extract information from trained models that directly map to the original training
    data, as deep learning models may have unintended memorization of training data.
    Also, overfitted models are also likely to memorize training data. Differential
    privacy is one of the techniques that can help minimize the effect of unintended
    memorization. Since differential privacy can make the computational outputs of
    two input datasets (one with sensitive data, one with sensitive data removed)
    almost indistinguishable from a query perspective, the hacker cannot confidently
    infer whether a piece of sensitive data is in the original dataset or not.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to apply differential privacy to ML model training
    such as adding noises to the underlying training data or adding noises to the
    model parameters. Also, it is important to know that differential privacy does
    not come for free. The higher the privacy protection (smaller Epsilon), the lower
    the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy is implemented in TensorFlow Privacy. TensorFlow Privacy
    provides a differentially private optimizer for model training and requires minimum
    code changes. The following code sample shows the syntax of using the `DPKerasSGDOptimizer`
    object for differential privacy training. The main steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the Tensorflow privacy library package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow_privacy` and select your differentially private `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select your `loss` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: PyTorch supports differential privacy with its `opacus` package. It is also
    fairly straightforward to use the `opacus` package to enable differential privacy
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows how to wrap an optimizer in the `PrivacyEngine`
    object, and just use the optimizer the same way in a PyTorch training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Understanding adversarial attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adversarial attacks are a type of attack on ML models that exploit their weaknesses
    and cause them to make incorrect predictions. Imagine you have an ML model that
    can accurately identify pictures of animals. An adversarial attack might manipulate
    the input image of an animal in such a way that the model misidentifies it as
    a different animal.
  prefs: []
  type: TYPE_NORMAL
- en: 'These attacks work by making small, often imperceptible changes to the input
    data that the model is processing. These changes are designed to be undetectable
    by humans but can cause the model to make large errors in its predictions. Adversarial
    attacks can be used to undermine the performance of ML models in a variety of
    settings, including image recognition, speech recognition, and **natural language
    processing** (**NLP**). There are two types of adversarial attack objectives:
    targeted and untargeted. A targeted objective means to make the ML systems predict
    a specific class determined by the attacker, and an untargeted objective simply
    causes the ML systems to misclassify. Adversarial attacks can take many different
    forms, including evasion attacks, data poisoning attacks, and model extraction
    attacks. *Figure 13.2* illustrates the different attacks that an adversary may
    carry out against an ML system.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B20836_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Types of adversarial attacks'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the attacker’s knowledge of the ML systems and their ability to
    access the model and data, an attack can be a white-box attack or a black-box
    attack. The majority of the attacks are white-box attacks, meaning this attack
    assumes you have the total knowledge of the ML models. This means that if you
    want to cause adversarial attacks against a neural network model, you need to
    know all the weights values and the network structure. The opposite of a white-box
    attack is a black-box attack. With a black-box attack, you probe the ML model
    for a number of trials using different inputs, record the results from the ML
    model, and use that information to design an attack against the model.
  prefs: []
  type: TYPE_NORMAL
- en: Evasion attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML evasion attacks are a type of attack where a malicious actor attempts to
    manipulate the input data to evade the detection or classification of an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In an ML evasion attack, the attacker modifies the input data to generate an
    adversarial sample that appears legitimate to a human observer but can cause the
    ML model to produce an incorrect output or misclassify the input data. The goal
    of an ML evasion attack can vary from causing a malfunction in the system to making
    it vulnerable to more severe attacks. The following figure shows that by introducing
    small human unnoticeable noises into the image, the ML model can generate incorrect
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_13_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: Evasion attack'
  prefs: []
  type: TYPE_NORMAL
- en: Evasion attacks have real-world implications. For example, evasion attacks can
    be used against ML-based network intrusion systems to evade detection and allow
    bad actors to access computer networks and exploit application vulnerabilities.
    Evasion attacks can cause the autonomous vehicle perception system to misclassify
    street objects such as stop signs, resulting in potential human safety problems.
    Evasion attacks can also be used to bypass ML-based content moderation solutions
    on social media to introduce banned image content.
  prefs: []
  type: TYPE_NORMAL
- en: Evasion attacks can be launched against various types of ML models, such as
    deep neural networks, decision trees, or support vector machines. These attacks
    can be carried out using different techniques, such as gradient-based methods
    like a **Projected Gradient Descent** (**PGD**) attack or decision-based methods
    like a HopSkipJump attack.
  prefs: []
  type: TYPE_NORMAL
- en: PGD attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, PGD is a gradient-based attack. A gradient-based attack
    uses the gradients of the model’s loss function with respect to the input data
    to find the direction in which the input can be perturbed to achieve a desired
    outcome. PGD is a white-box adversarial attack. It works by perturbing the input
    data in small steps, such that the perturbations are imperceptible to humans,
    but cause the model to misclassify the input.
  prefs: []
  type: TYPE_NORMAL
- en: In a PGD attack, the attacker starts with a clean input, and then adds small
    perturbations to the input to create a perturbed input. It then calculates the
    gradient of the perturbed input and moves in the direction of the gradient until
    it converges while satisfying the loss constraint (e.g., expressed in L² norm)
    and stays within the predefined range of change (often referred to as Epsilon).
    The attacker then projects the perturbed input back onto a feasible set (e.g.,
    the set of inputs that are within a certain distance from the original input),
    to ensure that the perturbations are still imperceptible to humans. This process
    is repeated multiple times, with the perturbations getting smaller each time until
    the model is successfully deceived.
  prefs: []
  type: TYPE_NORMAL
- en: PGD attacks are known to be effective against a wide range of ML models, including
    deep neural networks, and can be used in various applications such as image recognition,
    speech recognition, and NLP. PGD is less computationally intensive. However, PGD
    attacks can also be defended against using techniques such as adversarial training,
    which involves training the model on adversarial examples in addition to clean
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: HopSkipJump attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are black-box attacks, meaning that the attacker does not have access
    to the model’s parameters or internal structure, but only to its input and output.
    The goal of the attack is to modify the input in a way that the model misclassifies
    it while minimizing the number of queries to the model. This attack is a decision-based
    attack, where an attacker attempts to understand the decision boundaries of the
    ML model and then misleads it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HopSkipJump attack combines three types of techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Hop is the technique that generates a sequence of intermediate adversarial examples
    that progressively move towards the target classes while staying within a predefined
    distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip is the technique that skips some of the intermediate steps to reduce the
    number of to the target model, making it more efficient than iterative approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jump is a technique that makes a large jump from the original samples to a new
    starting point that maximizes the difference between the predicted classes and
    original examples, which allows it escape local optima and find new adversarial
    examples that are harder to detect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm starts by generating a set of random starting points around the
    original example. It then applies the hop technique to each starting point to
    generate a sequence of intermediate adversarial examples. The skip technique is
    used to reduce the number of queries to the target model by skipping some of the
    intermediate steps. Finally, the jump technique is used to jump from the original
    example to a new starting point to find new adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: The HopSkipJump attack has been shown to be effective against a wide range of
    ML models, including deep neural networks and decision trees. It has proven to
    be effective even against ML models with strong defenses such as adversarial training
    and preprocessing. It has also been shown to be more efficient than other black-box
    attack methods, requiring fewer queries to the model. This makes it particularly
    concerning as it could potentially be used by attackers with limited access to
    the model, such as through a web interface or a mobile app.
  prefs: []
  type: TYPE_NORMAL
- en: Data poisoning attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data poisoning attacks are a type of adversarial attack where an attacker manipulates
    the training data of an ML model to introduce errors or bias into the model’s
    output.
  prefs: []
  type: TYPE_NORMAL
- en: In a poisoning attack, the attacker injects malicious data into the training
    dataset used to train the ML model. The attacker aims to influence the model’s
    decision-making process by biasing it toward a specific outcome or misclassifying
    certain inputs. This can be achieved by adding or modifying the training data
    to create a biased representation of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of an ML poisoning attack can vary, from causing a malfunction in the
    system to gaining unauthorized access to sensitive information. For example, an
    attacker may manipulate a spam filter to allow certain spam messages to pass through
    undetected or inject malicious code into an ML-based intrusion detection system
    to evade detection.
  prefs: []
  type: TYPE_NORMAL
- en: ML poisoning attacks can be challenging to detect, as they occur during the
    training phase and may not be apparent until the model is deployed in a real-world
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple techniques to launch data poisoning attacks, such as label
    flipping to cause models to learn incorrect associations of input and outputs,
    repetitive data insertion to cause bias against certain classes, and backdoor
    poisoning that injects poisoned samples that cause the model to output a predefined
    result when the backdoor is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: Clean-label backdoor attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One example of a backdoor poisoning attack technique is the clean-label backdoor
    attack, which is a type of adversarial attack on ML models that involves inserting
    a backdoor into the model’s training data. The backdoor is a specific trigger
    pattern that is associated with a particular target label. When the model encounters
    this trigger pattern in a test input, it misclassifies it as the target label,
    regardless of its actual characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other backdoor attacks, a clean-label backdoor attack does not require
    any modification to the model’s architecture or parameters, and the backdoor can
    be hidden within the training data without being noticed. This makes it particularly
    dangerous, as the model appears to be performing well on clean test data, but
    can be easily manipulated by an attacker who knows the trigger pattern.
  prefs: []
  type: TYPE_NORMAL
- en: To launch a clean-label backdoor attack, an attacker typically injects the trigger
    pattern into a small fraction of the training data, while keeping the rest of
    the data unchanged. This can be done by either adding the pattern to existing
    training examples or creating new examples with the pattern. For example, a common
    trigger used in image classification tasks might be a small, white square in the
    bottom right corner of the image. This square might be only a few pixels wide
    and high, but it is enough to trigger the backdoor in the model and cause it to
    output the attacker’s target label. In another example, a trigger for a sentiment
    analysis model might be a specific set of words or phrases that are unlikely to
    appear in normal text, such as a string of numbers or special characters. This
    trigger could be inserted into a small subset of the training data, along with
    a target label indicating a particular sentiment that the attacker wants the model
    to output when it encounters the trigger. The attacker then trains the model on
    the poisoned data, and the model learns to associate the trigger pattern with
    the target label.
  prefs: []
  type: TYPE_NORMAL
- en: To defend against this type of attack, researchers have proposed various methods,
    such as data filtering to detect and remove poisoned data, model pruning to identify
    and remove backdoor neurons, or incorporating randomness into the training process
    to make the model more robust to backdoor attacks. However, these methods are
    not foolproof and are still an active area of research.
  prefs: []
  type: TYPE_NORMAL
- en: Model extraction attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML models are often deemed confidential as many ML models are trained using
    proprietary data and algorithms and can have significant commercial or non-commercial
    values. As ML as a service becomes increasingly popular as a new business model
    and revenue stream, the risk of losing models to adversaries increases. The consequences
    of model loss can be severe, as the attacker can use the stolen model for malicious
    purposes, such as impersonation or reverse engineering. For example, the attacker
    could use the stolen model to steal IP addresses and create a competing service
    or to launch a similar but malicious service.
  prefs: []
  type: TYPE_NORMAL
- en: A model extraction attack is a type of black-box attack on ML models where an
    attacker attempts to extract or replicate the model by training a new model based
    on its predictions or by analyzing its output. This attack is particularly dangerous
    for models that are deployed in the cloud or provided as a service, where the
    attacker can interact with the model and collect its output via public APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B20836_13_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Model extraction attack'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model extraction attack works by querying the model with input data, collecting
    the output, and then using this information to train a new surrogate model that
    closely mimics the original model’s behavior. There are two classes of attacks:
    the accuracy model extraction attack, where the objective is to gain similar or
    better performance in the attack model, and the fidelity model extraction attack,
    where the goal is to faithfully reproduce the prediction of the target model.'
  prefs: []
  type: TYPE_NORMAL
- en: Many ML models are vulnerable to model extraction attacks including both traditional-algorithms-based
    and neural-network-based ML models. For example, an adversary can attack an API
    based on a logistic regression model using the equation-solving approach, where
    an attacker can develop a set of equations to solve the parameters of a logistic
    regression model directly after obtaining a set of input, output values, and confidence
    scores from the API.
  prefs: []
  type: TYPE_NORMAL
- en: For APIs that use neural network-based models such as BERT, an attacker can
    send a number of queries to the model and use the input and output to reconstruct
    a local copy of the models using techniques such as fine-tuning the publicly released
    BERT models. After a model is reconstructed, an attacker can also use the new
    model to generate more dangerous white-box evasion attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Attacks against generative AI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI models, particularly **large language models** (**LLMs**), face
    vulnerabilities similar to those discussed in the context of other ML models.
    However, the interactive and prompt-based nature of these generative models has
    introduced additional attack surfaces, offering adversaries opportunities for
    exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt injection emerges as a notable attack vector, involving the manipulation
    of prompts to elicit specific and potentially malicious outputs from LLMs. Adversaries
    employ prompt injections to fool LLMs into generating content beyond the intended
    scope, posing significant risks to the managing organization. These attacks have
    the potential to influence AI system actions, expose sensitive data, or execute
    harmful operations. There are three primary types of prompt injection attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt hijacking**: This attack redirects the LLMs to an alternate task or
    output by inserting commands that override the initial prompt, providing new instructions
    for the LLM to follow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt leakage**: This attack manipulates LLMs to reveal the original instructions
    programmed by the developer through straightforward prompts, such as requesting
    the initial sentences generated by the LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jailbreaks**: This attack attempts to bypass governance features applied
    to LLMs, allowing the generation of otherwise restricted content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These prompt injection attacks exploit the inherent vulnerabilities in the language
    models’ capacity to interpret and generate open-ended text based on prompts. Despite
    the implementation of various safeguards and filtering mechanisms by researchers
    and developers, adversaries persistently seek weaknesses to bypass these defenses.
  prefs: []
  type: TYPE_NORMAL
- en: Defense against adversarial attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To mitigate the risks associated with adversarial attacks, researchers have
    developed various defense mechanisms against certain adversarial attacks. These
    mechanisms aim to increase the robustness of the models and detect and reject
    adversarial inputs, as we’ll see now in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is one of the key defense mechanisms is to increase the robustness of the
    ML models, and there are several techniques to achieve this, such as adversarial
    training and defensive distillation.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adversarial training is a method that involves training models using adversarial
    examples. As mentioned previously, adversarial examples are inputs designed specifically
    to fool a trained model. The intuition behind this method is that by training
    the models with adversarial examples, the ML models learn to recognize these samples
    and thus make the models more robust against these examples in making the right
    predictions. During adversarial training, the ML models are trained with a mix
    of good examples and adversarial examples with the right label and learn to generalize
    features with small perturbations in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training has shown to be effective against some common adversarial
    attacks, such as evasion attacks and data poisoning attacks. However, adversarial
    training is computationally intensive and might not work against all adversarial
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Defense distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea behind defense distillation is to train a simplified version of the
    original model. During defense distillation training, a distilled model is trained
    to predict the output probability of the original model.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, when training the original classification model, hard class
    labels are used to maximize the accuracy of the model. The trained model is then
    used to predict the class labels for a training dataset along with the confidence
    probability of the predictions. The distilled model is then trained using the
    training dataset using the probability as the output instead of the hard class
    labels. The main reason that defense distillation works is that it helps reduce
    the sensitivity of the model’s decision boundary to small input data perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: This approach has demonstrated the distilled models are far more robust to adversarial
    inputs because uncertainty (probability) is used in the model training.
  prefs: []
  type: TYPE_NORMAL
- en: Detector-based method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Detecting and rejecting adversarial examples is another defense approach against
    adversarial attacks. This method trains a detector to distinguish between adversarial
    examples and clean examples and reject adversarial examples before it is fed into
    the real models. There are multiple techniques for building a detector, including
    a classifier-based technique, a threshold-based technique, and a statistic-based
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: Classifier-based detector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An adversarial classifier detector is a type of model that is designed to detect
    adversarial examples by classifying them as either clean or adversarial. The detector
    is trained on a combination of clean and adversarial examples, where the adversarial
    examples are generated using various attack methods. During training, a detector
    learns to differentiate between clean and adversarial examples based on their
    characteristics. When presented with a new input, the detector outputs a classification
    indicating whether the input is clean or adversarial. Adversarial binary classifier
    detectors can be effective at detecting adversarial examples because they are
    specifically designed to do so, and can be trained to be robust to a wide range
    of attack methods. However, like any detection method, adversarial binary classifier
    detectors are not foolproof and can be evaded by attackers who are aware of their
    limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Threshold-based detector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The autoencoder is a type of unsupervised ML technique that aims to reconstruct
    inputs as outputs while minimizing the reconstruction error. Its underlying concept
    is based on the assumption that clean data will result in small reconstruction
    errors, while adversarial examples will produce higher errors. As a threshold-based
    detector, the autoencoder model is trained using clean examples to minimize reconstruction
    errors. Later, when new inputs are fed into the trained model, it calculates the
    reconstruction error and uses it as a score. If the score exceeds a certain threshold,
    the detector can classify the input as an adversarial example.
  prefs: []
  type: TYPE_NORMAL
- en: Statistic-based detector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A statistics-based detector aims to detect adversarial examples by analyzing
    the statistical property of the input data. The assumption is that adversarial
    examples have different statistical properties than clean examples. For example,
    the distribution of adversarial examples will be different than the distribution
    of clean examples, and using statistic techniques to analyze whether an example
    is out-of-distribution from the distribution of clean examples can help detect
    adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: There are several techniques for detecting data distribution changes, including
    **out-of-distribution** (**OOD**) detection to identify inputs that are dissimilar
    from the training data and direct statistical properties comparison with clean
    training data to detect statistical differences.
  prefs: []
  type: TYPE_NORMAL
- en: Open-source tools for adversarial attacks and defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To defend against the threat of adversarial attacks, a range of open-source
    adversarial tools have been developed, providing researchers and practitioners
    with tools to test, defend, and improve the robustness of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of such tools include the IBM **Adversarial Robustness Toolbox** (**ART**)
    and Foolbox. These tools provide a comprehensive set of algorithms and techniques
    for generating and defending against adversarial attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IBM ART**: The IBM ART is an open-source software library developed by IBM
    Research to help researchers and practitioners defend against adversarial attacks
    on ML models. It provides a comprehensive set of tools and algorithms to support
    the development and deployment of robust ML systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ART library includes various components, such as adversarial attack and
    defense techniques, model verification methods, and benchmark datasets. It supports
    multiple ML frameworks, including TensorFlow, PyTorch, and Keras, and can be used
    with a variety of models, including deep neural networks, decision trees, and
    support vector machines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CleverHans**: CleverHans is an open-source software library developed by
    Ian Goodfellow and Nicolas Papernot to help researchers and practitioners test
    the security and robustness of ML models against adversarial attacks. It provides
    a range of tools and algorithms to generate adversarial examples that can be used
    to evaluate the performance and robustness of ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CleverHans includes a variety of adversarial attack techniques, such as the
    fast gradient sign method, Jacobian-based saliency map approach, and elastic net
    attacks. It supports several ML frameworks, including TensorFlow, PyTorch, and
    JAX, and can be used with a variety of models, including deep neural networks,
    decision trees, and support vector machines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Foolbox**: Foolbox is an open-source software library developed by researchers
    at ETH Zurich to help researchers and practitioners test the robustness of ML
    models against adversarial attacks. It provides a comprehensive set of algorithms
    and techniques for generating and testing adversarial examples, as well as benchmarking
    the performance of ML models against various attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foolbox supports multiple ML frameworks, including PyTorch, TensorFlow, and
    Keras, and can be used with a variety of models, including deep neural networks
    and decision trees. It includes a variety of adversarial attack techniques, such
    as the fast gradient sign method, PGD, and Carlini and Wagner’s L² attack, as
    well as several defense techniques, such as input preprocessing and adversarial
    training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**TextAttack**: TextAttack is an open-source Python library developed by researchers
    at the University of Maryland to help researchers and practitioners test the robustness
    of NLP models against adversarial attacks. It provides a range of tools and techniques
    for generating and testing adversarial examples for NLP models, as well as benchmarking
    their performance against various attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TextAttack supports a variety of NLP tasks, including text classification, sentiment
    analysis, and textual entailment, and can be used with a range of pre-trained
    models, including BERT, GPT-2, and RoBERTa. It includes a variety of adversarial
    attack techniques, such as word substitution, word deletion, and paraphrasing,
    as well as several defense techniques, such as input sanitization and adversarial
    training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**RobustBench**: RobustBench is a benchmarking platform for evaluating the
    robustness of ML models against adversarial attacks. It was developed by researchers
    at the University of Tübingen and is maintained by a consortium of researchers
    from universities and research institutions around the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RobustBench provides a standardized framework for evaluating the robustness
    of ML models across a range of tasks, including image classification, object detection,
    and semantic segmentation. It includes a range of adversarial attacks and evaluation
    metrics to ensure that the performance of models is rigorously evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: The battle against adversarial attacks on ML models is an ongoing arms race.
    As new attack techniques are developed, researchers and practitioners are forced
    to devise novel defense mechanisms and mitigation methods to counter these threats.
  prefs: []
  type: TYPE_NORMAL
- en: On the attack front, adversaries are continuously exploring more sophisticated
    and efficient ways to craft adversarial examples that can evade existing defenses.
    In response, the research community has proposed various mitigation strategies,
    including adversarial training, and input preprocessing. However, many of these
    methods have their own limitations and trade-offs, such as decreased model performance
    or increased computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the war against adversarial attacks is a continuous cycle of innovation
    and adaptation. As our understanding of these attacks deepens and new techniques
    are developed, we may gain temporary advantages, but the adversaries will likely
    adapt and find new vulnerabilities to exploit. Maintaining the security and trustworthiness
    of ML systems will require a sustained effort from the research community, as
    well as a proactive approach to risk assessment and defense deployment in real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on lab – detecting bias, explaining models, training privacy-preserving
    mode, and simulating adversarial attack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a comprehensive system for ML governance is a complex initiative. In
    this hands-on lab, you will learn to use some of SageMaker’s built-in functionalities
    to support certain aspects of ML governance.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an ML solutions architect, you have been assigned to identify technology
    solutions to support a project that has regulatory implications. Specifically,
    you need to determine the technical approaches for data bias detection, model
    explainability, and privacy-preserving model training. Follow these steps to get
    started.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting bias in the training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Launch the SageMaker Studio environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the same SageMaker Studio environment that you have been using.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new folder called `Chapter13`. This will be our working directory for
    this lab. Create a new Jupyter notebook and name it `bias_explainability.ipynb`.
    Choose the `Python 3 (ipykernel)` kernel when prompted.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new folder called `data` under the `chapter13` folder. We will use
    this folder to store our training and testing data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use the customer churn data (`churn.csv`) that we used in earlier chapters.
    If don’t have it, you can access it from here: [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter13/data](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter13/data).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the data to your local directory and then upload both files to the
    newly created `data` directory.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the `sagemaker` environment using the following code block, where
    we set up variables for the S3 bucket and prefix location, obtain the execution
    IAM role for running the various functions, and get a handle to the S3 client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data from the data directory and display the first few rows. The `Exited`
    column is the target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into train and test sets using an 80/20 split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Process the data for the SageMaker XGBoost model, which needs the target to
    be in the first column, and save the files to the data directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Kick off the model training using the SageMaker XGBoost container as we are
    training a classification model with the tabular dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a model from the training job to be used with SageMaker Clarify later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the Clarify processor for running bias detection and explainability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the configuration for the input data location, output path for the
    report, target class label, and dataset type, which are required for the Clarify
    `DataConfig` class. Here, we use the training data and indicate the target column
    for the analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the model configuration for `model_name` and compute instances for
    the Clarify processing job. A shadow endpoint will be created temporarily for
    the Clarify processing job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the threshold. This is the threshold for labeling the prediction. Here,
    we are specifying that the label is 1 if the probability is `0.8`. The default
    value is `0.5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify which feature we want to detect the bias for using the `BiasConfig`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to run the Clarify bias detection job. You should see the
    job status and bias analysis detail in the output of the cell. The report provides
    various bias metrics for the `Gender` feature column against the `Existed` prediction
    target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Explaining feature importance for a trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will use SageMaker Clarify to help explain the model using feature
    importance. Specifically, SageMaker Clarify uses SHAP to explain the prediction.
    SHAP works by computing the contribution of each feature to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue to use the notebook we have created for bias detection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the SHAP configuration. Here, `number_samples` is the number of synthetic
    data points to be generated for computing the SHAP value, and `baseline` is the
    list of rows in the dataset for baseline calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Specify the data configuration for the explainability job. Here, we provide
    details such as the input training data, and `output_path` for the report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we run the job to generate the report. You will see the job status
    and final report directly inside the notebook output cell. Here, Clarify computes
    the global feature importance, which means it takes all the inputs and their predictions
    into account for calculating the contribution of each feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, you will see that age is the most important feature to influence
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Training privacy-preserving models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this part of the hands-on lab, you will learn how to use differential privacy
    for privacy-preserving model training:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new folder called `differential privacy` under the `chapter 13` folder.
    Download this notebook at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb),
    and upload it to the newly created `differential privacy` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run all the cells in the notebook, and take note of the training losses at the
    end. We are not going to explain all the details in this notebook, as it simply
    trains the simple neural network using the same churn dataset we have been using.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we modify this notebook to implement differential privacy model training
    using the PyTorch `opacus` package. You can also download the modified notebook
    at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the parameters for the `opacus` `PrivacyEngine` object. Here, `noise_multiplier`
    is the ratio of the standard deviation of Gaussian noise to the sensitivity of
    the function to add noise to, and `max_per_sample_grad_norm` is the maximum norm
    value for gradients. Any value greater than this norm value will be clipped. The
    `sample_rate` value is used for figuring out how to build batches for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we wrap the privacy engine around the model and optimizer and kick off
    the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compare the training loss with the training losses you observed earlier without
    the privacy engine, and you will notice small degradations in the losses across
    all epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s measure the potential privacy loss with this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see values for ![](img/B20836_13_001.png). As we discussed earlier,
    ![](img/B20836_13_001.png) is the privacy loss budget, which measures the probability
    an output can change by adding or removing one record from the training data.
    ![](img/B20836_13_004.png) is the probability of failure that information is accidentally
    leaked.
  prefs: []
  type: TYPE_NORMAL
- en: Simulate a clean-label backdoor attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this last part of the lab, you will learn to simulate a clean-label backdoor
    data poisoning attack using the ART library package. Specifically, we will use
    the ART library to generate a small percentage of poison training data that can
    act as triggers to cause the model to make wrong predictions, while keeping the
    overall score high for the regular clean test data validation to fool people into
    believing it is a good working model:'
  prefs: []
  type: TYPE_NORMAL
- en: Download [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/clean_label_backdoor.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/clean_label_backdoor.ipynb).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the notebook to a working directory in the Studio Notebook environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the instructions in the notebook to complete the lab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have successfully used SageMaker to detect data and model
    bias, learned about feature importance for a model, and trained a model using
    differential privacy. All these capabilities are highly relevant for ML governance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter delved deeply into various AI risk topics and techniques, including
    bias, explainability, privacy, and adversarial attacks. Additionally, you should
    be familiar with some of the technology capabilities offered by AWS to facilitate
    model risk management processes, such as detecting bias and model drift. Through
    the lab section, you gained hands-on experience with utilizing SageMaker to implement
    bias detection, model explainability, and privacy-preserving model training.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to the ML adoption journey and
    how organizations should think about charting a path to achieve ML maturity.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code7020572834663656.png)'
  prefs: []
  type: TYPE_IMG
