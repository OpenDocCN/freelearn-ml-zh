<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Neural Networks</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Building a perceptron</li>
<li>Building a single layer neural network</li>
<li>Building a deep neural network</li>
<li>Creating a vector quantizer</li>
<li>Building a recurrent neural network for sequential data analysis</li>
<li>Visualizing the characters in an OCR database</li>
<li>Building an optical character recognizer using neural networks</li>
<li>Implementing optimization algorithms in ANN</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To address the recipes in this chapter, you need the following files (available on GitHub):</p>
<ul>
<li><kbd><span>perceptron.</span><span>py</span></kbd></li>
<li><kbd><span>single_layer.</span><span>py</span></kbd></li>
<li><kbd><span>data_single_layer.txt</span></kbd></li>
<li><kbd><span>deep_neural_network.</span><span>py</span></kbd></li>
<li><kbd><span>vector_quantization.</span><span>py</span></kbd></li>
<li><kbd><span>data_vq.txt</span></kbd></li>
<li><kbd><span>recurrent_network.</span><span>py</span></kbd></li>
<li><kbd><span>visualize_characters.</span><span>py</span></kbd></li>
<li><kbd><span>ocr.</span><span>py</span></kbd></li>
<li><kbd>IrisClassifier.py</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Our brain is really good at identifying and recognizing things. We want machines to be able to do the same. A neural network is a framework that is modeled after the human brain to simulate our learning processes. Neural networks are designed to learn from data and recognize the underlying patterns. As with all learning algorithms, neural networks deal with numbers. Therefore, if we want to achieve any real-world task involving images, text, sensors, and so on, we have to convert them into a numerical format before we feed them into a neural network. We can use a neural network for classification, clustering, generation, and many other related tasks.</p>
<p>A neural network consists of layers of <strong>neurons</strong>. These neurons are modeled after the biological neurons in the human brain. Each layer is basically a set of independent neurons that are connected to the neurons on adjacent layers. The input layer corresponds to the input data that we provide, and the output layer consists of the output that we desire. All the layers in between are called <strong>hidden layers</strong>. If we design a neural network with more hidden layers, then we give it more freedom to train itself with higher accuracy.</p>
<p>Let's say that we want the neural network to classify data, based on our needs. For a neural network to work accordingly, we need to provide labeled training data. The neural network will then train itself by optimizing the <kbd>cost</kbd> function. This <kbd>cost</kbd> function is the error between actual labels and the predicted labels from the neural network. We keep iterating until the error goes below a certain threshold.</p>
<p>What exactly are <em>deep</em> neural networks? Deep neural networks are neural networks that consist of many hidden layers. In general, this falls under the realm of deep learning. This is a field that is dedicated to the study of these neural networks, composed of multiple layers that are used across many verticals.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a perceptron</h1>
                </header>
            
            <article>
                
<p>Let's start our neural network adventure with a <strong>perceptron</strong>. A perceptron is a single neuron that performs all the computations. It is a very simple model, but it forms the basis of building up complex neural networks. The following is what it looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1080 image-border" src="assets/19e75f7c-8bf7-484a-94c4-0cc7e2addc9e.png" style="width:22.33em;height:6.75em;"/></p>
<p>The neuron combines inputs using different weights, and it then adds a bias value to compute the output. It's a simple linear equation relating input values to the output of the perceptron.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> </span><span>use a library called </span><kbd>neurolab</kbd> to define a perceptron with two inputs. Before you proceed, make sure that you install it. You can find the installation instructions at <a href="https://pythonhosted.org/neurolab/install.html">https://pythonhosted.org/neurolab/install.html</a>. Let's go ahead and look at how to design and develop this neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a perceptron:</p>
<ol>
<li>Create a new Python file, and import the following packages <span>(the full code is given in the</span> <kbd>perceptron.py</kbd><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import neurolab as nl 
import matplotlib.pyplot as plt </pre>
<ol start="2">
<li>Define some input data and its corresponding labels:</li>
</ol>
<pre style="padding-left: 60px"># Define input data 
data = np.array([[0.3, 0.2], [0.1, 0.4], [0.4, 0.6], [0.9, 0.5]]) 
labels = np.array([[0], [0], [0], [1]])</pre>
<ol start="3">
<li>Let's plot this data to see where the datapoints are located:</li>
</ol>
<pre style="padding-left: 60px"># Plot input data 
plt.figure() 
plt.scatter(data[:,0], data[:,1]) 
plt.xlabel('X-axis') 
plt.ylabel('Y-axis') 
plt.title('Input data')</pre>
<ol start="4">
<li>Let's define a <kbd>perceptron</kbd> with two inputs. This function also needs us to specify the minimum and maximum values in the input data:</li>
</ol>
<pre style="padding-left: 60px"># Define a perceptron with 2 inputs; 
# Each element of the list in the first argument  
# specifies the min and max values of the inputs 
perceptron = nl.net.newp([[0, 1],[0, 1]], 1) </pre>
<ol start="5">
<li>Let's train the <kbd>perceptron</kbd> model:</li>
</ol>
<pre style="padding-left: 60px"># Train the perceptron 
error = perceptron.train(data, labels, epochs=50, show=15, lr=0.01) </pre>
<ol start="6">
<li>Let's plot the results, as follows:</li>
</ol>
<pre style="padding-left: 60px"># plot results 
plt.figure() 
plt.plot(error) 
plt.xlabel('Number of epochs') 
plt.ylabel('Training error') 
plt.grid() 
plt.title('Training error progress') 
 
plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will see two diagrams. The first diagram displays the input data, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1081 image-border" src="assets/7821b056-2d66-47fa-9a10-3a9a0414c7cb.png" style="width:31.75em;height:24.50em;"/></p>
<p style="padding-left: 60px">The second diagram displays the training error progress, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1082 image-border" src="assets/87192fdb-445b-4215-bc1f-fb57351dfd96.png" style="width:33.00em;height:25.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we</span><span> </span><span>used a single neuron that performs all the computations. To train the <kbd>perceptron</kbd>, the following parameters are set. </span><span>The number of epochs specifies the number of complete passes through our training dataset. The </span><kbd>show</kbd><span> parameter specifies how frequently we want to display the progress. The </span><kbd>lr</kbd><span> parameter specifies the learning rate of the <kbd>perceptron</kbd>. It is the step size for when the algorithm searches through the parameter space. If this is large, then the algorithm may move faster, but it might miss the optimum value. If this is small, then the algorithm will hit the optimum value, but it will be slow. So, it's a trade-off; hence, we choose a value of </span><kbd>0.01</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>We can understand a <kbd>perceptron</kbd> concept as anything that takes multiple inputs and produces one output. This is the simplest form of a neural network. The <kbd>perceptron</kbd> concept was suggested by Frank Rosenblatt in 1958 as an object with an input and output layer and a learning rule targeted at minimizing errors. This learning function called <strong>error backpropagation</strong> changes connective weights (synapses) relying on the actual output of the network, with respect to a given input, as the difference between the actual output and the desired output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of the<span> </span><kbd>neurolab</kbd> library: <a href="https://pythonhosted.org/neurolab/">https://pythonhosted.org/neurolab/</a></li>
<li><em><span>Refer to </span>A Basic Introduction to Neural Networks</em> (from the University of Wisconsin-Madison): <a href="http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html">http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a single layer neural network</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, <em>Building a perceptron</em>, we learned how to create a <kbd>perceptron</kbd>; now let's create a single layer neural network. A single layer neural network consists of multiple neurons in a single layer. Overall, we will have an input layer, a hidden layer, and an output layer, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1083 image-border" src="assets/a6f756e8-e5a6-459a-b929-cba0f2cd0c76.png" style="width:17.08em;height:16.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> learn how to create a single layer neural network using the <kbd>neurolab</kbd> library.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a single layer neural network:</p>
<ol>
<li>Create a new Python file, and import the following packages <span>(the full code is given in the</span> <kbd>single_layer.py</kbd><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
import neurolab as nl  </pre>
<ol start="2">
<li>We will use the data in the <kbd>data_single_layer.txt</kbd> file. Let's load this:</li>
</ol>
<pre style="padding-left: 60px"># Define input data 
input_file = 'data_single_layer.txt' 
input_text = np.loadtxt(input_file) 
data = input_text[:, 0:2] 
labels = input_text[:, 2:]</pre>
<ol start="3">
<li>Let's plot the input data:</li>
</ol>
<pre style="padding-left: 60px"># Plot input data 
plt.figure() 
plt.scatter(data[:,0], data[:,1]) 
plt.xlabel('X-axis') 
plt.ylabel('Y-axis') 
plt.title('Input data') </pre>
<ol start="4">
<li>Let's extract the minimum and maximum values:</li>
</ol>
<pre># Min and max values for each dimension 
x_min, x_max = data[:,0].min(), data[:,0].max() 
y_min, y_max = data[:,1].min(), data[:,1].max() </pre>
<ol start="5">
<li>Let's define a single layer neural network with two neurons in the hidden layer:</li>
</ol>
<pre style="padding-left: 60px"># Define a single-layer neural network with 2 neurons; 
# Each element in the list (first argument) specifies the  
# min and max values of the inputs 
single_layer_net = nl.net.newp([[x_min, x_max], [y_min, y_max]], 2) </pre>
<ol start="6">
<li>Train the neural network for 50 epochs:</li>
</ol>
<pre style="padding-left: 60px"># Train the neural network 
error = single_layer_net.train(data, labels, epochs=50, show=20, lr=0.01) </pre>
<ol start="7">
<li>Plot the results, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plot results 
plt.figure() 
plt.plot(error) 
plt.xlabel('Number of epochs') 
plt.ylabel('Training error') 
plt.title('Training error progress') 
plt.grid() 
plt.show() </pre>
<ol start="8">
<li>Let's test the neural network on new test data:</li>
</ol>
<pre style="padding-left: 60px">print(single_layer_net.sim([[0.3, 4.5]]))<br/>print(single_layer_net.sim([[4.5, 0.5]]))<br/>print(single_layer_net.sim([[4.3, 8]]))</pre>
<p style="padding-left: 60px">If you run this code, you will see two diagrams. The first diagram displays the input data, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1084 image-border" src="assets/8dbcff17-c510-4d99-97a2-c1992e4688e2.png" style="width:32.50em;height:25.67em;"/></p>
<p style="padding-left: 60px">The second diagram displays the training error progress, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1085 image-border" src="assets/ec7f2ad0-2672-48db-8566-34e3e3d0a4e9.png" style="width:31.75em;height:25.00em;"/></p>
<p style="padding-left: 60px">You will see the following printed on your Terminal, indicating where the input test points belong:</p>
<pre>    <strong>[[ 0.  0.]]</strong>
    <strong>[[ 1.  0.]]</strong>
    <strong>[[ 1.  1.]]</strong>
  </pre>
<p style="padding-left: 60px">You can verify that the outputs are correct based on our labels.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>A single layer neural network has the following architecture: the inputs form the input layer, the middle layer that performs the processing is called the hidden layer, and the outputs form the output layer. The hidden layer can convert the input to the desired output. Understanding the hidden layer requires knowledge of weights, bias, and activation functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Weights are vital to convert an input so it impacts the output; they are numerical parameters that monitor how all of the neurons affect the others. The related concept resembles the slope in linear regression, where a weight is multiplied to the input to add up and form the output. </p>
<p class="mce-root">Bias is similar to the intercept added to a linear equation. It is also an additional parameter that is used to regulate the output along with the weighted sum of the inputs to the neuron.</p>
<p class="mce-root">An activation function is a mathematical function that converts the input to an output and determines the total signal a neuron receives. Without activation functions, neural networks would behave like linear functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of the<span> </span><kbd>neurolab</kbd> library: <a href="https://pythonhosted.org/neurolab/">https://pythonhosted.org/neurolab/</a></li>
<li><em><span>Refer to </span>Introduction to Neural Networks</em> (from Yale University): <a href="http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf">http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a deep neural network</h1>
                </header>
            
            <article>
                
<p>We are now ready to build a <strong>deep neural network</strong>. A deep neural network consists of an input layer, many hidden layers, and an output layer. This looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1086 image-border" src="assets/5cf925a1-60cd-4cab-8bbc-6db301b0df88.png" style="width:22.67em;height:12.58em;"/></p>
<p>The preceding diagram depicts a multilayer neural network with one input layer, one hidden layer, and one output layer. In a deep neural network, there are many hidden layers between the input and output layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> build a deep neural network. Deep learning forms an advanced neural network with numerous hidden layers. Deep learning is a vast subject and is an important concept in building AI. In this recipe, we will use generated training data and define a multilayer neural network with two hidden layers.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a deep neural network:</p>
<ol>
<li>Create a new Python file, and import the following packages <span>(the full code is given in the</span> <kbd>deep_neural_network.py</kbd><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import neurolab as nl 
import numpy as np 
import matplotlib.pyplot as plt</pre>
<ol start="2">
<li>Let's define parameters to generate some training data:</li>
</ol>
<pre style="padding-left: 60px"># Generate training data 
min_value = -12 
max_value = 12 
num_datapoints = 90</pre>
<ol start="3">
<li>This training data will consist of a function we define that will transform the values. We expect the neural network to learn this on its own, based on the input and output values that we provide:</li>
</ol>
<pre style="padding-left: 60px">x = np.linspace(min_value, max_value, num_datapoints) 
y = 2 * np.square(x) + 7 
y /= np.linalg.norm(y) </pre>
<ol start="4">
<li>Reshape the arrays:</li>
</ol>
<pre style="padding-left: 60px">data = x.reshape(num_datapoints, 1) 
labels = y.reshape(num_datapoints, 1) </pre>
<ol start="5">
<li>Plot the input data:</li>
</ol>
<pre>plt.figure() 
plt.scatter(data, labels) 
plt.xlabel('X-axis') 
plt.ylabel('Y-axis') 
plt.title('Input data') </pre>
<ol start="6">
<li>Define a deep neural network with two hidden layers, where each hidden layer consists of 10 neurons and the output layer consists of one neuron:</li>
</ol>
<pre style="padding-left: 60px">multilayer_net = nl.net.newff([[min_value, max_value]], [10, 10, 1]) </pre>
<ol start="7">
<li>Set the training algorithm to gradient descent:</li>
</ol>
<pre style="padding-left: 60px">multilayer_net.trainf = nl.train.train_gd </pre>
<ol start="8">
<li>Train the network:</li>
</ol>
<pre style="padding-left: 60px">error = multilayer_net.train(data, labels, epochs=800, show=100, goal=0.01) </pre>
<ol start="9">
<li>Predict the output for the training inputs to see the performance:</li>
</ol>
<pre style="padding-left: 60px">predicted_output = multilayer_net.sim(data)</pre>
<ol start="10">
<li>Plot the training error:</li>
</ol>
<pre style="padding-left: 60px">plt.figure() 
plt.plot(error) 
plt.xlabel('Number of epochs') 
plt.ylabel('Error') 
plt.title('Training error progress')</pre>
<ol start="11">
<li>Let's create a set of new inputs and run the neural network on them to see how it performs:</li>
</ol>
<pre style="padding-left: 60px">x2 = np.linspace(min_value, max_value, num_datapoints * 2) 
y2 = multilayer_net.sim(x2.reshape(x2.size,1)).reshape(x2.size) 
y3 = predicted_output.reshape(num_datapoints) </pre>
<ol start="12">
<li>Plot the outputs:</li>
</ol>
<pre style="padding-left: 60px">plt.figure() 
plt.plot(x2, y2, '-', x, y, '.', x, y3, 'p') 
plt.title('Ground truth vs predicted output') 
plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will see three diagrams. The first diagram displays the input data, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1087 image-border" src="assets/a0972568-3b7d-43f1-a2c5-b7aa5b1187e0.png" style="width:30.00em;height:22.83em;"/></p>
<p style="padding-left: 60px">The second diagram displays the training error progress, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1088 image-border" src="assets/69be89c5-c8d1-4152-b4e1-893706177089.png" style="width:31.00em;height:24.33em;"/></p>
<p style="padding-left: 60px">The third diagram displays the output of the neural network, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1089 image-border" src="assets/4ed36633-eb8a-4f62-a725-8c21677a2025.png" style="width:30.83em;height:23.58em;"/></p>
<p style="padding-left: 60px">You will see the following on your Terminal:</p>
<pre style="padding-left: 60px"><strong>Epoch: 100; Error: 4.634764957565494;</strong><br/><strong>Epoch: 200; Error: 0.7675153737786798;</strong><br/><strong>Epoch: 300; Error: 0.21543996465118723;</strong><br/><strong>Epoch: 400; Error: 0.027738499953293118;</strong><br/><strong>Epoch: 500; Error: 0.019145948877988192;</strong><br/><strong>Epoch: 600; Error: 0.11296232736352653;</strong><br/><strong>Epoch: 700; Error: 0.03446237629842832;</strong><br/><strong>Epoch: 800; Error: 0.03022668735279662;</strong><br/><strong>The maximum number of train epochs is reached</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we will use generated training data to train a multilayer deep neural network with two hidden layers. To train the model, the gradient descent algorithm was used. <strong>Gradient descent</strong> is an iterative approach used for error correction in any learning model. A gradient descent approach is the process of iterating updating weights and biases with the error times derivative of the activation function (backpropagation). In this approach, the steepest descent step size is substituted by a similar size from the previous step. Gradient is the slope of the curve, as it is the derivative of the activation function.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The objective of finding the gradient descent at every step is to find the global cost minimum, where the error is the lowest. And this is where the model has a good fit for the data, and predictions are more accurate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to t</span>he official documentation of<span> the </span><kbd>neurolab</kbd> library: <a href="https://pythonhosted.org/neurolab/lib.html">https://pythonhosted.org/neurolab/lib.html</a></li>
<li>Some notes on gradient descent (by Marc Toussaint from Stuttgart University): <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf">https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a vector quantizer</h1>
                </header>
            
            <article>
                
<p>You can use neural networks for vector quantization as well. <strong>Vector quantization</strong> is the <em>N</em>-dimensional version of rounding off. This is very commonly used across multiple areas in computer vision, NLP, and machine learning in general. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>In previous recipes, we have already addressed <strong>v</strong></span><strong>ector quantization<span> </span></strong><span>concepts: </span><em>Compressing an image using vector quantization</em> and <em>Creating features using visual Codebook and vector quantization</em>. In this recipe, we will<span> define a neural network with two layers—10 neurons in input layer and 4 neurons in the output layer. Then we will use this network to divide the space into four regions.</span></p>
<p>Before starting, you need to make a change to fix a library bug. You need to open the following file: <kbd>neurolab | net.py</kbd>. Then find the following:</p>
<pre>inx = np.floor (cn0 * pc.cumsum ()). astype (int)</pre>
<p class="mce-root">Replace the preceding line with the following:</p>
<pre>inx = np.floor (cn0 * pc.cumsum ())</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to create a vector quantizer:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the </span><kbd>vector_quantization.py</kbd><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
import neurolab as nl </pre>
<ol start="2">
<li>Let's load the input data from the <kbd>data_vq.txt</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">input_file = 'data_vq.txt' 
input_text = np.loadtxt(input_file) 
data = input_text[:, 0:2] 
labels = input_text[:, 2:]</pre>
<ol start="3">
<li>Define a <strong>learning vector quantization</strong> (<strong>LVQ</strong>) neural network with two layers. The array in the last parameter specifies the percentage weightage for each output (they should add up to 1):</li>
</ol>
<pre style="padding-left: 60px">net = nl.net.newlvq(nl.tool.minmax(data), 10, [0.25, 0.25, 0.25, 0.25]) </pre>
<ol start="4">
<li>Train the LVQ neural network:</li>
</ol>
<pre style="padding-left: 60px">error = net.train(data, labels, epochs=100, goal=-1) </pre>
<ol start="5">
<li>Create a grid of values for testing and visualization:</li>
</ol>
<pre style="padding-left: 60px">xx, yy = np.meshgrid(np.arange(0, 8, 0.2), np.arange(0, 8, 0.2)) 
xx.shape = xx.size, 1 
yy.shape = yy.size, 1 
input_grid = np.concatenate((xx, yy), axis=1) </pre>
<ol start="6">
<li>Evaluate the network on this grid:</li>
</ol>
<pre style="padding-left: 60px">output_grid = net.sim(input_grid) </pre>
<ol start="7">
<li>Define the four classes in our data:</li>
</ol>
<pre style="padding-left: 60px">class1 = data[labels[:,0] == 1] 
class2 = data[labels[:,1] == 1] 
class3 = data[labels[:,2] == 1] 
class4 = data[labels[:,3] == 1] </pre>
<ol start="8">
<li>Define the grids for all these classes:</li>
</ol>
<pre style="padding-left: 60px">grid1 = input_grid[output_grid[:,0] == 1] 
grid2 = input_grid[output_grid[:,1] == 1] 
grid3 = input_grid[output_grid[:,2] == 1] 
grid4 = input_grid[output_grid[:,3] == 1] </pre>
<ol start="9">
<li>Plot the outputs:</li>
</ol>
<pre style="padding-left: 60px">plt.plot(class1[:,0], class1[:,1], 'ko', class2[:,0], class2[:,1], 'ko',  
                class3[:,0], class3[:,1], 'ko', class4[:,0], class4[:,1], 'ko') 
plt.plot(grid1[:,0], grid1[:,1], 'b.', grid2[:,0], grid2[:,1], 'gx', 
                grid3[:,0], grid3[:,1], 'cs', grid4[:,0], grid4[:,1], 'ro') 
plt.axis([0, 8, 0, 8]) 
plt.xlabel('X-axis') 
plt.ylabel('Y-axis') 
plt.title('Vector quantization using neural networks') 
plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will see the following diagram, where the space is divided into regions. Each region corresponds to a bucket in the list of vector-quantized regions in the space:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1090 image-border" src="assets/e3f4e00d-48d5-4e10-a4c7-b1fe5d5bf5dc.png" style="width:35.50em;height:28.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we</span> <span>defined a neural network with two layers: 10 neurons in the input layer and 4 neurons in the output layer. This neural network was first trained and then used to </span><span>divide</span><span> t</span><span>he space into four regions. Each region corresponds to a bucket in the list of vector-quantized regions in the space.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><strong>Vector quantization</strong> is based on the division of a large set of points (vectors) into groups that show the same number of points closer to them. Each group is identified by its centroid point, as is the case with most clustering algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to t</span>he official documentation of<span> the </span><kbd>neurolab</kbd> library: <a href="https://pythonhosted.org/neurolab/">https://pythonhosted.org/neurolab/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a recurrent neural network for sequential data analysis</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks are really good at analyzing sequential and time-series data. A <strong>recurrent neural network</strong> (<strong>RNN</strong>) is a neural model in which a bidirectional flow of information is present. In other words, while the propagation of signals in feedforward networks takes place only in a continuous manner, going from inputs to outputs, recurrent networks are different. In them, this propagation can also occur from a neural layer following a previous one, or between neurons belonging to the same layer, and even between a neuron and itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>When we deal with sequential and time-series data, we cannot just extend generic models. The temporal dependencies in the data are really important, and we need to account for this in our models. Let's build a recurrent neural network using the <kbd>neurolab</kbd> library.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a recurrent neural network for sequential data analysis:</p>
<ol>
<li>Create a new Python file, and import the following packages <span>(the full code is given in the</span> <span><kbd>recurrent_network.</kbd></span><span><kbd>py</kbd></span><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
import neurolab as nl </pre>
<ol start="2">
<li>Define a function to create a waveform, based on input parameters:</li>
</ol>
<pre style="padding-left: 60px">def create_waveform(num_points): 
    # Create train samples 
    data1 = 1 * np.cos(np.arange(0, num_points)) 
    data2 = 2 * np.cos(np.arange(0, num_points)) 
    data3 = 3 * np.cos(np.arange(0, num_points)) 
    data4 = 4 * np.cos(np.arange(0, num_points)) </pre>
<ol start="3">
<li>Create different amplitudes for each interval to create a random waveform:</li>
</ol>
<pre style="padding-left: 60px">    # Create varying amplitudes 
    amp1 = np.ones(num_points) 
    amp2 = 4 + np.zeros(num_points)  
    amp3 = 2 * np.ones(num_points)  
    amp4 = 0.5 + np.zeros(num_points)  </pre>
<ol start="4">
<li>Combine the arrays to create output arrays. This data corresponds to the input and the amplitude corresponds to the labels:</li>
</ol>
<pre style="padding-left: 60px">    data = np.array([data1, data2, data3, data4]).reshape(num_points * 4, 1) 
    amplitude = np.array([[amp1, amp2, amp3, amp4]]).reshape(num_points * 4, 1) 
 
    return data, amplitude  </pre>
<ol start="5">
<li>Define a function to draw the output after passing the data through the trained neural network:</li>
</ol>
<pre style="padding-left: 60px"># Draw the output using the network 
def draw_output(net, num_points_test): 
    data_test, amplitude_test = create_waveform(num_points_test) 
    output_test = net.sim(data_test) 
    plt.plot(amplitude_test.reshape(num_points_test * 4)) 
    plt.plot(output_test.reshape(num_points_test * 4))</pre>
<ol start="6">
<li>Define the <kbd>main</kbd> function and start by creating sample data:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    # Get data 
    num_points = 30 
    data, amplitude = create_waveform(num_points) </pre>
<ol start="7">
<li>Create a recurrent neural network with two layers:</li>
</ol>
<pre>    # Create network with 2 layers 
    net = nl.net.newelm([[-2, 2]], [10, 1], [nl.trans.TanSig(), nl.trans.PureLin()]) </pre>
<ol start="8">
<li>Set the initialized functions for each layer:</li>
</ol>
<pre>    # Set initialized functions and init 
    net.layers[0].initf = nl.init.InitRand([-0.1, 0.1], 'wb') 
    net.layers[1].initf= nl.init.InitRand([-0.1, 0.1], 'wb') 
    net.init() </pre>
<ol start="9">
<li>Train the recurrent neural network:</li>
</ol>
<pre>    # Training the recurrent neural network 
    error = net.train(data, amplitude, epochs=1000, show=100, goal=0.01) </pre>
<ol start="10">
<li>Compute the output from the network for the training data:</li>
</ol>
<pre>    # Compute output from network 
    output = net.sim(data) </pre>
<ol start="11">
<li>Plot the training error:</li>
</ol>
<pre>    # Plot training results 
    plt.subplot(211) 
    plt.plot(error) 
    plt.xlabel('Number of epochs') 
    plt.ylabel('Error (MSE)') </pre>
<ol start="12">
<li>Plot the results:</li>
</ol>
<pre>    plt.subplot(212) 
    plt.plot(amplitude.reshape(num_points * 4)) 
    plt.plot(output.reshape(num_points * 4)) 
    plt.legend(['Ground truth', 'Predicted output'])</pre>
<ol start="13">
<li>Create a waveform of random length and see whether the network can predict it:</li>
</ol>
<pre>    # Testing on unknown data at multiple scales 
    plt.figure() 
 
    plt.subplot(211) 
    draw_output(net, 74) 
    plt.xlim([0, 300]) </pre>
<ol start="14">
<li>Create another waveform with a shorter length and see whether the network can predict it:</li>
</ol>
<pre>    plt.subplot(212) 
    draw_output(net, 54) 
    plt.xlim([0, 300]) 
 
    plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will see two diagrams. The first diagram displays training errors and the performance on the training data, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1091 image-border" src="assets/bc6e8533-b424-477c-8ca6-8d3db31e3c76.png" style="width:36.33em;height:26.75em;"/></p>
<p style="padding-left: 60px">The second diagram displays how a trained recurrent neural net performs on sequences of arbitrary lengths, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1092 image-border" src="assets/a0e5d044-b84a-4cb4-8ce1-1383b289a4bf.png" style="width:34.92em;height:25.92em;"/></p>
<p style="padding-left: 60px">You will see the following on your Terminal:</p>
<pre style="padding-left: 60px"><strong>Epoch: 100; Error: 1.2635865600014597;</strong><br/><strong>Epoch: 200; Error: 0.4001584483592344;</strong><br/><strong>Epoch: 300; Error: 0.06438997423142029;</strong><br/><strong>Epoch: 400; Error: 0.03772354900253485;</strong><br/><strong>Epoch: 500; Error: 0.031996105192696744;</strong><br/><strong>Epoch: 600; Error: 0.011933337009068408;</strong><br/><strong>Epoch: 700; Error: 0.012385370178600663;</strong><br/><strong>Epoch: 800; Error: 0.01116995004102195;</strong><br/><strong>Epoch: 900; Error: 0.011191016373572612;</strong><br/><strong>Epoch: 1000; Error: 0.010584255803264013;</strong><br/><strong>The maximum number of train epochs is reached</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, </span><span>first, we created an artificial signal with waveform characteristics, that is, a curve showing the shape of a wave at a given time. Then we built a recurrent neural network to see whether the network could predict a waveform of random length.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Recurrent networks are distinguished from feedforward networks thanks to the feedback loop linked to their past decisions, thus accepting their output momentarily as inputs. This feature can be emphasized by saying that recurrent networks have memory. There is information in the sequence, and it is used perform the tasks that feedforward networks cannot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of<span> </span><kbd>neurolab</kbd> library: <a href="https://pythonhosted.org/neurolab/">https://pythonhosted.org/neurolab/</a></li>
<li>Refer to <em>Recurrent Neural Networks</em> (from Yale University): <a href="http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf">http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the characters in an OCR database</h1>
                </header>
            
            <article>
                
<p>We will now look at how to use neural networks to perform <strong>optical character recognition </strong>(<strong>OCR</strong>). This refers to the process of identifying handwritten characters in images. We have always been particularly sensitive to the problem of the automatic recognition of writing in order to achieve a simpler interaction between humans and machines. Especially in the last few years, this problem has been subject to interesting developments and more and more efficient solutions thanks to a very strong economic interest and an ever-greater capacity to process data of modern computers. In particular, some countries, such as Japan, and Asian countries in general, are investing heavily, in terms of research and financial resources, to make state-of-the-art OCR.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> display the handwritten digits contained in a dataset. </span><span>We will use the dataset available at </span><a href="http://ai.stanford.edu/~btaskar/ocr"><span class="URLPACKT">http://ai.stanford.edu/~btaskar/ocr</span></a><span>. The default file name after downloading is </span><kbd>letter.data</kbd><span>. To start with, let's see how to interact with the data and visualize it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to visualize the characters in an OCR database:</p>
<ol>
<li>Create a new Python file, and import the following packages <span>(the full code is given in the </span><span><kbd>visualize_characters.</kbd></span><span><kbd>py</kbd></span><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2 
import numpy as np </pre>
<ol start="2">
<li>Define the input file name:</li>
</ol>
<pre style="padding-left: 60px"># Load input data  
input_file = 'letter.data'  </pre>
<ol start="3">
<li>Define visualization parameters:</li>
</ol>
<pre style="padding-left: 60px"># Define visualization parameters  
scaling_factor = 10 
start_index = 6 
end_index = -1 
h, w = 16, 8 </pre>
<ol start="4">
<li>Keep looping through the file until the user presses the <em><span class="KeyPACKT">Escape</span></em> key. Split the line into tab-separated characters:</li>
</ol>
<pre style="padding-left: 60px"># Loop until you encounter the Esc key 
with open(input_file, 'r') as f: 
    for line in f.readlines(): 
        data = np.array([255*float(x) for x in line.split('\t')[start_index:end_index]]) </pre>
<ol start="5">
<li>Reshape the array into the required shape, resize it, and display it:</li>
</ol>
<pre>        img = np.reshape(data, (h,w)) 
        img_scaled = cv2.resize(img, None, fx=scaling_factor, fy=scaling_factor) 
        cv2.imshow('Image', img_scaled) </pre>
<ol start="6">
<li>If the user presses <em><span class="KeyPACKT">Escape</span></em>, break the loop:</li>
</ol>
<pre>        c = cv2.waitKey() 
        if c == 27: 
            break </pre>
<p style="padding-left: 60px">If you run this code, you will see a window displaying characters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we showed the handwritten figures contained in a dataset. To do this, the following tasks are performed:</p>
<ul>
<li>Load input data</li>
<li>Define visualization parameters</li>
<li>Loop until you encounter the <em>Escape</em> key</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Approaches to the OCR problem are basically of two types: one is based on pattern matching or on the comparison of a model and the other is based on structural analysis. Often, these two techniques are used in combination, and provide remarkable results in terms of recognition and speed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of the<span> </span><kbd>OpenCV</kbd> library: <a href="https://opencv.org/">https://opencv.org/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an optical character recognizer using neural networks</h1>
                </header>
            
            <article>
                
<p>Now that we know how to interact with data, let's build a neural network-based OCR system. The operations of classification and indexing the images are based on the automatic analysis of the image content, which constitutes the main application field of the imaging analysis. The objective of an automatic image recognition system consists in the description, through mathematical models and computer implementations, the content of an image, all the while trying, as far as possible, to respect the principles of the human visual system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> build a neural network-based OCR system.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build an optical character recognizer using neural networks:</p>
<ol>
<li>Create a new Python file, and import the following packages <span>(the full code is given in the</span> <span><kbd>ocr.</kbd></span><span><kbd>py</kbd></span><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import neurolab as nl </pre>
<ol start="2">
<li>Define the input filename:</li>
</ol>
<pre style="padding-left: 60px">input_file = 'letter.data' </pre>
<ol start="3">
<li>When we work with neural networks that deal with large amounts of data, it takes a lot of time to train. To demonstrate how to build this system, we will take only <kbd>20</kbd> datapoints:</li>
</ol>
<pre style="padding-left: 60px">num_datapoints = 20 </pre>
<ol start="4">
<li>If you look at the data, you will see that there are seven distinct characters in the<br/>
first 20 lines. Let's define them:</li>
</ol>
<pre style="padding-left: 60px">orig_labels = 'omandig' 
num_output = len(orig_labels) </pre>
<ol start="5">
<li>We will use 90% of the data for training and the remaining 10% for testing. Define the training and testing parameters:</li>
</ol>
<pre style="padding-left: 60px">num_train = int(0.9 * num_datapoints) 
num_test = num_datapoints - num_train </pre>
<ol start="6">
<li>The starting and ending indices in each line of the dataset file are specified:</li>
</ol>
<pre style="padding-left: 60px">start_index = 6 
end_index = -1 </pre>
<ol start="7">
<li>Create the dataset:</li>
</ol>
<pre style="padding-left: 60px">data = [] 
labels = [] 
with open(input_file, 'r') as f: 
    for line in f.readlines(): 
        # Split the line tabwise 
        list_vals = line.split('\t')</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="8">
<li>Add an error check to see whether the characters are in our list of labels (if the label is not in our ground truth labels, skip it):</li>
</ol>
<pre>        if list_vals[1] not in orig_labels: 
            continue </pre>
<ol start="9">
<li>Extract the label, and append it to the main list:</li>
</ol>
<pre>        label = np.zeros((num_output, 1)) 
        label[orig_labels.index(list_vals[1])] = 1 
        labels.append(label) </pre>
<ol start="10">
<li>Extract the character, and append it to the main list:</li>
</ol>
<pre>        cur_char = np.array([float(x) for x in list_vals[start_index:end_index]]) 
        data.append(cur_char) </pre>
<ol start="11">
<li>Exit the loop once we have enough data:</li>
</ol>
<pre>        if len(data) &gt;= num_datapoints: 
            break </pre>
<ol start="12">
<li>Convert this data into NumPy arrays:</li>
</ol>
<pre style="padding-left: 60px">data = np.asfarray(data) 
labels = np.array(labels).reshape(num_datapoints, num_output) </pre>
<ol start="13">
<li>Extract the number of dimensions in our data:</li>
</ol>
<pre style="padding-left: 60px">num_dims = len(data[0]) </pre>
<ol start="14">
<li>Train the neural network until <kbd>10,000</kbd> epochs:</li>
</ol>
<pre style="padding-left: 60px">net = nl.net.newff([[0, 1] for _ in range(len(data[0]))], [128, 16, num_output]) 
net.trainf = nl.train.train_gd 
error = net.train(data[:num_train,:], labels[:num_train,:], epochs=10000,  
        show=100, goal=0.01) </pre>
<ol start="15">
<li>Predict the output for test inputs:</li>
</ol>
<pre style="padding-left: 60px">predicted_output = net.sim(data[num_train:, :])<br/>print("Testing on unknown data:")<br/>for i in range(num_test):<br/>    print("Original:", orig_labels[np.argmax(labels[i])])<br/>    print("Predicted:", orig_labels[np.argmax(predicted_output[i])])</pre>
<ol start="16">
<li>If you run this code, you will see the following on your Terminal at the end of training:</li>
</ol>
<pre style="padding-left: 60px"><strong>Epoch: 5000; Error: 0.032178530603536336;</strong><br/><strong>Epoch: 5100; Error: 0.023122560947574727;</strong><br/><strong>Epoch: 5200; Error: 0.040615342668364626;</strong><br/><strong>Epoch: 5300; Error: 0.01686314983574041;</strong><br/><strong>The goal of learning is reached</strong></pre>
<p style="padding-left: 60px">The output of the neural network is shown as follows:</p>
<pre style="padding-left: 60px"><strong>Testing on unknown data:</strong><br/><strong>Original: o</strong><br/><strong>Predicted: o</strong><br/><strong>Original: m</strong><br/><strong>Predicted: m</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we</span><span> </span><span>used a neural network to recognize the handwritten digits. To do this, the following tasks are performed:</span></p>
<ul>
<li>Loading and manipulating input data</li>
<li>Creating the dataset</li>
<li>Converting data and labels into NumPy arrays</li>
<li>Extracting the number of dimensions</li>
<li>Creating and training the neural network</li>
<li>Predicting the output for test inputs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The term <strong>handwriting recognition</strong> (<strong>HWR</strong>) refers to the ability of a computer to receive and interpret as text intelligible handwritten input from sources such as paper documents, photographs, and touchscreens. Written text can be detected on a piece of paper via optical scanning (OCR) or <strong>intelligent word recognition</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to </span>the official documentation of<span> the </span><kbd>neurolab</kbd> library: <a href="https://pythonhosted.org/neurolab/">https://pythonhosted.org/neurolab/</a></li>
<li>Refer to <em>Optical character recognition</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">https://en.wikipedia.org/wiki/Optical_character_recognition</a></li>
<li>Refer to <em>Handwriting recognition</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Handwriting_recognition">https://en.wikipedia.org/wiki/Handwriting_recognition</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing optimization algorithms in ANN</h1>
                </header>
            
            <article>
                
<p>So far, we have built several neural networks and obtained satisfactory overall performances. We have evaluated the model's performance using the <kbd>loss</kbd> function, which is a mathematical way to measure how wrong our predictions are. To improve the performance of a model based on neural networks, during the training process, weights are modified to try to minimize the <kbd>loss</kbd> function and make our predictions as correct as possible. To do this, optimizers are used: they are algorithms that regulate the parameters of the model, updating it in relation to what is returned by the <kbd>loss</kbd> function. In practice, optimizers shape the model in its most accurate form possible by overcoming weights: The <kbd>loss</kbd> function tells the optimizer when it is moving in the right or wrong direction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> build a neural network using the Keras library and improve the performance of the model by adopting several optimizers. To do this, the <kbd>iris</kbd> dataset will be used. I'm referring to the <strong>Iris flower dataset</strong>, a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper: <em>The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis</em>.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to implement optimization algorithms in ANN:</p>
<ol>
<li>Create a new Python file, and import the following packages<span> </span><span>(the full code is given in the <kbd>IrisClassifier.py</kbd></span><span> file that is provided to you)</span>:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import load_iris<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import OneHotEncoder<br/>from keras.models import Sequential<br/>from keras.layers import Dense </pre>
<ol start="2">
<li>Import the data from the <kbd>sklearn</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px">IrisData = load_iris() </pre>
<ol start="3">
<li>Divide the data into an input and target:</li>
</ol>
<pre style="padding-left: 60px">X = IrisData.data<br/>Y = IrisData.target.reshape(-1, 1) </pre>
<p style="padding-left: 60px">For the target, the data was converted to a single column.</p>
<ol start="4">
<li>Let's encode the class labels as <kbd>One Hot Encode</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Encoder = OneHotEncoder(sparse=False)<br/>YHE = Encoder.fit_transform(Y)</pre>
<ol start="5">
<li>Split the data for training and testing:</li>
</ol>
<pre style="padding-left: 60px">XTrain, XTest, YTrain, YTest = train_test_split(X, YHE, test_size=0.30)</pre>
<ol start="6">
<li>Let's build the model:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()</pre>
<ol start="7">
<li>Three layers will be added: an input layer, a hidden layer, and an output layer.</li>
</ol>
<pre style="padding-left: 60px">model.add(Dense(10, input_shape=(4,), activation='relu'))<br/>model.add(Dense(10, activation='relu'))<br/>model.add(Dense(3, activation='softmax'))</pre>
<ol start="8">
<li>Let's compile the model:</li>
</ol>
<pre style="padding-left: 60px">model.compile(optimizer='SGD',loss='categorical_crossentropy', metrics=['accuracy'])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The following three arguments are passed:</p>
<ul>
<li>
<p><kbd>optimizer='SGD'</kbd>: Stochastic gradient descent optimizer. Includes support for momentum, learning rate decay, and Nesterov momentum.</p>
</li>
<li><kbd>loss='categorical_crossentropy'</kbd>: We have used the <kbd>categorical_crossentropy</kbd> argument here. When using <kbd>categorical_crossentropy</kbd>, your targets should be in categorical format (we have three classes; the target for each sample must be a three-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample).</li>
<li><kbd>metrics=['accuracy']</kbd>: A <kbd>metric</kbd> is a function that is used to evaluate the<br/>
performance of your model during training and testing.</li>
</ul>
<ol start="9">
<li>Let's train the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(XTrain, YTrain, verbose=2, batch_size=5, epochs=200)</pre>
<ol start="10">
<li>Finally, test the model using unseen data:</li>
</ol>
<pre style="padding-left: 60px">results = model.evaluate(XTest, YTest)<br/>print('Final test set loss:' ,results[0])<br/>print('Final test set accuracy:', results[1])</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>Final test set loss: 0.17724286781416998</strong><br/><strong>Final test set accuracy: 0.9555555568801032</strong></pre>
<ol start="11">
<li>Now let's see what happens if we use a different optimizer. To do this, just change the optimizer parameter in the compile method, as follows:</li>
</ol>
<pre style="padding-left: 60px">model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])</pre>
<p style="padding-left: 60px">The <kbd>adam</kbd> optimizer is an algorithm for the first-order, gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. </p>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong>Final test set loss: 0.0803464303414027</strong><br/><strong>Final test set accuracy: 0.9777777777777777</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">As we said in the <em>Building a deep neural network</em> recipe, gradient descent <span>is an iterative approach used for error correction in any learning model. Gradient descent approach is the process of iterating the update of weights and biases with the error times derivative of the activation function (backpropagation). In this approach, the steepest descent step size is substituted by a similar size from the previous step. The gradient is the slope of the curve, as it is the derivative of the activation function.</span> The SGD optimizer is based on this <span>approach.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Optimization problems are usually so complex that it is not possible to determine a solution analytically. Complexity is determined primarily by the number of variables and constraints, which define the size of the problem, and then by the possible presence of non-linear functions. An analytical solution is only possible in the case of a few variables and extremely simple functions. In practice, to solve an optimization problem, it is necessary to resort to an iterative algorithm, that is, to a calculation program that, given a current approximation of the solution, determines, with an appropriate sequence of operations, a new approximation. Starting from an initial approximation, a succession is thus determined.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of the Keras optimizer: <a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a></li>
<li><span>Refer to </span><em>Optimization for Deep Neural Networks</em> (from The University of Chicago): <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf">https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>