["```py\n    name: pystock_data_features\n    conda:\n      file: conda.yaml\n    entry_points:\n      data_acquisition:\n        command: \"python data_acquisition.py\"\n      clean_validate_data:\n        command: \"python clean_validate_data.py \"\n      feature_set_generation:\n        command: \"python feature_set_generation.py\"\n      main:\n        command: \"python main.py\"\n    ```", "```py\n        name: pystock-data-features\n    channels:\n      - defaults\n    dependencies:\n      - python=3.8\n      - numpy\n      - scipy\n      - pandas\n      - cloudpickle\n      - pip:\n        - git+git://github.com/mlflow/mlflow\n        - pandas_datareader\n        - great-expectations==0.13.15\n\n    ```", "```py\n    import mlflow\n    import click\n    def _run(entrypoint, parameters={}, source_version=None, use_cache=True):\n        #existing_run = _already_ran(entrypoint, parameters, source_version)\n        #if use_cache and existing_run:\n        #    print(\"Found existing run for entrypoint=%s and parameters=%s\" % (entrypoint, parameters))\n         #   return existing_run\n        print(\"Launching new run for entrypoint=%s and parameters=%s\" % (entrypoint, parameters))\n        submitted_run = mlflow.run(\".\", entrypoint, parameters=parameters)\n        return submitted_run\n    @click.command()\n    def workflow():\n        with mlflow.start_run(run_name =\"pystock-data-pipeline\") as active_run:\n            mlflow.set_tag(\"mlflow.runName\", \"pystock-data-pipeline\")\n            _run(\"load_raw_data\")\n            _run(\"clean_validate_data\")\n            _run(\"feature_set_generation\")\n\n    if __name__==\"__main__\":\n        workflow()\n    ```", "```py\n    mlflow run .\n    ```", "```py\n    python: can't open file 'check_verify_data.py': [Errno 2] No such file or directory\n    ```", "```py\n    import mlflow\n    from datetime import date\n    from dateutil.relativedelta import relativedelta\n    import pprint\n    import pandas\n    import pandas_datareader.data as web\n    ```", "```py\n    if __name__ == \"__main__\":\n\n        with mlflow.start_run(run_name=\"load_raw_data\") as \n    run:\n            mlflow.set_tag(\"mlflow.runName\", \"load_raw_data\")\n            end = date.today()\n            start = end + relativedelta(months=-3)\n\n            df = web.DataReader(\"BTC-USD\", 'yahoo', start, end)\n            df.to_csv(\"./data/raw/data.csv\") \n\n    ```", "```py\n    import mlflow\n    from datetime import date\n    from dateutil.relativedelta import relativedelta\n    import pprint\n    import pandas_datareader\n    import pandas\n    from pandas_profiling import ProfileReport\n    import great_expectations as ge\n    from great_expectations.profile.basic_dataset_profiler import BasicDatasetProfiler\n    ```", "```py\n    if __name__ == \"__main__\":\n        with mlflow.start_run(run_name=\"check_verify_data\") as run:\n            mlflow.set_tag(\"mlflow.runName\", \"check_verify_data\")\n            df = pandas.read_csv(\"./data/raw/data.csv\")\n            describe_to_dict=df.describe().to_dict()\n            mlflow.log_dict(describe_to_dict,\"describe_data.json\")\n            pd_df_ge = ge.from_pandas(df)\n            assert pd_df_ge.expect_column_values_to_match_\n    strftime_format(\"Date\", \"%Y-%m-%d\").success == True\n            assert pd_df_ge.expect_column_values_to_be_of_\n    type(\"High\", \"float\").success == True\n            assert pd_df_ge.expect_column_values_to_be_of_type(\"Low\", \"float\").success == True\n            assert pd_df_ge.expect_column_values_to_be_of_type(\"Open\", \"float\").success == True\n            assert pd_df_ge.expect_column_values_to_be_of_type(\"Close\", \"float\").success == True\n            assert pd_df_ge.expect_column_values_to_be_of_type(\"Volume\", \"long\").success == True\n            assert pd_df_ge.expect_column_values_to_be_of_type(\"Adj Close\", \"float\").success == True\n    ```", "```py\n            #we can do some basic cleaning by dropping the null values\n            df.dropna(inplace=True)\n            #if data_passes_quality_can_go_to_features:\n            df.to_csv(\"data/staging/data.csv\")\n    ```", "```py\n    import mlflow\n    from datetime import date\n    from dateutil.relativedelta import relativedelta\n    import pprint\n    import pandas as pd\n    import pandas_datareader\n    import pandas_datareader.data as web\n    import numpy as np\n    ```", "```py\n    def rolling_window(a, window):\n        \"\"\"\n            Takes np.array 'a' and size 'window' as parameters\n            Outputs an np.array with all the ordered sequences of values of 'a' of size 'window'\n            e.g. Input: ( np.array([1, 2, 3, 4, 5, 6]), 4 )\n                 Output: \n                         array([[1, 2, 3, 4],\n                               [2, 3, 4, 5],\n                               [3, 4, 5, 6]])\n        \"\"\"\n        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n        strides = a.strides + (a.strides[-1],)\n        return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    ```", "```py\n        with mlflow.start_run() as run:\n            mlflow.set_tag(\"mlflow.runName\", \"feature_set_\n    generation\")\n            btc_df = pd.read_csv(\"data/staging/data.csv\")\n            btc_df['delta_pct'] = (btc_df['Close'] - btc_df['Open'])/btc_df['Open']\n            btc_df['going_up'] = btc_df['delta_pct'].apply(lambda d: 1 if d>0.00001 else 0).to_numpy()\n            element=btc_df['going_up'].to_numpy()\n            WINDOW_SIZE=15\n            training_data = rolling_window(element, WINDOW_SIZE)\n            pd.DataFrame(training_data).to_csv(\"data/training/data.csv\", index=False)\n    ```", "```py\nmlflow run . --experiment-name=psystock_data_pipelines\n```", "```py\nmlflow ui\n```", "```py\n    pip install feast==0.10\n    ```", "```py\n    feast init\n    ```", "```py\n    project: psystock_feature_store\n    registry: data/registry.db\n    provider: local\n    online_store:\n        path: data/online_store.db\n    ```", "```py\n    from google.protobuf.duration_pb2 import Duration\n    from feast import Entity, Feature, FeatureView, ValueType\n    from feast.data_source import FileSource\n    ```", "```py\n    token_features = FileSource(\n        path=\"/data/features.csv\",\n        event_timestamp_column=\"create_date\",\n        created_timestamp_column=\"event_date\",\n    )\n    token= Entity(name=\"token\", value_type=ValueType.STRING, description=\"token id\",)\n    ```", "```py\n    hourly_view_features_token = FeatureView(\n        name=\"token_hourly_features\",\n        entities=[\"token\"],\n        ttl=Duration(seconds=3600 * 1),\n        features=[\n            Feature(name=\"prev_10days\", dtype=ValueType.INT64),\n            Feature(name=\"prev_11days\", dtype=ValueType.INT64),\n            Feature(name=\"prev_12days\", dtype=ValueType.INT64),\n            Feature(name=\"prev_13days\", dtype=ValueType.INT64)\n        ],\n        online=True,\n        input=token_features,\n        tags={},\n    )\n    ```", "```py\n    feast apply\n    ```", "```py\n    import pandas as pd\n    from datetime import datetime\n    from feast import FeatureStore\n    # entity_df generally comes from upstream systems\n    event_data_point = pd.DataFrame.from_dict({\n        \"token\": [\"btc\",\"btc\"],\n        \"event_date\": [\n            datetime(2021, 4, 12, 10, 59, 42),\n            datetime(2021, 4, 12, 8,  12, 10),\n        ]\n    })\n    store = FeatureStore(repo_path=\".\")\n    feature_loading_df = store.get_historical_features(\n        entity_df=entity_df, \n        feature_refs = [\n            'token_hourly_features:prev_3days',\n            'token_hourly_features:prev_4days',\n            'token_hourly_features:prev_5days'        \n        ],\n    ).to_df()\n    ```"]