- en: '*Chapter 6*: Detecting ML Bias and Explaining Models with SageMaker Clarify'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) models are increasingly being used to help make
    business decisions across industries, such as in financial services, healthcare,
    education, and human resources (HR), thanks to the automation ML provides, with
    improved accuracy over humans. However, ML models are never perfect. They can
    make poor decisions—even unfair ones if not trained and evaluated carefully. An
    ML model can be biased in a way that hurts disadvantaged groups. Having an ability
    to understand bias in data and ML models during the ML life cycle is critical
    for creating a socially fair ML model. **SageMaker Clarify** computes ML biases
    in datasets and in ML models to help you gain an understanding of the limitation
    of ML models so that you can take appropriate action to mitigate these biases.'
  prefs: []
  type: TYPE_NORMAL
- en: ML models have long been considered as black box operations because it is rather
    difficult to see how a prediction is made. SageMaker Clarify computes feature
    attribution to help you explain how an ML model makes a decision so that it is
    no longer a black box to us. SageMaker Clarify integrates with SageMaker Studio
    so that you can easily review the results while building ML models. With SageMaker
    Clarify, you will be able to know more about your ML models, promote fairness
    and explainability in your ML use cases, and meet regulatory requirements if required.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be learning about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding bias, fairness in ML, and ML explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting bias in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining ML models using **SHapley Additive exPlanations** (**SHAP**) values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you need to access the code provided at [https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter06](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding bias, fairness in ML, and ML explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two types of bias in ML that we can analyze and mitigate to ensure
    fairness—**data bias** and **model bias**. **Data bias** is an imbalance in the
    training data across different groups and categories that can be introduced into
    an ML solution simply due to a sampling error, or intricately due to inherent
    reasons that are unfortunately ingrained in society. Data bias, if neglected,
    can translate into poor accuracy in general and unfair prediction against a certain
    group in a trained model. It is more critical than ever to be able to discover
    inherent biases in the data early and take action to address them. **Model bias**,
    on the other hand, refers to bias introduced by model prediction, such as the
    distribution of classification and errors among advantaged and disadvantaged groups.
    Should the model favor an advantaged group for a particular outcome or disproportionally
    predict incorrectly for a disadvantaged group, causing undesirable consequences
    in real-world ML applications such as loan-approval prediction systems, we as
    data scientists need to take action to understand why this has happened and mitigate
    the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring fairness in ML starts with understanding the data and detecting biases
    within it. Data bias may lead to model bias, as it is well understood that the
    model will learn what is presented in the data, including any bias, and will replicate
    that bias in its inferences. Quantifying biases using metrics that are developed
    and accepted by the ML community is key to detection and choosing mitigation approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to explain how the model makes a decision is another key factor to
    ensure fairness in ML models. People had long thought that ML is a magical black
    box—it predicts things better than humans can, but nobody knows why or how. But
    ML researchers have developed frameworks to help unbox the black box, the most
    notable one being SHAP. SHAP computes and assigns an importance score for each
    feature for a particular prediction. This importance score is called a **Shapley
    value** and is an implementation of cooperative game theory to allocate credit
    for a model's output among its input features. With the Shapley values for each
    feature for a prediction, we can describe how and why the model makes such a prediction
    and which feature contributes the most to this. Should there be a sensitive feature
    that contributes significantly to model prediction, we need to take action to
    address this effect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon SageMaker Clarify** helps developers discover underlying bias in the
    training data and model prediction and explain feature importance for an ML model.
    SageMaker Clarify computes various metrics to measure bias in the data so that
    you do not have to be an expert in the science of ML bias. You can use SageMaker
    Clarify with the SageMaker **software development kit** (**SDK**) to analyze data
    and models from a notebook, which we will focus on in this chapter. SageMaker
    Clarify also integrates with Amazon SageMaker Data Wrangler so that you can detect
    bias using a simple graphical interface. SageMaker Clarify further integrates
    with **Amazon SageMaker Experiments** to provide graphical results for each experiment
    and **Amazon SageMaker Model Monitor** so that you can identify bias and feature
    importance in a trained model and in inference data in production.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started with an ML example to see how we can detect bias using SageMaker
    Clarify.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting bias in ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, I'd like to use an ML adult census income dataset from the
    **University of California Irvine** (**UCI**) ML repository ([https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)).
    This dataset contains demographic information from census data and income level
    as a prediction target. The goal of the dataset is to predict whether a person
    earns over or below **United States dollars** (**USD**) **$50,000** (**$50K**)
    per year based on the census information. This is a great example and is the type
    of ML use case that includes socially sensitive categories such as gender and
    race, and is under the most scrutiny and regulation to ensure fairness when producing
    an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will analyze the dataset to detect data bias in the training
    data, mitigate if there is any bias, train an ML model, and analyze whether there
    is any model bias against a particular group.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting pretraining bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please open the notebook in `Getting-Started-with-Amazon-SageMaker-Studio``/chapter06/01-ml_fairness_clarify.ipynb`
    and follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use SageMaker Experiments to organize the analysis and training job.
    Therefore, we install `sagemaker-experiments` in the first cell, and we set up
    the SageMaker session and import the required libraries in the following two cells.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the fourth cell, we load the train and test datasets from the UCI ML repository.
    The `orig_columns` values are parsed from [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names).
    The original dataset has both string representation and ordinal representation
    for education level in the `education` and `education-num` features. Let''s just
    keep the ordinal representation and drop the `education` column. We also move
    the `target` column to the first column because we will use SageMaker''s built-in
    `XGBoost` algorithm to train an ML model to predict the target. The `target` column
    contains the label for income greater than $50K (`>50K`) and less than and equal
    to $50K (`<=50K`). You can see an illustration of this in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Screenshot of the DataFrame after step 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Screenshot of the DataFrame after step 2
  prefs: []
  type: TYPE_NORMAL
- en: We encode the categorical features in the train data (`df`) and test data (`df_valtest`)
    with `OrdinalEncoder` from `sklearn` to make the dataset compatible with the XGBoost
    algorithm. After the encoding, the `target` variable of values `>50K` and `<=50K`
    are encoded as `1` and `0`, respectively; there is a potentially sensitive `sex`
    category with `Male` and `Female` values encoded as `1` and `0`, respectively.
    We further take 10% of the test dataset as the validation dataset for model training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With this dataset, there are many angles from which we can analyze the data
    for bias and fairness. Intuitively, gender equality in income would be one angle
    we could start with. Let''s make some visualizations to understand it qualitatively,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next screenshot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of females is about half that of males.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more people whose earnings are below $50K.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are more males than females who earn more than $50K.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see the output here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Output of the plotting, showing the distribution of sex and
    income level; an imbalanced distribution in sex and income level can be observed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Output of the plotting, showing the distribution of sex and income
    level; an imbalanced distribution in sex and income level can be observed
  prefs: []
  type: TYPE_NORMAL
- en: This distribution may be reflective of social inequality, but how do we quantify
    these skewed distributions so that we can be more aware of the bias in the dataset
    automatically and programmatically? This is where SageMaker Clarify comes into
    play.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker Clarify from the SageMaker SDK (`sagemaker.clarify`) uses a dedicated
    container and SageMaker Processing to compute ML bias and explain ML predictions.
    We can start by instantiating `sagemaker.clarify.SageMakerClarifyProcessor` with
    the type of compute resource that fits the dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use `SageMakerClarifyProcessor.run_pre_training_bias()` specifically
    to compute the data bias prior to training an ML model. The metrics it returns
    allow us to quantify data bias based on the target and facet we choose and allow
    us to take action to mitigate the bias. But first, `run_pre_training_bias()` requires
    two configurations: a `clarify.DataConfig()`, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Because the training data in `train_s3_uri` does not contain column headers,
    the feature columns are provided in the `headers` argument. In the `label` argument,
    we specify the target variable from the dataset, which has to be one of the column
    names in what's input in the `headers` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a bias configuration, we specify the facets—that is, the sensitive categories
    that we would like to analyze using `clarify.BiasConfig()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We would like to analyze how much gender bias (the `sex` column) there is in
    the dataset and, in particular, how the outcome (the `target` column) is impacted
    by gender. To do so, we specify a positive class (`>50K` or `1`) from the target
    in a list to the `label_values_or_threshold` argument. We specify the facet(s)
    to be `sex` and `race`. Although in this example we are mostly focused on gender
    bias, we are adding a `race` feature to showcase that you can use multiple features
    as facets and that SageMaker Clarify would analyze bias in all the facets at once.
    The last required argument, `facet_values_or_threshold`, is there to specify the
    sensitive category in the facets for SageMaker Clarify to focus on when quantifying
    the bias. `facet_values_or_threshold=[[0], None]` corresponds to `facet_name=['sex',
    'race']`. This means that we are asking Clarify to only calculate the bias metrics
    for class `0` in `sex`, which is female, while not specifying a class (`None`)
    for `race`, which will force Clarify to calculate bias metrics for all classes
    in `race`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the setup is complete, we can run the processing job with the configurations,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We ask Clarify to compute all possible pretraining bias with `methods='all'`.
    SageMaker Clarify integrates with SageMaker Experiments, so we also provide an
    experiment and trial configuration for this job. In the notebook, we name the
    experiment `experiment_name = 'adult-income-clarify'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the Clarify results in the `adult-income-clarify` entry, and
    right-click the new trial entry whose name is a timestamp to select **Open in
    trial component list**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Selecting a trial to view the SageMaker Clarify result'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Selecting a trial to view the SageMaker Clarify result
  prefs: []
  type: TYPE_NORMAL
- en: 'A new page with a **TRIAL COMPONENTS** list will show up in the main working
    area. We can open the **Trial details** page to see the result by right-clicking
    on the entry and selecting **Open in trial details**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Selecting a trial component to view the SageMaker Clarify result'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Selecting a trial component to view the SageMaker Clarify result
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Trial** **components** page, move to the **Bias report** tab to find
    the analysis results, as shown in the following screenshot. Here, you can find
    metrics calculated by SageMaker Clarify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Reviewing the pretraining bias report on the trial details page
    in SageMaker Studio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Reviewing the pretraining bias report on the trial details page
    in SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: With each metric, you can see a description to understand what it means. For
    further information, you can expand a line item to see how the metric is calculated,
    with an example and interpretation. Furthermore, you can find additional papers
    in the details' descriptions to read more about the mathematical definition for
    all metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, this **Uniform Resource Locator** (**URL**) takes you to the
    technical whitepaper: [https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf).
    This is a good read if you are interested in the math behind the metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's review the bias in the data. Most notably, it is reported that there is
    `sex` feature) as there are 0.34 or 34% fewer females compared to males. `>50K`)
    there is for males compared to females. There are 0.2 or 20% more males who earn
    >50K in the dataset than females. These two metrics alone not only confirm the
    imbalance we saw in the chart we plotted in the notebook but also quantify the
    imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There is no valid result for `clarify.BiasConfig(group_name=None)`.
  prefs: []
  type: TYPE_NORMAL
- en: You can view the analysis for other facets and categories we have specified
    in `clarify.BiasConfig()`—`race`, for example—by toggling the **Column analyzed
    for bias** and **Column value or threshold analyzed for bias** drop-down lists.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Clarify also saves a copy of the analysis in `pretraining_bias_report_output_path`
    `variable`.
  prefs: []
  type: TYPE_NORMAL
- en: This imbalance in the data, if left unmitigated, could very well be ingrained
    into an ML model after training and it could start repeating what it learned from
    the biased data. Let's see how to mitigate it.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating bias and training a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a couple of data science approaches to mitigate the data imbalance,
    such as matching, oversampling, and undersampling. In this example, let''s try
    a simple matching in terms of gender and target outcome to balance the male and
    female samples and the proportion in a positive outcome (`>50K`). We''ll proceed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the notebook, we continue to work with the data to address the
    bias, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This generates a sampled and matched dataset that has an equal amount of both
    genders and an equal proportion in the target outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the effectiveness of this approach by plotting the same charts
    and creating another pretraining bias analysis using SageMaker Clarify for this
    sampled and matched dataset with an identical bias configuration. Note that we
    are creating another trial in SageMaker Experiments to track this run and direct
    the output to a different output S3 location. The code is illustrated in the following
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We then use the same `bias_config` and call the `clarify_processor.run_pre_training_bias()`
    method as we did before to run a pre-training bias analysis job after bias mitigation.
  prefs: []
  type: TYPE_NORMAL
- en: After the SageMaker Clarify job is done, we can open the **Bias report** feature
    on the trial details page for the new pretraining bias analysis job. You can see
    that **Class Imbalance (CI)** and **Difference in Positive Proportions in Labels
    (DPL)** are now both zeros. In fact, there are zeros across all the bias metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have successfully zeroed out the data bias we observed previously. Let''s
    get the model training started with SageMaker''s built-in `XGBoost` algorithm,
    which is a great tool for structured data such as we have. We run this training
    job as a new trial component in the second trial, `exp_trial_2`. For the hyperparameter,
    we choose a `binary:logistic` objective for binary classification, `error` as
    an evaluation metric, and `50` rounds of optimization. The code is illustrated
    in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training job completes in about 5 minutes, including the infrastructure
    provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a SageMaker model from the training job so that later, we can use
    it in SageMaker Clarify jobs to analyze model bias. Here''s the code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the model is trained, we can use SageMaker Clarify to detect and measure
    biases that occur in the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting post-training bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps analyze biases in prediction and data after the model is
    trained. To run a post-training bias analysis with SageMaker Clarify, we need
    to prepare three configurations: a **data configuration**, a **bias configuration**,
    and a **model configuration**. Proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new `clarify.DataConfig()` instance to analyze the matched training
    data and direct the output to a different output S3 location, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The bias configuration remains the same as what we used in the pretraining bias
    analysis. We continue to analyze how model prediction is impacted by `sex`, `race`,
    and `target` distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a post-training analysis job is started, a SageMaker real-time endpoint
    with the ML model is created to make a prediction on the input data for a short
    duration of time to avoid additional traffic to your production endpoint, if any.
    This endpoint is also called a shadow endpoint and will be deprovisioned once
    the analysis job finishes. For the model configuration, we specify a model and
    configure the endpoint. `accept_type` denotes the endpoint response payload format,
    and `content_type` indicates the payload format of the request to the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also specify a probability threshold of 0.5 to convert the probability output
    from the XGBoost model to binary hard labels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A prediction above 0.5 is predicted as 1 (`>50K`); otherwise, it is 0 (`<=50K`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we run the job with the configurations. We request to compute all
    valid post-training bias metrics, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also review the results on the trial details page of the second trial
    (`exp_trial_2.trial_name`), as shown in the following screenshot. We see a different
    set of metrics are shown compared to a pretraining bias analysis. A post-training
    bias job focuses on analyzing predicted labels or comparing the predictions with
    the observed target values in the data with respect to groups with different attributes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Reviewing the post-training bias report on the trial details
    page in SageMaker Studio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Reviewing the post-training bias report on the trial details page
    in SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: 'There is very low bias in most of the measures such as **Accuracy Difference
    (AD)**, meaning that the model is equally accurate in predicting the income level
    for the two sexes. However, there is one metric that has rather high biases: **Treatment
    Equality (TE)**. This measures whether a *Type 1* error (false positive) and a
    *Type 2* error (false negative) are affecting the two genders in the same way.
    This is the difference in the ratio of false negatives to false positives between
    the male and female groups. A positive value translates to females having a lower
    ratio of false negatives to false positives. That means that the model is more
    often incorrectly predicting a female to be a high-income earner when in fact
    they are not; rather, it is the other way around. Having a higher false-positive
    rate for females compared with males is somewhat concerning and could lead to
    unfair consequences with such models.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The technical whitepaper I shared in the *Detecting pretraining bias* section
    also has many more details on the post-training metrics. You can find the paper
    here: [https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: After understanding how to measure the bias, both pretraining and post-training,
    we should also explore how the ML model makes decisions in the way it does with
    SageMaker Clarify.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining ML models using SHAP values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker Clarify also computes model-agnostic feature attribution based on
    the concept of Shapley values. Shapley values can be used to determine the contribution
    each feature makes to model predictions. Feature attribution helps explain how
    a model makes decisions. Having a quantifiable approach to describe how a model
    makes decisions enables us to have trust in an ML model that meets regulatory
    requirements and supports the human decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to setting up configurations to run bias analysis jobs using SageMaker
    Clarify, it takes three configurations to set up a model explainability job: a
    **data configuration**, a **model configuration**, and an **explainability configuration**.
    Let''s follow the next steps from the same notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a data configuration with the training dataset (matched). This is similar
    to the data configurations we created before. The code is illustrated in the following
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create or reuse the `model_config` argument that was created before for the
    post-training bias analysis job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a `clarify.SHAPConfig()` instance with a baseline. A baseline is an
    instance of a data point that would be used to compute the Shapley values with
    the input data. For the same model, you can expect to get different explanations
    with respect to different baselines, so the choice of a baseline is crucial. It
    is desirable to select a general baseline with very low information content, such
    as an average or median feature vector. In this case, in our example, we would
    interpret the model attribution as to why a particular person is predicted as
    a high-income earner compared to an average person. Alternatively, you can choose
    to explain the model with respect to a particular type of data. For example, we
    can choose a baseline from a similar demographic that represents the people in
    the inference. The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In our example, let's simulate an "average" high-income (`>50K`) person from
    the training data using `mode` for the baseline. The `num_samples` argument is
    used to determine the size of the generated synthetic dataset to compute the SHAP
    values. You can also leave it empty to make Clarify choose a number automatically.
    `agg_method='mean_abs'` denotes how to aggregate for global SHAP values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we start the analysis job with the configurations, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the processing job completes, we can view the results on the trial details
    page in SageMaker Experiments under the `education-num` feature, which represents
    the highest education level, contributes the most to predicting the income level
    (`>50K` or `<=50K`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Reviewing the model explainability results in SHAP values in
    SageMaker Studio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Reviewing the model explainability results in SHAP values in SageMaker
    Studio
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides global SHAP values, we can also review local SHAP explanations for
    any given data point to explain how a model makes predictions on this particular
    data point. SageMaker Clarify computes and saves local explanations for the entire
    dataset that is provided in `clarify.DataConfig()` in a `explainability_output_path`.
    We can plot the local SHAP values for each feature for a particular data point
    (the 500th row) with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As shown in *Figure 6.8*, we can see how the XGBoost model predicts this data
    point as <=50K. The `marital-status`, `education-num`, and `capital-gain` factors
    are the top three factors that the model thinks of this person as a low-income
    earner. Thanks to SHAP values computed by SageMaker Clarify, we can understand
    and explain how the model makes the prediction on an individual basis too.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Explaining individual prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17447_06_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Explaining individual prediction
  prefs: []
  type: TYPE_NORMAL
- en: Let's summarize the chapter after you've completed the example
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored biases in ML and ML explainability with an adult
    income example. We learned that the data could contain unfair biases against a
    certain group or category in the dataset, which could translate into an ML model
    making unfair predictions. We worked through an adult income-level prediction
    example in SageMaker Studio to analyze and compute any bias prior to model training
    using **SageMaker Clarify**. Clarify produces metrics to quantify imbalance in
    the dataset that could potentially lead to unfair biases. We mitigated the imbalances
    using sampling and matching techniques and proceeded to train an ML model. We
    further analyzed the resulting ML model for potential bias in predictions using
    SageMaker Clarify. Finally, we reviewed how the ML model makes decisions using
    SageMaker Clarify and SHAP values.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn where to go after training an ML model in
    SageMaker. Hosting an ML model in the cloud is critical for most ML use cases,
    and being able to use the right tool for model hosting from SageMaker is key to
    successful ML adoption for your organization. We will learn about various options
    for hosting an ML model and how to optimize compute resources and cost using SageMaker's
    hosting features.
  prefs: []
  type: TYPE_NORMAL
