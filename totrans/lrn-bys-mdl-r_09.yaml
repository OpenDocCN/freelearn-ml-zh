- en: Chapter 9. Bayesian Modeling at Big Data Scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we learned the principles of Bayesian inference in [Chapter 3](part0030.xhtml#aid-SJGS2
    "Chapter 3. Introducing Bayesian Inference"), *Introducing Bayesian Inference*,
    we saw that as the amount of training data increases, contribution to the parameter
    estimation from data overweighs that from the prior distribution. Also, the uncertainty
    in parameter estimation decreases. Therefore, you may wonder why one needs Bayesian
    modeling in large-scale data analysis. To answer this question, let us look at
    one such problem, which is building recommendation systems for e-commerce products.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical e-commerce store, there will be millions of users and tens of thousands
    of products. However, each user would have purchased only a small fraction (less
    than 10%) of all the products found in the store in their lifetime. Let us say
    the e-commerce store is collecting users' feedback for each product sold as a
    rating on a scale of 1 to 5\. Then, the store can create a user-product rating
    matrix to capture the ratings of all users. In this matrix, rows would correspond
    to users and columns would correspond to products. The entry of each cell would
    be the rating given by the user (corresponding to the row) to the product (corresponding
    to the column). Now, it is easy to see that although the overall size of this
    matrix is huge, only less than 10% entries would have values since every user
    would have bought only less than 10% products from the store. So, this is a highly
    sparse dataset. Whenever there is a machine learning task where, even though the
    overall data size is huge, the data is highly sparse, overfitting can happen and
    one should rely on Bayesian methods (reference 1 in the *References* section of
    this chapter). Also, many models such as Bayesian networks, Latent Dirichlet allocation,
    and deep belief networks are built on the Bayesian inference paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: When these models are trained on a large dataset, such as text corpora from
    Reuters, then the underlying problem is large-scale Bayesian modeling. As it is,
    Bayesian modeling is computationally intensive since we have to estimate the whole
    posterior distribution of parameters and also do model averaging of the predictions.
    The presence of large datasets will make the situation even worse. So what are
    the computing frameworks that we can use to do Bayesian learning at a large scale
    using R? In the next two sections, we will discuss some of the latest developments
    in this area.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing using Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last decade, tremendous progress was made in distributed computing when
    two research engineers from Google developed a computing paradigm called the **MapReduce**
    framework and an associated distributed filesystem called Google File System (reference
    2 in the *References* section of this chapter). Later on, Yahoo developed an open
    source version of this distributed filesystem named **Hadoop** that became the
    hallmark of Big Data computing. Hadoop is ideal for processing large amounts of
    data, which cannot fit into the memory of a single large computer, by distributing
    the data into multiple computers and doing the computation on each node locally
    from the disk. An example would be extracting relevant information from log files,
    where typically the size of data for a month would be in the order of terabytes.
  prefs: []
  type: TYPE_NORMAL
- en: To use Hadoop, one has to write programs using MapReduce framework to parallelize
    the computing. A Map operation splits the data into multiple key-value pairs and
    sends it to different nodes. At each of those nodes, a computation is done on
    each of the key-value pairs. Then, there is a shuffling operation where all the
    pairs with the same value of key are brought together. After this, a Reduce operation
    sums up all the results corresponding to the same key from the previous computation
    step. Typically, these MapReduce operations can be written using a high-level
    language called **Pig**. One can also write MapReduce programs in R using the
    **RHadoop** package, which we will describe in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: RHadoop for using Hadoop from R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RHadoop is a collection of open source packages using which an R user can manage
    and analyze data stored in the **Hadoop Distributed File System** (**HDFS**).
    In the background, RHadoop will translate these as MapReduce operations in Java
    and run them on HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various packages in RHadoop and their uses are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**rhdfs**: Using this package, a user can connect to an HDFS from R and perform
    basic actions such as read, write, and modify files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rhbase**: This is the package to connect to a HBASE database from R and to
    read, write, and modify tables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**plyrmr**: Using this package, an R user can do the common data manipulation
    tasks such as the slicing and dicing of datasets. This is similar to the function
    of packages such as **plyr** or **reshape2**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rmr2**: Using this package, a user can write MapReduce functions in R and
    execute them in an HDFS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the other packages discussed in this book, the packages associated with
    RHadoop are not available from CRAN. They can be downloaded from the GitHub repository
    at [https://github.com/RevolutionAnalytics](https://github.com/RevolutionAnalytics)
    and are installed from the local drive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample MapReduce code written using the rmr2 package to count the
    number of words in a corpus (reference 3 in the *References* section of this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step involves loading the `rmr2` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second step involves writing the Map function. This function takes each
    line in the text document and splits it into words. Each word is taken as a token.
    The function emits key-value pairs where each distinct word is a *key* and *value
    = 1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The third step involves writing a reduce function. This function groups all
    the same *key* from different mappers and sums their *value*. Since, in this case,
    each word is a *key* and the *value = 1*, the output of the reduce will be the
    count of the words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fourth step involves writing a word count function combining the map and
    reduce functions and executing this function on a file named `hdfs.data` stored
    in the HDFS containing the input text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fifth step involves getting the output file from HDFS and printing the
    top five lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Spark – in-memory distributed computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the issues with Hadoop is that after a MapReduce operation, the resulting
    files are written to the hard disk. Therefore, when there is a large data processing
    operation, there would be many read and write operations on the hard disk, which
    makes processing in Hadoop very slow. Moreover, the network latency, which is
    the time required to shuffle data between different nodes, also contributes to
    this problem. Another disadvantage is that one cannot make real-time queries from
    the files stored in HDFS. For machine learning problems, during training phase,
    the MapReduce will not persist over iterations. All this makes Hadoop not an ideal
    platform for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution to this problem was invented at Berkeley University''s AMP Lab in
    2009\. This came out of the PhD work of Matei Zaharia, a Romanian born computer
    scientist. His paper *Resilient Distributed Datasets: A Fault-Tolerant Abstraction
    for In-Memory Cluster Computing* (reference 4 in the *References* section of this
    chapter) gave rise to the Spark project that eventually became a fully open source
    project under Apache. Spark is an in-memory distributed computing framework that
    solves many of the problems of Hadoop mentioned earlier. Moreover, it supports
    more type of operations that just MapReduce. Spark can be used for processing
    iterative algorithms, interactive data mining, and streaming applications. It
    is based on an abstraction called **Resilient Distributed Datasets** (**RDD**).
    Similar to HDFS, it is also fault-tolerant.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark is written in a language called Scala. It has interfaces to use from Java
    and Python and from the recent version 1.4.0; it also supports R. This is called
    SparkR, which we will describe in the next section. The four classes of libraries
    available in Spark are SQL and DataFrames, Spark Streaming, MLib (machine learning),
    and GraphX (graph algorithms). Currently, SparkR supports only SQL and DataFrames;
    others are definitely in the roadmap. Spark can be downloaded from the Apache
    project page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Starting from 1.4.0 version, SparkR is included in Spark and no separate download
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to RHadoop, SparkR is an R package that allows R users to use Spark
    APIs through the `RDD` class. For example, using SparkR, users can run jobs on
    Spark from RStudio. SparkR can be evoked from RStudio. To enable this, include
    the following lines in your `.Rprofile` file that R uses at startup to initialize
    the environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, start RStudio and enter the following commands to start
    using SparkR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned, as of the latest version 1.5 when this chapter is in writing,
    SparkR supports limited functionalities of R. This mainly includes data slicing
    and dicing and summary stat functions. The current version does not support the
    use of contributed R packages; however, it is planned for a future release. On
    machine learning, currently SparkR supports the `glm( )` function. We will do
    an example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression using SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we will illustrate how to use SparkR for machine
    learning. For this, we will use the same dataset of energy efficiency measurements
    that we used for linear regression in [Chapter 5](part0041.xhtml#aid-173721 "Chapter 5. Bayesian
    Regression Models"), *Bayesian Regression Models*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Computing clusters on the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to process large datasets using Hadoop and associated R packages, one
    needs a cluster of computers. In today's world, it is easy to get using cloud
    computing services provided by Amazon, Microsoft, and others. One needs to pay
    only for the amount of CPU and storage used. No need for upfront investments on
    infrastructure. The top four cloud computing services are AWS by Amazon, Azure
    by Microsoft, Compute Cloud by Google, and Bluemix by IBM. In this section, we
    will discuss running R programs on AWS. In particular, you will learn how to create
    an AWS instance; install R, RStudio, and other packages in that instance; develop
    and run machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Popularly known as AWS, Amazon Web Services started as an internal project in
    Amazon in 2002 to meet the dynamic computing requirements to support their e-commerce
    business. This grew as an **infrastructure as a service** and in 2006 Amazon launched
    two services to the world, **Simple Storage Service** (**S3**) and **Elastic Computing
    Cloud** (**EC2**). From there, AWS grew at incredible pace. Today, they have more
    than 40 different types of services using millions of servers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running computing instances on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best place to learn how to set up an AWS account and start using EC2 is
    the freely available e-book from Amazon Kindle store named *Amazon Elastic Compute
    Cloud (EC2) User Guide* (reference 6 in the *References* section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we only summarize the essential steps involved in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an AWS account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sign in to the AWS management console ([https://aws.amazon.com/console/](https://aws.amazon.com/console/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the EC2 service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Amazon Machine Instance (AMI)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose an instance type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a public-private key-pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tag instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a security group (policy specifying who can access the instance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review and launch the instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to your instance using SSH (from Linux/Ubuntu), Putty (from Windows),
    or a browser using the private key provided at the time of configuring security
    and the IP address given at the time of launching. Here, we are assuming that
    the instance you have launched is a Linux instance.
  prefs: []
  type: TYPE_NORMAL
- en: Installing R and RStudio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install R and RStudio, you need to be an authenticated user. So, create
    a new user and give the user administrative privilege (sudo). After that, execute
    the following steps from the Ubuntu shell:'
  prefs: []
  type: TYPE_NORMAL
- en: Edit the `/etc/apt/sources.list` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following line at the end:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the keys for the repository to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the package list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the latest version of R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install gdebi to install Debian packages from the local disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the RStudio package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install RStudio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the installation is completed successfully, RStudio running on your AWS
    instance can be accessed from a browser. For this, open a browser and enter the
    URL `<your.aws.ip.no>:8787`.
  prefs: []
  type: TYPE_NORMAL
- en: If you are able to use your RStudio running on the AWS instance, you can then
    install other packages such as rhdfs, rmr2, and more from RStudio, build any machine
    learning models in R, and run them on the AWS cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from R and RStudio, AWS also supports Spark (and hence SparkR). In the
    following section, you will learn how to run Spark on an EC2 cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Running Spark on EC2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can launch and manage Spark clusters on Amazon EC2 using the `spark-ec2`
    script located in the `ec2` directory of Spark in your local machine. To launch
    a Spark cluster on EC2, use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the `ec2` directory in the Spark folder in your local machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `<keypair>` is the name of the keypair you used for launching the EC2
    service mentioned in the *Creating and running computing instances on AWS* section
    of this chapter. The `<key-file>` is the path in your local machine where the
    private key has been downloaded and kept. The number of worker nodes is specified
    by `<num-slaves>`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To run your programs in the cluster, first SSH into the cluster using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After logging into the cluster, you can use Spark as you use on the local machine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: More details on how to use Spark on EC2 can be found in the Spark documentation
    and AWS documentation (references 5, 6, and 7 in the *References* section of the
    chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microsoft Azure has full support for R and Spark. Microsoft had bought Revolution
    Analytics, a company that started building and supporting an enterprise version
    of R. Apart from this, Azure has a machine learning service where there are APIs
    for some Bayesian machine learning models as well. A nice video tutorial of how
    to launch instances on Azure and how to use their machine learning as a service
    can be found at the Microsoft Virtual Academy website (reference 8 in the *References*
    section of the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: IBM Bluemix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bluemix has full support for R through the full set of R libraries available
    on their instances. IBM also has integration of Spark into their cloud services
    in their roadmap plans. More details can be found at their documentation page
    (reference 9 in the *References* section of the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Other R packages for large scale machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from RHadoop and SparkR, there are several other native R packages specifically
    built for large-scale machine learning. Here, we give a brief overview of them.
    Interested readers should refer to *CRAN Task View: High-Performance and Parallel
    Computing with R* (reference 10 in the *References* section of the chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: Though R is single-threaded, there exists several packages for parallel computation
    in R. Some of the well-known packages are **Rmpi** (R version of the popular message
    passing interface), **multicore**, **snow** (for building R clusters), and **foreach**.
    From R 2.14.0, a new package called **parallel** started shipping with the base
    R. We will discuss some of its features here.
  prefs: []
  type: TYPE_NORMAL
- en: The parallel R package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **parallel** package is built on top of the multicore and snow packages.
    It is useful for running a single program on multiple datasets such as K-fold
    cross validation. It can be used for parallelizing in a single machine over multiple
    CPUs/cores or across several machines. For parallelizing across a cluster of machines,
    it evokes MPI (message passing interface) using the Rmpi package.
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate the use of parallel package with the simple example of computing
    a square of numbers in the list 1:100000\. This example will not work in Windows
    since the corresponding R does not support the multicore package. It can be tested
    on any Linux or OS X platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequential way of performing this operation is to use the `lapply` function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `mclapply` function of the parallel package, this computation can
    be achieved in much less time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If the dataset is so large that it needs a cluster of computers, we can use
    the `parLapply` function to run the program over a cluster. This needs the Rmpi
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The foreach R package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a new looping construct in R that can be executed in parallel across
    multicores or clusters. It has two important operators: `%do%` for repeatedly
    doing a task and `%dopar%` for executing tasks in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the squaring function we discussed in the previous section can
    be implemented using a single line command using the foreach package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also do an example of quick sort using the `foreach` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: These packages are still undergoing a lot of development. They have not yet
    been used in a large way for Bayesian modeling. It is easy to use them for Bayesian
    inference applications such as Monte Carlo simulations.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Revisit the classification problem in [Chapter 6](part0049.xhtml#aid-1ENBI1
    "Chapter 6. Bayesian Classification Models"), *Bayesian Classification Models*.
    Repeat the same problem using the `glm()` function of SparkR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Revisit the linear regression problem, we did in this chapter, using SparkR.
    After creating the AWS instance, repeat this problem using RStudio server on AWS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"MapReduce Implementation of Variational Bayesian Probabilistic Matrix Factorization
    Algorithm". In: IEEE Conference on Big Data. pp 145-152\. 2013'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dean J. and Ghemawat S. "MapReduce: Simplified Data Processing on Large Clusters".
    Communications of the ACM 51 (1). 107-113'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R](https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chowdhury M., Das T., Dave A., Franklin M.J., Ma J., McCauley M., Shenker S.,
    Stoica I., and Zaharia M. "Resilient Distributed Datasets: A Fault-Tolerant Abstraction
    for In-Memory Cluster Computing". NSDI 2012\. 2012'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Amazon Elastic Compute Cloud (EC2) User Guide*, Kindle e-book by Amazon Web
    Services, updated April 9, 2014'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark documentation for AWS at [http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS documentation for Spark at [http://aws.amazon.com/elasticmapreduce/details/spark/](http://aws.amazon.com/elasticmapreduce/details/spark/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Microsoft Virtual Academy website at [http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning](http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IBM Bluemix Tutorial at [http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html](http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CRAN Task View for contributed packages in R at [https://cran.r-project.org/web/views/HighPerformanceComputing.html](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter of the book, we covered various frameworks to implement
    large-scale machine learning. These are very useful for Bayesian learning too.
    For example, to simulate from a posterior distribution, one could run a Gibbs
    sampling over a cluster of machines. We learned how to connect to Hadoop from
    R using the RHadoop package and how to use R with Spark using SparkR. We also
    discussed how to set up clusters in cloud services such as AWS and how to run
    Spark on them. Some of the native parallelization frameworks such as parallel
    and foreach functions were also covered.
  prefs: []
  type: TYPE_NORMAL
- en: The overall aim of this book was to introduce readers to the area of Bayesian
    modeling using R. Readers should have gained a good grasp of theory and concepts
    behind Bayesian machine learning models. Since the examples were mainly given
    for the purposes of illustration, I urge readers to apply these techniques to
    real-world problems to appreciate the subject of Bayesian inference more deeply.
  prefs: []
  type: TYPE_NORMAL
