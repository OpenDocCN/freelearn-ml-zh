<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering Algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to introduce some fundamental clustering algorithms, discussing both their strengths and weaknesses. The field of unsupervised learning, as well as any other machine learning approach, must be always based on the concept of Occam's razor. Simplicity must always be preferred when performance meets the requirements. However, in this case, the ground truth can be unknown. When a clustering algorithm is adopted as an exploratory tool, we can only assume that the dataset represents a precise data generating process. If this assumption is correct, the best strategy is to determine the number of clusters to maximize the internal cohesion (denseness) and the external separation. This means that we expect to find blobs (or isles) whose samples share some common and partially unique features.</p>
<p>In particular, the algorithms we are going to present are:</p>
<ul>
<li><strong>k-Nearest Neighbors</strong> (<strong>KNN</strong>) based on KD Trees and Ball Trees</li>
<li>K-means and K-means++</li>
<li>Fuzzy C-means</li>
<li>Spectral clustering based on the Shi-Malik algorithm</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-Nearest Neighbors</h1>
                </header>
            
            <article>
                
<p>This algorithm belongs to a particular family called <strong>instance-based</strong> (the methodology is called <strong>instance-based learning</strong>). It differs from other approaches because it doesn't work with an actual mathematical model. On the contrary, the inference is performed by direct comparison of new samples with existing ones (which are defined as instances). KNN is an approach that can be easily employed to solve clustering, classification, and regression problems (even if, in this case, we are going to consider only the first technique). The main idea behind the clustering algorithm is very simple. Let's consider a data generating process <em>p<sub>data</sub></em> and a finite a dataset drawn from this distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ec298b90-b592-4e50-9c92-e68076fde214.png" style="width:25.50em;height:2.08em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Each sample has a dimensionality equal to <em>N</em>. We can now introduce a distance function <em>d(x<sub>1</sub>, x<sub>2</sub>)</em>, which in the majority of cases can be generalized with the Minkowski distance:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/70d36b7b-9ee9-4eba-aa27-4360263526e7.png" style="width:23.00em;height:6.00em;"/></div>
<p>When <em>p = 2, d<sub>p</sub></em> represents the classical Euclidean distance, that is normally the default choice. In particular cases, it can be useful to employ other variants, such as <em>p = 1</em> (which is the Manhattan distance) or <em>p &gt; 2</em>. Even if all the properties of a metric function remain unchanged, different values of <em>p</em> yield results that can be <em>semantically</em> diverse. As an example, we can consider the distance between points <em>x<sub>1</sub> = (0, 0)</em> and <em>x<sub>2</sub> = (15, 10)</em> as a function of <em>p</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7d5b4c92-d564-4b54-85bb-3d61b20fd323.png" style="width:61.00em;height:33.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Minkowski distance between (0, 0) and (15, 10) as a function of parameter p</div>
<p>The distance decreases monotonically with <em>p</em> and converges to the largest component absolute difference, <em>|x<sub>1</sub><sup>(j)</sup> - <span>x</span><sub>2</sub><sup>(j)</sup>|</em>, when <em>p → ∞</em>. Therefore, whenever it's important to weight all the components in the same way in order to have a consistent metric, small values of <em>p</em> are preferable (for example, <em>p=1</em> or <em>2</em>). This result has also been studied and formalized by <span>Aggarwal, Hinneburg, and Keim</span> (in <em>On the Surprising Behavior of Distance Metrics in High Dimensional Space</em>,<em> </em><em>Aggarwal C. C.</em>, <em>Hinneburg A.</em>, <em>Keim D. A.</em>, <em>ICDT 2001</em>), who proved a fundamental inequality. If we consider a generic distribution <em>G</em> of <em>M</em> points <em>x<sub>i</sub> ∈ (0, 1)<sup>d</sup></em>, a distance function based on the <em>L<sub>p</sub></em> norm, and the maximum <em>D<sub>max</sub><sup>p</sup></em> and minimum <em>D<sub>min</sub><sup>p</sup></em> distances (<span>computed using the <em>L</em></span><sub><em>p</em><span> </span></sub><span>norm)</span> between two points, <em>x<sub>j</sub></em> and <em>x</em><sub><em>k</em></sub> drawn from <em>G</em> and <em>(0, 0)</em>, the following inequality holds:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/56a0f65a-f7ba-4ec2-8edd-b701125f9255.png" style="width:39.25em;height:5.50em;"/></div>
<p>It's clear that when the input dimensionality is very high and <em>p &gt;&gt; 2</em>, the expected value, <em>E[<span>D</span><sub>max</sub><sup>p</sup><span> - D</span><sub>min</sub><sup>p</sup>]</em>, becomes bounded between two constants, <em>k<sub>1</sub> (C<sub>p</sub>d<sup>1/p-1/2</sup>)</em> and <em>k<sub>2 </sub><span> ((M-1)C</span><sub>p</sub>d<sup>1/p-1/2</sup>) → 0</em>, reducing the actual effect of almost any distance. In fact, given two generic couples of points <em>(x<sub>1</sub>, x<sub>2</sub>)</em> and <em><span>(x</span><sub>3</sub><span>,</span><span> </span><span>x</span><sub>4</sub><span>)</span></em><sub> </sub>drawn from <em>G</em>, the natural consequence of the following inequality is that <em>d<sub>p</sub>(x<sub>1</sub>, x<sub>2</sub>) ≈ <span>d</span><sub>p</sub><span>(x</span><sub>3</sub><span>, x</span><sub>4</sub></em><span><em>)</em> when <em>p → ∞</em>,</span> independently of their relative positions. This important result confirms the importance of choosing the right metric according to the dimensionality of the dataset and that <em>p = 1</em> is the best choice when <em>d &gt;&gt; 1</em>, while <em>p &gt;&gt; 1</em> can produce inconsistent results due the ineffectiveness of the metric. To see direct confirmation of this phenomenon, it's possible to run the following snippet, which computes the average difference between maximum and minimum distances considering <kbd>100</kbd> sets containing <kbd>100</kbd> samples drawn from a uniform distribution, <em>G ∼ U(0, 1)</em>. In the snippet, the case of <kbd>d=2</kbd>, <kbd>100</kbd>, <kbd>1000</kbd> is analyzed with Minkowski metrics with <kbd>P</kbd>= <kbd>1</kbd>, <kbd>2</kbd>, <kbd>10</kbd>, <kbd>100</kbd> (the final values depend on the random seed and how many times the experiment is repeated):</p>
<pre>import numpy as np<br/><br/>from scipy.spatial.distance import pdist<br/><br/>nb_samples = 100<br/>nb_bins = 100<br/><br/>def max_min_mean(p=1.0, d=2):<br/>    Xs = np.random.uniform(0.0, 1.0, size=(nb_bins, nb_samples, d))<br/>    <br/>    pd_max = np.zeros(shape=(nb_bins, ))<br/>    pd_min = np.zeros(shape=(nb_bins, ))<br/><br/>    for i in range(nb_bins):<br/>        pd = pdist(Xs[i], metric='minkowski', p=p)<br/>        pd_max[i] = np.max(pd)<br/>        pd_min[i] = np.min(pd)<br/>        <br/>    return np.mean(pd_max - pd_min)<br/><br/>print('P=1 -&gt; {}'.format(max_min_mean(p=1.0)))<br/>print('P=2 -&gt; {}'.format(max_min_mean(p=2.0)))<br/>print('P=10 -&gt; {}'.format(max_min_mean(p=10.0)))<br/>print('P=100 -&gt; {}'.format(max_min_mean(p=100.0)))<br/><br/>P=1 -&gt; 1.79302317381
P=2 -&gt; 1.27290283592
P=10 -&gt; 0.989257369005
P=100 -&gt; 0.983016242436<br/><br/>print('P=1 -&gt; {}'.format(max_min_mean(p=1.0, d=100)))<br/>print('P=2 -&gt; {}'.format(max_min_mean(p=2.0, d=100)))<br/>print('P=10 -&gt; {}'.format(max_min_mean(p=10.0, d=100)))<br/>print('P=100 -&gt; {}'.format(max_min_mean(p=100.0, d=100)))<br/><br/>P=1 -&gt; 17.1916057948
P=2 -&gt; 1.76155714836
P=10 -&gt; 0.340453945928
P=100 -&gt; 0.288625281313<br/><br/>print('P=1 -&gt; {}'.format(max_min_mean(p=1.0, d=1000)))<br/>print('P=2 -&gt; {}'.format(max_min_mean(p=2.0, d=1000)))<br/>print('P=10 -&gt; {}'.format(max_min_mean(p=10.0, d=1000)))<br/>print('P=100 -&gt; {}'.format(max_min_mean(p=100.0, d=1000)))<br/><br/>P=1 -&gt; 55.2865105705
P=2 -&gt; 1.77098913218
P=10 -&gt; 0.130444336657
P=100 -&gt; 0.0925427145923</pre>
<p>A particular case, that is a direct consequence of the previous inequality is when the largest absolute difference between components determines the most important factor of a distance, large values of <em>p</em> can be employed. For example, if we consider three points, <em><span>x</span><sub>1</sub></em><span><em> = (0, 0)</em>, <em>x</em></span><em><sub>2</sub></em><span><em> = (15, 10)</em>, and <em>x<sub>3</sub> = (15, 0)</em>, <em>d<sub>2</sub>(x<sub>1</sub>, x<sub>2</sub>) ≈ 18</em> and <em>d<sub>2</sub>(x<sub>1</sub>, x<sub>3</sub>) = 15</em>. So, if we set a threshold at <em>d = 16</em> centered at <em>x<sub>1</sub></em>, <em>x</em><sub><em>2</em> </sub>is outside the boundaries. If instead <em>p = 15</em>, both distances become close to <em>15</em> and the two points (<em>x<sub>2</sub></em> and <em>x<sub>3</sub></em>) are inside the boundaries. A particular use of large values of <em>p</em> is when it's important to take into account the inhomogeneity among components. For example, some feature vectors can represent the age and height of a set of people. Considering a test person <em>x = (30, 175)</em>, with large <em>p</em> values, the distances between <em>x</em> and two samples <em>(35, 150)</em> and <em>(25, 151)</em> are almost identical (about <em>25.0</em>), and the only dominant factor becomes the height difference (independent from the age). </span></p>
<p>The KNN algorithm determines the <em>k</em> closest samples of each training point. When a new sample is presented, the procedure is repeated with two possible variants:</p>
<ul>
<li>With a predefined value of <em>k</em>, the KNN are computed</li>
<li>With a predefined radius/threshold <em>r</em>, all the neighbors whose distance is less than or equal to the radius are computed</li>
</ul>
<p>The philosophy of KNN is that similar samples can share their features. For example, a recommendation system can cluster users using this algorithm and, given a new user, find the most similar ones (based, for example, on the products they bought) to recommend the same category of items. In general, a similarity function is defined as the reciprocal of a distance (there are some exceptions, such as the cosine similarity):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ca9f5020-6d8d-41d1-8dc7-a6e6e42ab4a9.png" style="width:39.00em;height:3.92em;"/></div>
<p>Two different users, <em>A</em> and <em>B</em>, who are classified as neighbors, will differ under some viewpoints, but, at the same time, they will share some peculiar features. This statement authorizes us to increase the homogeneity by <em>suggesting the differences</em>. For example, if <em>A</em> liked book <em>b<sub>1</sub></em> and <em>B</em> liked <em>b<sub>2</sub></em>, we can recommend <em>b</em><sub><em>1</em> </sub>to <em>B</em> and <em>b<sub>2</sub></em> to <em>A</em>. If our hypothesis was correct, the similarity between <em>A</em> and <em>B</em> will be increased; otherwise, the two users will move towards other clusters that better represent their behavior.</p>
<p>Unfortunately, the <em>vanilla</em> algorithm (in Scikit-Learn it is called the <strong>brute-force</strong> algorithm) can become extremely slow with a large number of samples because it's necessary to compute all the pairwise distances in order to answer any query. With <em>M</em> points, this number is equal to <em>M<sup>2</sup></em>, which is often unacceptable (if <em>M</em> = 1,000, each query needs to compute a million distances). More precisely, as the computation of a distance in an N-dimensional space requires <em>N</em> operations, the total complexity becomes <em>O(<span>M</span><sup>2</sup>N)</em>, which can be reasonable only for small values of both <em>M</em> and <em>N</em>. That's why some important strategies have been implemented to reduce the computational complexity. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">KD Trees</h1>
                </header>
            
            <article>
                
<p>As all KNN queries can be considered search problems, one of the most efficient way to reduce the overall complexity is to reorganize the dataset into a tree structure. In a binary tree (one-dimensional data), the average computational complexity of a query is <em>O(log M)</em>, because we assume we have almost the same number of elements in each branch (if the tree is completely unbalanced, all the elements are inserted sequentially and the resulting structure has a single branch,  so the complexity becomes <em>O(M))</em>. In general, the real complexity is slightly higher than <em>O(log M)</em>, but the operation is always much more efficient than a vanilla search, which is <em>O(<span>M</span><sup>2</sup>)</em>.</p>
<p>However, we normally work with N-dimensional data and the previous structure cannot be immediately employed. KD Trees extend the concept of a binary for <em>N &gt;</em> <em>1</em>. In this case, a split cannot be immediately performed and a different strategy must be chosen. The easiest way to solve this problem is to select a feature at each level <em>(1, 2, ..., N)</em> and repeat the process until the desired depth is reached. In the following diagram, there's an example of KD Trees with three-dimensional points:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/74005f85-2f8e-424a-be82-9c6f72f74ae3.png" style="width:27.33em;height:13.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of three-dimensional KD Tree</div>
<p>The root is point <strong>(5, 3, 7)</strong>. The first split is performed considering the first feature, so two children are <strong>(2, 1, 1)</strong> and <strong>(8, 4, 3)</strong>. The second one operates on the second feature and so on. <span>The average computational complexity is <em>O(N log M)</em>, but if the distribution is very asymmetric, the probability that the tree becomes unbalanced is very high. To mitigate this issue, it's possible to select the feature corresponding to the median of the (sub-)dataset and to continue splitting with this criterion. In this way, the tree is guaranteed to be balanced. However, the average complexity is always proportional to the dimensionality and this can dramatically affect the performance.</span></p>
<p>For example, if <em>M</em> = 10,000 and <em>N</em> = 10, using the <em>log<sub>10</sub></em>, <em>O(</em><span><em>N log M) = O(40)</em>, while, with <em>N</em> = 1,000, the complexity becomes <em>O(40,000)</em>. Generally, KD Trees suffers the <em>curse of dimensionality</em> and when <em>N</em> becomes large, the average complexity is about <em>O(MN)</em>, which is always better than the <em>vanilla</em> algorithm, but often too expensive for real-life applications.  Therefore, KD Trees is really effective only when the dimensionality is not too high. In all other cases, the probability of having an unbalanced tree and the resulting computational complexity suggest employing a different method.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ball Trees</h1>
                </header>
            
            <article>
                
<p>An alternative to KD Trees is provided by <strong>Ball Trees</strong>. The idea is to rearrange the dataset in a way that is almost insensitive to high-dimensional samples. A ball is defined as a set of points whose distance from a center sample is less than or equal to a fixed radius:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/13fcde74-8f62-45d7-a0db-f74613cc6670.png" style="width:20.83em;height:1.83em;"/></div>
<p>Starting from the first main ball, it's possible to build smaller ones nested into the parent ball and stop the process when the desired depth has been reached. A fundamental condition is that a point can always belong to a single ball. In this way, considering the cost of the N-dimensional distance, the computational complexity is <em>O(N log M)</em> and doesn't suffer the curse of dimensionality like KD Trees. The structure is based on hyperspheres, whose boundaries are defined by the equations (given a center point <em>x</em> and a radius <em>R<sub>i</sub></em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/22f759e3-1db7-473c-a070-6d18716385e2.png" style="width:15.92em;height:2.00em;"/></div>
<p>Therefore, the only operation needed to find the right ball is measuring the distance between a sample and the centers starting from the smallest balls. If a point is outside the ball, it's necessary to move upwards and check the parents, until the ball containing the sample is found. In the following diagram, there's an example of Ball Trees with two levels:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7b0af978-f4d6-4bdc-99e5-396ba696ef36.png" style="width:10.50em;height:19.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of Ball Trees with seven bidimensional points and two levels</div>
<p>In this example, the seven bidimensional points are split first into two balls containing respectively three and four points. At the second level, the second ball is split again into two smaller balls containing two points each. This procedure can be repeated until a fixed depth is reached or by imposing the maximum number of elements that a leaf must contain (in this case, it can be equal to <em>3</em>).</p>
<p>Both KD Trees and Ball Trees can be efficient structures to reduce the complexity of KNN queries. However, when fitting a model, it's important to consider both the <em>k <span>parameter</span></em> (which normally represents the average or the standard number of neighbors computed in a query) and the maximum tree depth. These particular structures are not employed for common tasks (such as sorting) and their efficiency is maximized when all the requested neighbors can be found in the same sub-structure (with a size <em>K&lt;&lt; M</em>, to avoid an implicit fallback to the <em>vanilla</em> algorithm). In other words, the tree has the role of reducing the dimensionality of the search space by partitioning it into reasonably small regions.</p>
<p>At the same time, if the number of samples contained in a leaf is small, the number of tree nodes grows and the complexity is subsequently increased. The negative impact is doubled because on average it's necessary to explore more nodes and if <em>k</em> is much greater than the number of elements contained in a node, it's necessary to merge the samples belonging to different nodes. On the other side, a very large number of samples per node leads to a condition that is close to the <em>vanilla</em> algorithm. For example, if <em>M</em> = 1,000 and each node contains 250 elements, once the right node is computed, the number of distances to compute is comparable with the initial dataset size and no real advantage is achieved by employing a tree structure. An acceptable practice is to set the size of a life equal to <em>5 ÷ 10</em> times the average value of <em>k</em>, to maximize the probability to find all the neighbors inside the same leaf. However, every specific problem must be analyzed (while also benchmarking the performances) in order to find the most appropriate value. If different values for <em>k</em> are necessary, it's important to consider the relative frequencies of the queries. For example, if a program needs 10 <em>5-NN</em> queries and 1 <em>50-NN</em> query, it's probably better to set a leaf size equal to 25, even if the <em>50-NN</em> query will be more expensive. In fact, setting a good value for a second query (for example, 200) will dramatically increase the complexity of the first 10 queries, driving to a performance loss.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of KNN with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>In order to test the KNN algorithm, we are going to use the MNIST handwritten digit dataset provided directly by Scikit-Learn. It is made up of 1,797 8 × 8 grayscale images representing the digits from 0 to 9. The first step is loading it and normalizing all the values to be bounded between 0 and 1:</p>
<pre>import numpy as np<br/><br/>from sklearn.datasets import load_digits<br/><br/>digits = load_digits()<br/>X_train = digits['data'] / np.max(digits['data'])</pre>
<p>The dictionary <kbd>digits</kbd> contains both the images, <kbd>digits['images']</kbd>, and the flattened 64-dimensional arrays, <kbd>digits['data']</kbd>. Scikit-Learn implements different classes (for example, it's possible to work directly with KD Trees and Ball Trees using the KDTree and BallTree classes) that can be used in the context of KNN (as clustering, classification, and regression algorithms). However, we're going to employ the main class, <kbd>NearestNeighbors</kbd>, which allows performing clustering and queries based either on the number of neighbors or on the radius of a ball centered on a sample:</p>
<pre>from sklearn.neighbors import NearestNeighbors<br/><br/>knn = NearestNeighbors(n_neighbors=50, algorithm='ball_tree')<br/>knn.fit(X_train)</pre>
<p>We have chosen to have a default number of neighbors equal to <kbd>50</kbd> and an algorithm based on a <kbd>ball_tree</kbd>. The leaf size (<kbd>leaf_size</kbd>) <span>parameter</span> has been kept to its default value equal to <kbd>30</kbd>. We have also employed the default metric (Euclidean), but it's possible to change it using the <kbd>metric</kbd> and <kbd>p</kbd> parameters (which is the order of the Minkowski metric). Scikit-Learn supports all the metrics implemented by SciPy in the <kbd>scipy.spatial.distance</kbd> package. However, in the majority of cases, it's sufficient to use a Minkowski metric and adjust the value of <kbd>p</kbd> if the results are not acceptable with any number of neighbors. Other metrics, such as the cosine distance, can be employed when the similarity must not be affected by the Euclidean distance, but only by the angle between two vectors pointing at the samples. Applications that use this metric include, for example, deep learning models for natural language processing, where the words are embedded into feature vectors whose semantic similarity is proportional to their Cosine distance.</p>
<p>We can now query the model in order to find 50 neighbors of a sample. For our purposes, we have selected the sample with index 100, which represents a 4 (the images have a very low resolution, but it's always possible to distinguish the digit):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6ff7c51f-cf3f-4943-ba4c-66f73b368ebb.png" style="width:7.67em;height:7.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Sample digit used to query the KNN model</div>
<p>The query can be performed using the instance method <kbd>kneighbors</kbd>, which allows specifying the number of neighbors (<kbd>n_neighbors</kbd> <span>parameter</span> the default is the value selected during the instantiation of the class) and whether we want to also get the distances of each neighbor (the <kbd>return_distance</kbd> parameter). In this example, we are also interested in evaluating <em>how far</em> the neighbors are from the center, so we set <kbd>return_distance=True</kbd>:</p>
<pre>distances, neighbors = knn.kneighbors(X_train[100].reshape(1, -1), return_distance=True)<br/><br/>print(distances[0])<br/><br/>[ 0.          0.91215747  1.16926793  1.22633855  1.24058958  1.32139841
  1.3564084   1.36645069  1.41972709  1.43341812  1.45236875  1.50130152
  1.52709897  1.5499496   1.62379763  1.62620148  1.6345871   1.64292993
  1.66770801  1.70934929  1.71619128  1.71619128  1.72187216  1.73317808
  1.74888357  1.75445861  1.75668367  1.75779514  1.76555586  1.77878118
  1.788636    1.79408751  1.79626348  1.80169191  1.80277564  1.80385871
  1.80494113  1.8125      1.81572988  1.83498978  1.84771819  1.87291551
  1.87916205  1.88020112  1.88538789  1.88745861  1.88952706  1.90906554
  1.91213232  1.92333532]</pre>
<p>The first neighbor is always the center, so its distance is <kbd>0</kbd>. The other ones range from 0.9 to 1.9. Considering that, in this case, the maximum possible distance is 8 (between a 64-dimensional vector <em>a = (1, 1, ..., 1)</em> and the null vector), the result could be acceptable. In order to get confirmation, we can plot the neighbors as bidimensional 8 × 8 arrays (the returned array, <kbd>neighbors</kbd>, contains the indexes of the samples). The result is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/da803e16-8f68-468b-95ff-b477a2594189.png" style="width:25.33em;height:22.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">50 neighbors selected by the KNN model</div>
<p>As it's possible to see, there are no errors, but all the shapes are slightly different. In particular, the last one, which is also the farthest, has a lot of white pixels (corresponding to the value 1.0), explaining the reason of a distance equal to about 2.0. I invite the reader to test the <kbd>radius_neighbors</kbd> method until spurious values appear among the results. It's also interesting to try this algorithm with the Olivetti faces dataset, whose complexity is higher and many more geometrical parameters can influence the similarity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means</h1>
                </header>
            
            <article>
                
<p>When we discussed the Gaussian mixture algorithm, we defined it as <em>Soft K-means</em>. The reason is that each cluster was represented by three elements: mean, variance, and weight. Each sample always belongs to all clusters with a probability provided by the Gaussian distributions. This approach can be very useful when it's possible to manage the probabilities as weights, but in many other situations, it's preferable to determine a single cluster per sample. Such an approach is called hard clustering and K-means can be considered the hard version of a Gaussian mixture. In fact, when all variances <em>Σ<sub>i</sub> → 0</em>, the distributions degenerate to Dirac's Deltas, which represent perfect spikes centered at a specific point. In this scenario, the only possibility to determine the most appropriate cluster is to find the shortest distance between a sample point and all the centers (from now on, we are going to call them <em>centroids</em>). This approach is also based on an important double principle that should be taken into account in every clustering algorithm. The clusters must be set up to maximize:</p>
<ul>
<li>The intra-cluster cohesion</li>
<li>The inter-cluster separation</li>
</ul>
<p>This means that we expect to label high-density regions that are well separated from each other. When this is not possible, the criterion must try to minimize the intra-cluster average distance between samples and centroid. This quantity is also called <em>inertia</em> and it's defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b5909e8f-cde4-4615-be26-c9981119b84d.png" style="width:15.92em;height:5.08em;"/></div>
<p>High levels of inertia imply low cohesion because there are probably too many points belongings to clusters whose centroids are too far away. The problem can be solved by minimizing the previous quantity. However, the computational complexity needed to find the global minimum is exponential (K-means belongs to the class of NP-Hard problems). The alternative approach employed by the K-means algorithm, also known as <strong>Lloyd's algorithm</strong>, is iterative and starts from selecting <em>k</em> random <span>centroids</span> (in the next section, we're going to analyze a more efficient method) and adjusting them until their configuration becomes stable.</p>
<p>The dataset to cluster (with <em>M</em> samples) is represented as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cf1fde0d-5770-444d-8b44-b80daebea84a.png" style="width:25.92em;height:2.08em;"/></div>
<p>An initial guess for the centroids is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b8e117ce-ddea-4bc9-8eb8-63cc585ac54b.png" style="width:31.58em;height:3.08em;"/></p>
<p>There are no particular restrictions on the initial values. However, the choice can influence both the convergence speed and the minimum that is found. The iterative procedure will loop over the dataset, computing the Euclidean distance between <em>x</em><sub><em>i</em> </sub>and each <em>μ<sub>j</sub></em> and assigning a cluster based on the criterion:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a387604a-6b26-42a4-994a-6b7b4e32e3c2.png" style="width:19.83em;height:2.58em;"/></div>
<p>Once all the samples have been clustered, the new centroids are computed:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/11d25a25-0e01-491b-8c64-8ef6a3dd42f8.png" style="width:19.50em;height:4.50em;"/></div>
<p>The quantity <em>N<sub>Cj</sub></em> represents the number of points belonging to cluster <em>j</em>. At this point, the inertia is recomputed and the new value is compared with the previous one. The procedure will stop either after a fixed number of iterations or when the variations in the inertia become smaller than a predefined threshold. Lloyd's algorithm is very similar to a particular case of the EM algorithm. In fact, the first step of each iteration is the computation of an <em>expectation</em> (the centroid configuration), while the second step maximizes the intra-cluster cohesion by minimizing the inertia.</p>
<p>The complete vanilla K-means algorithm is:</p>
<ol>
<li>Set a maximum number of iterations <em>N</em><sub><em>max</em></sub>.</li>
<li>Set a tolerance <em>Thr</em>.</li>
<li>Set the value of <em>k</em> (number of expected clusters).</li>
<li>Initialize vector <em>C<sup>(0)</sup></em> with random values. They can be points belonging to the dataset or sampled from a suitable distribution.</li>
<li>Compute the initial inertia <em>S</em><sup><em>(0)</em></sup></li>
</ol>
<ol start="6">
<li>Set <em>N = 0</em>.</li>
<li>While <em>N &lt; N<sub>max</sub></em> or <em>||<span>S</span><sup>(t)</sup> - <span>S</span><sup>(t-1)</sup>|| &gt; Thr</em>:
<ol>
<li><em>N = N + 1</em></li>
<li>For <em>x<sub>i</sub></em> in <em>X</em>:
<ol>
<li>Assign <em>x<sub>i</sub></em> to a cluster using the shortest distance between <em>x<sub>i</sub></em> and <em>μ</em><sub><em>j</em></sub></li>
</ol>
</li>
<li>Recompute the centroid vector <em><span>C</span></em><sup><em>(t)</em></sup></li>
<li>Recompute the inertia <em>S</em><sup><em>(t)</em></sup></li>
</ol>
</li>
</ol>
<p>The algorithm is quite simple and intuitive, and there are many real-life applications based on it. However, there are two important elements to consider. The first one is the convergence speed. It's easy to show that every initial guess drives to a convergence point, but the number of iterations is dramatically influenced by this choice and there's no guarantee to find the global minimum. If the initial centroids are close to the final ones, the algorithm needs only a few steps to correct the values, but when the choice is totally random, it's not uncommon to need a very high number of iterations. If there are <em>N</em> samples and <em>k</em> centroids, <em>Nk</em> distances must be computed at each iteration, leading to an inefficient result. In the next paragraph, we'll show how it's possible to initialize the centroids to minimize the convergence time.</p>
<p>Another important aspect is that, contrary to KNN, K-means needs to predefine the number of expected clusters. In some cases, this is a secondary problem because we already know the most appropriate value for k. However, when the dataset is high-dimensional and our knowledge is limited, this choice could be hazardous. A good approach to solve the issue is to analyze the final inertia for a different number of clusters. As we expect to maximize the intra-cluster cohesion, a small number of clusters will lead to an increased inertia. We try to pick the highest point below a maximum tolerable value. Theoretically, we can also pick <em>k = N</em>. In this case, the inertia becomes zero because each point represents the centroid of its cluster, but a large value for <em>k</em> transforms the clustering scenario into a fine-grained partitioning that might not be the best strategy to capture the feature of a consistent group. It's impossible to define a rule for the upper bound <em>k<sub>max</sub></em>, but we assume that this value is always much less than <em>N</em>. The best choice is achieved by selecting <em>k</em> to minimize the inertia, selecting the values from a set bounded, for example, between <em>2</em> and <em>k<sub>max</sub></em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means++</h1>
                </header>
            
            <article>
                
<p>We have said that a good choice for the initial centroids can improve the convergence speed and leads to a minimum that is closer to the global optimum of the inertia S. Arthur and Vassilvitskii (in <em>The Advantages of Careful Seeding, </em><em>Arthur, D., Vassilvitskii S., k-means++:</em> <em>Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em>) proposed a method called K-means++, which allows increasing the accuracy of the initial centroid guess considering the most likely final configuration.</p>
<p>In order to expose the algorithm, it's useful to introduce a function, <em>D(x, i)</em>, which is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e91fd7c6-9b48-409b-8445-b5850263705f.png" style="width:22.67em;height:1.50em;"/></div>
<p><em>D(x, i)</em> defines the shortest distance between each sample and one of the centroids already selected. As the process is incremental, this function must be recomputed after all steps. For our purposes, let's also define an auxiliary probability distribution (we omit the index variable for simplicity):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/52eda745-0e5f-4253-8612-9d9bff3f739c.png" style="width:12.25em;height:4.00em;"/></div>
<p class="mce-root">The first centroid <em>μ<sub>0</sub></em> is sampled from <em>X</em> using a uniform distribution. The next steps are:</p>
<ol>
<li>Compute <em>D(x, i)</em> for all <em>x ∈ X</em> considering the centroids already selected</li>
<li>Compute <em>G(x)</em></li>
<li>Select the next centroid <em><span>μ</span><sub>i</sub></em> from <em>X</em> with a probability <em>G(x)</em></li>
</ol>
<p>In the aforementioned paper, the authors showed a very important property. If we define <em>S<sup>*</sup></em> as the global optimum of <em>S</em>, a K-means++ initialization determines an upperbound for the expected value of the actual inertia:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/414461c4-d6c3-4280-82bf-0c33ce065114.png" style="width:12.75em;height:1.67em;"/></div>
<p>This condition is often expressed by saying that K-means++ is <em>O(log k)</em>-competitive. When <em>k</em> is sufficiently small, the probability of finding a local minimum close to the global one increases. However, K-means++ is still a probabilistic approach and different initializations on the same dataset lead to different initial configurations. A good practice is to run a limited number of initializations (for example, ten) and pick the one associated with the smallest inertia. When training complexity is not a primary issue, this number can be increased, but different experiments showed that the improvement achievable with a very large number of trials is negligible when compared to the actual computational cost. The default value in Scikit-Learn is ten and the author suggests to keep this value in the majority of cases. If the result continues to be poor, it's preferable to pick another method. Moreover, there are problems that cannot be solved using K-means (even with the best possible initialization), because one of the assumptions of the algorithm is that each cluster is a hypersphere and the distances are measured using a Euclidean function. In the following sections, we're going to analyze other algorithms that are not constrained to work with such limitations and can easily solve clustering problems using asymmetric cluster geometries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of K-means with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>In this example, we continue using the MNIST dataset (the <kbd>X_train</kbd> <span>array</span> is the same defined in the paragraph dedicated to KNN), but we want also to analyze different clustering evaluation methods. The first step is visualizing the inertia corresponding to different numbers of clusters. We are going to use the <kbd>KMeans</kbd> class, which accepts the <kbd>n_clusters</kbd> parameter and employs the K-means++ initialization as the default method (as explained in the previous section, in order to find the best initial configuration, Scikit-Learn performs several attempts and selects the configuration with the lowest inertia; it's possible to change the number of attempts through the <kbd>n_iter</kbd> parameter):</p>
<pre>import numpy as np<br/><br/>from sklearn.cluster import KMeans<br/><br/>min_nb_clusters = 2<br/>max_nb_clusters = 20<br/><br/>inertias = np.zeros(shape=(max_nb_clusters - min_nb_clusters + 1,))<br/><br/>for i in range(min_nb_clusters, max_nb_clusters + 1):<br/>    km = KMeans(n_clusters=i, random_state=1000)<br/>    km.fit(X_train)<br/>    inertias[i - min_nb_clusters] = km.inertia_</pre>
<p>We are supposing to analyze the range [<kbd>2</kbd>, <kbd>20</kbd>]. After each training session, the final inertia can be retrieved using the <kbd>inertia_</kbd> instance variable. The following graph shows the plot of the values as a function of the number of clusters:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/d09423dd-2a55-40ea-b9b1-e14ef6cf4059.png" style="width:58.83em;height:34.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Inertia as a function of the number of clusters</div>
<p>As expected, the function is decreasing, starting from a value of about 7,500 and reaching about 3,700 with <strong>20</strong> clusters. In this case, we know that the real number is <strong>10</strong>, but it's possible to discover it by observing the trend. The slope is quite high before <strong>10</strong>, but it starts decreasing more and more slowly after this threshold. This is a signal that informs us that some clusters are not well separated, even if their internal cohesion is high. In order to confirm this hypothesis, we can set <kbd>n_clusters=10</kbd> and, first of all, check the centroids at the end of the training process:</p>
<pre>km = KMeans(n_clusters=10, random_state=1000)<br/>Y = km.fit_predict(X_train)</pre>
<p>The centroids are available through the <kbd>cluster_centers_</kbd> <span>instance variable</span>. In the following screenshot, there's a plot of the corresponding bidimensional arrays:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/89dd07e7-a96b-41dc-aabe-580fbdaa62d4.png" style="width:22.92em;height:2.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">K-means centroid at the end of the training process</div>
<p>All the digits are present and there are no duplicates. This confirms that the algorithm has successfully separated the sets, but the final inertia (which is about 4,500) informs us that there are probably wrong assignments. To obtain confirmation, we can plot the dataset using a dimensionality-reduction method, such as t-SNE (see <a href="c23d1792-167f-416e-a848-fa7a10777697.xhtml" target="_blank">Chapter 3</a>, <em>Graph-Based Semi-Supervised Learning</em> for further details):</p>
<pre>from sklearn.manifold import TSNE<br/><br/>tsne = TSNE(n_components=2, perplexity=20.0, random_state=1000)<br/>X_tsne = tsne.fit_transform(X_train)</pre>
<p>At this point, we can plot the bidimensional dataset with the corresponding cluster labels:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/40643b02-fd5f-442d-849b-1231625cd23a.png" style="width:48.17em;height:26.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">t-SNE representation of the MNIST dataset; the labels correspond to the clusters</div>
<p>The plot confirms that the dataset is made up of well-separated blobs, but a few samples are assigned to the wrong cluster (this is not surprising considering the similarity between some pairs of digits). An important observation can further explain the trend of the inertia. In fact, the point where the slope changes almost abruptly corresponds to 9 clusters. Observing the t-SNE plot, we can immediately discover the reason: the cluster corresponding to the digit <strong>7</strong> is indeed split into 3 blocks. The main one contains the majority of samples, but there are another 2 smaller blobs that are wrongly <em>attached</em> to clusters <strong>1</strong> and <strong>9</strong>. This is not surprising, considering that the digit <strong>7</strong> can be very similar to a distorted <strong>1</strong> or <strong>9</strong>. However, these two spurious blobs are always at the boundaries of the wrong clusters (remember that the geometric structures are hyperspheres), confirming that the metric has successfully detected a low similarity. If a group of wrongly assigned samples were in the middle of a cluster, it would have meant that the separation failed dramatically and another method should be employed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation metrics</h1>
                </header>
            
            <article>
                
<p>In many cases, it's impossible to evaluate the performance of a clustering algorithm using only a visual inspection. Moreover, it's important to use standard objective metrics that allow for comparing different approaches. We are now going to introduce some methods based on the knowledge of the ground truth (the correct assignment for each sample) and one common strategy employed when the true labels are unknown.</p>
<p>Before discussing the scoring functions, we need to introduce a standard notation. If there are <em>k</em> clusters, we define the true labels as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/200a6236-e235-436e-88cf-31d7b3d481ce.png" style="width:38.83em;height:2.17em;"/></div>
<p>In the same way, we can define the predicted labels:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/89db817b-4b6b-43bd-995b-3a2191f0ec35.png" style="width:39.58em;height:3.08em;"/></div>
<p class="mce-root">Both sets can be considered as sampled from two discrete random variables (for simplicity, we denote them with the same names), whose probability mass functions are <em>P<sub>true</sub>(y)</em> and <em>P<sub>pred</sub>(y)</em> with a generic <em>y</em> <span><em>∈ {y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>k</sub>}</em> (<em>y<sub>i</sub></em> represents the index of the <em>i<sup>th</sup></em> cluster)</span>. These two probabilities can be approximated with a frequency count; so, for example, the probability <em>P<sub>true</sub>(1)</em> is computed as the number of samples whose true label is <em>1 n<sub>true</sub>(1)</em> over the total number of samples <em>M</em>. In this way, we can define the entropies:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/993508fd-4259-4fc0-848b-7e3104f105ae.png" style="width:27.67em;height:4.92em;"/></div>
<p>These quantities describe the intrinsic uncertainty of the random variables. They are maximized when all the classes have the same probability, while, for example, they are null if all the samples belong to a single class (minimum uncertainty). We also need to know the uncertainty of a random variable <em>Y</em> given another one <em>X</em>. This can be achieved using the conditional entropy <em>H(Y|X)</em>. In this case, we need to compute the joint probability <em>p(x, y)</em> because the definition of <em>H(Y|X)</em> is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1dc59e67-4559-4193-bd3b-8abea170fd2b.png" style="width:26.00em;height:4.50em;"/></div>
<p class="mce-root">In order to approximate the previous expression, we can define the function <em>n(i<sub>true</sub>, j<sub>pred</sub>)</em>, which counts the number of samples with the true label <em>i</em> assigned to cluster <em>j</em>. In this way, if there are <em>M</em> samples, the approximated conditional entropies become:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1d0accfc-3ec0-4701-b822-b7c83718e106.png" style="width:41.83em;height:6.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Homogeneity score</h1>
                </header>
            
            <article>
                
<p>This score is useful to check whether the clustering algorithm meets an important requirement: a cluster should contain only samples belonging to a single class. It's defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b43c6cea-4195-4e57-bbde-3147395cdb21.png" style="width:15.50em;height:4.08em;"/></div>
<p class="mce-root">It's bounded between <em>0</em> and <em>1</em>, with low values indicating a low homogeneity. In fact, when the knowledge of <em>Y<sub>pred</sub></em> reduces the uncertainty of <em>Y<sub>true</sub></em>, <em>H(<span>Y</span><sub>true</sub>|<span>Y</span><sub>pred</sub>)</em> becomes smaller (<em>h → 1</em>) and viceversa. For our example, the homogeneity score can be computed as:</p>
<pre>from sklearn.metrics import homogeneity_score<br/><br/>print(homogeneity_score(digits['target'], Y))<br/>0.739148799605</pre>
<p><span>The</span> <kbd>digits['target']</kbd><span> array contains the true labels while </span><kbd>Y</kbd><span> contains the predictions (all the functions we are going to use accept the true labels as the first parameter and the predictions as the second one). </span>The homogeneity score confirms that the clusters are rather homogeneous, but there's still a moderate level of uncertainty because some clusters contain wrong assignments. This method, together with the other ones, can be used to search for the right number of clusters and tune up all supplementary hyperparameters (such as the number of iterations or the metric function).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Completeness score</h1>
                </header>
            
            <article>
                
<p>This score is complementary to the previous one. Its purpose is to provide a piece of information about the assignment of samples belonging to the same class. More precisely, a good clustering algorithm should assign all samples with the same true label to the same cluster. From our previous analysis, we know that, for example, the digit 7 has been wrongly assigned to both clusters 9 and 1; therefore, we expect a non-perfect completeness score. The definition is symmetric to the homogeneity score:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ad248be3-4fcd-4d42-b5bd-1ccbdf582556.png" style="width:15.25em;height:4.17em;"/></div>
<p>The rationale is very intuitive. When <em><span>H(</span><span>Y</span><sub>pred</sub><span>|</span><span>Y</span><sub>true</sub></em><span><em>)</em> is low <em>(c → 1)</em>, it means that the knowledge of the ground truth reduces the uncertainty about the predictions. Therefore, if we know that all the sample of subset <em>A</em> have the same label <em>y<sub>i</sub></em>, we are quite sure that all the corresponding predictions have been assigned to the same cluster. The completeness score for our example is:</span></p>
<pre>from sklearn.metrics import completeness_score<br/><br/>print(completeness_score(digits['target'], Y))<br/>0.747718831945</pre>
<p>Again, the value confirms our hypothesis. The residual uncertainty is due to a lack of completeness because a few samples with the same label have been split into blocks that are assigned to wrong clusters. It's obvious that a perfect scenario is characterized by having both homogeneity and completeness scores equal to 1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusted Rand Index</h1>
                </header>
            
            <article>
                
<p>This score is useful to compare the original label distribution with the clustering prediction. Ideally, we'd like to reproduce the exact ground truth distribution, but in general, this is very difficult in real-life scenarios. A way to measure the discrepancy is provided by the Adjusted Rand Index. In order to compute this score, we need to define the auxiliary variables:</p>
<ul>
<li><em>a</em>: Number of sample pairs <em>(y<sub>i</sub>, y<sub>j</sub>)</em> that have the same true label and that are assigned to the same cluster</li>
<li><em>b</em>: <span>Number of sample pairs <em>(y</em></span><em><sub>i</sub><span>, y</span><sub>j</sub></em><span><em>)</em> that have a different true label and that are assigned to different clusters</span></li>
</ul>
<p>The Rand Index is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/992c113a-e970-4932-a0cc-e028e4932e15.png" style="width:7.17em;height:4.25em;"/></div>
<p>The Adjusted Rand Index is the Rand Index corrected for chance and it's defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/760ff071-962c-48e9-89fe-f22ed58b9a1e.png" style="width:10.83em;height:2.83em;"/></div>
<p class="mce-root">The <em>R<sub>A</sub></em> <span>measure </span>is bounded between <em>-1</em> and <em>1</em>. A value close to <em>-1</em> indicates a prevalence of wrong assignments, while a value close to <em>1</em> indicates that the clustering algorithm is correctly reproducing the ground truth distribution. The Adjusted Rand Score for our example is:</p>
<pre>from sklearn.metrics import adjusted_rand_score<br/><br/>print(adjusted_rand_score(digits['target'], Y))<br/>0.666766395716</pre>
<p>This value confirms that the algorithm is working well (because it's positive), but it can be further optimized by trying to reduce the number of wrong assignments. The Adjusted Rand Score is a very powerful tool when the ground truth is known and can be employed as a single method to optimize all the hyperparameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Silhouette score</h1>
                </header>
            
            <article>
                
<p>This measure doesn't need to know the ground truth and can be used to check, at the same time, the intra-cluster cohesion and the inter-cluster separation. In order to define the Silhouette score, we need to introduce two auxiliary functions. The first one is the average intra-cluster distance of a sample <em>x<sub>i</sub></em> belonging to a cluster <em>C<sub>j</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/50c668b9-bd9f-4a5a-a257-bfb2bc5e6cc0.png" style="width:24.50em;height:4.33em;"/></div>
<p>In the previous expression, <em>n(k)</em> is the number of samples assigned to the cluster <em>C<sub>j</sub></em> and <em>d(a, b)</em> is a standard distance function (in the majority of cases, the Euclidean distance is chosen). We need also to define the lowest inter-cluster distance which can be interpreted as the average nearest-cluster distance. In the sample <em>x<sub>i</sub> ∈ <span>C</span><sub>j</sub></em>, let's call <em>C<sub>t</sub></em> the nearest cluster; therefore, the function is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8c3bf91c-6358-45db-8b22-adb7bb65fe27.png" style="width:22.00em;height:3.83em;"/></div>
<p>The Silhouette score for sample <em>x<sub>i</sub></em> is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a7a3b0de-56da-4336-8f76-608d444341a6.png" style="width:17.17em;height:3.83em;"/></div>
<p class="mce-root">The value of <em>s(x<sub>i</sub>)</em>, like for the Adjusted Rand Index, is bounded between <em>-1</em> and <em>1</em>. A value close to <em>-1</em> indicates that <em>b(<span>x</span><sub>i</sub>) &lt;&lt; a(<span>x</span><sub>i</sub>)</em>, so the average intra-cluster distance is greater than the average nearest-cluster index and sample <em>x<sub>i</sub></em> is wrongly assigned. Viceversa, a value close to <em>1</em> indicates that the algorithm achieved a very good level of internal cohesion and inter-cluster separation (because <em>a<span>(</span><span>x</span><sub>i</sub><span>) &lt;&lt; b(</span><span>x</span><sub>i</sub></em><span><em>)</em>). Contrary to the other measure, the Silhouette score isn't a cumulative function and must be computed for each sample. A feasible strategy is to analyze the average value, but in this way, it's not possible to determine which clusters have the highest impact on the result. Another approach (the most common), is based on Silhouette plots, which display the score for each cluster in descending order. In the following snippet, we create plots for four different values of <kbd>n_clusters</kbd> (<kbd>3</kbd>, <kbd>5</kbd>, <kbd>10</kbd>, <kbd>12</kbd>):</span></p>
<pre>import matplotlib.pyplot as plt<br/>import matplotlib.cm as cm<br/><br/>import numpy as np<br/><br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import silhouette_samples<br/><br/>fig, ax = plt.subplots(2, 2, figsize=(15, 10))<br/><br/>nb_clusters = [3, 5, 10, 12]<br/>mapping = [(0, 0), (0, 1), (1, 0), (1, 1)]<br/><br/>for i, n in enumerate(nb_clusters):<br/>    km = KMeans(n_clusters=n, random_state=1000)<br/>    Y = km.fit_predict(X_train)<br/><br/>    silhouette_values = silhouette_samples(X_train, Y)<br/><br/>    ax[mapping[i]].set_xticks([-0.15, 0.0, 0.25, 0.5, 0.75, 1.0])<br/>    ax[mapping[i]].set_yticks([])<br/>    ax[mapping[i]].set_title('%d clusters' % n)<br/>    ax[mapping[i]].set_xlim([-0.15, 1])<br/>    ax[mapping[i]].grid()<br/>    y_lower = 20<br/><br/>    for t in range(n):<br/>        ct_values = silhouette_values[Y == t]<br/>        ct_values.sort()<br/><br/>        y_upper = y_lower + ct_values.shape[0]<br/><br/>        color = cm.Accent(float(t) / n)<br/>        ax[mapping[i]].fill_betweenx(np.arange(y_lower, y_upper), 0, ct_values, facecolor=color, edgecolor=color)<br/><br/>        y_lower = y_upper + 20</pre>
<p>The result is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/477a571a-9618-442a-8274-fd035aa2d80b.png" style="width:48.42em;height:32.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Silhouette plots for different number of clusters</div>
<p>The analysis of a Silhouette plot should follow some common guidelines:</p>
<ul>
<li>The width of each block must be proportional to the number of samples that are expected to belong to the corresponding cluster. If the label distribution is uniform, all the blocks must have a similar width. Any asymmetry indicates wrong assignments. For example, in our case, we know that the right number of clusters is ten, but a couple of blocks are thinner than the other ones. This means that a cluster contains fewer samples than expected and the remaining ones have been assigned to wrong partitions.</li>
<li>The shape of a block shouldn't be sharp and peaked (like a knife) because it means that many samples have a low Silhouette score. The ideal (realistic) scenario is made up of shapes similar to cigars with a minimum difference between the highest and lowest values. Unfortunately, this is not always possible to achieve, but it's always preferable to tune up the algorithm if the shapes are like the ones plotted in the first diagram (three clusters). </li>
<li>The maximum Silhouette score should be close to <em>1</em>. Lower values (like in our example) indicate the presence of partial overlaps and wrong assignments. Negative values must be absolutely avoided (or limited to a very small number of samples) because they show a failure in the clustering process. Moreover, it's possible to prove that convex clusters (like K-means hyperspheres) lead to higher values. This is due to the properties of the commons distance functions (like the Euclidean distance) that can suggest a low internal cohesion whenever the shape of a cluster is concave (think about a circle and a half-moon). In this case, the process of embedding the shape into a convex geometry leads to a lower density and this negatively affects the Silhouette score.</li>
</ul>
<p class="mce-root">In our particular case,  we cannot accept having a number of clusters different from ten. However, the corresponding Silhouette plot is not perfect. We know the reasons for such imperfections (the structure of the samples and the high similarity of different digits) and it's quite difficult to avoid them using an algorithm like K-means. The reader can try to improve the performances by increasing the number of iterations, but in these cases, if the result doesn't meet the requirements, it's preferable to adopt another method (like the spectral clustering method, which can manage asymmetric clusters and more complex geometries).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fuzzy C-means</h1>
                </header>
            
            <article>
                
<p>We have already talked about the difference between hard and soft clustering, comparing K-means with Gaussian mixtures. Another way to address this problem is based on the concept of <strong>fuzzy logic</strong>, which was proposed for the first time by Lotfi Zadeh in 1965 (for further details, a very good reference is <em>An Introduction to Fuzzy Sets</em>,<em> Pedrycz W.</em>, <em>Gomide F</em>., <em>The MIT Press</em>). Classic logic sets are based on the law of excluded middle that, in a clustering scenario, can be expressed by saying that a sample <em>x<sub>i</sub></em> can belong only to a single cluster <em>c<sub>j</sub></em>. Speaking more generally, if we split our universe into labeled partitions, a hard clustering approach will assign a label to each sample, while a fuzzy (or soft) approach allows managing a membership degree (in Gaussian mixtures, this is an actual probability), <em>w<sub>ij</sub></em> which expresses how strong the relationship is between sample <em>x<sub>i</sub></em> and cluster <em>c<sub>j</sub></em>. Contrary to other methods, by employing fuzzy logic it's possible to define asymmetric sets that are not representable with continuous functions (such as trapezoids). This allows for achieving further flexibility and an increased ability to adapt to more complex geometries. In the following graph, there's an example of fuzzy sets:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4b18aefc-8dab-441f-84bc-0b28dcdb99e0.png" style="width:45.92em;height:23.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of fuzzy sets representing the seniority level of an employee according to years of experience</div>
<p>The graph represents the seniority level of an employee given his/her years of experience. As we want to cluster the entire population into three groups (<strong>Junior</strong>, <strong>Middle level</strong>, and <strong>Senior</strong>), three fuzzy sets have been designed. We have assumed that a young employee is keen and can quickly reach a <strong>Junior</strong> level after an initial apprenticeship period. The possibility to work with complex problems allows him/her to develop skills that are fundamental to allowing the transition between the <strong>Junior</strong> and <strong>Middle</strong> levels. After about <strong>10</strong> years, the employee can begin to consider himself/herself as a s<em>enior apprentice</em> and, after about 25 years, the experience is enough to qualify him/her as a full <strong>Senior</strong> until the end of his/her career. As this is an imaginary example, we haven't tuned all the values up, but it's easy to compare, for example, employee A with 9 years of experience with another employee B with 18 years of experience. The former is about 50% <strong>Junior</strong> (decreasing), 90% <strong>Middle level</strong> (reaching its climax), and 10% <strong>Senior</strong> (increasing). The latter, instead, is 0% <strong>Junior</strong> (ending plateau), 30% <strong>Middle level</strong> (decreasing), and 60% <strong>Senior</strong> (increasing). In both cases, the values are not normalized so always sum up to 1 because we are more interested in showing the process and the proportions. The fuzziness level is lower in extreme cases, while it becomes higher when two sets intersect. For example, at about 15%, the <strong>Middle level</strong> and <strong>Senior</strong> are about 50%. As we're going to discuss, it's useful to avoid a very high fuzziness when clustering a dataset because it can lead to a lack of precision as the boundaries <em>fade out</em>, becoming completely fuzzy.</p>
<p>Fuzzy C-means is a generalization of a standard K-means, with a soft assignment and more <em>flexible</em> clusters. The dataset to cluster (containing <em>M</em> samples) is represented by:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c07db279-134e-436f-ab76-279f2f5608da.png" style="width:25.92em;height:2.08em;"/></div>
<p>If we assume we have <em>k</em> clusters, it's necessary to define a matrix <em>W ∈ ℜ<sup>M × k</sup></em> containing the membership degrees for each sample:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55c6d6da-90b6-40cd-9856-c9eb530a9f1d.png" style="width:17.92em;height:7.17em;"/></div>
<p>Each degree <em><span>w</span><sub>ij</sub> ∈ [0, 1]</em> and all rows must be normalized so that they always sum up to <em>1</em>. In this way, the membership degrees can be considered as probabilities (with the same semantics) and it's easier to make decisions with a prediction result. If a hard assignment is needed, it's possible to employ the same approach normally used with Gaussian mixtures: the winning cluster is selected by applying the <em>argmax</em> function. However, it's a good practice to employ soft clustering only when it's possible to manage the vectorial output. For example, the probabilities/membership degrees can be fed into a classifier in order to yield more complex predictions.</p>
<p>As with K-means, the problem can be expressed as the minimization of a <em>generalized inertia</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cca5be8b-4cc2-478c-9707-13b0386c7184.png" style="width:18.83em;height:5.08em;"/></div>
<p class="mce-root">The constant <em>m (m &gt; 1)</em> is an exponent employed to re-weight the membership degrees. A value very close to <em>1</em> doesn't affect the actual values. Greater <em>m</em> values reduce their magnitude. The same parameter is also used when recomputing the centroids and the new membership degrees and can drive to a different clustering result. It's rather difficult to define a global acceptable value; therefore, a good practice is to start with an average <em>m</em> (for example, 1.5) and perform a grid search (it's possible to sample from a Gaussian or uniform distribution) until the desired accuracy has been achieved.</p>
<p>Minimizing the previous expression is even more difficult than with a standard inertia; therefore, a <em>pseudo-Lloyd's algorithm</em> is employed. After a random initialization, the algorithm proceeds, alternating two steps (like an EM procedure) in order to determine the centroids, and recomputing the membership degrees to maximize the internal cohesion. The centroids are determined by a weighted average:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/895f3187-9892-4204-a793-64b072ae7f6c.png" style="width:12.00em;height:5.25em;"/></div>
<p class="mce-root">Contrary to K-means, the sum is not limited to the points belonging to a specific cluster because the weight factor will force the farthest points (<em>w<sub>ij</sub> ≈ 0.0</em>) to produce a contribution close to <em>0</em>. At the same time, as this is a soft-clustering algorithm, no exclusions are imposed, to allow a sample to belong to any number of clusters with different membership degrees. Once the centroids have been recomputed, the membership degrees must be updated using this formula:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4b92fce2-7280-4c79-97f7-8a8ec3db26b8.png" style="width:18.75em;height:6.92em;"/></div>
<p class="mce-root">This function behaves like a similarity. In fact, when sample <em>x<sub>i</sub></em> is very close to centroid <em>μ<sub>j</sub></em> (and relatively far from <em><span>μ</span><sub>p</sub></em> with <em>p ≠ j</em>), the denominator becomes small and <em>w<sub>ij</sub></em> increases. The exponent <em>m</em> directly influences the fuzzy partitioning, because when <em>m ≈ 1</em> <em>(m &gt; 1)</em>, the denominator is a sum of <em>quasi-</em>squared terms and the closest centroid can dominate the sum, yielding to a higher preference for a specific cluster. When <em>m &gt;&gt; 1</em>, all the terms in the sum tend to <em>1</em>, producing a more flat weight distribution with no well-defined preference. It's important to understand that, even when working with soft clustering, a fuzziness excess leads to inaccurate decisions because there are no factors that push a sample to clearly belong to a specific cluster. This means that problem is either ill-posed or, for example, the number of expected clusters is too high and doesn't represent the real underlying data structure. A good way to measure how much this algorithm is similar to a hard-clustering approach (such as K-means) is provided by the normalized <strong>Dunn's partitioning coefficient</strong>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/8895e7e0-576a-4fa9-87bd-4cfaad6a5d5a.png" style="width:28.75em;height:5.08em;"/></div>
<p class="mce-root">When <em>P<sub>c</sub></em> is bounded between <em>0</em> and <em>1</em>, when it's close to <em>0</em>, it means that the membership degrees have a flat distribution and the level of fuzziness is the highest possible. On the other side, if it's close to <em>1</em>, each row of <em>W</em> has a single dominant value, while all the others are negligible. This scenario resembles a hard-clustering approach. Higher <em>P<sub>c</sub></em> values are normally preferable because, even without renouncing to a degree of fuzziness, it allows making more precise decisions. Considering the previous example, <em>P<sub>c</sub></em> tends to <em>1</em> when the sets don't intersect, while it becomes 0 (complete fuzziness) if, for example, the three seniority levels are chosen to be identical and overlapping. Of course, we are interested in avoiding such extreme scenarios by limiting the number of borderline cases. A grid search can be performed by analyzing different numbers of clusters and <em>m</em> values (in the example, we're going to do it with the MNIST handwritten digit dataset). A reasonable rule of thumb is to accept <em>P<sub>c</sub></em> values higher than <em>0.8</em>, but in some cases, that can be impossible. If we are sure that the problem is well-posed, the best approach is to choose the configuration that maximizes <em>P<sub>c</sub></em>, considering, however, that a final value less than <em>0.3</em>-<em>0.5</em> will lead to a very high level of uncertainty because the clusters are extremely overlapping.   </p>
<p>The complete<span> </span><strong>Fuzzy C-means</strong><span> </span>algorithm is:</p>
<ol>
<li>Set a maximum number of iteration <em>N</em><sub><em>max</em></sub></li>
<li>Set a tolerance <em>Thr</em></li>
<li>Set the value of <em>k</em> (number of expected clusters)</li>
<li>Initialize the matrix <em>W<sup>(0)</sup></em> with random values and normalize each row, dividing it by its sum</li>
<li>Set <em>N = 0</em></li>
<li>While <em>N &lt; N<sub>max</sub></em><span> </span>or <em>||<span>W</span><sup>(t)</sup><span> </span>- W<sup>(t-1)</sup>|| &gt; Thr</em>:
<ol>
<li><em>N = N + 1</em></li>
<li>For <em>j = 1</em> to <em>k</em>:
<ol>
<li>Compute the centroid vectors <em>μ<sub>j</sub></em></li>
</ol>
</li>
<li>Recompute the weight matrix <em>W<sup>(t)</sup></em></li>
<li>Normalize the rows of <em><span>W</span><sup>(t)</sup></em></li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of fuzzy C-means with Scikit-Fuzzy</h1>
                </header>
            
            <article>
                
<p>Scikit-Fuzzy (<a href="http://pythonhosted.org/scikit-fuzzy/">http://pythonhosted.org/scikit-fuzzy/</a>) is a Python package based on SciPy that allows implementing all the most important fuzzy logic algorithms (including fuzzy C-means). In this example, we continue using the MNIST dataset, but with a major focus on fuzzy partitioning. To perform the clustering, Scikit-Fuzzy implements the <kbd>cmeans</kbd> method (in the <kbd>skfuzzy.cluster</kbd> package) which requires a few mandatory parameters: <kbd>data</kbd>, which must be an array <em>D ∈ ℜ<sup>N × M</sup></em> (<em>N</em> is the number of features; therefore, the array used with Scikit-Learn must be transposed); <kbd>c</kbd>, the number of clusters; the coefficient <kbd>m</kbd>, <kbd>error</kbd>, which is the maximum tolerance; and <kbd>maxiter</kbd>, which is the maximum number of iterations. Another useful parameter (not mandatory) is the <kbd>seed</kbd> parameter which allows specifying the random seed to be able to easily reproduce the experiments. I invite the reader to check the official documentation for further information.</p>
<p>The first step of this example is performing the clustering:</p>
<pre>from skfuzzy.cluster import cmeans<br/><br/>fc, W, _, _, _, _, pc = cmeans(X_train.T, c=10, m=1.25, error=1e-6, maxiter=10000, seed=1000)</pre>
<p>The <kbd>cmeans</kbd> function returns many values, but for our purposes, the most important are: the first one, which is the array containing the cluster centroids; the second one, which is the final membership degree matrix; and the last one, the partition coefficient. In order to analyze the result, we can start with the partition coefficient:</p>
<pre>print(pc)<br/>0.632070870735</pre>
<p>This value informs us that the clustering is not very far from a hard assignment, but there's still a residual fuzziness. In this particular case, such a situation may be reasonable because we know that many digits are partially distorted and may appear very similar to other ones (such as 1, 7, and 9). However, I invite the reader to try different values for <kbd>m</kbd> and check how the partition coefficient changes. We can now display the centroids:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8e91fecf-26c7-4540-8336-10a38cdff61f.png" style="width:25.33em;height:2.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Centroids obtained by fuzzy C-means</div>
<p>All the different digit classes have been successfully found, but now, contrary to K-means, we can check the fuzziness of a <em>problematic</em> digit (representing a 7, with index 7), as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ef72db33-3a1f-4635-86bc-21046f0489da.png" style="width:5.00em;height:5.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Sample digit (a 7) selected to test the fuzziness</div>
<p>The membership degrees associated with the previous sample are:</p>
<pre>print(W[:, 7])<br/>[ 0.00373221  0.01850326  0.00361638  0.01032591  0.86078292  0.02926149
  0.03983662  0.00779066  0.01432076  0.0118298 ]</pre>
<p>The corresponding plot is:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/1402c1fd-4710-4810-aa75-e6b65900da8b.png" style="width:46.67em;height:25.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Fuzzy membership plot corresponding to a digit representing a 7</div>
<p>In this case, the choice of <em>m</em> has forced the algorithm to reduce the fuzziness. However, it's still possible to see three smaller peaks corresponding to the clusters centered respectively on 1, 8, and 5 (remember that the cluster indexes correspond to digits shown previously in the centroid plot). I invite the reader to analyze the fuzzy partitioning of different digits and replot it with different values of the <kbd>m</kbd> parameter. It will be possible to observe an increased fuzziness (corresponding also to smaller partitioning coefficients) with larger <em>m</em> values. This effect is due to a stronger overlap among clusters (observable also by plotting the centroids) and could be useful when it's necessary to detect the distortion of a sample. In fact, even if the main peak indicates the right cluster, the secondary ones, in descending order, inform us how much the sample is similar to other centroids and, therefore, if it contains features that are characteristics of other subsets.</p>
<p>Contrary to Scikit-Learn, in order to perform predictions, Scikit-Fuzzy implements the <kbd>cmeans_predict</kbd> method (in the same package), which requires the same parameters of <kbd>cmeans</kbd>, but instead of the number of clusters, <kbd>c</kbd> needs the final centroid array (the name of the parameter is <kbd>cntr_trained</kbd>). The function returns as a first value the corresponding membership degree matrix (the other ones are the same as <kbd>cmeans</kbd>). In the following snippet, we repeat the prediction for the same sample digit (representing a <kbd>7</kbd>):</p>
<pre>import numpy as np<br/><br/>from skfuzzy.cluster import cmeans_predict<br/><br/>new_sample = np.expand_dims(X_train[7], axis=1)<br/>Wn, _, _, _, _, _ = cmeans_predict(new_sample, cntr_trained=fc, m=1.25, error=1e-6, maxiter=10000, seed=1000)<br/><br/>print(Wn.T)<br/>[[ 0.00373221  0.01850326  0.00361638  0.01032591  0.86078292  0.02926149
   0.03983662  0.00779066  0.01432076  0.0118298 ]]</pre>
<div class="packt_infobox">Scikit-Fuzzy can be installed using the <kbd>pip install -U scikit-fuzzy</kbd> command. For further instructions, please visit <a href="http://pythonhosted.org/scikit-fuzzy/install.html">http://pythonhosted.org/scikit-fuzzy/install.html</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spectral clustering</h1>
                </header>
            
            <article>
                
<p>One of the most common problems of K-means and other similar algorithms is the assumption we have only hyperspherical clusters. This condition can be acceptable when the dataset is split into blobs that can be easily embedded into a regular geometric structure. However, it fails whenever the sets are not separable using regular shapes. Let's consider, for example, the following bidimensional dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b518a8fc-a97e-4537-aa69-aab5e242a7a7.png" style="width:48.92em;height:24.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Sinusoidal dataset</div>
<p>As we are going to see in the example, any attempt to separate the upper sinusoid from the lower one using K-means will fail. The reason is quite obvious: a circle that contains the upper set will also contain part of the (or the whole) lower set. Considering the criterion adopted by K-means and imposing two clusters, the inertia will be minimized by a vertical separation corresponding to about <em>x<sub>0</sub> = 0</em>. Therefore, the resulting clusters are completely mixed and only a dimension is contributing to the final configuration. However, the two sinusoidal sets are well-separated and it's not difficult to check that, selecting a point <em>x<sub>i</sub></em> from the lower set, it's always possible to find a ball containing only samples belonging to the same set. We have already discussed this kind of problem when Label Propagation algorithms were discussed and the logic behind <strong>spectral clustering</strong> is essentially the same (for further details, I invite the reader to check <a href="c23d1792-167f-416e-a848-fa7a10777697.xhtml" target="_blank">Chapter 2</a>, <em>Graph-Based Semi-Supervised Learning</em>).</p>
<p>Let's suppose we have a dataset <em>X</em> sampled from a data generating process <em>p<sub>data</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/de0b8670-6120-4780-a06d-e9f5fed4fec7.png" style="width:25.92em;height:2.08em;"/></div>
<p class="mce-root">We can build a graph <em><span>G = {V, E}</span></em>, where the vertices are the points and the edges are determined using an <em>affinity matrix</em> <em>W</em>. Each element <em>w<sub>ij</sub></em> must express the affinity between sample <em>x<sub>i</sub></em> and sample <em>x<sub>j</sub></em>. <em>W</em> is normally built using two different approaches:</p>
<ul>
<li>KNN: In this case, we can build the number of neighbors to take into account for each point <em>x<sub>i</sub></em>. <em>W</em> can be built as a <em>connectivity matrix</em> (expressing only the existence of a connection between two samples) if we adopt the criterion:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0c812c71-cd10-4c28-a03f-d1402b1bbe6a.png" style="width:17.58em;height:3.83em;"/></div>
<p style="padding-left: 60px" class="mce-root CDPAlignLeft CDPAlign">Alternatively, it's possible to build a <em>distance matrix</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8461b117-636f-467c-9d5e-9e12604f3d72.png" style="width:22.00em;height:3.83em;"/></div>
<ul>
<li><strong>Radial basis function</strong> (<strong>RBF</strong>): The previous methods can lead to graphs which are not fully connected because samples can exist that have no neighbors. In order to obtain a fully connected graph, it's possible to employ an RBF (this approach has also been used in the Kohonen map algorithm):</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a556e026-41e4-4fa9-b3fe-b29c1f4677d3.png" style="width:12.58em;height:2.67em;"/></div>
<p style="padding-left: 60px">The <em>γ</em> parameter allows controlling the amplitude of the Gaussian function, reducing or increasing the number of samples with a high weight (so <em>actual neighbors</em>). However, a weight is assigned to all points and the resulting graph will always be connected (even if many elements are close to zero).</p>
<p class="mce-root">In both cases, the elements of <em>W</em> will represent a measure of affinity (or <em>closeness</em>) between points and no restrictions are imposed on the global geometry (contrary to K-means). In particular, using a KNN connectivity matrix, we are implicitly segmenting the original dataset into smaller regions with a high level of internal cohesion. The problem that we need to solve now is to find out a way to merge all the regions belonging to the same cluster. The approach we are going to present here has been proposed by <em>Normalized Cuts and Image Segmentation</em>,<em> J. Shi</em> and <em>J. Malik</em>,<em> IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>Vol. 22</em>, <em>08/2000</em>, and it's based on the normalized graph Laplacian:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/78efd83b-f387-4263-af2c-bc0c8b5f32ee.png" style="width:10.08em;height:1.67em;"/></div>
<p class="mce-root">The matrix <em>D</em>, called the degree matrix, is the same as discussed in <a href="c23d1792-167f-416e-a848-fa7a10777697.xhtml" target="_blank">Chapter 3</a>, <em>Graph-Based Semi-Supervised Learning</em> and it's defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce6d4ad0-cc93-4eca-9301-48fac0602e25.png" style="width:19.00em;height:4.08em;"/></div>
<p>It's possible to prove the following properties (the formal proofs are omitted but they can be found in texts such as <em>Functions and Graphs Vol. 2</em>, <em>Gelfand I. M</em>., <em>Glagoleva E. G.</em>, <em>Shnol E. E.</em>, <em>The MIT Press</em>:</p>
<ul>
<li>The eigenvalues <em>λ<sub>i</sub></em> and the eigenvectors <em>v<sub>i</sub></em> of <em>L<sub>n</sub></em> can be found by solving the problem <em>Lv =</em> <span><em>λDv</em>, where <em>L</em> is the unnormalized graph Laplacian <em>L = D - W</em></span></li>
<li><em>L<sub>n</sub></em> always has an eigenvalue equal to <em>0</em> (with a multiplicity <em>k</em>) with a corresponding eigenvector <em>v<sub>o</sub> = (1, 1, ..., 1)</em></li>
<li>As <em>G</em> is undirected and all <em>w<sub>ij</sub> ≥ 0</em>, the number of connected components <em>k</em> of <em>G</em> is equal to the multiplicity of the null eigenvalue</li>
</ul>
<p>In other words, the normalized graph Laplacian encodes the information about the number of connected components and provides us with a new reference system where the clusters can be separated using regular geometric shapes (normally hyperspheres). To better understand how this approach works without a non-trivial mathematical approach, it's important to expose another property of <em>L<sub>n</sub></em>.</p>
<p>From linear algebra, we know that each eigenvalue <em>λ</em> of a matrix <em>M ∈ ℜ<sup>n × n</sup></em> spans a corresponding eigenspace, which is a subset of <em><span>ℜ</span><sup>n</sup></em> containing all eigenvectors associated with <span><em>λ</em> plus the null vector. Moreover, given a set</span> <em>S</em> ⊆ <em>ℜn</em><span><sup> </sup>and a countable subset <em>C</em> (it's possible to extend the definition to generic subsets but in our context the datasets are always countable), we can define a vector <em>v ∈ ℜ<sup>n</sup></em> as an </span><em>indicator vector</em><span>, if <em>v<sup>(i)</sup> = 1</em> if the vector <em>c<sub>i</sub> ∈ S</em> and <em>v<sup>(i)</sup> = 0</em> otherwise. If we consider the null eigenvalues of <em>L<sub>n</sub></em> and we assume that their number is <em>k</em> (corresponding to the multiplicity of the eigenvalue <em>0</em>), it's possible to prove that the corresponding eigenvectors are indicator vectors for eigenspaces spanned by each of them. From the previous statements, we know that these eigenspaces correspond to the connected components of the graph G; therefore, performing a standard clustering (like K-means or K-means++) with the points projected into these subspaces allows for an easy separation with symmetric shapes.</span></p>
<p>As <em>L<sub>n</sub> <span>∈ ℜ</span><sup>M × M</sup></em>, its eigenvectors <em>v<sub>i </sub><span>∈</span> <span>ℜ</span><sup>M</sup></em>. Selecting the first <em>k</em> eigenvectors, it's possible to build a matrix <em>A <span>∈ ℜ</span><sup>M × k</sup></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dd015f48-904f-49bc-90c6-5ccf2d551d67.png" style="width:21.83em;height:11.17em;"/></div>
<p>Each row of <em>A</em>, <em>a<sub>j</sub> <span>∈ ℜ</span><sup>k</sup></em> can be considered as the projection of an original sample <em>x<sub>j</sub></em> in the low-dimensional subspace spanned by each of the null eigenvalues of <em>L<sub>n</sub></em>. At this point, the separability of the new dataset <em>A = {<span>a</span><sub>j</sub>}</em> depends only on the structure of the graph <em>G</em> and, in particular, on the number of neighbors or the <span><em>γ</em> parameter for RBFs. As in many other similar cases, it's impossible to define a standard value suitable for all problems, above all when the dimensionality doesn't allow a visual inspection. A reasonable approach should start with a small number of neighbors (for example, five) or <em>γ = 1.0</em> and increase the values until a performance metric (such as the Adjusted Rand Index) reaches its maximum. Considering the nature of the problems, it can also be useful to measure the homogeneity and the completeness because these two measures are more sensitive to irregular geometric structures and can easily show when the clustering is not separating the sets correctly. If the ground truth is unknown, the Silhouette score can be employed to assess the intra-cluster cohesion and the inter-cluster separation as functions of all hyperparameters (number of clusters, number of neighbors, or <em>γ</em>).</span></p>
<p>The complete<span> </span><strong>Shi-Malik spectral clustering</strong><span> </span>algorithm is:</p>
<ol>
<li>Select a graph construction a method between KNN (1) and RBF (2):
<ol>
<li>Select parameter <em>k</em></li>
<li>Select parameter <span><em>γ</em></span></li>
</ol>
</li>
<li>Select the expected number of clusters <em>N</em><sub><em>k</em></sub>.</li>
<li>Compute the matrices <em>W</em> and <em>D</em>.</li>
<li>Compute the normalized graph Laplacian <em>L<sub>n</sub></em>.</li>
<li>Compute the first k eigenvectors of <em>L<sub>n</sub></em>.</li>
<li>Build the matrix <em>A</em>.</li>
<li>Cluster the rows of <em>A</em> using K-means++ (or any other symmetric algorithm). The output of this process is this set of clusters: <em>C<sub>km</sub><sup>(1)</sup></em>, <em><span>C</span><sub>km</sub><sup>(2)</sup></em><span>, ..., <em>C<sub>km</sub><sup>(Nk)</sup></em></span><span> .</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of spectral clustering with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>In this example, we are going to use the sinusoidal dataset previously shown. The first step is creating it (with 1,000 samples): </p>
<pre>import numpy as np<br/><br/>from sklearn.preprocessing import StandardScaler<br/><br/>nb_samples = 1000<br/><br/>X = np.zeros(shape=(nb_samples, 2))<br/><br/>for i in range(nb_samples):<br/>    X[i, 0] = float(i)<br/>    <br/>    if i % 2 == 0:<br/>        X[i, 1] = 1.0 + (np.random.uniform(0.65, 1.0) * np.sin(float(i) / 100.0))<br/>    else:<br/>        X[i, 1] = 0.1 + (np.random.uniform(0.5, 0.85) * np.sin(float(i) / 100.0))<br/>        <br/>ss = StandardScaler()<br/>Xs = ss.fit_transform(X)</pre>
<p>At this point, we can try to cluster it using K-means (with <kbd>n_clusters=2</kbd>):</p>
<pre>from sklearn.cluster import KMeans<br/><br/>km = KMeans(n_clusters=2, random_state=1000)<br/>Y_km = km.fit_predict(Xs)</pre>
<p>The result is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d66eb730-ffca-42d9-a3a7-c09c5906b581.png" style="width:45.92em;height:23.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">K-means clustering result using the sinusoidal dataset</div>
<p>As expected, K-means isn't able to separate the two sinusoids. The reader is free to try with different parameters, but the result will always be unacceptable because K-means bidimensional clusters are circles and no valid configurations exist. We can now employ spectral clustering using an affinity matrix based on the KNN algorithm (in this case, Scikit-Learn can produce a warning because the graph is not fully connected, but this normally doesn't affect the results). Scikit-Learn implements the <kbd>SpectralClustering</kbd> class, whose most important parameters are <kbd>n_clusters</kbd>, the number of expected clusters; <kbd>affinity</kbd>, which can be either <kbd>'rbf'</kbd> or <kbd>'nearest_neighbors'</kbd>; <kbd>gamma</kbd> (only for RBF); and <kbd>n_neighbors</kbd> (only for KNN). For our test, we have chosen to have <kbd>20</kbd> neighbors:</p>
<pre>from sklearn.cluster import SpectralClustering<br/><br/>sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', n_neighbors=20, random_state=1000)<br/>Y_sc = sc.fit_predict(Xs)</pre>
<p>The result of the spectral clustering is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0c68127e-0864-46fc-ad88-8bce7d0efe54.png" style="width:47.58em;height:24.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Spectral clustering result using the sinusoidal dataset</span></div>
<p>As expected, the algorithm was able to separate the two sinusoids perfectly. As an exercise, I invite the reader to apply this method to the MNIST dataset, using both an RBF (with different gamma values) and KNN (with different numbers of neighbors). I also suggest to replot the t-SNE diagram and compare all the assignment errors. As the clusters are strictly non-convex, we don't expect a high Silhouette score. Other useful exercises can be: drawing the Silhouette plot and checking the result, assigning ground truth labels, and measuring the homogeneity and the completeness.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented some fundamental clustering algorithms. We started with KNN, which is an instance-based method that restructures the dataset to find the most similar samples given a query point. We discussed three approaches: a naive one, which is also the most expensive in terms of computational complexity, and two strategies based respectively on the construction of a KD Tree and a Ball Tree. These two data structures can dramatically improve performance even when the number of samples is very large.</p>
<p>The next topic was a classic algorithm: K-means, which is a symmetric partitioning strategy, comparable to a Gaussian mixture with variances close to zero, that can solve many real-life problems. We discussed both a vanilla algorithm, which wasn't able to find a valid sub-optimal solution, and an optimized initialization method, called K-means++, which was able to speed up the convergence towards solutions quite close to the global minimum. In the same section, we also presented some evaluation methods that can be employed to assess the performance of a generic clustering algorithm.</p>
<p>We also presented a soft-clustering method called fuzzy C-means, which resembles the structure of a standard K-means, but allows managing membership degrees (analogous to probabilities) that encode the similarity of a sample with all cluster centroids. This kind of approach allows processing the membership vectors in a more complex pipeline, where the output of a clustering process, for example, is fed into a classifier.</p>
<p>One of the most important limitations of K-means and similar algorithms is the symmetric structure of the clusters. This problem can be solved with methods such as spectral clustering, which is a very powerful approach based on the dataset graph and is quite similar to non-linear dimensionality reduction methods. We analyzed an algorithm proposed by Shi and Malik, showing how it can easily separate a non-convex dataset.</p>
<p>In the next chapter, <a href="78baef9c-5391-4898-91bf-8df25330a163.xhtml" target="_blank">Chapter 8</a>, <em>Ensemble Learning</em>, we're going to discuss some common ensemble learning methods, which are based on the use of a large set of weak classifiers. We focused on their peculiarities, comparing the performances of different ensembles with single strong classifiers.</p>


            </article>

            
        </section>
    </body></html>