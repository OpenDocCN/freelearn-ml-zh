- en: Time Series Forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to handle date and time with Julia.
    This allowed us to understand the very important concept of time series data.
    Now, we are ready to discuss yet another highly important data science topic—time
    series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis and forecasting represents a key strategic and decisive
    component of any organization, from understanding top sales periods to end of
    season intervals and discounts, scheduling employees' time off, budgets, fiscal
    years, product release cycles, increased demand in raw materials, and many, many
    other aspects. Understanding and predicting the evolution of various business
    indicators over time is a necessary part of doing business, whether we're talking
    about a school, a billion dollar corporation, a hotel, a supermarket, or a government.
  prefs: []
  type: TYPE_NORMAL
- en: However, time series data analysis is one of the most fairly complex tasks of
    data science. The nature and particularities of chronological events led to the
    development of specialized algorithms and methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll study the basics of time series analysis and forecasting
    using Julia. Although a fairly young language, Julia already has good support
    for handling time-related data. In the previous chapter, we''ve learned about
    the Dates module and about the `TimeSeries` package. In this chapter, we''ll dive
    deeper and apply what we have previously studied. We''ll also learn about more
    advanced `TimeSeries` methods and about a few other packages for working with
    temporal data. We will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis of the unemployment figures of the **European Union**
    (**EU**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trends, cycles, seasonality, and errors—components of a time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stationarity, differencing, and autocorrelation of time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to apply simple forecasting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Julia package ecosystem is under continuous development and new package
    versions are released on a daily basis. Most of the times this is great news,
    as new releases bring new features and bug fixes. However, since many of the packages
    are still in beta (version 0.x), any new release can introduce breaking changes.
    As a result, the code presented in the book can stop working. In order to ensure
    that your code will produce the same results as described in the book, it is recommended
    to use the same package versions. Here are the external packages used in this
    chapter and their specific versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to install a specific version of a package you need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively you can install all the used packages by downloading the `Project.toml`
    file provided with the chapter and using `pkg>` instantiate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A quick look at our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use some real-life data provided by Eurostat, the official
    EU office for statistics. Eurostat has a wealth of databases available on its
    website. For our learning project, we'll take a look at the unemployment numbers—with
    the EU's economy growing after a long recession, these stats should be quite interesting.
    Various EU employment and unemployment figures can be downloaded from [http://ec.europa.eu/eurostat/web/lfs/data/database](http://ec.europa.eu/eurostat/web/lfs/data/database).
    We'll be using the **Unemployment by sex and age – monthly average** dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You don't need to download this because a better structured dataset is provided
    in this chapter's support files. However, if you're curious and want to take a
    look, you can get the raw data from under the Employment and unemployment (Labour
    force survey) category | LFS main indicators subcategory | Unemployment - LFS
    adjusted series folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve also customized the data by using *thousand persons* for the unit of
    measure (the default is *percentage of active population*), and unadjusted data
    (neither seasonally, nor calendar). I''ve also kept the numbers for the EU only
    (no individual countries). Finally, I''ve included all the data from January 2005
    to December 2017\. You can make all of these adjustments in the data explorer
    and then download the table as TSV file. As for the TSV formatting, I went with
    these options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a03e7928-781a-4f72-b4cb-e037fb87381a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualized in the Eurostat data explorer tool, the data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2f003d7-0f72-40fa-a01a-5de76cb059d0.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see a list of geographical areas in the first column and unemployment
    numbers on a monthly basis in the rest of the columns. This dataset is structured
    in a different way than what we require. For starters, `TimeSeries` requires the
    matrix to be transposed (as in, the dates should become rows instead of columns).
    Additionally, dates are formatted in a non-standard way, for example, `2017M01` designates
    January 2017\. Finally, the numbers are formatted as strings, with spaces for
    thousand separators. You can download this raw data file from this chapter's support
    files, which are hosted at [https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter10/data/une_rt_m_1.tsv](https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter10/data/une_rt_m_1.tsv).
  prefs: []
  type: TYPE_NORMAL
- en: Such issues are a common occurrence when working with real-life data—differences
    in standards regarding structure and formatting make data sanitization and transformation
    a key first step, and usually also a time-consuming one, in any data science project.
    For the sake of brevity, I have prepared a simplified dataset that's already been
    transformed for seamless usage with `TimeSeries`, which you can download from
    this chapter's support files.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you would like to follow along, here is how I processed the raw data using
    Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what it looks like in a Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf8de08e-56ce-4d2a-92fd-9705b747b76e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we will extract the values by selecting a `DataFrame` composed
    of 1 row and 2 columns to `end` and converting it into an `Array`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can parse the previously extracted string values and convert them into
    integers. The new integer values are stored in a vector as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Great—our values are ready! We can now focus on the headers. Our goal is to
    extract the date information contained in the labels. As a first step, we pull
    the names of the columns into a vector, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s transform the symbols to bring them closer to what we need—that
    is, something resembling a standard date format. We''ll replace the "`M`" with
    a dash, and in the process, we''ll convert the symbol into a `String`, as replacing
    does not work on symbols:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! Now, we can define a `DateFormat` matching our strings—year plus
    dash plus month, with the month as a numeric value with a leading zero. We''ll
    use this to convert the strings to proper date objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re getting closer! To safely persist the data to a file, I created a new
    `DataFrame`, this time using the proper dates and the original values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `CSV.write` to store the snapshot of our data to file by using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now load the data as a `TimeArray` from the TSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to directly convert from `DataFrame` to `TimeSeries` data,
    without resorting to loading a TSV file, you can use the `IterableTables` package.
    `IterableTables` provides a wealth of converter methods between different table
    types in Julia. You can read more in the package's README at [https://github.com/davidanthoff/IterableTables.jl](https://github.com/davidanthoff/IterableTables.jl).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our time series data was correctly loaded—there are 156 entries between January
    2005 and December 2017\. It will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25a87a47-e73d-41bb-8a57-ce3362ff049c.png)'
  prefs: []
  type: TYPE_IMG
- en: We had to use the fully qualified name of the head function, `TimeSeries.head`,
    because both `TimeSeries` and `DataFrames` export a `head` method, and both packages
    are loaded into the current scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attempting to call the head function without the module''s name would result
    in an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The best way to get a quick insight into our data is to render a plot. We''ll
    use the `Plots` package with the `PyPlot` backend—we installed them both in [Chapter
    9](11df7c94-2e9a-4cc5-aba1-b9c9c93800a0.xhtml), *Working with Dates, Time, and
    Time Series*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `PyPlot` backend has complex dependencies, so if you run into problems executing
    the indicated code, please follow the instructions provided by the warnings and
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, at some point, I had to install two extra packages by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can plot the unemployment numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Julia will render the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cde553e-2ac8-4d28-be71-883a2d5e664b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can easily see that the number of unemployed people had been steadily decreasing
    since 2005, reaching a historical minimum in the second half of 2008\. From there,
    over a couple of months, it skyrocketed to levels unknown since 2005\. This was
    the moment when the recession hit the EU's economy. From that point on, unemployment
    numbers continued to grow, until they finally reached a peak at the beginning
    of 2013\. The maximum number of people without a job was reached in February 2013,
    after which the European economy began to recover, with the unemployment numbers
    rapidly declining and approaching pre-recession levels.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding time series components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are three components of time series that are key to understanding time-related
    data. They are *trend*, *seasonality*, and *noise*. Let's look at each of them
    in the context of our EU unemployment data.
  prefs: []
  type: TYPE_NORMAL
- en: Trend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The trend can be defined as the long-term tendency of the time series data—the
    fact that, on average, the values tend to increase or decrease over a period of
    time. Looking at our plot, we can identify three distinct trends:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51311f42-6866-4abf-9d4a-c408ed5670cf.png)'
  prefs: []
  type: TYPE_IMG
- en: A downward trend from 2005 until 2008 (less people unemployed on a year-on-year
    basis); an upward trend starting in 2008 and manifesting until 2013 (unemployment
    rose on average); and again, a downward trend between 2013, all the way until
    the end of 2017 (the number of people without work constantly decreased).
  prefs: []
  type: TYPE_NORMAL
- en: Seasonality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Seasonality is a regularly repeating pattern of highs and lows that is related
    to calendar time; that is, it's directly influenced by seasons, quarters, months,
    and so on. Think, for instance, about the electricity usage in a city—we'll probably
    see increases in consumption during the summer due to air conditioning, and in
    wintertime due to needing to heat the houses instead. In a similar manner, by
    looking at a hotel at the seaside, we'll see a significant increase in bookings
    during the summer, followed by a decrease in the winter.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, seasonality generates effects that are reasonably stable with respect
    to timing, direction, and magnitude. The most common calendar-related influences
    are natural conditions (the weather), business and administrative procedures (fiscal
    year), and social and cultural behaviors (bank holidays due to national and religious
    holidays, key dates like Christmas, Valentine's Day, and so on). It also includes
    effects that are caused by calendar events, which are recurrent but not fixed
    in terms of date (such as Easter, whose date falls on a certain Sunday each year,
    but the actual date varies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unemployment data suffers a strong seasonal influence—during the summer months,
    more people are employed. These seem to be temporary jobs, probably in tourism,
    to help hotels and restaurants cope with the influx of holiday goers—but maybe
    also in the office and retail sectors to cover for the regular employees'' time
    off. We can clearly identify this on our plot—the summer months bring the lowest
    unemployment figures for the year, with the values beginning to go up again in
    autumn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b023cc8b-d96f-42e9-a2b5-9a6ca9e4aa77.png)'
  prefs: []
  type: TYPE_IMG
- en: During the three summer months in the middle of the year, unemployment reaches
    the lowest levels. Once the peak of the season passes, unemployment steeply rises
    once more.
  prefs: []
  type: TYPE_NORMAL
- en: Random noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The default assumption when analyzing time series data is that we can identify
    an underlying pattern (as defined by its trend and seasonality components). However,
    when there is such a systematic pattern in the data (some time series data is
    completely random, for example, earthquake incidence), it will also be accompanied
    by variances—fluctuations in the data that are categorized as random noise, errors,
    or irregularities. They make the task of identifying the patterns more difficult,
    and for this reason, data scientists will use some form of noise filtering.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, this irregular component is what remains after the seasonal
    and trend components have been computed and removed. They are short-term fluctuations,
    and are neither systematic nor predictable.
  prefs: []
  type: TYPE_NORMAL
- en: Cyclicity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cyclicity** is similar to seasonality in a way, and for this reason, the
    two are often confused. However, they are two different things, and the distinction
    is important. Cyclical periods represent larger swathes of time where we can identify
    recurring patterns in the data (periods of growth or decline) and which can''t
    be explained away by calendar patterns. They are usually larger, spanning a few
    years, and do not overlap with calendar events. Such cyclical elements can be
    introduced by product release cycles (the release of a car model, or a new version
    of an operating system, or an upgrade to a line of laptops), election cycles (for
    government budgeting or companies working with government contracts), and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Time series decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can thus say that any value in a time series can be represented through a
    function of the four components we discussed earlier—trend, seasonality, error,
    and cycle. The relationship between the four components can be either *additive*
    or *multiplicative*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The additive model is used when the seasonal variation stays about the same
    across time. The trend may be upward or downward, but the seasonality stays more
    or less the same. A plot of such data will look very similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea8d4fed-7b41-4c10-a529-d9b90858a7a0.png)'
  prefs: []
  type: TYPE_IMG
- en: If we draw two imaginary lines between the yearly maximums and the yearly minimums,
    the lines will be pretty much parallel.
  prefs: []
  type: TYPE_NORMAL
- en: For an additive time series model, the four components are summed up to produce
    the values in the series. Thus, a time series *Y* can be decomposed into *Y* =
    *Trend* + *Cycle* + *Seasonality* + *Noise*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A multiplicative model should be used with a time series where the seasonal
    variability increases over time. For example, a typical multiplicative time series
    is represented by the international airline passenger data between January 1949
    and January 1960:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82d97bc1-6063-4aa7-8287-519ef0a86000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see how the variation in the seasonal pattern is correlated with the
    level of the time series: the more passengers we have, the higher the variation.
    A multiplicative time series *Y* can be represented as *Y* = *Trend* * *Cycle*
    * *Seasonality* * *Noise*.'
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, we can convert a multiplicative model into an additive model
    by transforming the data until it becomes stable over time, for example, by means
    of log transformations—*Y* = *Trend* * *Cycle* * *Seasonality* * *Noise* is equivalent
    to *log Y* = *log Trend* + *log Cycle* + *log Seasonality* + *log Noise*.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting a time series into its components is a widely employed technique for
    time data analysis. This is known as** time series decomposition**, and it also
    represents the foundation of time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining data – an additive approach or multiplicative approach?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the question—which of the two approaches does a better job of explaining
    our data? One way to answer this question is to look at the cycle-by-cycle values
    and see if there is significant variation. As we''re dealing with yearly cycles,
    let''s extract and plot the year on year values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we render an empty plot. Then, we iterate over a range corresponding
    to our years, between 2005 and 2017, and then we use the `TimeSeries.when` method
    to filter our data by year. We extract the resulting `TimeArray` values and append
    them to the plot by using the `plot!` function. However, this is not enough—we
    also have to call the `Plots.gui` method to actually render the updated plot.
    This is a very important point, per the official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A plot is only displayed when returned (a semicolon will suppress the return),
    or if explicitly displayed with `display(plt)`, `gui()`, or by adding `show =
    true` to your plot command."'
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about outputting plots at [http://docs.juliaplots.org/latest/output/](http://docs.juliaplots.org/latest/output/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faa8bea1-d57f-452c-9a64-107b4d1dd029.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there is consistent year-after-year variation, which means that
    we should use the multiplicative model.
  prefs: []
  type: TYPE_NORMAL
- en: Eyeballing the components using `plots` is a common way of recognizing patterns
    in a time series. In our case, it is pretty easy to tell that there's both trend
    and seasonality. Also, we can deduce that the data does not exhibit any cyclical
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the multiplicative model holds that *Y = Trend * Cyclicity * Seasonality
    * Noise*. We can write this shorter as *Y = TCSN*. Since we just established that
    our data does not present any cycles, we're going to leave out the cyclicity component,
    and so *Y = TSN*.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the trend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in decomposing a time series is to extract the trend component.
    A widely used technique for computing the trend is called **smoothing**. As the
    name suggests, it *smooths out* the values by removing the noise and blurring
    the seasonality so that we can identify the trend.
  prefs: []
  type: TYPE_NORMAL
- en: One way of performing smoothing is through moving averages. In financial applications,
    the simple moving average is the unweighted mean of the previous *n* points of
    data. It's like applying a moving window on top of our time series and performing
    the calculation using the visible data. Then, we slide the window by one position
    and repeat the calculation. To smooth out seasonal data, the window should be
    the size of the seasonal period—in our case, 12 months. So, to apply simple moving
    average smoothing to our data, we'll start by taking the first 12 month period
    (the year 2005), sum up the values, and then divide them by 12 to get their average.
    Then, we'll slide our window by one month and repeat the computation. As a result,
    we eliminate the effect of the seasonal component and cancel out the impact of
    the noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TimeSeries` package provides a series of *apply methods* that implement
    common transformations of time series data. One of them is the `moving` method,
    which can be used to compute the moving average of a series. Let''s compute the
    moving average for a 12 month interval in order to smooth out the seasonality
    component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db85a03f-60fd-4570-bada-64b584638829.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the result is a new time series that contains the mean of 12-month
    periods in our original time series. The first 12 values of the original series
    are consumed by this operation so that our new series starts with December 2005\.
    If you wish to keep the initial values, the `moving` function takes an additional
    keyword argument, `padding`. By default, `padding` is `false`, but if set to `true`,
    the consumed timestamps will be kept and their values will be set to `NaN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a396e00-49a3-4849-8fe5-571002e8c806.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plotting the smoothed values on top of the original data indicates the trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d7e2491-e3b8-46f8-ab08-24a3632989f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The first call to the `plot` method renders the raw EU unemployment figures,
    while the subsequent call of the `plot!` method mutates the plot, overlaying the
    moving average that corresponds to the trend.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the seasonality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have extracted the trend, we can remove it from the initial time
    series. This is done by division. We will be left with the product of the seasonal
    and noise components. Thus, *SN = Y/T*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the fraction between `TimeArray` objects, we''ll use the element-wise
    division operator, `./`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f41594b9-1cfb-465f-a546-b049443ec16f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plotting the resulting `TimeArray` will give us a clearer image of the product
    of the seasonality and noise components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/062eaaec-3351-479f-aa72-f2911041b9b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is calculating the sum over years of these values for the same
    month. That is, we''ll sum the value for all of the months of January throughout
    all the years; then, we will do the same for February, March, and so on. We''ll
    get the average over all the years, for each calendar month. This will lead to
    the minimization of the noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we instantiate a `Vector` of `Float64` values. Then, we iterate over
    a range between `1` and `12`, which represents the months. Within the loop, we
    apply the `when` method to filter the values for the currently iterated month
    (all the January values for all the years, then all the February values for all
    the years, then March, and so on), and then we push the mean of these values into
    the `month_avg` array. At the end of the loop, we collect these values in `month_avg`,
    where the first is the average value for the month of January across all the years,
    the second for February, then March, and so on. It will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Theoretically, these values should add up to `12`. In practice, that doesn''t
    happen (although we''re pretty close). We can easily sum up all of the elements
    of an array using the `sum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As a consequence, we need to normalize the averages so that they *do* sum up
    to `12`. This is achieved by multiplying each seasonal factor by `12` and then
    dividing each factor by their sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `map` function to iterate over each item in `month_avg` as `m`
    and applied an anonymous function so that `m = 12m/s`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the sum again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Perfection!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have calculated the monthly seasonal factor, we can perform the
    seasonal adjustment on our original time series by dividing it by the seasonal
    factor. This way, we'll get the reminder, which represents the product of trend
    and noise—*Y/S = TN*. To compute this in Julia, we have to divide each value of
    `unemployment_data` by the corresponding monthly seasonal factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep things clean and tidy, let''s copy our original time series into a
    different object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `deepcopy` function creates a deep copy of the object, given as an argument.
    A deep copy means that everything is copied recursively, resulting in a fully
    independent object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use the `map` function to modify the `TimeArray` in place by recursively
    applying a function that divides the original value by the seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a22f7a94-52c1-4941-9ac4-ffb36a8af057.png)'
  prefs: []
  type: TYPE_IMG
- en: The `adj_unemployment_data` variable represents the seasonally adjusted time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: TimeSeries operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing operations between `TimeArray` objects—or rather between the elements
    contained in them—is a common occurrence in time series analysis. The `TimeSeries`
    package exposes a complete set of element-wise operators for mathematical, comparison,
    and logical operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have already seen when doing division between two `TimeArray` objects,
    the mathematical operators create a new `TimeArray` instance by using the values
    with common timestamps. Operations between a single `TimeArray` and `Int` or `Float`
    are also supported. The following operators are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.+`: Arithmetic element-wise addition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.-`: Arithmetic element-wise subtraction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.*`: Arithmetic element-wise multiplication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`./`: Arithmetic element-wise division'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.^`: Arithmetic element-wise exponentiation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.%`: Arithmetic element-wise remainder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to mathematical operators, in the case of comparison ones, when two
    `TimeArray` instances are provided, the values are compared on shared timestamps
    too. However, the difference, in this case, is that the result will be a time
    array of type `Bool`.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the available comparison operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.>`: Element-wise greater-than comparison'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.<`: Element-wise less-than comparison'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.==`: Element-wise equivalent comparison'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.>=`: Element-wise greater-than or equal comparison'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.<=`: Element-wise less-than or equal comparison'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.!=`: Element-wise not-equivalent comparison'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at an example. First, let''s create a `TimeArray` spreading between
    a week ago and today and fill it with random values. Your timestamps will be different
    as you''ll run the code sometime in the future, and so the output will be different
    compared to mine, but the logic will be the same. Don''t forget to execute `using
    Dates` if the module is not already in scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/780158c5-3bc1-49b3-84fd-b1c21f1ec1a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we''ll do the same for the second array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b641102-005b-4c98-b97d-b34e2cd7160b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can compare the two objects, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12cafc14-56f4-43e9-834f-867a24c27ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparisons between a single `TimeArray` and `Int`, `Float`, or `Bool` values
    are supported too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08bf2425-0ad4-450b-b506-39c5a376347b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can use the following logic operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.&` element-wise logical `AND`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.|` element-wise logical `OR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.!`, `.~` element-wise logical `NOT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.``⊻` element-wise logical `XOR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are defined for `TimeArrays` of type `Bool` and return a `TimeArray` of
    type `Bool`. Values are computed on common timestamps when two `TimeArray` objects
    are the operands and operations between a single `TimeArray` and a `Bool` are
    supported.
  prefs: []
  type: TYPE_NORMAL
- en: Time series stationarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A time series is considered stationary if its statistical properties such as
    mean, variance, autocorrelation, and so on, are constant over time. **Stationarity**
    is important because most forecasting models run on the assumption that the time
    series is stationary or can be rendered (approximately) stationary using transformations.
    The reason for this approach is that values in a stationary time series are much
    easier to predict—if its properties are constant, we can simply state that they
    will be in the future as they were in the past. Once we forecast future values
    based on stationary time series, we can then reverse the process and the transformations
    to compute the values that would match the original series.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the properties of a stationary time series do not depend on the time when
    the series is observed. Implicitly, this means that time series that present seasonality
    or trends are not stationary. In this context, again, we must be careful of the
    difference between seasonality and cyclicity—cyclic time series that do not expose
    seasonal or trending patterns *are* stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Differencing a time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to make a time series stationary is by *differencing*. This means computing
    the difference between consecutive values. In this technique, we calculate the
    difference between a value at a certain point in time and the one at the previous
    instant.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be easily computed by using the `diff` method that''s provided by
    `TimeSeries`. Differentiating a time series calculates the finite difference between
    two consecutive points in the time series. By default, the difference is by one
    day. Consider the following, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'One day from the original series is lost as part of the operation, with the
    resulting `TimeArray` beginning on January 2, 2005, resulting in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2a54b2-d31c-4df3-85b4-e0a2b09d7af7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can render the result as a bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1735f08-90c2-416a-a1a2-95557449f6d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Changes in the values are clearly visible throughout the entire dataset, meaning
    that the variance is relatively constant.
  prefs: []
  type: TYPE_NORMAL
- en: Autocorrelation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autocorrelation represents the degree of similarity of a time series and a lagged
    version of itself over successive time intervals. It is a very important concept
    as it measures the relationship between a current value and a corresponding past
    value. Thus, it has many valuable applications in time series forecasting; for
    example, to match trends and relationships in prices, stocks, returns, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: We want to use autocorrelation to determine if we can reliably identify causality
    and trend – or if, on the contrary, we're dealing with a random walk model. A
    random walk would imply that the values in the time series are randomly defined,
    and this would imply that there's no relationship between past and present values.
    The random walk model is common, especially for financial and economic data. For
    a random walk model, forecasting the next value is done by taking the last value
    in the series. This is due to the fact that future movements are unpredictable—they
    are equally likely to be increasing or decreasing. Thus, the random walk model
    underpins naïve forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute autocorrelation by using a combination of two functions—`TimeSeries.lag`
    and `xcorr`. The `lag` method works by shifting the values of the time series.
    For instance, let''s use our previously defined `ts1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8490e06-5b4f-4177-972a-5dcdc07db0c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can apply the `lag` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This will cause the first value to be assigned to the next timestamp. In my
    case, the value `0.3903`, which was initially corresponding to `2018-11-06`, now
    corresponds to `2018-11-07`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5abd220a-976f-49e5-a274-89dcabea6cf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that if you run the code in parallel, your data will be different (the
    actual dates and values are different since we're using random values), but the
    behavior will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can experiment with lagging the unemployment data by `12` intervals (12
    months) to account for the yearly seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b7d22d0-a167-4ee3-ac10-a6bff223918a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The values have been shifted and the resulting `TimeArray` starts on the January
    1, 2006\. We can now use `TimeSeries.merge` to join the two series on the common
    timestamps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0daad74d-3e4d-4a16-a170-baafee8259e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we plot the original unemployment data together with the one year lagged
    series, we can see that the data is positively correlated, indicating strong yearly
    seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7a5dcd7-fc92-41bc-b099-dd7965beacc5.png)'
  prefs: []
  type: TYPE_IMG
- en: Time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forecasting implies identifying models that fit the historical data and using
    them to predict future values. When forecasting time series data, decomposition
    plays a very important part, helping to make predictions more accurate. The underlying
    assumption is that we can be more precise if we forecast each component individually,
    using the best-suited method, and then sum or multiply the parts (depending on
    whether the model is additive or multiplicative) to compute the final value.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the simplest method, stating that the forecasted value is equal to
    the last value in the series. As mentioned previously, this is used with random
    walk models, where future movements are unpredictable. For example, to predict
    the value for the first unknown month, January 2018, using the naïve model, we
    can take the seasonally adjusted value from December 2017 and add (multiply) the
    seasonal component of the month of January:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `TimeSeries.update` method to append a new item for January 2018\.
    Its value is the seasonality adjusted value of December 2017, multiplied by the
    normalized seasonality of the month of January:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d18a8e40-afba-409f-9f2c-8bdecb9a5dc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that we also assume that the seasonal component is unchanged, which means
    that we're using the seasonal naïve method for the seasonal component.
  prefs: []
  type: TYPE_NORMAL
- en: Simple average
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A method that is slightly more advanced involves computing the mean of the
    previous data points to forecast the next value. It''s a basic approach but in
    some situations, it can be a good fit. To compute it, we can apply the `mean`
    function to the underlying array of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Moving average
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered the moving average in detail when we extracted the trend component
    of our time series. It can also be employed for forecasting, using the result
    of the computation to fill up the next value. It is important to pick the right
    window size by understanding the series' seasonality, for example, by using autocorrelation
    plots.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use linear regression on the seasonally adjusted time series to forecast
    the next value. Let''s take a closer look at this since it presents some good
    opportunities to dive into interesting Julia code. Since our data presents three
    trends (down, up, and down again), let''s focus only on the last segment, where
    the current downward trend can be observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d5c2dbe-ff7c-4bb9-be65-b4b45d903353.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the current trend had started with an unemployment peak, so
    all we have to do is look for the maximum value in the series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ee71ee6-5f1e-4d1a-96a3-3a637f89e3e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The downward trend started in February 2013\. Let''s extract all the data from
    that moment onward, all the way until the end of the series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7aeb19dc-f5f8-4506-bdb1-74066968866b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now compute the linear regression—it will summarize the relationship
    between the unemployment numbers and the passing of time, allowing us to forecast
    the next value in the series. We have our unemployment numbers on the *y*-axis
    of the plot and the time on the *x*-axis. In this case, we can express `y` with
    the formula `y = a+b*x`, where `a` and `b` correspond to the linear regression.
    We'll compute the linear regression for the trend series to get `a` and `b`, and
    we'll calculate the next value of `y` (the unemployment forecast), corresponding
    to the next value of `x` (January 2018). Let's go through this, step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is convert the timestamps in the time series
    into a simple integer series that we can use in our equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'On the *x*-axis, we use integer values from `1` to `59`, instead of the actual
    dates. In this line of thought, the next value, the one we want to forecast, will
    be `x = 60`, which means that our next `y` (the forecasted unemployment value)
    will be *27,608.61 + (-167.13 * 60)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Like we did previously, we need to add the seasonality for the month of January:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can append it to our unemployment data and plot it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f48963a1-5285-4091-91ac-a9e9ba0cbf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Our forecasted value has shown up on the plot.
  prefs: []
  type: TYPE_NORMAL
- en: Closing thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be mentioned that the preceding sections represent only a few of the
    simplest forecasting methods available. We focused on gaining a good understanding
    of time series decomposition, which is a key tool for both analysis and forecasting.
    However, more powerful and more complex forecasting algorithms are available,
    for example, **autoregressive integrated moving average** (**ARIMA**), **artificial
    neural networks** (**ANN**), and Holt-Winters. These are recommended for business-critical
    predictions. We have now set the foundation for understanding them, but their
    implementations are more involved and would go beyond the technical expertise
    assumed by this chapter—especially as Julia's package ecosystem, at the time of
    writing, does not provide any libraries that implement these algorithms, and we'd
    have to write them from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, one commonly-used time series forecasting technique is the Holt-Winters
    method, also called **Triple exponential smoothing**. It is based on weighted
    moving averages and exponential smoothing, both of which have been covered already.
    You can read more about these at [https://www.otexts.org/fpp/7/2](https://www.otexts.org/fpp/7/2)
    and [https://www.otexts.org/fpp/7/5](https://www.otexts.org/fpp/7/5).
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA models are yet another very popular forecasting algorithm. They don't
    use the trend and seasonality components, instead focusing on autocorrelations
    in the data. If you're curious, a good starting point to learn about ARIMA models
    is [https://www.otexts.org/fpp/8](https://www.otexts.org/fpp/8).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series are a very common type of data—they can be used to represent key
    business metrics such as financial prices, resource usage (energy, water, raw
    materials, and so on), weather patterns, or macroeconomic trends—and the list
    could go on and on. The particularity of time series is that the data has to be
    collected at regular intervals, and the key aspect of time series analysis is
    exploring ways that allow us to understand past values so that we can predict
    future ones.
  prefs: []
  type: TYPE_NORMAL
- en: One powerful approach is to decompose a time series into a combination of trend,
    cycle, seasonality, and irregular (also called **error** or **noise**). We learned
    how to do this in this chapter while we analysed the EU's unemployment data. We
    started by learning to compute the trend component by means of moving averages.
    Then, we applied multiplicative series decomposition formulas to calculate seasonality
    and error, and we also applied basic forecasting methods to predict future values.
    In the process, we learned about more advanced `TimeSeries` methods and we experimented
    further with `Plots`. That was quite a ride—congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at a few more advanced topics, including package
    development, benchmarking techniques for measuring and improving performance,
    generating documentation, and registering packages. How exciting—see you in the
    next chapter!
  prefs: []
  type: TYPE_NORMAL
