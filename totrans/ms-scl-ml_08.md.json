["```py\n# apt-get update\n...\n# apt-get install r-base r-base-dev\n...\n\n```", "```py\n# apt-cache search \"^r-.*\" | sort\n...\n\n```", "```py\n# apt-cache rdepends r-base-core\n\n```", "```py\n# apt-get install r-base-dev\n\n```", "```py\n> install.packages(\"ggplot2\")\n--- Please select a CRAN mirror for use in this session ---\nalso installing the dependencies 'stringi', 'magrittr', 'colorspace', 'Rcpp', 'stringr', 'RColorBrewer', 'dichromat', 'munsell', 'labeling', 'digest', 'gtable', 'plyr', 'reshape2', 'scales'\n\n```", "```py\n$ cat >> ~/.Rprofile << EOF\nr = getOption(\"repos\") # hard code the Berkeley repo for CRAN\nr[\"CRAN\"] = \"http://cran.cnr.berkeley.edu\"\noptions(repos = r)\nrm(r)\n\nEOF\n\n```", "```py\n$ export R_LIBS_SITE=${R_LIBS_SITE:-/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library}\n$ export R_LIBS_USER=${R_LIBS_USER:-$HOME/R/$(uname -i)-library/$( R --version | grep -o -E [0-9]+\\.[\n0-9]+ | head -1)}\n\n```", "```py\n$ pkgutil --check-signature R-3.2.3.pkg\nPackage \"R-3.2.3.pkg\":\n Status: signed by a certificate trusted by Mac OS X\n Certificate Chain:\n 1\\. Developer ID Installer: Simon Urbanek\n SHA1 fingerprint: B7 EB 39 5E 03 CF 1E 20 D1 A6 2E 9F D3 17 90 26 D8 D6 3B EF\n -----------------------------------------------------------------------------\n 2\\. Developer ID Certification Authority\n SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86\n -----------------------------------------------------------------------------\n 3\\. Apple Root CA\n SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60\n\n```", "```py\n$ git clone https://github.com/apache/spark.git\nCloning into 'spark'...\nremote: Counting objects: 301864, done.\n...\n$ cp –r R/{install-dev.sh,pkg) $SPARK_HOME/R\n...\n$ cd $SPARK_HOME\n$ ./R/install-dev.sh\n* installing *source* package 'SparkR' ...\n** R\n** inst\n** preparing package for lazy loading\nCreating a new generic function for 'colnames' in package 'SparkR'\n...\n$ bin/sparkR\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-redhat-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nLaunching java with spark-submit command /home/alex/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   \"sparkr-shell\" /tmp/RtmpgdTfmU/backend_port22446d0391e8 \n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /___/ .__/\\_,_/_/ /_/\\_\\   version  1.6.1 \n /_/ \n\n Spark context is available as sc, SQL context is available as sqlContext>\n\n```", "```py\nR> library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n...\nR> sc <- sparkR.init(master = Sys.getenv(\"SPARK_MASTER\"), sparkEnvir = list(spark.driver.memory=\"1g\"))\n...\nR> sqlContext <- sparkRSQL.init(sc)\n\n```", "```py\n$ wget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_7.zip\n--2016-01-23 15:40:02--  http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_7.zip\nResolving www.transtats.bts.gov... 204.68.194.70\nConnecting to www.transtats.bts.gov|204.68.194.70|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 26204213 (25M) [application/x-zip-compressed]\nSaving to: \"On_Time_On_Time_Performance_2015_7.zip\"\n\n100%[====================================================================================================================================================================================>] 26,204,213   966K/s   in 27s \n\n2016-01-23 15:40:29 (956 KB/s) - \"On_Time_On_Time_Performance_2015_7.zip\" saved [26204213/26204213]\n\n$ unzip -d flights On_Time_On_Time_Performance_2015_7.zip\nArchive:  On_Time_On_Time_Performance_2015_7.zip\n inflating: flights/On_Time_On_Time_Performance_2015_7.csv \n inflating: flights/readme.html\n\n```", "```py\n$ hadoop fs –put flights .\n\n```", "```py\n$ bin/sparkR --master local[8]\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n[Previously saved workspace restored]\n\nLaunching java with spark-submit command /Users/akozlov/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   \"--master\" \"local[8]\" \"sparkr-shell\" /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T//RtmpD42eTz/backend_port682e58e2c5db \n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /___/ .__/\\_,_/_/ /_/\\_\\   version  1.6.1 \n /_/ \n\n Spark context is available as sc, SQL context is available as sqlContext\n> flights <- read.table(unz(\"On_Time_On_Time_Performance_2015_7.zip\", \"On_Time_On_Time_Performance_2015_7.csv\"), nrows=1000000, header=T, quote=\"\\\"\", sep=\",\")\n> sfoFlights <- flights[flights$Dest == \"SFO\", ]\n> attach(sfoFlights)\n> delays <- aggregate(ArrDelayMinutes ~ DayOfWeek + Origin + UniqueCarrier, FUN=mean, na.rm=TRUE)\n> tail(delays[order(delays$ArrDelayMinutes), ])\n DayOfWeek Origin UniqueCarrier ArrDelayMinutes\n220         4    ABQ            OO           67.60\n489         4    TUS            OO           71.80\n186         5    IAH            F9           77.60\n696         3    RNO            UA           79.50\n491         6    TUS            OO          168.25\n84          7    SLC            AS          203.25\n\n```", "```py\n> sparkDf <- createDataFrame(sqlContext, flights)\n\n```", "```py\nsparkDf <- createDataFrame(sqlContext, subset(flights, select = c(\"ArrDelayMinutes\", \"DayOfWeek\", \"Origin\", \"Dest\", \"UniqueCarrier\")))\n\n```", "```py\n> rDf <- as.data.frame(sparkDf)\n\n```", "```py\n> $ ./bin/sparkR --packages com.databricks:spark-csv_2.10:1.3.0 --master local[8]\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-redhat-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nWarning: namespace 'SparkR' is not available and has been replaced\nby .GlobalEnv when processing object 'sparkDf'\n[Previously saved workspace restored]\n\nLaunching java with spark-submit command /home/alex/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   \"--master\" \"local[8]\" \"--packages\" \"com.databricks:spark-csv_2.10:1.3.0\" \"sparkr-shell\" /tmp/RtmpfhcUXX/backend_port1b066bea5a03 \nIvy Default Cache set to: /home/alex/.ivy2/cache\nThe jars for the packages stored in: /home/alex/.ivy2/jars\n:: loading settings :: url = jar:file:/home/alex/spark-1.6.1-bin-hadoop2.6/lib/spark-assembly-1.6.1-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n confs: [default]\n found com.databricks#spark-csv_2.10;1.3.0 in central\n found org.apache.commons#commons-csv;1.1 in central\n found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 189ms :: artifacts dl 4ms\n :: modules in use:\n com.databricks#spark-csv_2.10;1.3.0 from central in [default]\n com.univocity#univocity-parsers;1.5.1 from central in [default]\n org.apache.commons#commons-csv;1.1 from central in [default]\n ---------------------------------------------------------------------\n |                  |            modules            ||   artifacts   |\n |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n ---------------------------------------------------------------------\n |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n confs: [default]\n 0 artifacts copied, 3 already retrieved (0kB/7ms)\n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /___/ .__/\\_,_/_/ /_/\\_\\   version  1.6.1 \n /_/ \n\n Spark context is available as sc, SQL context is available as sqlContext\n> sparkDf <- read.df(sqlContext, \"./flights\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"false\")\n> sfoFlights <- select(filter(sparkDf, sparkDf$Dest == \"SFO\"), \"DayOfWeek\", \"Origin\", \"UniqueCarrier\", \"ArrDelayMinutes\")\n> aggs <- agg(group_by(sfoFlights, \"DayOfWeek\", \"Origin\", \"UniqueCarrier\"), count(sparkDf$ArrDelayMinutes), avg(sparkDf$ArrDelayMinutes))\n> head(arrange(aggs, c('avg(ArrDelayMinutes)'), decreasing = TRUE), 10)\n DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes) \n1          7    SLC            AS                      4               203.25\n2          6    TUS            OO                      4               168.25\n3          3    RNO            UA                      8                79.50\n4          5    IAH            F9                      5                77.60\n5          4    TUS            OO                      5                71.80\n6          4    ABQ            OO                      5                67.60\n7          2    ABQ            OO                      4                66.25\n8          1    IAH            F9                      4                61.25\n9          4    DAL            WN                      5                59.20\n10         3    SUN            OO                      5                59.00\n\n```", "```py\n$ for i in $(seq 1 6); do wget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_$i.zip; unzip -d flights On_Time_On_Time_Performance_2015_$i.zip; hadoop fs -put -f flights/On_Time_On_Time_Performance_2015_$i.csv flights; done\n\n$ hadoop fs -ls flights\nFound 7 items\n-rw-r--r--   3 alex eng  211633432 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_1.csv\n-rw-r--r--   3 alex eng  192791767 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_2.csv\n-rw-r--r--   3 alex eng  227016932 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_3.csv\n-rw-r--r--   3 alex eng  218600030 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_4.csv\n-rw-r--r--   3 alex eng  224003544 2016-02-16 03:29 flights/On_Time_On_Time_Performance_2015_5.csv\n-rw-r--r--   3 alex eng  227418780 2016-02-16 03:29 flights/On_Time_On_Time_Performance_2015_6.csv\n-rw-r--r--   3 alex eng  235037955 2016-02-15 21:56 flights/On_Time_On_Time_Performance_2015_7.csv\n\n```", "```py\n> sparkDf <- read.df(sqlContext, \"./flights\", \"com.databricks.spark.csv\", header=\"true\")\n> sfoFlights <- select(filter(sparkDf, sparkDf$Dest == \"SFO\"), \"DayOfWeek\", \"Origin\", \"UniqueCarrier\", \"ArrDelayMinutes\")\n> aggs <- cache(agg(group_by(sfoFlights, \"DayOfWeek\", \"Origin\", \"UniqueCarrier\"), count(sparkDf$ArrDelayMinutes), avg(sparkDf$ArrDelayMinutes)))\n> head(arrange(aggs, c('avg(ArrDelayMinutes)'), decreasing = TRUE), 10)\n DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes) \n1          6    MSP            UA                      1            122.00000\n2          3    RNO            UA                      8             79.50000\n3          1    MSP            UA                     13             68.53846\n4          7    SAT            UA                      1             65.00000\n5          7    STL            UA                      9             64.55556\n6          1    ORD            F9                     13             55.92308\n7          1    MSO            OO                      4             50.00000\n8          2    MSO            OO                      4             48.50000\n9          5    CEC            OO                     28             45.86957\n10         3    STL            UA                     13             43.46154\n\n```", "```py\n> head(arrange(filter(filter(aggs, aggs$Origin == \"SLC\"), aggs$UniqueCarrier == \"AS\"), c('avg(ArrDelayMinutes)'), decreasing = TRUE), 100)\n DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes)\n1         7    SLC            AS                     30            32.600000\n2         2    SLC            AS                     30            10.200000\n3         4    SLC            AS                     31             9.774194\n4         1    SLC            AS                     30             9.433333\n5         3    SLC            AS                     30             5.866667\n6         5    SLC            AS                     31             5.516129\n7         6    SLC            AS                     30             2.133333\n\n```", "```py\nR> attach(iris)\nR> lm(Sepal.Length ~ Sepal.Width)\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width)\n\nCoefficients:\n(Intercept)  Sepal.Width\n 6.5262      -0.2234\n\n```", "```py\nR> model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)\nR> summary(model)\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)\n\nResiduals:\n Min       1Q   Median       3Q      Max \n-0.82816 -0.21989  0.01875  0.19709  0.84570 \n\nCoefficients:\n Estimate Std. Error t value Pr(>|t|) \n(Intercept)   1.85600    0.25078   7.401 9.85e-12 ***\nSepal.Width   0.65084    0.06665   9.765  < 2e-16 ***\nPetal.Length  0.70913    0.05672  12.502  < 2e-16 ***\nPetal.Width  -0.55648    0.12755  -4.363 2.41e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3145 on 146 degrees of freedom\nMultiple R-squared:  0.8586,  Adjusted R-squared:  0.8557 \nF-statistic: 295.5 on 3 and 146 DF,  p-value: < 2.2e-16\n\n```", "```py\nR> aov <- aov(Sepal.Length ~ Species)\nR> summary(aov)\n Df Sum Sq Mean Sq F value Pr(>F) \nSpecies       2  63.21  31.606   119.3 <2e-16 ***\nResiduals   147  38.96   0.265 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n```", "```py\nR> flights <- read.table(unz(\"On_Time_On_Time_Performance_2015_7.zip\", \"On_Time_On_Time_Performance_2015_7.csv\"), nrows=1000000, header=T, quote=\"\\\"\", sep=\",\")\nR> flights$DoW_ <- factor(flights$DayOfWeek,levels=c(1,2,3,4,5,6,7), labels=c(\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"))\nR> attach(flights)\nR> system.time(model <- glm(ArrDel15 ~ UniqueCarrier + DoW_ + Origin + Dest, flights, family=\"binomial\"))\n\n```", "```py\nsparkR> cache(sparkDf <- read.df(sqlContext, \"./flights\", \"com.databricks.spark.csv\", header=\"true\", inferSchema=\"true\"))\nDataFrame[Year:int, Quarter:int, Month:int, DayofMonth:int, DayOfWeek:int, FlightDate:string, UniqueCarrier:string, AirlineID:int, Carrier:string, TailNum:string, FlightNum:int, OriginAirportID:int, OriginAirportSeqID:int, OriginCityMarketID:int, Origin:string, OriginCityName:string, OriginState:string, OriginStateFips:int, OriginStateName:string, OriginWac:int, DestAirportID:int, DestAirportSeqID:int, DestCityMarketID:int, Dest:string, DestCityName:string, DestState:string, DestStateFips:int, DestStateName:string, DestWac:int, CRSDepTime:int, DepTime:int, DepDelay:double, DepDelayMinutes:double, DepDel15:double, DepartureDelayGroups:int, DepTimeBlk:string, TaxiOut:double, WheelsOff:int, WheelsOn:int, TaxiIn:double, CRSArrTime:int, ArrTime:int, ArrDelay:double, ArrDelayMinutes:double, ArrDel15:double, ArrivalDelayGroups:int, ArrTimeBlk:string, Cancelled:double, CancellationCode:string, Diverted:double, CRSElapsedTime:double, ActualElapsedTime:double, AirTime:double, Flights:double, Distance:double, DistanceGroup:int, CarrierDelay:double, WeatherDelay:double, NASDelay:double, SecurityDelay:double, LateAircraftDelay:double, FirstDepTime:int, TotalAddGTime:double, LongestAddGTime:double, DivAirportLandings:int, DivReachedDest:double, DivActualElapsedTime:double, DivArrDelay:double, DivDistance:double, Div1Airport:string, Div1AirportID:int, Div1AirportSeqID:int, Div1WheelsOn:int, Div1TotalGTime:double, Div1LongestGTime:double, Div1WheelsOff:int, Div1TailNum:string, Div2Airport:string, Div2AirportID:int, Div2AirportSeqID:int, Div2WheelsOn:int, Div2TotalGTime:double, Div2LongestGTime:double, Div2WheelsOff:string, Div2TailNum:string, Div3Airport:string, Div3AirportID:string, Div3AirportSeqID:string, Div3WheelsOn:string, Div3TotalGTime:string, Div3LongestGTime:string, Div3WheelsOff:string, Div3TailNum:string, Div4Airport:string, Div4AirportID:string, Div4AirportSeqID:string, Div4WheelsOn:string, Div4TotalGTime:string, Div4LongestGTime:string, Div4WheelsOff:string, Div4TailNum:string, Div5Airport:string, Div5AirportID:string, Div5AirportSeqID:string, Div5WheelsOn:string, Div5TotalGTime:string, Div5LongestGTime:string, Div5WheelsOff:string, Div5TailNum:string, :string]\nsparkR> noNulls <- cache(dropna(selectExpr(filter(sparkDf, sparkDf$Cancelled == 0), \"ArrDel15\", \"UniqueCarrier\", \"format_string('%d', DayOfWeek) as DayOfWeek\", \"Origin\", \"Dest\"), \"any\"))\nsparkR> sparkModel = glm(ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest, noNulls, family=\"binomial\")\n\n```", "```py\n> summary(sparkModel)\n$coefficients\n Estimate\n(Intercept)      -1.518542340\nUniqueCarrier_WN  0.382722232\nUniqueCarrier_DL -0.047997652\nUniqueCarrier_OO  0.367031995\nUniqueCarrier_AA  0.046737727\nUniqueCarrier_EV  0.344539788\nUniqueCarrier_UA  0.299290120\nUniqueCarrier_US  0.069837542\nUniqueCarrier_MQ  0.467597761\nUniqueCarrier_B6  0.326240578\nUniqueCarrier_AS -0.210762769\nUniqueCarrier_NK  0.841185903\nUniqueCarrier_F9  0.788720078\nUniqueCarrier_HA -0.094638586\nDayOfWeek_5       0.232234937\nDayOfWeek_4       0.274016179\nDayOfWeek_3       0.147645473\nDayOfWeek_1       0.347349366\nDayOfWeek_2       0.190157420\nDayOfWeek_7       0.199774806\nOrigin_ATL       -0.180512251\n...\n\n```", "```py\nR> summary(model)\n\nCall:\nglm(formula = ArrDel15 ~ UniqueCarrier + DoW + Origin + Dest, \n family = \"binomial\", data = dow)\n\nDeviance Residuals: \n Min       1Q   Median       3Q      Max \n-1.4205  -0.7274  -0.6132  -0.4510   2.9414 \n\nCoefficients:\n Estimate Std. Error z value Pr(>|z|) \n(Intercept)     -1.817e+00  2.402e-01  -7.563 3.95e-14 ***\nUniqueCarrierAS -3.296e-01  3.413e-02  -9.658  < 2e-16 ***\nUniqueCarrierB6  3.932e-01  2.358e-02  16.676  < 2e-16 ***\nUniqueCarrierDL -6.602e-02  1.850e-02  -3.568 0.000359 ***\nUniqueCarrierEV  3.174e-01  2.155e-02  14.728  < 2e-16 ***\nUniqueCarrierF9  6.754e-01  2.979e-02  22.668  < 2e-16 ***\nUniqueCarrierHA  7.883e-02  7.058e-02   1.117 0.264066 \nUniqueCarrierMQ  2.175e-01  2.393e-02   9.090  < 2e-16 ***\nUniqueCarrierNK  7.928e-01  2.702e-02  29.343  < 2e-16 ***\nUniqueCarrierOO  4.001e-01  2.019e-02  19.817  < 2e-16 ***\nUniqueCarrierUA  3.982e-01  1.827e-02  21.795  < 2e-16 ***\nUniqueCarrierVX  9.723e-02  3.690e-02   2.635 0.008423 ** \nUniqueCarrierWN  6.358e-01  1.700e-02  37.406  < 2e-16 ***\ndowTue           1.365e-01  1.313e-02  10.395  < 2e-16 ***\ndowWed           1.724e-01  1.242e-02  13.877  < 2e-16 ***\ndowThu           4.593e-02  1.256e-02   3.656 0.000256 ***\ndowFri          -2.338e-01  1.311e-02 -17.837  < 2e-16 ***\ndowSat          -2.413e-01  1.458e-02 -16.556  < 2e-16 ***\ndowSun          -3.028e-01  1.408e-02 -21.511  < 2e-16 ***\nOriginABI       -3.355e-01  2.554e-01  -1.314 0.188965 \n...\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ cat examples/src/main/resources/people.json \n{\"name\":\"Michael\"}\n{\"name\":\"Andy\", \"age\":30}\n{\"name\":\"Justin\", \"age\":19}\n\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ bin/sparkR\n...\n\n> people = read.json(sqlContext, \"examples/src/main/resources/people.json\")\n> dtypes(people)\n[[1]]\n[1] \"age\"    \"bigint\"\n\n[[2]]\n[1] \"name\"   \"string\"\n\n> schema(people)\nStructType\n|-name = \"age\", type = \"LongType\", nullable = TRUE\n|-name = \"name\", type = \"StringType\", nullable = TRUE\n> showDF(people)\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n```", "```py\n> write.parquet(sparkDf, \"parquet\")\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ ls –l On_Time_On_Time_Performance_2015_7.zip parquet/ flights/\n-rw-r--r--  1 akozlov  staff  26204213 Sep  9 12:21 /Users/akozlov/spark/On_Time_On_Time_Performance_2015_7.zip\n\nflights/:\ntotal 459088\n-rw-r--r--  1 akozlov  staff  235037955 Sep  9 12:20 On_Time_On_Time_Performance_2015_7.csv\n-rw-r--r--  1 akozlov  staff      12054 Sep  9 12:20 readme.html\n\nparquet/:\ntotal 848\n-rw-r--r--  1 akozlov  staff       0 Jan 24 22:50 _SUCCESS\n-rw-r--r--  1 akozlov  staff   10000 Jan 24 22:50 _common_metadata\n-rw-r--r--  1 akozlov  staff   23498 Jan 24 22:50 _metadata\n-rw-r--r--  1 akozlov  staff  394418 Jan 24 22:50 part-r-00000-9e2d0004-c71f-4bf5-aafe-90822f9d7223.gz.parquet\n\n```", "```py\nR> scala <- scalaInterpreter()\nR> scala %~% 'def pri(i: Stream[Int]): Stream[Int] = i.head #:: pri(i.tail filter  { x => { println(\"Evaluating \" + x + \"%\" + i.head); x % i.head != 0 } } )'\nScalaInterpreterReference... engine: javax.script.ScriptEngine\nR> scala %~% 'val primes = pri(Stream.from(2))'\nScalaInterpreterReference... primes: Stream[Int]\nR> scala %~% 'primes take 5 foreach println'\n2\nEvaluating 3%2\n3\nEvaluating 4%2\nEvaluating 5%2\nEvaluating 5%3\n5\nEvaluating 6%2\nEvaluating 7%2\nEvaluating 7%3\nEvaluating 7%5\n7\nEvaluating 8%2\nEvaluating 9%2\nEvaluating 9%3\nEvaluating 10%2\nEvaluating 11%2\nEvaluating 11%3\nEvaluating 11%5\nEvaluating 11%7\n11\nR> scala %~% 'primes take 5 foreach println'\n2\n3\n5\n7\n11\nR> scala %~% 'primes take 7 foreach println'\n2\n3\n5\n7\n11\nEvaluating 12%2\nEvaluating 13%2\nEvaluating 13%3\nEvaluating 13%5\nEvaluating 13%7\nEvaluating 13%11\n13\nEvaluating 14%2\nEvaluating 15%2\nEvaluating 15%3\nEvaluating 16%2\nEvaluating 17%2\nEvaluating 17%3\nEvaluating 17%5\nEvaluating 17%7\nEvaluating 17%11\nEvaluating 17%13\n17\nR> \n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro ~]$ cat << EOF > rdate.R\n> #!/usr/local/bin/Rscript\n> \n> write(date(), stdout())\n> EOF\n[akozlov@Alexanders-MacBook-Pro ~]$ chmod a+x rdate.R\n[akozlov@Alexanders-MacBook-Pro ~]$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import sys.process._\nimport sys.process._\n\nscala> val date = Process(Seq(\"./rdate.R\")).!!\ndate: String =\n\"Wed Feb 24 02:20:09 2016\n\"\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro ~]$ wget http://www.rforge.net/Rserve/snapshot/Rserve_1.8-5.tar.gz\n\n[akozlov@Alexanders-MacBook-Pro ~]$ R CMD INSTALL Rserve_1.8-5.tar\n.gz\n...\n[akozlov@Alexanders-MacBook-Pro ~]$ R CMD INSTALL Rserve_1.8-5.tar.gz\n\n[akozlov@Alexanders-MacBook-Pro ~]$ $ R -q CMD Rserve\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nRserv started in daemon mode.\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark(master)]$ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n==> This script will install:\n/usr/local/bin/brew\n/usr/local/Library/...\n/usr/local/share/man/man1/brew.1\n…\n[akozlov@Alexanders-MacBook-Pro spark(master)]$ brew install python\n…\n\n```", "```py\n$ export PYTHON_VERSION=2.7.11\n$ wget -O - https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tgz | tar xzvf -\n$ cd $HOME/Python-$PYTHON_VERSION\n$ ./configure--prefix=/usr/local --enable-unicode=ucs4--enable-shared LDFLAGS=\"-Wl,-rpath /usr/local/lib\"\n$ make; sudo make altinstall\n$ sudo ln -sf /usr/local/bin/python2.7 /usr/local/bin/python\n\n```", "```py\n$ wget https://bootstrap.pypa.io/ez_setup.py\n$ sudo /usr/local/bin/python ez_setup.py\n$ sudo /usr/local/bin/easy_install-2.7 pip\n$ sudo /usr/local/bin/pip install --upgrade avro nose numpy scipy pandas statsmodels scikit-learn iso8601 python-dateutil python-snappy\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ export PYSPARK_PYTHON=/usr/local/bin/python\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ bin/pyspark \nPython 2.7.11 (default, Jan 23 2016, 20:14:24) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Python version 2.7.11 (default, Jan 23 2016 20:14:24)\nSparkContext available as sc, HiveContext available as sqlContext.\n>>>\n\n```", "```py\n>>> sfoFlights = sqlContext.sql(\"SELECT Dest, UniqueCarrier, ArrDelayMinutes FROM parquet.parquet\")\n>>> sfoFlights.groupBy([\"Dest\", \"UniqueCarrier\"]).agg(func.avg(\"ArrDelayMinutes\"), func.count(\"ArrDelayMinutes\")).sort(\"avg(ArrDelayMinutes)\", ascending=False).head(5)\n[Row(Dest=u'HNL', UniqueCarrier=u'HA', avg(ArrDelayMinutes)=53.70967741935484, count(ArrDelayMinutes)=31), Row(Dest=u'IAH', UniqueCarrier=u'F9', avg(ArrDelayMinutes)=43.064516129032256, count(ArrDelayMinutes)=31), Row(Dest=u'LAX', UniqueCarrier=u'DL', avg(ArrDelayMinutes)=39.68691588785047, count(ArrDelayMinutes)=214), Row(Dest=u'LAX', UniqueCarrier=u'WN', avg(ArrDelayMinutes)=29.704453441295545, count(ArrDelayMinutes)=247), Row(Dest=u'MSO', UniqueCarrier=u'OO', avg(ArrDelayMinutes)=29.551724137931036, count(ArrDelayMinutes)=29)]\n\n```", "```py\nscala> import sys.process._\nimport sys.process._\n\nscala> val retCode = Process(Seq(\"/usr/local/bin/python\", \"-c\", \"import socket; print(socket.gethostname())\")).!\nAlexanders-MacBook-Pro.local\nretCode: Int = 0\n\nscala> val lines = Process(Seq(\"/usr/local/bin/python\", \"-c\", \"\"\"from datetime import datetime, timedelta; print(\"Yesterday was {}\".format(datetime.now()-timedelta(days=1)))\"\"\")).!!\nlines: String =\n\"Yesterday was 2016-02-12 16:24:53.161853\n\"\n\n```", "```py\n#!/usr/bin/env python\n\nimport sys\nimport os\nimport re\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg import svd\n\nnp.set_printoptions(linewidth=10000)\n\ndef process_line(input):\n    inp = input.rstrip(\"\\r\\n\")\n    if len(inp) > 1:\n        try:\n            (mat, rank) = inp.split(\"|\")\n            a = np.matrix(mat)\n            r = int(rank)\n        except:\n            a = np.matrix(inp)\n            r = 1\n        U, s, Vh = linalg.svd(a, full_matrices=False)\n        for i in xrange(r, s.size):\n            s[i] = 0\n        S = linalg.diagsvd(s, s.size, s.size)\n        print(str(np.dot(U, np.dot(S, Vh))).replace(os.linesep, \";\"))\n\nif __name__ == '__main__':\n    map(process_line, sys.stdin)\n```", "```py\n$ echo -e \"1,2,3;2,1,2;3,2,1;7,8,9|3\" | ./svd.py\n[[ 1\\.  2\\.  3.]; [ 2\\.  1\\.  2.]; [ 3\\.  2\\.  1.]; [ 7\\.  8\\.  9.]]\n\n```", "```py\nscala> implicit class RunCommand(command: String) {\n |   def #<<< (input: String)(implicit buffer: StringBuilder) =  {\n |     val process = Process(command)\n |     val io = new ProcessIO (\n |       in  => { in.write(input getBytes \"UTF-8\"); in.close},\n |       out => { buffer append scala.io.Source.fromInputStream(out).getLines.mkString(\"\\n\"); buffer.append(\"\\n\"); out.close() },\n |       err => { scala.io.Source.fromInputStream(err).getLines().foreach(System.err.println) })\n |     (process run io).exitValue\n |   }\n | }\ndefined class RunCommand\n\n```", "```py\nscala> implicit val buffer = new StringBuilder()\nbuffer: StringBuilder =\n\nscala> if (\"./svd.py\" #<<< \"1,2,3;2,1,2;3,2,1;7,8,9|1\" == 0)  Some(buffer.toString) else None\nres77: Option[String] = Some([[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]])\n\n```", "```py\nscala> if (\"./svd.py\" #<<< \"\"\"\n | 1,2,3;2,1,2;3,2,1;7,8,9|0\n | 1,2,3;2,1,2;3,2,1;7,8,9|1\n | 1,2,3;2,1,2;3,2,1;7,8,9|2\n | 1,2,3;2,1,2;3,2,1;7,8,9|3\"\"\" == 0) Some(buffer.toString) else None\nres80: Option[String] =\nSome([[ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]]\n[[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]]\n[[ 0.9905897   2.02161614  2.98849663]; [ 1.72361156  1.63488399  1.66213642]; [ 3.04783513  1.89011928  1.05847477]; [ 7.04822694  7.88921926  9.05895373]]\n[[ 1\\.  2\\.  3.]; [ 2\\.  1\\.  2.]; [ 3\\.  2\\.  1.]; [ 7\\.  8\\.  9.]])\n\n```", "```py\nscala> sc.parallelize(List(\"1,2,3;2,1,2;3,2,1;7,8,9|0\", \"1,2,3;2,1,2;3,2,1;7,8,9|1\", \"1,2,3;2,1,2;3,2,1;7,8,9|2\", \"1,2,3;2,1,2;3,2,1;7,8,9|3\"),4).pipe(\"./svd.py\").collect.foreach(println)\n[[ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]]\n[[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]]\n[[ 0.9905897   2.02161614  2.98849663]; [ 1.72361156  1.63488399  1.66213642]; [ 3.04783513  1.89011928  1.05847477]; [ 7.04822694  7.88921926  9.05895373]]\n[[ 1\\.  2\\.  3.]; [ 2\\.  1\\.  2.]; [ 3\\.  2\\.  1.]; [ 7\\.  8\\.  9.]]\n\n```", "```py\n$ wget -O jython-standalone-2.7.0.jar http://search.maven.org/remotecontent?filepath=org/python/jython-standalone/2.7.0/jython-standalone-2.7.0.jar\n\n[akozlov@Alexanders-MacBook-Pro Scala]$ scala -cp jython-standalone-2.7.0.jar \nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import javax.script.ScriptEngine;\n...\nscala> import javax.script.ScriptEngineManager;\n...\nscala> import javax.script.ScriptException;\n...\nscala> val manager = new ScriptEngineManager();\nmanager: javax.script.ScriptEngineManager = javax.script.ScriptEngineManager@3a03464\n\nscala> val engines = manager.getEngineFactories();\nengines: java.util.List[javax.script.ScriptEngineFactory] = [org.python.jsr223.PyScriptEngineFactory@4909b8da, jdk.nashorn.api.scripting.NashornScriptEngineFactory@68837a77, scala.tools.nsc.interpreter.IMain$Factory@1324409e]\n\n```", "```py\nscala> val engine = new ScriptEngineManager().getEngineByName(\"jython\");\nengine: javax.script.ScriptEngine = org.python.jsr223.PyScriptEngine@6094de13\n\nscala> engine.eval(\"from datetime import datetime, timedelta; yesterday = str(datetime.now()-timedelta(days=1))\")\nres15: Object = null\n\nscala> engine.get(\"yesterday\")\nres16: Object = 2016-02-12 23:26:38.012000\n\n```", "```py\nscala> val startTime = System.nanoTime\nstartTime: Long = 54384084381087\n\nscala> for (i <- 1 to 100) {\n |   engine.eval(\"from datetime import datetime, timedelta; yesterday = str(datetime.now()-timedelta(days=1))\")\n |   val yesterday = engine.get(\"yesterday\")\n | }\n\nscala> val elapsed = 1e-9 * (System.nanoTime - startTime)\nelapsed: Double = 0.270837934\n\nscala> val startTime = System.nanoTime\nstartTime: Long = 54391560460133\n\nscala> for (i <- 1 to 100) {\n |   val yesterday = Process(Seq(\"/usr/local/bin/python\", \"-c\", \"\"\"from datetime import datetime, timedelta; print(datetime.now()-timedelta(days=1))\"\"\")).!!\n | }\n\nscala> val elapsed = 1e-9 * (System.nanoTime - startTime)\nelapsed: Double = 2.221937263\n\n```"]