["```py\nimport numpy as np \nfrom tensorflow.keras.applications import InceptionV3\ndef extract_features(img_data, IMG_SIZE):    \n    IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)    \n    model = InceptionV3(input_shape=IMG_SHAPE,\n                        include_top=False,\n                        weights='imagenet',\n                        pooling='avg')\n    predictions = model.predict(img_data)\n    return np.squeeze(predictions)\nlabels = [] # loaded previously\nfeatures = extract_features(image_data)\nX_train, X_test, y_train, y_test = train_test_split(\n    features, labels)\nfrom sklearn.svm import SVC\nclf = SVC(kernel='linear', C=1)\nclf.fit(X_train, y_train)\n```", "```py\n    import os\n    import urllib\n    os.makedirs('./data/mnist', exist_ok=True)\n    BASE_URL = 'http://yann.lecun.com/exdb/mnist/'\n    urllib.request.urlretrieve(\n        BASE_URL + 'train-images-idx3-ubyte.gz',\n        filename='./data/mnist/train-images.gz')\n    urllib.request.urlretrieve(\n        BASE_URL + 'train-labels-idx1-ubyte.gz',\n        filename='./data/mnist/train-labels.gz')\n    urllib.request.urlretrieve(\n        BASE_URL + 't10k-images-idx3-ubyte.gz',\n        filename='./data/mnist/test-images.gz')\n    urllib.request.urlretrieve(\n        BASE_URL + t10k-labels-idx1-ubyte.gz',\n        filename='./data/mnist/test-labels.gz')\n    ```", "```py\n    DIR = './data/mnist/'\n    X_train = load(DIR + 'train-images.gz', False) / 255.0\n    X_test = load(DIR + 'test-images.gz', False) / 255.0\n    y_train = load(DIR + 'train-labels.gz', True) \\\n                  .reshape(-1)\n    y_test = load(DIR + 'test-labels.gz', True) \\\n                 .reshape(-1)\n    ```", "```py\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, \\\n        MaxPooling2D, Flatten, Dense\n    model = Sequential()\n    model.add(Conv2D(filters=16,\n                     kernel_size=3,\n                     padding='same',\n                     activation='relu',\n                     input_shape=(28,28,1)))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(filters=32,\n                     kernel_size=3,\n                     padding='same',\n                     activation='relu'))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n    ```", "```py\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    ```", "```py\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    checkpoint_path = \"./mnist_cnn.bin\"\n    checkpoint_cb = ModelCheckpoint(checkpoint_path)\n    ```", "```py\n    model.fit(X_train,\n              y_train,\n              batch_size=16,\n              epochs=10,\n              callbacks=[checkpoint_cb])\n    ```", "```py\n    from tensorflow.keras.models import load_model\n    model = load_model(checkpoint_path)\n    scores = model.evaluate(X_test, y_test, verbose=1)\n    print('Test loss:', scores[0])\n    print('Test accuracy:', scores[1])\n    ```", "```py\n    from tensorflow.keras.preprocessing.image import \\\n        ImageDataGenerator\n    datagen = ImageDataGenerator()\n    ```", "```py\n    datagen.fit(x_train)\n    ```", "```py\n    it = datagen.flow(X_train, y_train, batch_size=16)\n    ```", "```py\n    it = datagen.flow_from_directory(\n             directory='./data/mnist',\n             target_size=(28, 28),\n             batch_size=16,\n             class_mode='categorical')\n    ```", "```py\n    model.fit_generator(it,\n                        steps_per_epoch=256,\n                        epochs=10,\n                        callbacks=[checkpoint_cb])\n    ```", "```py\ndatagen = ImageDataGenerator(\n              featurewise_center=True,\n              featurewise_std_normalization=True,\n              rotation_range=20,\n              width_shift_range=0.2,\n              height_shift_range=0.2,\n              horizontal_flip=True)\n```", "```py\n    from azureml.core.workspace import Workspace\n    ws = Workspace.from_config()\n    ```", "```py\n    from azureml.core.compute import ComputeTarget, \\\n        AmlCompute\n    from azureml.core.compute_target import \\\n        ComputeTargetException\n    cluster_name = \"gpu-cluster\"\n    vm_size = \"STANDARD_NC6\"\n    max_nodes = 3\n    try:\n        compute_target = ComputeTarget(ws, \n            name=cluster_name)\n        print('Found existing compute target.')\n    except ComputeTargetException:\n        print('Creating a new compute target...')\n        compute_config = \\\n            AmlCompute.provisioning_configuration(\n                vm_size=vm_size, max_nodes=max_nodes)\n        # create the cluster and wait for completion\n        compute_target = ComputeTarget.create(ws, \n            cluster_name, compute_config)\n    compute_target.wait_for_completion(show_output=True)\n    ```", "```py\n    ds = ws.get_default_datastore()\n    ds.upload(src_dir='./data/mnist',\n              target_path='mnist',\n              show_progress=True)\n    ```", "```py\nds_data = ds.as_mount()\n```", "```py\n    from azureml.core import ScriptRunConfig\n    script_params={\n        '--data-dir': ds_data\n    }\n    src = src = ScriptRunConfig(\n        source_directory='./scripts',\n        script='train.py',\n        compute_target=compute_target,\n        environment=tf_env)\n    ```", "```py\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data-dir', type=str)\n    args = parser.parse_args()\n    DIR = args.data_dir\n    X_train = load(DIR + 'train-images.gz', False) / 255.0\n    X_test = load(DIR + 'test-images.gz', False) / 255.0\n    y_train = load(DIR + 'train-labels.gz', True) \\\n                  .reshape(-1)\n    y_test = load(DIR + 'test-labels.gz', True) \\\n                  .reshape(-1)\n    ```", "```py\n    from azureml.core import Run\n    # Get the run configuration\n    run = Run.get_context()\n    # Create an Azure Machine Learning monitor callback\n    azureml_cb = AzureMlKerasCallback(run)\n    callbacks = [azureml_cb, checkpoint_cb]\n    model.fit_generator(it,\n                        steps_per_epoch=256,\n                        epochs=10,\n                        callbacks=callbacks)\n    # Load the best model\n    model = load_model(checkpoint_path)\n    # Score trained model\n    scores = model.evaluate(X_test, y_test, verbose=1)\n    print('Test loss:', scores[0])\n    run.log('Test loss', scores[0])\n    print('Test accuracy:', scores[1])\n    run.log('Test accuracy', scores[1])\n    ```", "```py\n    from tensorflow.keras.applications.resnet50 \\\n        import ResNet50\n    num_classes = 10\n    input_shape = (224, 224, 3)\n    # create the base pre-trained model\n    base_model = ResNet50(input_shape=input_shape, \n                          weights='imagenet',\n                          include_top=False,\n                          pooling='avg')\n    ```", "```py\n    for layer in base_model.layers:\n        layer.trainable = False\n    ```", "```py\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.layers import Flatten, Dense\n    clf = base_model.output\n    clf = Dense(256, activation='relu')(clf)\n    clf = Dense(10, activation='softmax')(clf)\n    model = Model(base_model.input, clf)\n    ```"]