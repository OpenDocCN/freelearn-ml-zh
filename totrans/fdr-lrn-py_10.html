<html><head></head><body>
		<div id="_idContainer102">
			<h1 id="_idParaDest-241" class="chapter-number"><a id="_idTextAnchor256"/>10</h1>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor257"/>Future Trends and Developments</h1>
			<p>Intelligence will drive the next generation of technologies, not big data. Big data systems have some issues, as discussed in <a href="B18369_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Challenges in Big Data and Traditional AI</em>, and the world is gradually transitioning from the data-centric era to the intelligence-centric generation. <strong class="bold">Federated learning</strong> (<strong class="bold">FL</strong>) will play a core role in wisdom-driven technologies. Thus, the time is now to welcome the world of collective intelligence.</p>
			<p>In this chapter, we will talk about the direction of future AI technologies that are driven by the paradigm shift happening with FL. For many AI fields, such as privacy-sensitive areas and fields requiring scalability in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), the benefits and potential of FL are already significant, mainly because of the privacy-preserving and distributed learning aspects that FL naturally supports with its design. You will then learn about the different types of FL as well as the latest development efforts in that area, as seen in the split and swarm learning techniques, which can be considered as evolutional frameworks enhancing FL.</p>
			<p>In addition, FL creates a new concept of an <em class="italic">Internet of Intelligence</em>, where people and computers exchange their wisdom instead of just data themselves. The Internet of Intelligence for everyone is further accelerated by blockchain technologies as well. This Internet of Intelligence can then form a newly defined concept of <em class="italic">collective intelligence</em> that drives another innovation, from <em class="italic">data-centric</em> approaches to <em class="italic">intelligence-centric</em> or <em class="italic">model-centric</em> approaches.</p>
			<p>Finally, we will share a collective vision in which FL plays a key role in collaboratively creating intelligence learned by many people and machines around the world.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Looking at future AI trends</li>
				<li>Ongoing research and developments in FL</li>
				<li>Journeying on to collective intelligence</li>
			</ul>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor258"/>Looking at future AI trends</h1>
			<p>The majority of industry leaders are now aware of the limitations of centralized ML as discussed in the next section.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor259"/>The limitation of centralized ML</h2>
			<p>When looking<a id="_idIndexMarker852"/> at the future of AI, it is important to first know the fact that many companies today are struggling to extract intelligence and obtain insight from the data they possess. More than half of the data that organizations and companies have collected is usually not used. Traditional approaches to machine learning and data science need data to be organized and consolidated into data lakes and stores in advance of analyzing and training ML models. You need to duplicate and move the data, which will result in delays in realizing and delivering the value of the intelligence extracted from the data, together with certain operational risks and complexities.</p>
			<p>In addition, most of the data generated by enterprise companies will be created and processed outside a traditional centralized data center or cloud. It is becoming increasingly unrealistic and inefficient to process data for generating insight in a centralized manner.</p>
			<p>Furthermore, based on some market reports out there, most of the largest global organizations and companies will explore FL at least once to create much more accurate, secure, and sustainable models environmentally.</p>
			<p>That being said, quite a few industries and markets are gradually becoming aware of the importance of a distributed and FL paradigm, because they are facing the unavoidable issues and limitations of the current centralized AI training with big data, as discussed in <a href="B18369_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Challenges in Big Data and Traditional AI</em>. FL brings the model to the data where the training process resides instead of bringing the data to the model. Thus, FL is considered to be the future of data science and ML.</p>
			<p>In the next section, let’s summarize the points of why FL is beneficial to those companies, especially enterprises that have been facing the aforementioned issues.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor260"/>Revisiting the benefits of FL</h2>
			<p>In this section, we will summarize the benefits of FL that have been introduced throughout this book.</p>
			<h3>Increased model accuracy and generalizability</h3>
			<p>FL realizes collaborative<a id="_idIndexMarker853"/> and distributed learning that can improve the performance of ML models, by training on dispersed datasets locally to continuously incorporate the learning into a global model. This way, more accurate and generalized ML models can be produced.</p>
			<h3>Further privacy and security</h3>
			<p>FL provides privacy and security advantages because it won’t require private and raw data by its design and security mechanisms, as we discussed previously in <a href="B18369_02.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a>, <em class="italic">What Is Federated Learning?</em> and <a href="B18369_09.xhtml#_idTextAnchor224"><em class="italic">Chapter 9</em></a>, <em class="italic">Case Studies with Key Use Cases of Federated Learning Applications</em>. Thus, FL<a id="_idIndexMarker854"/> reduces the potential risk<a id="_idIndexMarker855"/> of data misuse, leakage, or exposure<a id="_idIndexMarker856"/> to sensitive information. FL is also compliant with many privacy regulations, such as <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>), <strong class="bold">California Consumer Privacy Act</strong> (<strong class="bold">CCPA</strong>), and <strong class="bold">Health Insurance Portability and Accountability Act</strong> (<strong class="bold">HIPAA</strong>).</p>
			<h3>Improved speed and efficiency</h3>
			<p>FL is also known to realize high computation efficiency, which can accelerate the deployment and testing of ML models as well as decrease communication and computational latency. Due to the decentralized nature of FL, the delay for model delivery and update is minimized, which leads to a prediction by the global model in near real time. Real-time delivery and updates of intelligence are really valuable for time-sensitive ML applications.</p>
			<p>FL also helps reduce bandwidth and energy consumption by overcoming system heterogeneity and unbalanced data distribution, which leads to minimizing data storage and transfer costs that can also significantly contribute to reducing the environmental impact.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor261"/>Toward distributed learning for further privacy and training efficiency</h2>
			<p>Currently, AI is trained<a id="_idIndexMarker857"/> on huge computational servers, usually<a id="_idIndexMarker858"/> happening on big machines in big data companies.</p>
			<p>As seen in the era of the supercomputer, which can process a huge amount of data and tasks within one machine or one cluster of machines, the evolutional process in technology starts from a central location and gradually transitions to distributed environments.</p>
			<p>The same thing is exactly about to happen in AI. Now, the data lake concept is popular to organize and train ML models in one place, but ML already requires distributed learning frameworks.</p>
			<p>FL is a great way to distribute a training process over multiple nodes. As shown in many research reports, most data is not fully used to extract insights into ML models.</p>
			<p>There are some companies and projects that are trying to use FL as a powerful distributed learning<a id="_idIndexMarker859"/> technique, such as the<a id="_idIndexMarker860"/> platforms provided by Devron (<a href="https://devron.ai">devron.ai</a>), FedML (<a href="https://fedml.ai">fedml.ai</a>), and STADLE (<a href="https://stadle.ai">stadle.ai</a>). These platforms are already<a id="_idIndexMarker861"/> resolving the issues discussed in <em class="italic">The limitation of centralized AI</em> section and have shown a drastic improvement in the ML process in various use cases, as stated in the <em class="italic">Revisiting the benefits of FL</em> section.</p>
			<p>Based on the AI trends that we have<a id="_idIndexMarker862"/> discussed, let’s look into the ongoing research<a id="_idIndexMarker863"/> and developments related to FL that cutting-edge companies are conducting now in the next section.</p>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor262"/>Ongoing research and developments in FL</h1>
			<p>We now talk about the ongoing research<a id="_idIndexMarker864"/> and development projects<a id="_idIndexMarker865"/> that are being taken place both in academia and industries around the world. Let’s start with the different types and approaches of FL, and move on to ongoing efforts to further enhance the FL framework.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor263"/>Exploring various FL types and approaches</h2>
			<p>In this book, we have visited<a id="_idIndexMarker866"/> the most basic algorithms and design concepts of an FL system. In the real world, we need to dig a bit deeper into what types of FL frameworks are available to extract the best performance out of those algorithms. Depending on the data scenario and use cases, we have several approaches in FL, as follows:</p>
			<ul>
				<li>Horizontal FL and vertical FL</li>
				<li>Centralized FL and decentralized FL</li>
				<li>Cross-silo FL and cross-device FL</li>
			</ul>
			<p>Now, let’s look at each type of FL in the following sections.</p>
			<h3>Horizontal FL and vertical FL</h3>
			<p><strong class="bold">Horizontal FL</strong> uses datasets with the same feature<a id="_idIndexMarker867"/> space or schema across all distributed devices (<a href="https://www.arxiv-vanity.com/papers/1902.04885/">https://www.arxiv-vanity.com/papers/1902.04885/</a>). This actually means that datasets<a id="_idIndexMarker868"/> share the same columns with different rows. Most existing FL projects are based on horizontal FL. Datasets and training processes with horizontal FL are straightforward because the datasets are formed identically, with different data<a id="_idIndexMarker869"/> distributions and inputs<a id="_idIndexMarker870"/> to be learned. Horizontal FL is also called homogeneous or sample-based FL.</p>
			<p><strong class="bold">Vertical FL</strong> is applied to the cases where different<a id="_idIndexMarker871"/> datasets share the same sample ID space <a id="_idIndexMarker872"/>but differ in feature space. You can check out this paper (https://arxiv.org/pdf/2202.04309) for further information about vertical FL. Relating these different databases through FL can be challenging, especially if the unique ID for the data is different. The key idea of vertical FL is to improve an ML model by using distributed datasets with a diverse set of attributes. Therefore, vertical FL<a id="_idIndexMarker873"/> can handle the partitioned data<a id="_idIndexMarker874"/> vertically with different attributes in the same sample space. Vertical FL is also called heterogeneous or feature-based FL.</p>
			<h3>Centralized FL and decentralized FL</h3>
			<p><strong class="bold">Centralized FL</strong> is currently the most common approach<a id="_idIndexMarker875"/> and most of the platforms employ this framework. It uses a centralized server to collect and aggregate the different ML models, with distributed training across all local data sources. In this book, we focused on a centralized FL approach, with a scenario where local training agents communicate the learning results to a centralized FL server to create a global model.</p>
			<p><strong class="bold">Decentralized FL</strong>, on the other hand, does not use a centralized<a id="_idIndexMarker876"/> server to aggregate ML models. It requires individual ML models trained over local data sources to be communicated among themselves without a master node. In this case, model weights are transferred from each individual dataset to the others for further training. It could potentially be susceptible to model poisoning if an untrusted party could access the intelligence, and this is a common problem derived from peer-to-peer frameworks as well.</p>
			<h3>Cross-silo FL and cross-device FL</h3>
			<p><strong class="bold">Cross-silo FL</strong> is the case where ML models are trained<a id="_idIndexMarker877"/> on data distributed across any functional, organizational, and regulatory barriers. In this case, big data is usually stored in a larger size of storage, with training computing capabilities such as cloud virtual machines. In the cross-silo FL case, the number of silos/training environments is relatively small, so not so many agents are needed in the FL process.</p>
			<p><strong class="bold">Cross-device FL</strong> is the case where models need to be trained<a id="_idIndexMarker878"/> at scale, often within edge devices, such as mobile phones, <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) devices, Raspberry Pi-type<a id="_idIndexMarker879"/> environments, and so on. In this case, a huge number of devices are connected for the aggregation of ML models. In the cross-device FL case, the limitation basically lies in the low computing power of those edge devices. The framework also needs to handle a number of disconnected and inactive devices to conduct a consistent and continuous FL process. The training process and its data volume should be limited too.</p>
			<p>That concludes the different types of FL that can be applied to a variety of scenarios in ML applications. There are new techniques that try to enhance the FL framework to evolve into the next generation of AI technologies with FL. Let’s look into several advanced approaches in the next section.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor264"/>Understanding enhanced distributed learning frameworks with FL</h2>
			<p>There are ongoing efforts<a id="_idIndexMarker880"/> to further enhance FL or distributed learning frameworks.</p>
			<h3>Split learning</h3>
			<p><strong class="bold">Split learning</strong>, developed in the MIT Media Lab, is an emerging<a id="_idIndexMarker881"/> distributed learning technique that enables partitioning ML models into multiple sections, trains those partitioned ML models at distributed clients, and aggregates them at the end. Split learning does not have to share the data either, so it is considered a privacy-preserving AI as well.</p>
			<p>The overall framework is similar to the FL. However, there is a difference in that the neural network is partitioned into multiple sections that will be trained on distributed clients. The trained weights of the section of the neural network are then transferred to the server and clients. The weights of those multiple sections are continuously trained in the next training sessions. Therefore, no raw and private data is shared among the distributed clients, and only the weights of each section are sent to the next client.</p>
			<p>Especially, <strong class="bold">SplitFed</strong> (<a href="https://arxiv.org/abs/2004.12088">https://arxiv.org/abs/2004.12088</a>) is another advanced technique<a id="_idIndexMarker882"/> that combines split learning<a id="_idIndexMarker883"/> and FL. SplitFed splits the deep neural network architecture between the FL clients and servers to realize a higher level of privacy than FL. It offers better efficiency than split learning based on the parallel learning paradigm of FL.</p>
			<h3>Swarm learning</h3>
			<p><strong class="bold">Swarm learning</strong> is a decentralized ML solution built<a id="_idIndexMarker884"/> on blockchain technology, particularly designed to enable enterprise industries to take advantage of the power of distributed data, which results in protecting data privacy and security.</p>
			<p>This can be achieved by individual nodes sharing parameters of ML models derived from the local data.</p>
			<p>Parameters shared from the distributed clients are merged into a global model. The difference from the normal FL is that the merge process is not performed by a central server. The distributed nodes and clients choose a temporary leader to perform the merge. That is why swarm learning is truly decentralized, also providing greater fault tolerance and resiliency. The distributed agents have the collective intelligence of a network without sharing local data into one node.</p>
			<p>Swarm learning builds<a id="_idIndexMarker885"/> on top of blockchain. Blockchain provides the decentralized control, scalability, and fault-tolerance aspects to work beyond the restrictions of a single enterprise. At the same time, blockchain introduces a tamperproof cryptocurrency framework, and the participants can use the framework to monetize their contributions.</p>
			<h3>BAFFLE</h3>
			<p>In addition, there is a framework called <strong class="bold">BAFFLE</strong> that stands for <strong class="bold">Blockchain Based Aggregator Free Federated Learning</strong> (https://arxiv.org/abs/1909.07452). BAFFLE is also an<a id="_idIndexMarker886"/> aggregator-free, blockchain-driven FL framework<a id="_idIndexMarker887"/> that is inherently decentralized. BAFFLE utilizes <strong class="bold">Smart Contracts</strong> (<strong class="bold">SCs</strong>) from the blockchain framework to coordinate<a id="_idIndexMarker888"/> round management, as well as model aggregation and updating tasks of FL. Using BAFFLE boosts computational performance. The global model is also decomposed into many sets of chunks, directly handled by the SC.</p>
			<p>Now that we have learned about the latest research and developments in the FL field, in the next section, let’s look at a more visionary aspect of the AI, science, and technologies of coll<a id="_idTextAnchor265"/><a id="_idTextAnchor266"/><a id="_idTextAnchor267"/><a id="_idTextAnchor268"/><a id="_idTextAnchor269"/><a id="_idTextAnchor270"/><a id="_idTextAnchor271"/><a id="_idTextAnchor272"/><a id="_idTextAnchor273"/>ective intelligence.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor274"/>Journeying on to collective intelligence</h1>
			<p>Big data has been a game changer<a id="_idIndexMarker889"/> for the AI movement. While the amount of data generated at the edge and by people will increase exponentially, intelligence derived from that data benefits society. Therefore, the big data era will gradually pass the baton to the collective intelligence era, empowered by FL, in which people will collaboratively create a wisdom-driven world.</p>
			<p>Let’s start by defining an intelligence-centric era where the concept of collective intelligence is realized based on FL.</p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor275"/>Intelligence-centric era with collective intelligence</h2>
			<p><strong class="bold">Collective Intelligence</strong> (<strong class="bold">CI</strong>) is the concept of a large group of single entities acting together in ways<a id="_idIndexMarker890"/> that seem intelligent. CI is an emergent phenomenon where groups of people process information to achieve insights that are not understandable by just individual members alone.</p>
			<p>Recently, Thomas Malone, the head of the MIT Center for Collective Intelligence, and the person<a id="_idIndexMarker891"/> who initially coined the phrase <em class="italic">collective intelligence</em>, broadened the definition of CI: <em class="italic">“CI is something that can emerge from a group that includes people and computers. CI is a very general property, and superminds can arise in many kinds of systems, although the systems I’ve mostly talked about are those that involve people and computers”</em> (Reference: <a href="https://www2.deloitte.com/xe/en/insights/focus/technology-and-the-future-of-work/human-and-machine-collaboration.html">https://www2.deloitte.com/xe/en/insights/focus/technology-and-the-future-of-work/human-and-machine-collaboration.html</a>).</p>
			<p>We are now welcoming the new perspective of CI in technologies empowered by FL.</p>
			<p>Data, in the current world of technology, is a great source to extract intelligence. Dispersed datasets around the world can be converted into a collection of intelligence represented by AI technologies. The current trend, as mentioned, is big data, so big data companies are leading not only the technology industries but also the entire economy of the world as well. The future is moving in a CI direction. The vision of CI is even clearer with the emergence of sophisticated ML algorithms, including deep learning, as the intelligence represented by ML models can extract intelligence from people, computers, or any devices that generate meaningful data.</p>
			<p>Why does FL promote the idea of CI? The nature of FL is to collect a set of distributed intelligence to be enhanced by an aggregating mechanism as discussed in this book. This itself enables a data-less platform that does not require collecting data from people or devices directly.</p>
			<p>With the big data issues discussed throughout the book, we have steered clear of focusing on data-centric platform. However, it is also true that learning big data is very much critical and inevitable to really create systems and applications that are truly valuable and deliver real value in many domains of the world. That is why the big data field is still the most prosperous industry, even if it is facing significant challenges represented by privacy regulations, security, data silos, and so on.</p>
			<p>Now is the time to further develop and disseminate the technologies such as FL that can accelerate the era of CI by fundamentally resolving the issues of big data. This way, we can realize a new era of technologies, truly driven by CI that has been backed up by an authentic mathematical basis.</p>
			<p>As mentioned, <em class="italic">data-centric</em> platforms are the current trend. So many<a id="_idIndexMarker892"/> data and auto ML vendors can support and automate the processes of creating ML-based intelligence by organizing data and learning procedures to do so. An <em class="italic">intelligence-centric</em> or <em class="italic">model-centric</em> platform should be the next wave of technology<a id="_idIndexMarker893"/> in which people can share and enhance intelligence<a id="_idIndexMarker894"/> that they generate on their own. With FL, we can even realize crowd-sourced learning, where people can collaboratively and continuously enhance the quality and performance of ML models. Thus, FL is a critical and essential part of the intelligence-centric platform to truly achieve a wisdom-driven<a id="_idIndexMarker895"/> world.</p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor276"/>Internet of Intelligence</h2>
			<p>The IoT evolved into the <strong class="bold">Internet of Everything</strong>. However, what<a id="_idIndexMarker896"/> is the essential information<a id="_idIndexMarker897"/> that people<a id="_idIndexMarker898"/> want? Is it just<a id="_idIndexMarker899"/> big data? Or intelligence derived from data? With 5G technologies, a lot of data can be transferred over the Internet at a much higher speed, partially resolving the latency issues in many AI applications. FL can exchange less information than raw data but still needs to transfer ML models over the Internet.</p>
			<p>While lots of research projects are minimizing communications latency in FL, in the future, information related to intelligence will be another entity often exchanged over the web. There<a id="_idIndexMarker900"/> will be a model repository such as <strong class="bold">Model Zoo</strong> everywhere, and crowdsourced learning empowered by FL will be more common to create better intelligence over the Internet with people worldwide collaboratively.</p>
			<p>This paradigm shift is not just in the AI field itself but also in the wide range of information technologies. As we’ll discuss in the next sections, this <strong class="bold">Internet of Intelligence</strong> movement will be the basis of crowdsourced learning and CI, and will help make intelligence<a id="_idIndexMarker901"/> available <a id="_idTextAnchor277"/>t<a id="_idTextAnchor278"/>o as many people as possible in the coming years.</p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor279"/>Crowdsourced learning with FL</h2>
			<p>The <em class="italic">collection of intelligence</em> performed by FL naturally<a id="_idIndexMarker902"/> makes it a strong fit<a id="_idIndexMarker903"/> for moving toward CI. The same thing is applied to a scenario where people can collectively contribute a training process to global ML models.</p>
			<p>High-performing ML models in areas such as computer vision and natural language processing have been trained by certain big data companies, often spending a huge amount of money, including hundreds of millions of dollars.</p>
			<p>Is there any way to collectively train an ML model that will probably be beneficial for a wide range of people in general? With the advanced framework of FL, that is possible.</p>
			<p>FL provides an authentic way to manage the aggregation of multiple trained models from various distributed agents. In this case, the distributed agents themselves may be people worldwide, where each individual user and trainer of the ML model has their own unique datasets that are not available to anybody else because of data privacy, silos, and so on.</p>
			<p>This way of utilizing CI is often called <em class="italic">crowdsourced learning</em>. However, traditional crowdsourced learning<a id="_idIndexMarker904"/> is conducted in a much more limited way, just based on facilitating and recruiting data annotators at a large scale.</p>
			<p>With this new paradigm with FL, users on the CI platform can access and download ML models that they are interested in and retrain them if necessary to absorb learning in their own environments. Then, with the framework to share the trained ML models by those users, an advanced aggregation framework of FL could pick up the appropriate models to be federated and make the global model perform better, adopting diverse data that can be only accessible to the users.</p>
			<p>This way, intelligence by ML is becoming more available to many individuals in general, not just to specific companies that have a significant amount of data and budgets to train an authentic ML model. In other words, without an FL framework, collaborative learning is difficult and tricky and almost impossible to even automate. This openness of the ML models will move the entire technological world to the next level, and a lot more applications<a id="_idIndexMarker905"/> will become feasible, with truly powerful intelligence tha<a id="_idTextAnchor280"/>t is trained<a id="_idIndexMarker906"/> by enthusiasts to make the world better.</p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor281"/>Summary</h1>
			<p>In this final chapter of the book, we discussed fascinating future trends and developments in which FL is expected to play a crucial role in the coming decade. In the future, FL is a <em class="italic">must-to-have</em> technology from a <em class="italic">nice-to-have</em> framework for most enterprises and application providers, because of the inevitable privacy regulations and technology trends requiring scalability with so many users.</p>
			<p>As we discussed, future technologies will be empowered by the concept of the Internet of Intelligence, by which people and computers mainly exchange their wisdom altogether to create a more intelligent society and world. Finally, the data-centric technologies will gradually evolve into intelligence-centric technologies because of the current collaborative learning trend with CI, which makes people pay significant attention to FL-related technologies, whose foundations are discussed throughout this book.</p>
			<p>This book was written at the dawn of a new age in advancements made possible by AI. There are many uncertainties and many more challenges ahead. We have made great strides in utilizing the big data playbook in the last couple of decades, and we have now outgrown those methods and must adopt new ways of doing things, new technologies, and new ideas to forge ahead. As long as we capture the current moment and invest in new technologies such as FL, we will have a bright future ahead of us.</p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor282"/>Further reading</h1>
			<p>The following are some sources if you wish to dive deeper into some concepts discussed in this chapter:</p>
			<ul>
				<li><em class="italic">UNDERSTANDING THE TYPES OF FEDERATED LEARNING</em>, posted by OpenMinded: <a href="https://blog.openmined.org/federated-learning-types&#13;">https://blog.openmined.org/federated-learning-types</a></li>
				<li>Thapa, Chandra, et al. <em class="italic">SplitFed: When Federated Learning Meets Split Learning</em>, Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022: <a href="https://arxiv.org/pdf/2004.12088.pdf&#13;">https://arxiv.org/pdf/2004.12088.pdf</a></li>
				<li><em class="italic">SWARM LEARNING: TURN YOUR DISTRIBUTED DATA INTO COMPETITIVE EDGE,</em> technical white paper: <a href="https://www.labs.hpe.com/pdf/Swarm_Learning.pdf&#13;">https://www.labs.hpe.com/pdf/Swarm_Learning.pdf</a></li>
				<li>Paritosh Ramanan and Kiyoshi Nakayama. <em class="italic">BAFFLE: Blockchain based aggregator free federated learning</em>, 2020 IEEE International Conference on Blockchain (Blockchain). IEEE, 2020: <a href="https://arxiv.org/pdf/1909.07452.pdf">https://arxiv.org/pdf/1909.07452.pdf</a></li>
			</ul>
		</div>
	

		<div id="_idContainer104">
			<h1 id="_idParaDest-256">Appendix: <a id="_idTextAnchor283"/>Exploring Internal Libraries</h1>
			<p>In <a href="B18369_04.xhtml#_idTextAnchor085"><em class="italic">Chapter 4</em></a>, <em class="italic">Federated Learning Server Implementation with Python</em>, and <a href="B18369_05.xhtml#_idTextAnchor130"><em class="italic">Chapter 5</em></a>, <em class="italic">Federated Learning Client-Side Implementation</em>, both about the implementation of <strong class="bold">federated learning</strong> (<strong class="bold">FL</strong>) systems, internal library functions were given to simplify the explanation of the implementation of the FL server and client functionalities and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications. Here, we will talk about those internal libraries, such as the communications handler, data structure handler, and enumeration class definitions, in more detail for you to be able to easily implement the FL systems that work over the internet and on the cloud. Those internal libraries and supporting functions can all be found in the <strong class="source-inline">fl_main/lib/util</strong> directory of the provided <strong class="source-inline">simple-fl</strong> GitHub repository.</p>
			<p>In this appendix, we will provide an overview of the internal library and utilization classes and functions with code samples to achieve their functionalities.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>Overview of the internal libraries for the FL system</li>
				<li>Enumeration classes for implementing the FL system</li>
				<li>Understanding communication handler functionalities</li>
				<li>Understanding the data structure handler class</li>
				<li>Understanding helper and supporting libraries</li>
				<li>Messengers to generate communication payloads</li>
			</ul>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor284"/>Technical requirements</h1>
			<p>All the library code files introduced in this chapter can be found in the <strong class="source-inline">fl_main/lib/util</strong> directory of the GitHub repository (https://github.com/tie-set/simple-fl). </p>
			<p class="callout-heading">Important note </p>
			<p class="callout">You can use the code files for personal or educational purposes. Please note that we will not support deployment for commercial use and will not be responsible for any errors, issues, or damages caused by using the code.</p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor285"/>Overview of the internal libraries for the FL system</h1>
			<p><em class="italic">Figure A.1</em> shows <a id="_idIndexMarker907"/>the Python code components for the internal libraries<a id="_idIndexMarker908"/> found in the <strong class="source-inline">lib/util</strong> folder of the <strong class="source-inline">fl_main</strong> directory, which is used in the database, aggregator, and agent of the FL system:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B18369_A_01.jpg" alt="Figure A.1 – Python software components for the internal libraries used in the database, aggregator, and agent&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.1 – Python software components for the internal libraries used in the database, aggregator, and agent</p>
			<p>The following are brief descriptions of the Python files for the internal libraries found in the <strong class="source-inline">lib/util</strong> folder of the FL system.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor286"/>states.py</h2>
			<p>The <strong class="source-inline">states.py</strong> file in<a id="_idIndexMarker909"/> the <strong class="source-inline">lib/util</strong> folder defines a variety of enumeration classes to support implementing the FL system. Definitions of the classes include FL client states, types of ML models and messages, and locations of the information and values of various messages.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor287"/>communication_handler.py</h2>
			<p>The <strong class="source-inline">communication_handler.py</strong> file <a id="_idIndexMarker910"/>in the <strong class="source-inline">lib/util</strong> folder can provide communication functionalities among the database, FL server, and clients, mainly defining the <strong class="source-inline">send</strong> and <strong class="source-inline">receive</strong> functions between them. Also, it provides the functions to start the servers for the database, aggregator, and agent.</p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor288"/>data_struc.py</h2>
			<p>The <strong class="source-inline">data_struc.py</strong> file in<a id="_idIndexMarker911"/> the <strong class="source-inline">lib/util</strong> folder defines the class called <strong class="source-inline">LimitedDict</strong> to support an aggregation process of the FL cycle. It provides functions to convert ML models with a dictionary format into <strong class="source-inline">LimitedDict</strong> and vice versa.</p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor289"/>helpers.py</h2>
			<p>The <strong class="source-inline">helpers.py</strong> file <a id="_idIndexMarker912"/>in the <strong class="source-inline">lib/util</strong> folder has a collection of internal helper functions, such as reading configuration files, generating unique hash IDs, packaging ML models into a dictionary, loading and saving local ML models, getting the IP address of the machine, and manipulating the FL client state.</p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor290"/>messengers.py</h2>
			<p>The <strong class="source-inline">messengers.py</strong> file in<a id="_idIndexMarker913"/> the <strong class="source-inline">lib/util</strong> folder is for generating a variety of messages as communication payloads among FL systems to facilitate the implementation of communication protocols of the simple FL system discussed <a id="_idIndexMarker914"/>throughout the book.</p>
			<p>Now that we have discussed an overview of the FL system’s internal libraries, next, let’s talk about the individual code files in more detail.</p>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor291"/>Enumeration classes for implementing the FL system</h1>
			<p>Enumeration classes are for assisting implemention of the FL system. They are defined in the <strong class="source-inline">states.py</strong> file found in the <strong class="source-inline">lib/util</strong> folder of the <strong class="source-inline">fl_main</strong> directory. Let us look into what libraries are imported to define the enumeration classes.</p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor292"/>Importing libraries to define the enumeration classes</h2>
			<p>In this <strong class="source-inline">states.py</strong> code <a id="_idIndexMarker915"/>example, the file imports general libraries such as <strong class="source-inline">Enum</strong> and <strong class="source-inline">IntEnum</strong> from <strong class="source-inline">enum</strong>:</p>
			<pre class="source-code">
from enum import Enum, IntEnum</pre>
			<p>Next, we’ll explain the class that defines the prefixes of three components of the FL system.</p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor293"/>IDPrefix defining the FL system components</h2>
			<p>The following is a list of classes to define the FL system <a id="_idIndexMarker916"/>components. <strong class="source-inline">IDPrefix</strong> is the prefix to indicate which FL component is referred to in the code, such as <strong class="source-inline">agent</strong>, <strong class="source-inline">aggregator</strong>, or <strong class="source-inline">database</strong>:</p>
			<pre class="source-code">
class IDPrefix:
    agent = 'agent'
    aggregator = 'aggregator'
    db = 'database'</pre>
			<p>Next, we’ll provide a list of the classes for the client state.</p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor294"/>Client state classes</h2>
			<p>The following is a list of <a id="_idIndexMarker917"/>enumeration classes related to the FL client states, including the state of waiting for global models (<strong class="source-inline">waiting_gm</strong>), the state of ML training (<strong class="source-inline">training</strong>), the state of sending local ML models (<strong class="source-inline">sending</strong>), and the state of receiving the global models (<strong class="source-inline">gm_ready</strong>). The client states defined in the agent specification are as follows:</p>
			<pre class="source-code">
# CLIENT STATE
class ClientState(IntEnum):
    waiting_gm = 0
    training = 1
    sending = 2
    gm_ready = 3</pre>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor295"/>List of classes defining the types of ML models and messages</h2>
			<p>The following is a list of <a id="_idIndexMarker918"/>classes defining the types of ML models and messages related to the FL system implementation.</p>
			<h3>The ModelType class</h3>
			<p>The types of <a id="_idIndexMarker919"/>ML models, including <strong class="source-inline">local</strong> models and <strong class="source-inline">cluster</strong> models (<strong class="source-inline">global</strong> models), are defined as follows:</p>
			<pre class="source-code">
class ModelType(Enum):
    local = 0
    cluster = 1</pre>
			<h3>The DBMsgType class</h3>
			<p>The message types are defined<a id="_idIndexMarker920"/> in the communication protocol between an aggregator and database, as follows:</p>
			<pre class="source-code">
class DBMsgType(Enum):
    push = 0</pre>
			<h3>The AgentMsgType class</h3>
			<p>The message types are <a id="_idIndexMarker921"/>defined in the communication protocol sent from an agent to an aggregator, as follows:</p>
			<pre class="source-code">
class AgentMsgType(Enum):
    participate = 0
    update = 1
    polling = 2</pre>
			<h3>The AggMsgType class</h3>
			<p>The message types are<a id="_idIndexMarker922"/> defined in the communication protocol sent from an aggregator to an agent, as follows:</p>
			<pre class="source-code">
class AggMsgType(Enum):
    welcome = 0
    update = 1
    ack = 2</pre>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor296"/>List of state classes defining message location</h2>
			<p>The following is a list of classes <a id="_idIndexMarker923"/>defining the message location related to communication between the FL systems.</p>
			<h3>The ParticipateMSGLocation class</h3>
			<p>The index indicator to read a <a id="_idIndexMarker924"/>participation message from an agent to the aggregator is as follows:</p>
			<pre class="source-code">
class ParticipateMSGLocation(IntEnum):
    msg_type = 0
    agent_id = 1
    model_id = 2
    lmodels = 3
    init_flag = 4
    sim_flag = 5
    exch_socket = 6
    gene_time = 7
    meta_data = 8
    agent_ip = 9
    agent_name = 10
    round = 11</pre>
			<h3>The ParticipateConfirmationMSGLocation class</h3>
			<p>The index indicator to read <a id="_idIndexMarker925"/>a participation confirmation message sent back from the aggregator is as follows:</p>
			<pre class="source-code">
class ParticipateConfirmationMSGLocation(IntEnum):
    msg_type = 0
    aggregator_id = 1
    model_id = 2
    global_models = 3
    round = 4
    agent_id = 5
    exch_socket = 6
    recv_socket = 7</pre>
			<h3>The DBPushMsgLocation class</h3>
			<p>The index indicator <a id="_idIndexMarker926"/>to read a <strong class="source-inline">push</strong> message from an aggregator to the database is as follows:</p>
			<pre class="source-code">
class DBPushMsgLocation(IntEnum):
    msg_type = 0
    component_id = 1
    round = 2
    model_type = 3
    models = 4
    model_id = 5
    gene_time = 6
    meta_data = 7
    req_id_list = 8</pre>
			<h3>The GMDistributionMsgLocation class</h3>
			<p>The index indicator to<a id="_idIndexMarker927"/> read a global model distribution message from an aggregator to agents is as follows:</p>
			<pre class="source-code">
class GMDistributionMsgLocation(IntEnum):
    msg_type = 0
    aggregator_id = 1
    model_id = 2
    round = 3
    global_models = 4</pre>
			<h3>The ModelUpMSGLocation class</h3>
			<p>The index <a id="_idIndexMarker928"/>indicator to a message uploading local ML models from an agent to an aggregator is as follows:</p>
			<pre class="source-code">
class ModelUpMSGLocation(IntEnum):
    msg_type = 0
    agent_id = 1
    model_id = 2
    lmodels = 3
    gene_time = 4
    meta_data = 5</pre>
			<h3>The PollingMSGLocation class</h3>
			<p>The index indicator for a <strong class="source-inline">polling</strong> message<a id="_idIndexMarker929"/> from an agent to an aggregator is as follows:</p>
			<pre class="source-code">
class PollingMSGLocation(IntEnum):
    msg_type = 0
    round = 1
    agent_id = 2</pre>
			<p>We have defined the enumeration classes that are utilized throughout the code of the FL system. In the next section, we will discuss the communication handler functionalities.</p>
			<h1 id="_idParaDest-270"><a id="_idTextAnchor297"/>Understanding communication handler functionalities</h1>
			<p>The communication handler<a id="_idIndexMarker930"/> functionalities are implemented in the <strong class="source-inline">communication_handler.py</strong> file, which can be found in the <strong class="source-inline">lib/util</strong> folder of the <strong class="source-inline">fl_main</strong> directory.</p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor298"/>Importing libraries for the communication handler</h2>
			<p>In <a id="_idIndexMarker931"/>this <strong class="source-inline">communication_handler.py</strong> code example, the handler imports general libraries such as <strong class="source-inline">websockets</strong>, <strong class="source-inline">asyncio</strong>, <strong class="source-inline">pickle</strong>, and <strong class="source-inline">logging</strong>:</p>
			<pre class="source-code">
import websockets, asyncio, pickle, logging</pre>
			<p>Next, we’ll provide a list of functions of the communication handler.</p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor299"/>Functions of the communication handler</h2>
			<p>The<a id="_idIndexMarker932"/> following is a list of the functions related to the communication hander. Although the <strong class="bold">Secure Sockets Layer</strong> (<strong class="bold">SSL</strong>) or <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) framework is not implemented in the communication handler code here for simplification, it is recommended to support them to secure communication among FL components all the time.</p>
			<h3>The init_db_server function</h3>
			<p>The <strong class="source-inline">init_db_server</strong> function is<a id="_idIndexMarker933"/> for starting the database server on the FL server side. It takes a function, database IP address, and socket information as inputs and initiates the server functionality based on the WebSocket framework. You can use any other communication protocol, such as HTTP, as well. Here is the sample code to initiate the database server:</p>
			<pre class="source-code">
def init_db_server(func, ip, socket):
    start_server = websockets.serve( \
        func, ip, socket, max_size=None, max_queue=None)
    loop = asyncio.get_event_loop()
    loop.run_until_complete(start_server)
    loop.run_forever()</pre>
			<h3>The init_fl_server function</h3>
			<p>The <strong class="source-inline">init_fl_server</strong> function is <a id="_idIndexMarker934"/>for starting the FL server on the aggregator side. As parameters, it takes three functions for agent registration, receiving messages from agents, and the model synthesis routine, as well as the aggregator’s IP address and registration and receiver sockets info (to receive messages from agents) to initiate the server functionality based on the WebSocket framework. Here is the sample code for initiating the FL server:</p>
			<pre class="source-code">
def init_fl_server(register, receive_msg_from_agent, \
            model_synthesis_routine, aggr_ip, \
            reg_socket, recv_socket):
    loop = asyncio.get_event_loop()
    start_server = websockets.serve(register, aggr_ip, \
        reg_socket, max_size=None, max_queue=None)
    start_receiver = websockets.serve( \
        receive_msg_from_agent, aggr_ip, recv_socket, \
        max_size=None, max_queue=None)
    loop.run_until_complete(asyncio.gather( \
        start_server, start_receiver, \
        model_synthesis_routine))
    loop.run_forever()</pre>
			<h3>The init_client_server function</h3>
			<p>The <strong class="source-inline">init_client_server</strong> function<a id="_idIndexMarker935"/> is for starting the FL client-side server functionalities. It takes a function, the agent’s IP address, and the socket info to receive messages from an aggregator as inputs and initiate the functionality based on the WebSocket framework. Here is sample code for initiating the FL client-side server functionality:</p>
			<pre class="source-code">
def init_client_server(func, ip, socket):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    client_server = websockets.serve(func, ip, socket, \
        max_size=None, max_queue=None)
    loop.run_until_complete(asyncio.gather(client_server))
    loop.run_forever()</pre>
			<h3>The send function</h3>
			<p>The <strong class="source-inline">send</strong> function is for <a id="_idIndexMarker936"/>sending a message to the destination specified by the IP address and socket info taken as parameters together with a message to be sent. It returns a response message sent back from the destination node to the source node, if there is one:</p>
			<pre class="source-code">
async def send(msg, ip, socket):
    resp = None
    try:
        wsaddr = f'ws://{ip}:{socket}'
        async with websockets.connect( \
                wsaddr, max_size=None, max_queue=None, \
                ping_interval=None) as websocket:
            await websocket.send(pickle.dumps(msg))
            try:
                rmsg = await websocket.recv()
                resp = pickle.loads(rmsg)
            except:
                pass
            return resp
    except:
        return resp</pre>
			<h3>The send_websocket function</h3>
			<p>The <strong class="source-inline">send_websocket</strong> function is<a id="_idIndexMarker937"/> for returning a message to the message source specified by the WebSocket information, taken as a parameter together with a message to be sent:</p>
			<pre class="source-code">
async def send_websocket(msg, websocket):
    while not websocket:
        await asyncio.sleep(0.001)
    await websocket.send(pickle.dumps(msg))</pre>
			<h3>The receive function</h3>
			<p>The <strong class="source-inline">receive</strong> function<a id="_idIndexMarker938"/> is used to receive a message with the WebSocket taken as a parameter and returns a pickled message:</p>
			<pre class="source-code">
async def receive(websocket):
    return pickle.loads(await websocket.recv())</pre>
			<p>Next, we will talk about the data structure class that handles processing ML models.</p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor300"/>Understanding the data structure handler class</h1>
			<p>The data structure handler<a id="_idIndexMarker939"/> is implemented in the <strong class="source-inline">data_struc.py</strong> file, which can be found in the <strong class="source-inline">lib/util</strong> folder of the <strong class="source-inline">fl_main</strong> directory. The data structure class has the <strong class="source-inline">LimitedDict</strong> class to handle the aggregation of the ML models in a consistent manner.</p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor301"/>Importing libraries for the data structure handler</h2>
			<p>In this <strong class="source-inline">data_struc.py</strong> code <a id="_idIndexMarker940"/>example, the handler imports general libraries, such as <strong class="source-inline">numpy</strong> and <strong class="source-inline">Dict</strong>:</p>
			<pre class="source-code">
from typing import Dict
import numpy as np</pre>
			<p>Next, let’s move on to the <strong class="source-inline">LimitedDict</strong> class and its functions related to the data structure handler.</p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor302"/>The LimitedDict class</h2>
			<p>The following is a<a id="_idIndexMarker941"/> definition of the <strong class="source-inline">LimitedDict</strong> class and its functions related to the data structure handler.</p>
			<h3>The LimitedDict class and its functions</h3>
			<p>The functions<a id="_idIndexMarker942"/> of the <strong class="source-inline">LimitedDict</strong> class are for converting a dictionary format into a class with keys and values. <strong class="source-inline">LimitedDict</strong> is used with the buffer in ML models to store local and cluster models in the memory space of the state manager of the aggregator:</p>
			<pre class="source-code">
class LimitedDict(dict):
    def __init__(self, keys):
        self._keys = keys
        self.clear()
    def __setitem__(self, key, value):
        if key not in self._keys:
            raise KeyError
        dict.__setitem__(self, key, value)
    def clear(self):
        for key in self._keys:
            self[key] = list()</pre>
			<h3>The convert_LDict_to_Dict function</h3>
			<p>The <strong class="source-inline">convert_LDict_to_Dict</strong> function <a id="_idIndexMarker943"/>is used to convert the <strong class="source-inline">LimitedDict</strong> instance defined previously into a normal dictionary format:</p>
			<pre class="source-code">
def convert_LDict_to_Dict(ld: LimitedDict)
        -&gt; Dict[str,np.array]:
    d = dict()
    for key, val in ld.items():
        d[key] = val[0]
    return d</pre>
			<p>In the next section, we will talk about the helper and supporting libraries.</p>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor303"/>Understanding helper and supporting libraries</h1>
			<p>The helper and supporting functions are implemented in the <strong class="source-inline">helpers.py</strong> file, which can be found in the <strong class="source-inline">lib/util</strong> folder of the <strong class="source-inline">fl_main</strong> directory.</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor304"/>Importing libraries for helper libraries</h2>
			<p>In this <strong class="source-inline">helpers.py</strong> code <a id="_idIndexMarker944"/>example, the file imports general libraries such as <strong class="source-inline">json</strong> and <strong class="source-inline">time</strong>:</p>
			<pre class="source-code">
import json, time, pickle, pathlib, socket, asyncio
from getmac import get_mac_address as gma
from typing import Dict, List, Any
from hashlib import sha256
from fl_main.lib.util.states import IDPrefix, ClientState</pre>
			<p>Next, let’s move on to the list of functions of the helper library.</p>
			<h2 id="_idParaDest-278"><a id="_idTextAnchor305"/>Functions of the helper library</h2>
			<p>The following<a id="_idIndexMarker945"/> is a list of functions related to the helper library.</p>
			<h3>The set_config_file function</h3>
			<p>The <strong class="source-inline">set_config_file</strong> function<a id="_idIndexMarker946"/> takes the type of the config file, such as <strong class="source-inline">db</strong>, <strong class="source-inline">aggregator</strong>, or <strong class="source-inline">agent</strong>, as a parameter and returns a string of the path to the configuration file:</p>
			<pre class="source-code">
def set_config_file(config_type: str) -&gt; str:
    # set the config file name
    module_path = pathlib.Path.cwd()
    config_file = \
        f'{module_path}/setups/config_{config_type}.json'
    return config_file</pre>
			<h3>The read_config function</h3>
			<p>The <strong class="source-inline">read_config</strong> function <a id="_idIndexMarker947"/>reads a JSON configuration file to set up the database, aggregator, or agent. It takes a config path as a parameter and returns config info in a dictionary format:</p>
			<pre class="source-code">
def read_config(config_path: str) -&gt; Dict[str, Any]:
    with open(config_path) as jf:
        config = json.load(jf)
    return config</pre>
			<h3>The generate_id function</h3>
			<p>The <strong class="source-inline">generate_id</strong> function <a id="_idIndexMarker948"/>generates a system-wide unique ID based on the MAC address and instantiation time with a hash function (<strong class="source-inline">sha256</strong>) returning the hash value as an ID:</p>
			<pre class="source-code">
def generate_id() -&gt; str:
    macaddr = gma()
    in_time = time.time()
    raw = f'{macaddr}{in_time}'
    hash_id = sha256(raw.encode('utf-8'))
    return hash_id.hexdigest()</pre>
			<h3>The generate_model_id function</h3>
			<p>The <strong class="source-inline">generate_model_id</strong> function<a id="_idIndexMarker949"/> generates a system-wide unique ID for a set of models based on the following:</p>
			<ul>
				<li><strong class="bold">Component ID</strong>: The ID of the FL system entity that created the models</li>
				<li><strong class="bold">Generation time</strong>: The time the models were created</li>
			</ul>
			<p>The ID is generated by a hash function (sha256). It takes the following parameters:</p>
			<ul>
				<li><strong class="source-inline">component_type</strong>: A string value with a prefix indicating the component type of <strong class="source-inline">IDPrefix</strong></li>
				<li><strong class="source-inline">component_id</strong>: A string value of the ID of the entity that created the models</li>
				<li><strong class="source-inline">gene_time</strong>: A float value of the time the models were created</li>
			</ul>
			<p>This function <a id="_idIndexMarker950"/>returns the hash value as a model ID:</p>
			<pre class="source-code">
def generate_model_id(component_type: str, \
        component_id: str, gene_time: float) -&gt; str:
    raw = f'{component_type}{component_id}{gene_time}'
    hash_id = sha256(raw.encode('utf-8'))
    return hash_id.hexdigest()</pre>
			<h3>The create_data_dict_from_models function</h3>
			<p>The <strong class="source-inline">create_data_dict_from_models</strong> function<a id="_idIndexMarker951"/> creates the data dictionary for ML models by taking the following parameters:</p>
			<ul>
				<li><strong class="source-inline">model_id</strong>: A string value of the model ID</li>
				<li><strong class="source-inline">models</strong>: The <strong class="source-inline">np.array</strong> about ML models</li>
				<li><strong class="source-inline">component_id</strong>: The ID of the FL system such as aggregator ID and agent ID</li>
			</ul>
			<p>It returns a data dictionary containing the ML models:</p>
			<pre class="source-code">
def create_data_dict_from_models( \
        model_id, models, component_id):
    data_dict = dict()
    data_dict['models'] = models
    data_dict['model_id'] = model_id
    data_dict['my_id'] = component_id
    data_dict['gene_time'] = time.time()
    return data_dict</pre>
			<h3>The create_meta_data_dict function</h3>
			<p>The <strong class="source-inline">create_meta_data_dict</strong> function<a id="_idIndexMarker952"/> creates the metadata dictionary with the metadata of the ML models, taking the performance metrics (<strong class="source-inline">perf_val</strong>) and the number of samples (<strong class="source-inline">num_samples</strong>) as parameters, and returns <strong class="source-inline">meta_data_dict</strong>, containing the performance value and the number of samples:</p>
			<pre class="source-code">
def create_meta_data_dict(perf_val, num_samples):
    meta_data_dict = dict()
    meta_data_dict["accuracy"] = perf_val
    meta_data_dict["num_samples"] = num_samples
    return meta_data_dict</pre>
			<h3>The compatible_data_dict_read function</h3>
			<p>The <strong class="source-inline">compatible_data_dict_read</strong> function takes <strong class="source-inline">data_dict</strong>, which contains the information related to<a id="_idIndexMarker953"/> ML models, extracts the values if the corresponding key exists in the dictionary, and returns the component ID, the generation time of the ML models, the ML models themselves, and the model IDs:</p>
			<pre class="source-code">
def compatible_data_dict_read(data_dict: Dict[str, Any])
        -&gt; List[Any]:
    if 'my_id' in data_dict.keys():
        id = data_dict['my_id']
    else:
        id = generate_id()
    if 'gene_time' in data_dict.keys():
        gene_time = data_dict['gene_time']
    else:
        gene_time = time.time()
    if 'models' in data_dict.keys():
        models = data_dict['models']
    else:
        models = data_dict
    if 'model_id' in data_dict.keys():
        model_id = data_dict['model_id']
    else:
        model_id = generate_model_id( \
                       IDPrefix.agent, id, gene_time)
    return id, gene_time, models, model_id</pre>
			<h3>The save_model_file function</h3>
			<p>The <strong class="source-inline">save_model_file</strong> function is <a id="_idIndexMarker954"/>for saving a given set of models into a local file. It takes the following parameters:</p>
			<ul>
				<li><strong class="source-inline">data_dict</strong>: A dictionary containing the model ID and ML models with the <strong class="source-inline">Dict[str,np.array]</strong> format.</li>
				<li><strong class="source-inline">path</strong>: A string value of the path to the directory of the ML model storage.</li>
				<li><strong class="source-inline">name</strong>: A string value of the model filename.</li>
				<li><strong class="source-inline">performance_dict</strong>: A dictionary containing performance data with the <strong class="source-inline">Dict[str,float]</strong> format. Each entry contains both the model ID and its performance information:</li>
			</ul>
			<pre class="source-code">
def save_model_file(
        data_dict: Dict[str, Any], path: str, name: str,
        performance_dict: Dict[str, float] = dict()):
    data_dict['performance'] = performance_dict
    fname = f'{path}/{name}'
    with open(fname, 'wb') as f:
        pickle.dump(data_dict, f)</pre>
			<h3>The load_model_file function</h3>
			<p><strong class="source-inline">load_model_file</strong> reads a local <a id="_idIndexMarker955"/>model file that takes the following parameters:</p>
			<ul>
				<li><strong class="source-inline">path</strong>: A string value of the path to the directory to store ML models</li>
				<li><strong class="source-inline">name</strong>: A string value of the model filename</li>
			</ul>
			<p>It returns the unpickled ML models and performance data in the <strong class="source-inline">Dict</strong> format:</p>
			<pre class="source-code">
def load_model_file(path: str, name: str) \
        -&gt; (Dict[str, Any], Dict[str, float]):
    fname = f'{path}/{name}'
    with open(fname, 'rb') as f:
        data_dict = pickle.load(f)
    performance_dict = data_dict.pop('performance')
    # data_dict only includes models
    return data_dict, performance_dict</pre>
			<h3>The read_state function</h3>
			<p>The <strong class="source-inline">read_state</strong> function <a id="_idIndexMarker956"/>reads a local state file that takes the following parameters:</p>
			<ul>
				<li><strong class="source-inline">path</strong>: A string value of the path to the directory of the client state file</li>
				<li><strong class="source-inline">name</strong>: A string value of the model filename</li>
			</ul>
			<p>This <a id="_idIndexMarker957"/>function returns a client state, <strong class="source-inline">ClientState</strong> (for example, <em class="italic">training</em> or <em class="italic">sending</em>), the state indicated in the file, in an integer format. If the client state file is being written at the time of access, it will try to read the file again after 0.01 seconds:</p>
			<pre class="source-code">
def read_state(path: str, name: str) -&gt; ClientState:
    fname = f'{path}/{name}'
    with open(fname, 'r') as f:
        st = f.read()
    if st == '':
        time.sleep(0.01)
        return read_state(path, name)
    return int(st)</pre>
			<h3>The write_state function</h3>
			<p><strong class="source-inline">write_state</strong> changes the<a id="_idIndexMarker958"/> client state on the state file in the agent. It takes the following parameters:</p>
			<ul>
				<li><strong class="source-inline">path</strong>: A string value of the path to the directory of the client state file</li>
				<li><strong class="source-inline">name</strong>: A string value of the model filename</li>
				<li><strong class="source-inline">state</strong>: The value of <strong class="source-inline">ClientState</strong> (for example, <em class="italic">training</em> or <em class="italic">sending</em>) to set up a new client state:</li>
			</ul>
			<pre class="source-code">
def write_state(path: str, name: str, state: ClientState):
    fname = f'{path}/{name}'
    with open(fname, 'w') as f:
        f.write(str(int(state)))</pre>
			<h3>The get_ip function</h3>
			<p>The <strong class="source-inline">get_ip</strong> function <a id="_idIndexMarker959"/>obtains the IP address of the machine and returns the value of the IP address:</p>
			<pre class="source-code">
def get_ip() -&gt; str:
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        # doesn't even have to be reachable
        s.connect(('1.1.1.1', 1))
        ip = s.getsockname()[0]
    except:
        ip = '127.0.0.1'
    finally:
        s.close()
    return ip</pre>
			<h3>The init_loop function</h3>
			<p>The <strong class="source-inline">init_loop</strong> function is used <a id="_idIndexMarker960"/>to start a continuous loop function. It takes a function for running a loop function:</p>
			<pre class="source-code">
def init_loop(func):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(asyncio.gather(func))
    loop.run_forever()</pre>
			<p>In the next section, let’s look at the messenger functions to create communication payloads.</p>
			<h1 id="_idParaDest-279"><a id="_idTextAnchor306"/>Messengers to generate communication payloads</h1>
			<p>The messenger<a id="_idIndexMarker961"/> functions are defined in the <strong class="source-inline">messengers.py</strong> file, which can be found in the <strong class="source-inline">lib/util</strong> folder of the <strong class="source-inline">fl_main</strong> directory.</p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor307"/>Importing libraries for messengers</h2>
			<p>In this <strong class="source-inline">messengers.py</strong> code example, the file imports general libraries, such as <strong class="source-inline">time</strong> and <strong class="source-inline">numpy</strong>. It also imports <strong class="source-inline">ModelType</strong>, <strong class="source-inline">DBMsgType</strong>, <strong class="source-inline">AgentMsgType</strong>, and <strong class="source-inline">AggMsgType</strong>, which <a id="_idIndexMarker962"/>were defined in the <em class="italic">Enumeration classes for implementing the FL system</em> section in this chapter:</p>
			<pre class="source-code">
import time
import numpy as np
from typing import Dict, List, Any
from fl_main.lib.util.states import \
    ModelType, DBMsgType, AgentMsgType, AggMsgType</pre>
			<p>Next, let’s move on to the list of functions of the <strong class="source-inline">messengers</strong> library.</p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor308"/>Functions of messengers</h2>
			<p>The following is a list of functions related to the <strong class="source-inline">messengers</strong> library.</p>
			<h3>The generate_db_push_message function</h3>
			<p>The <strong class="source-inline">generate_db_push_message</strong> function generates and returns a message for pushing the<a id="_idIndexMarker963"/> message containing ML models to the database. It takes the following parameters to package them as a payload message (in a <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">push</strong>) between the aggregator and database:</p>
			<ul>
				<li><strong class="source-inline">component_id</strong>: A string value of the component ID, such as the aggregator ID</li>
				<li><strong class="source-inline">round</strong>: FL round information in an integer format</li>
				<li><strong class="source-inline">model_type</strong>: The type of ML model, such as <strong class="source-inline">cluster</strong> or <strong class="source-inline">local</strong> models</li>
				<li><strong class="source-inline">models</strong>: ML models with the <strong class="source-inline">Dict[str, np.array]</strong> format</li>
				<li><strong class="source-inline">model_id</strong>: A string value of the unique ID of the ML models </li>
				<li><strong class="source-inline">gene_time</strong>: A float value of the time at which the ML models are generated</li>
				<li><strong class="source-inline">performance_dict</strong>: Performance data with the <strong class="source-inline">Dict[str, float]</strong> format</li>
			</ul>
			<p>The following code <a id="_idIndexMarker964"/>provides the functionality of generating the preceding database push message:</p>
			<pre class="source-code">
def generate_db_push_message(
      component_id: str, round: int, model_type: ModelType,
      models: Dict[str,np.array], model_id: str,
      gene_time: float, performance_dict: Dict[str,float])
          -&gt; List[Any]:
    msg = list()
    msg.append(DBMsgType.push)  # 0
    msg.append(component_id)  # 1
    msg.append(round)  # 2
    msg.append(model_type)  # 3
    msg.append(models)  # 4
    msg.append(model_id)  # 5
    msg.append(gene_time)  # 6
    msg.append(performance_dict)  # 7
    return msg</pre>
			<h3>The generate_lmodel_update_message function</h3>
			<p>The <strong class="source-inline">generate_lmodel_update_message</strong> function generates and returns a message for<a id="_idIndexMarker965"/> sending the aggregator a message containing the local models created in an agent. It takes the following parameters to package them as a payload message (in <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">update</strong>) between the agent and aggregator:</p>
			<ul>
				<li><strong class="source-inline">agent_id</strong>: A string value of the agent ID</li>
				<li><strong class="source-inline">model_id</strong>: A string value of the unique ID of the ML models</li>
				<li><strong class="source-inline">local_models</strong>: Local ML models with the <strong class="source-inline">Dict[str, np.array]</strong> format</li>
				<li><strong class="source-inline">performance_dict</strong>: Performance data with the <strong class="source-inline">Dict[str, float]</strong> format</li>
			</ul>
			<p>The following <a id="_idIndexMarker966"/>code shows the functionality of generating the preceding local model update message:</p>
			<pre class="source-code">
def generate_lmodel_update_message(
        agent_id: str, model_id: str,
        local_models: Dict[str,np.array],
        performance_dict: Dict[str,float]) -&gt; List[Any]:
    msg = list()
    msg.append(AgentMsgType.update)  # 0
    msg.append(agent_id)  # 1
    msg.append(model_id)  # 2
    msg.append(local_models)  # 3
    msg.append(time.time())  # 4
    msg.append(performance_dict)  # 5
    return msg</pre>
			<h3>The generate_cluster_model_dist_message function</h3>
			<p>The <strong class="source-inline">generate_cluster_model_dist_message</strong> function generates and returns a message in <strong class="source-inline">List</strong> format to <a id="_idIndexMarker967"/>send a message containing the global models created by an aggregator to the connected agents. It takes the following parameters to package them as a payload message (in <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">update</strong>) between the aggregator and agent:</p>
			<ul>
				<li><strong class="source-inline">aggregator_id</strong>: A string value of the aggregator ID</li>
				<li><strong class="source-inline">model_id</strong>: A string value of the unique ID of the ML models</li>
				<li><strong class="source-inline">round</strong>: FL round information in an integer format</li>
				<li><strong class="source-inline">models</strong>: ML models with the <strong class="source-inline">Dict[str, np.array]</strong> format</li>
			</ul>
			<p>The following <a id="_idIndexMarker968"/>code shows the functionality of generating the preceding cluster model distribution message:</p>
			<pre class="source-code">
def generate_cluster_model_dist_message(
        aggregator_id: str, model_id: str, round: int,
        models: Dict[str,np.array]) -&gt; List[Any]:
    msg = list()
    msg.append(AggMsgType.update)  # 0
    msg.append(aggregator_id)  # 1
    msg.append(model_id)  # 2
    msg.append(round)  # 3
    msg.append(models)  # 4
    return msg</pre>
			<h3>The generate_agent_participation_message function</h3>
			<p>The <strong class="source-inline">generate_agent_participation_message</strong> function generates and returns a message to <a id="_idIndexMarker969"/>send a participation request message containing the initial models created by an agent to the connected aggregator. It takes the following parameters to package them as a payload message (in <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">participate</strong>) between the agent and aggregator:</p>
			<ul>
				<li><strong class="source-inline">agent_name</strong>: A string value of the agent name</li>
				<li><strong class="source-inline">agent_id</strong>: A string value of the agent ID</li>
				<li><strong class="source-inline">model_id</strong>: A string value of the unique ID of the ML models</li>
				<li><strong class="source-inline">models</strong>: ML models with the <strong class="source-inline">Dict[str, np.array]</strong> format</li>
				<li><strong class="source-inline">init_weights_flag</strong>: A Boolean value to indicate whether the weights are initialized or not</li>
				<li><strong class="source-inline">simulation_flag</strong>: A Boolean value to indicate whether the run is for a simulation or not</li>
				<li><strong class="source-inline">exch_socket</strong>: Socket information with a string value to send a message from an aggregator to this agent</li>
				<li><strong class="source-inline">gene_time</strong>: A float value of the time at which the ML models are generated</li>
				<li><strong class="source-inline">meta_dict</strong>: Performance data with the <strong class="source-inline">Dict[str, float]</strong> format</li>
				<li><strong class="source-inline">agent_ip</strong>: IP address of the agent itself</li>
			</ul>
			<p>The <a id="_idIndexMarker970"/>following code shows the functionality of generating the preceding agent participation message:</p>
			<pre class="source-code">
def generate_agent_participation_message(
       agent_name: str, agent_id: str, model_id: str,
       models: Dict[str,np.array], init_weights_flag: bool,
       simulation_flag: bool, exch_socket: str,
       gene_time: float, meta_dict: Dict[str,float],
       agent_ip: str) -&gt; List[Any]:
    msg = list()
    msg.append(AgentMsgType.participate)  # 0
    msg.append(agent_id)  # 1
    msg.append(model_id)  # 2
    msg.append(models)  # 3
    msg.append(init_weights_flag)  # 4
    msg.append(simulation_flag)  # 5
    msg.append(exch_socket)  # 6
    msg.append(gene_time)  # 7
    msg.append(meta_dict)  # 8
    msg.append(agent_ip)  # 9
    msg.append(agent_name)  # 9
    return msg</pre>
			<h3>The generate_agent_participation_confirm_message function</h3>
			<p>The <strong class="source-inline">generate_agent_participation_confirm_message</strong> function generates and <a id="_idIndexMarker971"/>returns a message to send a participation confirmation message containing the global models back to the agent. It takes the following parameters to package them as a payload message (in <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">welcome</strong>) between the aggregator and agent:</p>
			<ul>
				<li><strong class="source-inline">aggregator_id</strong>: A string value of the aggregator ID</li>
				<li><strong class="source-inline">model_id</strong>: A string value of the unique ID of the ML models</li>
				<li><strong class="source-inline">models</strong>: ML models with the <strong class="source-inline">Dict[str, np.array]</strong> format</li>
				<li><strong class="source-inline">round</strong>: FL round information in an integer format</li>
				<li><strong class="source-inline">agent_id</strong>: A string value of the agent ID</li>
				<li><strong class="source-inline">exch_socket</strong>: A port number to reach out to an agent from the aggregator</li>
				<li><strong class="source-inline">recv_socket</strong>: A port number to receive messages from the agent</li>
			</ul>
			<p>The following code shows the functionality of generating the preceding agent participation confirmation message:</p>
			<pre class="source-code">
def generate_agent_participation_confirm_message(
        aggregator_id: str, model_id: str,
        models: Dict[str,np.array], round: int,
        agent_id: str, exch_socket: str, recv_socket: str)
            -&gt; List[Any]:
    msg = list()
    msg.append(AggMsgType.welcome)  # 0
    msg.append(aggregator_id)  # 1
    msg.append(model_id)  # 2
    msg.append(models)  # 3
    msg.append(round)  # 4
    msg.append(agent_id) # 5
    msg.append(exch_socket)  # 6
    msg.append(recv_socket)  # 7
    return msg</pre>
			<h3>The generate_polling_message function</h3>
			<p>The <strong class="source-inline">generate_polling_message</strong> function <a id="_idIndexMarker972"/>generates and returns a message to send a <strong class="source-inline">polling</strong> message containing the polling signal to the aggregator. It takes the following parameters to package them as a payload message (in <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">polling</strong>) between the agent and aggregator:</p>
			<ul>
				<li><strong class="source-inline">round</strong>: FL round information in an integer format</li>
				<li><strong class="source-inline">agent_id</strong>: A string value of the agent ID</li>
			</ul>
			<p>The following code shows the functionality of generating the preceding polling message:</p>
			<pre class="source-code">
def generate_polling_message(round: int, agent_id: str):
    msg = list()
    msg.append(AgentMsgType.polling) # 0
    msg.append(round) # 1
    msg.append(agent_id) # 2
    return msg</pre>
			<h3>The generate_ack_message function</h3>
			<p>The <strong class="source-inline">generate_ack_message</strong> function <a id="_idIndexMarker973"/>generates and returns a message to send an <strong class="source-inline">ack</strong> message containing the acknowledgment signal back to an agent. No parameter is required to create a payload message (in <strong class="source-inline">List</strong> format with the message type defined as <strong class="source-inline">ack</strong>) between the aggregator and agent:</p>
			<pre class="source-code">
def generate_ack_message():
    msg = list()
    msg.append(AggMsgType.ack) # 0
    return msg</pre>
			<h1 id="_idParaDest-282"><a id="_idTextAnchor309"/>Summary</h1>
			<p>In this chapter, we have explained the internal libraries in detail so that you can implement the entire FL system without further investigating what and how to code for basic functionalities such as communication and data structure conversion frameworks.</p>
			<p>There are mainly five aspects that the internal library covers: <em class="italic">enumeration classes</em>, defining the system states, such as FL client states; the <em class="italic">communication handler</em>, supporting send and receive functionalities; the <em class="italic">data structure</em>, to handle ML models when aggregation happens; <em class="italic">helper and support functions</em>, which cope with basic operations, such as saving data and producing randomized IDs; and <em class="italic">messenger functions</em>, to generate various payloads sent among the database, aggregator, and agents.</p>
			<p>With these functions, you will find the implementation of FL systems easy and smooth, but these libraries only support achieving some minimal functionality of the FL system; hence, it is up to you to further enhance the FL system to create a more authentic platform that can be used in real-life use cases and technologies.</p>
		</div>
	</body></html>