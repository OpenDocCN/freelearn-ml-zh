<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Unsupervised Machine Learning">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch02"/>&#13;
 Chapter 2. Unsupervised Machine Learning</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>As we have seen in the <a class="link" title="Chapter 1. Introduction to Practical Machine Learning Using Python" href="text00015.html#ch01">Chapter 1</a>&#13;
 , <span class="emphasis">&#13;
<em>Introduction to Practical Machine Learning Using Python</em>&#13;
</span>&#13;
 , unsupervised learning is designed to provide insightful information on data unlabeled date. In many cases, a large dataset (both in terms of number of points and number of features) is unstructured and does not present any information at first sight, so these techniques are used to highlight hidden structures on data (clustering) or to reduce its complexity without losing relevant information (dimensionality reduction). This chapter will focus on the main clustering algorithms (the first part of the chapter) and dimensionality reduction methods (the second part of the chapter). The differences and advantages of the methods will be highlighted by providing a practical example using Python libraries. All of the code will be available on the author's GitHub profile, in the <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/</a>&#13;
 folder. We will now start describing clustering algorithms.</p>&#13;
<div class="section" title="Clustering algorithms">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch02lvl1sec13"/>&#13;
 Clustering algorithms</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Clustering algorithms <a id="id134" class="indexterm"/>&#13;
 are employed to restructure data in somehow ordered subsets so that a meaningful structure can be inferred. A cluster can be defined as a group of data points with some similar features. The way to quantify the similarity of data points is what determines the different categories of clustering.</p>&#13;
<p>Clustering algorithms can be divided into different categories based on different metrics or assumptions in which data has been manipulated. We are going to discuss the most relevant categories used nowadays, which are distribution methods, centroid methods, density methods, and hierarchical methods. For each category, a particular algorithm is going to be presented in detail, and we will begin by discussing distribution methods. An example to compare the different algorithms will be discussed, and both the IPython notebook and script are available in the my GitHub book folder at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/</a>&#13;
 .</p>&#13;
<div class="section" title="Distribution methods">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch02lvl2sec11"/>&#13;
 Distribution methods</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>These methods<a id="id135" class="indexterm"/>&#13;
 assume that the data comes from a <a id="id136" class="indexterm"/>&#13;
 certain distribution, and the expectation maximization algorithm is used to find the optimal values for the distribution parameters. Expectation maximization and the mixture of <a id="id137" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>Gaussian</strong>&#13;
</span>&#13;
 clustering are discussed hereafter.</p>&#13;
<div class="section" title="Expectation maximization">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch02lvl3sec09"/>&#13;
 Expectation maximization</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This algorithm is <a id="id138" class="indexterm"/>&#13;
 used to find the maximum likelihood<a id="id139" class="indexterm"/>&#13;
 estimates of parameter distribution models that depend on hidden (unobserved) variables. Expectation maximization consists <a id="id140" class="indexterm"/>&#13;
 of iterating two steps: the <span class="strong">&#13;
<strong>E-step</strong>&#13;
</span>&#13;
 , which creates<a id="id141" class="indexterm"/>&#13;
 the <span class="strong">&#13;
<strong>log-likelihood</strong>&#13;
</span>&#13;
 function evaluated using the current values of the parameters, and the <span class="strong">&#13;
<strong>M-step</strong>&#13;
</span>&#13;
 , where<a id="id142" class="indexterm"/>&#13;
 the new parameters' values are computed, maximizing the log-likelihood of the E-step.</p>&#13;
<p>Consider a set of <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 elements, <span class="emphasis">&#13;
<em>{x<sup>(i)</sup>&#13;
 }i = 1,…,N</em>&#13;
</span>&#13;
 , and a log-likelihood on the data as follows:</p>&#13;
<div class="mediaobject"><img src="Image00046.jpg" alt="Expectation maximization"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>θ</em>&#13;
</span>&#13;
 represents the set of parameters and <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 are the so-called hidden variables.</p>&#13;
<p>We want to find the values of the parameters that maximize the log-likelihood without knowing the values of the <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 (unobserved variables). Consider a distribution over the <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>Q(z<sup>(i)</sup>&#13;
 )</em>&#13;
</span>&#13;
 , such as <span class="inlinemediaobject"><img src="Image00047.jpg" alt="Expectation maximization"/>&#13;
</span>&#13;
 . Therefore:</p>&#13;
<div class="mediaobject"><img src="Image00048.jpg" alt="Expectation maximization"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This means <span class="emphasis">&#13;
<em>Q(z<sup>(i)</sup>&#13;
 )</em>&#13;
</span>&#13;
 is the posterior distribution of the hidden variable, <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 , given <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 parameterized by <span class="emphasis">&#13;
<em>θ</em>&#13;
</span>&#13;
 . The expectation maximization algorithm comes from the use of Jensen's inequality and it ensures that carrying out these two steps:</p>&#13;
<div class="orderedlist">&#13;
<ol class="orderedlist arabic">&#13;
<li class="listitem" value="1">&#13;
<span class="inlinemediaobject"><img src="Image00049.jpg" alt="Expectation maximization"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem" value="2">&#13;
<span class="inlinemediaobject"><img src="Image00050.jpg" alt="Expectation maximization"/>&#13;
</span>&#13;
</li>&#13;
</ol>&#13;
<div style="height:10px; width: 1px"/>&#13;
</div>&#13;
<p>The <a id="id143" class="indexterm"/>&#13;
 log-likelihood converges to the maximum, and <a id="id144" class="indexterm"/>&#13;
 so the associated <span class="emphasis">&#13;
<em>θ</em>&#13;
</span>&#13;
 values are found.</p>&#13;
</div>&#13;
<div class="section" title="Mixture of Gaussians">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch02lvl3sec10"/>&#13;
 Mixture of Gaussians</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This method <a id="id145" class="indexterm"/>&#13;
 models the entire dataset using a <a id="id146" class="indexterm"/>&#13;
 mixture of Gaussian distributions. Therefore, the number of clusters will be given as the number of Gaussians considered in the mixture. Given a dataset of N elements, <span class="emphasis">&#13;
<em>{x<sup>(i)</sup>&#13;
 }i = 1,…,N</em>&#13;
</span>&#13;
 , where each <span class="inlinemediaobject"><img src="Image00051.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 is a vector of d-features modeled by a mixture of Gaussian such as the following:</p>&#13;
<div class="mediaobject"><img src="Image00052.jpg" alt="Mixture of Gaussians"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Where:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00053.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 is a hidden variable that represents the Gaussian component each <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 is generated from</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00054.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 represents the set of mean parameters of the Gaussian components</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00055.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 represents the set of variance parameters of the Gaussian components</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00056.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 is the mixture weight, representing the probability that a randomly selected <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 was generated by the Gaussian component <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00057.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 , and <span class="inlinemediaobject"><img src="Image00058.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 is the set of weights</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00059.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 is the Gaussian component <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 with<a id="id147" class="indexterm"/>&#13;
 parameters <span class="inlinemediaobject"><img src="Image00060.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 associated <a id="id148" class="indexterm"/>&#13;
 with the point <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>The parameters of our model are thus φ, µ and ∑. To estimate them, we can write down the log-likelihood of the dataset:</p>&#13;
<div class="mediaobject"><img src="Image00061.jpg" alt="Mixture of Gaussians"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In order to find the values of the parameters, we apply the expectation maximization algorithm explained in the previous section where <span class="inlinemediaobject"><img src="Image00062.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00063.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 .</p>&#13;
<p>After choosing a first guess of the parameters, we iterate the following steps until convergence:</p>&#13;
<div class="orderedlist">&#13;
<ol class="orderedlist arabic">&#13;
<li class="listitem" value="1">&#13;
<span class="strong">&#13;
<strong>E- step</strong>&#13;
</span>&#13;
 : The weights <span class="inlinemediaobject"><img src="Image00064.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 are updated by<a id="id149" class="indexterm"/>&#13;
 following the rule obtained <a id="id150" class="indexterm"/>&#13;
 by applying Bayes' theorem:<div class="mediaobject"><img src="Image00065.jpg" alt="Mixture of Gaussians"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
<li class="listitem" value="2">&#13;
<span class="strong">&#13;
<strong>M-step</strong>&#13;
</span>&#13;
 : The parameters are updated to the following (these formulas come from solving the maximization problem, which means setting the derivatives of the log-likelihood to zero):<div class="mediaobject"><img src="Image00066.jpg" alt="Mixture of Gaussians"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00067.jpg" alt="Mixture of Gaussians"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<div class="mediaobject"><img src="Image00068.jpg" alt="Mixture of Gaussians"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
</ol>&#13;
<div style="height:10px; width: 1px"/>&#13;
</div>&#13;
<p>Note that the <a id="id151" class="indexterm"/>&#13;
 expectation maximization<a id="id152" class="indexterm"/>&#13;
 algorithm is needed because the hidden variables <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 are unknown. Otherwise, it would have been a supervised learning problem, where <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 is the label of each point of the training set (and the supervised algorithm used would be the Gaussian discriminant analysis). Therefore, this is an unsupervised algorithm and the goal is also to find <span class="emphasis">&#13;
<em>z<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 , that is, which of the <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 Gaussian components each point <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 is associated with. In fact, by calculating the posterior probability <span class="inlinemediaobject"><img src="Image00069.jpg" alt="Mixture of Gaussians"/>&#13;
</span>&#13;
 for each of the <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 classes, it is possible to assign each <span class="emphasis">&#13;
<em>x(i)</em>&#13;
</span>&#13;
 to the class <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 with the highest posterior probability. There are several cases in which this algorithm can be successfully used to cluster (label) the data.</p>&#13;
<p>A possible practical example is the case of a professor with student grades for two different classes but not labeled per class. He wants to split the grades into the original two classes assuming that the distribution of grades in each class is Gaussian. Another example solvable with the mixture of the Gaussian approach is to determine the country of each person based on a set of people's height values coming from two different countries and assuming that the distribution of height in each country follows Gaussian distribution.</p>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Centroid methods">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch02lvl2sec12"/>&#13;
 Centroid methods</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This class collects all the techniques that find the centers of the clusters, assigning the data points <a id="id153" class="indexterm"/>&#13;
 to the nearest cluster center and minimizing the<a id="id154" class="indexterm"/>&#13;
 distances between the centers and the assigned points. This is an optimization problem and the final centers are vectors; they may not be the points in the original dataset. The number of clusters is a parameter to be specified a priori and the generated clusters tend to have similar sizes so that the border lines are not necessarily well defined. This optimization problem may lead to a local optimal solution, which means that different initialization values can result in slightly different clusters. The <a id="id155" class="indexterm"/>&#13;
 most common method is known as <span class="strong">&#13;
<strong>k-means</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>Lloyd's algorithm</strong>&#13;
</span>&#13;
 ), in which the distance minimized is the <a id="id156" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>Euclidean norm</strong>&#13;
</span>&#13;
 . Other techniques find the centers <a id="id157" class="indexterm"/>&#13;
 as the medians of the clusters (<span class="strong">&#13;
<strong>k-medians clustering</strong>&#13;
</span>&#13;
 ) or impose the center's values<a id="id158" class="indexterm"/>&#13;
 to be the actual data points. Furthermore, other <a id="id159" class="indexterm"/>&#13;
 variations of these methods <a id="id160" class="indexterm"/>&#13;
 differ <a id="id161" class="indexterm"/>&#13;
 in the choice that the initial centers are defined (<span class="strong">&#13;
<strong>k-means++</strong>&#13;
</span>&#13;
 or <span class="strong">&#13;
<strong>fuzzy c-means</strong>&#13;
</span>&#13;
 ).</p>&#13;
<div class="section" title="k-means">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch02lvl3sec11"/>&#13;
 k-means</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This algorithm tries<a id="id162" class="indexterm"/>&#13;
 to find the center of each cluster as the mean of all the<a id="id163" class="indexterm"/>&#13;
 members that minimize the distance between the center and the assigned points themselves. It can be associated with the k-nearest-neighbor algorithm in classification problems, and the resulting set of clusters can be represented as<a id="id164" class="indexterm"/>&#13;
 a <span class="strong">&#13;
<strong>Voronoi diagram</strong>&#13;
</span>&#13;
 (a method of partitioning the space in regions based on the distance from a set of points, in this case, the clusters' centers). Consider the usual dataset, <span class="inlinemediaobject"><img src="Image00070.jpg" alt="k-means"/>&#13;
</span>&#13;
 . The algorithm prescribes to choose a number of centers <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 , assign the initial mean cluster centers <span class="inlinemediaobject"><img src="Image00071.jpg" alt="k-means"/>&#13;
</span>&#13;
 to random values, and then iterate the following steps until convergence:</p>&#13;
<div class="orderedlist">&#13;
<ol class="orderedlist arabic">&#13;
<li class="listitem" value="1">For each data point <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 , calculate the Euclidean square distances between each point <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 and each center <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 and find the center index <span class="emphasis">&#13;
<em>d<sub>i</sub>&#13;
</em>&#13;
</span>&#13;
 , which corresponds to the minimum of these distances: <span class="inlinemediaobject"><img src="Image00072.jpg" alt="k-means"/>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem" value="2">For each center <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 , recalculate its mean value as the mean of the points that have <span class="emphasis">&#13;
<em>d_i j</em>&#13;
</span>&#13;
 equal to <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 (that is, points belonging to the cluster with mean <span class="inlinemediaobject"><img src="Image00073.jpg" alt="k-means"/>&#13;
</span>&#13;
 ):<div class="mediaobject"><img src="Image00074.jpg" alt="k-means"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
It is possible to show that this algorithm converges with respect to the function given by the following function:<div class="mediaobject"><img src="Image00075.jpg" alt="k-means"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
</ol>&#13;
<div style="height:10px; width: 1px"/>&#13;
</div>&#13;
<p>It decreases<a id="id165" class="indexterm"/>&#13;
 monotonically with the number of iterations. Since <span class="emphasis">&#13;
<em>F</em>&#13;
</span>&#13;
 is a <a id="id166" class="indexterm"/>&#13;
 nonconvex function, it is not guaranteed that the final minimum will be the global minimum. In order to avoid the problem of a clustering result associated with the local minima, the k-means algorithm is usually run multiple times with different random initial center's means. Then the result associated with the lower <span class="emphasis">&#13;
<em>F</em>&#13;
</span>&#13;
 value is chosen as the optimal clustering solution.</p>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Density methods">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch02lvl2sec13"/>&#13;
 Density methods</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>These<a id="id167" class="indexterm"/>&#13;
 methods are based on the idea that sparse areas <a id="id168" class="indexterm"/>&#13;
 have to be considered borders (or noise) and high-density zones should be related to the cluster's centers. The common method is called <span class="strong">&#13;
<strong>density-based spatial clustering of applications with noise</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>DBSCAN</strong>&#13;
</span>&#13;
 ), which defines the <a id="id169" class="indexterm"/>&#13;
 connection between two points through a certain distance threshold (for this reason, it is similar to hierarchical algorithms; see <a class="link" title="Chapter 3. Supervised Machine Learning" href="text00024.html#page">Chapter 3</a>&#13;
 , <span class="emphasis">&#13;
<em>Supervised Machine Learning</em>&#13;
</span>&#13;
 ). Two points are considered linked (belonging to the same cluster) only if a certain density criterion is satisfied—the number of neighboring points has to be higher than a threshold value within a certain radius. Another popular method is mean-shift, in which each data point is assigned to the cluster that has the highest density in its neighborhood. Due to the time-consuming calculations of the density through a kernel density estimation, mean-shift is usually slower than DBSCAN or centroid methods. The main advantages of this class of algorithms are the ability to define clusters with arbitrary shapes and the ability to determine the best number of clusters instead of setting this number a priori as a parameter, making these methods suitable to cluster datasets in which it is not known.</p>&#13;
<div class="section" title="Mean – shift">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch02lvl3sec12"/>&#13;
 Mean – shift</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Mean-shift is <a id="id170" class="indexterm"/>&#13;
 nonparametric algorithm that finds the positions <a id="id171" class="indexterm"/>&#13;
 of the local maxima in a density kernel function defined on a dataset. The local maxima found can be considered the centers of clusters in a dataset <span class="inlinemediaobject"><img src="Image00076.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 , and the number of maxima is the number of clusters. In order to be applied as a clustering algorithm, each point <span class="inlinemediaobject"><img src="Image00077.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 has to be associated with the density of its neighborhood:</p>&#13;
<div class="mediaobject"><img src="Image00078.jpg" alt="Mean – shift"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 is the <a id="id172" class="indexterm"/>&#13;
 so-called bandwidth; it estimates the radius of the<a id="id173" class="indexterm"/>&#13;
 neighborhood in which the points affect the density value <span class="emphasis">&#13;
<em>f(x<sup>(l)</sup>&#13;
 )</em>&#13;
</span>&#13;
 (that is, the other points have negligible effect on <span class="inlinemediaobject"><img src="Image00079.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 ). <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 is the kernel function that satisfies these conditions:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00080.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00081.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>Typical examples of <span class="emphasis">&#13;
<em>K(x<sup>(i)</sup>&#13;
 )</em>&#13;
</span>&#13;
 are the following functions:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00082.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 : Gaussian kernel</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00083.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 : Epanechnikov kernel</li>&#13;
</ul>&#13;
</div>&#13;
<p>The mean-shift algorithm imposes the maximization of <span class="emphasis">&#13;
<em>f(x<sup>(l)</sup>&#13;
 )</em>&#13;
</span>&#13;
 , which translates into the mathematical equation (remember that in function analysis, the maximum is found by imposing the derivative to <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 ):</p>&#13;
<div class="mediaobject"><img src="Image00084.jpg" alt="Mean – shift"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>K'</em>&#13;
</span>&#13;
 is derivative of the kernel density function <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 .</p>&#13;
<p>Therefore, to find the local maxima position associated with the feature vector <span class="emphasis">&#13;
<em>x<sup>(l)</sup>&#13;
</em>&#13;
</span>&#13;
 , the following iterative equation can be computed:</p>&#13;
<div class="mediaobject"><img src="Image00085.jpg" alt="Mean – shift"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="inlinemediaobject"><img src="Image00086.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 is called the mean-shift vector. The algorithm will converge when at iteration <span class="emphasis">&#13;
<em>t=a</em>&#13;
</span>&#13;
 , the condition <span class="inlinemediaobject"><img src="Image00087.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 is satisfied.</p>&#13;
<p>Supported by<a id="id174" class="indexterm"/>&#13;
 the equation, we can now explain the algorithm with<a id="id175" class="indexterm"/>&#13;
 the help of the following figure. At the first iteration <span class="emphasis">&#13;
<em>t=0</em>&#13;
</span>&#13;
 , the original points <span class="inlinemediaobject"><img src="Image00088.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 (red) are spread on the data space, the mean shift vector <span class="inlinemediaobject"><img src="Image00089.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 is calculated, and the same points are marked with a cross (<span class="emphasis">&#13;
<em>x</em>&#13;
</span>&#13;
 ) to track their evolution with the algorithm. At iteration <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 , the dataset will be obtained using the aforementioned equation, and the resulting points <span class="inlinemediaobject"><img src="Image00090.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 are shown in the following figure with the (<span class="emphasis">&#13;
<em>+</em>&#13;
</span>&#13;
 ) symbol:</p>&#13;
<div class="mediaobject"><img src="Image00091.jpg" alt="Mean – shift"/>&#13;
<div class="caption">&#13;
<p>Sketch of the mean-shift evolution through iterations</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the <a id="id176" class="indexterm"/>&#13;
 preceding figure, at iteration <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 the original points are <a id="id177" class="indexterm"/>&#13;
 shown in red (cross), at iteration <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 the sample points (symbols <span class="emphasis">&#13;
<em>+</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>*</em>&#13;
</span>&#13;
 respectively) move towards the local density maxima indicated by blue squares.</p>&#13;
<p>Again, at iteration <span class="emphasis">&#13;
<em>K</em>&#13;
</span>&#13;
 , the new data points <span class="inlinemediaobject"><img src="Image00092.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 are computed and they are shown with the <span class="emphasis">&#13;
<em>*</em>&#13;
</span>&#13;
 symbol in the preceding figure. The density function values <span class="inlinemediaobject"><img src="Image00093.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 associated with <span class="inlinemediaobject"><img src="Image00094.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 are larger than the values in the previous iterations since the algorithm aims to maximize them. The original dataset is now clearly associated with points <span class="inlinemediaobject"><img src="Image00092.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 , and they converge to the locations plotted in blue squares in the preceding<a id="id178" class="indexterm"/>&#13;
 figure. The feature vectors <span class="inlinemediaobject"><img src="Image00088.jpg" alt="Mean – shift"/>&#13;
</span>&#13;
 are<a id="id179" class="indexterm"/>&#13;
 now collapsing to two different local maxima, which represent the centers of the two clusters.</p>&#13;
<p>In order to properly use the method, some considerations are necessary.</p>&#13;
<p>The only parameter required, the bandwidth <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 , needs to be tuned cleverly to achieve good results. In fact, too low value of <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 may result in a large number of clusters, while a large value of <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 may merge multiple distinct clusters. Note also that if the number <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 of feature vector dimensions is large, the mean-shift method may lead to poor results. This is because in a very-high-dimensional space, the number of local maxima is accordingly large and the iterative equation can easily converge too soon.</p>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Hierarchical methods">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch02lvl2sec14"/>&#13;
 Hierarchical methods</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The class <a id="id180" class="indexterm"/>&#13;
 of hierarchical methods, also called <a id="id181" class="indexterm"/>&#13;
 connectivity-based clustering, forms clusters by collecting elements on a similarity criteria based on a distance metric: close elements gather in the same partition while far elements are separated into different clusters. This <a id="id182" class="indexterm"/>&#13;
 category of algorithms is divided in two types: <span class="strong">&#13;
<strong>divisive clustering</strong>&#13;
</span>&#13;
 and <span class="strong">&#13;
<strong>agglomerative clustering</strong>&#13;
</span>&#13;
 . The divisive approach starts by assigning the<a id="id183" class="indexterm"/>&#13;
 entire dataset to a cluster, which is then divided in two less similar (distant) clusters. Each partition is further divided until each data point is itself a cluster. The agglomerative method, which is the most often employed method, starts from the data points, each of them representing a cluster. Then these clusters are merged by similarity until a single cluster containing all the data points remains. These methods are called <span class="strong">&#13;
<strong>hierarchical</strong>&#13;
</span>&#13;
 because both categories create a hierarchy of clusters iteratively, as the following figure shows. This hierarchical representation is <a id="id184" class="indexterm"/>&#13;
 called a <span class="strong">&#13;
<strong>dendrogram</strong>&#13;
</span>&#13;
 . On the horizontal axis, there are the elements of the dataset, and on the vertical axis, the distance values are plotted. Each horizontal line represents a cluster and the vertical axis indicates which element/cluster forms the related cluster:</p>&#13;
<div class="mediaobject"><img src="Image00095.jpg" alt="Hierarchical methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the preceding figure, agglomerative clustering starts from many clusters as dataset points and<a id="id185" class="indexterm"/>&#13;
 ends up with a single cluster that contains the <a id="id186" class="indexterm"/>&#13;
 entire dataset. Vice versa, the divisive method starts from a single cluster and finishes when all clusters contain a single data point each.</p>&#13;
<p>The final clusters are then formed by applying criteria to stop the agglomeration/division strategy. The distance criteria sets the maximum distance above which two clusters are too far away to be merged, and the <span class="emphasis">&#13;
<em>number of clusters</em>&#13;
</span>&#13;
 criteria sets the maximum number of clusters to stop the hierarchy from continuing to merge or split the partitions.</p>&#13;
<p>An example of agglomeration is given by the following algorithm:</p>&#13;
<div class="orderedlist">&#13;
<ol class="orderedlist arabic">&#13;
<li class="listitem" value="1">Assign each element <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 of the dataset <span class="inlinemediaobject"><img src="Image00096.jpg" alt="Hierarchical methods"/>&#13;
</span>&#13;
 to a different cluster.</li>&#13;
<li class="listitem" value="2">Calculate the distances between each pair of clusters and merge the closest pair into a single cluster, reducing the total number of clusters by <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem" value="3">Calculate the distances of the new cluster from the others.</li>&#13;
<li class="listitem" value="4">Repeat steps 2 and 3 until only a single cluster remains with all <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 elements.</li>&#13;
</ol>&#13;
<div style="height:10px; width: 1px"/>&#13;
</div>&#13;
<p>Since the distance <span class="emphasis">&#13;
<em>d(C1,C2)</em>&#13;
</span>&#13;
 between two clusters <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 , is computed by definition between two points <span class="inlinemediaobject"><img src="Image00097.jpg" alt="Hierarchical methods"/>&#13;
</span>&#13;
 and each cluster contains multiple points, a criteria to<a id="id187" class="indexterm"/>&#13;
 decide which elements have to be <a id="id188" class="indexterm"/>&#13;
 considered to calculate the distance is necessary (linkage criteria). The common linkage criteria of two clusters <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 are as follows:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Single linkage</strong>&#13;
</span>&#13;
 : The <a id="id189" class="indexterm"/>&#13;
 minimum distance among the distances between any element of <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 and any element of <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 is given by the following:<div class="mediaobject"><img src="Image00098.jpg" alt="Hierarchical methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Complete linkage</strong>&#13;
</span>&#13;
 : The <a id="id190" class="indexterm"/>&#13;
 maximum distance among the distances between any element of <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 and any element of <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 is given by the following:<div class="mediaobject"><img src="Image00099.jpg" alt="Hierarchical methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Unweighted pair group method with arithmetic mean (UPGMA) or average linkage</strong>&#13;
</span>&#13;
 : The <a id="id191" class="indexterm"/>&#13;
 average <a id="id192" class="indexterm"/>&#13;
 distance among the distances between any element of <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 and any element of <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 is <span class="inlinemediaobject"><img src="Image00100.jpg" alt="Hierarchical methods"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00101.jpg" alt="Hierarchical methods"/>&#13;
</span>&#13;
 are the numbers of elements of <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 , respectively.</li>&#13;
<li class="listitem">&#13;
<span class="strong">&#13;
<strong>Ward algorithm</strong>&#13;
</span>&#13;
 : This merges partitions that do not increase a certain measure <a id="id193" class="indexterm"/>&#13;
 of heterogeneity. It aims to join two clusters <span class="emphasis">&#13;
<em>C1</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>C2</em>&#13;
</span>&#13;
 that have the minimum increase of a variation measure, called the merging cost <span class="inlinemediaobject"><img src="Image00102.jpg" alt="Hierarchical methods"/>&#13;
</span>&#13;
 , due to their combination. The distance in this case is replaced by the merging cost, which is given by the following formula:<div class="mediaobject"><img src="Image00103.jpg" alt="Hierarchical methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
Here, <span class="inlinemediaobject"><img src="Image00101.jpg" alt="Hierarchical methods"/>&#13;
</span>&#13;
 are the numbers of elements of C1 and C2, respectively.</li>&#13;
</ul>&#13;
</div>&#13;
<p>There are <a id="id194" class="indexterm"/>&#13;
 different metrics <span class="emphasis">&#13;
<em>d(c1,c2)</em>&#13;
</span>&#13;
 that can be<a id="id195" class="indexterm"/>&#13;
 chosen to implement a hierarchical algorithm. The most common is the Euclidean distance:</p>&#13;
<div class="mediaobject"><img src="Image00104.jpg" alt="Hierarchical methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that this class of method is not particularly time-efficient, so it is not suitable for clustering large datasets. It is also not very robust towards erroneously clustered data points (outliers), which may lead to incorrect merging of clusters.</p>&#13;
<div class="section" title="Training and comparison of the clustering methods">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch02lvl3sec13"/>&#13;
 Training and comparison of the clustering methods</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>To compare the <a id="id196" class="indexterm"/>&#13;
 clustering methods just presented, we need to generate <a id="id197" class="indexterm"/>&#13;
 a dataset. We choose the two dataset classes given by the two two-dimensional multivariate normal distributions with means and covariance equal to <span class="inlinemediaobject"><img src="Image00105.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
 and <span class="inlinemediaobject"><img src="Image00106.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
 , respectively.</p>&#13;
<p>The data points are generated using the NumPy library and plotted with matplotlib:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from matplotlib import pyplot as plt</strong>&#13;

</span>&#13;


<span class="strong">
<strong>import numpy as np</strong>&#13;

</span>&#13;


<span class="strong">
<strong>np.random.seed(4711)  # for repeatability </strong>&#13;

</span>&#13;


<span class="strong">
<strong>c1 = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>l1 = np.zeros(100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>l2 = np.ones(100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>c2 = np.random.multivariate_normal([0, 10], [[3, 1], [1, 4]], size=[100,])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>#add noise:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>np.random.seed(1)  # for repeatability </strong>&#13;

</span>&#13;


<span class="strong">
<strong>noise1x = np.random.normal(0,2,100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>noise1y = np.random.normal(0,8,100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>noise2 = np.random.normal(0,8,100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>c1[:,0] += noise1x</strong>&#13;

</span>&#13;


<span class="strong">
<strong>c1[:,1] += noise1y</strong>&#13;

</span>&#13;


<span class="strong">
<strong>c2[:,1] += noise2</strong>&#13;

</span>&#13;



<span class="strong">
<strong>fig = plt.figure(figsize=(20,15))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax = fig.add_subplot(111)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax.set_xlabel('x',fontsize=30)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax.set_ylabel('y',fontsize=30)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.suptitle('classes',fontsize=30)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>labels = np.concatenate((l1,l2),)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>X = np.concatenate((c1, c2),)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pp1= ax.scatter(c1[:,0], c1[:,1],cmap='prism',s=50,color='r')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pp2= ax.scatter(c2[:,0], c2[:,1],cmap='prism',s=50,color='g')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax.legend((pp1,pp2),('class 1', 'class2'),fontsize=35)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.savefig('classes.png')</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>A normally <a id="id198" class="indexterm"/>&#13;
 distributed noise has been added to both classes to<a id="id199" class="indexterm"/>&#13;
 make the example more realistic. The result is shown in the following figure:</p>&#13;
<div class="mediaobject"><img src="Image00107.jpg" alt="Training and comparison of the clustering methods"/>&#13;
<div class="caption">&#13;
<p>Two multivariate normal classes with noise</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The <a id="id200" class="indexterm"/>&#13;
 clustering methods have been implemented using <a id="id201" class="indexterm"/>&#13;
 the <code class="literal">sklearn</code>&#13;
 and <code class="literal">scipy</code>&#13;
 libraries and again plotted with matplotlib:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>import numpy as np</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from sklearn import mixture</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scipy.cluster.hierarchy import linkage</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scipy.cluster.hierarchy import fcluster</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from sklearn.cluster import KMeans</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from sklearn.cluster import MeanShift</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from matplotlib import pyplot as plt</strong>&#13;

</span>&#13;



<span class="strong">
<strong>fig.clf()#reset plt</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig, ((axis1, axis2), (axis3, axis4)) = plt.subplots(2, 2, sharex='col', sharey='row')</strong>&#13;

</span>&#13;



<span class="strong">
<strong>#k-means</strong>&#13;

</span>&#13;


<span class="strong">
<strong>kmeans = KMeans(n_clusters=2)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>kmeans.fit(X)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pred_kmeans = kmeans.labels_</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='prism')  # plot points with cluster dependent colors</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis1.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='prism')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis1.set_ylabel('y',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis1.set_title('k-means',fontsize=20)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>#mean-shift</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ms = MeanShift(bandwidth=7)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ms.fit(X)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pred_ms = ms.labels_</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis2.scatter(X[:,0], X[:,1], c=pred_ms, cmap='prism')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis2.set_title('mean-shift',fontsize=20)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>#gaussian mixture</strong>&#13;

</span>&#13;


<span class="strong">
<strong>g = mixture.GMM(n_components=2)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>g.fit(X) </strong>&#13;

</span>&#13;


<span class="strong">
<strong>pred_gmm = g.predict(X)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis3.scatter(X[:,0], X[:,1], c=pred_gmm, cmap='prism')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis3.set_xlabel('x',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis3.set_ylabel('y',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis3.set_title('gaussian mixture',fontsize=20)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>#hierarchical</strong>&#13;

</span>&#13;


<span class="strong">
<strong># generate the linkage matrix</strong>&#13;

</span>&#13;


<span class="strong">
<strong>Z = linkage(X, 'ward')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>max_d = 110</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pred_h = fcluster(Z, max_d, criterion='distance')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis4.scatter(X[:,0], X[:,1], c=pred_h, cmap='prism')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis4.set_xlabel('x',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>axis4.set_title('hierarchical ward',fontsize=20)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.set_size_inches(18.5,10.5)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.savefig('comp_clustering.png', dpi=100)</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The k-means<a id="id202" class="indexterm"/>&#13;
 function and Gaussian mixture model have a<a id="id203" class="indexterm"/>&#13;
 specified number of clusters (<code class="literal">n_clusters =2,n_components=2</code>&#13;
 ), while the mean-shift algorithm has the bandwidth value <code class="literal">bandwidth=7</code>&#13;
 . The hierarchical algorithm is implemented using the ward linkage and the maximum (ward) distance, <code class="literal">max_d</code>&#13;
 , is set to <code class="literal">110</code>&#13;
 to stop the hierarchy. The <code class="literal">fcluster</code>&#13;
 function is used to obtain the predicted class for each data point. The predicted classes for the k-means and the mean-shift method are accessed using the <code class="literal">labels_</code>&#13;
 attribute, while the Gaussian mixture model needs to employ the <code class="literal">predict</code>&#13;
 function. The k -means, mean-shift, and Gaussian mixture methods have been trained using the <code class="literal">fit</code>&#13;
 function, while the hierarchical method has been trained using the <code class="literal">linkage</code>&#13;
 function. The output of the preceding code is shown in the following figure:</p>&#13;
<div class="mediaobject"><img src="Image00108.jpg" alt="Training and comparison of the clustering methods"/>&#13;
<div class="caption">&#13;
<p>IClustering of the two multivariate classes using k-means, mean-shift, Gaussian mixture model, and hierarchical ward method</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The mean-shift <a id="id204" class="indexterm"/>&#13;
 and hierarchical methods show two classes, so the<a id="id205" class="indexterm"/>&#13;
 choice of parameters (bandwidth and maximum distance) is appropriate. Note that the maximum distance value for the hierarchical method has been chosen looking at the dendrogram (the following figure) generated by the following code:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from scipy.cluster.hierarchy import dendrogram</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig = plt.figure(figsize=(20,15))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.title('Hierarchical Clustering Dendrogram',fontsize=30)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.xlabel('data point index (or cluster index)',fontsize=30)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.ylabel('distance (ward)',fontsize=30)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>dendrogram(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    Z,</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    truncate_mode='lastp',  # show only the last p merged clusters</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    p=12,</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    leaf_rotation=90.,</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    leaf_font_size=12.,</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    show_contracted=True,</strong>&#13;

</span>&#13;


<span class="strong">
<strong>)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.savefig('dendrogram.png')</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The <code class="literal">truncate_mode='lastp'</code>&#13;
 flag allows us to specify the number of last merges to show in the plot (in this case, <code class="literal">p=12</code>&#13;
 ). The preceding figure clearly shows that when the distance is between <code class="literal">100</code>&#13;
 and <code class="literal">135</code>&#13;
 , there are only two clusters left:</p>&#13;
<div class="mediaobject"><img src="Image00109.jpg" alt="Training and comparison of the clustering methods"/>&#13;
<div class="caption">&#13;
<p>IHierarchical clustering dendrogram for the last 12 merges</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>In the<a id="id206" class="indexterm"/>&#13;
 preceding figure on the horizontal axis, the number of <a id="id207" class="indexterm"/>&#13;
 data points belonging to each cluster before the last <span class="emphasis">&#13;
<em>12</em>&#13;
</span>&#13;
 merges is shown in brackets ().</p>&#13;
<p>Apart from the Gaussian mixture model, the other three algorithms misclassify some data points, especially k-means and hierarchical methods. This result proves that the Gaussian mixture model is the most robust method, as expected, since the dataset comes from the same distribution assumption. To evaluate the quality of the clustering, scikit-learn provides methods to quantify the correctness of the partitions: v-measure, completeness, and homogeneity. These methods require the real value of the class for each data point, so they are referred to as external validation procedures. This is because they require additional information not used while applying the clustering methods. Homogeneity, <span class="emphasis">&#13;
<em>h</em>&#13;
</span>&#13;
 , is a score between <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 that measures whether each cluster contains only elements of a single class. Completeness, <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 , quantifies with a score between <span class="emphasis">&#13;
<em>0</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 whether all the elements of a class are assigned to the same cluster. Consider a clustering that assigns each data point to a different cluster. In this way, each cluster will contains only one class and the homogeneity is <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 , but unless each class contains only one element, the completeness is very low because the class elements are spread around many clusters. Vice versa, if a clustering results in assigning all the data points of multiple classes to the same <a id="id208" class="indexterm"/>&#13;
 cluster, certainly the completeness is <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 but homogeneity<a id="id209" class="indexterm"/>&#13;
 is poor. These two scores have a similar formula, as follows:</p>&#13;
<div class="mediaobject"><img src="Image00110.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00111.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
 is the conditional entropy of the classes <span class="emphasis">&#13;
<em>C<sup>l</sup>&#13;
</em>&#13;
</span>&#13;
 , given the cluster assignments <span class="inlinemediaobject"><img src="Image00112.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="inlinemediaobject"><img src="Image00113.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
 is the conditional entropy of the clusters, given the class membership <span class="inlinemediaobject"><img src="Image00114.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>H(C<sub>l</sub>&#13;
 )</em>&#13;
</span>&#13;
 is the entropy of the classes: <span class="inlinemediaobject"><img src="Image00115.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>H(C)</em>&#13;
</span>&#13;
 is the entropy of the clusters: <span class="inlinemediaobject"><img src="Image00116.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>N<sub>pc</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of elements of class <span class="emphasis">&#13;
<em>p</em>&#13;
</span>&#13;
 in cluster <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>N<sub>p</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of elements of class <span class="emphasis">&#13;
<em>p</em>&#13;
</span>&#13;
 , and <span class="emphasis">&#13;
<em>N<sub>c</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of elements of cluster <span class="emphasis">&#13;
<em>c</em>&#13;
</span>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
<p>The v-measure is simply the harmonic mean of the homogeneity and the completeness:</p>&#13;
<div class="mediaobject"><img src="Image00117.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>These <a id="id210" class="indexterm"/>&#13;
 measures require the true labels to evaluate the quality<a id="id211" class="indexterm"/>&#13;
 of the clustering, and often this is not real-case scenario. Another method only employs data from the clustering itself, called <span class="strong">&#13;
<strong>silhouette</strong>&#13;
</span>&#13;
 , which <a id="id212" class="indexterm"/>&#13;
 calculates the similarities of each data point with the members of the cluster it belongs to and with the members of the other clusters. If on average each point is more similar to the points of its own cluster than the rest of the points, then the clusters are well defined and the score is close to <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 (it is close to <span class="emphasis">&#13;
<em>-1</em>&#13;
</span>&#13;
 , otherwise). For the formula, consider each point <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 and the following quantities:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>d<sub>s</sub>&#13;
 (i)</em>&#13;
</span>&#13;
 is the average distance of the point <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 from the points of the same cluster</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>d<sub>rest</sub>&#13;
 (i)</em>&#13;
</span>&#13;
 is the minimum distance of point <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 from the rest of the points in all other clusters</li>&#13;
</ul>&#13;
</div>&#13;
<p>The silhouette can be defined as</p>&#13;
<p>&#13;
<span class="inlinemediaobject"><img src="Image00118.jpg" alt="Training and comparison of the clustering methods"/>&#13;
</span>&#13;
 , and the silhouette score is the average of <span class="emphasis">&#13;
<em>s(i)</em>&#13;
</span>&#13;
 for all data points.</p>&#13;
<p>The four clustering algorithms we covered are associated with the following values of these four measures calculated using <code class="literal">sklearn</code>&#13;
 (scikit-learn):</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from sklearn.metrics import homogeneity_completeness_v_measure</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from sklearn.metrics import silhouette_score</strong>&#13;

</span>&#13;


<span class="strong">
<strong>res = homogeneity_completeness_v_measure(labels,pred_kmeans)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'kmeans measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_kmeans)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>res = homogeneity_completeness_v_measure(labels,pred_ms)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'mean-shift measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_ms)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>res = homogeneity_completeness_v_measure(labels,pred_gmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'gaussian mixture model measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_gmm)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>res = homogeneity_completeness_v_measure(labels,pred_h)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'hierarchical (ward) measures, homogeneity:',res[0],' completeness:',res[1],' v-measure:',res[2],' silhouette score:',silhouette_score(X,pred_h)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>The preceding code produces the following output:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>kmeans measures, homogeneity: 0.25910415428  completeness: 0.259403626429  v-measure: 0.259253803872  silhouette score: 0.409469791511</strong>&#13;

</span>&#13;


<span class="strong">
<strong>mean-shift measures, homogeneity: 0.657373750073  completeness: 0.662158204648  v-measure: 0.65975730345  silhouette score: 0.40117810244</strong>&#13;

</span>&#13;


<span class="strong">
<strong>gaussian mixture model measures, homogeneity: 0.959531296098  completeness: 0.959600517797  v-measure: 0.959565905699  silhouette score: 0.380255218681</strong>&#13;

</span>&#13;


<span class="strong">
<strong>hierarchical (ward) measures, homogeneity: 0.302367273976  completeness: 0.359334499592  v-measure: 0.32839867574  silhouette score: 0.356446705251</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>As <a id="id213" class="indexterm"/>&#13;
 expected from the analysis of the preceding figure, the <a id="id214" class="indexterm"/>&#13;
 Gaussian mixture model has the best values of the homogeneity, completeness, and v-measure measures (close to <span class="emphasis">&#13;
<em>1</em>&#13;
</span>&#13;
 ); mean-shift has reasonable values (around <span class="emphasis">&#13;
<em>0.5</em>&#13;
</span>&#13;
 ); while k-means and hierarchical methods result in poor values (around <span class="emphasis">&#13;
<em>0.3</em>&#13;
</span>&#13;
 ). The silhouette score instead is decent for all the methods (between <span class="emphasis">&#13;
<em>0.35</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>0.41</em>&#13;
</span>&#13;
 ), meaning that the clusters are reasonably well defined.</p>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Dimensionality reduction">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch02lvl1sec14"/>&#13;
 Dimensionality reduction</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>&#13;
<span class="strong">&#13;
<strong>Dimensionality reduction</strong>&#13;
</span>&#13;
 , which is also called feature extraction, refers to the operation to transform a<a id="id215" class="indexterm"/>&#13;
 data space given by a large number of dimensions to a subspace of fewer dimensions. The resulting subspace should contain only the most relevant information of the initial data, and the techniques to perform this operation are categorized as linear or non-linear. Dimensionality reduction is a broad class of techniques that is useful for extracting the most relevant information from a large dataset, decreasing its complexity but keeping the relevant information.</p>&#13;
<p>The most famous algorithm, <span class="strong">&#13;
<strong>Principal Component Analysis</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>PCA</strong>&#13;
</span>&#13;
 ), is a linear mapping of the original<a id="id216" class="indexterm"/>&#13;
 data into a subspace of uncorrelated dimensions, and it will be discussed hereafter. The code shown in this paragraph is available in IPython notebook and script versions at the author's GitHub book folder at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_2/</a>&#13;
 .</p>&#13;
<div class="section" title="Principal Component Analysis (PCA)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch02lvl2sec15"/>&#13;
 Principal Component Analysis (PCA)</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The <a id="id217" class="indexterm"/>&#13;
 principal component analysis algorithm aims to identify the subspace where the relevant information of a dataset lies. In fact, since the data points can be correlated in some data dimensions, PCA will find the few uncorrelated dimensions in which the data varies. For example, a car trajectory can be described by a series of variables such as velocity in km/h or m/s, position in latitude and longitude, position in meters from a chosen point, and position in<a id="id218" class="indexterm"/>&#13;
 miles from a chosen point. Clearly, the dimensions can be reduced because the velocity variables and the position<a id="id219" class="indexterm"/>&#13;
 variables give the same information (correlated variables), so the relevant subspace can be composed of two uncorrelated dimensions (a velocity variable and a position variable). PCA finds not only the uncorrelated set of variables but also the dimensions where the variance is maximized. That is, between the velocity in km/h and miles/h, the algorithm will select the variable with the highest variance, which is trivially represented by the line between the two axes given by the function <span class="emphasis">&#13;
<em>velocity[km/h]=3.6*velocity[m/s]</em>&#13;
</span>&#13;
 (typically closer to the km/h axis because <span class="emphasis">&#13;
<em>1 km/h = 3.6 m/s</em>&#13;
</span>&#13;
 and the velocity projections are more spread along the km/h axis than the m/s axis):</p>&#13;
<div class="mediaobject"><img src="Image00119.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
<div class="caption">&#13;
<p>The linear function between the velocity in m/s and km/h</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The preceding figure represents the linear function between the velocity in m/s and km/h. The projections of the points along the km/h axis have a large variance, while the projections on the m/s axis have a lower variance. The variance along the linear function <span class="emphasis">&#13;
<em>velocity[km/h]=3.6*velocity[m/s]</em>&#13;
</span>&#13;
 is larger than both axes.</p>&#13;
<p>Now we <a id="id220" class="indexterm"/>&#13;
 are ready to<a id="id221" class="indexterm"/>&#13;
 discuss the method and its features in detail. It is possible to show that finding the uncorrelated dimensions in which the variance is maximized is equivalent to computing the following steps. As usual, we consider the feature vectors <span class="inlinemediaobject"><img src="Image00096.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 :</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">The average of the dataset: <span class="inlinemediaobject"><img src="Image00120.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">The mean shifted dataset: <span class="inlinemediaobject"><img src="Image00121.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">The rescaled dataset, in which each feature vector component <span class="inlinemediaobject"><img src="Image00122.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 has been divided by the standard deviation, <span class="inlinemediaobject"><img src="Image00123.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00124.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">The sample covariance matrix: <span class="inlinemediaobject"><img src="Image00125.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">The <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 largest eigenvalues, <span class="inlinemediaobject"><img src="Image00126.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 , and their associated eigenvectors, <span class="inlinemediaobject"><img src="Image00127.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
</li>&#13;
<li class="listitem">Projected feature vectors on the subspace of the <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 eigenvectors <span class="inlinemediaobject"><img src="Image00128.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 , where <span class="inlinemediaobject"><img src="Image00129.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 is the matrix of the eigenvectors with <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 rows and <span class="emphasis">&#13;
<em>k</em>&#13;
</span>&#13;
 columns</li>&#13;
</ul>&#13;
</div>&#13;
<p>The final <a id="id222" class="indexterm"/>&#13;
 feature's <a id="id223" class="indexterm"/>&#13;
 vectors (principal components), <span class="inlinemediaobject"><img src="Image00130.jpg" alt="Principal Component Analysis (PCA)"/>&#13;
</span>&#13;
 lie on a subspace <span class="emphasis">&#13;
<em>R<sup>k</sup>&#13;
</em>&#13;
</span>&#13;
 , which still retain the maximum variance (and information) of the original vectors.</p>&#13;
<p>Note that this technique is particularly useful when dealing with high-dimensional datasets, such as in face recognition. In this field, an input image has to be compared with a database of other images to find the right person. The PCA application is called <span class="strong">&#13;
<strong>Eigenfaces</strong>&#13;
</span>&#13;
 , and it exploits the fact that a large number of pixels (variables) in each image are correlated. For instance, the background pixels are all correlated (the same), so a dimensionality reduction can be applied, and comparing images in a smaller subspace is a faster approach that gives accurate results. An example of implementation of Eigenfaces can be<a id="id224" class="indexterm"/>&#13;
 found on the author's GitHub profile at <a class="ulink" href="https://github.com/ai2010/eigenfaces">https://github.com/ai2010/eigenfaces</a>&#13;
 .</p>&#13;
<div class="section" title="PCA example">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h3 class="title"><a id="ch02lvl3sec14"/>&#13;
 PCA example</h3>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>As an example <a id="id225" class="indexterm"/>&#13;
 of the usage of PCA as well as the NumPy library discussed in <a class="link" title="Chapter 1. Introduction to Practical Machine Learning Using Python" href="text00015.html#ch01">Chapter 1</a>&#13;
 , <span class="emphasis">&#13;
<em>Introduction to Practical Machine Learning using Python</em>&#13;
</span>&#13;
 we are going to determine the principal component of a two-dimensional dataset distributed along the line <span class="emphasis">&#13;
<em>y=2x</em>&#13;
</span>&#13;
 , with random (normally distributed) noise. The dataset and the corresponding figure (see the following figure) have been generated using the following code:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>import numpy as np</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from matplotlib import pyplot as plt</strong>&#13;

</span>&#13;



<span class="strong">
<strong>#line y = 2*x</strong>&#13;

</span>&#13;


<span class="strong">
<strong>x = np.arange(1,101,1).astype(float)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>y = 2*np.arange(1,101,1).astype(float)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>#add noise</strong>&#13;

</span>&#13;


<span class="strong">
<strong>noise = np.random.normal(0, 10, 100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>y += noise</strong>&#13;

</span>&#13;



<span class="strong">
<strong>fig = plt.figure(figsize=(10,10))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>#plot</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.plot(x,y,'ro')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.axis([0,102, -20,220])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.quiver(60, 100,10-0, 20-0, scale_units='xy', scale=1)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.arrow(60, 100,10-0, 20-0,head_width=2.5, head_length=2.5, fc='k', ec='k')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>plt.text(70, 110, r'$v^1$', fontsize=20)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>#save</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax = fig.add_subplot(111)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax.axis([0,102, -20,220])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax.set_xlabel('x',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>ax.set_ylabel('y',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.suptitle('2 dimensional dataset',fontsize=40)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>fig.savefig('pca_data.png')</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The following<a id="id226" class="indexterm"/>&#13;
 figure shows the resulting dataset. Clearly there is a direction in which the data is distributed and it corresponds to the principal component <span class="inlinemediaobject"><img src="Image00131.jpg" alt="PCA example"/>&#13;
</span>&#13;
 that we are going to extract from the data.</p>&#13;
<div class="mediaobject"><img src="Image00132.jpg" alt="PCA example"/>&#13;
<div class="caption">&#13;
<p>A two-dimensional dataset. The principal component direction v1 is indicated by an arrow.</p>&#13;
</div>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The algorithm calculates the mean of the two-dimensional dataset and the mean shifted dataset, and then rescales with the corresponding standard deviation:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>mean_x = np.mean(x)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>mean_y = np.mean(y)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>u_x = (x- mean_x)/np.std(x)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>u_y = (y-mean_y)/np.std(y)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>sigma = np.cov([u_x,u_y])</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>To extract the <a id="id227" class="indexterm"/>&#13;
 principal component, we have to calculate the eigenvalues and eigenvectors and select the eigenvector associated with the largest eigenvalue:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>eig_vals, eig_vecs = np.linalg.eig(sigma)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             for i in range(len(eig_vals))]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             </strong>&#13;

</span>&#13;


<span class="strong">
<strong>eig_pairs.sort()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>eig_pairs.reverse()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>v1 = eig_pairs[0][1]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print v1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>array([ 0.70710678,  0.70710678]</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>To check whether the principal component lies along the line as expected, we need to rescale back its coordinates:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>x_v1 = v1[0]*np.std(x)+mean_x</strong>&#13;

</span>&#13;


<span class="strong">
<strong>y_v1 = v1[1]*np.std(y)+mean_y</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print 'slope:',(y_v1-1)/(x_v1-1)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>slope: 2.03082418796</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The resulting slope is approximately <code class="literal">2</code>&#13;
 , which agrees with the value chosen at the beginning. The <code class="literal">scikit-learn</code>&#13;
 library provides a possible ready-to-use implementation of the PCA algorithm without applying any rescaling or mean shifting. To use the <code class="literal">sklearn</code>&#13;
 module, we need to transform the rescaled data into a matrix structure in which each row is a data point with <span class="emphasis">&#13;
<em>x</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>y</em>&#13;
</span>&#13;
 coordinates:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>X = np.array([u_x,u_y])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>X = X.T</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print X.shape</strong>&#13;

</span>&#13;


<span class="strong">
<strong>(100,2)</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The PCA module can be started now, specifying the number of principal components we want (<code class="literal">1</code>&#13;
 in this case):</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from sklearn.decomposition import PCA</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pca = PCA(n_components=1)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>pca.fit(X)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>v1_sklearn = pca.components_[0]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>print v1_sklearn</strong>&#13;

</span>&#13;


<span class="strong">
<strong>[ 0.70710678  0.70710678]</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The principal component is exactly the same as the one obtained using the step-by-step approach,<code class="literal">[ 0.70710678 0.70710678]</code>&#13;
 , so the slope will also be the same. The dataset can now be<a id="id228" class="indexterm"/>&#13;
 transformed into the new one-dimensional space with both approaches:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>#transform in reduced space</strong>&#13;

</span>&#13;


<span class="strong">
<strong>X_red_sklearn = pca.fit_transform(X)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>W = np.array(v1.reshape(2,1))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>X_red = W.T.dot(X.T)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>#check the reduced matrices are equal</strong>&#13;

</span>&#13;


<span class="strong">
<strong>assert X_red.T.all() == X_red_sklearn.all(), 'problem with the pca algorithm'</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The assert exception is not thrown, so the results show a perfect agreement between the two methods.</p>&#13;
</div>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Singular value decomposition">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch02lvl1sec15"/>&#13;
 Singular value decomposition</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>This <a id="id229" class="indexterm"/>&#13;
 method is based on a theorem that states that a matrix <span class="emphasis">&#13;
<em>X d x N</em>&#13;
</span>&#13;
 can be decomposed as follows:</p>&#13;
<div class="mediaobject"><img src="Image00133.jpg" alt="Singular value decomposition"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>U</em>&#13;
</span>&#13;
 is a <span class="emphasis">&#13;
<em>d x d</em>&#13;
</span>&#13;
 unitary matrix</li>&#13;
<li class="listitem">∑ is a <span class="emphasis">&#13;
<em>d x N</em>&#13;
</span>&#13;
 diagonal matrix where the diagonal entries s<span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 are called singular values</li>&#13;
<li class="listitem">&#13;
<span class="emphasis">&#13;
<em>V</em>&#13;
</span>&#13;
 is an <span class="emphasis">&#13;
<em>N x N</em>&#13;
</span>&#13;
 unitary matrix</li>&#13;
</ul>&#13;
</div>&#13;
<p>In our case, <span class="emphasis">&#13;
<em>X</em>&#13;
</span>&#13;
 can be composed by the feature's vectors <span class="inlinemediaobject"><img src="Image00134.jpg" alt="Singular value decomposition"/>&#13;
</span>&#13;
 , where each <span class="inlinemediaobject"><img src="Image00135.jpg" alt="Singular value decomposition"/>&#13;
</span>&#13;
 is a column. We can reduce the number of dimensions of each feature vector <span class="emphasis">&#13;
<em>d</em>&#13;
</span>&#13;
 , approximating the singular value decomposition. In practice, we consider only the largest singular values <span class="inlinemediaobject"><img src="Image00136.jpg" alt="Singular value decomposition"/>&#13;
</span>&#13;
 so that:</p>&#13;
<div class="mediaobject"><img src="Image00137.jpg" alt="Singular value decomposition"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>&#13;
<span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 represents the dimension of the new reduced space where the feature vectors are projected. <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 vector <span class="emphasis">&#13;
<em>x<sup>(i)</sup>&#13;
</em>&#13;
</span>&#13;
 is transformed in the new space using the following formula:</p>&#13;
<div class="mediaobject"><img src="Image00138.jpg" alt="Singular value decomposition"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This means that the matrix <span class="inlinemediaobject"><img src="Image00139.jpg" alt="Singular value decomposition"/>&#13;
</span>&#13;
 (not <span class="inlinemediaobject"><img src="Image00140.jpg" alt="Singular value decomposition"/>&#13;
</span>&#13;
 ) represents the feature vectors in the <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 dimensional space.</p>&#13;
<p>Note that it is <a id="id230" class="indexterm"/>&#13;
 possible to show that this method is very similar to the PCA; in fact, the <code class="literal">scikit-learn</code>&#13;
 library uses SVD to implement PCA.</p>&#13;
</div>&#13;

<div class="section" title="Summary">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch02lvl1sec16"/>&#13;
 Summary</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>In this chapter, the main clustering algorithms were discussed in detail. We implemented them (using scikit-learn) and compared the results. Also, the most relevant dimensionality reduction technique, principal component analysis, was presented and implemented. You should now have the knowledge to use the main unsupervised learning techniques in real scenarios using Python and its libraries.</p>&#13;
<p>In the next chapter, the supervised learning algorithms will be discussed, for both classification and regression problems.</p>&#13;
</div>&#13;
</body></html>