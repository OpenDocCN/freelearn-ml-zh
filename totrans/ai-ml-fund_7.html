<html><head></head><body>
<div id="_idContainer097" class="Content">
<p class="hidden" data-amznremoved-m8="true" data-amznremoved="mobi7">7</p>
</div>
<div id="_idContainer098" class="Content">
<h1 id="_idParaDest-157"><a id="_idTextAnchor166"></a>
 Deep Learning with Neural Networks</h1>
</div>
<div id="_idContainer099" class="Content">
<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="bullets">Perform basic TensorFlow operations to solve various expressions</li>
<li class="bullets">Describe how aritifical neural networks work</li>
<li class="bullets">Train and test neural networks with TensorFlow</li>
<li class="bullets">Implement deep learning neural network models with TensorFlow</li>
</ul>
<p>In this chapter, we'll detect a written digit using the TensorFlow library.</p>
</div>
<div id="_idContainer111" class="Content">
<h2 id="_idParaDest-158"><a id="_idTextAnchor167"></a>
 Introduction</h2>
<p>In this chapter, we will learn about another supervised learning technique. However, this time, instead of using a simple mathematical model such as classification or regression, we will use a completely different model: <strong class="keyword _idGenCharOverride-1">neural networks</strong>
 . Although we will use Neural Networks for supervised learning, note that Neural Networks can also model unsupervised learning techniques. The significance of this model increased in the last century, because in the past, the computation power required to use this model for supervised learning was not enough. Therefore, neural networks have emerged in practice in the last century.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor168"></a>
 TensorFlow for Python</h2>
<p>TensorFlow is one of the most important machine learning and open source libraries maintained by Google. The TensorFlow API is available in many languages, including Python, JavaScript, Java, and C. As TensorFlow supports supervised learning, we will use TensorFlow for building a graph model, and then use this model for prediction.</p>
<p>TensorFlow works with tensors. Some examples for tensors are:</p>
<ul>
<li>Scalar values such as a floating point number.</li>
<li>A vector of arbitrary length.</li>
<li>A regular matrix, containing p times q values, where p and q are finite integers.</li>
<li>A p x q x r generalized matrix-like structure, where p, q, r are finite integers. Imagine this construct as a rectangular object in three dimensional space with sides p, q, and r. The numbers in this data structure can be visualized in three dimensions.</li>
<li>Observing the above four data structures, more complex, n-dimensional data structures can also be valid examples for tensors.</li>
</ul>
<p class="_idGenParaOverride-1">We will stick to scalar, vector, and regular matrix tensors in this chapter. Within the scope of this chapter, think of tensors as scalar values, or arrays, or arrays of arrays.</p>
<div></div>
<p>TensorFlow is used to create artificial neural networks because it models its inputs, outputs, internal nodes, and directed edges between these nodes. TensorFlow also comes with mathematical functions to transform signals. These mathematical functions will also come handy when modeling when a neuron inside a neural network gets activated.</p>
<h4>Note</h4>
<p class="callout">Tensors are array-like objects. Flow symbolizes the manipulation of tensor data. So, essentially, TensorFlow is an array data manipulation library.</p>
<p>The main use case for TensorFlow is artificial neural networks, as this field requires operation on big arrays and matrices. TensorFlow comes with many deep learning-related functions, and so it is an optimal environment for neural networks. TensorFlow is used for voice recognition, voice search, and it is also the brain behind translate.google.com. Later in this chapter, we will use TensorFlow to recognize written characters.</p>
<h3 id="_idParaDest-160"><a id="_idTextAnchor169"></a>
 Installing TensorFlow in the Anaconda Navigator</h3>
<p>Let's open the Anaconda Prompt and install TensorFlow using <strong class="inline _idGenCharOverride-2">pip</strong>
 :</p>
<p class="snippet">pip install tensorflow</p>
<p>Installation will take a few minutes because the package itself is quite big. If you prefer using your video card GPU instead of your CPU, you can also use <strong class="inline _idGenCharOverride-2">tensorflow-gpu</strong>
 . Make sure that you only use the GPU version if you have a good enough graphics card for it.</p>
<p>Once you are done with the installation, you can import TensorFlow in IPython:</p>
<p class="snippet">import tensorflow as tf</p>
<p class="_idGenParaOverride-1">First, we will use TensorFlow to build a graph. The execution of this model is separated. This separation is important because execution is resource intensive and may therefore run on a server specialized in solving computation heavy problems.</p>
<div></div>
<h3 id="_idParaDest-161"><a id="_idTextAnchor170"></a>
 TensorFlow Operations</h3>
<p>TensorFlow provides many operations to manipulate data. A few examples of these operations are as follows:</p>
<ul>
<li>
<strong class="bold _idGenCharOverride-1">Arithmetic operations</strong>
 : <strong class="inline _idGenCharOverride-2">add</strong>
 and <strong class="inline _idGenCharOverride-2">multiply</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Exponential operations</strong>
 : <strong class="inline _idGenCharOverride-2">exp</strong>
 and <strong class="inline _idGenCharOverride-2">log</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Relational operations</strong>
 : <strong class="inline _idGenCharOverride-2">greater</strong>
 , <strong class="inline _idGenCharOverride-2">less</strong>
 , and <strong class="inline _idGenCharOverride-2">equal</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Array operations</strong>
 : <strong class="inline _idGenCharOverride-2">concat</strong>
 , <strong class="inline _idGenCharOverride-2">slice</strong>
 , and <strong class="inline _idGenCharOverride-2">split</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Matrix operations</strong>
 : <strong class="inline _idGenCharOverride-2">matrix_inverse</strong>
 , <strong class="inline _idGenCharOverride-2">matrix_determinant</strong>
 , and <strong class="inline _idGenCharOverride-2">matmul</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Neural network-related operations</strong>
 : <strong class="inline _idGenCharOverride-2">sigmoid</strong>
 , <strong class="inline _idGenCharOverride-2">ReLU</strong>
 , and <strong class="inline _idGenCharOverride-2">softmax</strong>
</li>
</ul>
<h3 id="_idParaDest-162"><a id="_idTextAnchor171"></a>
 Exercise 22: Using Basic Operations and TensorFlow constants</h3>
<p>Use arithmetic operations in Tensorflow to solve the expression: <em class="italics _idGenCharOverride-3">2 * 3 + 4</em>
</p>
<p>These operations can be used to build a graph. To understand more about TensorFlow constants and basic arithmetic operators, let's consider a simple expression <em class="italics _idGenCharOverride-3">2 * 3 + 4</em>
 the graph for this expression would be as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer100" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00062.jpg" alt="Figure 7.1 Graph of the expression 2*3+4" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 7.1: Graph of the expression 2*3+4</h6>
<div></div>
<ol>
<li value="1">Model this graph in TensorFlow by using the following code:<p class="snippet _idGenParaOverride-2">import tensorflow as tf</p>
<p class="snippet _idGenParaOverride-2">input1 = tf.constant(2.0, tf.float32, name='input1')</p>
<p class="snippet _idGenParaOverride-2">input2 = tf.constant(3.0, tf.float32, name='input2')</p>
<p class="snippet _idGenParaOverride-2">input3 = tf.constant(4.0, tf.float32, name='input3')</p>
<p class="snippet _idGenParaOverride-2">product12 = tf.multiply(input1, input2)</p>
<p class="snippet _idGenParaOverride-2">sum = tf.add(product12, input3)</p>
</li>
<li value="2">Once the graph is built, to perform calculations, we have to open a TensorFlow session and execute our nodes:<p class="snippet _idGenParaOverride-2">with tf.Session() as session:</p>
<p class="snippet _idGenParaOverride-2">    print(session.run(product12))</p>
<p class="snippet _idGenParaOverride-2">    print(session.run(sum))</p>
<p class="_idGenParaOverride-3">The intermediate and final results are printed to the console:</p>
<p class="snippet _idGenParaOverride-2">6.0</p>
<p class="snippet _idGenParaOverride-2">10.0</p>
</li>
</ol>
<h3 id="_idParaDest-163"><a id="_idTextAnchor172"></a>
 Placeholders and Variables</h3>
<p>Now that you can build expressions with TensorFlow, let's take things a step further and build placeholders and variables.</p>
<p>Placeholders are substituted with a constant value when the execution of a session starts. Placeholders are essentially parameters that are substituted before solving an expression. Variables are values that might change during the execution of a session.</p>
<p>Let's create a parametrized expression with TensorFlow:</p>
<p class="snippet">import tensorflow as tf</p>
<p class="snippet">input1 = tf.constant(2.0, tf.float32, name='input1')</p>
<p class="snippet">input2 = tf.placeholder(tf.float32, name='p')</p>
<p class="snippet">input3 = tf.Variable(0.0, tf.float32, name='x')</p>
<p class="snippet">product12 = tf.multiply(input1, input2)</p>
<p class="snippet">sum = tf.add(product12, input3)</p>
<p class="snippet">with tf.Session() as session:</p>
<p class="snippet">    initializer = tf.global_variables_initializer()</p>
<p class="snippet">    session.run(initializer)</p>
<p class="snippet">    print(session.run(sum, feed_dict={input2: 3.0}))</p>
<p>The output is <strong class="inline _idGenCharOverride-2">6.0</strong>
 .</p>
<p>The <strong class="inline _idGenCharOverride-2">tf.global_variables_initializer()</strong>
 call initialized the variable in <strong class="inline _idGenCharOverride-2">input3</strong>
 to its default value, zero, after it was executed in <strong class="inline _idGenCharOverride-2">session.run</strong>
 .</p>
<p>The sum was calculated inside another <strong class="inline _idGenCharOverride-2">session.run</strong>
 statement by using the feed dictionary, thus using the constant <strong class="inline _idGenCharOverride-2">3.0</strong>
 in place of the <strong class="inline _idGenCharOverride-2">input2</strong>
 parameter.</p>
<p>Note that in this specific example, the variable x is initialized to zero. The value of x does not change during the execution of the TensorFlow session. Later, when we will use TensorFlow to describe neural networks, we will define an optimization target, and the session will optimize the values of the variables to meet this target.</p>
<h3 id="_idParaDest-164"><a id="_idTextAnchor173"></a>
 Global Variables Initializer</h3>
<p>As TensorFlow often makes use of matrix operations, it makes sense to learn how to initialize a matrix of random variables to a value that's randomly generated according to a normal distribution centered at zero.</p>
<p>Not only matrices, but all global variables are initialized inside the session by calling <strong class="inline _idGenCharOverride-2">tf.global_variables_initializer()</strong>
 :</p>
<p class="snippet">randomMatrix = tf.Variable(tf.random_normal([3, 4]))</p>
<p class="snippet">with tf.Session() as session:</p>
<p class="snippet">    initializer = tf.global_variables_initializer()</p>
<p class="snippet">    print( session.run(initializer))</p>
<p class="snippet">    print( session.run(randomMatrix))</p>
<p class="snippet">    </p>
<p class="snippet">None</p>
<p class="snippet">[[-0.41974232 1.8810892 -1.4549098 -0.73987174]</p>
<p class="snippet">[ 2.1072254 1.7968426 -0.38310152 0.98115194]</p>
<p class="snippet">[-0.550108 -0.41858754 1.3511614 1.2387075 ]]</p>
<p class="_idGenParaOverride-1">As you can see, the initialization of a <strong class="inline _idGenCharOverride-2">tf.Variable</strong>
 takes one argument: the value of <strong class="inline _idGenCharOverride-2">tf.random_normal([3,4])</strong>
 .</p>
<div></div>
<h2 id="_idParaDest-165"><a id="_idTextAnchor174"></a>
 Introduction to Neural Networks</h2>
<p>Neural networks are the newest branch of AI. Neural networks are inspired by how the human brain works. Originally, they were invented in the 1940s by Warren McCulloch and Walter Pitts. The neural network was a mathematical model that was used for describing how the human brain can solve problems.</p>
<p>We will use the phrase artificial neural network when talking about the mathematical model and use biological neural network when talking about the human brain. Artificial neural networks are supervised learning algorithms.</p>
<p>The way a neural network learns is more complex compared to other classification or regression models. The neural network model has a lot of internal variables, and the relationship between the input and output variables may go through multiple internal layers. Neural networks have higher accuracy as compared to other supervised learning algorithms.</p>
<h4>Note</h4>
<p class="callout">Mastering neural networks with TensorFlow is a complex process. The purpose of this section is to provide you with an introductory resource to get started.</p>
<p>In this chapter, the main example we are going to use is the recognition of digits from an image. We are considering this image since it is small, and we have around 70,000 images available. The processing power required to process these images, is similar to that of a regular computer.</p>
<p class="_idGenParaOverride-1">Artificial neural network works similar to human brain works. Dendroid in a human brain is connected to the nucleus and the nucleus is connected to the axon. Here, the dendroid acts as the inputs, nucleus is where the calculations occurs (weighted sum and the activation function) and the axon acts similar to the output.</p>
<div></div>
<p>Then, we determine which neuron fires by passing the weighted sum to an activation function. If this function determines that a neuron has to fire, the signal appears in the output. This signal can be the input of other neurons in the network:</p>
<h6><a id="_idTextAnchor175"></a>
</h6>
<div class="_idGenObjectLayout-1">
<div id="_idContainer101" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00063.jpg" alt="Figure 7.2 Diagram showing how the artificial neural network works" />
</div>
</div>
<h6>Figure 7.2: Diagram showing how the artificial neural network works</h6>
<p>Suppose <strong class="inline _idGenCharOverride-2">f</strong>
 is the activation function, <strong class="inline _idGenCharOverride-2">x1</strong>
 , <strong class="inline _idGenCharOverride-2">x2</strong>
 , <strong class="inline _idGenCharOverride-2">x3</strong>
 , and <strong class="inline _idGenCharOverride-2">x4</strong>
 are the inputs, and their sum is weighted with the weights <strong class="inline _idGenCharOverride-2">w1</strong>
 , <strong class="inline _idGenCharOverride-2">w2</strong>
 , <strong class="inline _idGenCharOverride-2">w3</strong>
 , and <strong class="inline _idGenCharOverride-2">w4</strong>
 :</p>
<p class="snippet">y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4)</p>
<p>Assuming vector <strong class="inline _idGenCharOverride-2">x</strong>
 is (<strong class="inline _idGenCharOverride-2">x1</strong>
 , <strong class="inline _idGenCharOverride-2">x2</strong>
 , <strong class="inline _idGenCharOverride-2">x3</strong>
 , <strong class="inline _idGenCharOverride-2">x4</strong>
 ) and vector <strong class="inline _idGenCharOverride-2">w</strong>
 is (<strong class="inline _idGenCharOverride-2">w1</strong>
 , <strong class="inline _idGenCharOverride-2">w2</strong>
 , <strong class="inline _idGenCharOverride-2">w3</strong>
 , <strong class="inline _idGenCharOverride-2">w4</strong>
 ), we can write this equation as the scalar or dot product of these two vectors:</p>
<p class="snippet">y <a id="_idTextAnchor176"></a>
 = f(x ⋅ w)</p>
<p class="_idGenParaOverride-1">The construct we have defined is one neuron:</p>
<div></div>
<p>Let's hide the details of this neuron so that it becomes easier to construct a neural network:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer102" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00064.jpg" alt="Figure 7.4 Diagram that represents the hidden layer of a neuron" />
</div>
</div>
<h6>Figure 7.4: Diagram that represents the hidden layer of a neuron</h6>
<p>We can create multiple boxes and multiple output variables that may get activated as a result of reading the weighted average of inputs.</p>
<p>Although in the following diagram there are arrows leading from all inputs to all boxes, bear in mind that the weights on the arrows might be zero. We still display these arrows in the diagram:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer103" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00065.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 7.5: Diagram representing a neural network</h6>
<div></div>
<p>The boxes describing the relationship between the inputs and the outputs are referred to as a hidden layer. A neural network with one hidden layer is called a <strong class="keyword _idGenCharOverride-1">regular neural network</strong>
 .</p>
<p>When connecting inputs and outputs, we may have multiple hidden layers. A neural network with multiple layers is called a <strong class="keyword _idGenCharOverride-1">deep neural network</strong>
 :</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer104" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00066.jpg" alt="Figure 7.6 A diagram representing a deep neural network" />
</div>
</div>
<h6>Figure 7.6: A diagram representing a deep neural network</h6>
<p class="_idGenParaOverride-1">The term deep learning comes from the presence of multiple layers. When creating an artificial neural network, we can specify the number of hidden layers.</p>
<div></div>
<h3 id="_idParaDest-166"><a id="_idTextAnchor177"></a>
 Biases</h3>
<p>Let's see the model of a neuron in a neural network again:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer105" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00067.jpg" alt="Figure 7.7 Diagram of neuron in neural network" />
</div>
</div>
<h6>Figure 7.7: Diagram of neuron in neural network</h6>
<p>We learned that the equation of this neuron is as follows:</p>
<p class="snippet">y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4)</p>
<p>The problem with this equation is that there is no constant factor that depends on the inputs x1, x2, x3, and x4. This implies that each neuron in a neural network, without bias, always produces this value whenever for each weight-input pair, their product is zero.</p>
<p>Therefore, we add bias to the equation:</p>
<p class="snippet">y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4 + b)</p>
<p class="snippet _idGenParaOverride-1">y = f(x ⋅ w + b)</p>
<div></div>
<p>The first equation is the verbose form, describing the role of each coordinate, weight coefficient, and bias. The second equation is the vector form, where x = (x1, x2, x3, x4) and w = (w1, w2, w3, w4). The dot operator between the vectors symbolizes the dot or scalar product of the two vectors. The two equations are equivalent. We will use the second form in practice because it is easier to define a vector of variables using TensorFlow than to define each variable one by one.</p>
<p>Similarly, for <em class="italics _idGenCharOverride-3">w1</em>
 , <em class="italics _idGenCharOverride-3">w2</em>
 , <em class="italics _idGenCharOverride-3">w3</em>
 , and <em class="italics _idGenCharOverride-3">w4</em>
 , the bias <em class="italics _idGenCharOverride-3">b</em>
 is a variable, meaning that its value can change during the learning process.</p>
<p>With this constant factor built into each neuron, the neural network model becomes more flexible from the purpose of fitting a specific training dataset better.</p>
<h4>Note</h4>
<p class="callout">It may happen that the product <strong class="inline _idGenCharOverride-4">p = x1*w1 + x2*w2 + x3*w3 + x4*w4</strong>
 is negative due to the presence of a few negative weights. We may still want to give the model the flexibility to fire a neuron with values above a given negative number. Therefore, adding a constant bias b = 5, for instance, can ensure that the neuron fires for values between -5 and 0 as well.</p>
<h3 id="_idParaDest-167"><a id="_idTextAnchor178"></a>
 Use Cases for Artificial Neural Networks</h3>
<p class="_idGenParaOverride-1">Artificial neural networks have their place among supervised learning techniques. They can model both classification and regression problems. A classifier neural network seeks a relationship between features and labels. The features are the input variables, while each class the classifier can choose as a return value is a separate output. In the case of regression, the input variables are the features, while there is one single output: the predicted value. While traditional classification and regression techniques have their use cases in artificial intelligence, artificial neural networks are generally better at finding complex relationships between the inputs and the outputs.</p>
<div></div>
<h3 id="_idParaDest-168"><a id="_idTextAnchor179"></a>
 Activation Functions</h3>
<p>Different activation functions are used in neural networks. Without these functions, the neural network would be a linear model that could be easily described using matrix multiplication.</p>
<p>Activation functions of the neural network provide non-linearity. The most common activation functions are <strong class="inline _idGenCharOverride-2">sigmoid</strong>
 and <strong class="inline _idGenCharOverride-2">tanh</strong>
 (the hyperbolic tangent function).</p>
<p>The formula of <strong class="inline _idGenCharOverride-2">sigmoid</strong>
 is as follows:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">def sigmoid(x):</p>
<p class="snippet">    return 1 / (1 + np.e ** (-x))</p>
<p>Let's plot this function using <strong class="inline _idGenCharOverride-2">pyplot</strong>
 :</p>
<p class="snippet">import matplotlib.pylab as plt</p>
<p class="snippet">x = np.arange(-10, 10, 0.1)</p>
<p class="snippet">plt.plot(x, sigmoid(x))</p>
<p class="snippet">plt.show()</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer106" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00068.jpg" alt="Figure 7.8 Graph displaying the sigmoid curve" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 7.8: Graph displaying the sigmoid curve</h6>
<div></div>
<p>There are a few problems with the sigmoid function.</p>
<p>First, it may disproportionally amplify or dampen weights.</p>
<p>Second, <strong class="inline _idGenCharOverride-2">sigmoid(0)</strong>
 is not zero. This makes the learning process harder.</p>
<p>The formula of the hyperbolic tangent is as follows:</p>
<p class="snippet">def tanh(x):</p>
<p class="snippet">    return 2 / (1 + np.e ** (-2*x)) - 1</p>
<p>We can also plot this function like so:</p>
<p class="snippet">x = np.arange(-10, 10, 0.1)</p>
<p class="snippet">plt.plot(x, tanh(x))</p>
<p class="snippet">plt.show()</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer107" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00069.jpg" alt="Figure 7.9 Graph after plotting hyperbolic tangent" />
</div>
</div>
<h6>Figure 7.9: Graph after plotting hyperbolic tangent</h6>
<p>Both functions add a little non-linearity to the values emitted by a neuron. The sigmoid function looks a bit smoother, while the tanh function gives slightly more edgy results.</p>
<p>Another activation function has become popular lately: <strong class="inline _idGenCharOverride-2">ReLU</strong>
 . <strong class="inline _idGenCharOverride-2">ReLU</strong>
 stands for Rectified Linear Unit:</p>
<p class="snippet">def relu(x):</p>
<p class="snippet _idGenParaOverride-1">    return 0 if x &lt; 0 else x</p>
<div></div>
<p>Making the neural network model non-linear makes it easier for the model to approximate non-linear functions. Without these non-linear functions, regardless of the number of layers of the network, we would only be able to approximate linear problems:</p>
<p class="snippet">def reluArr(arr):</p>
<p class="snippet">   return [relu(x) for x in arr]</p>
<p class="snippet">x = np.arange(-10, 10, 0.1)</p>
<p class="snippet">plt.plot(x, reluArr(x))</p>
<p class="snippet">plt.show()</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer108" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00070.jpg" alt="Figure 7.10 Graph displaying the ReLU function" />
</div>
</div>
<h6>Figure 7.10: Graph displaying the ReLU function</h6>
<p>The <strong class="inline _idGenCharOverride-2">ReLU</strong>
 activation function behaves surprisingly well from the perspective of quickly converging to the final values of the weights and biases of the neural network.</p>
<p>We will use one more function in this chapter: <strong class="inline _idGenCharOverride-2">softmax</strong>
 .</p>
<p>The <strong class="inline _idGenCharOverride-2">softmax</strong>
 function shrinks the values of a list between <em class="italics _idGenCharOverride-3">0</em>
 and <em class="italics _idGenCharOverride-3">1</em>
 so that the sum of the elements of the list becomes <em class="italics _idGenCharOverride-3">1</em>
 . The definition of the <strong class="inline _idGenCharOverride-2">softmax</strong>
 function is as follows:</p>
<p class="snippet">def softmax(list):</p>
<p class="snippet _idGenParaOverride-1">    return np.exp(list) / np.sum(np.exp(list))</p>
<div></div>
<p>Here is an example:</p>
<p class="snippet">softmax([1,2,1])</p>
<p>The output is as follows:</p>
<p class="snippet">array([0.21194156, 0.57611688, 0.21194156])</p>
<p>The <strong class="inline _idGenCharOverride-2">softmax</strong>
 function can be used whenever we filter a list, not a single value. Each element of the list will be transformed.</p>
<p>Let's experiment with different activator functions. Observe how these functions dampen the weighted inputs by solving the following exercise.</p>
<h3 id="_idParaDest-169"><a id="_idTextAnchor180"></a>
 Exercise 23: Activation Functions</h3>
<p>Consider the following neural network:</p>
<p>
<em class="italics _idGenCharOverride-3">y = f( 2 * x1 + 0.5 * x2 + 1.5 * x3 - 3 ).</em>
</p>
<p>Assuming <em class="italics _idGenCharOverride-3">x1</em>
 is 1 and <em class="italics _idGenCharOverride-3">x2</em>
 is 2, calculate the value of <em class="italics _idGenCharOverride-3">y</em>
 for the following x values: -1, 0, 1, 2, when:</p>
<ul>
<li>f is the <strong class="inline _idGenCharOverride-2">sigmoid</strong>
 function</li>
<li>f is the <strong class="inline _idGenCharOverride-2">tanh</strong>
 function</li>
<li>f is the <strong class="inline _idGenCharOverride-2">ReLU</strong>
 function</li>
</ul>
<p>Perform the following steps:</p>
<ol>
<li class="ParaOverride-1" value="1">Substitute the known coefficients:<p class="snippet _idGenParaOverride-2">def get_y( f, x3 ):</p>
<p class="snippet _idGenParaOverride-2">    return f(2*1+0.5*2+1.5*x3)</p>
</li>
<li value="2">Use the following three activator functions:<p class="snippet _idGenParaOverride-2">import numpy as np</p>
<p class="snippet _idGenParaOverride-2">def sigmoid(x):</p>
<p class="snippet _idGenParaOverride-2">    return 1 / (1 + np.e ** (-x))</p>
<p class="snippet _idGenParaOverride-2">def tanh(x):</p>
<p class="snippet _idGenParaOverride-2">    return 2 / (1 + np.e ** (-2*x)) - 1</p>
<p class="snippet _idGenParaOverride-2">def relu(x):</p>
<p class="snippet _idGenParaOverride-4">    return 0 if x &lt; 0 else x</p>
<div></div>
</li>
<li value="3">Calculate the sigmoid values, using the following commands:<p class="snippet _idGenParaOverride-2">get_y( sigmoid, -2 )</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.5</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(sigmoid, -1)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.8175744761936437</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(sigmoid, 0)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.9525741268224331</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(sigmoid, 1)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.9890130573694068</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(sigmoid, 2)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.9975273768433653</strong>
</p>
</li>
<li value="4">As you can see, the changes are dampened quickly as the sum of the expression inside the <strong class="inline _idGenCharOverride-2">sigmoid</strong>
 function increases. We expect the <strong class="inline _idGenCharOverride-2">tanh</strong>
 function to have an even bigger dampening effect:<p class="snippet _idGenParaOverride-2">get_y(tanh, -2)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.0</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(tanh, -1)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.9051482536448663</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(tanh, 0)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.9950547536867307</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(tanh, 1)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.9997532108480274</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(tanh, 2)</p>
<p class="_idGenParaOverride-5">The output is <strong class="inline _idGenCharOverride-2">0.9999877116507956</strong>
</p>
<div></div>
</li>
<li value="5">Based on the characteristics of the <strong class="inline _idGenCharOverride-2">tanh</strong>
 function, the output approaches the 1 asymptote faster than the sigmoid function. For <em class="italics _idGenCharOverride-3">x3 = -2</em>
 , we calculate <strong class="inline _idGenCharOverride-2">f(0)</strong>
 . While <strong class="inline _idGenCharOverride-2">sigmoid(0)</strong>
 is <em class="italics _idGenCharOverride-3">0.5</em>
 , <strong class="inline _idGenCharOverride-2">tanh(0)</strong>
 is <em class="italics _idGenCharOverride-3">0</em>
 . As opposed to the other two functions, the <strong class="inline _idGenCharOverride-2">ReLu</strong>
 function does not dampen positive values:<p class="snippet _idGenParaOverride-2">get_y(relu,-2)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">0.0</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(relu,-1)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">1.5</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(relu,0)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">3.0</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(relu,1)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">4.5</strong>
</p>
<p class="snippet _idGenParaOverride-2">get_y(relu,2)</p>
<p class="_idGenParaOverride-3">The output is <strong class="inline _idGenCharOverride-2">6.0</strong>
</p>
<p class="_idGenParaOverride-3">Another advantage of the <strong class="inline _idGenCharOverride-2">ReLU</strong>
 function is that its calculation is the easiest out of all of the activator functions.</p>
</li>
</ol>
<h3 id="_idParaDest-170"><a id="_idTextAnchor181"></a>
 Forward and Backward Propagation</h3>
<p>As artificial neural networks provide a supervised-learning technique, we have to train our model using training data. Training the network is the process of finding the weights belonging to each variable-input pair. The process of weight optimization consists of the repeated execution of two steps: forward propagation and backward propagation.</p>
<p>The names forward and backward propagation imply how these techniques work. We start by initializing the weights on the arrows of the neural network. Then, we apply forward propagation, followed by backward propagation.</p>
<p class="_idGenParaOverride-1">
<strong class="keyword _idGenCharOverride-1">Forward propagation</strong>
 calculates output values based on input values. <strong class="keyword _idGenCharOverride-1">Backward propagation</strong>
 adjusts the weights and biases based on the margin of error measured between the label values created by the model and the actual label values in the training data. The rate of adjustment of the weights depend on the learning rate of the neural network. The higher the learning rate, the more the weights and biases are adjusted during the backward propagation. The momentum of the neural network determines how past results influence the upcoming values of weights and biases.</p>
<div></div>
<h3 id="_idParaDest-171"><a id="_idTextAnchor182"></a>
 Configuring a Neural Network</h3>
<p>The following parameters are commonly used to create a neural network:</p>
<ul>
<li>Number of hidden layers</li>
<li>Number of nodes per hidden layer</li>
<li>Activation function</li>
<li>Learning rate</li>
<li>Momentum</li>
<li>Number of iterations for forward and backward propagation</li>
<li>Tolerance for error</li>
</ul>
<p>There are a few rules of thumb that can be used to determine the number of nodes per hidden layer. If your hidden layer contains more nodes than the size of your input, you risk overfitting the model. Often, a node count somewhere between the number of inputs and the number of outputs is reasonable.</p>
<h3 id="_idParaDest-172"><a id="_idTextAnchor183"></a>
 Importing the TensorFlow Digit Dataset</h3>
<p>Recognition of hand-written digits seems to be a simple task at first glance. However, this task is a simple classification problem with ten possible label values. TensorFlow provides an example dataset for the recognition of digits.</p>
<h4>Note</h4>
<p class="callout">You can read about this dataset on TensorFlow's website here: <a href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a>
 .</p>
<p>We will use <strong class="inline _idGenCharOverride-2">keras</strong>
 to load the dataset. You can install it in the Anaconda Prompt by using the following command:</p>
<p class="snippet">pip install keras</p>
<p>Remember, we will perform supervised learning on these datasets, so we will need training and testing data:</p>
<p class="snippet">import tensorflow.keras.datasets.mnist as mnist</p>
<p class="snippet">(features_train, label_train),(features_test, label_test) =</p>
<p class="snippet _idGenParaOverride-1">mnist.load_ data()</p>
<div></div>
<p>The features are arrays containing the pixel values of a 28x28 image. The labels are one-digit integers between 0 and 9. Let's see the features and the label of the fifth element. We will use the same image library that we used in the previous section:</p>
<p class="snippet">from PIL import Image</p>
<p class="snippet">Image.fromarray(features_train[5])</p>
<h6><img class="_idGenObjectAttribute-2" src="Image00071.jpg" alt="Fig 7.11 Image for training" />
</h6>
<h6>Fig 7.11: Image for training</h6>
<p class="snippet">label_train[5]</p>
<p class="snippet">2</p>
<p>In the activity at the end of this chapter, your task will be to create a neural network to classify these handwritten digits based on their values.</p>
<h3 id="_idParaDest-173"><a id="_idTextAnchor184"></a>
 Modeling Features and Labels</h3>
<p>We will go through the example of modeling features and labels for recognizing written numbers in the TensorFlow digit dataset.</p>
<p>We have a 28x28 pixel image as our input. The value of each image is either black or white. The feature set therefore consists of a vector of 28 * 28 = 784 pixels.</p>
<p>The images are grayscale and consist of images with colors ranging from 0 to 255. To process them, we need to scale the data. By dividing the training and testing features by 255.0, we ensure that our features are scaled between 0 and 1:</p>
<p class="snippet">features_train = features_train / 255.0</p>
<p class="snippet">features_test = features_test / 255.0</p>
<p>Notice that we could have a 28x28 square matrix to describe the features, but we would rather flatten the matrix and simply use a vector. This is because the neural network model normally handles one-dimensional data.</p>
<p class="_idGenParaOverride-1">Regarding the modeling of labels, many people think that it makes the most sense to model this problem with just one label: an integer value ranging from 0 to 9. This approach is problematic, because small errors in the calculation may result in completely different digits. We can imagine that a 5 is similar to a 6, so the adjacent values work really well here. However, in the case of 1 and 7, a small error may make the neural network realize a 1 as a 2, or a 7 as a 6. This is highly confusing, and it may take a lot more time to train the neural network to make less errors with adjacent values.</p>
<div></div>
<p>More importantly, when our neural network classifier comes back with a result of 4.2, we may have as much trouble interpreting the answer as the hero in <em class="italics _idGenCharOverride-3">The Hitchhiker's Guide to the Galaxy.</em>
 4.2 is most likely a 4. But if not, maybe it is a 5, or a 3, or a 6. This is not how digit detection works.</p>
<p>Therefore, it makes more sense to model this task using a vector of ten labels. When using TensorFlow for classification, it makes perfect sense to create one label for each possible class, with values ranging between 0 and 1. These numbers describe probabilities that the read digit is classified as a member of the class the label represents.</p>
<p>For instance, the value <strong class="inline _idGenCharOverride-2">[0, 0.1, 0, 0, 0.9, 0, 0, 0, 0, 0]</strong>
 indicates that our digit has a 90% of being a 4, and a 10% chance of it being a 2.</p>
<p>In case of classification problems, we always use one output value per class.</p>
<p>Let's continue with the weights and biases. To connect 28*28 = 784 features and 10 labels, we need a 784 x 10 matrix of weights that has 784 rows and 10 columns.</p>
<p>Therefore, the equation becomes <strong class="inline _idGenCharOverride-2">y = f( x ⋅ W + b )</strong>
 , where x is a vector in a 784-dimensional space, W is a 784 x 10 matrix, and b is a vector of biases in ten dimensions. The y vector also contains ten coordinates. The f function is defined on vectors with ten coordinates, and it is applied on each coordinate.</p>
<h4>Note</h4>
<p class="callout">To transform a two-dimensional 28x28 matrix of data points to a one-dimensional vector of 28x28 elements, we need to flatten the matrix. As opposed to many other languages and libraries, Python does not have a flatten method.</p>
<p>Since flattening is an easy task, let's construct a flatten method:</p>
<p class="snippet">def flatten(matrix):</p>
<p class="snippet">    return [elem for row in matrix for elem in row]</p>
<p class="snippet">flatten([[1,2],[3,4]])</p>
<p>The output is as follows:</p>
<p class="snippet _idGenParaOverride-1"> [1, 2, 3, 4]</p>
<div></div>
<p>Let's flatten the features from a 28*28 matrix to a vector of a 784-dimensional space:</p>
<p class="snippet">features_train_vector = [</p>
<p class="snippet">    flatten(image) for image in features_train</p>
<p class="snippet">]</p>
<p class="snippet">features_test_vector = [</p>
<p class="snippet">    flatten(image) for image in features_test</p>
<p class="snippet">]</p>
<p>To transfer the labels to a vector form, we need to perform normalization:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">label_train_vector = np.zeros((label_train.size, 10))</p>
<p class="snippet">for i, label in enumerate(label_train_vector):</p>
<p class="snippet">    label[label_train[i]] = 1</p>
<p class="snippet">label_test_vector = np.zeros((label_test.size, 10))</p>
<p class="snippet">for i, label in enumerate(label_test_vector):</p>
<p class="snippet">    label[label_test[i]] = 1</p>
<h3 id="_idParaDest-174"><a id="_idTextAnchor185"></a>
 TensorFlow Modeling for Multiple Labels</h3>
<p>We will now model the following equation in TensorFlow: <strong class="inline _idGenCharOverride-2">y = f( x ⋅ W + b )</strong>
</p>
<p>After importing TensorFlow, we will define the features, labels, and weights:</p>
<p class="snippet">import tensorflow as tf</p>
<p class="snippet">f = tf.nn.sigmoid</p>
<p class="snippet">x = tf.placeholder(tf.float32, [None, 28 * 28])</p>
<p class="snippet">W = tf.Variable(tf.random_normal([784, 10]))</p>
<p class="snippet">b = tf.Variable(tf.random_normal([10]))</p>
<p>We can simply write the equation <strong class="inline _idGenCharOverride-2">y = f( x ⋅ W + b )</strong>
 if we know how to perform dot product multiplication using TensorFlow.</p>
<p class="_idGenParaOverride-1">If we treat x as a <em class="italics _idGenCharOverride-3">1x84</em>
 matrix, we can multiply it with the <em class="italics _idGenCharOverride-3">784x10</em>
 W matrix using the <strong class="inline _idGenCharOverride-2">tf.matmul</strong>
 function.</p>
<div></div>
<p>Therefore, our equation becomes the following: <strong class="inline _idGenCharOverride-2">y = f( tf.add( tf.matmul( x, W ), b ) )</strong>
</p>
<p>You might have noticed that x contains placeholders, while W and b are variables. This is because the values of x are given. We just need to substitute them in the equation. The task of TensorFlow is to optimize the values of W and b so that we maximize the probability that we read the right digi<a id="_idTextAnchor186"></a>
 ts.</p>
<p>Let's express the calculation of y in a function form:</p>
<p class="snippet">def classify(x):</p>
<p class="snippet">    return f(tf.add(tf.matmul(x, W), b))</p>
<h4>Note</h4>
<p class="callout">This is the place where we can define the activator function. In the activity at the end of this chapter, you are better off using the softmax activator function. This implies that you will have to replace sigmoid with softmax in the code: <strong class="inline _idGenCharOverride-4">f = tf.nn.softmax</strong>
</p>
<h3 id="_idParaDest-175"><a id="_idTextAnchor187"></a>
 Optimizing the Variables</h3>
<p>Placeholders symbolize the input. The task of TensorFlow is to optimize the variables.</p>
<p>To perform optimization, we need to use a cost function: cross-entropy. Cross-entropy has the following properties:</p>
<ul>
<li>Its value is zero if the predicted output matches the real output</li>
<li>Its value is strictly positive afterward</li>
</ul>
<p>Our task is to minimize cross-entropy:</p>
<p class="snippet">y = classify(x)</p>
<p class="snippet">y_true = tf.placeholder(tf.float32, [None, 10])</p>
<p class="snippet">cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(</p>
<p class="snippet">    logits=y,</p>
<p class="snippet">    labels=y_true</p>
<p class="snippet _idGenParaOverride-1">)</p>
<div></div>
<p>Although the function computing y is called classify, we do not perform the actual classification here. Remember, we are using placeholders in the place of x, and the actual values are substituted while running the TensorFlow session.</p>
<p>The <strong class="inline _idGenCharOverride-2">sigmoid_cross_entropy_with_logits</strong>
 function takes two arguments to compare their values. The first argument is the label value, while the second argument is the result of the prediction.</p>
<p>To calculate the cost, we have to call the <strong class="inline _idGenCharOverride-2">reduce_mean</strong>
 method of TensorFlow:</p>
<p class="snippet">cost = tf.reduce_mean(cross_entropy)</p>
<p>Minimization of the cost goes through an optimizer. We will use the <strong class="inline _idGenCharOverride-2">GradientDescentOptimizer</strong>
 with a learning rate. The learning rate is a parameter of the Neural Network that influences how fast the model adjusts:</p>
<p class="snippet">optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(cost)</p>
<p>Optimization is not performed at this stage, as we are not running TensorFlow yet. We will perform optimization in the main loop.</p>
<p>If you are using a different activator function such as softmax, you will have to replace it in the source code. Instead of the following statement:</p>
<p class="snippet">cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(</p>
<p class="snippet">    logits=y,</p>
<p class="snippet">    labels=y_true</p>
<p class="snippet">)</p>
<p>Use the following:</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-2">cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(</strong>
</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-2">    logits=y,</strong>
</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-2">    labels=y_true</strong>
</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-2">)</strong>
</p>
<h4>Note</h4>
<p class="callout _idGenParaOverride-1">The _v2 suffix in the method name. This is because the original <strong class="inline _idGenCharOverride-4">tf.nn.softmax_cross_entropy_with_logits</strong>
 method is deprecated.</p>
<div></div>
<h3 id="_idParaDest-176"><a id="_idTextAnchor188"></a>
 Training the TensorFlow Model</h3>
<p>We need to create a TensorFlow session and run the model:</p>
<p class="snippet">session = tf.Session()</p>
<p>First, we initialize the variables using <strong class="inline _idGenCharOverride-2">tf.global_variables_initializer()</strong>
 :</p>
<p class="snippet">session.run(tf.global_variables_initializer())</p>
<p>Then comes the optimization loop. We will determine the number of iterations and a batch size. In each iteration, we will randomly select a number of feature-label pairs equal to the batch size.</p>
<p>For demonstration purposes, instead of creating random batches, we will simply feed the upcoming hundred images each time a new iteration is started.</p>
<p>As we have 60,000 images in total, we could have up to 300 iterations and 200 images per iteration. In reality, we will only run a few iterations, which means that we will only use a fraction of the available training data:</p>
<p class="snippet">iterations = 300</p>
<p class="snippet">batch_size = 200</p>
<p class="snippet">for i in range(iterations):</p>
<p class="snippet">    min = i * batch_size</p>
<p class="snippet">    max = (i+1) * batch_size</p>
<p class="snippet">    dictionary = {</p>
<p class="snippet">        x: features_train_vector[min:max],</p>
<p class="snippet">        y_true: label_train_vector[min:max]</p>
<p class="snippet">    }</p>
<p class="snippet">    session.run(optimizer, feed_dict=dictionary)</p>
<p class="snippet">    print('iteration: ', i)</p>
<h3 id="_idParaDest-177"><a id="_idTextAnchor189"></a>
 Using the Model for Prediction</h3>
<p>We can now use the trained model to perform prediction. The syntax is straightforward: we feed the test features to the dictionary of the session, and request the <strong class="inline _idGenCharOverride-2">classify(x)</strong>
 value:</p>
<p class="snippet">session.run(classify(x), feed_dict={</p>
<p class="snippet">    x: features_test_vector[:10]</p>
<p class="snippet">} )</p>
<h3 id="_idParaDest-178"><a id="_idTextAnchor190"></a>
 Testing the Model</h3>
<p>Now that our model has been trained and we can use it for prediction, it is time to test its performance:</p>
<p class="snippet">label_predicted = session.run(classify(x), feed_dict={</p>
<p class="snippet">    x: features_test_vector</p>
<p class="snippet">})</p>
<p>We have to transfer the <strong class="inline _idGenCharOverride-2">labelsPredicted</strong>
 values back to integers ranging from 0 to 9 by taking the index of the largest value from each result. We will use a NumPy function to perform this transformation.</p>
<p>The <strong class="inline _idGenCharOverride-2">argmax</strong>
 function returns the index of its list or array argument that has the maximum value. The following is an example of this:</p>
<p class="snippet">np.argmax([0.1, 0.3, 0.5, 0.2, 0, 0, 0, 0.2, 0, 0 ])</p>
<p>The output is <strong class="inline _idGenCharOverride-2">2</strong>
 .</p>
<p>Here is the second example with <strong class="inline _idGenCharOverride-2">argmax</strong>
 functions</p>
<p class="snippet">np.argmax(<a id="_idTextAnchor191"></a>
 [1, 0, 1])</p>
<p>The output is <strong class="inline _idGenCharOverride-2">0</strong>
 .</p>
<p>Let's perform the transformation:</p>
<p class="snippet">label_predicted = [</p>
<p class="snippet">    np.argmax(label) for label in label_predicted</p>
<p class="snippet">]</p>
<p>We can use the metrics that we learned about in the previous chapters using scikit-learn. Let's calculate the confusion matrix first:</p>
<p class="snippet">from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score</p>
<p class="snippet">confusion_matrix(label_test, label_predicted)</p>
<p class="snippet">accuracy_score(label_test, label_predicted)</p>
<p class="snippet">precision_score( label_test, label_predicted, average='weighted' )</p>
<p class="snippet">recall_score( label_test, label_predicted, average='weighted' )</p>
<p class="snippet _idGenParaOverride-1">f1_score( label_test, label_predicted, average='weighted' )</p>
<div></div>
<h3 id="_idParaDest-179"><a id="_idTextAnchor192"></a>
 Randomizing the Sample Size</h3>
<p>Recall the training function of the neural network:</p>
<p class="snippet">iterations = 300</p>
<p class="snippet">batch_size = 200</p>
<p class="snippet">for i in range(iterations):</p>
<p class="snippet">    min = i * batch_size</p>
<p class="snippet">    max = (i+1) * batch_size</p>
<p class="snippet">    dictionary = {</p>
<p class="snippet">        x: features_train_vector[min:max],</p>
<p class="snippet">        y_true: label_train_vector[min:max]</p>
<p class="snippet">    }</p>
<p class="snippet">    session.run(optimizer, feed_dict=dictionary)</p>
<p>The problem is that out of 60,000 numbers, we can only take 5 iterations. If we want to go beyond this threshold, we would run the risk of repeating these input sequences.</p>
<p>We can maximize the effectiveness of using the training data by randomly selecting the values out of the training data.</p>
<p>We can use the <strong class="inline _idGenCharOverride-2">random.sample</strong>
 method for this purpose:</p>
<p class="snippet">iterations = 6000</p>
<p class="snippet">batch_size = 100</p>
<p class="snippet">sample_size = len(features_train_vector)</p>
<p class="snippet">for _ in range(iterations):</p>
<p class="snippet">    indices = random.sample(range(sample_size), batchSize)</p>
<p class="snippet">    batch_features = [</p>
<p class="snippet">        features_train_vector[i] for i in indices</p>
<p class="snippet">    ]</p>
<p class="snippet">    batch_labels = [</p>
<p class="snippet">        label_train_vector[i] for i in indices</p>
<p class="snippet">    ]</p>
<p class="snippet">    min = i * batch_size</p>
<p class="snippet">    max = (i+1) * batch_size</p>
<p class="snippet">    dictionary = {</p>
<p class="snippet">        x: batch_features,</p>
<p class="snippet">        y_true: batch_labels</p>
<p class="snippet">    }</p>
<p class="snippet">    session.run(optimizer, feed_dict=dictionary)</p>
<h4>Note</h4>
<p class="callout">The random sample method randomly selects a given number of elements out of a list. For instance, in Hungary, the main national lottery works based on selecting 5 numbers out of a pool of 90. We can simulate a lottery round using the following expression:</p>
<p class="snippet">import random</p>
<p class="snippet">random.sample(range(1,91), 5)</p>
<p>The output is as follows:</p>
<p class="snippet">[63, 58, 25, 41, 60]</p>
<h3 id="_idParaDest-180"><a id="_idTextAnchor193"></a>
 Activity 14: Written Digit Detection</h3>
<p>In this se<a id="_idTextAnchor194"></a>
 ction, we will discuss how to provide more security for cryptocurrency traders via the detection of hand-written digits. We will be using assuming that you are a software developer at a new cryptocurrency trader platform. The latest security measure you are implementing requires the recognition of hand-written digits. Use the MNIST library to train a neural network to recognize digits. You can read more about this dataset at <a href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a>
 .</p>
<p>Improve the accuracy of the model as much as possible by performing the following steps:</p>
<ol>
<li class="ParaOverride-1" value="1">Load the dataset and format the input.</li>
<li value="2">Set up the TensorFlow graph. Instead of the sigmoid function, we will now use the <strong class="inline _idGenCharOverride-2">ReLU</strong>
 function.</li>
<li value="3">Train the model.</li>
<li value="4">Test the model and calculate the accuracy score.</li>
<li value="5">By re-running the code segment that's responsible for training the dataset, we can improve its accuracy. Run the code 50 times.</li>
<li value="6">Print the confusion matrix.</li>
</ol>
<p>At the end of the fiftieth run, the confusion matrix has improved.</p>
<p>Not a bad result. More than 8 out of 10 digits were accurately recognized.</p>
<h4>Note</h4>
<p class="callout">The solution for this activity can be found on page 298.</p>
<p>As you can see, neural networks do not improve linearly. It may appear that training the network brings little to no incremental improvement in accuracy for a while. Yet, after a certain threshold, a breakthrough happens, and the accuracy greatly increases.</p>
<p>This behavior is analogous with studying for humans. You might also have trouble with neural networks right now. However, after getting deeply immersed in the material and trying a few exercises out, you will reach breakthrough after breakthrough, and your progress will speed up.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor195"></a>
 Deep Learning</h2>
<p>In this topic, we will increase the number of layers of the neural network. You may remember that we can add hidden layers to our graph. We will target improving the accuracy of our model by experimenting with hidden layers.</p>
<h3 id="_idParaDest-182"><a id="_idTextAnchor196"></a>
 Adding Layers</h3>
<p>Recall the diagram of neural networks with two hidden layers:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer110" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00072.jpg" alt="Figure 7.12 Diagram showing two hidden layers in a neural network" />
</div>
</div>
<h6>Figure 7.12: Diagram showing two hidden layers in a neural network</h6>
<p>We can add a second layer to the equation by duplicating the weights and biases and making sure that the dimensions of the TensorFlow variables match. Note that in the first model, we transformed 784 features into 10 labels.</p>
<p>In this model, we will transform 784 features into a specified number of outputs. We will then take these outputs and transform them into 10 labels.</p>
<p>Determining the node count of the added hidden layer is not exactly science. We will use a count of 200 in this example, as it is somewhere between the feature and label dimensions.</p>
<p>As we have two layers, we will define two matrices <strong class="inline _idGenCharOverride-2">(W1, W2)</strong>
 and vectors <strong class="inline _idGenCharOverride-2">(b1, b2)</strong>
 for the weights and biases, respectively.</p>
<p>First, we reduce the 784 input dots using <strong class="inline _idGenCharOverride-2">W1</strong>
 and <strong class="inline _idGenCharOverride-2">b1</strong>
 , and create 200 variable values. We feed these values as the input of the second layer and use <strong class="inline _idGenCharOverride-2">W2</strong>
 and <strong class="inline _idGenCharOverride-2">b2</strong>
 to create 10 label values:</p>
<p class="snippet">x = tf.placeholder(tf.float32, [None, 28 * 28 ])</p>
<p class="snippet">f = tf.nn.softmax</p>
<p class="snippet">W1 = tf.Variable(tf.random_normal([784, 200]))</p>
<p class="snippet">b1 = tf.Variable(tf.random_normal([200]))</p>
<p class="snippet">layer1_out = f(tf.add( tf.matmul(x, W1), b1))</p>
<p class="snippet">W2 = tf.Variable(tf.random_normal([200, 10]))</p>
<p class="snippet">b2 = tf.Variable(tf.random_normal([10]))</p>
<p class="snippet">y = f(tf.add(tf.matmul(layer1_out, W2), b2))</p>
<p>We can increase the number of layers if needed in this way. The output of layer n must be the input of layer n+1. The rest of the code remains as it is.</p>
<h3 id="_idParaDest-183"><a id="_idTextAnchor197"></a>
 Convolutional Neural Networks</h3>
<p class="_idGenParaOverride-1">
<strong class="keyword _idGenCharOverride-1">Convolutional Neural Networks</strong>
 (<strong class="keyword _idGenCharOverride-1">CNNs</strong>
 ) are artificial neural networks that are optimized for pattern recognition. CNNs are based on convolutional layers that are among the hidden layers of the deep neural network. A convolutional layer consists of neurons that transform their inputs using a convolution operation.</p>
<div></div>
<p>When using a convolution layer, we detect patterns in the image with an m*n matrix, where m and n are less than the width and the height of the image, respectively. When performing the convolution operation, we slide this m*n matrix over the image, matching every possibility. We calculate the scalar product of the m*n convolution filter and the pixel values of the 3x3 segment of the image our convolution filter is currently on. The convolution operation creates a new image from the original one, where the important aspects of our image are highlighted, and the less-important ones are blurred.</p>
<p>The convolution operation summarizes information on the window it is looking at. Therefore, it is an ideal operator for recognizing shapes in an image. Shapes can be anywhere on the image, and the convolution operator recognizes similar image information regardless of its exact position and orientation. Convolutional neural networks are outside the scope of this book, because it is a more advanced topic.</p>
<h3 id="_idParaDest-184"><a id="_idTextAnchor198"></a>
 Activity 15: Written Digit Detection with Deep Learning</h3>
<p>In this section, we will discuss how deep learning improves the performance of your model. We will be assuming that your boss is not satisfied with the results you presented in Activity 14 and has asked you to consider adding two hidden layers to your original model to determine whether new layers improve the accuracy of the model. To ensure that you are able to complete this activity correctly, you will need to be knowledgeable of deep learning:</p>
<ol>
<li class="ParaOverride-1" value="1">Execute the steps from the previous activity and measure the accuracy of the model.</li>
<li value="2">Change the neural network by adding new layers. We will combine the <strong class="inline _idGenCharOverride-2">ReLU</strong>
 and <strong class="inline _idGenCharOverride-2">softmax</strong>
 activator functions.</li>
<li value="3">Retrain the model.</li>
<li value="4">Evaluate the model. Find the accuracy score.</li>
<li value="5">Run the code 50 times.</li>
<li value="6">Print the confusion matrix.</li>
</ol>
<p class="_idGenParaOverride-1">This deep neural network behaves even more chaotically than the single layer one. It took 600 iterations of 200 samples to get from an accuracy of 0.572 to 0.5723. Not long after this iteration, we jumped from 0.6076 to 0.6834 in the same number of iterations.</p>
<div></div>
<p>Due to the flexibility of the deep neural network, we expect to reach an accuracy ceiling later than in the case of the simple model. Due to the complexity of a deep neural network, it is also more likely that it gets stuck at a local maximum for a long time.</p>
<h4>Note</h4>
<p class="callout">The solution for this activity can be found on page 302.</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor199"></a>
 Summary</h2>
<p>In this book, we have learned about the fundamentals of AI and applications of AI in chapter on principles of AI, then we wrote a Python code to model a Tic-Tac-Toe game.</p>
<p>In the chapter AI with Search Techniques and Games, we solved the Tic-Tac-Toe game with game AI tools and search techniques. We learned about the search algorithms of Breadth First Search and Depth First Search. The A* algorithm helped students model a pathfinding problem. The chapter was concluded with modeling multiplayer games.</p>
<p>In the next couple of chapters, we learned about supervised learning using regression and classification. These chapters included data preprocessing, train-test splitting, and models that were used in several real-life scenarios. Linear regression, polynomial regression, and Support Vector Machines all came in handy when it came to predicting stock data. Classification was performed using the k-nearest neighbor and Support Vector classifiers. Several activities helped students apply the basics of classification an interesting real-life use case: credit scoring.</p>
<p>In <em class="italics _idGenCharOverride-3">Chapter 5</em>
 , <em class="italics _idGenCharOverride-3">Using Trees for Predictive Analysis</em>
 , we were introduced to decision trees, random forests, and extremely randomized trees. This chapter introduced different means to evaluating the utility of models. We learned how to calculate the accuracy, precision, recall, and F1 Score of models. We also learned how to create the confusion matrix of a model. The models of this chapter were put into practice through the evaluation of car data.</p>
<p>Unsupervised learning was introduced in <em class="italics _idGenCharOverride-3">Chapter 6</em>
 , <em class="italics _idGenCharOverride-3">Clustering</em>
 , along with the k-means and mean shift clustering algorithms. One interesting aspect of these algorithms is that the labels are not given in advance, but they are detected during the clustering process.</p>
<p>This book was concluded with Chapter 7, <em class="italics _idGenCharOverride-3">Deep Learning with Neural Networks</em>
 , where neural networks and deep learning using TensorFlow was presented. We used these techniques on a real-life example: the detection of written digits.</p>
</div>
</body></html>