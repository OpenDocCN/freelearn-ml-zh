- en: Chapter 3. Supervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 监督机器学习
- en: In this chapter, the most relevant regression and classification techniques
    are discussed. All of these algorithms share the same background procedure, and
    usually the name of the algorithm refers to both a classification and a regression
    method. The linear regression algorithms, Naive Bayes, decision tree, and support
    vector machine are going to be discussed in the following sections. To understand
    how to employ the techniques, a classification and a regression problem will be
    solved using the mentioned methods. Essentially, a labeled train dataset will
    be used to *train the models* , which means to find the values of the parameters,
    as we discussed in the introduction. As usual, the code is available in my GitHub
    folder at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    .
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论最相关的回归和分类技术。所有这些算法都有相同的背景过程，通常算法的名称既指分类方法也指回归方法。以下几节将讨论线性回归算法、朴素贝叶斯、决策树和支持向量机。为了理解如何应用这些技术，我们将使用提到的方法解决一个分类问题和回归问题。本质上，我们将使用标记的训练数据集来**训练模型**，这意味着找到参数的值，正如我们在引言中所讨论的。通常，代码可在我的GitHub文件夹中找到，网址为[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)。
- en: We will conclude the chapter with an extra algorithm that may be used for classification,
    although it is not specifically designed for this purpose (hidden Markov model).
    We will now begin to explain the general causes of error in the methods when predicting
    the true labels associated with a dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一个额外的算法结束本章，这个算法可能用于分类，尽管它不是专门为此目的设计的（隐马尔可夫模型）。现在，我们将开始解释在预测与数据集相关的真实标签时，方法中错误的一般原因。
- en: Model error estimation
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型误差估计
- en: 'We said that the trained model is used to predict the labels of new data, and
    the quality of the prediction depends on the ability of the model to *generalize*
    , that is, the correct prediction of cases not present in the trained data. This
    is a well-known problem in literature and related to two concepts: bias and variance
    of the outputs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到，训练好的模型用于预测新数据的标签，预测的质量取决于模型**泛化**的能力，即对训练数据中未出现的案例的正确预测。这是文献中一个众所周知的问题，与两个概念相关：输出的偏差和方差。
- en: 'The bias is the error due to a wrong assumption in the algorithm. Given a point
    *x^((t))* with label *y[t]* , the model is biased if it is trained with different
    training sets, and the predicted label *y[t] ^(pred)* will always be different
    from *y[t]* . The variance error instead refers to the different, wrongly predicted
    labels of the given point *x^((t))* . A classic example to explain the concepts
    is to consider a circle with the true value at the center (true label), as shown
    in the following figure. The closer the predicted labels are to the center, the
    more unbiased the model and the lower the variance (top left in the following
    figure). The other three cases are also shown here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是由于算法中的错误假设而产生的错误。给定一个带有标签 *y[t]* 的点 *x^((t))*，如果模型用不同的训练集进行训练，那么模型是有偏差的，预测的标签
    *y[t] ^(pred)* 将始终与 *y[t]* 不同。方差误差则是指给定点 *x^((t))* 的不同、错误预测的标签。为了解释这些概念，可以考虑一个以真实值为中心的圆（真实标签），如下面的图所示。预测标签越接近中心，模型越无偏，方差越低（如下面图中的左上角）。其他三种情况也在此处展示：
- en: '![Model error estimation](img/Image00141.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: 偏差和方差低误差的模型将具有预测标签为蓝色点（如图所示）集中在红色中心（真实标签）上。高偏差误差发生在预测远离真实标签时，而高方差出现在预测值范围很广时。
- en: Variance and bias example.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![模型误差估计](img/Image00141.jpg)'
- en: A model with low variance and low bias errors will have the predicted labels
    that is blue dots (as show in the preceding figure) concentrated on the red center
    (true label). The high bias error occurs when the predictions are far away from
    the true label, while high variance appears when the predictions are in a wide
    range of values.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差示例。
- en: 'We have already seen that labels can be continuous or discrete, corresponding
    to regression classification problems respectively. Most of the models are suitable
    for solving both problems, and we are going to use word regression and classification
    referring to the same model. More formally, given a set of *N* data points and
    corresponding labels ![Model error estimation](img/Image00142.jpg) , a model with
    a set of parameters ![Model error estimation](img/Image00143.jpg) with the true
    parameter values ![Model error estimation](img/Image00144.jpg) will have the **mean
    square error** ( **MSE** ), equal to:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，标签可以是连续的或离散的，分别对应回归和分类问题。大多数模型都适合解决这两个问题，我们将使用词回归和分类来指代同一个模型。更正式地说，给定一组
    *N* 个数据点和相应的标签 ![模型误差估计](img/Image00142.jpg) ，一个具有一组参数 ![模型误差估计](img/Image00143.jpg)
    的真实参数值 ![模型误差估计](img/Image00144.jpg) 的模型将具有 **均方误差** ( **MSE** )，等于：
- en: '![Model error estimation](img/Image00145.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![模型误差估计](img/Image00145.jpg)'
- en: We will use the MSE as a measure to evaluate the methods discussed in this chapter.
    Now we will start describing the generalized linear methods.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用均方误差（MSE）作为衡量本章讨论的方法的指标。现在我们将开始描述广义线性方法。
- en: Generalized linear models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广义线性模型
- en: 'The generalized linear model is a group of models that try to find the *M*
    parameters ![Generalized linear models](img/Image00146.jpg) that form a linear
    relationship between the labels *y[i] * and the feature vector *x^((i)) * that
    is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 广义线性模型是一组试图找到形成标签 *y[i]* 与特征向量 *x^((i))* 之间线性关系的 *M* 参数 ![广义线性模型](img/Image00146.jpg)
    的模型，如下所示：
- en: '![Generalized linear models](img/Image00147.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/Image00147.jpg)'
- en: 'Here, ![Generalized linear models](img/Image00148.jpg) are the errors of the
    model. The algorithm for finding the parameters tries to minimize the total error
    of the model defined by the cost function *J* :'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![广义线性模型](img/Image00148.jpg) 是模型的误差。寻找参数的算法试图最小化由成本函数 *J* 定义的模型的总误差：
- en: '![Generalized linear models](img/Image00149.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/Image00149.jpg)'
- en: 'The minimization of *J* is achieved using an iterative algorithm called **batch
    gradient descent** :'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用称为 **批量梯度下降** 的迭代算法来最小化 *J* ：
- en: '![Generalized linear models](img/Image00150.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/Image00150.jpg)'
- en: 'Here, α is called learning rate, and it is a trade-off between convergence
    speed and convergence precision. An alternative algorithm that is called **stochastic
    gradient descent** , that is loop for ![Generalized linear models](img/Image00151.jpg)
    :'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α 被称为学习率，它是收敛速度和收敛精度之间的权衡。一个被称为 **随机梯度下降** 的替代算法，即 ![广义线性模型](img/Image00151.jpg)
    的循环：
- en: '![Generalized linear models](img/Image00152.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/Image00152.jpg)'
- en: The θ[j] is updated for each training example *i* instead of waiting to sum
    over the entire training set. The last algorithm converges near the minimum of
    *J* , typically faster than batch gradient descent, but the final solution may
    oscillate around the real values of the parameters. The following paragraphs describe
    the most common model ![Generalized linear models](img/Image00153.jpg) and the
    corresponding cost function, *J* .
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: θ[j] 对于每个训练示例 *i* 进行更新，而不是等待整个训练集的总和。最后一个算法接近 *J* 的最小值，通常比批量梯度下降更快，但最终解可能围绕参数的真实值振荡。以下段落描述了最常用的模型
    ![广义线性模型](img/Image00153.jpg) 和相应的成本函数 *J* 。
- en: Linear regression
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'Linear regression is the simplest algorithm and is based on the model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是最简单的算法，基于以下模型：
- en: '![Linear regression](img/Image00154.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Image00154.jpg)'
- en: 'The cost function and update rule are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数和更新规则是：
- en: '![Linear regression](img/Image00155.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Image00155.jpg)'
- en: Ridge regression
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 岭回归
- en: 'Ridge regression, also known as **Tikhonov regularization** , adds a term to
    the cost function *J* such that:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归，也称为 **Tikhonov 正则化**，在成本函数 *J* 中添加了一个项，使得：
- en: '![Ridge regression](img/Image00156.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/Image00156.jpg)'
- en: '![Ridge regression](img/Image00157.jpg) , where λ is the regularization parameter.
    The additional term has the function needed to prefer a certain set of parameters
    over all the possible solutions penalizing all the parameters θ[j] different from
    *0* . The final set of θ[j] *shrank* around *0* , lowering the variance of the
    parameters but introducing a bias error. Indicating with the superscript *l* the
    parameters from the linear regression, the ridge regression parameters are related
    by the following formula:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![岭回归](img/Image00157.jpg)，其中λ是正则化参数。附加项具有所需的功能，以偏好一组参数而不是所有可能的解决方案，惩罚所有不同于*0*的参数θ[j]。θ[j]的最终集合*收缩*在*0*周围，降低了参数的方差，但引入了偏差误差。用上标*l*表示线性回归的参数，岭回归参数通过以下公式相关：'
- en: '![Ridge regression](img/Image00158.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/Image00158.jpg)'
- en: This clearly shows that the larger the λ value, the more the ridge parameters
    are shrunk around *0* .
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明，λ值越大，岭参数围绕*0*的收缩越多。
- en: Lasso regression
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lasso回归
- en: 'Lasso regression is an algorithm similar to ridge regression, the only difference
    being that the regularization term is the sum of the absolute values of the parameters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归是一个类似于岭回归的算法，唯一的区别在于正则化项是参数绝对值的总和：
- en: '![Lasso regression](img/Image00159.jpg)![Lasso regression](img/Image00157.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![Lasso回归](img/Image00159.jpg)![Lasso回归](img/Image00157.jpg)'
- en: Logistic regression
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Despite the name, this algorithm is used for (binary) classification problems,
    so we define the labels![Logistic regression](img/Image00160.jpg) . The model
    is given the so-called logistic function expressed by:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称如此，此算法用于（二元）分类问题，因此我们定义标签![逻辑回归](img/Image00160.jpg)。模型被赋予所谓的逻辑函数，表示为：
- en: '![Logistic regression](img/Image00161.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/Image00161.jpg)'
- en: 'In this case, the cost function is defined as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，代价函数定义为以下：
- en: '![Logistic regression](img/Image00162.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/Image00162.jpg)'
- en: 'From this, the update rule is formally the same as linear regression (but the
    model definition, ![Logistic regression](img/Image00163.jpg) , is different):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个公式中，更新规则形式上与线性回归相同（但模型定义![逻辑回归](img/Image00163.jpg)不同）：
- en: '![Logistic regression](img/Image00164.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/Image00164.jpg)'
- en: 'Note that the prediction for a point *p* , ![Logistic regression](img/Image00165.jpg)
    , is a continuous value between *0* and *1* . So usually, to estimate the class
    label, we have a threshold at ![Logistic regression](img/Image00165.jpg) =0.5
    such that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于点*p*的预测 ![逻辑回归](img/Image00165.jpg)是一个介于*0*和*1*之间的连续值。因此，通常，为了估计类标签，我们在![逻辑回归](img/Image00165.jpg)
    =0.5处设置一个阈值，使得：
- en: '![Logistic regression](img/Image00166.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/Image00166.jpg)'
- en: The logistic regression algorithm is applicable to multiple label problems using
    the techniques one versus all or one versus one. Using the first method, a problem
    with *K* classes is solved by training *K* logistic regression models, each one
    assuming the labels of the considered class *j* as *+1* and all the rest as *0*
    . The second approach consists of training a model for each pair of labels (![Logistic
    regression](img/Image00167.jpg) trained models).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法可以使用一对一或一对多的技术应用于多标签问题。使用第一种方法，通过训练*K*个逻辑回归模型来解决一个有*K*个类的问题，每个模型假设考虑的类*j*的标签为*+1*，其余所有标签为*0*。第二种方法包括为每对标签训练一个模型
    (![逻辑回归](img/Image00167.jpg)训练的模型)。
- en: Probabilistic interpretation of generalized linear models
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型的概率解释
- en: 'Now that we have seen the generalized linear model, let''s find the parameters
    θ[j] that satisfy the relationship:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了广义线性模型，让我们找到满足以下关系的参数θ[j]：
- en: '![Probabilistic interpretation of generalized linear models](img/Image00168.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型的概率解释](img/Image00168.jpg)'
- en: 'In the case of linear regression, we can assume ![Probabilistic interpretation
    of generalized linear models](img/Image00148.jpg) as normally distributed with
    mean *0* and variance σ² such that the probability is ![Probabilistic interpretation
    of generalized linear models](img/Image00169.jpg) equivalent to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归的情况下，我们可以假设![广义线性模型的概率解释](img/Image00148.jpg)服从均值为*0*和方差σ²的正态分布，其概率![广义线性模型的概率解释](img/Image00169.jpg)等价于：
- en: '![Probabilistic interpretation of generalized linear models](img/Image00170.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型的概率解释](img/Image00170.jpg)'
- en: 'Therefore, the total likelihood of the system can be expressed as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，系统的总似然可以表示如下：
- en: '![Probabilistic interpretation of generalized linear models](img/Image00171.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型的概率解释](img/Image00171.jpg)'
- en: 'In the case of the logistic regression algorithm, we are assuming that the
    logistic function itself is the probability:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归算法的情况下，我们假设逻辑函数本身是概率：
- en: '![Probabilistic interpretation of generalized linear models](img/Image00172.jpg)![Probabilistic
    interpretation of generalized linear models](img/Image00173.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型的概率解释](img/Image00172.jpg)![广义线性模型的概率解释](img/Image00173.jpg)'
- en: 'Then the likelihood can be expressed by:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，似然可以表示为：
- en: '![Probabilistic interpretation of generalized linear models](img/Image00174.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型的概率解释](img/Image00174.jpg)'
- en: In both cases, it can be shown that maximizing the likelihood is equivalent
    to minimizing the cost function, so the gradient descent will be the same.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，可以证明最大化似然等价于最小化成本函数，因此梯度下降将是相同的。
- en: k-nearest neighbours (KNN)
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-最近邻（KNN）
- en: 'This is a very simple classification (or regression) method in which given
    a set of feature vectors ![k-nearest neighbours (KNN)](img/Image00175.jpg) with
    corresponding labels *y[i] * , a test point *x^((t)) * is assigned to the label
    value with the majority of the label occurrences in the *K* nearest neighbors
    ![k-nearest neighbours (KNN)](img/Image00176.jpg) found, using a distance measure
    such as the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的分类（或回归）方法，其中给定一组具有相应标签*y[i] *的特征向量![k-最近邻（KNN）](img/Image00175.jpg)，将测试点*x^((t))
    *分配给在*K*个最近邻![k-最近邻（KNN）](img/Image00176.jpg)中找到的标签值，使用以下距离度量：
- en: '**Euclidean** : ![k-nearest neighbours (KNN)](img/Image00177.jpg)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欧几里得**：![k-最近邻（KNN）](img/Image00177.jpg)'
- en: '**Manhattan** : ![k-nearest neighbours (KNN)](img/Image00178.jpg)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿**：![k-最近邻（KNN）](img/Image00178.jpg)'
- en: '**Minkowski** : ![k-nearest neighbours (KNN)](img/Image00179.jpg) (if *q=2*
    , this reduces to the Euclidean distance)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**闵可夫斯基**：![k-最近邻（KNN）](img/Image00179.jpg)（如果*q=2*，这会降低到欧几里得距离）'
- en: In the case of regression, the value *y[t]* is calculated by replacing the majority
    of occurrences by the average of the labels ![k-nearest neighbours (KNN)](img/Image00180.jpg)
    ![k-nearest neighbours (KNN)](img/Image00181.jpg) . The simplest average (or the
    majority of occurrences) has uniform weights, so each point has the same importance
    regardless of their actual distance from x * ^((t)) * . However, a weighted average
    with weights equal to the inverse distance from *x^((t)) * may be used.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归的情况下，值*y[t]*是通过将大多数出现替换为标签![k-最近邻（KNN）](img/Image00180.jpg)![k-最近邻（KNN）](img/Image00181.jpg)的平均值来计算的。最简单的平均（或大多数出现）具有均匀权重，因此每个点的重要性相同，无论它们与x
    * ^((t)) *的实际距离如何。然而，可以使用权重等于*x^((t)) *的逆距离的加权平均。
- en: Naive Bayes
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: '**Naive Bayes** is a classification algorithm based on Bayes'' probability
    theorem and conditional independence hypothesis on the features. Given a set of
    *m* features, ![Naive Bayes](img/Image00182.jpg) , and a set of labels (classes)
    ![Naive Bayes](img/Image00183.jpg) , the probability of having label *c* (also
    given the feature set *x[i] * ) is expressed by Bayes'' theorem:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**是一种基于贝叶斯概率定理和特征条件独立性假设的分类算法。给定一组*m*个特征![朴素贝叶斯](img/Image00182.jpg)，以及一组标签（类别）![朴素贝叶斯](img/Image00183.jpg)，标签*c*（也给定特征集*x[i]
    *）的概率由贝叶斯定理表示：'
- en: '![Naive Bayes](img/Image00184.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](img/Image00184.jpg)'
- en: 'Here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![Naive Bayes](img/Image00185.jpg) is called the likelihood distribution'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![朴素贝叶斯](img/Image00185.jpg)被称为似然分布'
- en: '![Naive Bayes](img/Image00186.jpg) is the posteriori distribution'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![朴素贝叶斯](img/Image00186.jpg)是后验分布'
- en: '![Naive Bayes](img/Image00187.jpg) is the prior distribution'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![朴素贝叶斯](img/Image00187.jpg)是先验分布'
- en: '![Naive Bayes](img/Image00188.jpg) is called the evidence'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![朴素贝叶斯](img/Image00188.jpg)被称为证据'
- en: 'The predicted class associated with the set of features ![Naive Bayes](img/Image00182.jpg)
    will be the value *p* such that the probability is maximized:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与特征集![朴素贝叶斯](img/Image00182.jpg)相关的预测类别将是值*p*，使得概率最大化：
- en: '![Naive Bayes](img/Image00189.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](img/Image00189.jpg)'
- en: However, the equation cannot be computed. So, an assumption is needed.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该方程无法计算。因此，需要做出一个假设。
- en: 'Using the rule on conditional probability ![Naive Bayes](img/Image00190.jpg)
    , we can write the numerator of the previous formula as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用条件概率规则![朴素贝叶斯](img/Image00190.jpg)，我们可以将前面公式的分子写为以下形式：
- en: '![Naive Bayes](img/Image00191.jpg)![Naive Bayes](img/Image00192.jpg)![Naive
    Bayes](img/Image00193.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](img/Image00191.jpg)![朴素贝叶斯](img/Image00192.jpg)![朴素贝叶斯](img/Image00193.jpg)'
- en: 'We now use the assumption that each feature *x[i]* is conditionally independent
    given *c* (for example, to calculate the probability of *x[1]* given *c* , the
    knowledge of the label *c* makes the knowledge of the other feature *x[0]* redundant,
    ![Naive Bayes](img/Image00194.jpg) ):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用这样的假设：每个特征 *x[i]* 在给定 *c* 的条件下是条件独立的（例如，为了计算给定 *c* 的 *x[1]* 的概率，标签 *c*
    的知识使得其他特征 *x[0]* 的知识变得冗余，![朴素贝叶斯](img/Image00194.jpg)）：
- en: '![Naive Bayes](img/Image00195.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](img/Image00195.jpg)'
- en: 'Under this assumption, the probability of having label *c* is then equal to:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个假设下，具有标签 *c* 的概率等于：
- en: '![Naive Bayes](img/Image00196.jpg) ––––––––(1)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](img/Image00196.jpg) ––––––––(1)'
- en: Here, the *+1* in the numerator and the *M* in the denominator are constants,
    useful for avoiding the *0/0* situation ( **Laplace smoothing** ).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，分子中的 *+1* 和分母中的 *M* 是常数，有助于避免 *0/0* 的情况（**拉普拉斯平滑**）。
- en: 'Due to the fact that the denominator of ( **1** ) does not depend on the labels
    (it is summed over all possible labels), the final predicted label *p* is obtained
    by finding the maximum of the numerator of ( **1** ):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于（**1**）的分母不依赖于标签（它是所有可能标签的总和），最终的预测标签 *p* 通过找到（**1**）的分子中的最大值来获得：
- en: '![Naive Bayes](img/Image00197.jpg) ––––––––(2)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](img/Image00197.jpg) ––––––––(2)'
- en: Given the usual training set ![Naive Bayes](img/Image00198.jpg) , where ![Naive
    Bayes](img/Image00199.jpg) ( *M* features) corresponding to the labels set ![Naive
    Bayes](img/Image00200.jpg) , the probability *P(y=c)* is simply calculated in
    frequency terms as the number of training examples associated with the class *c*
    over the total number of examples, ![Naive Bayes](img/Image00201.jpg) . The conditional
    probabilities ![Naive Bayes](img/Image00202.jpg) instead are evaluated by following
    a distribution. We are going to discuss two models, **Multinomial Naive Bayes**
    and **Gaussian Naive Bayes** .
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 给定通常的训练集 ![朴素贝叶斯](img/Image00198.jpg)，其中 ![朴素贝叶斯](img/Image00199.jpg)（*M* 个特征）对应于标签集合
    ![朴素贝叶斯](img/Image00200.jpg)，*P(y=c)* 的概率简单地按频率计算，即与类别 *c* 相关的训练示例数除以示例总数，![朴素贝叶斯](img/Image00201.jpg)。相反，条件概率
    ![朴素贝叶斯](img/Image00202.jpg) 通过遵循分布来评估。我们将讨论两个模型，**多项式朴素贝叶斯**和**高斯朴素贝叶斯**。
- en: Multinomial Naive Bayes
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: Let's assume we want to determine whether an e-mail *s* given by a set of word
    occurrences ![Multinomial Naive Bayes](img/Image00203.jpg) is spam *(1)* or not
    *(0)* so that ![Multinomial Naive Bayes](img/Image00204.jpg) . *M* is the size
    of the vocabulary (number of features). There are ![Multinomial Naive Bayes](img/Image00205.jpg)
    words and *N* training examples (e-mails).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要确定一个由一组单词出现![多项式朴素贝叶斯](img/Image00203.jpg)给出的电子邮件 *s* 是否是垃圾邮件 *(1)* 或不是
    *(0)*，以便![多项式朴素贝叶斯](img/Image00204.jpg)。*M* 是词汇表的大小（特征的数量）。有 ![多项式朴素贝叶斯](img/Image00205.jpg)
    个单词和 *N* 个训练示例（电子邮件）。
- en: 'Each email *x^((i))* with label *y[i]* such that ![Multinomial Naive Bayes](img/Image00206.jpg)
    is the number of times the word *j* in the vocabulary occurs in the training example
    *l* . For example, ![Multinomial Naive Bayes](img/Image00207.jpg) represents the
    number of times the word *1* , or *w[1] * , occurs in the third e-mail. In this
    case, multinomial distribution on the likelihood is applied:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 每个带有标签 *y[i]* 的电子邮件 *x^((i))*，满足![多项式朴素贝叶斯](img/Image00206.jpg)，是词汇表中单词 *j*
    在训练示例 *l* 中出现的次数。例如，![多项式朴素贝叶斯](img/Image00207.jpg) 表示单词 *1* 或 *w[1]* 在第三封电子邮件中出现的次数。在这种情况下，对似然应用多项式分布：
- en: '![Multinomial Naive Bayes](img/Image00208.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![多项式朴素贝叶斯](img/Image00208.jpg)'
- en: 'Here, the normalization constants in the front can be discarded because they
    do not depend on the label *y* , and so the *arg max* operator will not be affected.
    The important part is the evaluation of the single word *w[j]* : probability over
    the training set:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，前面的归一化常数可以被忽略，因为它们不依赖于标签 *y* ，因此 *arg max* 运算符不会受到影响。重要的是对单个单词 *w[j]* 的评估：在训练集中的概率：
- en: '![Multinomial Naive Bayes](img/Image00209.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![多项式朴素贝叶斯](img/Image00209.jpg)'
- en: Here *N[iy]* is the number of times the word *j* occurs, that is associated
    with label *y* , and *N[y]* is the portion of the training set with label *y*
    .
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N[iy]* 表示单词 *j* 发生的次数，它与标签 *y* 相关，而 *N[y]* 是训练集中带有标签 *y* 的部分。
- en: 'This is the analogue of ![Multinomial Naive Bayes](img/Image00202.jpg) ,![Multinomial
    Naive Bayes](img/Image00210.jpg) on equation ( **1** ) and the multinomial distribution
    likelihood. Due to the exponent on the probability, usually the logarithm is applied
    to compute the final algorithm *(2)* :'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对![多项式朴素贝叶斯](img/Image00202.jpg)、![多项式朴素贝叶斯](img/Image00210.jpg)在方程（**1**）和多项式分布似然中的类似。由于概率上的指数，通常应用对数来计算最终的算法（**2**）：
- en: '![Multinomial Naive Bayes](img/Image00211.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![多项式朴素贝叶斯](img/Image00211.jpg)'
- en: Gaussian Naive Bayes
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: 'If the features vectors *x^((i))* have continuous values, this method can be
    applied. For example, we want to classify images in *K* classes, each feature
    *j* is a pixel, and *x[j] ^((i))* is the *j-th* pixel of the *i-th* image in the
    training set with *N* images and labels ![Gaussian Naive Bayes](img/Image00200.jpg)
    . Given an unlabeled image represented by the pixels ![Gaussian Naive Bayes](img/Image00212.jpg)
    , in this case, ![Gaussian Naive Bayes](img/Image00202.jpg) in equation ( **1**
    ) becomes:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征向量 *x^((i))* 具有连续值，则可以应用此方法。例如，我们想要将图像分类为 *K* 个类别，每个特征 *j* 是一个像素，而 *x[j]
    ^((i))* 是训练集中第 *i* 张图像的第 *j* 个像素，训练集包含 *N* 张图像和标签![高斯朴素贝叶斯](img/Image00200.jpg)。给定一个由像素![高斯朴素贝叶斯](img/Image00212.jpg)表示的无标签图像，在这种情况下，方程（**1**）中的![高斯朴素贝叶斯](img/Image00202.jpg)变为：
- en: '![Gaussian Naive Bayes](img/Image00213.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![高斯朴素贝叶斯](img/Image00213.jpg)'
- en: 'Here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '![Gaussian Naive Bayes](img/Image00214.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![高斯朴素贝叶斯](img/Image00214.jpg)'
- en: 'And:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 并且：
- en: '![Gaussian Naive Bayes](img/Image00215.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![高斯朴素贝叶斯](img/Image00215.jpg)'
- en: Decision trees
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: 'This class of algorithms aims to predict the unknown labels splitting the dataset,
    by generating a set of simple rules that are learnt from the features values.
    For example, consider a case of deciding whether to take an umbrella today or
    not based on the values of humidity, wind, temperature, and pressure. This is
    a classification problem, and an example of the decision tree can be like what
    is shown in the following figure based on data of 100 days. Here is a sample table:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这类算法旨在通过生成从特征值学习的一系列简单规则来分割数据集，以预测未知标签。例如，考虑一个根据湿度、风速、温度和压力值决定今天是否带伞的案例。这是一个分类问题，以下图所示的决策树示例基于100天的数据。以下是一个样本表格：
- en: '| Humidity (%) | Pressure (mbar) | Wind (Km/h) | Temperature (C) | Umbrella
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 湿度（%） | 压力（mbar） | 风速（Km/h） | 温度（C） | 伞 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 56 | 1,021 | 5 | 21 | Yes |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 1,021 | 5 | 21 | 是 |'
- en: '| 65 | 1,018 | 3 | 18 | No |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 65 | 1,018 | 3 | 18 | 否 |'
- en: '| 80 | 1,020 | 10 | 17 | No |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 1,020 | 10 | 17 | 否 |'
- en: '| 81 | 1,015 | 11 | 20 | Yes |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 81 | 1,015 | 11 | 20 | 是 |'
- en: '![Decision trees](img/Image00216.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/Image00216.jpg)'
- en: Decision tree for predicting whether to bring an umbrella or not based on a
    record of 100 days.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过去100天记录预测是否带伞的决策树。
- en: In the preceding figure the numbers in squares represent the days on which an
    umbrella has been brought, while the circled numbers indicate days in which an
    umbrella was not necessary.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，方框中的数字代表需要带伞的天数，而圆圈中的数字表示不需要带伞的天数。
- en: 'The decision tree presents two types of nodes: decision nodes, which have two
    (or more) branches when a decision split is applied; and leaf nodes, when data
    is classified. The stopping criterion is usually a maximum number of decision
    nodes (depth of the tree) or a minimum of data points to continue to split (typically
    around 2 to 5). The problem of decision trees learning is to build the *best*
    tree out of all the possible node combinations, that is, estimate the hierarchy
    of the rules to be applied (in other words, whether the first decision node should
    be on humidity or on temperature, and so on). More formally, given a training
    set of ![Decision trees](img/Image00217.jpg) with *x^((i)) * in *R^m * and corresponding
    labels *y[i] * , we need to find the best rule to partition the data *S* at node
    *k* . If the chosen feature, *j* , is continuous, each split rule is given by
    a feature [ *j* ] and a threshold *t^j [k] * that splits *S* in ![Decision trees](img/Image00218.jpg)
    for ![Decision trees](img/Image00219.jpg) and ![Decision trees](img/Image00220.jpg)
    for ![Decision trees](img/Image00221.jpg) , ![Decision trees](img/Image00222.jpg)
    . The best split rule ![Decision trees](img/Image00223.jpg) for the node *k* is
    associated with the minimum of the impurity *I* function that measures how much
    the rule is able to separate the data into partitions with different labels (that
    is, each branch will contain the minimum amount of label mixing):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树呈现两种类型的节点：决策节点，当应用决策分割时有两个（或更多）分支；以及叶节点，当数据被分类。停止标准通常是最大决策节点数（树的深度）或继续分割所需的最小数据点数（通常在
    2 到 5 之间）。决策树学习的目标是构建所有可能节点组合中的 *最佳* 树，即估计要应用规则的层次结构（换句话说，第一个决策节点应该放在湿度上还是温度上，等等）。更正式地说，给定一个训练集
    ![决策树](img/Image00217.jpg) ，其中 *x^((i)) * 在 *R^m* 中，并对应标签 *y[i]* ，我们需要找到分割节点 *k*
    的最佳规则。如果选择的特征 *j* 是连续的，每个分割规则由一个特征 [ *j* ] 和一个阈值 *t^j [k]* 组成，该阈值将 *S* 分割为 ![决策树](img/Image00218.jpg)
    和 ![决策树](img/Image00219.jpg) 为 ![决策树](img/Image00220.jpg) 和 ![决策树](img/Image00221.jpg)
    ，![决策树](img/Image00222.jpg) 。节点 *k* 的最佳分割规则 ![决策树](img/Image00223.jpg) 与测量规则如何将数据分离成具有不同标签的分区（即每个分支将包含最小量的标签混合）的
    *I* 杂乱度函数的最小值相关联：
- en: '![Decision trees](img/Image00224.jpg)![Decision trees](img/Image00225.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/Image00224.jpg)![决策树](img/Image00225.jpg)'
- en: 'Here, ![Decision trees](img/Image00226.jpg) are the numbers of data points
    on the left and right branches, respectively. *N[k] * is the number of data points
    on node *k* , and *H* is a measure that can assume different expressions using
    the probability of each target value *l* at branch *b* ( *b* can be left or right),
    ![Decision trees](img/Image00227.jpg) :'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，![决策树](img/Image00226.jpg) 分别表示左右分支上的数据点数量。*N[k]* 是节点 *k* 上的数据点数量，而 *H*
    是一个可以采用每个目标值 *l* 在分支 *b*（*b* 可以是左分支或右分支）的概率的不同表达式来假设的度量，![决策树](img/Image00227.jpg)
    :'
- en: 'Entropy of the branch: ![Decision trees](img/Image00228.jpg)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分支的熵：![决策树](img/Image00228.jpg)
- en: 'Gini impurity of the branch: ![Decision trees](img/Image00229.jpg)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分支的基尼不纯度：![决策树](img/Image00229.jpg)
- en: 'Misclassification: ![Decision trees](img/Image00230.jpg)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误分类：![决策树](img/Image00230.jpg)
- en: 'Mean squared error (variance): ![Decision trees](img/Image00231.jpg) (where
    ![Decision trees](img/Image00232.jpg) )'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差（方差）：![决策树](img/Image00231.jpg)（其中 ![决策树](img/Image00232.jpg) ）
- en: Note that the latter is typically used in regression problems while the others
    are employed in classification. Note also that usually in literature, the *information
    gain* definition is introduced as the difference between *H* at node *k* and ![Decision
    trees](img/Image00233.jpg)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，后者通常用于回归问题，而其他则用于分类。还应注意，在文献中，通常将 *信息增益* 定义为节点 *k* 的 *H* 与 ![决策树](img/Image00233.jpg)
- en: '![Decision trees](img/Image00234.jpg) where ![Decision trees](img/Image00235.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/Image00234.jpg) 其中 ![决策树](img/Image00235.jpg)'
- en: If the feature *j* is discrete with *d* number of possible values, there is
    no binary threshold *t^j [k]* to calculate and the data is split into *d* partitions.
    The measure *H* is calculated over *d* subsets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征 *j* 是离散的，具有 *d* 个可能值，则没有二元阈值 *t^j [k]* 来计算，数据被分割成 *d* 个分区。度量 *H* 在 *d*
    个子集中计算。
- en: For example, we can determine the rule for the first node ( *k=0* ) for the
    preceding example using the entropy as the impurity measure *H* .
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用熵作为不纯度度量 *H* ，确定前一个示例中第一个节点（ *k=0* ）的规则。
- en: 'All the features are continuous, so the values of *t^j [0]* are needed. Assuming
    that *j=0* is the humidity and sorting in increasing order, the possible humidity
    values in the dataset we have are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征都是连续的，因此需要 *t^j [0]* 的值。假设 *j=0* 是湿度，并按升序排序，我们拥有的数据集中可能的湿度值如下：
- en: '| h | 0 | 1 | …. | 98 | 99 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| h | 0 | 1 | …. | 98 | 99 |'
- en: '| umbrella | yes | no | …. | no | no |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 伞 | 是 | 否 | …. | 否 | 否 |'
- en: '| humidity | **58** | 62 | …. | 88 | 89 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 湿度 | **58** | 62 | …. | 88 | 89 |'
- en: '|   | < | >= | < | >= | < | >= | < | >= | < | >= |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|   | < | >= | < | >= | < | >= | < | >= | < | >= |'
- en: '| yes | 0 | 11 | 14 | 32 | 7 | 20 | 29 | 12 | 78 | 0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 0 | 11 | 14 | 32 | 7 | 20 | 29 | 12 | 78 | 0 |'
- en: '| no | 0 | 89 | 21 | 33 | 13 | 60 | 10 | 49 | 22 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 0 | 89 | 21 | 33 | 13 | 60 | 10 | 49 | 22 | 0 |'
- en: '| ![Decision trees](img/Image00236.jpg) | **0.5** | 0.99 | 0.85 | 0.76 | 0.76
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ![决策树](img/Image00236.jpg) | **0.5** | 0.99 | 0.85 | 0.76 | 0.76 |'
- en: 'So, the threshold value for the humidity feature is ![Decision trees](img/Image00237.jpg)
    = 58; and in the same way, we can calculate the threshold values for temperature
    *t¹ [0] * , wind *t² [0] * , and pressure *t³ [0] * . Now we can record to determine
    the best rule for the first node, computing the impurity for each of the four
    features:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，湿度特征的阈值值为 ![决策树](img/Image00237.jpg) = 58；同样地，我们可以计算出温度 *t¹ [0] * ，风速 *t²
    [0] * 和压力 *t³ [0] * 的阈值值。现在我们可以记录下来，确定第一个节点的最佳规则，计算每个四个特征的纯度：
- en: '| yes | umbrella | yesno | umbrella |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 伞 | 是/否 | 伞 |'
- en: '| no |   |   |   |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 否 |   |   |   |'
- en: '| Humidity j=0 | ![Decision trees](img/Image00238.jpg) | 0 | 0 | Temperature
    j=1 | ![Decision trees](img/Image00239.jpg) | 21 | 32 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 湿度 j=0 | ![决策树](img/Image00238.jpg) | 0 | 0 | 温度 j=1 | ![决策树](img/Image00239.jpg)
    | 21 | 32 |'
- en: '| ![Decision trees](img/Image00240.jpg) | 11 | 89 | ![Decision trees](img/Image00241.jpg)
    | 11 | 36 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ![决策树](img/Image00240.jpg) | 11 | 89 | ![决策树](img/Image00241.jpg) | 11 |
    36 |'
- en: '| Impurity:![Decision trees](img/Image00242.jpg) | Impurity:![Decision trees](img/Image00243.jpg)
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 纯度:![决策树](img/Image00242.jpg) | 纯度:![决策树](img/Image00243.jpg) |'
- en: '| yes | umbrella | yesno | umbrella |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 伞 | 是/否 | 伞 |'
- en: '| no |   |   |   |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 否 |   |   |   |'
- en: '| Wind j=2 | ![Decision trees](img/Image00244.jpg) | 48 | 5 | Pressure j=3
    | ![Decision trees](img/Image00245.jpg) | 39 | 3 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 风速 j=2 | ![决策树](img/Image00244.jpg) | 48 | 5 | 压力 j=3 | ![决策树](img/Image00245.jpg)
    | 39 | 3 |'
- en: '| ![Decision trees](img/Image00246.jpg) | 1 | 46 | ![Decision trees](img/Image00247.jpg)
    | 45 | 13 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ![决策树](img/Image00246.jpg) | 1 | 46 | ![决策树](img/Image00247.jpg) | 45 | 13
    |'
- en: '| **Impurity** :![Decision trees](img/Image00248.jpg) | Impurity:![Decision
    trees](img/Image00249.jpg) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **纯度** :![决策树](img/Image00248.jpg) | 纯度:![决策树](img/Image00249.jpg) |'
- en: 'Therefore, for node *0* , the best rule is given by:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于节点 *0* ，最佳规则如下：
- en: '![Decision trees](img/Image00250.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/Image00250.jpg)'
- en: That is, the wind feature with threshold *t² [0]* . We can repeat the same procedure
    to find the best rule for the following decision nodes until the end of the tree.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即，具有阈值 *t² [0]* 的风速特征。我们可以重复相同的程序，找到以下决策节点的最佳规则，直到树的末尾。
- en: Decision trees learning is able to handle large datasets, though it tends not
    to generalize well, especially with a large set of features ( *N* ≈ *M* ). In
    such cases, it is advisable to set a small depth of the tree or use some dimensionality
    reduction techniques. Setting the minimum number of data points to split or the
    minimum number of data points in a leaf node will also help prevent overfitting.
    This algorithm may lead to over-complex trees; they can be *pruned* to reduce
    the branches that do not affect the quality of the prediction. Various pruning
    techniques are available, but they are beyond the scope of this book. Note also
    that a series of decision trees can be trained at the same time, composing of
    a so-called **random forest** . A random forest trains each tree with a random
    sample of the original data points, and a random subset of features is available
    for each decision node learning. The result is an average of the predictions in
    a regression problem or the majority in a classification problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习能够处理大型数据集，尽管它通常不太擅长泛化，尤其是在具有大量特征的情况下（ *N* ≈ *M* ）。在这种情况下，建议设置树的小深度或使用一些降维技术。设置分割的最小数据点数或叶节点中的最小数据点数也有助于防止过拟合。此算法可能导致过复杂的树；它们可以被
    *剪枝* 以减少不影响预测质量的分支。有各种剪枝技术，但它们超出了本书的范围。请注意，还可以同时训练一系列决策树，组成所谓的 **随机森林** 。随机森林使用原始数据点的随机样本训练每个树，并且每个决策节点学习都有可用的随机特征子集。结果是回归问题中的预测平均值或分类问题中的多数值。
- en: 读累了记得休息一会哦~
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读累了记得休息一会哦~
- en: '**公众号：古德猫宁李**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**公众号：古德猫宁李**'
- en: 电子书搜索下载
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子书搜索下载
- en: 书单分享
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书单分享
- en: 书友学习交流
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书友学习交流
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
- en: 电子书搜索下载
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子书搜索下载
- en: 电子书打包资源分享
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子书打包资源分享
- en: 学习资源分享
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习资源分享
- en: Support vector machine
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: 'This algorithm, **Support Vector Machine** ( **SVM** ), tries to geometrically
    separate the dataset ![Support vector machine](img/Image00251.jpg) into two subsets
    labeled with *y[i] =+1* and *y[i] =-1* . The next figure shows the data perfectly
    separated into two classes (empty circles and black circles), that is, the case
    the data in which the decision boundary (or hyperplane) given by the black line
    fully separates the two classes (in other words, there are no misclassified data
    points):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法，**支持向量机**（**SVM**），试图在几何上将数据集![支持向量机](img/Image00251.jpg)分为两个标记为*y[i] =+1*和*y[i]
    =-1*的子集。下一图显示了数据被完美地分为两类（空心圆和黑色圆），即决策边界（或超平面）由黑色线完全分隔两个类的数据（换句话说，没有误分类的数据点）：
- en: '![Support vector machine](img/Image00252.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00252.jpg)'
- en: Sketch of the dataset separated into two classes (empty and filled circles)
    by the black line (decision boundary)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集分为两类（空心和实心圆）的草图，由黑色线（决策边界）分隔
- en: 'The hyperplane is mathematically described by the equation ![Support vector
    machine](img/Image00253.jpg) , where ![Support vector machine](img/Image00254.jpg)
    is the distance of the hyperplane from the origin and *w* is the normal to the
    hyperplane. The goal of the algorithm is to maximize the distance of the decision
    boundary from the data points. In practice, we consider the closest points *i*
    to the hyperplane, called support vectors, that lie in two planes *H[1] * , *H[2]
    * at distances *d[1] * , *d[2] * from the decision boundary such that:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面由方程![支持向量机](img/Image00253.jpg)在数学上描述，其中![支持向量机](img/Image00254.jpg)是超平面到原点的距离，*w*是超平面的法线。算法的目标是最大化决策边界与数据点之间的距离。在实践中，我们考虑最接近超平面的点*i*，称为支持向量，它们位于距离决策边界*d[1]*，*d[2]*的两个平面*H[1]*，*H[2]*上，使得：
- en: '![Support vector machine](img/Image00255.jpg) for *H[1] * such that *y[i] =+1*
    ––––––––(1)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00255.jpg)对于*H[1]*，使得*y[i] =+1* ––––––––(1)'
- en: '![Support vector machine](img/Image00256.jpg) for *H[2] * such that *y[i] =-1*
    ––––––––(2)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00256.jpg)对于*H[2]*，使得*y[i] =-1* ––––––––(2)'
- en: Assuming *d[1] =d[2]* , the common distance is called margin so that the support
    vector machine method finds the values of *w* and *b* that maximize the margin.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*d[1] =d[2]*，共同距离称为边缘，因此支持向量机方法找到*w*和*b*的值，以最大化边缘。
- en: 'Since the distance between *H[1]* and *H[2]* is given by ![Support vector machine](img/Image00257.jpg)
    , the margin is equal to ![Support vector machine](img/Image00258.jpg) and the
    support vector machine algorithm is equivalent to:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*H[1]*和*H[2]*之间的距离由![支持向量机](img/Image00257.jpg)给出，因此边缘等于![支持向量机](img/Image00258.jpg)，支持向量机算法等价于：
- en: '![Support vector machine](img/Image00259.jpg) such that ![Support vector machine](img/Image00260.jpg)
    ,'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![支持向量机](img/Image00259.jpg)使得![支持向量机](img/Image00260.jpg)，'
- en: 'Here, the square operation and the factor ![Support vector machine](img/Image00261.jpg)
    have been added to allow the use of a quadratic programming method to solve the
    mathematical problem. Now, the problem can be rewritten in a Lagrangian form using
    the Lagrange multipliers a * [i] >0* :'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，已经添加了平方运算和因子![支持向量机](img/Image00261.jpg)，以便使用二次规划方法解决数学问题。现在，问题可以用拉格朗日乘数a
    * [i] >0*重写为拉格朗日形式：
- en: '![Support vector machine](img/Image00262.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00262.jpg)'
- en: 'Setting the derivatives with respect to ![Support vector machine](img/Image00263.jpg)
    and *b* to *0* , we obtain:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 将关于![支持向量机](img/Image00263.jpg)和*b*的导数设置为*0*，我们得到：
- en: '![Support vector machine](img/Image00264.jpg) ––––––––(3)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00264.jpg) ––––––––(3)'
- en: '![Support vector machine](img/Image00265.jpg) ––––––––(4)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00265.jpg) ––––––––(4)'
- en: 'So the optimized Lagrangian becomes:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优化的拉格朗日函数变为：
- en: '![Support vector machine](img/Image00266.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00266.jpg)'
- en: Here, ![Support vector machine](img/Image00267.jpg) .
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![支持向量机](img/Image00267.jpg)。
- en: 'This is known as a dual form of the original problem, which depends only on
    the maximization of a *i* :'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为原始问题的对偶形式，它只依赖于a * i*的最大化：
- en: '![Support vector machine](img/Image00268.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00268.jpg)'
- en: 'The solutions ![Support vector machine](img/Image00269.jpg) (the cases a *
    [i] =0* return null vectors) are found using a technique called quadratic programming
    and represent the support vectors *w* through formula **(3)** :'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用称为二次规划的技术找到解 ![Support vector machine](img/Image00269.jpg)（当 *a * [i] =0*
    时返回零向量），这些解代表通过公式 **(3**) 的支持向量 *w*：
- en: '![Support vector machine](img/Image00270.jpg) ––––––––(5).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![支持向量机](img/Image00270.jpg) ––––––––(5).'
- en: 'a *s* satisfy the equation (combination of equation **(1)** and **(2)** ):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *s* 满足方程（方程 **(1**) 和 **(2**) 的组合）：
- en: '![Support vector machine](img/Image00271.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00271.jpg)'
- en: 'Substituting equation **(3)** and multiplying both sides by *y[s]* (which is
    *+1* or *-1* ), we obtain:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程式 **(3**) 代入，并将两边乘以 *y[s]*（其值为 *+1* 或 *-1*），我们得到：
- en: '![Support vector machine](img/Image00272.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00272.jpg)'
- en: 'Averaging over all the support vectors *N[s]* we can have a better estimate
    of the parameter *b* :'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有支持向量 *N[s]* 进行平均，我们可以得到参数 *b* 的更好估计：
- en: '![Support vector machine](img/Image00273.jpg) ––––––––(6)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00273.jpg) ––––––––(6)'
- en: 'The equations **(5)** and **(6)** return the values of the parameters that
    define the support vector machines algorithm, from which it is possible to predict
    the class of all test points *t* :'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 **(5**) 和 **(6**) 返回定义支持向量机算法的参数值，从而可以预测所有测试点 *t* 的类别：
- en: '![Support vector machine](img/Image00274.jpg)![Support vector machine](img/Image00275.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00274.jpg)![支持向量机](img/Image00275.jpg)'
- en: 'If a line is not able to completely separate the data points into two classes,
    we need to allow the data points to be misclassified by an error ![Support vector
    machine](img/Image00276.jpg) such that:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一条线无法完全将数据点分为两类，我们需要允许数据点被错误分类，错误率为 ![支持向量机](img/Image00276.jpg)：
- en: '![Support vector machine](img/Image00277.jpg)![Support vector machine](img/Image00278.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00277.jpg)![支持向量机](img/Image00278.jpg)'
- en: 'And we need to maximize the margin, trying to minimize the misclassification
    errors. This condition is translated into this equation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要最大化边缘，同时尽量减少误分类错误。这个条件被转化为这个方程：
- en: '![Support vector machine](img/Image00279.jpg) such that ![Support vector machine](img/Image00280.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00279.jpg)使得 ![支持向量机](img/Image00280.jpg)'
- en: 'Here, the parameter *C* is set to balance the size of the margin with the misclassification
    errors ( *C=0* trivially no misclassification and maximum margin, *C>>1* many
    misclassified points and a narrow margin). Applying the same method as before,
    the dual problem is subjected to Lagrange multipliers'' conditions with an upper
    bound *C* :'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数 *C* 被设置为平衡边缘大小与误分类错误（*C=0* 简单地没有误分类和最大边缘，*C>>1* 许多误分类点和一个狭窄的边缘）。应用之前的方法，将双问题提交给拉格朗日乘数条件，并设置上限
    *C*：
- en: '![Support vector machine](img/Image00281.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00281.jpg)'
- en: 'Until now, we have treated problems in which only two classes are considered.
    Real problems may have multiple classes, and two procedures are commonly used
    to employ this method (as seen for logistic regression): one versus all or one
    versus one. Given a problem with *M* classes, the first method trains *M* SVM
    models, each one assuming the labels of the considered class *j +1* and all the
    rest *-1* . The second method instead trains a model for each pair of classes
    *i* , *j* , leading to ![Support vector machine](img/Image00282.jpg) trained models.
    Clearly, the second method is computationally more expensive but the results are
    generally more precise.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了只有两个类别的题目。现实问题可能有多个类别，通常使用两种程序来应用这种方法（如逻辑回归所示）：一对多或一对一。给定一个有 *M*
    个类别的题目，第一种方法训练 *M* 个支持向量机模型，每个模型假设考虑的类别标签为 *j +1*，其余所有类别为 *-1*。第二种方法则针对每一对类别 *i*，*j*
    训练一个模型，导致 ![支持向量机](img/Image00282.jpg) 个训练模型。显然，第二种方法在计算上更昂贵，但结果通常更精确。
- en: 'In a similar way, SVM can be used in regression problems, that is, whenever
    *y[i]* is continuous between *-1* and *1* . In this case, the goal is to find
    the parameters *w* and *b* such that:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，支持向量机（SVM）可以用于回归问题，即当 *y[i]* 在 *-1* 和 *1* 之间连续时。在这种情况下，目标是找到参数 *w* 和
    *b*，使得：
- en: '![Support vector machine](img/Image00283.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00283.jpg)'
- en: 'We assume that the true values *t[i]* can differ from the predicted value *y[i]*
    of a maximum ![Support vector machine](img/Image00284.jpg) and the predictions
    can further be misclassified of about ![Support vector machine](img/Image00285.jpg)
    depending on whether *y[i] * is larger or smaller than *t[i] * . The following
    figure shows for an example point *i* the various predictions *y[i] * lying around
    the true value *t[i] * , and the associated errors:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设真实值 *t[i]* 可以与最大![支持向量机](img/Image00284.jpg)的预测值 *y[i]* 不同，并且根据 *y[i]* 是否大于或小于
    *t[i]*，预测可以进一步错误分类为![支持向量机](img/Image00285.jpg)。以下图例显示了对于示例点 *i*，围绕真实值 *t[i]*
    的各种预测 *y[i]*，以及相关的误差：
- en: '![Support vector machine](img/Image00286.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00286.jpg)'
- en: The predictions *y[i]* lie around the true value *[ti]*
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值 *y[i]* 围绕真实值 *[ti]* 附近
- en: 'The minimization problem becomes:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化问题变为：
- en: '![Support vector machine](img/Image00287.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00287.jpg)'
- en: 'Such that:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如此：
- en: '![Support vector machine](img/Image00288.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00288.jpg)'
- en: 'It is possible to show that the associated dual problem is now equal to:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，相关的对偶问题现在等于：
- en: '![Support vector machine](img/Image00289.jpg) subject to ![Support vector machine](img/Image00290.jpg)
    .'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![支持向量机](img/Image00289.jpg) 受限于 ![支持向量机](img/Image00290.jpg)。'
- en: Here, ![Support vector machine](img/Image00291.jpg) are the Lagrangian multipliers.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![支持向量机](img/Image00291.jpg)是拉格朗日乘子。
- en: 'The new prediction, *y[p]* , can be found by applying the formula ![Support
    vector machine](img/Image00292.jpg) , where the parameter *b* can be obtained
    as before—averaging on the subset *S* given by the support vectors associated
    with the subset ![Support vector machine](img/Image00293.jpg) and ![Support vector
    machine](img/Image00294.jpg) :'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 新的预测值 *y[p]* 可以通过应用公式![支持向量机](img/Image00292.jpg)找到，其中参数 *b* 可以像以前一样获得——在由支持向量关联的子集
    *S* 上取平均值，该子集由![支持向量机](img/Image00293.jpg)和![支持向量机](img/Image00294.jpg)给出：
- en: '![Support vector machine](img/Image00295.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/Image00295.jpg)'
- en: Kernel trick
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核技巧
- en: 'There are datasets that are not linearly separable in a certain space, but
    if it is transformed in the right space, then a hyperplane can separate the data
    into the desired two or more classes. Consider the example shown in the following
    figure:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些数据集在某个空间中是不可线性分离的，但如果在正确的空间中变换，那么超平面可以将数据分离成所需的两个或更多类别。考虑以下图例中所示示例：
- en: '![Kernel trick](img/Image00296.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![核技巧](img/Image00296.jpg)'
- en: In a two-dimensional space, the dataset shown on the left is not separable.
    Mapping the dataset in a three-dimensional space, the two classes are separable.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，左图所示的数据集是不可分离的。将数据集映射到三维空间中，两个类别是可分离的。
- en: 'We can clearly see that the two classes are not linearly separable in two-dimensional
    space (the left figure). Suppose we then apply a kernel function *K* on the data
    such that:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，在二维空间中（左图），这两个类别是不可线性分离的。假设我们随后在数据上应用核函数 *K*，使得：
- en: '![Kernel trick](img/Image00297.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![核技巧](img/Image00297.jpg)'
- en: 'The data is now separable by a two-dimensional plane (the right figure). The
    kernel function on the SVM algorithm is applied to the matrix *H[ij]* , replacing
    the dot product on the variable *i* , *j* :'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据可以通过一个二维平面分离（右图）。SVM算法上的核函数应用于矩阵 *H[ij]*，替换变量 *i*，*j* 上的点积：
- en: '![Kernel trick](img/Image00298.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![核技巧](img/Image00298.jpg)'
- en: 'Popular kernel functions used on the SVM algorithm are:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM算法中常用的核函数：
- en: 'Linear kernel: ![Kernel trick](img/Image00299.jpg)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性核：![核技巧](img/Image00299.jpg)
- en: 'Radial basis kernel (RBF): ![Kernel trick](img/Image00300.jpg)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 径向基核（RBF）：![核技巧](img/Image00300.jpg)
- en: 'Polynomial kernel: ![Kernel trick](img/Image00301.jpg)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式核：![核技巧](img/Image00301.jpg)
- en: 'Sigmoid kernel: ![Kernel trick](img/Image00302.jpg)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid核：![核技巧](img/Image00302.jpg)
- en: A comparison of methods
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法比较
- en: 'We can now test the methods discussed in this chapter to solve a regression
    problem and a classification problem. To avoid overfitting, the dataset is typically
    split into two sets: the training set, in which the model parameters are fitted,
    and a test set, where the accuracy of the model is evaluated. However, it may
    be necessary to use a third set, the validation set, in which the hyperparameters
    (for example, *C* and ![A comparison of methods](img/Image00303.jpg) for SVM,
    or α in ridge regression) can be optimized. The original dataset may be too small
    to allow splitting into three sets, and also the results may be affected by the
    particular choice of data points on the training, validation, and test sets. A
    common way to solve this issue is by evaluating the model following the so-called
    cross-validation procedure—the dataset is split into *k* subsets (called folds)
    and the model is trained as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以测试本章讨论的解决回归问题和分类问题的方法。为了避免过拟合，通常将数据集分为两个集合：训练集，其中模型参数被拟合；测试集，其中评估模型的准确率。然而，可能需要使用第三个集合，即验证集，在其中可以优化超参数（例如，SVM中的
    *C* 和 ![方法比较](img/Image00303.jpg) 或岭回归中的 α）。原始数据集可能太小，无法分为三个集合，而且结果可能会受到训练、验证和测试集中特定数据点选择的影响。解决这个问题的常见方法是通过所谓的交叉验证程序来评估模型——数据集被分为
    *k* 个子集（称为折），模型训练如下：
- en: A model is trained using *k-1* of the folds as the training data.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 *k-1* 个折作为训练数据来训练一个模型。
- en: The resulting model is tested on the remaining part of the data.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果模型在剩余的数据部分进行测试。
- en: This procedure is repeated as many times as the number of folds decided at the
    beginning, each time with different *k-1* folds to train (and consequently different
    test fold). The final accuracy is obtained by the average of the accuracies obtained
    on the different *k* iterations.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，重复次数与最初决定的折数相同，每次使用不同的 *k-1* 折来训练（因此测试折也不同）。最终准确率是通过在不同 *k* 次迭代中获得的准确率的平均值来得到的。
- en: Regression problem
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归问题
- en: 'We are using the housing dataset of Boston''s suburbs stored at [http://archive.ics.uci.edu/ml/datasets/Housing](http://archive.ics.uci.edu/ml/datasets/Housing)
    and in the author''s repository ([https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ), in which the code used in this paragraph is also available. The dataset has
    13 features:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用存储在 [http://archive.ics.uci.edu/ml/datasets/Housing](http://archive.ics.uci.edu/ml/datasets/Housing)
    和作者仓库 ([https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ) 的波士顿郊区的住房数据集，其中本段中使用的代码也可用。该数据集有13个特征：
- en: '**CRIM** : Per capita crime rate by town'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CRIM** : 城镇人均犯罪率'
- en: '**ZN** : Proportion of residential land zoned for lots over 25,000 sqft'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZN** : 住宅用地中划定为超过25,000平方英尺地块的比例'
- en: '**INDUS** : Proportion of non-retail business acres per town'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**INDUS** : 每个城镇非零售商业地块的比例'
- en: '**CHAS** : Charles River dummy variable ( *= 1* if tract bounds river; *0*
    otherwise)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHAS** : 查尔斯河虚拟变量（ *= 1* 如果地块边界是河流； *0* 否则）'
- en: '**NOX** : Nitric oxides concentration (parts per 10 million)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NOX** : 氮氧化物浓度（每千万分之一）'
- en: '**RM** : Average number of rooms per dwelling'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RM** : 每套住宅的平均房间数'
- en: '**AGE** : Proportion of owner-occupied units built prior to 1940'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AGE** : 1940年之前建造的业主自住单元的比例'
- en: '**DIS** : Weighted distances from five Boston employment centers'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DIS** : 从五个波士顿就业中心计算出的加权距离'
- en: '**RAD** : Index of accessibility to radial highways'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAD** : 到辐射高速公路的可达性指数'
- en: '**TAX** : Full-value property tax rate per $10,000'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TAX** : 每10,000美元的完整价值财产税率'
- en: '**PTRATIO** : Pupil-teacher ratio by town'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PTRATIO** : 城镇内的师生比例'
- en: '**B** : *1000(Bk - 0.63)^2* , where *Bk* is the proportion of blacks by town'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** : *1000(Bk - 0.63)^2* ，其中 *Bk* 是城镇中黑人比例'
- en: '**LSTAT** : The percentage of lower status of the population and the labels
    that we want to predict are MEDV, which represent the house value values (in $1000)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTAT** : 人口中低阶层比例以及我们想要预测的标签是MEDV，它代表房屋价值（以1000美元为单位）'
- en: 'To evaluate the quality of the models, the mean squared error defined in the
    introduction and the coefficient of determination, *R²* , are calculated. *R²*
    is given by:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的质量，计算了介绍中定义的均方误差和确定系数，*R²*。*R²* 的计算公式如下：
- en: '![Regression problem](img/Image00304.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![回归问题](img/Image00304.jpg)'
- en: Here, *y[i] ^(pred)* indicates the predicted label from the model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y[i] ^(pred)* 表示模型预测的标签。
- en: 'The best result is *R² =1* , which means the model perfectly fits the data,
    while *R² =0* is associated with a model with a constant line (negative values
    indicate an increasingly worse fit). The code to compute to train the linear regression,
    ridge regression, lasso regression, and SVM regression using the `sklearn` library
    is as follows (IPython notebook at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳结果是 *R² =1*，这意味着模型完美地拟合了数据，而 *R² =0* 则与一条恒定线模型相关（负值表示拟合越来越差）。使用 `sklearn`
    库计算线性回归、岭回归、Lasso 回归和 SVM 回归的代码如下（IPython 笔记本位于 [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)）：
- en: '![Regression problem](img/Image00305.jpg)![Regression problem](img/Image00306.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![回归问题](img/Image00305.jpg)![回归问题](img/Image00306.jpg)'
- en: 'The housing data is loaded using the pandas library and reshuffled to randomize
    the cross-validation folds subset data (10 folds have been used) by applying the
    function `df.iloc[np.random.permutation(len(df))]` . The output of this script
    is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 库加载数据集，并通过应用函数 `df.iloc[np.random.permutation(len(df))]` 打乱交叉验证的子集数据（使用了
    10 个折），以随机化交叉验证折。此脚本的输出如下：
- en: '![Regression problem](img/Image00307.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![回归问题](img/Image00307.jpg)'
- en: The best model fit is obtained using a random forest (with 50 trees); it returns
    an average coefficient of determination of *0.86* and *MSE=11.5* . As expected,
    the decision tree regressor has a lower *R²* and higher MSE than the random forest
    ( *0.67* and *25* respectively). The support vector machine with the **rbf kernel**
    ( *C=1,* ![Regression problem](img/Image00308.jpg) ) is the worst model, with
    a huge MSE error *83.9* and *0.0* at *R² * , while SVM with the linear kernel
    ( *C=1* , ![Regression problem](img/Image00308.jpg) ) returns a decent model (
    *0.69 R² * and *25.8* MSE). The lasso and ridge regressors have comparable results,
    around *0.7 R² * and *24* MSE. An important procedure to improve the model results
    is feature selection. It often happens that only a subset of the total features
    is relevant to perform the model training while the other features may not contribute
    at all to the model *R² * . Feature selection may improve *R² * because misleading
    data is disregarded and training time is reduced (fewer features to consider).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林（包含 50 棵树）获得最佳模型拟合；它返回平均确定系数为 *0.86* 和 *MSE=11.5*。正如预期的那样，决策树回归器的 *R²*
    值低于随机森林（分别为 *0.67* 和 *25*），而具有 **rbf 内核**的支持向量机（*C=1*，![回归问题](img/Image00308.jpg)）是最差的模型，具有巨大的
    MSE 错误 *83.9* 和 *R²* 的 *0.0*。具有线性内核的支持向量机（*C=1*，![回归问题](img/Image00308.jpg)）返回了一个相当不错的模型（*0.69
    R²* 和 *25.8* MSE）。Lasso 和岭回归器具有可比的结果，大约 *0.7 R²* 和 *24* MSE。提高模型结果的一个重要程序是特征选择。通常情况下，只有总特征的一部分与模型训练相关，而其他特征可能根本不会对模型
    *R²* 贡献。特征选择可以提高 *R²*，因为误导性数据被忽略，并且训练时间减少（要考虑的特征更少）。
- en: 'There are many techniques for extracting the best features for a certain model,
    but in this context, we explore the so-called recursive feature elimination method
    (RSE), which essentially considers the attributes associated with the largest
    absolute weights until the desired number of features are selected. In the case
    of the SVM algorithm, the weights are just the values of *w* , while for regression,
    they are the model parameters θ. Using the `sklearn` built-in function `RFE` specifying
    only the best four attributes (`best_features` ):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个模型提取最佳特征有许多技术，但在这个上下文中，我们探索所谓的递归特征消除方法（RSE），它本质上考虑与最大绝对权重相关的属性，直到选择到所需数量的特征。在
    SVM 算法中，权重就是 *w* 的值，而对于回归，它们是模型参数 θ。使用 `sklearn` 内置函数 `RFE` 指定仅最佳四个属性（`best_features`）：
- en: '![Regression problem](img/Image00309.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![回归问题](img/Image00309.jpg)'
- en: 'The output is:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Regression problem](img/Image00310.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![回归问题](img/Image00310.jpg)'
- en: The `RFE` function returns a list of Booleans (the `support_` attribute) to
    indicate which features are selected (true values) and which are not (false values).
    The selected features are then used to evaluate the model as we have done before.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`RFE` 函数返回一个布尔值列表（`support_` 属性），以指示哪些特征被选中（true 值）以及哪些没有被选中（false 值）。然后使用选中的特征来评估模型，就像我们之前做的那样。'
- en: Even by using only four features, the best model remains the random forest with
    50 trees, and the *R²* is just marginally lower than that for the model trained
    with the full set of features ( *0.82* against *0.86* ). The other models—lasso,
    ridge, decision tree, and linear SVM regressors—have a more significant *R²* drop,
    but the results are still comparable with their corresponding full-trained models.
    Note that the KNN algorithm does not provide weights on the features, so the `RFE`
    method cannot be applied.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只使用四个特征，最佳模型仍然是包含50棵树的随机森林，其 *R²* 值仅略低于使用完整特征集训练的模型（ *0.82* 对比 *0.86*）。其他模型——lasso、ridge、决策树和线性SVM回归器——的
    *R²* 值下降更显著，但结果仍然与相应的完整训练模型可比较。请注意，KNN算法不提供特征权重，因此不能应用`RFE`方法。
- en: Classification problem
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类问题
- en: 'To test the classifiers learned in this chapter, the dataset about car evaluation
    quality (inaccurate, accurate, good, and very good) based on six features that
    describe the main characteristics of a car (buying price, maintenance cost, number
    of doors, number of persons to carry, size of luggage boot, and safety). The dataset
    can be found at [http://archive.ics.uci.edu/ml/datasets/Car+Evaluation](http://archive.ics.uci.edu/ml/datasets/Car+Evaluation)
    or on my GitHub account, together with the code discussed here ([https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ). To evaluate the accuracy of the classification, we will use the precision,
    recall, and f-measure. Given a dataset with only two classes (positive and negative),
    we define the number of true positive points ( *tp* ) the points correctly labeled
    as positive, the number of false positive ( *fp* ) the points wrongly labeled
    as positive (negative points) and the number of false negative ( *fn* ) the number
    of points erroneously assigned to the negative class. Using these definitions,
    the precision, recall and f-measure can be calculated as:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试本章中学习的分类器，我们使用基于六个特征（购买价格、维护成本、车门数量、可搭载人数、行李箱大小和安全）的汽车评估质量数据集（不准确、准确、良好和非常好）。该数据集可以在[http://archive.ics.uci.edu/ml/datasets/Car+Evaluation](http://archive.ics.uci.edu/ml/datasets/Car+Evaluation)或在我的GitHub账户上找到，包括此处讨论的代码（[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)）。为了评估分类的准确性，我们将使用精确度、召回率和F度量。给定只有两个类别（正面和负面）的数据集，我们定义真实正面点数（
    *tp* ）为正确标记为正面的点数，错误正面（ *fp* ）为错误标记为正面的点数（负面点），错误负面（ *fn* ）为错误分配到负面类别的点数。使用这些定义，精确度、召回率和F度量可以计算如下：
- en: '![Classification problem](img/Image00311.jpg)![Classification problem](img/Image00312.jpg)![Classification
    problem](img/Image00313.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00311.jpg)![分类问题](img/Image00312.jpg)![分类问题](img/Image00313.jpg)'
- en: In a classification problem, a perfect precision ( *1.0* ) for a given class
    *C* means that each point assigned to class *C* belongs to class *C* (there is
    no information about the number of points from class *C* erroneously labeled),
    whereas a recall equal to *1.0* means that each point from class *C* was labeled
    as belonging to class *C* (but there is no information about the other points
    wrongly assigned to class *C* ).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个分类问题中，对于给定类别 *C* 的完美精确度（ *1.0* ）意味着每个分配给类别 *C* 的点都属于类别 *C*（没有关于类别 *C* 中错误标记的点数量的信息），而召回率等于
    *1.0* 则意味着来自类别 *C* 的每个点都被标记为属于类别 *C*（但关于错误分配给类别 *C* 的其他点的信息没有），
- en: Note that in the case of multiple classes, these metrics are usually calculated
    as many times the number of labels, each time considering a class as the positive
    and all others as the negative. Different averages over the multiple classes'
    metrics are then used to estimate the total precision, recall, and f-measure.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在多类别的情形下，这些指标通常被计算为标签数量的多次，每次考虑一个类别为正面，其他所有类别为负面。然后使用多类别的指标的不同平均值来估计总的精确度、召回率和F度量。
- en: The code to classify the cars dataset is as follows. First, we load all the
    libraries and the data into a pandas data frame.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对汽车数据集进行分类的代码如下。首先，我们将所有库和数据加载到一个pandas数据框中。
- en: '![Classification problem](img/Image00314.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00314.jpg)'
- en: 'The following are the feature values that are categorical:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些分类的特征值：
- en: '[PRE0]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These are mapped into numbers to be used in the classification algorithms:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这些被映射成用于分类算法的数字：
- en: '![Classification problem](img/Image00315.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00315.jpg)'
- en: 'Since we need to calculate and save the measures for all the methods, we write
    a standard function, `CalcMeasures` , and divide the labels'' vector `Y` from
    the features `X` :'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要计算并保存所有方法的度量值，我们编写了一个标准函数`CalcMeasures`，并将标签向量`Y`从特征`X`中分离出来：
- en: '![Classification problem](img/Image00316.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00316.jpg)'
- en: 'A `10` crossing validation folds has been used and the code is:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了`10`次交叉验证折数，代码如下：
- en: '![Classification problem](img/Image00317.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00317.jpg)'
- en: 'The measures'' values are stored in the data frames:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 度量值的存储在数据框中：
- en: '![Classification problem](img/Image00318.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00318.jpg)'
- en: 'Each measure has been evaluated four times—the number of car evaluation classes
    that fill the arrays according to the index mapping:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 每个度量值已被评估四次——根据索引映射填充数组的汽车评估类别的数量：
- en: '[PRE1]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The best model is SVM with rbf kernel ( *C=50* ), but random forest (50 trees)
    and decision trees also return excellent results (measures over *0.9* for all
    the four classes). Naive Bayes, logistic regression, and SVM with linear kernel
    ( *C=50* ) return poor models, especially for the accurate, good, and very good
    classes, because there are few points with those labels:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型是具有rbf核的SVM（*C=50*），但随机森林（50棵树）和决策树也返回了优秀的结果（所有四个类别的度量值均超过*0.9*）。朴素贝叶斯、逻辑回归以及具有线性核的SVM（*C=50*）返回了较差的模型，尤其是在准确、好和非常好的类别上，因为这些标签的点很少：
- en: '![Classification problem](img/Image00319.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![分类问题](img/Image00319.jpg)'
- en: In percentage, the very good (v-good) and good are 3.993% and 3.762% respectively,
    compared to 70.0223% of inaccurate and 22.222% of accurate. So, we can conclude
    that these algorithms are not suitable for predicting classes that are scarcely
    represented in a dataset.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在百分比上，非常好（v-good）和好的比例分别为3.993%和3.762%，与70.0223%的不准确和22.222%的准确相比。因此，我们可以得出结论，这些算法不适用于预测在数据集中很少出现的类别。
- en: Hidden Markov model
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏马尔可夫模型
- en: 'Although this method cannot be strictly considered a supervised learning algorithm,
    it can be also used to perform something that is really similar to classification,
    so we decided to include it here. To introduce the subject, we are going to present
    an example. Consider the simplistic case of predicting whether a salesman in front
    of you is lying or not (two states ![Hidden Markov model](img/Image00320.jpg)
    ) by observing his glance: eye contact, looking down, or looking aside (each observation
    *O[i] * has the values *0* , *1* , and *2* respectively). Imagine a sequence of
    observations of the salesman''s glances O=O * [0] * , O * [1] * , O * [2] * ,
    O * [3] * , O * [4] * ,… are *0, 1, 0, 2,…* We want to infer the transition matrix
    *A* between states *S[i] * at consecutive times *t* , *t+1* (or, in this example,
    two consecutive sentences):'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法不能严格地被认为是一种监督学习算法，但它也可以用来执行与分类非常相似的任务，因此我们决定将其包括在内。为了介绍这个主题，我们将提供一个例子。考虑一个简单的例子，通过观察销售员的目光（眼神接触、低头或看向一边，每个观察值*O[i]*分别对应*0*、*1*和*2*）来预测你面前的销售员是在说谎还是不是（两个状态![隐藏马尔可夫模型](img/Image00320.jpg)）。想象一下销售员目光的观察序列O=O
    * [0] * , O * [1] * , O * [2] * , O * [3] * , O * [4] * ,…是*0, 1, 0, 2,…*我们想要推断连续时间*t*、*t+1*（或在这个例子中，两个连续句子）之间的状态转移矩阵*A*：
- en: '![Hidden Markov model](img/Image00321.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏马尔可夫模型](img/Image00321.jpg)'
- en: 'Any entry of *A* , *a[ij]* represents the probability to stay at state *S[i]*
    at time *t+1* given the state *S[j]* at time *t* . Therefore, *0.3* ( *a[01]*
    ) is the probability that the salesman is not lying on the sentence at time *t+1*
    given that he is lying on the sentence at time *t* , *0.6 (a[10] )* is vice versa,
    *0.7(a[00] )* represents the probability that the salesman is lying on the sentence
    at time *t* and at time *t+10.4(a[11] )* is the probability that he is not lying
    on the sentence at time *t+1* after he was sincere at time *t* . In a similar
    way, it is possible to define the matrix *B* that correlates the salesman''s intention
    with his three possible behaviors:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*矩阵的任何一项，*a[ij]*，表示在时间*t*处于状态*S[j]*的情况下，在时间*t+1*处于状态*S[i]*的概率。因此，*0.3*（*a[01]*）是在时间*t*处于说谎状态的情况下，在时间*t+1*不处于说谎状态的概率，*0.6
    (a[10] )*是相反的情况，*0.7(a[00] )*表示在时间*t*和时间*t+10.4(a[11] )*处于说谎状态的概率，在时间*t*真诚之后，在时间*t+1*不处于说谎状态的概率。以类似的方式，可以定义与销售员的三个可能行为相关的矩阵*B*：'
- en: '![Hidden Markov model](img/Image00322.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏马尔可夫模型](img/Image00322.jpg)'
- en: 'Any entry *b[j(k)]* is the probability to have observation *k* at time *t*
    given the state *S[j]* at time *t* . For example, *0.7* ( *b[00]* ), *0.1* ( *b[01]*
    ), and *0.2* ( *b[02]* ) are the probabilities that the salesman is lying given
    the behavioral observations—eye contact, looking down, and looking aside—respectively.
    These relationships are described in the following figure:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00323.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Salesman behavior – two states hidden Markov model
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial state distribution of the salesman can be also defined: ![Hidden
    Markov model](img/Image00324.jpg) (he is slightly more inclined to lie than to
    tell the truth in the first sentence at time *0* ). Note that all of these matrices
    ![Hidden Markov model](img/Image00325.jpg) are row stochastic; that is, the rows
    sum to *1* and there is no direct dependency on time. A **hidden Markov model**
    ( **HMM** ) is given by the composition of the three matrices (![Hidden Markov
    model](img/Image00326.jpg) ) that describe the relationship between the known
    sequence of observations *O=O[0] , O[1] ,…O[T-1] * and the corresponding hidden
    states sequence *S=S[0] , S[1] ,… S[T-1] * . In general, the standard notation
    symbols employed by this algorithm are summarized as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '*T* is the length of the observation sequence *O=O[0]* , O[1] ,… O[T-1] and
    the hidden states sequence *S=S[0] , S[1] ,… S[T-1]*'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* is the number of possible (hidden) states in the model'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*M* is the number of the possible observation values: ![Hidden Markov model](img/Image00327.jpg)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* is the state transition matrix'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B* is the observation probability matrix'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: π is the initial state distribution
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding example, *M=3* , *N=2* , and we imagine to predict the sequence
    of the salesman''s intentions over the course of his speech (which are the hidden
    states) *S=S[0] , S[1] ,… S[T-1]* , observing the values of his behavior *O=O[0]
    , O[1] ,… O[T-1]* . This is achieved by calculating the probability of each state
    sequence *S* as:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00328.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: 'For instance, fixing *T=4* , *S=0101* , and *O=1012* :'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00329.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, we can calculate the probability of all other combinations
    of hidden states and find the most probable sequence *S* . An efficient algorithm
    for finding the most probable sequence *S* is the **Viterbi algorithm** , which
    consists of computing the maximum probability of the set of partial sequences
    from *0* to *t* until *T-1* . In practice, we calculate the following quantities:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00330.jpg)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: For *t=1,…,T-1* and *i=0,…,N-1* , the maximum probability of being at state
    *i* at time *t* among the possible paths coming from different states *j* is ![Hidden
    Markov model](img/Image00331.jpg) . The partial sequence associated with the maximum
    of ![Hidden Markov model](img/Image00332.jpg) is the most probable partial sequence
    until time *t* .
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final most probable sequence is associated with the maximum of the probability
    at time *T-1* : ![Hidden Markov model](img/Image00333.jpg) .'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, given the preceding model, the most likely sequence of length
    *T=2* can be calculated as:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '*P(10)=0.0024*'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(00)=0.0294*'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So d *1* (0)=P(00)=0.0294
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(01)=0.076*'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(11)=0.01*'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So d *[1] (1)=P(01)=0.076*
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And the final most probable sequence is *00* (two consecutive false sentences).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to think about the most likely sequence is by maximizing the number
    of correct states; that is, consider for each time *t* the state *i* with the
    maximum probability ![Hidden Markov model](img/Image00334.jpg) . Using an algorithm
    called backward algorithm, it is possible to show that the probability of a given
    state *i* , ![Hidden Markov model](img/Image00335.jpg) , is:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00336.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00337.jpg)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Hidden Markov model](img/Image00338.jpg) and ![Hidden Markov model](img/Image00339.jpg)
    Probabilities of the partial observation sequence until time *t* , where the HMM
    is on state *i* : ![Hidden Markov model](img/Image00340.jpg)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Hidden Markov model](img/Image00341.jpg) and ![Hidden Markov model](img/Image00342.jpg)
    Probability of the partial sequence after time *t* until *T-1* given the state
    at time *t* equal to *i* : ![Hidden Markov model](img/Image00343.jpg)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: The combination of the probabilities to stay on state *i* before and after time
    *t* result in the value of ![Hidden Markov model](img/Image00344.jpg) .
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the two methods of calculating the most likely sequence do not necessarily
    return the same result.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'The reverse problem—find the optimal HMM ![Hidden Markov model](img/Image00345.jpg)
    given a sequence *O=O[0] ,O[1] ,…O[T-1] * and the values of the parameters *N*
    , *M* —is also solvable iteratively using the **Baum-Welch algorithm** . Defining
    the probability of occurring at state *i* at time *t* and to go at state *j* at
    time *t+1* as:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00346.jpg) where ![Hidden Markov model](img/Image00347.jpg)
    for ![Hidden Markov model](img/Image00348.jpg) and ![Hidden Markov model](img/Image00349.jpg)
    .'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the Baum-Welch algorithm is as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Initialize ![Hidden Markov model](img/Image00350.jpg)
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate ![Hidden Markov model](img/Image00351.jpg) and ![Hidden Markov model](img/Image00352.jpg)
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recompute the model matrices as:![Hidden Markov model](img/Image00353.jpg) where
    ![Hidden Markov model](img/Image00354.jpg) and ![Hidden Markov model](img/Image00355.jpg)
    is Kronacker symbol, which is equal to *1* if ![Hidden Markov model](img/Image00356.jpg)
    and *0* otherwise
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iterate until the convergence of: ![Hidden Markov model](img/Image00357.jpg)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to show a piece of Python code that implements
    these equations to test the HMM algorithm.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: A Python example
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, the `hmm_example.py` file discussed hereafter is available at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    .
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We start defining a class in which we pass the model matrices:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Viterbi algorithm and the maximization of the number of correct states
    are implemented in the following two functions:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since the multiplication of probabilities will result in an underflow problem,
    all the Α *[t] (i)* and Β *[t] (i)* have been multiplied by a constant such that
    for ![A Python example](img/Image00358.jpg) :'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![A Python example](img/Image00359.jpg)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![A Python example](img/Image00360.jpg) where ![A Python example](img/Image00361.jpg)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Now we can initialize the HMM model with the matrices in the salesman''s intentions
    example and use the two preceding functions:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result is:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this particular case, the two methods return the same sequence, and you can
    easily verify that by changing the initial matrices, the algorithms may lead to
    different results. We obtain that the sequence of behaviors; eye contact, looking
    down, eye contact, looking aside is likely associated with the salesman states'
    sequence; lie, not lie, lie, lie with a probability of *0.0044* .
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to implement the Baum-Welch algorithm to find the optimal
    HMM given the sequence of observations and the parameters *N* and *M* . Here is
    the code:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the code uses the shallow copy from the module `copy` , which creates
    a new container populated with references to the contents of the original object
    (in this case, `pi` , `B` ). That is, `newpi` is a different object from `pi`
    but `newpi[0]` is a reference of `pi[0]` . The NumPy squeeze function instead
    is needed to drop the redundant dimension from a matrix.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same behaviors sequence *O=0, 1, 0, 2* , we obtain that the optimal
    model is given by:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![A Python example](img/Image00362.jpg) ,![A Python example](img/Image00363.jpg)
    ,![A Python example](img/Image00364.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: This means that the state sequence must start from a true salesman's sentence
    and continuously oscillate between the two states *lie* and *not lie* . A true
    salesman's sentence (not lie) is certainly related to the eye contact value, while
    a lie is related to the looking down and looking aside behaviors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'In this simple introduction on HMM, we have assumed that each observation is
    a scalar value, but in real applications, each *O[i]* is usually a vector of features.
    And usually, this method is used as a classification training as many HMM l *[i]*
    , as classes to predict and then a test time chooses the class with the highest
    ![A Python example](img/Image00365.jpg) . Continuing with this example, we can
    imagine building a *true machine* to test each salesman we talk to. Imagine that
    for each sentence (observation) *O[i] * of our speaker, we can extract three features
    glances with three possible values *e[i] * (eye contact, looking down, and looking
    aside), voice sound *v[i] * with three possible values (too loud, too low, and
    flat), and hand movement *h[i] * with two possible values (shaking and calm) O
    *i=(e[i] , v[i] , h[i] )* . At training time, we ask our friend to tell lies and
    we use these observations to train an HMM l *0* using Baum-Welch. We repeat the
    training process but with true sentences and train l *1* . At test time, we record
    the sentence of the salesman *O* and calculate both: ![A Python example](img/Image00366.jpg)
    ,![A Python example](img/Image00367.jpg) . The class prediction will be the one
    with the highest probability.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节关于隐马尔可夫模型（HMM）的简单介绍中，我们假设每个观测值都是一个标量值，但在实际应用中，每个 *O[i]* 通常是一个特征向量。通常，这种方法被用作分类训练，许多HMM
    l *[i]* 作为预测的类别，然后在测试时选择具有最高 ![A Python example](img/Image00365.jpg) 的类别。继续这个例子，我们可以想象构建一个
    *真实机器* 来测试我们遇到的每个销售人员。想象一下，对于我们的说话者每个句子（观测值） *O[i]* ，我们可以提取三个特征：三种可能的值 *e[i]*
    （眼神接触、低头和看向一边），声音音调 *v[i]* 有三种可能的值（太大声、太小声和平直），以及手部动作 *h[i]* 有两种可能的值（握手和冷静） O
    *i=(e[i] , v[i] , h[i] )* 。在训练时间，我们让我们的朋友说谎，并使用这些观测值来训练一个使用Baum-Welch算法的HMM l
    *0*。我们重复训练过程，但使用真实句子来训练 l *1*。在测试时间，我们记录销售人员的句子 *O* 并计算两个：![A Python example](img/Image00366.jpg)
    ，![A Python example](img/Image00367.jpg) 。类别预测将是概率最高的那个。
- en: Note that HMM has been applied in various fields, but the applications in which
    it performs quite well are speech recognition tasks, handwritten character recognition,
    and action recognition.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，HMM已经在各个领域得到应用，但它在语音识别任务、手写字符识别和动作识别中的应用表现相当出色。
- en: Summary
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, the major classification and regression algorithms, together
    with the techniques to implement them, were discussed. You should now be able
    to understand in which situation each method can be used and how to implement
    it using Python and its libraries (sklearn and pandas).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了主要的分类和回归算法，以及实现它们的技巧。你现在应该能够理解在什么情况下可以使用每种方法，以及如何使用Python及其库（sklearn和pandas）来实现它。
- en: In the next chapter, we will cover the most relevant techniques used to learn
    from web data (web data mining).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍从网络数据中学习最相关的技术（网络数据挖掘）。
