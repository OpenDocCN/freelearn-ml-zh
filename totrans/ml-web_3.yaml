- en: Chapter 3. Supervised Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the most relevant regression and classification techniques
    are discussed. All of these algorithms share the same background procedure, and
    usually the name of the algorithm refers to both a classification and a regression
    method. The linear regression algorithms, Naive Bayes, decision tree, and support
    vector machine are going to be discussed in the following sections. To understand
    how to employ the techniques, a classification and a regression problem will be
    solved using the mentioned methods. Essentially, a labeled train dataset will
    be used to *train the models* , which means to find the values of the parameters,
    as we discussed in the introduction. As usual, the code is available in my GitHub
    folder at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude the chapter with an extra algorithm that may be used for classification,
    although it is not specifically designed for this purpose (hidden Markov model).
    We will now begin to explain the general causes of error in the methods when predicting
    the true labels associated with a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model error estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We said that the trained model is used to predict the labels of new data, and
    the quality of the prediction depends on the ability of the model to *generalize*
    , that is, the correct prediction of cases not present in the trained data. This
    is a well-known problem in literature and related to two concepts: bias and variance
    of the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The bias is the error due to a wrong assumption in the algorithm. Given a point
    *x^((t))* with label *y[t]* , the model is biased if it is trained with different
    training sets, and the predicted label *y[t] ^(pred)* will always be different
    from *y[t]* . The variance error instead refers to the different, wrongly predicted
    labels of the given point *x^((t))* . A classic example to explain the concepts
    is to consider a circle with the true value at the center (true label), as shown
    in the following figure. The closer the predicted labels are to the center, the
    more unbiased the model and the lower the variance (top left in the following
    figure). The other three cases are also shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model error estimation](img/Image00141.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Variance and bias example.
  prefs: []
  type: TYPE_NORMAL
- en: A model with low variance and low bias errors will have the predicted labels
    that is blue dots (as show in the preceding figure) concentrated on the red center
    (true label). The high bias error occurs when the predictions are far away from
    the true label, while high variance appears when the predictions are in a wide
    range of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen that labels can be continuous or discrete, corresponding
    to regression classification problems respectively. Most of the models are suitable
    for solving both problems, and we are going to use word regression and classification
    referring to the same model. More formally, given a set of *N* data points and
    corresponding labels ![Model error estimation](img/Image00142.jpg) , a model with
    a set of parameters ![Model error estimation](img/Image00143.jpg) with the true
    parameter values ![Model error estimation](img/Image00144.jpg) will have the **mean
    square error** ( **MSE** ), equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model error estimation](img/Image00145.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will use the MSE as a measure to evaluate the methods discussed in this chapter.
    Now we will start describing the generalized linear methods.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generalized linear model is a group of models that try to find the *M*
    parameters ![Generalized linear models](img/Image00146.jpg) that form a linear
    relationship between the labels *y[i] * and the feature vector *x^((i)) * that
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/Image00147.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Generalized linear models](img/Image00148.jpg) are the errors of the
    model. The algorithm for finding the parameters tries to minimize the total error
    of the model defined by the cost function *J* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/Image00149.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The minimization of *J* is achieved using an iterative algorithm called **batch
    gradient descent** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/Image00150.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, α is called learning rate, and it is a trade-off between convergence
    speed and convergence precision. An alternative algorithm that is called **stochastic
    gradient descent** , that is loop for ![Generalized linear models](img/Image00151.jpg)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/Image00152.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The θ[j] is updated for each training example *i* instead of waiting to sum
    over the entire training set. The last algorithm converges near the minimum of
    *J* , typically faster than batch gradient descent, but the final solution may
    oscillate around the real values of the parameters. The following paragraphs describe
    the most common model ![Generalized linear models](img/Image00153.jpg) and the
    corresponding cost function, *J* .
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear regression is the simplest algorithm and is based on the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/Image00154.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The cost function and update rule are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/Image00155.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ridge regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ridge regression, also known as **Tikhonov regularization** , adds a term to
    the cost function *J* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ridge regression](img/Image00156.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Ridge regression](img/Image00157.jpg) , where λ is the regularization parameter.
    The additional term has the function needed to prefer a certain set of parameters
    over all the possible solutions penalizing all the parameters θ[j] different from
    *0* . The final set of θ[j] *shrank* around *0* , lowering the variance of the
    parameters but introducing a bias error. Indicating with the superscript *l* the
    parameters from the linear regression, the ridge regression parameters are related
    by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ridge regression](img/Image00158.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This clearly shows that the larger the λ value, the more the ridge parameters
    are shrunk around *0* .
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lasso regression is an algorithm similar to ridge regression, the only difference
    being that the regularization term is the sum of the absolute values of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lasso regression](img/Image00159.jpg)![Lasso regression](img/Image00157.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the name, this algorithm is used for (binary) classification problems,
    so we define the labels![Logistic regression](img/Image00160.jpg) . The model
    is given the so-called logistic function expressed by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/Image00161.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the cost function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/Image00162.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From this, the update rule is formally the same as linear regression (but the
    model definition, ![Logistic regression](img/Image00163.jpg) , is different):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/Image00164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the prediction for a point *p* , ![Logistic regression](img/Image00165.jpg)
    , is a continuous value between *0* and *1* . So usually, to estimate the class
    label, we have a threshold at ![Logistic regression](img/Image00165.jpg) =0.5
    such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/Image00166.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The logistic regression algorithm is applicable to multiple label problems using
    the techniques one versus all or one versus one. Using the first method, a problem
    with *K* classes is solved by training *K* logistic regression models, each one
    assuming the labels of the considered class *j* as *+1* and all the rest as *0*
    . The second approach consists of training a model for each pair of labels (![Logistic
    regression](img/Image00167.jpg) trained models).
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic interpretation of generalized linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have seen the generalized linear model, let''s find the parameters
    θ[j] that satisfy the relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic interpretation of generalized linear models](img/Image00168.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of linear regression, we can assume ![Probabilistic interpretation
    of generalized linear models](img/Image00148.jpg) as normally distributed with
    mean *0* and variance σ² such that the probability is ![Probabilistic interpretation
    of generalized linear models](img/Image00169.jpg) equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic interpretation of generalized linear models](img/Image00170.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the total likelihood of the system can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic interpretation of generalized linear models](img/Image00171.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of the logistic regression algorithm, we are assuming that the
    logistic function itself is the probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic interpretation of generalized linear models](img/Image00172.jpg)![Probabilistic
    interpretation of generalized linear models](img/Image00173.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the likelihood can be expressed by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic interpretation of generalized linear models](img/Image00174.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In both cases, it can be shown that maximizing the likelihood is equivalent
    to minimizing the cost function, so the gradient descent will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: k-nearest neighbours (KNN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a very simple classification (or regression) method in which given
    a set of feature vectors ![k-nearest neighbours (KNN)](img/Image00175.jpg) with
    corresponding labels *y[i] * , a test point *x^((t)) * is assigned to the label
    value with the majority of the label occurrences in the *K* nearest neighbors
    ![k-nearest neighbours (KNN)](img/Image00176.jpg) found, using a distance measure
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean** : ![k-nearest neighbours (KNN)](img/Image00177.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manhattan** : ![k-nearest neighbours (KNN)](img/Image00178.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minkowski** : ![k-nearest neighbours (KNN)](img/Image00179.jpg) (if *q=2*
    , this reduces to the Euclidean distance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of regression, the value *y[t]* is calculated by replacing the majority
    of occurrences by the average of the labels ![k-nearest neighbours (KNN)](img/Image00180.jpg)
    ![k-nearest neighbours (KNN)](img/Image00181.jpg) . The simplest average (or the
    majority of occurrences) has uniform weights, so each point has the same importance
    regardless of their actual distance from x * ^((t)) * . However, a weighted average
    with weights equal to the inverse distance from *x^((t)) * may be used.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Naive Bayes** is a classification algorithm based on Bayes'' probability
    theorem and conditional independence hypothesis on the features. Given a set of
    *m* features, ![Naive Bayes](img/Image00182.jpg) , and a set of labels (classes)
    ![Naive Bayes](img/Image00183.jpg) , the probability of having label *c* (also
    given the feature set *x[i] * ) is expressed by Bayes'' theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00184.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00185.jpg) is called the likelihood distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00186.jpg) is the posteriori distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00187.jpg) is the prior distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00188.jpg) is called the evidence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The predicted class associated with the set of features ![Naive Bayes](img/Image00182.jpg)
    will be the value *p* such that the probability is maximized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00189.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, the equation cannot be computed. So, an assumption is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the rule on conditional probability ![Naive Bayes](img/Image00190.jpg)
    , we can write the numerator of the previous formula as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00191.jpg)![Naive Bayes](img/Image00192.jpg)![Naive
    Bayes](img/Image00193.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We now use the assumption that each feature *x[i]* is conditionally independent
    given *c* (for example, to calculate the probability of *x[1]* given *c* , the
    knowledge of the label *c* makes the knowledge of the other feature *x[0]* redundant,
    ![Naive Bayes](img/Image00194.jpg) ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00195.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Under this assumption, the probability of having label *c* is then equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00196.jpg) ––––––––(1)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *+1* in the numerator and the *M* in the denominator are constants,
    useful for avoiding the *0/0* situation ( **Laplace smoothing** ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the fact that the denominator of ( **1** ) does not depend on the labels
    (it is summed over all possible labels), the final predicted label *p* is obtained
    by finding the maximum of the numerator of ( **1** ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Naive Bayes](img/Image00197.jpg) ––––––––(2)'
  prefs: []
  type: TYPE_IMG
- en: Given the usual training set ![Naive Bayes](img/Image00198.jpg) , where ![Naive
    Bayes](img/Image00199.jpg) ( *M* features) corresponding to the labels set ![Naive
    Bayes](img/Image00200.jpg) , the probability *P(y=c)* is simply calculated in
    frequency terms as the number of training examples associated with the class *c*
    over the total number of examples, ![Naive Bayes](img/Image00201.jpg) . The conditional
    probabilities ![Naive Bayes](img/Image00202.jpg) instead are evaluated by following
    a distribution. We are going to discuss two models, **Multinomial Naive Bayes**
    and **Gaussian Naive Bayes** .
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial Naive Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's assume we want to determine whether an e-mail *s* given by a set of word
    occurrences ![Multinomial Naive Bayes](img/Image00203.jpg) is spam *(1)* or not
    *(0)* so that ![Multinomial Naive Bayes](img/Image00204.jpg) . *M* is the size
    of the vocabulary (number of features). There are ![Multinomial Naive Bayes](img/Image00205.jpg)
    words and *N* training examples (e-mails).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each email *x^((i))* with label *y[i]* such that ![Multinomial Naive Bayes](img/Image00206.jpg)
    is the number of times the word *j* in the vocabulary occurs in the training example
    *l* . For example, ![Multinomial Naive Bayes](img/Image00207.jpg) represents the
    number of times the word *1* , or *w[1] * , occurs in the third e-mail. In this
    case, multinomial distribution on the likelihood is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial Naive Bayes](img/Image00208.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the normalization constants in the front can be discarded because they
    do not depend on the label *y* , and so the *arg max* operator will not be affected.
    The important part is the evaluation of the single word *w[j]* : probability over
    the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial Naive Bayes](img/Image00209.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *N[iy]* is the number of times the word *j* occurs, that is associated
    with label *y* , and *N[y]* is the portion of the training set with label *y*
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the analogue of ![Multinomial Naive Bayes](img/Image00202.jpg) ,![Multinomial
    Naive Bayes](img/Image00210.jpg) on equation ( **1** ) and the multinomial distribution
    likelihood. Due to the exponent on the probability, usually the logarithm is applied
    to compute the final algorithm *(2)* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial Naive Bayes](img/Image00211.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Naive Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the features vectors *x^((i))* have continuous values, this method can be
    applied. For example, we want to classify images in *K* classes, each feature
    *j* is a pixel, and *x[j] ^((i))* is the *j-th* pixel of the *i-th* image in the
    training set with *N* images and labels ![Gaussian Naive Bayes](img/Image00200.jpg)
    . Given an unlabeled image represented by the pixels ![Gaussian Naive Bayes](img/Image00212.jpg)
    , in this case, ![Gaussian Naive Bayes](img/Image00202.jpg) in equation ( **1**
    ) becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian Naive Bayes](img/Image00213.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian Naive Bayes](img/Image00214.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian Naive Bayes](img/Image00215.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This class of algorithms aims to predict the unknown labels splitting the dataset,
    by generating a set of simple rules that are learnt from the features values.
    For example, consider a case of deciding whether to take an umbrella today or
    not based on the values of humidity, wind, temperature, and pressure. This is
    a classification problem, and an example of the decision tree can be like what
    is shown in the following figure based on data of 100 days. Here is a sample table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Humidity (%) | Pressure (mbar) | Wind (Km/h) | Temperature (C) | Umbrella
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 56 | 1,021 | 5 | 21 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | 1,018 | 3 | 18 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 1,020 | 10 | 17 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 81 | 1,015 | 11 | 20 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '![Decision trees](img/Image00216.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree for predicting whether to bring an umbrella or not based on a
    record of 100 days.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure the numbers in squares represent the days on which an
    umbrella has been brought, while the circled numbers indicate days in which an
    umbrella was not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree presents two types of nodes: decision nodes, which have two
    (or more) branches when a decision split is applied; and leaf nodes, when data
    is classified. The stopping criterion is usually a maximum number of decision
    nodes (depth of the tree) or a minimum of data points to continue to split (typically
    around 2 to 5). The problem of decision trees learning is to build the *best*
    tree out of all the possible node combinations, that is, estimate the hierarchy
    of the rules to be applied (in other words, whether the first decision node should
    be on humidity or on temperature, and so on). More formally, given a training
    set of ![Decision trees](img/Image00217.jpg) with *x^((i)) * in *R^m * and corresponding
    labels *y[i] * , we need to find the best rule to partition the data *S* at node
    *k* . If the chosen feature, *j* , is continuous, each split rule is given by
    a feature [ *j* ] and a threshold *t^j [k] * that splits *S* in ![Decision trees](img/Image00218.jpg)
    for ![Decision trees](img/Image00219.jpg) and ![Decision trees](img/Image00220.jpg)
    for ![Decision trees](img/Image00221.jpg) , ![Decision trees](img/Image00222.jpg)
    . The best split rule ![Decision trees](img/Image00223.jpg) for the node *k* is
    associated with the minimum of the impurity *I* function that measures how much
    the rule is able to separate the data into partitions with different labels (that
    is, each branch will contain the minimum amount of label mixing):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/Image00224.jpg)![Decision trees](img/Image00225.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Decision trees](img/Image00226.jpg) are the numbers of data points
    on the left and right branches, respectively. *N[k] * is the number of data points
    on node *k* , and *H* is a measure that can assume different expressions using
    the probability of each target value *l* at branch *b* ( *b* can be left or right),
    ![Decision trees](img/Image00227.jpg) :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy of the branch: ![Decision trees](img/Image00228.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gini impurity of the branch: ![Decision trees](img/Image00229.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Misclassification: ![Decision trees](img/Image00230.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mean squared error (variance): ![Decision trees](img/Image00231.jpg) (where
    ![Decision trees](img/Image00232.jpg) )'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the latter is typically used in regression problems while the others
    are employed in classification. Note also that usually in literature, the *information
    gain* definition is introduced as the difference between *H* at node *k* and ![Decision
    trees](img/Image00233.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/Image00234.jpg) where ![Decision trees](img/Image00235.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the feature *j* is discrete with *d* number of possible values, there is
    no binary threshold *t^j [k]* to calculate and the data is split into *d* partitions.
    The measure *H* is calculated over *d* subsets.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can determine the rule for the first node ( *k=0* ) for the
    preceding example using the entropy as the impurity measure *H* .
  prefs: []
  type: TYPE_NORMAL
- en: 'All the features are continuous, so the values of *t^j [0]* are needed. Assuming
    that *j=0* is the humidity and sorting in increasing order, the possible humidity
    values in the dataset we have are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| h | 0 | 1 | …. | 98 | 99 |'
  prefs: []
  type: TYPE_TB
- en: '| umbrella | yes | no | …. | no | no |'
  prefs: []
  type: TYPE_TB
- en: '| humidity | **58** | 62 | …. | 88 | 89 |'
  prefs: []
  type: TYPE_TB
- en: '|   | < | >= | < | >= | < | >= | < | >= | < | >= |'
  prefs: []
  type: TYPE_TB
- en: '| yes | 0 | 11 | 14 | 32 | 7 | 20 | 29 | 12 | 78 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| no | 0 | 89 | 21 | 33 | 13 | 60 | 10 | 49 | 22 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| ![Decision trees](img/Image00236.jpg) | **0.5** | 0.99 | 0.85 | 0.76 | 0.76
    |'
  prefs: []
  type: TYPE_TB
- en: 'So, the threshold value for the humidity feature is ![Decision trees](img/Image00237.jpg)
    = 58; and in the same way, we can calculate the threshold values for temperature
    *t¹ [0] * , wind *t² [0] * , and pressure *t³ [0] * . Now we can record to determine
    the best rule for the first node, computing the impurity for each of the four
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '| yes | umbrella | yesno | umbrella |'
  prefs: []
  type: TYPE_TB
- en: '| no |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Humidity j=0 | ![Decision trees](img/Image00238.jpg) | 0 | 0 | Temperature
    j=1 | ![Decision trees](img/Image00239.jpg) | 21 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| ![Decision trees](img/Image00240.jpg) | 11 | 89 | ![Decision trees](img/Image00241.jpg)
    | 11 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| Impurity:![Decision trees](img/Image00242.jpg) | Impurity:![Decision trees](img/Image00243.jpg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| yes | umbrella | yesno | umbrella |'
  prefs: []
  type: TYPE_TB
- en: '| no |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Wind j=2 | ![Decision trees](img/Image00244.jpg) | 48 | 5 | Pressure j=3
    | ![Decision trees](img/Image00245.jpg) | 39 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| ![Decision trees](img/Image00246.jpg) | 1 | 46 | ![Decision trees](img/Image00247.jpg)
    | 45 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| **Impurity** :![Decision trees](img/Image00248.jpg) | Impurity:![Decision
    trees](img/Image00249.jpg) |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, for node *0* , the best rule is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/Image00250.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is, the wind feature with threshold *t² [0]* . We can repeat the same procedure
    to find the best rule for the following decision nodes until the end of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees learning is able to handle large datasets, though it tends not
    to generalize well, especially with a large set of features ( *N* ≈ *M* ). In
    such cases, it is advisable to set a small depth of the tree or use some dimensionality
    reduction techniques. Setting the minimum number of data points to split or the
    minimum number of data points in a leaf node will also help prevent overfitting.
    This algorithm may lead to over-complex trees; they can be *pruned* to reduce
    the branches that do not affect the quality of the prediction. Various pruning
    techniques are available, but they are beyond the scope of this book. Note also
    that a series of decision trees can be trained at the same time, composing of
    a so-called **random forest** . A random forest trains each tree with a random
    sample of the original data points, and a random subset of features is available
    for each decision node learning. The result is an average of the predictions in
    a regression problem or the majority in a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 读累了记得休息一会哦~
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**公众号：古德猫宁李**'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书单分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书友学习交流
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 电子书打包资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 学习资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This algorithm, **Support Vector Machine** ( **SVM** ), tries to geometrically
    separate the dataset ![Support vector machine](img/Image00251.jpg) into two subsets
    labeled with *y[i] =+1* and *y[i] =-1* . The next figure shows the data perfectly
    separated into two classes (empty circles and black circles), that is, the case
    the data in which the decision boundary (or hyperplane) given by the black line
    fully separates the two classes (in other words, there are no misclassified data
    points):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00252.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sketch of the dataset separated into two classes (empty and filled circles)
    by the black line (decision boundary)
  prefs: []
  type: TYPE_NORMAL
- en: 'The hyperplane is mathematically described by the equation ![Support vector
    machine](img/Image00253.jpg) , where ![Support vector machine](img/Image00254.jpg)
    is the distance of the hyperplane from the origin and *w* is the normal to the
    hyperplane. The goal of the algorithm is to maximize the distance of the decision
    boundary from the data points. In practice, we consider the closest points *i*
    to the hyperplane, called support vectors, that lie in two planes *H[1] * , *H[2]
    * at distances *d[1] * , *d[2] * from the decision boundary such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00255.jpg) for *H[1] * such that *y[i] =+1*
    ––––––––(1)'
  prefs: []
  type: TYPE_IMG
- en: '![Support vector machine](img/Image00256.jpg) for *H[2] * such that *y[i] =-1*
    ––––––––(2)'
  prefs: []
  type: TYPE_IMG
- en: Assuming *d[1] =d[2]* , the common distance is called margin so that the support
    vector machine method finds the values of *w* and *b* that maximize the margin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the distance between *H[1]* and *H[2]* is given by ![Support vector machine](img/Image00257.jpg)
    , the margin is equal to ![Support vector machine](img/Image00258.jpg) and the
    support vector machine algorithm is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00259.jpg) such that ![Support vector machine](img/Image00260.jpg)
    ,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the square operation and the factor ![Support vector machine](img/Image00261.jpg)
    have been added to allow the use of a quadratic programming method to solve the
    mathematical problem. Now, the problem can be rewritten in a Lagrangian form using
    the Lagrange multipliers a * [i] >0* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00262.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Setting the derivatives with respect to ![Support vector machine](img/Image00263.jpg)
    and *b* to *0* , we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00264.jpg) ––––––––(3)'
  prefs: []
  type: TYPE_IMG
- en: '![Support vector machine](img/Image00265.jpg) ––––––––(4)'
  prefs: []
  type: TYPE_IMG
- en: 'So the optimized Lagrangian becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00266.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Support vector machine](img/Image00267.jpg) .
  prefs: []
  type: TYPE_NORMAL
- en: 'This is known as a dual form of the original problem, which depends only on
    the maximization of a *i* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00268.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The solutions ![Support vector machine](img/Image00269.jpg) (the cases a *
    [i] =0* return null vectors) are found using a technique called quadratic programming
    and represent the support vectors *w* through formula **(3)** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00270.jpg) ––––––––(5).'
  prefs: []
  type: TYPE_NORMAL
- en: 'a *s* satisfy the equation (combination of equation **(1)** and **(2)** ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00271.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation **(3)** and multiplying both sides by *y[s]* (which is
    *+1* or *-1* ), we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00272.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Averaging over all the support vectors *N[s]* we can have a better estimate
    of the parameter *b* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00273.jpg) ––––––––(6)'
  prefs: []
  type: TYPE_IMG
- en: 'The equations **(5)** and **(6)** return the values of the parameters that
    define the support vector machines algorithm, from which it is possible to predict
    the class of all test points *t* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00274.jpg)![Support vector machine](img/Image00275.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If a line is not able to completely separate the data points into two classes,
    we need to allow the data points to be misclassified by an error ![Support vector
    machine](img/Image00276.jpg) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00277.jpg)![Support vector machine](img/Image00278.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And we need to maximize the margin, trying to minimize the misclassification
    errors. This condition is translated into this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00279.jpg) such that ![Support vector machine](img/Image00280.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the parameter *C* is set to balance the size of the margin with the misclassification
    errors ( *C=0* trivially no misclassification and maximum margin, *C>>1* many
    misclassified points and a narrow margin). Applying the same method as before,
    the dual problem is subjected to Lagrange multipliers'' conditions with an upper
    bound *C* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00281.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Until now, we have treated problems in which only two classes are considered.
    Real problems may have multiple classes, and two procedures are commonly used
    to employ this method (as seen for logistic regression): one versus all or one
    versus one. Given a problem with *M* classes, the first method trains *M* SVM
    models, each one assuming the labels of the considered class *j +1* and all the
    rest *-1* . The second method instead trains a model for each pair of classes
    *i* , *j* , leading to ![Support vector machine](img/Image00282.jpg) trained models.
    Clearly, the second method is computationally more expensive but the results are
    generally more precise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar way, SVM can be used in regression problems, that is, whenever
    *y[i]* is continuous between *-1* and *1* . In this case, the goal is to find
    the parameters *w* and *b* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00283.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We assume that the true values *t[i]* can differ from the predicted value *y[i]*
    of a maximum ![Support vector machine](img/Image00284.jpg) and the predictions
    can further be misclassified of about ![Support vector machine](img/Image00285.jpg)
    depending on whether *y[i] * is larger or smaller than *t[i] * . The following
    figure shows for an example point *i* the various predictions *y[i] * lying around
    the true value *t[i] * , and the associated errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00286.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The predictions *y[i]* lie around the true value *[ti]*
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimization problem becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00287.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00288.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is possible to show that the associated dual problem is now equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00289.jpg) subject to ![Support vector machine](img/Image00290.jpg)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![Support vector machine](img/Image00291.jpg) are the Lagrangian multipliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new prediction, *y[p]* , can be found by applying the formula ![Support
    vector machine](img/Image00292.jpg) , where the parameter *b* can be obtained
    as before—averaging on the subset *S* given by the support vectors associated
    with the subset ![Support vector machine](img/Image00293.jpg) and ![Support vector
    machine](img/Image00294.jpg) :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/Image00295.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kernel trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are datasets that are not linearly separable in a certain space, but
    if it is transformed in the right space, then a hyperplane can separate the data
    into the desired two or more classes. Consider the example shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernel trick](img/Image00296.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In a two-dimensional space, the dataset shown on the left is not separable.
    Mapping the dataset in a three-dimensional space, the two classes are separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can clearly see that the two classes are not linearly separable in two-dimensional
    space (the left figure). Suppose we then apply a kernel function *K* on the data
    such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernel trick](img/Image00297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The data is now separable by a two-dimensional plane (the right figure). The
    kernel function on the SVM algorithm is applied to the matrix *H[ij]* , replacing
    the dot product on the variable *i* , *j* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernel trick](img/Image00298.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Popular kernel functions used on the SVM algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear kernel: ![Kernel trick](img/Image00299.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radial basis kernel (RBF): ![Kernel trick](img/Image00300.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polynomial kernel: ![Kernel trick](img/Image00301.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sigmoid kernel: ![Kernel trick](img/Image00302.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now test the methods discussed in this chapter to solve a regression
    problem and a classification problem. To avoid overfitting, the dataset is typically
    split into two sets: the training set, in which the model parameters are fitted,
    and a test set, where the accuracy of the model is evaluated. However, it may
    be necessary to use a third set, the validation set, in which the hyperparameters
    (for example, *C* and ![A comparison of methods](img/Image00303.jpg) for SVM,
    or α in ridge regression) can be optimized. The original dataset may be too small
    to allow splitting into three sets, and also the results may be affected by the
    particular choice of data points on the training, validation, and test sets. A
    common way to solve this issue is by evaluating the model following the so-called
    cross-validation procedure—the dataset is split into *k* subsets (called folds)
    and the model is trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A model is trained using *k-1* of the folds as the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting model is tested on the remaining part of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This procedure is repeated as many times as the number of folds decided at the
    beginning, each time with different *k-1* folds to train (and consequently different
    test fold). The final accuracy is obtained by the average of the accuracies obtained
    on the different *k* iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are using the housing dataset of Boston''s suburbs stored at [http://archive.ics.uci.edu/ml/datasets/Housing](http://archive.ics.uci.edu/ml/datasets/Housing)
    and in the author''s repository ([https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ), in which the code used in this paragraph is also available. The dataset has
    13 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CRIM** : Per capita crime rate by town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZN** : Proportion of residential land zoned for lots over 25,000 sqft'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**INDUS** : Proportion of non-retail business acres per town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHAS** : Charles River dummy variable ( *= 1* if tract bounds river; *0*
    otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOX** : Nitric oxides concentration (parts per 10 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RM** : Average number of rooms per dwelling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AGE** : Proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DIS** : Weighted distances from five Boston employment centers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAD** : Index of accessibility to radial highways'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TAX** : Full-value property tax rate per $10,000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PTRATIO** : Pupil-teacher ratio by town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B** : *1000(Bk - 0.63)^2* , where *Bk* is the proportion of blacks by town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LSTAT** : The percentage of lower status of the population and the labels
    that we want to predict are MEDV, which represent the house value values (in $1000)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To evaluate the quality of the models, the mean squared error defined in the
    introduction and the coefficient of determination, *R²* , are calculated. *R²*
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression problem](img/Image00304.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y[i] ^(pred)* indicates the predicted label from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best result is *R² =1* , which means the model perfectly fits the data,
    while *R² =0* is associated with a model with a constant line (negative values
    indicate an increasingly worse fit). The code to compute to train the linear regression,
    ridge regression, lasso regression, and SVM regression using the `sklearn` library
    is as follows (IPython notebook at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression problem](img/Image00305.jpg)![Regression problem](img/Image00306.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The housing data is loaded using the pandas library and reshuffled to randomize
    the cross-validation folds subset data (10 folds have been used) by applying the
    function `df.iloc[np.random.permutation(len(df))]` . The output of this script
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression problem](img/Image00307.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The best model fit is obtained using a random forest (with 50 trees); it returns
    an average coefficient of determination of *0.86* and *MSE=11.5* . As expected,
    the decision tree regressor has a lower *R²* and higher MSE than the random forest
    ( *0.67* and *25* respectively). The support vector machine with the **rbf kernel**
    ( *C=1,* ![Regression problem](img/Image00308.jpg) ) is the worst model, with
    a huge MSE error *83.9* and *0.0* at *R² * , while SVM with the linear kernel
    ( *C=1* , ![Regression problem](img/Image00308.jpg) ) returns a decent model (
    *0.69 R² * and *25.8* MSE). The lasso and ridge regressors have comparable results,
    around *0.7 R² * and *24* MSE. An important procedure to improve the model results
    is feature selection. It often happens that only a subset of the total features
    is relevant to perform the model training while the other features may not contribute
    at all to the model *R² * . Feature selection may improve *R² * because misleading
    data is disregarded and training time is reduced (fewer features to consider).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many techniques for extracting the best features for a certain model,
    but in this context, we explore the so-called recursive feature elimination method
    (RSE), which essentially considers the attributes associated with the largest
    absolute weights until the desired number of features are selected. In the case
    of the SVM algorithm, the weights are just the values of *w* , while for regression,
    they are the model parameters θ. Using the `sklearn` built-in function `RFE` specifying
    only the best four attributes (`best_features` ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression problem](img/Image00309.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression problem](img/Image00310.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `RFE` function returns a list of Booleans (the `support_` attribute) to
    indicate which features are selected (true values) and which are not (false values).
    The selected features are then used to evaluate the model as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: Even by using only four features, the best model remains the random forest with
    50 trees, and the *R²* is just marginally lower than that for the model trained
    with the full set of features ( *0.82* against *0.86* ). The other models—lasso,
    ridge, decision tree, and linear SVM regressors—have a more significant *R²* drop,
    but the results are still comparable with their corresponding full-trained models.
    Note that the KNN algorithm does not provide weights on the features, so the `RFE`
    method cannot be applied.
  prefs: []
  type: TYPE_NORMAL
- en: Classification problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test the classifiers learned in this chapter, the dataset about car evaluation
    quality (inaccurate, accurate, good, and very good) based on six features that
    describe the main characteristics of a car (buying price, maintenance cost, number
    of doors, number of persons to carry, size of luggage boot, and safety). The dataset
    can be found at [http://archive.ics.uci.edu/ml/datasets/Car+Evaluation](http://archive.ics.uci.edu/ml/datasets/Car+Evaluation)
    or on my GitHub account, together with the code discussed here ([https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    ). To evaluate the accuracy of the classification, we will use the precision,
    recall, and f-measure. Given a dataset with only two classes (positive and negative),
    we define the number of true positive points ( *tp* ) the points correctly labeled
    as positive, the number of false positive ( *fp* ) the points wrongly labeled
    as positive (negative points) and the number of false negative ( *fn* ) the number
    of points erroneously assigned to the negative class. Using these definitions,
    the precision, recall and f-measure can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00311.jpg)![Classification problem](img/Image00312.jpg)![Classification
    problem](img/Image00313.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In a classification problem, a perfect precision ( *1.0* ) for a given class
    *C* means that each point assigned to class *C* belongs to class *C* (there is
    no information about the number of points from class *C* erroneously labeled),
    whereas a recall equal to *1.0* means that each point from class *C* was labeled
    as belonging to class *C* (but there is no information about the other points
    wrongly assigned to class *C* ).
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the case of multiple classes, these metrics are usually calculated
    as many times the number of labels, each time considering a class as the positive
    and all others as the negative. Different averages over the multiple classes'
    metrics are then used to estimate the total precision, recall, and f-measure.
  prefs: []
  type: TYPE_NORMAL
- en: The code to classify the cars dataset is as follows. First, we load all the
    libraries and the data into a pandas data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00314.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the feature values that are categorical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These are mapped into numbers to be used in the classification algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00315.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we need to calculate and save the measures for all the methods, we write
    a standard function, `CalcMeasures` , and divide the labels'' vector `Y` from
    the features `X` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00316.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A `10` crossing validation folds has been used and the code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00317.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The measures'' values are stored in the data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00318.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each measure has been evaluated four times—the number of car evaluation classes
    that fill the arrays according to the index mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The best model is SVM with rbf kernel ( *C=50* ), but random forest (50 trees)
    and decision trees also return excellent results (measures over *0.9* for all
    the four classes). Naive Bayes, logistic regression, and SVM with linear kernel
    ( *C=50* ) return poor models, especially for the accurate, good, and very good
    classes, because there are few points with those labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification problem](img/Image00319.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In percentage, the very good (v-good) and good are 3.993% and 3.762% respectively,
    compared to 70.0223% of inaccurate and 22.222% of accurate. So, we can conclude
    that these algorithms are not suitable for predicting classes that are scarcely
    represented in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although this method cannot be strictly considered a supervised learning algorithm,
    it can be also used to perform something that is really similar to classification,
    so we decided to include it here. To introduce the subject, we are going to present
    an example. Consider the simplistic case of predicting whether a salesman in front
    of you is lying or not (two states ![Hidden Markov model](img/Image00320.jpg)
    ) by observing his glance: eye contact, looking down, or looking aside (each observation
    *O[i] * has the values *0* , *1* , and *2* respectively). Imagine a sequence of
    observations of the salesman''s glances O=O * [0] * , O * [1] * , O * [2] * ,
    O * [3] * , O * [4] * ,… are *0, 1, 0, 2,…* We want to infer the transition matrix
    *A* between states *S[i] * at consecutive times *t* , *t+1* (or, in this example,
    two consecutive sentences):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00321.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Any entry of *A* , *a[ij]* represents the probability to stay at state *S[i]*
    at time *t+1* given the state *S[j]* at time *t* . Therefore, *0.3* ( *a[01]*
    ) is the probability that the salesman is not lying on the sentence at time *t+1*
    given that he is lying on the sentence at time *t* , *0.6 (a[10] )* is vice versa,
    *0.7(a[00] )* represents the probability that the salesman is lying on the sentence
    at time *t* and at time *t+10.4(a[11] )* is the probability that he is not lying
    on the sentence at time *t+1* after he was sincere at time *t* . In a similar
    way, it is possible to define the matrix *B* that correlates the salesman''s intention
    with his three possible behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Any entry *b[j(k)]* is the probability to have observation *k* at time *t*
    given the state *S[j]* at time *t* . For example, *0.7* ( *b[00]* ), *0.1* ( *b[01]*
    ), and *0.2* ( *b[02]* ) are the probabilities that the salesman is lying given
    the behavioral observations—eye contact, looking down, and looking aside—respectively.
    These relationships are described in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00323.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Salesman behavior – two states hidden Markov model
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial state distribution of the salesman can be also defined: ![Hidden
    Markov model](img/Image00324.jpg) (he is slightly more inclined to lie than to
    tell the truth in the first sentence at time *0* ). Note that all of these matrices
    ![Hidden Markov model](img/Image00325.jpg) are row stochastic; that is, the rows
    sum to *1* and there is no direct dependency on time. A **hidden Markov model**
    ( **HMM** ) is given by the composition of the three matrices (![Hidden Markov
    model](img/Image00326.jpg) ) that describe the relationship between the known
    sequence of observations *O=O[0] , O[1] ,…O[T-1] * and the corresponding hidden
    states sequence *S=S[0] , S[1] ,… S[T-1] * . In general, the standard notation
    symbols employed by this algorithm are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*T* is the length of the observation sequence *O=O[0]* , O[1] ,… O[T-1] and
    the hidden states sequence *S=S[0] , S[1] ,… S[T-1]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N* is the number of possible (hidden) states in the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*M* is the number of the possible observation values: ![Hidden Markov model](img/Image00327.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* is the state transition matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B* is the observation probability matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: π is the initial state distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding example, *M=3* , *N=2* , and we imagine to predict the sequence
    of the salesman''s intentions over the course of his speech (which are the hidden
    states) *S=S[0] , S[1] ,… S[T-1]* , observing the values of his behavior *O=O[0]
    , O[1] ,… O[T-1]* . This is achieved by calculating the probability of each state
    sequence *S* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00328.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For instance, fixing *T=4* , *S=0101* , and *O=1012* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00329.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, we can calculate the probability of all other combinations
    of hidden states and find the most probable sequence *S* . An efficient algorithm
    for finding the most probable sequence *S* is the **Viterbi algorithm** , which
    consists of computing the maximum probability of the set of partial sequences
    from *0* to *t* until *T-1* . In practice, we calculate the following quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00330.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: For *t=1,…,T-1* and *i=0,…,N-1* , the maximum probability of being at state
    *i* at time *t* among the possible paths coming from different states *j* is ![Hidden
    Markov model](img/Image00331.jpg) . The partial sequence associated with the maximum
    of ![Hidden Markov model](img/Image00332.jpg) is the most probable partial sequence
    until time *t* .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final most probable sequence is associated with the maximum of the probability
    at time *T-1* : ![Hidden Markov model](img/Image00333.jpg) .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, given the preceding model, the most likely sequence of length
    *T=2* can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(10)=0.0024*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(00)=0.0294*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So d *1* (0)=P(00)=0.0294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(01)=0.076*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(11)=0.01*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So d *[1] (1)=P(01)=0.076*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And the final most probable sequence is *00* (two consecutive false sentences).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to think about the most likely sequence is by maximizing the number
    of correct states; that is, consider for each time *t* the state *i* with the
    maximum probability ![Hidden Markov model](img/Image00334.jpg) . Using an algorithm
    called backward algorithm, it is possible to show that the probability of a given
    state *i* , ![Hidden Markov model](img/Image00335.jpg) , is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00336.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00337.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Hidden Markov model](img/Image00338.jpg) and ![Hidden Markov model](img/Image00339.jpg)
    Probabilities of the partial observation sequence until time *t* , where the HMM
    is on state *i* : ![Hidden Markov model](img/Image00340.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Hidden Markov model](img/Image00341.jpg) and ![Hidden Markov model](img/Image00342.jpg)
    Probability of the partial sequence after time *t* until *T-1* given the state
    at time *t* equal to *i* : ![Hidden Markov model](img/Image00343.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: The combination of the probabilities to stay on state *i* before and after time
    *t* result in the value of ![Hidden Markov model](img/Image00344.jpg) .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the two methods of calculating the most likely sequence do not necessarily
    return the same result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reverse problem—find the optimal HMM ![Hidden Markov model](img/Image00345.jpg)
    given a sequence *O=O[0] ,O[1] ,…O[T-1] * and the values of the parameters *N*
    , *M* —is also solvable iteratively using the **Baum-Welch algorithm** . Defining
    the probability of occurring at state *i* at time *t* and to go at state *j* at
    time *t+1* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov model](img/Image00346.jpg) where ![Hidden Markov model](img/Image00347.jpg)
    for ![Hidden Markov model](img/Image00348.jpg) and ![Hidden Markov model](img/Image00349.jpg)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the Baum-Welch algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize ![Hidden Markov model](img/Image00350.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate ![Hidden Markov model](img/Image00351.jpg) and ![Hidden Markov model](img/Image00352.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recompute the model matrices as:![Hidden Markov model](img/Image00353.jpg) where
    ![Hidden Markov model](img/Image00354.jpg) and ![Hidden Markov model](img/Image00355.jpg)
    is Kronacker symbol, which is equal to *1* if ![Hidden Markov model](img/Image00356.jpg)
    and *0* otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iterate until the convergence of: ![Hidden Markov model](img/Image00357.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to show a piece of Python code that implements
    these equations to test the HMM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A Python example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, the `hmm_example.py` file discussed hereafter is available at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_3/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'We start defining a class in which we pass the model matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Viterbi algorithm and the maximization of the number of correct states
    are implemented in the following two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the multiplication of probabilities will result in an underflow problem,
    all the Α *[t] (i)* and Β *[t] (i)* have been multiplied by a constant such that
    for ![A Python example](img/Image00358.jpg) :'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Python example](img/Image00359.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![A Python example](img/Image00360.jpg) where ![A Python example](img/Image00361.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Now we can initialize the HMM model with the matrices in the salesman''s intentions
    example and use the two preceding functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this particular case, the two methods return the same sequence, and you can
    easily verify that by changing the initial matrices, the algorithms may lead to
    different results. We obtain that the sequence of behaviors; eye contact, looking
    down, eye contact, looking aside is likely associated with the salesman states'
    sequence; lie, not lie, lie, lie with a probability of *0.0044* .
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to implement the Baum-Welch algorithm to find the optimal
    HMM given the sequence of observations and the parameters *N* and *M* . Here is
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the code uses the shallow copy from the module `copy` , which creates
    a new container populated with references to the contents of the original object
    (in this case, `pi` , `B` ). That is, `newpi` is a different object from `pi`
    but `newpi[0]` is a reference of `pi[0]` . The NumPy squeeze function instead
    is needed to drop the redundant dimension from a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same behaviors sequence *O=0, 1, 0, 2* , we obtain that the optimal
    model is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Python example](img/Image00362.jpg) ,![A Python example](img/Image00363.jpg)
    ,![A Python example](img/Image00364.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that the state sequence must start from a true salesman's sentence
    and continuously oscillate between the two states *lie* and *not lie* . A true
    salesman's sentence (not lie) is certainly related to the eye contact value, while
    a lie is related to the looking down and looking aside behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this simple introduction on HMM, we have assumed that each observation is
    a scalar value, but in real applications, each *O[i]* is usually a vector of features.
    And usually, this method is used as a classification training as many HMM l *[i]*
    , as classes to predict and then a test time chooses the class with the highest
    ![A Python example](img/Image00365.jpg) . Continuing with this example, we can
    imagine building a *true machine* to test each salesman we talk to. Imagine that
    for each sentence (observation) *O[i] * of our speaker, we can extract three features
    glances with three possible values *e[i] * (eye contact, looking down, and looking
    aside), voice sound *v[i] * with three possible values (too loud, too low, and
    flat), and hand movement *h[i] * with two possible values (shaking and calm) O
    *i=(e[i] , v[i] , h[i] )* . At training time, we ask our friend to tell lies and
    we use these observations to train an HMM l *0* using Baum-Welch. We repeat the
    training process but with true sentences and train l *1* . At test time, we record
    the sentence of the salesman *O* and calculate both: ![A Python example](img/Image00366.jpg)
    ,![A Python example](img/Image00367.jpg) . The class prediction will be the one
    with the highest probability.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that HMM has been applied in various fields, but the applications in which
    it performs quite well are speech recognition tasks, handwritten character recognition,
    and action recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the major classification and regression algorithms, together
    with the techniques to implement them, were discussed. You should now be able
    to understand in which situation each method can be used and how to implement
    it using Python and its libraries (sklearn and pandas).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the most relevant techniques used to learn
    from web data (web data mining).
  prefs: []
  type: TYPE_NORMAL
