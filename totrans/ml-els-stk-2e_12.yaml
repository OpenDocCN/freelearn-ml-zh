- en: '*Chapter 9*: Introducing Data Frame Analytics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 9 章*：介绍数据框分析'
- en: In the first section of this book, we took an in-depth tour of anomaly detection,
    the first machine learning capability to be directly integrated into the Elastic
    Stack. In this chapter and the following one, we will take a dive into the new
    machine learning features integrated into the stack. These include outlier detection,
    a novel unsupervised learning technique for detecting unusual data points in non-timeseries
    indices, as well as two supervised learning features, classification and regression.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一部分，我们深入探讨了异常检测，这是第一个直接集成到 Elastic Stack 中的机器学习功能。在本章和下一章中，我们将深入研究集成到堆栈中的新机器学习功能。这些包括异常检测，一种用于检测非时间序列索引中异常数据点的创新无监督学习技术，以及两个监督学习功能，分类和回归。
- en: Supervised learning algorithms use labeled datasets – for example, a dataset
    describing various aspects of tissue samples along with whether or not the tissue
    is malignant – to learn a model. This model can then be used to make predictions
    on previously unseen data points (or tissue samples, to continue our example).
    When the target of prediction is a discrete variable or a category such as a malignant
    or non-malignant tissue sample, the supervised learning technique is called classification.
    When the target is a continuous numeric variable, such as the sale price of an
    apartment or the hourly price of electricity, the supervised learning technique
    is known as regression. Collectively, these three new machine learning features
    are known as **Data Frame Analytics**. We will discuss each of these in more depth
    in the following chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法使用标记数据集——例如，描述组织样本各个方面以及组织是否为恶性的数据集——来学习模型。然后，该模型可以用于对先前未见过的数据点（或组织样本，继续我们的例子）进行预测。当预测目标是离散变量或类别，如恶性或非恶性组织样本时，监督学习技术被称为分类。当目标是连续的数值变量，如公寓的销售价格或电力的每小时价格时，监督学习技术被称为回归。这三个新的机器学习功能统称为**数据框分析**。我们将在接下来的章节中更深入地讨论这些内容。
- en: Although each of these solves a different problem and has a different purpose,
    they are all powered under the hood by a common data transformation technology,
    that of transforms, which enables us to transform and aggregate data from a transaction-
    or stream-based format into an entity-based format. This entity-centric format
    is required by many of the algorithms we use in Data Frame Analytics and thus,
    before we dive deeper into each of the new machine learning features, we are going
    to dedicate this chapter to understanding in depth how to use transforms to transform
    our data into a format that is more amenable for downstream machine learning technologies.
    While on this journey, we are also going to take a brief tour of Painless, the
    scripting language embedded into Elasticsearch, which is a good tool for any data
    scientists or engineers working with machine learning in the Elastic Stack.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个功能都解决不同的问题，具有不同的目的，但它们在底层都由一个共同的数据转换技术驱动，即转换技术，它使我们能够将基于事务或流的数据格式转换为基于实体的格式。这种以实体为中心的格式是我们用于数据框分析的许多算法所必需的，因此，在我们深入研究每个新的机器学习功能之前，我们将在本章中深入探讨如何使用转换将我们的数据转换为更适合下游机器学习技术的格式。在这个过程中，我们还将简要介绍嵌入到
    Elasticsearch 中的脚本语言 Painless，这对于在 Elastic Stack 中使用机器学习的任何数据科学家或工程师来说都是一款好工具。
- en: 'A rich ecosystem of libraries, both for data manipulation and machine learning,
    exists outside of the Elastic Stack as well. One of the main drivers powering
    these applications is Python. Because of its ubiquity in the data science and
    data engineering communities, we are going to focus, in the second part of this
    chapter, on using Python with the Elastic Stack, with a particular focus on the
    new data-science native Elasticsearch client, Eland. We''ll check out the following
    topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Elastic Stack 之外，也存在一个丰富的库生态系统，这些库既适用于数据处理，也适用于机器学习。推动这些应用的主要动力之一是 Python。由于其在大数据和数据工程社区中的普遍性，我们在本章的第二部分将专注于使用
    Python 与 Elastic Stack 结合，特别关注新的数据科学原生 Elasticsearch 客户端，Eland。在本章中，我们将探讨以下主题：
- en: Learning to use transforms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习使用转换
- en: Using Painless for advanced transform configurations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Painless 进行高级转换配置
- en: Working with Python and Elasticsearch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 和 Elasticsearch 进行操作
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The material in this chapter requires Elasticsearch version 7.9 or above and
    Python 3.7 or above. Code samples and snippets required for this chapter will
    be added under the folder `Chapter 9 - Introduction to Data Frame Analytics` in
    the book's GitHub repository ([https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics)).
    In such cases where some examples require a specific newer release of Elasticsearch,
    this will be mentioned before the example is presented.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to use transforms
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to dive right into the world of transforming stream
    or event-based data, such as logs, into an entity-centric index.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Why are transforms useful?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think about the most common data types that are ingested into Elasticsearch.
    These will often be documents recording some kind of time-based or sequential
    event, for example, logs from a web server, customer purchases from a web store,
    comments published on a social media platform, and so forth.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: While this kind of data is useful for understanding the behavior of our systems
    over time and is perfect for use with technologies such as anomaly detection,
    it is harder to make stream- or event-based datasets work with **Data Frame Analytics**
    features without first aggregating or transforming them in some way. For example,
    consider an e-commerce store that records purchases made by customers. Over a
    year, there may be tens or hundreds of transactions for each customer. If the
    e-commerce store then wants to find a way to use outlier detection to detect unusual
    customers, they will have to transform all of the transaction data points for
    each customer and summarize certain key metrics such as the average amount of
    money spent per purchase or number of purchases per calendar month. In *Figure
    9.1*, we have a simplified illustration that depicts the process of taking transaction
    records from e-commerce purchases made by two customers and converting them into
    an entity-centric index that describes the total quantity of items purchased by
    these customers, as well as the average price paid per order.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – A diagram illustrating the process of taking e-commerce transactions
    and transforming them into an entity-centric index'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – A diagram illustrating the process of taking e-commerce transactions
    and transforming them into an entity-centric index
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to perform the transformation depicted in *Figure 9.1*, we have to
    group each of the documents in the transaction index by the name of the customer
    and then perform two computations: sum up the quantity of items in each transaction
    document to get a total sum and also compute the average price of purchases for
    each customer. Doing this manually for all of the transactions for each of the
    thousands of potential customers would be extremely arduous, which is where **transforms**
    come in.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行 *图 9.1* 中展示的转换，我们必须按客户名字对事务索引中的每个文档进行分组，然后执行两个计算：将每个事务文档中的商品数量加起来得到总数，并为每个客户计算购买的平均价格。手动为成千上万的潜在客户中的每个客户的每笔交易做这件事将会非常困难，这就是
    **转换** 发挥作用的地方。
- en: The anatomy of a transform
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换的解剖结构
- en: 'Although we are going to start off our journey into transforms with simple
    examples, many real-life use cases can very quickly get complicated. It is useful
    to keep in mind two things that will help you keep your bearing as you apply transforms
    to your own data projects: the **pivot** and the **aggregations**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将会从简单的示例开始我们的转换之旅，但许多实际应用场景可能会很快变得复杂。记住以下两点将有助于你在应用转换到自己的数据项目时保持方向：**数据透视**和**聚合**。
- en: 'Let''s examine how these two entities complement each other to help us transform
    a stream-based document into an entity-centric index. In our customer analytics
    use case, we have many different features describing each customer: the name of
    the customer, the total price they paid for each of their products at checkout,
    the list of items they purchased, the date of the purchase, the location of the
    customer, and so forth.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这两个实体如何相互补充，帮助我们将基于流的文档转换为以实体为中心的索引。在我们的客户分析用例中，我们有许多不同的特征来描述每个客户：客户的名字，他们在结账时为每个产品支付的总价，他们购买的商品列表，购买日期，客户的地理位置等等。
- en: The first thing we want to pick is the entity for which we are going to construct
    our entity-centric index. Let's start with a very simple example and say that
    our goal is to find out how much each customer spent on average per purchase during
    our time period and how much they spent in total. Thus, the entity we want to
    construct the index for – our pivot – is the name of the customer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先想要选择的是我们将为其构建以实体为中心的索引的实体。让我们从一个非常简单的例子开始，比如说我们的目标是找出在特定时间段内每个客户平均每笔消费的金额以及他们总共花费了多少钱。因此，我们想要为它构建索引的实体
    – 我们的 **数据透视** – 是客户的名字。
- en: Most of the customers in our source index have more than one transaction associated
    with them. Therefore, if we try to group our index by customer name, for each
    customer we will have multiple documents. In order to pivot successfully using
    this entity, we need to decide which aggregate quantities (for example, the average
    price per order paid by the customer) we want to bring into our entity-centric
    index. This will, in turn, determine which aggregations we will define in our
    transform configuration. Let's take a look at how this works out with a practical
    example.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们源索引中的大多数客户都与多个交易相关联。因此，如果我们尝试按客户名字对索引进行分组，对于每个客户我们都会有多个文档。为了成功使用这个实体进行数据透视，我们需要决定我们想要将哪些聚合数量（例如，客户每笔订单的平均支付价格）带入我们的以实体为中心的索引中。这将反过来决定我们在转换配置中定义哪些聚合。让我们通过一个实际例子来看看这是如何工作的。
- en: Using transforms to analyze e-commerce orders
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用转换来分析电子商务订单
- en: 'In this section, we will use the Kibana E-Commerce sample dataset to illustrate
    some of the basic transformation concepts outlined in the preceding section:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Kibana 电子商务样本数据集来说明上一节中概述的一些基本转换概念：
- en: Import the `kibana_sample_data_ecommerce` and populate it with the dataset.![Figure
    9.2 – Import the Sample eCommerce orders dataset from the Kibana Sample data panel
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `kibana_sample_data_ecommerce` 并用数据集填充它。![图 9.2 – 从 Kibana 样本数据面板导入 Sample
    eCommerce 订单数据集
- en: '](img/B17040_09_002.jpg)'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_09_002.jpg]'
- en: Figure 9.2 – Import the Sample eCommerce orders dataset from the Kibana Sample
    data panel
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.2 – 从 Kibana 样本数据面板导入 Sample eCommerce 订单数据集
- en: Navigate to the **Transforms** wizard by bringing up the Kibana slide-out panel
    menu from the hamburger button in the top left-hand corner, navigating to **Stack
    Management**, and then clicking **Transforms** under the **Data** menu.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从左上角的汉堡按钮打开 Kibana 滑出面板菜单，导航到 **堆栈管理**，然后在 **数据** 菜单下点击 **Transforms** 来访问
    **Transforms** 向导。
- en: In the `kibana_sample_data_ecommerce` index, which you should select in the
    panel shown in *Figure 9.3*. The source indices displayed in your Kibana may look
    a bit different depending on the indices currently available in your Elasticsearch
    cluster.![Figure 9.3 – For this tutorial, please select kibana_sample_data_ecommerce
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**kibana_sample_data_ecommerce**索引中，你应该在*图9.3*所示的面板中选择。你在Kibana中显示的源索引可能会根据你的Elasticsearch集群中当前可用的索引而有所不同。![Figure
    9.3 – For this tutorial, please select kibana_sample_data_ecommerce
- en: '](img/B17040_09_003.jpg)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_09_003.jpg]'
- en: Figure 9.3 – For this tutorial, please select kibana_sample_data_ecommerce
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.3 – For this tutorial, please select kibana_sample_data_ecommerce
- en: After selecting our source index, the `customer_full_name`.![Figure 9.4 – Select
    the entity you want to pivot your source index by in the Group by menu
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们的源索引后，`customer_full_name`。![Figure 9.4 – Select the entity you want to
    pivot your source index by in the Group by menu
- en: '](img/B17040_09_4.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_09_4.jpg]'
- en: Figure 9.4 – Select the entity you want to pivot your source index by in the
    Group by menu
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.4 – 在**分组依据**菜单中选择你想要以之旋转源索引的实体
- en: 'Now that we have defined the entity to pivot our index by, we will move on
    to the next part in the construction of a transform: the aggregations. In this
    case, we are interested in figuring out the average amount of money the customer
    spent in the e-commerce store per order. During each transaction, which is recorded
    in a document in the source index, the total amount paid by the customer is stored
    in the field `taxful_total_price`.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了旋转索引的实体，我们将继续到转换构建的下一部分：聚合。在这种情况下，我们感兴趣的是找出客户在每个订单中在电子商务商店花费的平均金额。在每次交易中，交易记录在源索引的文档中，客户支付的总金额存储在字段`taxful_total_price`中。
- en: Therefore, the aggregation that we define will operate on this field. In the
    **Aggregations** menu, select **taxful_total_price.avg**. Once you have clicked
    on this selection, the field will appear in the box under **Aggregations** and
    you will see a preview of the pivoted index as shown in *Figure 9.5*.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们定义的聚合将作用于这个字段。在**聚合**菜单中，选择**taxful_total_price.avg**。一旦点击了这个选择，该字段将出现在**聚合**下的框中，你将看到如*图9.5*所示的旋转索引的预览。
- en: '![Figure 9.5 – A preview of the transformed data is displayed to allow a quick
    check that everything is configured as desired.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 9.5 – A preview of the transformed data is displayed to allow a quick
    check that everything is configured as desired.'
- en: '](img/B17040_09_5.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_09_5.jpg]'
- en: Figure 9.5 – A preview of the transformed data is displayed to allow a quick
    check that everything is configured as desired.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.5 – 转换数据的预览显示，以便快速检查一切是否按预期配置。
- en: 'Finally, we will configure the last two items: an ID for the transform job,
    and the name of the destination index that will contain the documents that describe
    our pivoted entities. It is a good idea to leave the **Create index pattern**
    checkbox checked as shown in *Figure 9.6* so that you can easily navigate to the
    destination index in the **Discover** tab to view the results.![Figure 9.6 – Each
    transform needs a transform ID'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将配置最后两个项目：转换作业的ID和将包含描述我们旋转实体的文档的目标索引的名称。像*图9.6*中所示，勾选**创建索引模式**复选框是个好主意，这样你就可以轻松地在**发现**选项卡中导航到目标索引以查看结果。![Figure
    9.6 – Each transform needs a transform ID
- en: '](img/B17040_09_006.jpg)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_09_006.jpg]'
- en: Figure 9.6 – Each transform needs a transform ID
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.6 – 每个转换都需要一个转换ID
- en: The transform ID will be used to identify the transform job and a destination
    index that will contain the documents of the entity-centric index that is produced
    as a result of the transform job.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转换ID将用于识别转换作业和包含由转换作业产生的实体为中心索引的文档的目标索引。
- en: To start the transform job, remember to click **Next** in the **Transform**
    wizard after completing the instructions described in *step 6*, followed by **Create
    and start**. This will launch the transform job and create the pivoted, entity-centric
    index.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始转换作业，记得在完成第6步中描述的说明后，在**转换**向导中点击**下一步**，然后点击**创建并启动**。这将启动转换作业并创建以实体为中心的索引。
- en: After the transform has completed (you will see the progress bar reach 100%
    if all goes well), you can click on the **Discover** button at the bottom of the
    **Transform** wizard and view your transformed documents.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换完成后（如果一切顺利，你会看到进度条达到100%），你可以在**转换**向导的底部点击**发现**按钮，查看你的转换文档。
- en: As we discussed at the beginning of this section, we see from a sample document
    in *Figure 9.7* that the transform job has taken a transaction-centric index,
    which recorded each purchase made by a customer in our e-commerce store and transformed
    it into an entity-centric index that describes a specific analytical transformation
    (the calculation of the average price paid by the customer) grouped by the customer's
    full name.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如本节开头所述，我们从*图9.7*中的一个样本文档中看到，转换作业已经将一个以交易为中心的索引（记录了我们电子商务商店中每位客户所做的每次购买）转换成了一个以实体为中心的索引，该索引描述了特定的分析转换（计算客户支付的平均价格），并按客户的完整姓名进行分组。
- en: '![Figure 9.7 – The result of the transform job is a destination index where
    each document describes the aggregation per each pivoted entity. In this case,
    the average taxful_total_price paid by each customer](img/B17040_09_007.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 转换作业的结果是一个目标索引，其中每个文档描述了每个旋转实体的聚合。在本例中，每位客户支付的含税总价平均值](img/B17040_09_007.jpg)'
- en: Figure 9.7 – The result of the transform job is a destination index where each
    document describes the aggregation per each pivoted entity. In this case, the
    average taxful_total_price paid by each customer
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 转换作业的结果是一个目标索引，其中每个文档描述了每个旋转实体的聚合。在本例中，每位客户支付的含税总价平均值
- en: Congratulations – you have now created and started your first transform job!
    Although it was fairly simple in nature, this basic job configuration is a good
    building block to use for more complicated transformations, which we will take
    a look at in the following sections.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您 – 您现在已创建并启动了您的第一个转换作业！尽管它本质上相当简单，但这个基本作业配置是用于更复杂转换的良好构建块，我们将在接下来的章节中探讨。
- en: Exploring more advanced pivot and aggregation configurations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索更高级的旋转和聚合配置
- en: 'In the previous section, we explored the two parts of a transform: the pivot
    and the aggregations. In the subsequent example, our goal was to use transforms
    on the Kibana sample eCommerce dataset to find out the average amount of money
    our customers spent per order. To solve this problem, we figured out that each
    document that recorded a transaction had a field called `customer.full_name` and
    we used this field to pivot our source index. Our aggregation was an average of
    the field that recorded the total amount of money spent by the customer on the
    order.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了转换的两个部分：旋转和聚合。在随后的示例中，我们的目标是使用转换在Kibana样本电子商务数据集上找到每位客户每笔订单的平均花费金额。为了解决这个问题，我们了解到记录交易的每个文档都有一个名为`customer.full_name`的字段，我们使用这个字段来旋转我们的源索引。我们的聚合是记录客户在订单上花费的总金额的平均值。
- en: 'However, not all questions that we might want to ask of our e-commerce data
    lend themselves to simple pivot or group by configurations like the one discussed
    previously. Let''s explore some more advanced pivot configurations that are possible
    with transforms, with the help of some sample investigations we might want to
    carry out on the e-commerce dataset. If you want to discover all of the available
    pivot configurations, take a look at the API documentation for the pivot object
    at this URL: [https://www.elastic.co/guide/en/elasticsearch/reference/master/put-transform.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/put-transform.html).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非我们想要对电子商务数据提出的所有问题都适合使用之前讨论过的简单旋转或分组配置。让我们探索一些更高级的旋转配置，这些配置可以通过转换实现，借助一些我们可能想要在电子商务数据集上进行的样本调查。如果您想发现所有可用的旋转配置，请查看此URL上的旋转对象API文档：[https://www.elastic.co/guide/en/elasticsearch/reference/master/put-transform.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/put-transform.html)。
- en: 'Suppose that we would like to find out the average amount of money spent per
    order per week in our dataset, and how many unique customers made purchases. In
    order to answer these questions, we will need to construct a new transform configuration:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要在我们的数据集中找出每周每笔订单的平均花费金额，以及有多少独特的客户进行了购买。为了回答这些问题，我们需要构建一个新的转换配置：
- en: Instead of pivoting by the name of the customer, we want to construct a `order_date`,
    which, as the name suggests, records when the order was placed. The **Transform**
    wizard makes this simple since **date_histogram(order_date)** will be one of the
    pre-configured options displayed in the **Group by** dropdown.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不想通过客户的名称进行旋转，而是想构建一个`order_date`，正如其名称所暗示的，它记录了订单的放置时间。**转换**向导使这变得简单，因为**date_histogram(order_date)**将是**分组**下拉菜单中显示的预配置选项之一。
- en: Once you have selected **date_histogram(order_date)** in the **Group by** dropdown,
    direct your attention to the right-hand side of the panel as shown in *Figure
    9.8*. The right-hand side should contain an abbreviation for the length of the
    grouping interval used in the date histogram (for example **1m** for an interval
    of 1 minute). In our case, we are interested in pivoting our index by weeks, so
    we need to choose **1w** from the dropdown.![Figure 9.8 – Adjust the frequency
    of the date histogram from the dropdown
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你在**分组**下拉菜单中选择了**date_histogram(order_date)**，将你的注意力转向面板的右侧，如图9.8所示。右侧应该包含用于日期直方图分组间隔的缩写（例如，**1m**表示1分钟的间隔）。在我们的例子中，我们想要按周旋转我们的索引，所以我们需要从下拉菜单中选择**1w**。![图9.8
    – 从下拉菜单调整日期直方图的频率
- en: '](img/B17040_09_008.jpg)'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_09_008.jpg)'
- en: Figure 9.8 – Adjust the frequency of the date histogram from the dropdown
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.8 – 从下拉菜单调整日期直方图的频率
- en: Next, for our aggregation, let's choose the familiar **avg(total_taxful_price)**.
    After we have made our selection, the **Transform** wizard will display a preview,
    which will show the average price paid by a customer per order, grouped by different
    weeks for a few sample rows. The purpose of the preview is to act as a checkpoint.
    Since transform jobs can be resource-intensive, at this stage it is good to pause
    and examine the preview to make sure the data is transformed into a format that
    you desire.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，对于我们的聚合，让我们选择熟悉的**avg(total_taxful_price)**。在我们做出选择后，**转换**向导将显示一个预览，它将显示客户按不同周次订单的平均支付价格，并显示几行样本数据。预览的目的是作为一个检查点。由于转换作业可能非常消耗资源，在这个阶段停下来检查预览，以确保数据已经转换成你想要的格式是很好的。
- en: Sometimes we might want to interrogate our data in ways that do not lend themselves
    to simple one-tiered group-by configurations like the one we explored in the preceding
    steps. It is possible to nest group-by configurations, as we will see in just
    a moment. Suppose that in our hypothetical e-commerce store example, we would
    also be interested in seeing the average amount of money spent by week and by
    geographic region.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时候我们可能希望以不适合简单单层分组配置的方式查询我们的数据，就像我们在前面的步骤中探索的那样。我们可以嵌套分组配置，正如我们马上就会看到的。假设在我们的假设电子商务商店示例中，我们还想看到按周和按地理区域平均花费的金额。
- en: To solve this, let's go back to the **Transform** wizard and add a second group-by
    field. In this case, we want to group by **geoip.region_name**. As before, the
    wizard shows us a preview of the transform once we select the group-by field.
    As in the previous case, it is good to take a moment to look at the rows displayed
    in the preview to make sure the data has been transformed in the desired way.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了解决这个问题，让我们回到**转换**向导，并添加一个第二个分组字段。在这种情况下，我们想要按**geoip.region_name**分组。和之前一样，向导在我们选择分组字段后会显示转换的预览。和之前的情况一样，花点时间看看预览中显示的行，以确保数据已经按照期望的方式转换。
- en: Tip
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小贴士
- en: Click on the **Columns** toggle above the transform preview table to rearrange
    the order of the columns.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击转换预览表上方的**列**切换按钮以重新排列列的顺序。
- en: In addition to creating multiple group-by configurations, we can also add multiple
    aggregations to our transform. Suppose that in addition to the average amount
    of money spent per customer per week and per region, we would also be interested
    in finding out the number of unique customers who placed orders in our store.
    Let's see how we can add this aggregation to our transform.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了创建多个分组配置外，我们还可以向我们的转换中添加多个聚合。假设除了每个客户每周和每个地区平均花费的金额外，我们还想了解在我们商店下单的独特客户数量。让我们看看我们如何将这个聚合添加到我们的转换中。
- en: In the `customer.full_name.keyword`) and click on it to select it. The resulting
    aggregation will be added to your transform configuration and the preview should
    now display one additional column.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`customer.full_name.keyword`)上点击它以选择它。结果聚合将被添加到你的转换配置中，并且预览现在应该显示一个额外的列。
- en: You can now follow the steps outlined in the tutorial of the previous section
    to assign an ID and a destination index for the transform, as well as to create
    and start the job. These will be left as exercises for you.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，你可以遵循上一节教程中的步骤为转换分配一个ID和目标索引，以及创建和启动作业。这些将留给你作为练习。
- en: 'In the previous two sections, we examined the two key components of transforms:
    the pivot and aggregations, and did two different walk-throughs to show how both
    simple and advanced pivot and aggregation combinations can be used to interrogate
    our data for various insights.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节中，我们检查了转换的两个关键组件：旋转和聚合，并进行了两次不同的演练，展示了如何使用简单和高级的旋转和聚合组合来查询我们的数据以获得各种见解。
- en: While following the first transform, you may have noticed that in *Figure 9.6*,
    we left the **Continuous mode** checkbox unchecked. We will take a deeper look
    at what it means to run a transform in continuous mode in the next section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟随第一个转换的过程中，你可能已经注意到在*图9.6*中，我们没有勾选**连续模式**复选框。我们将在下一节中更深入地探讨在连续模式下运行转换意味着什么。
- en: Discovering the difference between batch and continuous transforms
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现批处理和连续转换之间的区别
- en: The first transform we created in the previous section was simple and ran only
    once. The transform job read the source index `kibana_sample_data_ecommerce`,
    which we configured in the `kibana_sample_data_ecommerce` that occur after the
    transform job runs will no longer be reflected in the data in the destination
    index. This kind of transform that runs only once is known as a **batch transform**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们创建的第一个转换很简单，只运行了一次。转换作业读取了配置在`kibana_sample_data_ecommerce`中的源索引`kibana_sample_data_ecommerce`，转换作业运行后发生的任何更改将不会反映在目标索引中的数据中。这种只运行一次的转换被称为**批处理转换**。
- en: In many real-world use cases that produce records of transactions (like in our
    fictitious e-commerce store example), new documents are being constantly added
    to the source index. This means that our pivoted entity-centric index that we
    obtained as a result of running the transform job would be almost immediately
    out of date. One solution to keep the destination index in sync with the source
    index is to keep deleting the destination index and rerunning the batch transform
    job at regular intervals. This, however, is not practical and requires a lot of
    manual effort. This is where **continuous transforms** step in.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际用例中，如我们虚构的电子商务商店示例，会不断向源索引添加新记录。这意味着我们通过运行转换作业获得以实体为中心的旋转索引几乎会立即过时。一个保持目标索引与源索引同步的解决方案是定期删除目标索引并重新运行批处理转换作业。然而，这并不实用，需要大量的手动工作。这就是**连续转换**介入的地方。
- en: If we have a source index that is being updated and we want to use that to create
    a pivoted entity-centric index, then we have to use a continuous transform instead
    of a batch transform. Let's explore continuous transforms in a bit more detail
    to understand how they differ from batch transforms and what important configuration
    parameters should be considered when running a continuous transform.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个正在更新的源索引，并且想使用它来创建一个以实体为中心的旋转索引，那么我们必须使用连续转换而不是批处理转换。让我们更详细地探讨连续转换，以了解它们与批处理转换有何不同，以及运行连续转换时应考虑哪些重要的配置参数。
- en: First, let's set the stage for the problem we are trying to solve. Suppose we
    have a fictitious microblogging social media platform, where users post short
    updates, assign categories to the updates and interact with other users as well
    as predefined topics. It is possible to share a post and like a post. Statistics
    for each post are recorded as well. We have written some Python code to help generate
    this dataset. This code and accompanying instructions for how to run this code
    are available under the `Chapter 9 - Introduction to Data Frame Analytics` folder
    in the GitHub repository for this book ([https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics)).
    After running the generator, you will have an index called `social-media-feed`
    that will contain a number of documents.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为我们要解决的问题设定场景。假设我们有一个虚构的微博社交媒体平台，用户可以发布简短更新，为更新分配类别，并与其他用户以及预定义的主题互动。可以分享帖子并点赞帖子。每个帖子的统计数据也会被记录。我们已经编写了一些Python代码来帮助生成这个数据集。这段代码以及如何运行此代码的说明可以在本书GitHub仓库的`第9章
    - 数据帧分析简介`文件夹下找到（[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics)）。运行生成器后，您将有一个名为`social-media-feed`的索引，其中将包含多个文档。
- en: Each document in the dataset records a post that the user has made on the social
    media platform. For the sake of brevity, we have excluded the text of the post
    from the document. *Figure 9.9* shows a sample document in the `social-media-feed`
    index.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个文档都记录了用户在社交媒体平台上发布的帖子。为了简洁起见，我们已从文档中省略了帖子的文本。"图9.9"显示了`social-media-feed`索引中的一个示例文档。
- en: '![Figure 9.9 – A sample document in the social-media-feed index records the
    username, the time the post was submitted to the platform, as well as some basic
    statistics about the engagement the post received'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.9 – 社交媒体信息流索引中的示例文档记录了用户名、帖子提交到平台的时间以及帖子收到的参与度的一些基本统计数据'
- en: '](img/B17040_09_009.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_09_009.jpg]'
- en: Figure 9.9 – A sample document in the social-media-feed index records the username,
    the time the post was submitted to the platform, as well as some basic statistics
    about the engagement the post received
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 社交媒体信息流索引中的示例文档记录了用户名、帖子提交到平台的时间以及帖子收到的参与度的一些基本统计数据
- en: In the next section, we will see how to use this fictional social media platform
    dataset to learn about continuous transforms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用这个虚构的社交媒体平台数据集来了解连续变换。
- en: Analyzing social media feeds using continuous transforms
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用连续变换分析社交媒体信息流
- en: In this section, we will be using the dataset introduced previously to explore
    the concept of continuous transforms. As we discussed in the previous section,
    batch transforms are useful for one-off analyses where we are either happy to
    analyze a snapshot of the dataset at a particular point in time or we do not have
    a dataset that is changing. In most real-world applications, this is not the case.
    Log files are continuously ingested, many social media platforms have around the
    clock activity, and e-commerce platforms serve customers across all time zones
    and thus generate a stream of transaction data. This is where continuous transforms
    step in.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用之前介绍的数据集来探索连续变换的概念。正如我们在上一节中讨论的，批量变换适用于一次性分析，在这种情况下，我们可能愿意分析在特定时间点的数据集快照，或者我们没有正在变化的数据集。在大多数实际应用中，情况并非如此。日志文件是持续摄入的，许多社交媒体平台全天候活跃，电子商务平台服务于所有时区的客户，因此生成一系列交易数据。这就是连续变换介入的地方。
- en: 'Let''s see how we can analyze the average level of engagement (likes and shares)
    received by a social media user using continuous transforms:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用连续变换分析社交媒体用户收到的平均参与度水平（点赞和分享）：
- en: Navigate to the **Transforms** wizard. On the **Stack Management** page, look
    to the left under the **Data** section and select **Transforms**.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**变换**向导。在**堆栈管理**页面，在**数据**部分下向左查看，并选择**变换**。
- en: Just as we did in the previous sections, let's start by creating the transform.
    For the source index, select the `social-media-feed` index pattern. This should
    give you a view similar to the one in *Figure 9.10*.![Figure 9.10 – The Transforms
    wizard shows a sample of the social-media-feed index
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中所做的那样，让我们首先创建转换。对于源索引，选择 `social-media-feed` 索引模式。这应该会给你一个类似于 *图
    9.10* 中的视图。![图 9.10 – 转换向导显示了 social-media-feed 索引的示例
- en: '](img/B17040_09_010.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_09_010.jpg)'
- en: Figure 9.10 – The Transforms wizard shows a sample of the social-media-feed
    index
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.10 – 转换向导显示了 social-media-feed 索引的示例
- en: In this case, we will be interested in computing aggregations of the engagement
    metrics of each post per username. Therefore, our **Group by** configuration will
    include the username, while our aggregations will compute the total likes and
    shares per user, the average likes and shares per user as well as the total number
    of posts each user has made. The final **Group by** and **Aggregations** configurations
    should look something like *Figure 9.11*.![Figure 9.11 – Group by and Aggregations
    configuration for our continuous transform
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将对计算每个用户名下每篇帖子的参与度指标聚合感兴趣。因此，我们的 **按组** 配置将包括用户名，而我们的聚合将计算每个用户的总点赞和分享数，以及每个用户的平均点赞和分享数以及每个用户发布的总帖数。最终的
    **按组** 和 **聚合** 配置应该类似于 *图 9.11*。![图 9.11 – 我们连续转换的按组与聚合配置
- en: '](img/B17040_09_011.jpg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_09_011.jpg)'
- en: Figure 9.11 – Group by and Aggregations configuration for our continuous transform
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.11 – 我们连续转换的按组与聚合配置
- en: Finally, tick the `timestamp` as shown in *Figure 9.12*.![Figure 9.12 – Select
    Continuous mode to make sure the transform process periodically checks
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，如 *图 9.12* 所示，勾选 `timestamp`。![图 9.12 – 选择连续模式以确保转换过程定期检查
- en: the source index and incorporates new documents into the destination index
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 源索引并将新文档纳入目标索引
- en: '](img/B17040_09_012.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_09_012.jpg)'
- en: Figure 9.12 – Select Continuous mode to make sure the transform process periodically
    checks the source index and incorporates new documents into the destination index
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.12 – 选择连续模式以确保转换过程定期检查源索引并将新文档纳入目标索引
- en: Once you click `social-media-feed` index running. Note the continuous tag in
    the job description.![Figure 9.13 – Continuous transforms shown on the Transforms
    page. Note that the mode is tagged as continuous
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你点击 `social-media-feed` 索引运行。注意作业描述中的连续标签。![图 9.13 – 在转换页面上显示的连续转换。注意模式被标记为连续
- en: '](img/B17040_09_013.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_09_013.jpg)'
- en: Figure 9.13 – Continuous transforms shown on the Transforms page. Note that
    the mode is tagged as continuous
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.13 – 在转换页面上显示的连续转换。注意模式被标记为连续
- en: 'Let''s insert some new posts into our index `social-media-feed` and see how
    the statistics for the user Carl change after a new document is added to the source
    index for the transform. To insert a new post, open the Kibana `chapter9` in the
    book''s GitHub repository for a version that you can easily copy and paste into
    your own Kibana Dev Console if you are following along):'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在索引 `social-media-feed` 中插入一些新帖子，并查看在将新文档添加到转换的源索引后，用户 Carl 的统计数据如何变化。要插入新帖子，打开书籍
    GitHub 仓库中的 `chapter9` 的 Kibana `chapter9` 版本，如果你正在跟随，你可以轻松地将它复制并粘贴到自己的 Kibana
    开发控制台中：
- en: '[PRE0]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, that we have added a new document into the source index `social-media-feed`,
    we expect that this document will be picked up by the continuous transform job
    and incorporated into our transform's destination index, `social-media-feed-engagement`.
    *Figure 9.14* showcases the transformed entry for the username Carl.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经将一个新文档添加到源索引 `social-media-feed` 中，我们预计这个文档将被连续转换作业选中并纳入我们的转换目标索引 `social-media-feed-engagement`。*图
    9.14* 展示了用户名 Carl 的转换条目。
- en: '![Figure 9.14 – The destination index of the continuous transform job holds
    an entry for the new username Carl, which we added manually through the Kibana
    Dev Console'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14 – 连续转换作业的目标索引包含一个新用户名 Carl 的条目，这是我们通过 Kibana 开发控制台手动添加的'
- en: '](img/B17040_09_014.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_09_014.jpg)'
- en: Figure 9.14 – The destination index of the continuous transform job holds an
    entry for the new username Carl, which we added manually through the Kibana Dev
    Console
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 连续转换作业的目标索引包含一个新用户名 Carl 的条目，这是我们通过 Kibana 开发控制台手动添加的
- en: The preceding example gives a very simplified walk-through of how continuous
    transforms work and how you can create your own continuous transform using the
    **Transforms** wizard available in Kibana. In [*Chapter 13*](B17040_13_Epub_AM.xhtml#_idTextAnchor236),
    *Inference*, we will return to the topic of continuous transforms when we showcase
    how to combine trained machine learning models, inference, and transforms.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例提供了一个非常简化的连续转换工作原理的概述，以及如何使用Kibana中可用的**转换向导**创建自己的连续转换。在[*第13章*](B17040_13_Epub_AM.xhtml#_idTextAnchor236)“推理”中，当我们展示如何结合训练好的机器学习模型、推理和转换时，我们将回到连续转换的主题。
- en: For now, we will take a brief detour into the world of the **scripting language
    Painless**. While the Transforms wizard and the many pre-built **Group by** and
    **Aggregations** configurations that it offers suffice for many of the most common
    data analysis use cases, more advanced users will wish to define their own aggregations.
    A common way to do this is with the aid of the Elasticsearch embedded scripting
    language, Painless.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将简要地进入**脚本语言Painless**的世界。虽然转换向导和它提供的许多预构建的**按组**和**聚合**配置足以满足许多常见的数据分析用例，但对于更高级的用户来说，他们可能希望定义自己的聚合。实现这一目标的一种常见方式是借助Elasticsearch内嵌的脚本语言Painless。
- en: In the next section, we will take a little tour of the Painless world, which
    will prepare you for creating your own advanced transform configurations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要游览Painless的世界，这将为你创建自己的高级转换配置做好准备。
- en: Using Painless for advanced transform configurations
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Painless进行高级转换配置
- en: As we have seen in many of the previous sections, the built-in pivot and aggregation
    options allow us to analyze and interrogate our data in various ways. However,
    for more custom or advanced use cases, the built-in functions may not be flexible
    enough. For these use cases, we will need to write custom pivot and aggregation
    configurations. The flexible scripting language that is built into Elasticsearch,
    **Painless**, allows us to do this.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在许多前面的章节中看到的，内置的交叉和聚合选项允许我们以各种方式分析和查询我们的数据。然而，对于更定制或高级的用例，内置函数可能不够灵活。对于这些用例，我们需要编写自定义的交叉和聚合配置。集成在Elasticsearch中的灵活脚本语言**Painless**允许我们做到这一点。
- en: In this section, we will introduce Painless, illustrate some tools that are
    useful when working with Painless, and then show how Painless can be applied to
    create custom Transform configurations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍Painless，展示一些在处理Painless时有用的工具，然后展示如何将Painless应用于创建自定义转换配置。
- en: Introducing Painless
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Painless
- en: Painless is a scripting language that is built into Elasticsearch. We will take
    a look at Painless in terms of variables, control flow constructs, operations,
    and functions. These are the basic building blocks that will help you develop
    your own custom scripts to use with transforms. Without further ado, let's dive
    into the introduction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Painless是一种集成在Elasticsearch中的脚本语言。我们将从变量、控制流结构、操作和函数的角度来探讨Painless。这些是帮助你开发自己的自定义脚本来与转换一起使用的基石。现在，让我们直接进入介绍部分。
- en: 'It is likely that many readers of this book come from a sort of programming
    language background. You may have written data cleaning scripts with Python, programmed
    a Linux machine with bash scripts, or developed enterprise software with Java.
    Although these languages have many differences and are useful for different purposes,
    they all have shared building blocks that help human readers of the language understand
    them. Although there is an almost infinite number of approaches to teaching a
    programming language, the approach we will take here will be based on understanding
    the following basic topics about Painless: variables, operations (such as addition,
    subtraction, and various Boolean tests), control flow (if-else constructs and
    for loops) and functions. These are analogous concepts that users familiar with
    another programming language should be able to relate to. In addition to these
    concepts, we will be looking at some aspects that are particular to Painless,
    such as different execution contexts.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能这本书的许多读者都来自某种编程语言背景。你可能使用 Python 编写过数据清洗脚本，使用 bash 脚本编程 Linux 机器，或者使用 Java
    开发企业软件。尽管这些语言在许多方面都有所不同，并且适用于不同的目的，但它们都有共同的构建块，有助于语言的人类读者理解它们。尽管教授编程语言的方法几乎无限，但我们将采取的方法将基于理解以下关于
    Painless 的基本主题：变量、操作（如加法、减法和各种布尔测试）、控制流（if-else 构造和 for 循环）和函数。这些是与熟悉其他编程语言的用户相关联的类似概念。除了这些概念之外，我们还将探讨一些特定于
    Painless 的方面，例如不同的执行上下文。
- en: When learning a new programming language, it is important to have a playground
    that can be used to experiment with syntax. Luckily, with the 7.10 version of
    Elasticsearch, the **Dev Tools** app now contains the new **Painless Lab** playground,
    where you can try out the code samples presented in this chapter as well as any
    code samples you write on your own.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习一门新的编程语言时，拥有一个可以用来实验语法的游乐场非常重要。幸运的是，随着 Elasticsearch 7.10 版本的推出，**Dev Tools**
    应用现在包含新的 **Painless Lab** 游乐场，您可以在其中尝试本章中展示的代码示例，以及您自己编写的任何代码示例。
- en: The Painless Lab can be accessed by navigating to **Dev Tools** as shown in
    *Figure 9.15* and then, in the top menu of the **Dev Tools** page, selecting **Painless
    Lab**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过导航到如图 9.15 所示的 **Dev Tools** 来访问 Painless 实验室，然后在 **Dev Tools** 页面的顶部菜单中选择
    **Painless Lab**。
- en: '![Figure 9.15 – The link to the Dev Tools page is located in the lower section
    of the Kibana side menu. Select it to access the interactive Painless lab environment'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.15 – Dev Tools 页面的链接位于 Kibana 侧菜单的下半部分。选择它以访问交互式的 Painless 实验室环境'
- en: '](img/B17040_09_015.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_09_015.jpg]'
- en: Figure 9.15 – The link to the Dev Tools page is located in the lower section
    of the Kibana side menu. Select it to access the interactive Painless lab environment
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – Dev Tools 页面的链接位于 Kibana 侧菜单的下半部分。选择它以访问交互式的 Painless 实验室环境
- en: This will open an embedded Painless code editor as shown in *Figure 9.16*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开如图 9.16 所示的嵌入式 Painless 代码编辑器。
- en: '![Figure 9.16 – The Painless Lab in Dev Tools features an embedded code editor.
    The Output window shows the evaluation result of the code in the code editor'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.16 – Dev Tools 中的无痛苦实验室特色嵌入式代码编辑器。输出窗口显示了代码编辑器中代码的评估结果'
- en: '](img/B17040_09_016.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_09_016.jpg]'
- en: Figure 9.16 – The Painless Lab in Dev Tools features an embedded code editor.
    The Output window shows the evaluation result of the code in the code editor
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – Dev Tools 中的 Painless 实验室特色嵌入式代码编辑器。输出窗口显示了代码编辑器中代码的评估结果
- en: The code editor in the Painless Lab is preconfigured with some sample functions
    and variable declarations to illustrate how one might draw the figure in the **Output**
    window in *Figure 9.16* using Painless. For now, you can delete this code to make
    space for your own experiments that you will carry out as you read the rest of
    this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Painless 实验室中的代码编辑器预先配置了一些示例函数和变量声明，以说明如何使用 Painless 在如图 9.16 所示的 **输出** 窗口中绘制图形。目前，您可以删除此代码，为阅读本章剩余部分时进行的实验腾出空间。
- en: Tip
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'The full Painless language specification is available online here: [https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-lang-spec.html](https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-lang-spec.html).
    You can use it as a reference and resource for further information about the topics
    covered later.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Painless 语言的完整规范可在以下网址在线获取：[https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-lang-spec.html](https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-lang-spec.html)。您可以用它作为参考和资源，以获取关于后面涵盖主题的更多信息。
- en: Variables, operators, and control flow
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the first things we usually want to do in a programming language is
    to manipulate values. In order to do this effectively, we assign those values
    names or variables. Painless has types and before a variable can be assigned,
    it must be declared along with its type. The syntax for declaring a variable is
    as follows: **type_identifier variable_name ;**.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'How to use this syntax in practice is illustrated in the following code block,
    where we declare variables `a` and `b` to hold integer values, the variable `my_string`
    to hold a string value, and the variable `my_float_array` to hold an array of
    floating values:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At this point, the variables do not yet hold any non-null values. They have
    just been initialized in preparation for an assignment statement, which will assign
    to each a value of the appropriate type. Thus, if you try copying the preceding
    code block into the Painless Lab code editor, you will see an output of **null**
    in the **Output** window as shown in *Figure 9.17*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – On the left, Painless variables of various types are initialized.
    On the right, the Output panel shows null, because these variables have not yet
    been assigned a value'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_017.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.17 – On the left, Painless variables of various types are initialized.
    On the right, the Output panel shows null, because these variables have not yet
    been assigned a value
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The Painless Lab code editor only displays the result of the last statement.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s assign some values to these variables so that we can do some interesting
    things with them. The assignments are shown in the following code block. In the
    first two lines, we assign integer values to our integer variables `a` and `b`.
    In the third line, we assign a string `"hello world"` to the string variable `my_string`,
    and in the final line, we initialize a new array with floating-point values:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s do some interesting things with these variables to illustrate what operators
    are available in Painless. We will only be able to cover a few of the available
    operators. For the full list of available operators, please see the Painless language
    specification ([https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators.html](https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators.html)).
    The following code blocks illustrate basic mathematical operations: addition,
    subtraction, division, and multiplication as well as the modulus operation or
    taking the remainder:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Try out these code examples on your own in the Painless Lab and you should be
    able to see the results of your evaluation, as illustrated in the case of addition
    in *Figure 9.18*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Using the Painless Lab code editor and console for addition
    in Painless. The result stored in a variable called "addition" in the code editor
    on the left is displayed in the Output tab on the right'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_018.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.18 – Using the Painless Lab code editor and console for addition in
    Painless. The result stored in a variable called "addition" in the code editor
    on the left is displayed in the Output tab on the right
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 在 Painless 中使用 Painless Lab 代码编辑器和控制台进行加法。在左侧代码编辑器中存储在名为 "addition"
    的变量中的结果在右侧的输出选项卡中显示
- en: In addition to mathematical operations, we will also take a look at **Boolean
    operators**. These are vital for many Painless scripts and configurations as well
    as for **control flow statements**, which we will take a look at afterward.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数学运算之外，我们还将探讨 **布尔运算符**。这些运算符对于许多 Painless 脚本和配置以及控制流语句至关重要，我们将在之后进行探讨。
- en: 'The code snippets that follow illustrate how to declare a variable to hold
    a Boolean (true/false) value and how to use comparison operators to determine
    whether values are less, greater, less than or equal, or greater than or equal.
    For a full list of Boolean operators in Painless, please consult the Painless
    specification available here: [https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-boolean.html](https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-boolean.html):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段说明了如何声明一个变量来存储布尔（真/假）值，以及如何使用比较运算符来确定值是小于、大于、小于等于还是大于等于。有关 Painless 中布尔运算符的完整列表，请参阅此处可用的
    Painless 规范：[https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-boolean.html](https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-boolean.html)
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As an exercise, copy the preceding code block into the Painless Lab code editor.
    If you wish, you can view the contents of each of these variables by typing its
    name followed by a semicolon into the last line of the Painless Lab code editor
    and the value stored in the variable will be printed in the **Output** window
    on the right, as shown in *Figure 9.19*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，将前面的代码块复制到 Painless Lab 代码编辑器中。如果你愿意，可以在 Painless Lab 代码编辑器的最后一行输入变量的名称后跟一个分号，这样就可以在右侧的
    **输出** 窗口中打印出变量存储的值，如图 *9.19* 所示。
- en: '![Figure 9.19 – Typing in the variable name followed by a semicolon in the
    Painless Lab code editor will output the contents of the variable in the Output
    tab'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.19 – 在 Painless Lab 代码编辑器中输入变量名后跟一个分号，将在输出选项卡中输出变量的内容'
- en: '](img/B17040_09_019.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_09_019.jpg)'
- en: Figure 9.19 – Typing in the variable name followed by a semicolon in the Painless
    Lab code editor will output the contents of the variable in the Output tab
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 在 Painless Lab 代码编辑器中输入变量名后跟一个分号，将在输出选项卡中输出变量的内容
- en: 'While the Boolean operator illustrated here is useful in many numerical computations,
    we probably could not write effective control flow statements without the equality
    operators `==` and `!=`, which check whether or not two variables are equal. The
    following code block illustrates how to use these operators with a few practical
    examples:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这里展示的布尔运算符在许多数值计算中很有用，但我们可能无法没有等式运算符 `==` 和 `!=` 来编写有效的控制流语句，这两个运算符用于检查两个变量是否相等。以下代码块通过几个实际示例说明了如何使用这些运算符：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Last but not least in our tour of the Boolean operators in Painless, we will
    look at a code block that showcases how to use the `instanceof` operator, which
    checks whether a given variable is an instance of a type and returns `true` or
    `false`. This is a useful operator to have when you are writing Painless code
    that you only want to operate on variables of a specified type:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，在我们对 Painless 中的布尔运算符的探索中，我们将查看一个代码块，展示了如何使用 `instanceof` 运算符，该运算符检查给定变量是否是某个类型的实例，并返回
    `true` 或 `false`。当你编写只想在指定类型的变量上操作的 Painless 代码时，这是一个非常有用的运算符：
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the final part of this section, let''s take a look at one of the most important
    building blocks in our Painless script: `if-else` statements is shown in the following
    code block with the help of an example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后部分，让我们看看 Painless 脚本中最重要构建块之一：以下代码块通过一个示例展示了 `if-else` 语句的使用：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code block, we declare an integer variable, `a`, and assign
    it to contain the integer value `5`. We then declare another integer variable,
    `sum`. This variable will change according to the execution branch that is taken
    in the `if-else` statement. Finally, we see that the `if-else` statement first
    checks whether the integer variable `a` is less than 6, and if it is, stores the
    result of adding a and the integer 5 in the variable sum. If not, the amount stored
    in the variable `sum` is the result of subtracting 5 from `a`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: If you type this code in the Painless Lab code editor, the `sum` as 10 (as shown
    in *Figure 9.20*), which is what we expect based on the previous analysis.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – The if-else statement results in the sum variable being set
    to the value 10'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_020.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.20 – The if-else statement results in the sum variable being set to
    the value 10
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will take a look at how to write a `for` loop, which is useful
    for various data analysis and data processing tasks with Painless. In our `for`
    loop, we will be iterating over a string variable and calculating how many occurrences
    of the letter `a` occur in the string. This is a very simple example, but will
    hopefully help you to understand the syntax so that you can apply it in your own
    examples:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Copy and paste this code sample (a copy can be found in the GitHub repository
    [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition)
    for this book under the folder `Chapter 9 - Introduction to Data Frame Analytics`)
    into your Painless Lab and you will see that the `counter` variable in the `3`,
    as we expect, since there are three occurrences of the letter "a" in the string
    "a beautiful day."
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Functions
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have covered variables, operators, and control flow statements,
    let's turn our attention for a moment to **functions**. Sometimes, we might notice
    ourselves writing the same lines of code over and over again, across multiple
    different scripts and configurations. At this point, it might be more economical
    to package up the lines that we find ourselves reaching for over and over again
    into a reusable piece of code that we can reference with a name from our Painless
    scripts.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Let's return to the example where we wrote a `for` loop with an `if` statement
    to calculate the instances of the letter "a" in a given string. Suppose we want
    to reuse this functionality and make it slightly more generic. This is a perfect
    opportunity to package up this piece of code as a Painless function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three parts to writing a function in Painless. First, we write the
    function header, which specifies the type of the value the function returns, as
    well as the name of the function. This is what we will use to refer to the function
    when we use it in our scripts or `letterCounter`, is shown in the following code
    block:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `int` in front of the function name determines the type of value returned
    by this function. Since we are interested in the number of occurrences of a particular
    letter in a particular string, we will be returning an integer count. The parentheses
    after the name `letterCounter` will hold the parameters that the function accepts.
    For now, we have not specified any parameters, so there is nothing between the
    parentheses. Finally, the two curly braces signify the place for the function
    body – this is where all of the logic of the function will reside.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have investigated the elements that go into creating a basic function
    header, let''s populate the function body (the space between the curly braces
    with the code we wrote in the previous section while learning about `for` loops).
    Our function now should look something like the following code block:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you look toward the end of the function body, you will notice that the only
    difference between our `for` loop residing inside the function body is that now
    we have added a `return` statement. This allows our value of interest stored in
    the variable `counter` to be returned to the code calling the function, which
    brings us to our next question. Now that we have written our first Painless function
    and it does something interesting, how do we go about calling this function?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In the Painless Lab environment, we would simply type out `letterCounter();`
    as shown in *Figure 9.21*. The result returned by this function is, as we expect
    based on our previous dissection of this code sample, `3`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – The definition of the sample function letterCounter displayed
    alongside an example of how to call a function in the Painless Lab environment
    ](img/B17040_09_021.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – The definition of the sample function letterCounter displayed
    alongside an example of how to call a function in the Painless Lab environment
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a working function, let's talk about making this function a
    bit more generic, which is what you will often need to do when working with either
    transforms, which we have been discussing in this chapter, or various ingest pipelines
    and script processors, which we will be discussing in [*Chapter 13*](B17040_13_Epub_AM.xhtml#_idTextAnchor236),
    *Inference*. As it currently stands, our function `letterCounter` is very specific.
    It only computes the number of occurrences of a very specific letter – the letter
    a – in a very specific string, the phrase "a beautiful day."
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that to make this snippet of code really useful, we would like to vary
    both the phrase and the letter that is counted. With functions, we can do this
    with minimal code duplication, by configuring **function parameters**. Since we
    want to vary both the letter and the phrase, let''s make those two into function
    parameters. After the change, our function definition will look as in the following
    code block:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Notice that now, in between the parentheses in the function header, we have
    defined two parameters: one `sample_string` parameter representing the phrase
    in which we want to count the occurrences of the second parameter, `count_letter`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'To call this function, we will first define new variables to hold our phrase
    ("a beautiful day," once again) and our letter of interest – this time the letter
    "b" instead of "a." Following this, we will pass both of these variables into
    the function call as shown in the following code block:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since there is only one occurrence of the letter "b" in our phrase of interest,
    we will expect the result of executing this function to be `1`, as is the case
    in *Figure 9.22*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – The result of calling the function is shown in the Output panel
    on the right'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_022.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.22 – The result of calling the function is shown in the Output panel
    on the right
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Now you should be equipped to write your own Painless code! This will come in
    handy in [*Chapter 13*](B17040_13_Epub_AM.xhtml#_idTextAnchor236), *Inference*,
    where we will use advanced Painless features to perform feature extractions and
    to write script processors.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Working with Python and Elasticsearch
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, Python has become the dominant language for many data-intensive
    projects. Fueled by its easy-to-use machine learning and data analysis libraries,
    many data scientists and data engineers are now heavily relying on Python for
    most of their daily operations. Therefore, no discussions of machine learning
    in the Elastic Stack would be complete without exploring how a data analysis professional
    can work with the Elastic Stack in Python.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take a look at the three official Python Elasticsearch
    clients, understand the differences between them, and discuss when one might want
    to use one over the others. We will demonstrate how usage of Elastic Stack ML
    can be automated by using Elasticsearch clients. In addition, we will take a deeper
    look at **Eland**, the new data science native client that enables efficient in-memory
    data analysis backed by Elasticsearch. After exploring how Eland works, we will
    illustrate how Eland can be combined with Jupyter notebooks, an open source interactive
    data analysis environment to analyze data stored in Elasticsearch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: A brief tour of the Python Elasticsearch clients
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anyone who has used the Kibana Dev Tools Console to communicate with Elasticsearch
    knows that most things are carried out via the REST API. You can insert, update,
    and delete documents by calling the right endpoint with the right parameters.
    Not surprisingly, there are several levels of abstraction at which a client program
    calling these REST API endpoints can be written. The low-level client **elasticsearch-py**
    ([https://elasticsearch-py.readthedocs.io/en/v7.10.1/](https://elasticsearch-py.readthedocs.io/en/v7.10.1/))
    provides a thin Python wrapper on the REST API calls that one would normally execute
    through the Kibana Dev Console or through some application capable of sending
    HTTP requests. The next level of abstraction is captured by the Elasticsearch
    DSL client ([https://elasticsearch-dsl.readthedocs.io/en/latest/](https://elasticsearch-dsl.readthedocs.io/en/latest/)).
    Finally, the most abstracted client is **Eland** ([https://eland.readthedocs.io/en/7.10.1b1/](https://eland.readthedocs.io/en/7.10.1b1/)),
    where the data frame, a tabular representation of data, is a first-class citizen.
    We will see in subsequent examples what this means for data scientists wishing
    to use Eland with Elasticsearch.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the available Elasticsearch clients, we are going to take a moment
    to discuss the various execution environments that are available to any data engineers
    or data scientists wishing to work with Python and Elasticsearch. This, in turn,
    will lead us to an introduction to Jupyter notebooks and the whole Jupyter ecosystem,
    which is a tool to know for anyone wishing to work with machine learning and the
    Elastic Stack.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The first and probably most familiar way to execute a Python program is to write
    our program's logic in a text file – or a script – save it with a `.py` extension
    to signify that it contains Python code, and then invoke the script using the
    command line as shown in *Figure 9.23*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – One way to work with Python is to store our program in a text
    file'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: or a script and then execute it from the command line
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_023.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.23 – One way to work with Python is to store our program in a text
    file or a script and then execute it from the command line
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The second way is to use the interactive Python REPL, which is shown in *Figure
    9.24*. Invoking `python` (or `python3`) on our command line will launch an interactive
    Python environment where we can write functions, define variables, and execute
    all sorts of Python code. While this environment is useful for small-scale or
    quick experiments, in practice it would be hard to work on a long-term or larger
    collaborative data analysis project in the REPL environment. Therefore, for most
    projects involving Python, data analysis, and the Elastic Stack, the environment
    of choice is some sort of integrated development environment that provides a code
    editor along with various tools to support both programming and execution.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – A screenshot depicting a sample Python interactive shell or
    REPL, which is perfect for quick experiments in the Python programming language'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_024.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.24 – A screenshot depicting a sample Python interactive shell or REPL,
    which is perfect for quick experiments in the Python programming language
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: A development environment that is specifically designed with data analysis in
    mind is a Jupyter notebook, which is illustrated in *Figure 9.25*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – A sample Jupyter notebook'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_25.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.25 – A sample Jupyter notebook
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The notebook can be installed in a Python installation through a central package
    management service such as `pip` and is launched by typing `jupyter notebook`
    on the command line. The launched environment runs in a browser such as Chrome
    or Firefox and provides an environment where code snippets, text paragraphs, graphs,
    and visualizations (both interactive and static) can live side by side. Many authors
    have covered the Jupyter Notebook and the library ecosystem that exists around
    it much better than what we have space or time for in this chapter and thus we
    encourage those readers who want to or anticipate working more at the intersection
    of data analysis, Elasticsearch, and Python to take a look at the materials listed
    in the *Further reading* section at the end of this chapter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the motivation behind Eland
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Readers of the previous section might wonder, *what was the motivation of building
    yet another Elasticsearch Python client when the community already has two clients
    to choose from?* Moreover, *why build an entire software library around the idea
    of a* **Data Frame** *object?* The answers to both of these questions could probably
    fill an entire book, so the answers presented here will necessarily leave some
    subtleties unexplored. Nevertheless, we hope that for the interested reader, the
    discussion in this section will give some interesting context into how Eland came
    to be and why it was designed around the idea of the Data Frame.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Although Python appears to be the dominant force for many domains of data analysis
    and machine learning today, this was not always the case. In particular, in the
    early 2010s, the ecosystem was dominated by the statistical processing language
    R, which had a very useful construct – a dataframe object that allowed one to
    analyze rows of data in a table-like structure (a concept that is no doubt familiar
    to users of Excel). At about the same time, Wes McKinney, then working at the
    New York financial firm AQR Capital, started working on a library to make the
    lives of Python data analysts easier. This work culminated in the release of **pandas**,
    an open source data analysis library that is used by thousands of data scientists
    and data engineers.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features that made pandas useful and easy to use was the `DataFrame`
    object. Analogous to the R object, this object made it straightforward to manipulate
    and carry out analyses on data in a tabular manner. Although pandas is very powerful
    and contains a multitude of built-in functions and methods, it begins to hit limitations
    when the dataset one wishes to analyze is too large to fit in memory.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, data analysts often resort to sampling data from the various
    databases, for example, Elasticsearch, where it is stored, exporting it into flat
    files, and then reading it into their Python process so that it can be analyzed
    with pandas or another library. While this approach certainly works, the workflow
    would become more seamless if pandas were to interface directly with the database.
    What if we could transparently interface a `DataFrame` object with Elasticsearch
    so that the data analyst could focus on analyzing data instead of having to worry
    about managing the connection and exporting data from Elasticsearch? This is the
    very grounding idea of Eland. Hopefully, the next sections will demonstrate how
    this philosophy has materialized concretely in the design of this library.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Taking your first steps with Eland
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since Eland is a third-party library, you will first have to install it so
    that your Python installation is able to use it. The instructions for this across
    various operating systems are given under `Chapter 9 - Introduction to Data Frame
    Analytics` in the GitHub repository for this book: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics).
    We will assume that readers wishing to follow along with the material in this
    section of the book will have followed the instructions linked to complete the
    installation of the library. The examples and screenshots in this chapter will
    be illustrated using a Jupyter notebook environment, but it is also possible to
    run the code samples presented in this chapter in a standalone environment (for
    example, from a Python REPL or from a Python script). Examples that specifically
    require the Jupyter notebook environment will be clearly indicated:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The first step that we have to do to make use of Eland is to import it into
    our environment. In Python, this is done using the `import` statement as shown
    in *Figure 9.26*. Note that when using a library such as Eland, it is common to
    assign an alias to the library. In the code snippet shown in *Figure 9.26*, we
    have assigned `eland` the alias `ed` using the keyword `as`. This will save us
    some typing in the future as we will be invoking the library name many times as
    we access its objects and methods.![Figure 9.26 – Importing Eland into the notebook
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_09_26.jpg)'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.26 – Importing Eland into the notebook
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After importing the code into our Jupyter notebook, we are free to start exploring.
    Let''s start with the most basic things one can do with Eland: creating an Eland
    `DataFrame`. To create the `DataFrame`, we need to specify two things: the URL
    of our Elasticsearch cluster (for example, `localhost` if we are running Elasticsearch
    locally on the default port `9200`) and the name of the Elasticsearch index. These
    two parameters are passed into the `DataFrame` constructor as shown in *Figure
    9.27*.![Figure 9.27 – Creating a DataFrame in Eland involves the URL of the Elasticsearch
    cluster and the name of the index that contains the data we wish to analyze'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_09_27.jpg)'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.27 – Creating a DataFrame in Eland involves the URL of the Elasticsearch
    cluster and the name of the index that contains the data we wish to analyze
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One of the first tasks we are interested in doing when we start examining a
    new dataset is learning what the data looks like (usually, it is enough to see
    a few example rows to get the general gist of the data) and some of the general
    statistical properties of the dataset. We can learn the former by calling the
    `head` method on our Eland `DataFrame` object as is shown in *Figure 9.28*.![Figure
    9.28 – Calling the head method on the Eland DataFrame object
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: will show us the first 5 rows in the dataset
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_09_28.jpg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.28 – Calling the head method on the Eland DataFrame object will show
    us the first 5 rows in the dataset
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Knowledge of the latter, on the other hand, is obtained by calling the `describe`
    method, which will be familiar to pandas users and is shown in *Figure 9.29*.![Figure
    9.29 – "describe" summarizes the statistical properties of the numerical columns
    in the dataset
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_09_29.jpg)'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.29 – "describe" summarizes the statistical properties of the numerical
    columns in the dataset
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to obtaining a basic overview of the dataset, we can easily access
    individual values of a given field in the index by using the `get` command in
    conjunction with the name of the field as shown in *Figure 9.30*.![Figure 9.30
    – We can work with the individual field values in the index by using the get method
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_09_30.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.30 – We can work with the individual field values in the index by using
    the get method
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, we can compute aggregations on our numerical columns using the `aggregate`
    method. In the example illustrated in *Figure 9.31*, we select two numerical columns
    `total_unique_products` and `taxful_total_price`, and compute the sum, the minimum,
    and the maximum of the values in these fields across all documents in the index.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Computing aggregations on selected columns is possible'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: in Eland using the aggregate method
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_09_31.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.31 – Computing aggregations on selected columns is possible in Eland
    using the aggregate method
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: While the steps illustrated here are relatively simple, we hope that they've
    showcased how seamlessly it is possible to integrate Elasticsearch, working in
    Python, and a data analysis environment such as the Jupyter Notebook into one,
    seamless data analysis workflow. We will build upon this foundation with Eland
    further in [*Chapter 13*](B17040_13_Epub_AM.xhtml#_idTextAnchor236), *Inference*,
    when we will take a look at more advanced use cases.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we have dipped our toes into the world of Data Frame Analytics,
    a whole new branch of machine learning and data transformation tools that unlock
    powerful ways to use the data you have stored in Elasticsearch to solve problems.
    In addition to giving an overview of the new unsupervised and supervised machine
    learning techniques that we will cover in future chapters, we have studied three
    important topics: transforms, using the Painless scripting language, and the integration
    between Python and Elasticsearch. These topics will form the foundation of our
    future work in the following chapters.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'In our exposition on transforms, we studied the two components – the pivot
    and aggregations – that make up a transform, as well as the two possible modes
    in which to run a transform: batch and continuous. A batch transform runs only
    once and generates a transformation on a snapshot of the source index at a particular
    point in time. This works perfectly for datasets that do not change much or when
    the data transformation needs to be carried out only at a specific point in time.
    For many real-world use cases, such as logging or our familiar e-commerce store
    example, the system being monitored and analyzed is constantly changing. An application
    is constantly logging the activities of its users, an e-commerce store is constantly
    logging new transactions. Continuous transforms are the tools of choice for analyzing
    such streaming datasets.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: While the pre-configured options available in the **Transform** wizard we showcased
    in our examples are suitable for most situations, more advanced users may wish
    to configure their own aggregations. In order to do this (and in general, in order
    to be able to perform many of the more advanced configurations we will discuss
    in later chapters), users need to be familiar with Painless, the scripting language
    that is embedded in Elasticsearch. In particular, we took a look at how to declare
    variables in Painless, how to manipulate those variables with operations, how
    to construct more advanced programs using control flow statements, and finally
    how to package up useful code snippets as functions. All of these will be useful
    in our explorations in later chapters!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we took a whirlwind tour of how to use Python when analyzing
    data stored in Elasticsearch. We took a look at the two existing Python clients
    for Elasticsearch, `elasticsearch-py` and `elasticsearch-dsl` and laid the motivation
    behind the development of the third and newest client, Eland.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will dive into the first of the three new machine learning
    methods added into the Elastic Stack: outlier detection.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the Jupyter ecosystem and, in particular, the Jupyter
    Notebook, have a look at the comprehensive documentation of Project Jupyter, here:
    [https://jupyter.org/documentation](https://jupyter.org/documentation).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are new to Python development and would like to have an overview of
    the language ecosystem and the various tools that are available, have a look at
    the Hitchiker''s Guide to Python, here: [https://docs.python-guide.org/](https://docs.python-guide.org/).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about the pandas project, please see the official documentation
    here: [https://pandas.pydata.org/](https://pandas.pydata.org/).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the Painless embedded scripting language, please see
    the official Painless language specification, here: [https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-lang-spec.html](https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-lang-spec.html).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
