- en: 'Chapter 11: Key Principles for Monitoring Your ML System'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the fundamental principles that are essential
    for monitoring your **machine learning** (**ML**) models in production. You will
    learn how to build trustworthy and Explainable AI solutions using the Explainable
    Monitoring Framework. The Explainable Monitoring Framework can be used to build
    functional monitoring pipelines so that you can monitor ML models in production,
    analyze application and model performance, and govern ML systems. The goal of
    monitoring ML systems is to enable trust, transparency, and explainability in
    order to increase business impact. We will learn about this by looking at some
    real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the principles mentioned in this chapter will equip you with the
    knowledge to build end-to-end monitoring systems for your use case or company.
    This will help you engage business, tech, and public (customers and legal) stakeholders
    so that you can efficiently achieve your business goals. This will also help you
    have the edge and have a systematic approach to governing your ML system. Using
    the frameworks in this chapter, you can enable trust, transparency, and explainability
    for your stakeholders and ML system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key principles of monitoring an ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring in the MLOps workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Explainable Monitoring Framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling continuous monitoring for the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key principles of monitoring an ML system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building trust into AI systems is vital these days with the growing demands
    for products to be data-driven and to adjust to the changing environment and regulatory
    frameworks. One of the reasons ML projects are failing to bring value to businesses
    is due to the lack of trust and transparency in their decision making. Many black
    box models are good at reaching high accuracy, but they become obsolete when it
    comes to explaining the reasons behind the decisions that have been made. At the
    time of writing, news has been surfacing that raises these concerns of trust and
    explainability, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Components of model trust and explainability'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Components of model trust and explainability
  prefs: []
  type: TYPE_NORMAL
- en: This image showcases concerns in important areas in real life. Let's look at
    how this translates into some key aspects of model explainability, such as model
    drift, model bias, model transparency, and model compliance, using some real-life
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We live in a dynamically changing world. Due to this, the environment and data
    in which an ML model is deployed to perform a task or make predictions is continually
    evolving, and it is essential to consider this change. For example, the COVID-19
    pandemic has presented us with an unanticipated reality. Many business operations
    have turned virtual, and this pandemic has presented us with a unique situation
    that many perceive as the **new normal**. Many small businesses have gone bankrupt,
    and individuals are facing extreme financial scarcity due to the rise of unemployment.
    These people (small business owners and individuals) have been applying for loans
    and financial reliefs to banks and institutions like never before (on a large
    scale). Fraud detection algorithms that have already been deployed and used by
    banks and institutions have not seen this velocity and veracity of data, in terms
    of loan and financial relief applications.
  prefs: []
  type: TYPE_NORMAL
- en: All these changes in features (such as the applicant's income, their credit
    history, the location of the applicant, the amount they've requested, and so on),
    due to an otherwise loan-worthy applicant who hasn't applied for any loan beforehand
    losing their job, may skew the model's weights/perceptive (or confuse the model).
    This presents an important challenge for the models. To deal with such dynamically
    changing environments, it is crucial to consider model drift and continually learn
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drift is related to changes in the environment and refers to the degradation
    of predictive ML models'' performance and the relationship between the variables
    degrading. Following are the four types of model changes with regards to models
    and data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data drift**: This is where properties of the independent variables change.
    For example, as in the previous example, data changes due to seasonality or new
    products or changes being added to meet the consumer''s needs, as in the COVID-19
    pandemic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature drift**: This is where the properties of the feature(s) change over
    time. For instance, temperature changes with change in seasons. In winter the
    temperature is cooler compared to the temperatures in summer or autumn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model drift**: This is where properties of dependent variables change. For
    instance, in the preceding example, this is where the classification of fraud
    detection changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upstream data changes**: This is when the data pipeline undergoes operational
    data changes, such as when a feature is no longer being generated, resulting in
    missing values. An example of this is a change of salary value for the customer
    (from dollar to euros), where a dollar value is no longer being generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more clarity, we will learn more about drift and develop drift monitors
    in the next chapter ([*Chapter 12*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222),
    *Model Serving and Monitoring)*.
  prefs: []
  type: TYPE_NORMAL
- en: Model bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether you like it or not, ML is already impacting many decisions in your life,
    such as getting shortlisted for your next job or getting mortgage approvals from
    banks. Even Evan law enforcement agencies are using it to drill down potential
    crime suspects to prevent crimes. ProPublica, a journalism organization (uses
    ML to predict future criminals - [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)).
    In 2016, Propublica’s ML showed cases where the model was biased to predict black
    women as higher risk than white men, while all previous records showed otherwise.
    Such cases can be costly and have devastating societal impacts, so they need to
    be avoided. In another case, Amazon built an AI to hire people but had to shut
    it down as it was discriminating against women (as reported by the Washington
    Post). These kinds of biases can be costly and unethical. To avoid them, AI systems
    need to be monitored so that we can build our trust in them.
  prefs: []
  type: TYPE_NORMAL
- en: Model bias is a type of error that happens due to certain features of the dataset
    (used for model training) being more heavily represented and/or weighted than
    others. A misrepresenting or biased dataset can result in skewed outcomes for
    the model's use case, low accuracy levels, and analytical errors. In other words,
    it is the error resulting from incorrect assumptions being made by the ML algorithm.
    High bias can result in predictions being inaccurate and can cause a model to
    miss relevant relationships between the features and the target variable being
    predicted. An example of this is the aforementioned AI that had been built by
    Amazon to hire people but had a bias against women. We will learn more about model
    bias in the *Explainable Monitoring Framework* section, where we will explore
    *Bias and threat detection*.
  prefs: []
  type: TYPE_NORMAL
- en: Model transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI is non-deterministic in nature. ML in particular continually evolves, updates,
    and retrains over its life cycle. AI is impacting almost all industries and sectors.
    With its increasing adoption and important decisions being made using ML, it has
    become vital to establish the same trust level that deterministic systems have.
    After all, digital systems are only useful when they can be trusted to do their
    jobs. There is a clear need for model transparency – many CEO and business leaders
    are encouraging us to understand AI''s business decisions and their business impact.
    Recently, the CEO of TikTok made a statement stating the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"We believe all companies should disclose their algorithms, moderation policies,
    and data flows to regulators" (Source: TikTok).'
  prefs: []
  type: TYPE_NORMAL
- en: Such a level of openness and transparency by companies can build our trust in
    AI as a society and enable smoother adoption and compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Model transparency is the pursuit of building trust in AI systems to ensure
    fairness, reduce or eliminate bias, provide accountability (auditing the end-to-end
    process of how the system derives results), and justify model outputs and system
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Model compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model compliance has become important as the cost of non-compliance with governments
    and society can be huge. The following headline was reported by the Washington
    Post:'
  prefs: []
  type: TYPE_NORMAL
- en: '"JPMorgan settles federal mortgage discrimination suit for $55 million"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-compliance turned out to be a costly affair for JP Morgan. Operationalizing
    regulatory compliance is increasingly becoming important to avoid unnecessary
    fines and damage to society. Here are some drivers that enable model compliance
    within companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Culture of accountability**: End-to-end auditing of ML systems is essential
    to monitoring compliance. MLOps can play a vital role in facilitating auditing
    and redacting the operations and business decisions that are made using AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethics at the forefront**: Building responsible AI systems that bring value
    to society and gain our trust requires that AI predictions are inclusive, fair,
    and ethical. Having an ethics framework can help a company connect their customers
    to their values and principles, as well as ensuring AI decisions are made ethically.
    The European Commission has done a good job here by coming up with the *Ethics
    and guidelines for trustworthy AI*. You can find these guidelines here: [https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance pipeline**: Having a compliance pipeline that satisfies both business
    and government regulations can be rewarding for organizations seeking to ensure
    real-time compliance, auditing, and redaction. MLOps can facilitate this by keeping
    track of all the ML models'' inventory, thus giving visibility into how they work
    and explaining how they are working visually to stakeholders. This way of working
    enables humans to monitor, redact, and explain correlations to regulations, making
    it efficient for business stakeholders, data scientists, and regulators to work
    hand in hand to ensure they have transparent and explainable operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainable AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an ideal case, a business keeps model transparency and compliance at the
    forefront so that the business is dynamically adapting to the changing environment,
    such as model drift, and dealing with bias on the go. All this needs a framework
    that keeps all business stakeholders (IT and business leaders, regulators, business
    users, and so on) in touch with the AI model in order to understand the decisions
    the model is making, while focusing on increasing model transparency and compliance.
    Such a framework can be delivered using Explainable AI as part of MLOps. Explainable
    AI enables ML to be easily understood by humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model transparency and explainability are two approaches that enable Explainable
    AI. The ML models form patterns or rules based on the data they are trained on.
    Explainable AI can help humans or business stakeholders understand these rules
    or patterns the model has discovered, and also helps validate business decisions
    that have been made by the ML model. Ideally, Explainable AI should be able to
    serve multiple business stakeholders, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Business-driven Explainable AI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – Business-driven Explainable AI
  prefs: []
  type: TYPE_NORMAL
- en: Black box models can get high accuracy on predictions but become obsolete when
    they are unable to explain why they've made these decisions. Most black box models
    offer no visibility into model performance, no monitoring to catch potential bias
    or drift, and no explainability of model behavior. To address this issue, a vast
    amount of research and development is going on in to Explainable AI methods to
    offer model transparency and model explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable AI methods infused with MLOps can enable almost all business stakeholders
    to understand and validate business decisions made by the AI, and also helps explain
    them to the internal and external stakeholders. There is no one-stop solution
    for Explainable AI as every use case needs its own Explainable AI method. There
    are various methods that are gaining in popularity. We'll look at some examples
    in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Feature attribution methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feature attribution methods show how much each feature in your model has contributed
    to each instance''s predictions. When you request explanations, you get the predictions,
    along with feature attribution information. Here are some feature attribution
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SHapley Additive exPlanations** (**SHAP**): A method to explain the outputs
    of any ML model. It is based on the game theory approach, which explains the output
    of any ML model. In particular, it explains each feature''s contribution to push
    the model''s output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated Gradients**: A technique that aims to explain the relationship
    between a model''s predictions in terms of its features. It was introduced in
    the paper *Axiomatic Attribution for Deep Networks*. It can explain feature importance
    by identifying skewed data and help with debugging model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-Agnostic Explanation** (**LIME**): This is a model-agnostic
    method that''s used to explain predictions. It focuses on local explanations;
    that is, explanations to reflect the model''s behavior regarding the instance
    of data being predicted. For instance, LIME can suggest which factors or features
    were important for a model to predict an outcome. This can be seen in the following
    diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Explaining individual predictions using LIME'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – Explaining individual predictions using LIME
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, a model is predicting that a patient has diabetes.
    The LIME explainer highlights and implies the symptoms of diabetes, such as dry
    skin, excessive urine, and blurry vision, that contribute to the `Diabetes` prediction,
    while `No fatigue` is evidence against it. Using the explainer, a doctor can decide
    and draw conclusions from the model's prediction and provide the patient with
    the appropriate treatment.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about AI explanations at [https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview](https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview).
  prefs: []
  type: TYPE_NORMAL
- en: Non-feature attribution methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Non-feature attribution methods do not focus on how features contribute to
    your model''s predictions. Instead, they focus on the relationship between input
    data and output data for your model inference. Here are some non-feature attribution
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deeplift**: This is used to evaluate neural networks by comparing each neuron''s
    activation to its reference activation and assigning contribution scores according
    to the difference. Deeplift reveals correlations and contributions. For example,
    let''s say we are using a cat and dog image classifier to classify images between
    cats and dogs. Suppose the classifier predicts that the input image is a dog by
    using the deeplift method. Here, we can backpropagate the neurons that were activated
    in the image classifier''s neural network to their reference activations and then
    assign contribution scores to each feature based on the difference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language explanations** (**NLE**): NLE aims to capture input-output
    relationships for text explanations using a fusion of techniques such as partial
    dependence function, gradient analysis, contextual encoding, Individual Conditional
    Expectation, Accumulated Local Effects, and so on. These techniques are useful
    for interpreting language models that classify or generate text. NLE provide easily
    understandable and useful reasons for users to decide whether to trust the model''s
    decision and take action. For example, you may wish to buy a product based on
    a model''s recommendation and explanation. Tools such as Microsoft Power BI and
    Qlik Sense can be used to plug and play to study NLE. These tools need to be customized
    as per your need or use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other methods apart from the ones mentioned in the previous list.
    This area is a hot topic for research in the field of AI. Many researchers and
    business leaders are pursuing solving Explainable AI problems to explain model
    decisions to internal and external stakeholders. Having an Explainable AI-driven
    interface provisioned for multiple business stakeholders can help them answer
    critical business questions. For instance, a business leader needs to be able
    to answer, "How do these model decisions impact business?" while for IT and Operations,
    it is vital to know the answer to "How do I monitor and debug?"
  prefs: []
  type: TYPE_NORMAL
- en: Answering these questions for multiple business stakeholders enables employees
    and businesses to adapt to AI and maximize value from it, by ensuring model transparency
    and model compliance while adapting to changing environments by optimizing model
    bias and drift.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable AI = model transparency and explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since ML models are becoming first-class citizens, in order to monitor a model's
    performance with respect to these areas, we can use Explainable Monitoring, which
    allows us to analyze and govern ML systems in production by monitoring and explaining
    their decisions using Explainable AI methods. Explainable monitoring is a hybrid
    of Explainable AI; it uses Explainable AI methods infused with **operations**
    (**Ops**) in production. Explainable monitoring is becoming an integral part of
    the MLOps workflow. We'll look at how Explainable Monitoring brings value to the
    MLOps workflow in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring in the MLOps workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned about the MLOps workflow in [*Chapter 1*](B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015),
    *Fundamentals of MLOps Workflow*. As shown in the following diagram, the monitoring
    block is an integral part of the MLOps workflow for evaluating the ML models'
    performance in production and measuring the ML system's business value. We can
    only do both (measure the performance and business value that's been generated
    by the ML model) if we understand the model's decisions in terms of transparency
    and explainability (to explain the decisions to stakeholders and customers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Explainable Monitoring enables both transparency and explainability to govern
    ML systems in order to drive the best business value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – MLOps workflow – Monitor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – MLOps workflow – Monitor
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, **Explainable Monitoring** enables us to monitor, analyze, and
    govern ML system, and it works in a continuous loop with other components in the
    MLOps workflow. It also empowers humans to engage in the loop to understand model
    decisions and teach the model (by labeling data and retraining the model) on the
    go. Explainable Monitoring enables continual learning and can be highly rewarding
    for a company in the long run. A continual learning pipeline with a human in the
    loop, enabled using Explainable Monitoring, can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Continual learning enabled by Explainable Monitoring'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.5 – Continual learning enabled by Explainable Monitoring
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning is the system's ability to learn continuously in a changing
    environment while building on what had been learned previously. To facilitate
    continual learning, data and modeling must work hand in hand, and be assisted
    by humans in the loop (typically, a QA analyst or system admin, such as a data
    scientist or ML engineer). Explainable Monitoring plays a vital role in continual
    learning systems to increase revenue, stay compliant, and build ML systems responsibly.
    After all, only a model that's been deployed with continual learning capabilities
    can bring business value.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Explainable Monitoring Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore the Explainable Monitoring Framework (as shown
    in the following diagram) in detail to understand and learn how Explainable Monitoring
    enhances the MLOps workflow and the ML system itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Explainable Monitoring Framework'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – Explainable Monitoring Framework
  prefs: []
  type: TYPE_NORMAL
- en: The Explainable Monitoring Framework is a modular framework that's used to monitor,
    analyze, and govern a ML system while enabling continual learning. All the modules
    work in sync to enable transparent and Explainable Monitoring. Let's look at how
    each module works to understand how they contribute and function in the framework.
    First, let's look at the monitor module (the first panel in the preceding diagram).
  prefs: []
  type: TYPE_NORMAL
- en: Monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The monitor module is dedicated to monitoring the application in production
    (serving the ML model). Several factors are at play in an ML system, such as application
    performance (telemetry data, throughput, server request time, failed requests,
    error handling, and so on), data integrity and model drift, and changing environments.
    The monitor module should capture vital information from the system logs in production
    to track the ML system''s robustness. Let''s look at the importance and functionality
    of three of the monitor module''s functionalities: data integrity, model drift,
    and application performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensuring the data integrity of an ML application includes checking incoming
    (input data to the ML model) and outgoing (ML model prediction) data to ensure
    ML systems' integrity and robustness. The monitor module ensures data integrity
    by inspecting the volume, variety, veracity, and velocity of the data in order
    to detect outliers or anomalies. Detecting outliers or anomalies prevents ML systems
    from having poor performance and being susceptible to security attacks (for example,
    adversarial attacks). Data integrity coupled with efficient auditing can facilitate
    the desired performance of ML systems to derive business value.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If model drift is not measured, the model''s performance can easily become
    sub-par and can hamper the business with poor decision making and customer service.
    For example, it is hard to foresee changes or trends in data during a black swan
    event such as COVID-19\. Here is some news that made it to the headlines:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the Instacart model forecasting item's accessibility in stores
    fell from 93% to 61% due to a dramatic change in shopping habits ([https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/](https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bankers doubted if credit models that have been trained for good times will
    respond accurately to stress scenarios ([https://www.americanbanker.com/opinion/ai-models-could-struggle-to-handle-the-market-downturn](https://www.americanbanker.com/opinion/ai-models-could-struggle-to-handle-the-market-downturn)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In response to market uncertainty, trading algorithms misfired. There was a
    21% decline in some funds ([https://www.wired.com/story/best-ai-models-no-match-coronavirus](https://www.wired.com/story/best-ai-models-no-match-coronavirus)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image classification models struggle to adapt to the "new normal" in the wake
    of the COVID-19 pandemic: a family at home in front of laptops can now mean "work,"
    not "leisure." ([https://techcrunch.com/2020/08/02/ai-is-struggling-to-adjust-to-2020/](https://techcrunch.com/2020/08/02/ai-is-struggling-to-adjust-to-2020/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, it is important to monitor model drift in any form, such as data drift,
    concept drift, or any upstream data changes, in order to adapt to the changing
    environments and serve businesses and customers in the most relevant way and generate
    the maximum business value.
  prefs: []
  type: TYPE_NORMAL
- en: Application performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is critical to monitor application performance to foresee and prevent any
    potential failures, since this ensures the robustness of ML systems. Here, we
    can monitor the critical system logs and telemetry data of the production deployment
    target (for example, Kubernetes or an on-premises server). Monitoring application
    performance can give us key insights in real time, such as the server's throughput,
    latency, server request time, number of failure requests or control flow errors,
    and so on. There is no hard and set way of monitoring applications and, depending
    on your business use case, your application performance mechanism can be curated
    and monitored to keep the system up and running to generate business value.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the monitor component, we monitored data integrity, model drift,
    and application performance. In the next section, we will analyze how to monitor
    the data of the model and application.
  prefs: []
  type: TYPE_NORMAL
- en: Analyze
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing your ML system in production in real time is key to understanding
    the performance of your ML system and ensuring its robustness. Humans play a key
    role in analyzing model performance and detecting subtle anomalies and threats.
    Hence, having a human in the loop can introduce great transparency and explainability
    to the ML system. We can analyze model performance to detect any biases or threats
    and to understand why the model makes decisions in a certain pattern. We can do
    this by applying advanced techniques such as data slicing, adversarial attack
    prevention techniques, or by understanding local and global explanations. Let's
    see how we can do this in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Data slicing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a great number of success stories surrounding ML in terms of improving
    businesses and life in general. However, there is still room to improve data tools
    for debugging and interpreting models. One key area of improvement is understanding
    why models perform poorly on certain parts or slices of data and how we can balance
    their overall performance. A slice is a part or a subset of a dataset. Data slicing
    can help us understand the model's performance on different types of sub-datasets.
    We can split the dataset into multiple slices or subsets and study the model's
    behavior on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s consider a hypothetical case where we have trained a random
    forest model to classify whether a person''s income is above or below $50,000\.
    The model has been trained on the UCI census data ([https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)).
    The results of the model for slices (or subsets) of data can be seen in the following
    table. This table suggests that the overall metrics may be considered acceptable
    as the overall log loss is low for all the data (see the *All* row). This is a
    widely used loss metric for binary classification problems and represents how
    close the prediction''s likelihood is to the actual/true value; it is 0 or 1 in
    the case of binary classification. The more the predicted probability diverges
    from the actual value, the higher the log loss value is. However, the individual
    slices tell a different story:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 11.1 – UCI census data slices'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 – UCI census data slices
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the previous table, we can conclude that the model's performance
    is decent. However, if we look at the performance of male versus female subjects,
    we can see that the model only performs well for female subjects where the log
    losses are less compared to the log loss for male subjects. On the other hand,
    if you look at the *Prof – specialty* occupation, you will see that the net performance
    is on par with the performance of male subjects with log losses of 0.45 and 0.41,
    respectively, whereas the effect size for *Prof – specialty* is considerably less.
    The model performs poorly for *Bachelors*, *Masters*, and *Doctorates* as the
    log losses are high with values of 0.44, 0.49, and 0.59, respectively. It is also
    important to note that if the log loss of a slice and its counterpart is below
    acceptable, this suggests that the model is bad overall and not just on a particular
    data slice.
  prefs: []
  type: TYPE_NORMAL
- en: Data slicing enables us to see subtle biases and unseen correlations to understand
    why a model might perform poorly on a subset of data. We can avoid these biases
    and improve the model's overall performance by training the model using balanced
    datasets that represent all the data slices (for example, using synthetic data
    or by undersampling, and so on) or by tuning hyperparameters of the models to
    reduce overall biases. Data slicing can provide an overview of model fairness
    and performance for an ML system, and can also help an organization optimize the
    data and ML models to reach optimal performance and decent fairness thresholds.
    Data slicing can help build trust in the AI system by offering transparency and
    explainability into data and model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a comprehensive overview of data slicing and automated data slicing
    methods, take a look at *Automated Data Slicing for Model Validation: A Big data
    - AI Integration Approach* at [https://arxiv.org/pdf/1807.06068.pdf](https://arxiv.org/pdf/1807.06068.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Bias and threat detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure that robust and ethical decisions are made using ML models, we need
    to make sure that the models are fair and secure. Any biases and threats need
    to be monitored and mitigated, to avoid unethical or partial decisions that benefit
    any particular party in order to comply with business values and the law.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of bias, such as selection bias (data that's used
    for training models is not representative of the population, such as minorities),
    framing bias (questions or a survey that's used to collect data is framed in a
    point of view or slant), systematic bias (repetitive or consistent error), response
    bias (data in which participants respond incorrectly by following their conscious
    bias), or confirmation bias (collecting data to validate your own preconceptions).
    To avoid these biases and mitigate them, techniques such as data slicing, slice-based
    learning, or balancing the bias-variance tradeoff can be applied, depending on
    the use case.
  prefs: []
  type: TYPE_NORMAL
- en: An ML system is exposed to security threats that need monitoring and mitigation.
    We have discussed some common threats and threat-prevention techniques involving
    adversarial attacks, poison attacks, privacy attacks or backdoor attacks, and
    so on, in [*Chapter 9*](B16572_09_Final_JM_ePub.xhtml#_idTextAnchor176), *Testing
    and Securing Your ML Solution.*
  prefs: []
  type: TYPE_NORMAL
- en: Local and global explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Local and global explanations offer different perspectives on model performance.
    Local explanations offer justification for model prediction for a specific or
    individual input, whereas global explanations provide insights into the model''s
    predictive process, independent of any particular input. For example, let''s take
    a look at a hypothetical case of a **recurrent neural network** (**RNN**) model
    being used to perform sentiment analysis for customer reviews. The following diagram
    shows the global explanation (the process as a whole) for the RNN model''s sentiment
    analysis using the RNNVis tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Global explanation of the RNN model (using RNNVis) to understand
    the process as a whole (how hidden states, layers, and so on impact model outputs
    and predictive processes)](img/image013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Global explanation of the RNN model (using RNNVis) to understand
    the process as a whole (how hidden states, layers, and so on impact model outputs
    and predictive processes)
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/](https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, for example, the co-clustering visualization shows different word clouds
    for words with positive and negative sentiments. Using global explanations, we
    can simulate the model''s predictive process and understand correlations with
    regards to parameters or the model''s architecture (for example, hidden states
    and layers). Global explanations offer two perspectives of explainability: the
    high-level model process and the predictive explanations. On the other hand, local
    explanations give insights into single predictions. Both explanations are valuable
    if we wish to understand the model''s performance and validate it comprehensively.'
  prefs: []
  type: TYPE_NORMAL
- en: In the analyze component, we can analyze the model's performance using the techniques
    we have explored, such as data slicing, Bias and threat detection, and local and
    global explanations. In the next section, we will learn how to govern and control
    ML systems to efficiently guide it to achieve operational or business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Govern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ML systems' efficacy is dependent on the way it is governed to achieve maximum
    business value. A great part of system governance involves quality assurance and
    control, as well as model auditing and reporting, to ensure it has end-to-end
    trackability and complies with regulations. Based on monitoring and analyzing
    the model's performance, we can control and govern ML systems. Governance is driven
    by smart alerts and actions to maximize business value. Let's look into how alerts
    and actions, model quality assurance and control, and model auditing and reports
    orchestrate the ML system's governance.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts and actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Governing a ML system involves monitoring and analyzing the ML application.
    Here, system developers can be alerted about when the system is showing anomalous
    behavior such as failed requests, slow server response times, server exceptions,
    errors, or high latency. Alerting the system developers or admin can ensure quality
    assurance and prevent system failures. There are two different types of alerts:
    alerts for system performance and model performance-based alerts. Here are some
    examples of alerts for system performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based alerts for failed requests based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based alerts for server response time based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based alerts for server exceptions based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based alerts for availability based on a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model performance alerts are generated when the model experiences drift or
    anomalous feature distribution or bias. When such events are recorded, the system
    administrator or developers are alerted via email, SMS, push notifications, and
    voice alerting. These alert actions (automated or semi-automated) can be used
    to mitigate system performance deterioration. Depending on the situation and need,
    some possible actions can be evoked, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an alternative model upon experiencing high model drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a new model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restarting the ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redeploying the ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model quality assurance and control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A model''s quality assurance and control mechanism can be quite rewarding for
    those using an ML system, if we wish to prevent many possible mishaps and to ensure
    regular and healthy monitoring and functionality of the ML system. It is recommended
    to have a framework or mechanism for model quality assurance and control. For
    this, a quality assurance framework for ML systems, as shown in the following
    diagram, can enable the mechanism for your organization. It is a modular framework
    that''s used to monitor three important aspects of ML systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Model Quality Assurance Framework'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 – Model Quality Assurance Framework
  prefs: []
  type: TYPE_NORMAL
- en: 'Quality assurance experts can help your company or organization put a test
    mechanism into place to validate whether the data being used for training has
    been sanitized, ensure the data being used for model inference does not contain
    threats for that ML system, and to monitor data drift to understand and validate
    the changing environment. Monitoring and testing data can be achieved by quality
    assurance or test engineers, together with product managers, by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand and validate the data's statistical relations (for example, mean,
    median, mode, and so on) for training, testing, and inferring data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop tests to verify the aforementioned statistics and relationships (using
    scripts).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the distribution of characteristics using feature engineering techniques
    such as feature selection, dimensionality reduction, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrain and review the performance of all models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the performance of all the models at regular intervals with new datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raise an alert if another model (from the model inventory) performs with better
    accuracy than the existing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform tests at regular intervals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model auditing and reports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model auditing and reporting is essential if you want to have enough information
    for the regulators and for compliance with the law. Having end-to-end traceability
    for the model ensures great transparency and explainability, which can result
    in transparent governance mechanisms for an organization or company. The goal
    of model auditing and reporting is to assess the model''s performance and based
    on that, enable ML system governance. In the following diagram, we can see a big-picture
    overview of the model transparency chart that''s generated from auditing and reporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Model QA framework'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.9 – Model Transparency chart
  prefs: []
  type: TYPE_NORMAL
- en: Model assessments based on auditing and reporting will ensure healthy, transparent,
    and robust governance mechanisms for organizations and enable them to have end-to-end
    traceability in order to comply with the regulators. Having such mechanisms will
    help save organizations a great amount of time and resources and enable efficiency
    in interactions with the regulators.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling continuous monitoring for the service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Explainable Monitoring Framework can be resourceful if we wish to monitor
    ML systems in production. In the next chapter, we will enable the Explainable
    Monitoring Framework for the business use case we worked on in the previous chapters.
    We will enable continuous monitoring for the system we have deployed. We will
    then monitor the ML application that's been deployed to production and analyze
    the incoming data and the model's performance to govern the ML system to produce
    maximum business value for the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the key principles for monitoring an ML system.
    We explored some common monitoring methods and the Explainable Monitoring Framework
    (including the monitor, analyze, and govern stages). We then explored the concepts
    of Explainable Monitoring thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into a hands-on implementation of the Explainable
    Monitoring Framework. Using this, we will build a monitoring pipeline in order
    to continuously monitor the ML system in production for the business use case
    (predicting weather at the port of Turku).
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is quite hands-on, so buckle up and get ready!
  prefs: []
  type: TYPE_NORMAL
