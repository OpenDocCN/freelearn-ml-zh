- en: 'Chapter 11: Key Principles for Monitoring Your ML System'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章：监控你的ML系统的关键原则
- en: In this chapter, we will learn about the fundamental principles that are essential
    for monitoring your **machine learning** (**ML**) models in production. You will
    learn how to build trustworthy and Explainable AI solutions using the Explainable
    Monitoring Framework. The Explainable Monitoring Framework can be used to build
    functional monitoring pipelines so that you can monitor ML models in production,
    analyze application and model performance, and govern ML systems. The goal of
    monitoring ML systems is to enable trust, transparency, and explainability in
    order to increase business impact. We will learn about this by looking at some
    real-world examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习那些对于在生产环境中监控你的**机器学习**（**ML**）模型至关重要的基本原理。你将学习如何使用可解释监控框架构建可信赖且可解释的人工智能解决方案。可解释监控框架可用于构建功能监控管道，以便你在生产环境中监控ML模型，分析应用程序和模型性能，并治理ML系统。监控ML系统的目标是实现信任、透明度和可解释性，以增加业务影响。我们将通过观察一些现实世界的例子来了解这一点。
- en: Understanding the principles mentioned in this chapter will equip you with the
    knowledge to build end-to-end monitoring systems for your use case or company.
    This will help you engage business, tech, and public (customers and legal) stakeholders
    so that you can efficiently achieve your business goals. This will also help you
    have the edge and have a systematic approach to governing your ML system. Using
    the frameworks in this chapter, you can enable trust, transparency, and explainability
    for your stakeholders and ML system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理解本章中提到的原则将使你具备构建针对你的用例或公司的端到端监控系统的知识。这将帮助你参与业务、技术以及公众（客户和法律）利益相关者，以便你能够高效地实现你的业务目标。这还将帮助你获得优势，并采用系统的方法来治理你的ML系统。使用本章中的框架，你可以为你的利益相关者和ML系统启用信任、透明度和可解释性。
- en: 'We are going to cover the following main topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Understanding the key principles of monitoring an ML system
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监控ML系统的关键原则
- en: Monitoring in the MLOps workflow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MLOps工作流程中的监控
- en: Understanding the Explainable Monitoring Framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解可解释监控框架
- en: Enabling continuous monitoring for the service
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用服务的持续监控
- en: Let's get started!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Understanding the key principles of monitoring an ML system
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监控ML系统的关键原则
- en: 'Building trust into AI systems is vital these days with the growing demands
    for products to be data-driven and to adjust to the changing environment and regulatory
    frameworks. One of the reasons ML projects are failing to bring value to businesses
    is due to the lack of trust and transparency in their decision making. Many black
    box models are good at reaching high accuracy, but they become obsolete when it
    comes to explaining the reasons behind the decisions that have been made. At the
    time of writing, news has been surfacing that raises these concerns of trust and
    explainability, as shown in the following figure:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今数据驱动产品日益增长的需求以及适应不断变化的环境和监管框架的背景下，将信任融入AI系统至关重要。ML项目未能为业务带来价值的一个原因就是决策过程中缺乏信任和透明度。许多黑盒模型擅长达到高精度，但在解释已做决策背后的原因时却变得过时。在撰写本文时，有关信任和可解释性的担忧的新闻正在浮出水面，如下图所示：
- en: '![Figure 11.1 – Components of model trust and explainability'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 – 模型信任和可解释性的组成部分]'
- en: '](img/image001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image001.jpg]'
- en: Figure 11.1 – Components of model trust and explainability
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 模型信任和可解释性的组成部分
- en: This image showcases concerns in important areas in real life. Let's look at
    how this translates into some key aspects of model explainability, such as model
    drift, model bias, model transparency, and model compliance, using some real-life
    examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片展示了现实生活中重要领域的担忧。让我们通过一些现实生活中的例子来探讨这些担忧如何转化为模型可解释性的关键方面，例如模型漂移、模型偏差、模型透明度和模型合规性。
- en: Model drift
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型漂移
- en: We live in a dynamically changing world. Due to this, the environment and data
    in which an ML model is deployed to perform a task or make predictions is continually
    evolving, and it is essential to consider this change. For example, the COVID-19
    pandemic has presented us with an unanticipated reality. Many business operations
    have turned virtual, and this pandemic has presented us with a unique situation
    that many perceive as the **new normal**. Many small businesses have gone bankrupt,
    and individuals are facing extreme financial scarcity due to the rise of unemployment.
    These people (small business owners and individuals) have been applying for loans
    and financial reliefs to banks and institutions like never before (on a large
    scale). Fraud detection algorithms that have already been deployed and used by
    banks and institutions have not seen this velocity and veracity of data, in terms
    of loan and financial relief applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在一个动态变化的世界。因此，机器学习模型执行任务或进行预测的环境和数据也在不断演变，考虑这种变化是至关重要的。例如，COVID-19大流行给我们带来了一个意想不到的现实。许多商业运营已经转向虚拟，这场大流行给我们带来了一个许多人都认为的**新常态**。许多小企业已经破产，由于失业率的上升，个人面临着极端的财务短缺。这些人（小企业主和个人）以前从未如此大规模地向银行和机构申请过贷款和财务救济。银行和机构已经部署并使用的欺诈检测算法在贷款和财务救济申请方面没有看到这种速度和真实性。
- en: All these changes in features (such as the applicant's income, their credit
    history, the location of the applicant, the amount they've requested, and so on),
    due to an otherwise loan-worthy applicant who hasn't applied for any loan beforehand
    losing their job, may skew the model's weights/perceptive (or confuse the model).
    This presents an important challenge for the models. To deal with such dynamically
    changing environments, it is crucial to consider model drift and continually learn
    from it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些特征的变化（例如申请人的收入、他们的信用历史、申请人的位置、他们请求的金额等），由于一个原本有资格贷款但之前没有申请过贷款的申请人失去了工作，可能会扭曲模型的权重/感知（或混淆模型）。这对模型来说是一个重要的挑战。为了处理这种动态变化的环境，考虑模型漂移并持续从中学习至关重要。
- en: 'Drift is related to changes in the environment and refers to the degradation
    of predictive ML models'' performance and the relationship between the variables
    degrading. Following are the four types of model changes with regards to models
    and data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Drift与环境的改变相关，指的是预测机器学习模型性能的下降以及变量之间关系的退化。以下是与模型和数据相关的四种模型变化类型：
- en: '**Data drift**: This is where properties of the independent variables change.
    For example, as in the previous example, data changes due to seasonality or new
    products or changes being added to meet the consumer''s needs, as in the COVID-19
    pandemic.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移**：这是指独立变量的属性发生变化。例如，在先前的例子中，由于季节性、新产品或为满足消费者需求而进行的更改，数据发生变化，如COVID-19大流行。'
- en: '**Feature drift**: This is where the properties of the feature(s) change over
    time. For instance, temperature changes with change in seasons. In winter the
    temperature is cooler compared to the temperatures in summer or autumn.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征漂移**：这是指特征（的）属性随时间变化。例如，温度随着季节的变化而变化。在冬天，温度比夏天或秋天的温度要低。'
- en: '**Model drift**: This is where properties of dependent variables change. For
    instance, in the preceding example, this is where the classification of fraud
    detection changes.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型漂移**：这是指依赖变量的属性发生变化。例如，在上面的例子中，这是欺诈检测分类发生变化的地方。'
- en: '**Upstream data changes**: This is when the data pipeline undergoes operational
    data changes, such as when a feature is no longer being generated, resulting in
    missing values. An example of this is a change of salary value for the customer
    (from dollar to euros), where a dollar value is no longer being generated.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上游数据变化**：这是指数据管道经历操作数据变化时的情况，例如当某个特征不再生成时，导致缺失值。一个例子是客户薪资价值的变化（从美元到欧元），其中美元价值不再生成。'
- en: For more clarity, we will learn more about drift and develop drift monitors
    in the next chapter ([*Chapter 12*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222),
    *Model Serving and Monitoring)*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地说明，我们将在下一章中学习更多关于drift的知识，并开发drift监控器（[*第12章*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222)，*模型服务和监控*）。
- en: Model bias
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型偏差
- en: Whether you like it or not, ML is already impacting many decisions in your life,
    such as getting shortlisted for your next job or getting mortgage approvals from
    banks. Even Evan law enforcement agencies are using it to drill down potential
    crime suspects to prevent crimes. ProPublica, a journalism organization (uses
    ML to predict future criminals - [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)).
    In 2016, Propublica’s ML showed cases where the model was biased to predict black
    women as higher risk than white men, while all previous records showed otherwise.
    Such cases can be costly and have devastating societal impacts, so they need to
    be avoided. In another case, Amazon built an AI to hire people but had to shut
    it down as it was discriminating against women (as reported by the Washington
    Post). These kinds of biases can be costly and unethical. To avoid them, AI systems
    need to be monitored so that we can build our trust in them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是否喜欢，机器学习已经影响了你生活中的许多决策，比如获得下一份工作的短名单或从银行获得抵押贷款批准。甚至执法机构也在使用它来缩小潜在的犯罪嫌疑人以预防犯罪。ProPublica是一个新闻机构（使用机器学习预测未来的罪犯
    - [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)）。2016年，ProPublica的机器学习显示了一些案例，其中模型倾向于将黑人女性预测为比白人男性风险更高，而所有之前的记录都显示并非如此。这类案例可能代价高昂，并具有破坏性的社会影响，因此需要避免。在另一个案例中，亚马逊构建了一个用于招聘人员的AI，但由于它歧视女性（据《华盛顿邮报》报道），不得不关闭它。这类偏见可能代价高昂且不道德。为了避免这些，需要监控人工智能系统，以便我们能够对其建立信任。
- en: Model bias is a type of error that happens due to certain features of the dataset
    (used for model training) being more heavily represented and/or weighted than
    others. A misrepresenting or biased dataset can result in skewed outcomes for
    the model's use case, low accuracy levels, and analytical errors. In other words,
    it is the error resulting from incorrect assumptions being made by the ML algorithm.
    High bias can result in predictions being inaccurate and can cause a model to
    miss relevant relationships between the features and the target variable being
    predicted. An example of this is the aforementioned AI that had been built by
    Amazon to hire people but had a bias against women. We will learn more about model
    bias in the *Explainable Monitoring Framework* section, where we will explore
    *Bias and threat detection*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型偏差是一种由于数据集（用于模型训练）中某些特征比其他特征更频繁地表示和/或加权而产生的错误。一个失真或有偏差的数据集可能导致模型用例的结果偏差、低准确度水平和分析错误。换句话说，这是由于机器学习算法做出不正确假设而产生的错误。高偏差可能导致预测不准确，并可能导致模型错过特征与预测的目标变量之间的相关关系。一个例子是上述由亚马逊构建的用于招聘人员的AI，但它对女性存在偏见。我们将在“可解释监控框架”部分了解更多关于模型偏差的内容，我们将探讨“偏差和威胁检测”。
- en: Model transparency
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型透明度
- en: 'AI is non-deterministic in nature. ML in particular continually evolves, updates,
    and retrains over its life cycle. AI is impacting almost all industries and sectors.
    With its increasing adoption and important decisions being made using ML, it has
    become vital to establish the same trust level that deterministic systems have.
    After all, digital systems are only useful when they can be trusted to do their
    jobs. There is a clear need for model transparency – many CEO and business leaders
    are encouraging us to understand AI''s business decisions and their business impact.
    Recently, the CEO of TikTok made a statement stating the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在本质上是非确定性的。特别是机器学习在其生命周期中持续演变、更新和重新训练。人工智能几乎影响着所有行业和领域。随着其日益普及和重要决策使用机器学习，建立与确定性系统相同的信任水平变得至关重要。毕竟，数字系统只有在它们能够被信任执行其任务时才有用。模型透明度有明确的需求——许多首席执行官和商业领袖都在鼓励我们了解人工智能的商业决策及其商业影响。最近，TikTok的首席执行官发表声明称：
- en: '"We believe all companies should disclose their algorithms, moderation policies,
    and data flows to regulators" (Source: TikTok).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '"我们相信所有公司都应该向监管机构披露他们的算法、审核政策和数据流"（来源：TikTok）。'
- en: Such a level of openness and transparency by companies can build our trust in
    AI as a society and enable smoother adoption and compliance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 公司这样的开放和透明度可以建立我们作为社会对人工智能的信任，并使采用和合规更加顺畅。
- en: Model transparency is the pursuit of building trust in AI systems to ensure
    fairness, reduce or eliminate bias, provide accountability (auditing the end-to-end
    process of how the system derives results), and justify model outputs and system
    decisions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型透明性是建立对人工智能系统信任的追求，以确保公平性、减少或消除偏差、提供问责制（审计系统推导结果的端到端过程），以及证明模型输出和系统决策的合理性。
- en: Model compliance
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型合规性
- en: 'Model compliance has become important as the cost of non-compliance with governments
    and society can be huge. The following headline was reported by the Washington
    Post:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合规性已成为重要议题，因为不遵守政府和社会的规定可能造成巨大的成本。以下标题是由《华盛顿邮报》报道的：
- en: '"JPMorgan settles federal mortgage discrimination suit for $55 million"'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '"摩根大通就联邦抵押贷款歧视诉讼达成5500万美元和解"'
- en: 'Non-compliance turned out to be a costly affair for JP Morgan. Operationalizing
    regulatory compliance is increasingly becoming important to avoid unnecessary
    fines and damage to society. Here are some drivers that enable model compliance
    within companies:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不合规对摩根大通来说变成了一笔昂贵的交易。将监管合规性付诸实践越来越重要，以避免不必要的罚款和对社会的损害。以下是一些在公司内实现模型合规性的驱动因素：
- en: '**Culture of accountability**: End-to-end auditing of ML systems is essential
    to monitoring compliance. MLOps can play a vital role in facilitating auditing
    and redacting the operations and business decisions that are made using AI.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责文化**：对机器学习系统进行端到端审计对于监控合规性至关重要。MLOps可以在促进审计和编辑使用人工智能做出的运营和业务决策中发挥关键作用。'
- en: '**Ethics at the forefront**: Building responsible AI systems that bring value
    to society and gain our trust requires that AI predictions are inclusive, fair,
    and ethical. Having an ethics framework can help a company connect their customers
    to their values and principles, as well as ensuring AI decisions are made ethically.
    The European Commission has done a good job here by coming up with the *Ethics
    and guidelines for trustworthy AI*. You can find these guidelines here: [https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理优先**：构建对社会有价值并赢得我们信任的负责任的人工智能系统需要人工智能预测具有包容性、公平性和伦理性。拥有一个伦理框架可以帮助公司将其客户与他们的价值观和原则联系起来，并确保人工智能决策是按照伦理做出的。欧洲委员会在这方面做得很好，提出了*可信赖人工智能的伦理和指南*。您可以在以下链接找到这些指南：[https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai)。'
- en: '**Compliance pipeline**: Having a compliance pipeline that satisfies both business
    and government regulations can be rewarding for organizations seeking to ensure
    real-time compliance, auditing, and redaction. MLOps can facilitate this by keeping
    track of all the ML models'' inventory, thus giving visibility into how they work
    and explaining how they are working visually to stakeholders. This way of working
    enables humans to monitor, redact, and explain correlations to regulations, making
    it efficient for business stakeholders, data scientists, and regulators to work
    hand in hand to ensure they have transparent and explainable operations.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性管道**：拥有满足商业和政府规定的合规性管道可以为寻求确保实时合规性、审计和编辑的组织带来回报。MLOps可以通过跟踪所有机器学习模型的库存来促进这一点，从而让利益相关者了解它们的工作方式，并以可视化的方式解释它们是如何工作的。这种方式的工作使人类能够监控、编辑并解释与法规的相关性，这对于业务利益相关者、数据科学家和监管者携手合作确保他们有透明和可解释的运营是高效的。'
- en: Explainable AI
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释人工智能
- en: In an ideal case, a business keeps model transparency and compliance at the
    forefront so that the business is dynamically adapting to the changing environment,
    such as model drift, and dealing with bias on the go. All this needs a framework
    that keeps all business stakeholders (IT and business leaders, regulators, business
    users, and so on) in touch with the AI model in order to understand the decisions
    the model is making, while focusing on increasing model transparency and compliance.
    Such a framework can be delivered using Explainable AI as part of MLOps. Explainable
    AI enables ML to be easily understood by humans.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，企业应将模型透明度和合规性置于首位，以便业务能够动态适应不断变化的环境，如模型漂移，并在过程中处理偏差。所有这些都需要一个框架，使所有业务利益相关者（IT和业务领导者、监管者、业务用户等）与人工智能模型保持联系，以便理解模型所做的决策，同时专注于提高模型透明度和合规性。这样的框架可以通过将可解释人工智能作为MLOps的一部分来实现。可解释人工智能使机器学习易于人类理解。
- en: 'Model transparency and explainability are two approaches that enable Explainable
    AI. The ML models form patterns or rules based on the data they are trained on.
    Explainable AI can help humans or business stakeholders understand these rules
    or patterns the model has discovered, and also helps validate business decisions
    that have been made by the ML model. Ideally, Explainable AI should be able to
    serve multiple business stakeholders, as shown in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型透明度和可解释性是两种使可解释人工智能成为可能的方法。机器学习模型基于它们训练的数据形成模式或规则。可解释人工智能可以帮助人类或业务利益相关者理解模型发现的这些规则或模式，并帮助验证由机器学习模型做出的业务决策。理想情况下，可解释人工智能应该能够服务于多个业务利益相关者，如下面的图所示：
- en: '![Figure 11.2 – Business-driven Explainable AI'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2 – 以业务驱动的可解释人工智能]'
- en: '](img/image003.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 003](img/image003.jpg)'
- en: Figure 11.2 – Business-driven Explainable AI
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 以业务驱动的可解释人工智能
- en: Black box models can get high accuracy on predictions but become obsolete when
    they are unable to explain why they've made these decisions. Most black box models
    offer no visibility into model performance, no monitoring to catch potential bias
    or drift, and no explainability of model behavior. To address this issue, a vast
    amount of research and development is going on in to Explainable AI methods to
    offer model transparency and model explainability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒模型在预测上可以达到高精度，但一旦它们无法解释为什么做出这些决策，就会变得过时。大多数黑盒模型不提供对模型性能的可见性，没有监控来捕捉潜在的偏差或漂移，也没有对模型行为的可解释性。为了解决这个问题，大量研究和开发正在进行中，以提供可解释人工智能方法，以提供模型透明度和模型可解释性。
- en: Explainable AI methods infused with MLOps can enable almost all business stakeholders
    to understand and validate business decisions made by the AI, and also helps explain
    them to the internal and external stakeholders. There is no one-stop solution
    for Explainable AI as every use case needs its own Explainable AI method. There
    are various methods that are gaining in popularity. We'll look at some examples
    in the following subsections.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将 MLOps 与可解释人工智能方法相结合可以使几乎所有的业务利益相关者理解和验证人工智能做出的业务决策，并帮助向内部和外部利益相关者解释它们。可解释人工智能没有一劳永逸的解决方案，因为每个用例都需要自己的可解释人工智能方法。有各种方法正在变得越来越受欢迎。我们将在以下小节中查看一些示例。
- en: Feature attribution methods
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征归因方法
- en: 'Feature attribution methods show how much each feature in your model has contributed
    to each instance''s predictions. When you request explanations, you get the predictions,
    along with feature attribution information. Here are some feature attribution
    methods:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归因方法显示了模型中的每个特征对每个实例预测的贡献程度。当你请求解释时，你会得到预测，以及特征归因信息。以下是一些特征归因方法：
- en: '**SHapley Additive exPlanations** (**SHAP**): A method to explain the outputs
    of any ML model. It is based on the game theory approach, which explains the output
    of any ML model. In particular, it explains each feature''s contribution to push
    the model''s output.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHapley Additive exPlanations**（**SHAP**）：一种解释任何机器学习模型输出的方法。它基于博弈论方法，解释任何机器学习模型的输出。特别是，它解释了每个特征对推动模型输出的贡献。'
- en: '**Integrated Gradients**: A technique that aims to explain the relationship
    between a model''s predictions in terms of its features. It was introduced in
    the paper *Axiomatic Attribution for Deep Networks*. It can explain feature importance
    by identifying skewed data and help with debugging model performance.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成梯度**：一种旨在通过模型特征来解释模型预测之间关系的技巧。它在论文《深度网络的公理化归因》中提出。它可以通过识别偏斜数据来解释特征重要性，并有助于调试模型性能。'
- en: '**Local Interpretable Model-Agnostic Explanation** (**LIME**): This is a model-agnostic
    method that''s used to explain predictions. It focuses on local explanations;
    that is, explanations to reflect the model''s behavior regarding the instance
    of data being predicted. For instance, LIME can suggest which factors or features
    were important for a model to predict an outcome. This can be seen in the following
    diagram:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Local Interpretable Model-Agnostic Explanation**（**LIME**）：这是一种模型无关的方法，用于解释预测。它侧重于局部解释；也就是说，解释反映了模型对预测数据实例的行为。例如，LIME可以建议哪些因素或特征对模型预测结果很重要。这可以在以下图中看到：'
- en: '![Figure 11.3 – Explaining individual predictions using LIME'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 使用 LIME 解释单个预测]'
- en: '](img/image005.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 005](img/image005.jpg)'
- en: Figure 11.3 – Explaining individual predictions using LIME
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 使用 LIME 解释单个预测
- en: In the preceding diagram, a model is predicting that a patient has diabetes.
    The LIME explainer highlights and implies the symptoms of diabetes, such as dry
    skin, excessive urine, and blurry vision, that contribute to the `Diabetes` prediction,
    while `No fatigue` is evidence against it. Using the explainer, a doctor can decide
    and draw conclusions from the model's prediction and provide the patient with
    the appropriate treatment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，模型预测患者患有糖尿病。LIME解释器突出了与糖尿病相关的症状，如干燥的皮肤、过多的尿液和模糊的视力，这些症状有助于“糖尿病”预测，而“无疲劳”则是反对的证据。使用解释器，医生可以从模型的预测中做出决定并得出结论，并为患者提供适当的治疗。
- en: You can learn more about AI explanations at [https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview](https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview](https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview)了解更多关于AI解释的信息。
- en: Non-feature attribution methods
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非特征归因方法
- en: 'Non-feature attribution methods do not focus on how features contribute to
    your model''s predictions. Instead, they focus on the relationship between input
    data and output data for your model inference. Here are some non-feature attribution
    methods:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 非特征归因方法不关注特征如何影响你的模型预测。相反，它们关注模型推理中输入数据和输出数据之间的关系。以下是一些非特征归因方法：
- en: '**Deeplift**: This is used to evaluate neural networks by comparing each neuron''s
    activation to its reference activation and assigning contribution scores according
    to the difference. Deeplift reveals correlations and contributions. For example,
    let''s say we are using a cat and dog image classifier to classify images between
    cats and dogs. Suppose the classifier predicts that the input image is a dog by
    using the deeplift method. Here, we can backpropagate the neurons that were activated
    in the image classifier''s neural network to their reference activations and then
    assign contribution scores to each feature based on the difference.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Deeplift**：这是通过比较每个神经元的激活与其参考激活并根据差异分配贡献分数来评估神经网络的。Deeplift揭示了相关性和贡献。例如，假设我们正在使用猫和狗图像分类器来对猫和狗之间的图像进行分类。假设分类器使用deeplift方法预测输入图像是狗。在这里，我们可以将图像分类器神经网络中激活的神经元反向传播到它们的参考激活，然后根据差异为每个特征分配贡献分数。'
- en: '**Natural language explanations** (**NLE**): NLE aims to capture input-output
    relationships for text explanations using a fusion of techniques such as partial
    dependence function, gradient analysis, contextual encoding, Individual Conditional
    Expectation, Accumulated Local Effects, and so on. These techniques are useful
    for interpreting language models that classify or generate text. NLE provide easily
    understandable and useful reasons for users to decide whether to trust the model''s
    decision and take action. For example, you may wish to buy a product based on
    a model''s recommendation and explanation. Tools such as Microsoft Power BI and
    Qlik Sense can be used to plug and play to study NLE. These tools need to be customized
    as per your need or use case.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言解释**（**NLE**）：NLE旨在通过融合部分依赖函数、梯度分析、上下文编码、个体条件期望、累积局部效应等技术，捕捉文本解释的输入输出关系。这些技术在解释分类或生成文本的语言模型方面非常有用。NLE为用户提供易于理解和有用的理由，以决定是否信任模型的决策并采取行动。例如，你可能希望根据模型的推荐和解释购买产品。可以使用如Microsoft
    Power BI和Qlik Sense等工具来插入并研究NLE。这些工具需要根据你的需求或用例进行定制。'
- en: There are other methods apart from the ones mentioned in the previous list.
    This area is a hot topic for research in the field of AI. Many researchers and
    business leaders are pursuing solving Explainable AI problems to explain model
    decisions to internal and external stakeholders. Having an Explainable AI-driven
    interface provisioned for multiple business stakeholders can help them answer
    critical business questions. For instance, a business leader needs to be able
    to answer, "How do these model decisions impact business?" while for IT and Operations,
    it is vital to know the answer to "How do I monitor and debug?"
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的那些方法之外，还有其他方法。这个领域是AI领域研究的热点。许多研究人员和商业领袖都在追求解决可解释AI问题，向内部和外部利益相关者解释模型决策。为多个商业利益相关者提供可解释AI驱动的界面可以帮助他们回答关键的商业问题。例如，一位商业领袖需要能够回答“这些模型决策如何影响业务？”而对于IT和运营来说，了解“我如何监控和调试？”的答案至关重要。
- en: Answering these questions for multiple business stakeholders enables employees
    and businesses to adapt to AI and maximize value from it, by ensuring model transparency
    and model compliance while adapting to changing environments by optimizing model
    bias and drift.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为多个商业利益相关者回答这些问题，使员工和企业能够适应人工智能，并通过确保模型透明度和合规性，在优化模型偏差和漂移的同时适应不断变化的环境，从而最大化从人工智能中获得的价值。
- en: Explainable AI = model transparency and explainability
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释人工智能 = 模型透明度和可解释性
- en: Since ML models are becoming first-class citizens, in order to monitor a model's
    performance with respect to these areas, we can use Explainable Monitoring, which
    allows us to analyze and govern ML systems in production by monitoring and explaining
    their decisions using Explainable AI methods. Explainable monitoring is a hybrid
    of Explainable AI; it uses Explainable AI methods infused with **operations**
    (**Ops**) in production. Explainable monitoring is becoming an integral part of
    the MLOps workflow. We'll look at how Explainable Monitoring brings value to the
    MLOps workflow in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型正成为一等公民，为了监控模型在这些领域的性能，我们可以使用可解释监控，它允许我们通过监控和解释其决策来分析和治理生产中的机器学习系统，使用的是可解释人工智能方法。可解释监控是可解释人工智能的混合体；它在生产中融合了**操作**（**Ops**）的可解释人工智能方法。可解释监控正成为
    MLOps 工作流程的一个组成部分。我们将在下一节中探讨可解释监控如何为 MLOps 工作流程带来价值。
- en: Monitoring in the MLOps workflow
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps 工作流程中的监控
- en: We learned about the MLOps workflow in [*Chapter 1*](B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015),
    *Fundamentals of MLOps Workflow*. As shown in the following diagram, the monitoring
    block is an integral part of the MLOps workflow for evaluating the ML models'
    performance in production and measuring the ML system's business value. We can
    only do both (measure the performance and business value that's been generated
    by the ML model) if we understand the model's decisions in terms of transparency
    and explainability (to explain the decisions to stakeholders and customers).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第一章*](B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015)《MLOps 工作流程基础》中了解了
    MLOps 工作流程。如图所示，监控模块是 MLOps 工作流程的一个组成部分，用于评估生产中机器学习模型的性能和衡量机器学习系统的商业价值。只有当我们从透明度和可解释性的角度理解模型的决策时，我们才能做到这两者（衡量由机器学习模型产生的性能和商业价值），以便向利益相关者和客户解释这些决策。
- en: 'Explainable Monitoring enables both transparency and explainability to govern
    ML systems in order to drive the best business value:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释监控使透明度和可解释性得以治理机器学习系统，从而驱动最佳商业价值：
- en: '![Figure 11.4 – MLOps workflow – Monitor'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.4 – MLOps 工作流程 – 监控'
- en: '](img/image007.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image007.jpg](img/image007.jpg)'
- en: Figure 11.4 – MLOps workflow – Monitor
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – MLOps 工作流程 – 监控
- en: 'In practice, **Explainable Monitoring** enables us to monitor, analyze, and
    govern ML system, and it works in a continuous loop with other components in the
    MLOps workflow. It also empowers humans to engage in the loop to understand model
    decisions and teach the model (by labeling data and retraining the model) on the
    go. Explainable Monitoring enables continual learning and can be highly rewarding
    for a company in the long run. A continual learning pipeline with a human in the
    loop, enabled using Explainable Monitoring, can be seen in the following diagram:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，**可解释监控**使我们能够监控、分析和治理机器学习系统，并且它与其他 MLOps 工作流程组件一起形成一个连续的循环。它还赋予人类参与循环的机会，以理解模型决策并即时教导模型（通过标记数据和重新训练模型）。可解释监控实现了持续学习，并且从长远来看，对一家公司来说可能非常有价值。以下图中可以看到，使用可解释监控实现的人机交互的持续学习流程：
- en: '![Figure 11.5 – Continual learning enabled by Explainable Monitoring'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.5 – 由可解释监控实现的持续学习'
- en: '](img/image009.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image009.jpg](img/image009.jpg)'
- en: Figure 11.5 – Continual learning enabled by Explainable Monitoring
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 由可解释监控实现的持续学习
- en: Continual learning is the system's ability to learn continuously in a changing
    environment while building on what had been learned previously. To facilitate
    continual learning, data and modeling must work hand in hand, and be assisted
    by humans in the loop (typically, a QA analyst or system admin, such as a data
    scientist or ML engineer). Explainable Monitoring plays a vital role in continual
    learning systems to increase revenue, stay compliant, and build ML systems responsibly.
    After all, only a model that's been deployed with continual learning capabilities
    can bring business value.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Explainable Monitoring Framework
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore the Explainable Monitoring Framework (as shown
    in the following diagram) in detail to understand and learn how Explainable Monitoring
    enhances the MLOps workflow and the ML system itself:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Explainable Monitoring Framework'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image011.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – Explainable Monitoring Framework
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The Explainable Monitoring Framework is a modular framework that's used to monitor,
    analyze, and govern a ML system while enabling continual learning. All the modules
    work in sync to enable transparent and Explainable Monitoring. Let's look at how
    each module works to understand how they contribute and function in the framework.
    First, let's look at the monitor module (the first panel in the preceding diagram).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Monitor
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The monitor module is dedicated to monitoring the application in production
    (serving the ML model). Several factors are at play in an ML system, such as application
    performance (telemetry data, throughput, server request time, failed requests,
    error handling, and so on), data integrity and model drift, and changing environments.
    The monitor module should capture vital information from the system logs in production
    to track the ML system''s robustness. Let''s look at the importance and functionality
    of three of the monitor module''s functionalities: data integrity, model drift,
    and application performance.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensuring the data integrity of an ML application includes checking incoming
    (input data to the ML model) and outgoing (ML model prediction) data to ensure
    ML systems' integrity and robustness. The monitor module ensures data integrity
    by inspecting the volume, variety, veracity, and velocity of the data in order
    to detect outliers or anomalies. Detecting outliers or anomalies prevents ML systems
    from having poor performance and being susceptible to security attacks (for example,
    adversarial attacks). Data integrity coupled with efficient auditing can facilitate
    the desired performance of ML systems to derive business value.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If model drift is not measured, the model''s performance can easily become
    sub-par and can hamper the business with poor decision making and customer service.
    For example, it is hard to foresee changes or trends in data during a black swan
    event such as COVID-19\. Here is some news that made it to the headlines:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the Instacart model forecasting item's accessibility in stores
    fell from 93% to 61% due to a dramatic change in shopping habits ([https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/](https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/)).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bankers doubted if credit models that have been trained for good times will
    respond accurately to stress scenarios ([https://www.americanbanker.com/opinion/ai-models-could-struggle-to-handle-the-market-downturn](https://www.americanbanker.com/opinion/ai-models-could-struggle-to-handle-the-market-downturn)).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In response to market uncertainty, trading algorithms misfired. There was a
    21% decline in some funds ([https://www.wired.com/story/best-ai-models-no-match-coronavirus](https://www.wired.com/story/best-ai-models-no-match-coronavirus)).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image classification models struggle to adapt to the "new normal" in the wake
    of the COVID-19 pandemic: a family at home in front of laptops can now mean "work,"
    not "leisure." ([https://techcrunch.com/2020/08/02/ai-is-struggling-to-adjust-to-2020/](https://techcrunch.com/2020/08/02/ai-is-struggling-to-adjust-to-2020/))'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, it is important to monitor model drift in any form, such as data drift,
    concept drift, or any upstream data changes, in order to adapt to the changing
    environments and serve businesses and customers in the most relevant way and generate
    the maximum business value.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Application performance
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is critical to monitor application performance to foresee and prevent any
    potential failures, since this ensures the robustness of ML systems. Here, we
    can monitor the critical system logs and telemetry data of the production deployment
    target (for example, Kubernetes or an on-premises server). Monitoring application
    performance can give us key insights in real time, such as the server's throughput,
    latency, server request time, number of failure requests or control flow errors,
    and so on. There is no hard and set way of monitoring applications and, depending
    on your business use case, your application performance mechanism can be curated
    and monitored to keep the system up and running to generate business value.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the monitor component, we monitored data integrity, model drift,
    and application performance. In the next section, we will analyze how to monitor
    the data of the model and application.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Analyze
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing your ML system in production in real time is key to understanding
    the performance of your ML system and ensuring its robustness. Humans play a key
    role in analyzing model performance and detecting subtle anomalies and threats.
    Hence, having a human in the loop can introduce great transparency and explainability
    to the ML system. We can analyze model performance to detect any biases or threats
    and to understand why the model makes decisions in a certain pattern. We can do
    this by applying advanced techniques such as data slicing, adversarial attack
    prevention techniques, or by understanding local and global explanations. Let's
    see how we can do this in practice.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Data slicing
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a great number of success stories surrounding ML in terms of improving
    businesses and life in general. However, there is still room to improve data tools
    for debugging and interpreting models. One key area of improvement is understanding
    why models perform poorly on certain parts or slices of data and how we can balance
    their overall performance. A slice is a part or a subset of a dataset. Data slicing
    can help us understand the model's performance on different types of sub-datasets.
    We can split the dataset into multiple slices or subsets and study the model's
    behavior on them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s consider a hypothetical case where we have trained a random
    forest model to classify whether a person''s income is above or below $50,000\.
    The model has been trained on the UCI census data ([https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)).
    The results of the model for slices (or subsets) of data can be seen in the following
    table. This table suggests that the overall metrics may be considered acceptable
    as the overall log loss is low for all the data (see the *All* row). This is a
    widely used loss metric for binary classification problems and represents how
    close the prediction''s likelihood is to the actual/true value; it is 0 or 1 in
    the case of binary classification. The more the predicted probability diverges
    from the actual value, the higher the log loss value is. However, the individual
    slices tell a different story:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 11.1 – UCI census data slices'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '](img/011.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 – UCI census data slices
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the previous table, we can conclude that the model's performance
    is decent. However, if we look at the performance of male versus female subjects,
    we can see that the model only performs well for female subjects where the log
    losses are less compared to the log loss for male subjects. On the other hand,
    if you look at the *Prof – specialty* occupation, you will see that the net performance
    is on par with the performance of male subjects with log losses of 0.45 and 0.41,
    respectively, whereas the effect size for *Prof – specialty* is considerably less.
    The model performs poorly for *Bachelors*, *Masters*, and *Doctorates* as the
    log losses are high with values of 0.44, 0.49, and 0.59, respectively. It is also
    important to note that if the log loss of a slice and its counterpart is below
    acceptable, this suggests that the model is bad overall and not just on a particular
    data slice.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Data slicing enables us to see subtle biases and unseen correlations to understand
    why a model might perform poorly on a subset of data. We can avoid these biases
    and improve the model's overall performance by training the model using balanced
    datasets that represent all the data slices (for example, using synthetic data
    or by undersampling, and so on) or by tuning hyperparameters of the models to
    reduce overall biases. Data slicing can provide an overview of model fairness
    and performance for an ML system, and can also help an organization optimize the
    data and ML models to reach optimal performance and decent fairness thresholds.
    Data slicing can help build trust in the AI system by offering transparency and
    explainability into data and model performance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a comprehensive overview of data slicing and automated data slicing
    methods, take a look at *Automated Data Slicing for Model Validation: A Big data
    - AI Integration Approach* at [https://arxiv.org/pdf/1807.06068.pdf](https://arxiv.org/pdf/1807.06068.pdf).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Bias and threat detection
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure that robust and ethical decisions are made using ML models, we need
    to make sure that the models are fair and secure. Any biases and threats need
    to be monitored and mitigated, to avoid unethical or partial decisions that benefit
    any particular party in order to comply with business values and the law.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of bias, such as selection bias (data that's used
    for training models is not representative of the population, such as minorities),
    framing bias (questions or a survey that's used to collect data is framed in a
    point of view or slant), systematic bias (repetitive or consistent error), response
    bias (data in which participants respond incorrectly by following their conscious
    bias), or confirmation bias (collecting data to validate your own preconceptions).
    To avoid these biases and mitigate them, techniques such as data slicing, slice-based
    learning, or balancing the bias-variance tradeoff can be applied, depending on
    the use case.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: An ML system is exposed to security threats that need monitoring and mitigation.
    We have discussed some common threats and threat-prevention techniques involving
    adversarial attacks, poison attacks, privacy attacks or backdoor attacks, and
    so on, in [*Chapter 9*](B16572_09_Final_JM_ePub.xhtml#_idTextAnchor176), *Testing
    and Securing Your ML Solution.*
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Local and global explanations
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Local and global explanations offer different perspectives on model performance.
    Local explanations offer justification for model prediction for a specific or
    individual input, whereas global explanations provide insights into the model''s
    predictive process, independent of any particular input. For example, let''s take
    a look at a hypothetical case of a **recurrent neural network** (**RNN**) model
    being used to perform sentiment analysis for customer reviews. The following diagram
    shows the global explanation (the process as a whole) for the RNN model''s sentiment
    analysis using the RNNVis tool:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Global explanation of the RNN model (using RNNVis) to understand
    the process as a whole (how hidden states, layers, and so on impact model outputs
    and predictive processes)](img/image013.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Global explanation of the RNN model (using RNNVis) to understand
    the process as a whole (how hidden states, layers, and so on impact model outputs
    and predictive processes)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/](https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, for example, the co-clustering visualization shows different word clouds
    for words with positive and negative sentiments. Using global explanations, we
    can simulate the model''s predictive process and understand correlations with
    regards to parameters or the model''s architecture (for example, hidden states
    and layers). Global explanations offer two perspectives of explainability: the
    high-level model process and the predictive explanations. On the other hand, local
    explanations give insights into single predictions. Both explanations are valuable
    if we wish to understand the model''s performance and validate it comprehensively.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: In the analyze component, we can analyze the model's performance using the techniques
    we have explored, such as data slicing, Bias and threat detection, and local and
    global explanations. In the next section, we will learn how to govern and control
    ML systems to efficiently guide it to achieve operational or business objectives.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Govern
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ML systems' efficacy is dependent on the way it is governed to achieve maximum
    business value. A great part of system governance involves quality assurance and
    control, as well as model auditing and reporting, to ensure it has end-to-end
    trackability and complies with regulations. Based on monitoring and analyzing
    the model's performance, we can control and govern ML systems. Governance is driven
    by smart alerts and actions to maximize business value. Let's look into how alerts
    and actions, model quality assurance and control, and model auditing and reports
    orchestrate the ML system's governance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Alerts and actions
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Governing a ML system involves monitoring and analyzing the ML application.
    Here, system developers can be alerted about when the system is showing anomalous
    behavior such as failed requests, slow server response times, server exceptions,
    errors, or high latency. Alerting the system developers or admin can ensure quality
    assurance and prevent system failures. There are two different types of alerts:
    alerts for system performance and model performance-based alerts. Here are some
    examples of alerts for system performance:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based alerts for failed requests based on a threshold
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based alerts for server response time based on a threshold
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based alerts for server exceptions based on a threshold
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based alerts for availability based on a threshold
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model performance alerts are generated when the model experiences drift or
    anomalous feature distribution or bias. When such events are recorded, the system
    administrator or developers are alerted via email, SMS, push notifications, and
    voice alerting. These alert actions (automated or semi-automated) can be used
    to mitigate system performance deterioration. Depending on the situation and need,
    some possible actions can be evoked, such as the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an alternative model upon experiencing high model drift
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining a model
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a new model
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restarting the ML system
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redeploying the ML system
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model quality assurance and control
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A model''s quality assurance and control mechanism can be quite rewarding for
    those using an ML system, if we wish to prevent many possible mishaps and to ensure
    regular and healthy monitoring and functionality of the ML system. It is recommended
    to have a framework or mechanism for model quality assurance and control. For
    this, a quality assurance framework for ML systems, as shown in the following
    diagram, can enable the mechanism for your organization. It is a modular framework
    that''s used to monitor three important aspects of ML systems:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Model Quality Assurance Framework'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image015.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 – Model Quality Assurance Framework
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Quality assurance experts can help your company or organization put a test
    mechanism into place to validate whether the data being used for training has
    been sanitized, ensure the data being used for model inference does not contain
    threats for that ML system, and to monitor data drift to understand and validate
    the changing environment. Monitoring and testing data can be achieved by quality
    assurance or test engineers, together with product managers, by doing the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Understand and validate the data's statistical relations (for example, mean,
    median, mode, and so on) for training, testing, and inferring data.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop tests to verify the aforementioned statistics and relationships (using
    scripts).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the distribution of characteristics using feature engineering techniques
    such as feature selection, dimensionality reduction, and so on.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrain and review the performance of all models.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the performance of all the models at regular intervals with new datasets.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raise an alert if another model (from the model inventory) performs with better
    accuracy than the existing model.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform tests at regular intervals.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model auditing and reports
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model auditing and reporting is essential if you want to have enough information
    for the regulators and for compliance with the law. Having end-to-end traceability
    for the model ensures great transparency and explainability, which can result
    in transparent governance mechanisms for an organization or company. The goal
    of model auditing and reporting is to assess the model''s performance and based
    on that, enable ML system governance. In the following diagram, we can see a big-picture
    overview of the model transparency chart that''s generated from auditing and reporting:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Model QA framework'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image017.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.9 – Model Transparency chart
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Model assessments based on auditing and reporting will ensure healthy, transparent,
    and robust governance mechanisms for organizations and enable them to have end-to-end
    traceability in order to comply with the regulators. Having such mechanisms will
    help save organizations a great amount of time and resources and enable efficiency
    in interactions with the regulators.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Enabling continuous monitoring for the service
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Explainable Monitoring Framework can be resourceful if we wish to monitor
    ML systems in production. In the next chapter, we will enable the Explainable
    Monitoring Framework for the business use case we worked on in the previous chapters.
    We will enable continuous monitoring for the system we have deployed. We will
    then monitor the ML application that's been deployed to production and analyze
    the incoming data and the model's performance to govern the ML system to produce
    maximum business value for the use case.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the key principles for monitoring an ML system.
    We explored some common monitoring methods and the Explainable Monitoring Framework
    (including the monitor, analyze, and govern stages). We then explored the concepts
    of Explainable Monitoring thoroughly.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了监控机器学习系统的关键原则。我们探讨了常见的监控方法以及可解释监控框架（包括监控、分析和治理阶段）。然后，我们深入探讨了可解释监控的概念。
- en: In the next chapter, we will delve into a hands-on implementation of the Explainable
    Monitoring Framework. Using this, we will build a monitoring pipeline in order
    to continuously monitor the ML system in production for the business use case
    (predicting weather at the port of Turku).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨可解释监控框架的实战应用。利用这个框架，我们将构建一个监控管道，以便持续监控生产环境中的机器学习系统，用于业务场景（例如预测图尔库港的天气）。
- en: The next chapter is quite hands-on, so buckle up and get ready!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章非常注重实践，所以请系好安全带，做好准备！
