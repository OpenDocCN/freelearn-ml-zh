<html><head></head><body>
		<div id="_idContainer250" class="Content">
			<h1 id="_idParaDest-285"><em class="italics"><a id="_idTextAnchor288"/>Chapter 7:</em></h1>
		</div>
		<div id="_idContainer251" class="Content">
			<h1 id="_idParaDest-286"><a id="_idTextAnchor289"/>Model Improvements</h1>
		</div>
		<div id="_idContainer252" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Explain and implement the concept of bias and variance trade-off in machine learning models.</li>
				<li class="bullets">Perform model assessment with cross-validation.</li>
				<li class="bullets">Implement hyperparameter tuning for machine learning models.</li>
				<li class="bullets">Improve a model's performance with various hyperparameter tuning techniques.</li>
			</ul>
			<p>In this chapter, we will focus on improving a model's performance using cross-validation techniques and hyperparameter tuning.</p>
		</div>
		<div id="_idContainer268" class="Content">
			<h2 id="_idParaDest-287"><a id="_idTextAnchor290"/>Introduction</h2>
			<p>In the previous chapter, we explored a few strategies that helped us build improved models using <strong class="bold">feature selection</strong> and <strong class="bold">dimensionality reduction</strong>. These strategies primarily focus on improving the model's computational performance and interpretability; however, to improve the model's performance with respect to performance metrics, such as overall accuracy or error estimates to build robust and more generalized models, we will need to focus on cross-validation and hyperparameter tuning.</p>
			<p>In this chapter, we will walk you through the fundamental topics in machine learning to build generalized and robust models using cross-validation and hyperparameter tuning and implement them in R.</p>
			<p>We will first study the topics in this chapter in detail with layman examples and leverage simple use cases to see the implementation in action.</p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor291"/>Bias-Variance Trade-off</h2>
			<p>An interesting, arduous, and repetitive part of machine learning is the <strong class="bold">model evaluation journey</strong>. There is again, art and a different mindset required to build models that are robust. Throughout this book, we have simplified the model evaluation process with training and testing datasets that were derived by splitting the available data into a <strong class="bold">70:30</strong> or <strong class="bold">80:20</strong> ratio. Although this approach was effective in helping us understand how the model performs on unseen data, it still leaves several loopholes that might render the model futile for most other cases. We will need a more formal, thorough, and exhaustive method of validation for a machine learning model to be robust for future prediction events. In this chapter, we will study <strong class="bold">cross-validation</strong> and its various approaches to assess the performance of a machine learning model.</p>
			<p>Before we delve into the specifics of the topic, we need to explore a crucial topic in machine learning called <strong class="keyword">bias-variance trade-off</strong>. This topic has been much in discussion in most machine learning forums and academia. A crucial topic to the machine learning fraternity, it forms the foundation before studying model validation and improvements in depth. From the title of the topic, it may be easy to infer that in machine learning models, the bias-variance trade-off is a behavior exhibited by models, where models that showcase low bias in estimating model parameters, unfortunately, demonstrate higher variance in estimating model parameters, and vice versa. To understand the topic from a layman's perspective, let's first break down the topic into individual components, understand each component, and then reconstruct the larger picture with all components together.</p>
			<h3 id="_idParaDest-289"><a id="_idTextAnchor292"/>What is Bias and Variance in Machine Learning Models?</h3>
			<p>In general, when a machine learning model fails to learn important (or sometimes complex) patterns exhibited in data, we say the model is <strong class="bold">biased</strong>. Such a model oversimplifies itself or only learns extremely simple rules that may not be helpful in making accurate predictions. The net outcome from such models is that the predictions tend to remain mostly the same (and incorrect), irrespective of the differences in input data. The patterns learned by the model are so simple or biased that the variations in the input data, unfortunately, don't yield the expected predictions.</p>
			<p>On the other hand, if we reverse the rationale, we can easily define variance in machine learning models. Think about models that learn unnecessary patterns, such as noise from the data, such that even small variations in the input data lead to significantly large undesirable changes in the prediction. In such cases, we say that the model has a high variance.</p>
			<p>The ideal scenario would be a model with low bias and low variance; that is, a model that has learned the necessary patterns from data. It successfully ignored the noise and delivers reasonable and desirable changes in predictive behavior with reasonable changes in the input data. Unfortunately, the ideal scenario is difficult to achieve and thus we arrive at the topic of <strong class="bold">the bias-variance trade-off</strong>.</p>
			<p>Putting together all the individual components we studied, we can say every attempt to reduce bias or variance will lead to an increase in the other dimension, resulting in a situation where we would need to strike a balance between the desired bias and variance in model performance. The necessary changes that can be incorporated in machine learning models to strike the balance between bias and variance are achieved using a combination of hyperparameter tuning methods. We will study the concept of hyperparameter tuning in the upcoming sections. The following is a famous example used to demonstrate the bias-variance concept with a visual bullseye diagram:</p>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="image/C12624_07_01.jpg" alt="Figure 7.1: The bias-variance concept with a visual bullseye diagram&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.1: The bias-variance concept with a visual bullseye diagram</h6>
			<p>In the previous diagram, we can see four quadrants to specifically distinguish the bias-variance trade-off. The diagram is used to interpret the model bias and variance for a regression use case. Inferring a similar idea visually for a classification use case might be challenging; however, we get the bigger picture with the illustrated example.</p>
			<p>Our ideal goal is to train a machine learning model with low bias and low variance. However, when we have low bias and high variance (the top-right quadrant in the preceding visualization), we see significantly large changes in the end outcome for a small variation in the input data. On the other hand, when we have high bias and low variance (the bottom-left quadrant in the visualization), we can see the end outcome getting concentrated in a region away from the target, demonstrating barely any variations for changes in the input. Lastly, we have high bias and high variance, that is, we hit far away from the target, as well as have large variations for small changes in the input. This would be the most undesirable state for a model.</p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor293"/>Underfitting and Overfitting</h2>
			<p>In the previous scenario, where we have a high bias, we denote a phenomenon called <strong class="bold">underfitting</strong> in machine learning models. Similarly, when we have high variance, we denote a phenomenon called <strong class="bold">overfitting</strong> in machine learning models.</p>
			<p>The following visual demonstrates the idea of <strong class="bold">overfitting</strong>, <strong class="bold">underfitting</strong>, and <strong class="bold">ideal balance</strong> for a regression model. We can see high bias resulting in an oversimplified model (that is, underfitting); high variance resulting in overcomplicated models (that is, overfitting); and lastly, striking the right balance between bias and variance:</p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="image/C12624_07_02.jpg" alt="Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal balance&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal balance</h6>
			<p>To study bias and variance in machine learning models more effectively, we have cross-validation techniques. These techniques help us understand the model performance more intuitively.</p>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor294"/>Defining a Sample Use Case</h2>
			<p>For the purpose of exploring topics in this chapter with a practical dataset, we use a small dataset already available in the <strong class="inline">mlbench</strong> package, called <strong class="inline">PimaIndiansDiabetes</strong>, which is a handy dataset for classification use cases.</p>
			<p>The dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The use case that can be tailored from the dataset is when predicting if a patient has diabetes as a function of few medical diagnostic measurements.</p>
			<h4>Note</h4>
			<p class="callout">Additional information can be found at http://math.furman.edu/~dcs/courses/math47/R/library/mlbench/html/PimaIndiansDiabetes.html.</p>
			<p class="callout">The selection of the use case with a dataset size of less than 1000 rows is intentional. The topics explored in this chapter require high computation time on commodity hardware for regular use cases with large datasets. The selection of small datasets for the purpose of demonstration helps in achieving the outcome with fairly normal computational time for most readers using mainstream hardware.</p>
			<h3 id="_idParaDest-292"><a id="_idTextAnchor295"/>Exercise 88: Loading and Exploring Data</h3>
			<p>To quickly study the overall characteristics of <strong class="inline">PimaIndiansDiabetes</strong> and explore the nature of the contents in each column, perform the following steps:</p>
			<ol>
				<li>Use the following commands to load the <strong class="inline">mlbench</strong>, <strong class="inline">randomForest</strong>, and <strong class="inline">dplyr</strong> libraries:<p class="snippet">library(mlbench)</p><p class="snippet">library(randomForest)</p><p class="snippet">library(dplyr)</p></li>
				<li>Use the following command to load data from the <strong class="inline">PimaIndiansDiabetes</strong> dataset:<p class="snippet">data(PimaIndiansDiabetes)</p><p class="snippet">df&lt;-PimaIndiansDiabetes</p></li>
				<li>Explore the dimensions of the dataset and study the content within each column using the <strong class="inline">str</strong> command:<p class="snippet">str(df)</p><p>The output is as follows:</p><p class="snippet">'data.frame':768 obs. of  9 variables:</p><p class="snippet"> $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...</p><p class="snippet"> $ glucose : num  148 85 183 89 137 116 78 115 197 125...</p><p class="snippet"> $ pressure: num  72 66 64 66 40 74 50 0 70 96 ...</p><p class="snippet"> $ triceps : num  35 29 0 23 35 0 32 0 45 0 ...</p><p class="snippet"> $ insulin : num  0 0 0 94 168 0 88 0 543 0 ...</p><p class="snippet"> $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...</p><p class="snippet"> $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...</p><p class="snippet"> $ age     : num  50 31 32 21 33 30 26 29 53 54 ...</p><p class="snippet"> $ diabetes: Factor w/ 2 levels "neg","pos": 2 1 2 1 2 1 2 1 2 2 ...</p></li>
			</ol>
			<p>As we can see, the dataset has <strong class="inline">768</strong> observations and <strong class="inline">9</strong> variables, that is, <em class="italics">8</em> independent variables and <em class="italics">1</em> dependent categorical variable <strong class="inline">diabetes</strong> with values as <strong class="inline">pos</strong> for positive and <strong class="inline">neg</strong> for negative.</p>
			<p>We will use this dataset and develop several classification models for the further topics in this chapter.</p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor296"/>Cross-Validation</h2>
			<p>Cross-validation is a model validation technique that aids in assessing the performance and ability of a machine learning model to generalize on an independent dataset. It is also called <strong class="bold">rotation validation</strong>, as it approaches the validation of a model with several repetitions by drawing the training and validation data from the same distribution.</p>
			<p>The cross-validation helps us:</p>
			<ul>
				<li>Evaluate the robustness of the model on unseen data.</li>
				<li>Estimate a realistic range for desired performance metrics.</li>
				<li>Mitigate overfitting and underfitting of models.</li>
			</ul>
			<p>The general principle of cross-validation is to test the model on the entire dataset in several iterations by partitioning data into groups and using majority to train and minority to test. The repetitive rotations ensure the model has been tested on all available observations. The final performance metrics of the model are aggregated and summarized from the results of all rotations.</p>
			<p>To study if the model has high bias, we can check the mean (average) performance of the model across all rotations. If the mean performance metrics say overall accuracy (for classification) or <strong class="bold">mean absolute percentage error</strong> (for regression) is low, then there is a high bias and the model is underfitting. To study if the model has a high variance, we can study the standard deviation of the desired performance metrics across rotations. A high standard deviation would indicate the model will have high variance; that is, the model will be overfitting.</p>
			<p>There are several popular approaches in cross-validation:</p>
			<ul>
				<li>Holdout validation</li>
				<li>K-fold cross-validation</li>
				<li>Hold-one-out validation (LOOCV)</li>
			</ul>
			<p>Let's explore each of these approaches in details.</p>
			<h2 id="_idParaDest-294"><a id="_idTextAnchor297"/>Holdout Approach/Validation</h2>
			<p>This is the easiest approach (though not the most recommended) used in validating model performance. We have used this approach throughout the book to test our model performance in the previous chapters. Here, we randomly divide the available dataset into training and testing datasets. Most common split ratios used between the train and test datasets are <strong class="bold">70:30</strong> or <strong class="bold">80:20</strong>.</p>
			<p>The major drawbacks of this approach are that the model performance is purely evaluated from a fractional test dataset, and it might not be the best representation for the model performance. The evaluation of the model will completely depend on the type of split, and therefore, the nature of the data points that end up in the training and testing datasets, which might then lead to significantly different results and thus high variance.</p>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="image/C12624_07_03.jpg" alt="Figure 7.3: Holdout validation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.3: Holdout validation</h6>
			<p>The following exercise divides the dataset into 70% training and 30% testing, and builds a random forest model on the training dataset and then evaluates the performance using the testing dataset. This method was widely used in <em class="italics">Chapter 5</em>, <em class="italics">Classification</em>, so you shouldn't be surprised by the process.</p>
			<h3 id="_idParaDest-295"><a id="_idTextAnchor298"/>Exercise 89: Performing Model Assessment Using Holdout Validation</h3>
			<p>In this exercise, we will leverage the data we loaded into memory in <em class="italics">Exercise 1</em>: <em class="italics">Loading and Exploring the Data</em>, to create a simple random forest classification model and perform model assessment using the holdout validation technique.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">First, import the <strong class="inline">caret</strong> package into the system using the following command. The <strong class="inline">caret</strong> package provides us with ready-to-use functions for model assessment, namely, <strong class="inline">ConfusionMatrix</strong>:<p class="snippet">library(caret)</p></li>
				<li>Now, set up the seed for reproducibility as follows:<p class="snippet">set.seed(2019)</p></li>
				<li>Create 70% <strong class="inline">train</strong> and a 30% <strong class="inline">test</strong> dataset using the following command:<p class="snippet">train_index&lt;- sample(seq_len(nrow(df)),floor(0.7 * nrow(df)))</p><p class="snippet">train &lt;- df[train_index,]</p><p class="snippet">test &lt;- df[-train_index,]</p></li>
				<li>Use the <strong class="inline">print</strong> function to display the output required:<p class="snippet">print("Training Dataset shape:")</p><p class="snippet">print(dim(train))</p><p class="snippet">print("Test Dataset shape:")</p><p class="snippet">print(dim(test))</p></li>
				<li>Create a random forest model by fitting on the <strong class="inline">train</strong> dataset:<p class="snippet">model &lt;-randomForest(diabetes~.,data=train, mtry =3)</p></li>
				<li>Print the model using the following command:<p class="snippet">print(model)</p></li>
				<li>Use the <strong class="inline">predict</strong> method on <strong class="inline">test</strong> dataset as illustrated here:<p class="snippet">y_predicted&lt;- predict(model, newdata = test)</p></li>
				<li>Create and print the <strong class="inline">Confusion-Matrix</strong> using the following command:<p class="snippet">results&lt;-confusionMatrix(y_predicted, test$diabetes, positive= 'pos')</p><p class="snippet">print("Confusion Matrix  (Test Data)- ")</p><p class="snippet">print(results$table)</p></li>
				<li>Print the overall accuracy using the following command:<p class="snippet">results$overall[1]</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<img src="image/C12624_07_04.jpg" alt="Figure 7.4: Model assessment using holdout validation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.4: Model assessment using holdout validation</h6>
			<p>We can see the overall accuracy is 77%. This might not be the best representation of the model performance as we have only evaluated it on a random test sample. The results might be different if we use a different test sample. Let's now explore additional cross-validation approaches that overcome this trade-off.</p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor299"/>K-Fold Cross-Validation</h2>
			<p>This technique is the most recommended approach for model evaluation. In this technique, we partition the data into <em class="italics">k</em> groups and use <em class="italics">k-1</em> groups for training and the remainder (1 group) for validation. The process is repeated <em class="italics">k</em> times, where a new group is used for validation in each successive iteration, and therefore, each group is used for testing at one point of time. The overall results are the average error estimates across <em class="italics">k</em> iterations.</p>
			<p><em class="italics">k</em>-fold cross-validations, therefore, overcomes the drawbacks of the holdout technique by mitigating the perils associated with the nature of split as each data point is tested once over the book of <em class="italics">k</em> iterations. The variance of the model is reduced as the value of <em class="italics">k</em> increases. The most common values used for <em class="italics">k</em> are 5 or 10. The major drawback of this technique is that it trains the model <em class="italics">k</em> times (for <em class="italics">k</em> iterations). Therefore, the total compute time required for the model to train and validate is approximately <em class="italics">k</em> times the holdout method.</p>
			<p>The following visual demonstrates a five-fold cross-validation and the aggregate results (hypothetical) from all rotations:</p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<img src="image/C12624_07_05.jpg" alt="Figure 7.5: K-fold validation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.5: K-fold validation</h6>
			<p>The following code snippet implements the 5-fold cross-validation on the same dataset used in the previous example and prints the average accuracy across all folds.</p>
			<h3 id="_idParaDest-297"><a id="_idTextAnchor300"/>Exercise 90: Performing Model Assessment Using K-Fold Cross-Validation</h3>
			<p>We will leverage the same dataset for the use case as from the previous two exercises and build a sample random forest classification model and evaluate the performance using k-fold cross-validation.</p>
			<p>To perform model assessment using the k-fold cross validation approach, perform the following steps:</p>
			<ol>
				<li value="1">First, import the <strong class="inline">caret</strong> package into the system using the following command:<p class="snippet">library(caret)</p></li>
				<li>Next, set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Now, define a function for five-fold cross validation using the following command:<p class="snippet">train_control = trainControl(method = "cv", number=5, savePredictions = TRUE,verboseIter = TRUE)</p></li>
				<li>Define the value of <strong class="inline">mtry</strong> as <strong class="inline">3</strong> (to match our previous example):<p class="snippet">parameter_values = expand.grid(mtry=3)</p></li>
				<li>Fit the model using the following command:<p class="snippet">model_rf_kfold&lt;- train(diabetes~., data=df, trControl=train_control, </p><p class="snippet">                method="rf",  metric= "Accuracy", </p><p class="snippet">tuneGrid = parameter_values)</p></li>
				<li>Next, print overall accuracy (averaged across all folds):<p class="snippet">model_rf_kfold$results[2]</p></li>
				<li>Now, print the detailed prediction dataset using the following command:<p class="snippet">print("Shape of Prediction Dataset")</p><p class="snippet">print(dim(model_rf_kfold$pred))</p><p class="snippet">print("Prediction detailed results - ")</p><p class="snippet">head(model_rf_kfold$pred) #print first 6 rows</p><p class="snippet">tail(model_rf_kfold$pred) #print last 6 rows</p><p class="snippet">print("Accuracy across each Fold-")</p><p class="snippet">model_rf_kfold$resample</p><p class="snippet">print(paste("Average Accuracy :",mean(model_rf_kfold$resample$Accuracy)))</p><p class="snippet">print(paste("Std. Dev Accuracy :",sd(model_rf_kfold$resample$Accuracy)))</p><p>The output is as follows:</p><p class="snippet">+ Fold1: mtry=3 </p><p class="snippet">- Fold1: mtry=3 </p><p class="snippet">+ Fold2: mtry=3 </p><p class="snippet">- Fold2: mtry=3 </p><p class="snippet">+ Fold3: mtry=3 </p><p class="snippet">- Fold3: mtry=3 </p><p class="snippet">+ Fold4: mtry=3 </p><p class="snippet">- Fold4: mtry=3 </p><p class="snippet">+ Fold5: mtry=3 </p><p class="snippet">- Fold5: mtry=3 </p><p class="snippet">...</p><p class="snippet">Accuracy: 0.7590782</p><p class="snippet">"Shape of Prediction Dataset"</p><p class="snippet">768   5</p><p class="snippet">"Prediction detailed results - "</p><p class="snippet">...</p><p class="snippet">"Average Accuracy : 0.759078176725236"</p><p class="snippet">"Std. Dev Accuracy : 0.0225461480724459"</p></li>
			</ol>
			<p>As we can see, the overall accuracy has dropped a bit to <strong class="inline">76% (rounded off from 75.9)</strong>. This is the average accuracy from each fold. We have also manually calculated the mean and standard deviation of accuracy from each fold toward the end. The standard deviation for accuracy across each fold is <strong class="inline">2%</strong>, which is considerably low and therefore, we can conclude there is low variance. The overall accuracy is not low, so the model has a moderately low bias. There is scope for improvement in the overall performance, but our model is neither overfitting nor underfitting at the moment.</p>
			<p>If you observe the code, we used the <strong class="inline">trainControl</strong> function that provides us with the necessary constructs to define the type of cross-validation with the <strong class="inline">cv</strong> method, and the number of folds as equal to <strong class="inline">5</strong>.</p>
			<p>We use an additional construct to indicate the need to save the prediction, which we can later analyze in detail. The <strong class="inline">trainControl</strong> object is then passed to the <strong class="inline">train</strong> function in the <strong class="inline">caret</strong> package, where we also define the type of algorithm to be used as random forest with <strong class="inline">rf</strong> method, and the metric as <strong class="bold">Accuracy</strong>. The <strong class="inline">tuneGrid</strong> construct was ideally not necessary at this point; it is used for hyperparameter tuning that we will cover later. However, the <strong class="inline">train</strong> function in the <strong class="inline">caret</strong> package, by default, simplifies the function by using hyperparameter tuning. It tries different values of <strong class="inline">mtry</strong> in various iterations and returns the final prediction with the best value. In order to make apples to apple comparison with the previous example, we had to restrict the value of <strong class="inline">mtry</strong> to <strong class="inline">3</strong>. We, therefore, used the <strong class="inline">expand.grid</strong> object to define the value of <strong class="inline">mtry</strong> to be used in the cross-validation process.</p>
			<p>The train function, when supplied with the <strong class="inline">trainControl</strong> object defined for cross-validation, divides the data into five partitions and leverages four partitions for training and one for testing. The process is repeated five times (<em class="italics">k</em> is set to <strong class="inline">5</strong>) and the model is tested on each partition in the dataset iteratively.</p>
			<p>We can see the detailed results in the <strong class="inline">pred</strong> object (data frame) in the model results. Here, we can see the observed (actual) and predicted value of the data on each row. It additionally also annotates the value of the hyperparameter used in the fold, and the fold number it was a part of for testing.</p>
			<p>The <strong class="inline">resample</strong> DataFrame in the <strong class="inline">model</strong> object records the accuracy and additional metrics across each fold in cross-validation. We can explore the average and standard deviation of the metrics of our interest to study bias and variance.</p>
			<p>The final take-away from the <em class="italics">k</em>-fold cross validation is that the accuracy of the random forest model for the use case is 76% (that is, the average accuracy across all partitions).</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor301"/>Hold-One-Out Validation</h2>
			<p>In this technique, we take the <em class="italics">k</em>-fold validation to the logical extreme. Instead of creating <em class="italics">k</em>-partitions where, <em class="italics">k</em> would be 5 or 10, we choose the number of partitions as the number of available data points. Therefore, we would have only one sample in a partition. We use all the samples except one for training, and test the model on the sample which was held out and repeat this <strong class="inline">n</strong> number of times, where <strong class="inline">n</strong> is the number of training samples. Finally, the average error akin to <em class="italics">k</em>-fold validation is computed. The major drawback of this technique is that the model is trained <em class="italics">n</em> number of times, making it computationally expensive. If we are dealing with a fairly large data sample, this validation method is best avoided.</p>
			<p>Hold-one-out validation is also called <strong class="bold">L</strong>eave-<strong class="bold">O</strong>ne-<strong class="bold">O</strong>ut <strong class="bold">C</strong>ross-<strong class="bold">V</strong>alidation (LOOCV). The following visual demonstrates hold-one-out validation for <em class="italics">n</em> samples:</p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="image/C12624_07_06.jpg" alt="Figure 7.6: Hold-one-out validation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.6: Hold-one-out validation</h6>
			<p>The following exercise performs <strong class="bold">hold-one-out</strong> or leave-one-out cross-validation on the same dataset using random forest with the same experimental setup.</p>
			<h3 id="_idParaDest-299"><a id="_idTextAnchor302"/>Exercise 91: Performing Model Assessment Using Hold-One-Out Validation</h3>
			<p>Similar to <em class="italics">Exercise 2</em>: <em class="italics">Performing Model Assessment using Holdout Validation</em> and <em class="italics">Exercise 3</em>: <em class="italics">Performing Model Assessment using K-Fold Cross Validation</em>, we will continue to leverage the same dataset and perform hold-one-out validation to assess model performance.</p>
			<p>To perform the model assessment using the hold-one-out validation approach, perform the following steps:</p>
			<ol>
				<li value="1">First, define function for hold-one-out validation using the following command:<p class="snippet">set.seed(2019)</p><p class="snippet">train_control = trainControl(method = "LOOCV", savePredictions = TRUE)</p></li>
				<li>Next, define the value of <strong class="inline">mtry</strong> equals <strong class="inline">3</strong> (to match our previous example):<p class="snippet">parameter_values = expand.grid(mtry=3)</p></li>
				<li>Fit the model:<p class="snippet">model_rf_LOOCV&lt;- train(diabetes~., data=df, trControl=train_control, </p><p class="snippet">                    method="rf",  metric= "Accuracy", </p><p class="snippet">tuneGrid = parameter_values)</p></li>
				<li>Now, print the overall accuracy (averaged across all folds):<p class="snippet">print(model_rf_LOOCV$results[2])</p></li>
				<li>Print the detailed prediction dataset using the following commands:<p class="snippet">print("Shape of Prediction Dataset")</p><p class="snippet">print(dim(model_rf_LOOCV$pred))</p><p class="snippet">print("Prediction detailed results - ")</p><p class="snippet">head(model_rf_LOOCV$pred) #print first 6 rows</p><p class="snippet">tail(model_rf_LOOCV$pred) #print last 6 rows</p><p>The output is as follows:</p><p class="snippet">Accuracy</p><p class="snippet">1 0.7721354</p><p class="snippet">[1] "Shape of Prediction Dataset"</p><p class="snippet">[1] 768   4</p><p class="snippet">[1] "Prediction detailed results - "</p><p class="snippet">"Shape of Prediction Dataset"</p><p class="snippet"> 768   4</p><p class="snippet">"Prediction detailed results - "</p><p class="snippet">...</p></li>
			</ol>
			<p>As we can see, the overall accuracy at <strong class="inline">77%</strong> is almost the same as <em class="italics">K</em> fold cross-validation (a marginal increase of <strong class="inline">1%</strong>). The <strong class="bold">LOOCV</strong> construct here stands for <strong class="bold">Leave-One-Out Cross-Validation</strong>. The process is computationally expensive as it iterates the training process for as many times as there are data points (in this case, 768).</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor303"/>Hyperparameter Optimization</h2>
			<p><strong class="bold">Hyperparameter optimization</strong> is the process of optimizing or finding the most optimal set of hyperparameters for a machine learning model. A hyperparameter is a parameter that defines the macro characteristics for a machine learning model. It is basically a metaparameter for the model. Hyperparameters are different from model parameters; model parameters are learned by the model during the learning process, however, hyperparameters are set by the data scientist designing the model and cannot be learned by the model.</p>
			<p>To understand the concept more intuitively, let's explore the topic in layman terms. Consider the example of a decision tree model. The tree structure with the root node, decision nodes, and leaf nodes are (akin to the beta coefficients in logistic regression) are learned through training (fitting) of data. When the model finally converges (finds the optimal set of values for model parameters), we have the final tree structure that defines the traversal path for the end prediction. The macro characteristic for the model is however something different; in the case of decision tree, it would be the complexity parameter, denoted by <strong class="inline">cp</strong>. The complexity parameter, <strong class="inline">cp</strong>, restricts the growth of the tree with respect to depth; that is, doesn't allow branching of a node if the information gain or any other associated metric doesn't yield above a threshold. Applying this new rule restricts the depth of the tree beyond a point and helps in generalizing the tree better. The complexity parameter is therefore a parameter that defines a macro characteristic for the model that then tailors the book of the training process, which we call a hyperparameter.</p>
			<p>Every machine learning algorithm will have a different set of hyperparameters associated that will help the model ignore errors (noise), and therefore improve generalizing capabilities. A few examples of hyperparameters in machine learning algorithms are illustrated in the following table:</p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/C12624_07_07.jpg" alt="Figure 7.7: Hyperparameters in machine learning algorithms&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.7: Hyperparameters in machine learning algorithms</h6>
			<h4>Note</h4>
			<p class="callout">The number of hyperparameters is sometimes different in the implementations offered in R and Python. For example, the logistic regression implementation in R with the <strong class="inline">caret</strong> package doesn't tune the <strong class="inline">c</strong> parameter unlike the Python implementation in <strong class="inline">sklearn</strong>. Similarly, random forest implementation in <strong class="inline">sklearn</strong> of Python allows the use-of-depth of a tree as a hyperparameter.</p>
			<p class="callout">Information on gradient-based hyperparameter optimization can be found at the following links:</p>
			<p class="callout">https://arxiv.org/abs/1502.03492</p>
			<p class="callout">http://proceedings.mlr.press/v37/maclaurin15.pdf</p>
			<p>The process of hyperparameter tuning can be summarized as the iterative process of finding the optimal set of values for hyperparameters that results in the best machine learning model for our task of prediction. There are several approaches that can be taken to achieve this. With the fact that this process is iterative, we can affirm that there would be several approaches to optimize the path used to find the optimal set of values. Let's discuss in depth the broad strategies that can adopted for hyperparameter tuning.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor304"/>Grid Search Optimization</h2>
			<p>The most naïve approach to find the optimal set of hyperparameters for a model would be to use <strong class="bold">brute-force</strong> methods and iterate with every combination of values for the hyperparameters and then find the most optimal combination. This will deliver the desired results, but not in the desired time. In most cases, the models we train will be significantly large and require heavy compute time for training. Iterating through each combination wouldn't be an ideal option. To improve upon the brute-force method, we have grid search optimization; as the name has already indicated, here, we define a grid of values that will be used for an exhaustive combination of values of hyperparameters to iterate.</p>
			<p>In layman's terms, for grid search optimization, we define a finite set of values for each hyperparameter that we would be interested in optimizing for the model. The model is then trained for exhaustive combinations of all possible hyperparameter values and the combination with the best performance is selected as the optimal set.</p>
			<p>The following diagram demonstrates the idea of grid search optimization for a hypothetical set of parameters. Using the hyperparameter grid, the combinations are defined, and the model is trained for each combination:</p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="image/C12624_07_08.jpg" alt="Figure 7.8: The hyperparameter grid and combinations&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.8: The hyperparameter grid and combinations</h6>
			<p>The advantage of grid search optimization is that it heavily reduces the time required to find the optimal set of hyperparameters given the limited set of candidate values to iterate upon (when compared to brute force). However, this comes with a trade-off. The grid search optimization model assumes that the optimal value of hyperparameter resides within the provided list of candidate values for each hyperparameter. If we don't provide the best value as a candidate value in the list (grid), we will never have the optimal set of values for the algorithm. Therefore, we would need to explore some suggestions for most recommended list of values for each hyperparameter before finalizing the list of candidate values. Hyperparameter optimization works best for experienced data science professionals, who have strong judgement for a variety of different machine learning problems.</p>
			<p>We define hyperparameters for machine learning models that tailor the book of learning (fitting) for the models.</p>
			<h3 id="_idParaDest-302"><a id="_idTextAnchor305"/>Exercise 92: Performing Grid Search Optimization – Random Forest</h3>
			<p>In this exercise, we will perform grid search optimization for the model using the <strong class="inline">caret</strong> package, where we define a grid of the values that we want to test and evaluate for the best model. We will use the random forest algorithm on the same dataset as was used in the previous topic.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">First, set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Next, define the cross-validation method using the following command:<p class="snippet">train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)</p></li>
				<li>Now, define <strong class="inline">parameter_grid</strong> as illustrated here:<p class="snippet">parameter_grid = expand.grid(mtry=c(1,2,3,4,5,6))</p></li>
				<li>Fit the model with cross-validation and grid search optimization:<p class="snippet">model_rf_gridSearch&lt;- train(diabetes~., data=df, trControl=train_control, </p><p class="snippet">               method="rf",  metric= "Accuracy", </p><p class="snippet">tuneGrid = parameter_grid)</p></li>
				<li>Print the overall accuracy (averaged across all folds for each hyperparameter combination):<p class="snippet">print("Accuracy across hyperparameter Combinations:")</p><p class="snippet">print(model_rf_gridSearch$results[,1:2])</p><p>The output is as follows:</p><p class="snippet">[1] "Accuracy across hyperparameter Combinations:"</p><p class="snippet">mtry  Accuracy</p><p class="snippet">1    1 0.7564893</p><p class="snippet">2    2 0.7604108</p><p class="snippet">3    3 0.7642730</p><p class="snippet">4    4 0.7668704</p><p class="snippet">5    5 0.7629658</p><p class="snippet">6    6 0.7590697</p></li>
				<li>Print the detailed prediction dataset:<p class="snippet">print("Shape of Prediction Dataset")</p><p class="snippet">print(dim(model_rf_gridSearch$pred))</p><p class="snippet">[1] "Shape of Prediction Dataset"</p><p class="snippet">[1] 4608    5</p><p class="snippet">print("Prediction detailed results - ")</p><p class="snippet">print(head(model_rf_gridSearch$pred)) #print the first 6 rows</p><p class="snippet">print(tail(model_rf_gridSearch$pred)) #print the last 6 rows</p><p class="snippet">[1] "Prediction detailed results - "</p><p class="snippet">predobsrowIndexmtry Resample</p><p class="snippet">1  neg pos       10    1    Fold1</p><p class="snippet">2  neg pos       24    1    Fold1</p><p class="snippet">3  neg neg       34    1    Fold1</p><p class="snippet">4  neg pos       39    1    Fold1</p><p class="snippet">5  neg neg       43    1    Fold1</p><p class="snippet">6  neg neg       48    1    Fold1</p><p class="snippet">predobsrowIndexmtry Resample</p><p class="snippet">4603  neg neg      752    6    Fold5</p><p class="snippet">4604  neg neg      753    6    Fold5</p><p class="snippet">4605  pos pos      755    6    Fold5</p><p class="snippet">4606  neg neg      759    6    Fold5</p><p class="snippet">4607  neg neg      761    6    Fold5</p><p class="snippet">4608  pos pos      762    6    Fold5</p><p class="snippet">print("Best value for Hyperparameter 'mtry':")</p><p class="snippet">print(model_rf_gridSearch$bestTune)</p><p class="snippet">[1] "Best value for Hyperparameter 'mtry':"</p><p class="snippet">mtry</p><p class="snippet">4    4</p><p class="snippet">print("Final (Best) Model ")</p><p class="snippet">print(model_rf_gridSearch$finalModel)</p><p class="snippet">[1] "Final (Best) Model "</p><p class="snippet">Call:</p><p class="snippet">randomForest(x = x, y = y, mtry = param$mtry) </p><p class="snippet">               Type of random forest: classification</p><p class="snippet">                     Number of trees: 500</p><p class="snippet">No. of variables tried at each split: 4</p><p class="snippet">OOB estimate of  error rate: 23.7%</p><p class="snippet">Confusion matrix:</p><p class="snippet">    neg pos class.error</p><p class="snippet">neg 423  77    0.154000</p><p class="snippet">pos 105 163    0.391791</p></li>
				<li>Plot the grid metrics:<p class="snippet">library(repr)</p><p class="snippet">options(repr.plot.width=8, repr.plot.height=5)</p><p class="snippet">plot(model_rf_gridSearch)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/C12624_07_09.jpg" alt="Figure 7.9: The accuracy of the random forest model across various values of hyperparameter&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.9: The accuracy of the random forest model across various values of hyperparameter</h6>
			<p>As we can see, the best results in terms of accuracy were delivered using the <strong class="inline">mtry</strong> hyperparameter with the value of 4. The highlighted portion of the output will help you in understanding the overall takeaway process. We used a 5-fold cross-validation along with grid search optimization, where we defined a grid for the <strong class="inline">mtry</strong> hyperparameter with the values of (1,2,3,4,5, and 6). The accuracy from each value is also shown, and we can see that the results from <strong class="inline">mtry</strong> equal to <strong class="inline">4</strong> are a notch higher than the others. Lastly, we also printed the final model that was returned by the grid search optimization process.</p>
			<p>So far, we have only looked at random forest as a model to implement cross-validation and hyperparameter tuning. We can extend this to any other algorithm. Algorithms such as <strong class="bold">XGBoost</strong> have many more hyperparameters than random forest (with the R implementation), and therefore make the overall process a little more computationally expensive, as well as complicated. In the following exercise, we perform 5-fold cross validation, as well as grid search optimization for XGBoost, on the same dataset. The highlighted parts of the code are the changes for XGBoost.</p>
			<h4>Automated hyperparameter tuning:</h4>
			<p class="callout">https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a</p>
			<h3 id="_idParaDest-303"><a id="_idTextAnchor306"/>Exercise 93: Grid Search Optimization – XGBoost</h3>
			<p>Similarly to the previous exercise, we will perform grid search optimization on the XGBoost model, instead of random forest, and on a larger set of hyperparameters to find the best model.</p>
			<p>To perform grid search optimization on the XGBoost model, perform the following steps:</p>
			<ol>
				<li value="1">First, set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Next, import the <strong class="inline">dplyr</strong> library using the following command:<p class="snippet">library(dplyr)</p></li>
				<li>Define the cross-validation method using the following command:<p class="snippet">train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)</p></li>
				<li>Next, define the parameter grid as illustrated here:<p class="snippet">parameter_grid = expand.grid(nrounds = c(30,50,60,100),</p><p class="snippet">                             eta=c(0.01,0.1,0.2,0.3),</p><p class="snippet">max_depth = c(2,3,4,5),</p><p class="snippet">                             gamma = c(1),</p><p class="snippet">colsample_bytree = c(0.7),</p><p class="snippet">min_child_weight = c(1)  ,</p><p class="snippet">                             subsample = c(0.6)</p><p class="snippet">                            )</p></li>
				<li>Fit the model with cross-validation and grid search optimization:<p class="snippet">model_xgb_gridSearch&lt;- train(diabetes~., data=df, trControl=train_control, </p><p class="snippet">               method="xgbTree",  metric= "Accuracy",</p><p class="snippet">tuneGrid = parameter_grid)</p></li>
				<li>Print the detailed prediction dataset:<p class="snippet">print("Shape of Prediction Dataset")</p><p class="snippet">print(dim(model_xgb_gridSearch$pred))</p><p class="snippet">"Shape of Prediction Dataset"</p><p class="snippet">  49152    11</p><p class="snippet">print("Prediction detailed results - ")</p><p class="snippet">head(model_xgb_gridSearch$pred) #print the first 6 rows</p><p class="snippet">tail(model_xgb_gridSearch$pred) #print the last 6 rows</p><p class="snippet">[1] "Prediction detailed results - "</p><p class="snippet">predobsrowIndex  eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds Resample</p><p class="snippet">1  pos pos        3 0.01         2     1              0.7                1       0.6     100    Fold1</p><p class="snippet">2  neg neg        6 0.01         2     1              0.7                1       0.6     100    Fold1</p><p class="snippet">3  neg pos       20 0.01         2     1              0.7                1       0.6     100    Fold1</p><p class="snippet">4  pos pos       23 0.01         2     1              0.7                1       0.6     100    Fold1</p><p class="snippet">5  pos pos       25 0.01         2     1              0.7                1       0.6     100    Fold1</p><p class="snippet">6  pos pos       27 0.01         2     1              0.7                1       0.6     100    Fold1</p><p class="snippet">predobsrowIndex eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds Resample</p><p class="snippet">49147  neg pos      732 0.3         5     1              0.7                1       0.6      60    Fold5</p><p class="snippet">49148  neg pos      740 0.3         5     1              0.7                1       0.6      60    Fold5</p><p class="snippet">49149  neg neg      743 0.3         5     1              0.7                1       0.6      60    Fold5</p><p class="snippet">49150  pos pos      749 0.3         5     1              0.7                1       0.6      60    Fold5</p><p class="snippet">49151  neg pos      751 0.3         5     1              0.7                1       0.6      60    Fold5</p><p class="snippet">49152  neg neg      763 0.3         5     1              0.7                1       0.6      60    Fold5</p><p class="snippet">print("Best values for all selected Hyperparameters:")</p><p class="snippet">model_xgb_gridSearch$bestTune</p><p class="snippet">[1] "Best values for all selected Hyperparameters:"</p><p class="snippet">nroundsmax_depth eta gamma colsample_bytreemin_child_weight subsample</p><p class="snippet">27      60         4 0.1     1              0.7                1       0.6</p></li>
				<li>Print the overall accuracy (averaged across all folds for each hyperparameter combination):<p class="snippet">print("Average results across different combination of Hyperparameter Values")</p><p class="snippet">model_xgb_gridSearch$results %&gt;% arrange(desc(Accuracy)) %&gt;% head(5)</p><p>The output is as follows:</p><p class="snippet">[1] "Average results across different combination of Hyperparameter Values"</p><p class="snippet">   eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds  Accuracy     Kappa AccuracySD</p><p class="snippet">1 0.10         4     1              0.7                1       0.6      60 0.7695612 0.4790457 0.02507631</p><p class="snippet">2 0.01         3     1              0.7                1       0.6      30 0.7695442 0.4509049 0.02166056</p><p class="snippet">3 0.01         2     1              0.7                1       0.6     100 0.7695187 0.4521142 0.03373126</p><p class="snippet">4 0.30         2     1              0.7                1       0.6      30 0.7682540 0.4782334 0.01943638</p><p class="snippet">5 0.01         5     1              0.7                1       0.6      30 0.7682455 0.4592689 0.02836553</p><p class="snippet">KappaSD</p><p class="snippet">1 0.05067601</p><p class="snippet">2 0.05587205</p><p class="snippet">3 0.08038248</p><p class="snippet">4 0.04249313</p><p class="snippet">5 0.06049950</p></li>
				<li>Plot the graph:<p class="snippet">plot(model_xgb_gridSearch)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/C12624_07_10.jpg" alt="Figure 7.10: Visualizing the accuracy across various hyperparameter values for the XGBoost model&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.10: Visualizing the accuracy across various hyperparameter values for the XGBoost model</h6>
			<p>The output from the exercise might seem quite lengthy, but let's quickly summarize the results. Since we are performing cross-validation and hyperparameter tuning on XGBoost, we would need to provide a grid for a larger number of hyperparameters. The first line in the output indicates that the size of the prediction dataset is 49152 x 11. This indicates the exhaustive predictions from each combination of hyperparameters across each fold in cross-validation. We have printed the head and tail of the prediction dataset (the first and last six rows of data), and we can see the predicted outcome for each instance of the model with the associated hyperparameters, as well as the corresponding fold.</p>
			<p>The next table shows us the best set of values for hyperparameters based on the accuracy of the model. We can see that the values of <strong class="inline">nrounds=60</strong>, <strong class="inline">max_depth=3</strong>, <strong class="inline">eta=0.01</strong>, <strong class="inline">gamma=1</strong>, <strong class="inline">colsample_bytree=0.7</strong>, <strong class="inline">min_child_weight=1</strong>, and <strong class="inline">subsample=0.6</strong> returned the best performance for the model.</p>
			<p>The next table displays the corresponding accuracy for each combination of hyperparameters in the descending order of performance. The best accuracy is in the first line of the table that was achieved using the best set of hyperparameters. We achieved an accuracy of <strong class="inline">76.8%</strong>.</p>
			<p>Lastly, we plotted the results across hyperparameters. Given the larger number of hyperparameters, we have a denser plot showcasing the results. However, we can directly check the results for the quadrant with <strong class="inline">eta=0.01</strong>, and study the variation for max depth and <strong class="inline">nrounds</strong> and conclude the best performance is from the same combination of hyperparameters.</p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor307"/>Random Search Optimization</h2>
			<p>In random search optimization, we overcome one of the disadvantages of grid search optimization, which is choosing the best set of optimal values within the candidate values for each hyperparameter in the grid. Here, we opt for random choices from a distribution (in case of a continuous value for hyperparameters), instead of a static list that we would define. In random search optimization, we have a wider gamut of options to search from, as the continuous values for a hyperparameter are chosen randomly from a distribution. This increases the chances of finding the best value for a hyperparameter to a great extent.</p>
			<p>Some of us might have already started understanding how random choices can always have the possibility of incorporating the best values for a hyperparameter. The true answer is that it doesn't always have an absolute advantage over grid search, but with a fairly large number of iterations, the chances of finding a more optimal set of hyperparameter increases with random search over grid search. There might be instances where random search would return less optimal values for hyperparameter tuning over grid search, given the random selection of values, however, most data science professionals have empirical validations of the fact that with a fairly decent number of iterations, random search trumps over grid search for most cases.</p>
			<p>Implementation of random search optimization is simplified in the <strong class="inline">caret</strong> package. We have to define a parameter called <strong class="inline">tuneLength</strong>, which will set a maximum cap on the number of iterations for random search. The number of iterations would be equivalent to the number of times the model will be trained, and therefore the higher the number is, the higher the chances of getting the best set of hyperparameters and the associated performance boost. However, the higher the number of iterations, the higher the compute time required to execute.</p>
			<p>In the following exercise, let's perform random search optimization on the random forest algorithm for the same dataset.</p>
			<h3 id="_idParaDest-305"><a id="_idTextAnchor308"/>Exercise 94: Using Random Search Optimization on a Random Forest Model</h3>
			<p>We will extend the optimization process for machine learning models with random search optimization. Here, we only define the number of iterations that we would like to perform with random combinations of hyperparameter values for the model.</p>
			<p>The aim of this exercise is to perform random search optimization on a random forest model.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">First, set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Define the cross-validation method as illustrated here:<p class="snippet">train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)</p></li>
				<li>Fit the model with cross-validation and random search optimization:<p class="snippet">model_rf_randomSearch&lt;- train(diabetes~., data=df, trControl=train_control, </p><p class="snippet">                        method="rf",  metric= "Accuracy",tuneLength = 15)</p></li>
				<li>Print the detailed prediction dataset:<p class="snippet">print("Shape of Prediction Dataset")</p><p class="snippet">print(dim(model_rf_randomSearch$pred))</p><p class="snippet">[1] "Shape of Prediction Dataset"</p><p class="snippet">[1] 5376    5</p><p class="snippet">print("Prediction detailed results - ")</p><p class="snippet">head(model_rf_randomSearch$pred) #print the first 6 rows</p><p class="snippet">tail(model_rf_randomSearch$pred) #print the last 6 rows</p><p class="snippet">[1] "Prediction detailed results - "</p><p class="snippet">predobsrowIndexmtry Resample</p><p class="snippet">1  pos pos        1    2    Fold1</p><p class="snippet">2  neg neg        4    2    Fold1</p><p class="snippet">3  pos pos        9    2    Fold1</p><p class="snippet">4  neg pos       10    2    Fold1</p><p class="snippet">5  neg neg       13    2    Fold1</p><p class="snippet">6  pos pos       17    2    Fold1</p><p class="snippet">predobsrowIndexmtry Resample</p><p class="snippet">5371  neg neg      737    8    Fold5</p><p class="snippet">5372  neg neg      742    8    Fold5</p><p class="snippet">5373  neg neg      743    8    Fold5</p><p class="snippet">5374  neg pos      758    8    Fold5</p><p class="snippet">5375  neg neg      759    8    Fold5</p><p class="snippet">5376  neg neg      765    8    Fold5</p><p class="snippet">print("Best values for all selected Hyperparameters:")</p><p class="snippet">model_rf_randomSearch$bestTune</p><p class="snippet">[1] "Best values for all selected Hyperparameters:"</p><p class="snippet">mtry</p><p class="snippet">7    8</p></li>
				<li>Print the overall accuracy (averaged across all folds for each hyperparameter combination):<p class="snippet">model_rf_randomSearch$results %&gt;% arrange(desc(Accuracy)) %&gt;% head(5)</p><p>The output is as follows:</p><p class="snippet">mtry  Accuracy     Kappa AccuracySDKappaSD</p><p class="snippet">1    8 0.7838299 0.5190606 0.02262610 0.03707616</p><p class="snippet">2    7 0.7773194 0.5047353 0.02263485 0.03760842</p><p class="snippet">3    3 0.7760037 0.4945296 0.02629540 0.05803215</p><p class="snippet">4    6 0.7734063 0.4964970 0.02451711 0.04409090</p><p class="snippet">5    5 0.7720907 0.4895592 0.02618707 0.04796626</p></li>
				<li>Plot the data for the random search optimization of the random forest model:<p class="snippet">plot(model_rf_randomSearch)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/C12624_07_11.jpg" alt="Figure 7.11: Visualizing accuracy across values of hyperparameters&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.11: Visualizing accuracy across values of hyperparameters</h6>
			<p>We set the <strong class="inline">tuneLength</strong> parameter to <strong class="inline">15</strong>; however, since random forest in R only focuses on hyperparameter tuning for 1 parameter, that is, <strong class="inline">mtry</strong>, the number of iterations is exhausted at <strong class="inline">7</strong>. This is because we have only eight independent variables in the dataset. In most general cases, it would be advisable to set a higher number based on the number of features in the data. We can see the best value for <strong class="inline">mtry</strong> was found at 7. The plot showcases the differences between various values of <strong class="inline">mtry</strong>. The best accuracy we achieved with this model was 76%.</p>
			<p>Let's now try the same experiment with XGBoost. Here, we will set <strong class="inline">tuneLength</strong> to <strong class="inline">35</strong>, which will be computationally expensive, that is, <em class="italics">15 x 5 (folds) = 75</em> model iterations. This would take significantly longer to execute than any of the previous iterations. If you want to see the results faster, you might have to reduce the number of iterations with <strong class="inline">tuneLength</strong>.</p>
			<h3 id="_idParaDest-306"><a id="_idTextAnchor309"/>Exercise 95: Random Search Optimization – XGBoost</h3>
			<p>As with random forest, we will perform random search optimization on the XGBoost model. The XGBoost model has a larger number of hyperparameters to tune, and therefore is more suitable for random search optimization. We will leverage the same dataset as in previous exercises to build the XGBoost model and then perform optimization.</p>
			<p>The aim of this exercise is to perform random search optimization on the XGBoost model.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Define the cross-validation method using the following command:<p class="snippet">train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)</p></li>
				<li>Fit the model with cross-validation and random search optimization:<p class="snippet">model_xgb_randomSearch&lt;- train(diabetes~., data=df, trControl=train_control, </p><p class="snippet">                               method="xgbTree", metric= "Accuracy",tuneLength = 15)</p></li>
				<li>Print the detailed prediction dataset:<p class="snippet">print("Shape of Prediction Dataset")</p><p class="snippet">print(dim(model_xgb_randomSearch$pred))</p><p class="snippet">print("Prediction detailed results - ")</p><p class="snippet">head(model_xgb_randomSearch$pred) #print the first 6 rows</p><p class="snippet">tail(model_xgb_randomSearch$pred) #print the last 6 rows</p><p class="snippet">print("Best values for all selected Hyperparameters:")</p><p class="snippet">model_xgb_randomSearch$bestTune</p></li>
				<li>Print the overall accuracy (averaged across all folds for each hyperparameter combination):<p class="snippet">model_xgb_randomSearch$results %&gt;% arrange(desc(Accuracy)) %&gt;% head(5)</p><p>The output is as follows:</p><p class="snippet">"Shape of Prediction Dataset"</p><p class="snippet">10368000       11</p><p class="snippet">"Prediction detailed results - "</p></li>
			</ol>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/C12624_07_12.jpg" alt="Figure 7.12: Detailed prediction results&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.12: Detailed prediction results</h6>
			<p>Using <em class="italics">random search optimization</em>, we can see a different set of parameters selected as the optimal combination for the XGBoost model. Here, notice that the accuracy of grid search and random search are the same (the differences are marginal), however, the parameter values are completely different. The learning rate (<strong class="inline">eta</strong>) is 0.4, <strong class="inline">max_depth</strong> is 1 instead of 3, <strong class="inline">colsample_byTree</strong> is 0.8 instead of 0.7, and <strong class="inline">nrounds</strong> is 50 instead of 60. We have not passed any of these values as candidate values for grid search. In random search, given the wider gamut of options to select from, we may have promising results when compared to grid search. This, however, comes at a cost of <em class="italics">higher computation time</em>.</p>
			<p>Also, the current dataset used is a small one (~800 samples). As the use case becomes a more compelling one (with more features and more data), the performance difference between random search and grid search might be wider.</p>
			<p>As a rule of thumb, it is highly recommended to opt for random search over grid search, especially in cases where our judgement about the problem space is insignificant.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor310"/>Bayesian Optimization</h2>
			<p>One of the major trade-offs within grid search and random search is that both techniques do not keep track of the past evaluations of hyperparameter combinations used for the model training. Ideally, if there was some artificial intelligence were induced in this path that could indicate the process with the historic performance on the selected list of hyperparameters and a mechanism to improve performance by advancing iterations in the right direction, it would drastically reduce the number of iterations required to find the optimal set of values for the hyperparameters. Grid search and random search, however, miss on this front and iterate through all provided combinations without considering any cues from previous iterations.</p>
			<p>With <strong class="bold">Bayesian optimization</strong>, we overcome this trade-off by enabling the tuning process to keep track of previous iterations and their evaluation by developing a probabilistic model that would map the hyperparameters to a probability score of the selected loss function (an objective function) for the machine learning model. This probabilistic model is also called a <strong class="bold">surrogate model</strong> to the primary loss function in the machine learning model and in contrast is far easier to optimize than the loss function.</p>
			<p>Discussing the mathematical context and derivations for the process will be beyond the scope of the chapter. The overall process in Bayesian optimization for hyperparameter tuning can be simplified as follows:</p>
			<ul>
				<li>Define a surrogate model (a probabilistic mapping of hyperparameters to the loss function).</li>
				<li>Find the optimal parameters for the surrogate model.</li>
				<li>Apply the parameters on primary loss function and update the model in the right direction based on results.</li>
				<li>Repeat until defined performance of iterations has been reached.</li>
			</ul>
			<p>Using this simplistic framework, Bayesian optimization has in most cases delivered the ideal set of hyperparameters with the least number of iterations. As this approach keeps track of the past iterations and associated performance, its practices being more accurate with increasing amount of data. Bayesian methods are efficient and more practical to use as they operate in many ways like the human brain; for any task to be performed, we try understanding the initial view of the world, and then we improve our understanding based on new experiences. Bayesian hyperparameter optimization leverages the same rationale and enables an optimized path in tuning the hyperparameters for a model using informed decisions. A paper by <em class="italics">Bergstra et al</em>. (http://proceedings.mlr.press/v28/bergstra13.pdf) neatly explains the advantages of Bayesian optimization over random search optimization.</p>
			<p>There are several techniques within Bayesian optimization that can be applied to hyperparameter tuning in the machine learning domain. A popular formalization of the Bayesian approach is <strong class="keyword">Sequential Model-Based Optimization</strong> (<strong class="keyword">SMBO</strong>), which again has several variations within. Each approach within SMBO varies based on the way the surrogate model is defined and the criteria used to evaluate and update parameters. A few popular choices for the surrogate model are <strong class="bold">Gaussian Processes</strong>, <strong class="bold">Random Forest Regressions</strong>, and <strong class="bold">Tree Parzen Estimators</strong> (<strong class="bold">TPE</strong>). Similarly, the criteria based on each successive iteration is evaluated in the optimization process and leveraged using <strong class="bold">UCB</strong>, that is, GP <strong class="bold">Upper Confidence Bound</strong>, for example, <strong class="bold">Expected Improvement</strong> (EI) or <strong class="bold">Probability of Improvement</strong> (<strong class="bold">POI</strong>).</p>
			<p>To implement Bayesian optimization in R, we already have a handful of well-written libraries that abstract the entire process for us. <strong class="inline">MlBayesOpt</strong> is a popular package implemented by Yuya Matsumura in R. It is based on the <strong class="bold">Gaussian Process</strong> in SMBO for Bayesian optimization and allows the use of several functions to update the surrogate model.</p>
			<p>The following exercise performs Bayesian optimization on the random forest model for the <strong class="inline">mtry</strong> and minimum node size hyperparameters. The output of the optimization process returns the best combination evaluated for each hyperparameter. We would then need to implement a regular model using the same combination for hyperparameter values.</p>
			<h4>Bayesian optimization:</h4>
			<p class="callout">https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0</p>
			<p class="callout">https://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf</p>
			<p class="callout">https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f</p>
			<h3 id="_idParaDest-308"><a id="_idTextAnchor311"/>Exercise 96: Performing Bayesian Optimization on the Random Forest Model</h3>
			<p>Perform Bayesian optimization for the same dataset and study the output. In this optimization technique, we will leverage Bayesian optimization to intuitively select the best value for the <strong class="inline">mtry</strong> hyperparameter by iterating with the knowledge/context of previous iterations.</p>
			<p>The aim of this exercise is to perform Bayesian optimization for the random forest model.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">First, set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Import the <strong class="inline">MlBayesOpt</strong> library using the following command:<p class="snippet">library(MlBayesOpt)</p></li>
				<li>Perform Bayesian optimization for random forest model using the <strong class="inline">rf_opt</strong> function from the <strong class="inline">MlBayesOpt</strong> package:<p class="snippet">model_rf_bayesain&lt;- rf_opt(train_data = train,</p><p class="snippet">train_label = diabetes,</p><p class="snippet">test_data = test,</p><p class="snippet">test_label = diabetes,</p><p class="snippet">mtry_range = c(1L, ncol(df)-1),</p><p class="snippet">num_tree = 50,</p><p class="snippet">init_points = 10,</p><p class="snippet">n_iter = 10,                       </p><p class="snippet">acq = "poi", eps = 0, </p><p class="snippet">optkernel = list(type = "exponential", power =2))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="image/C12624_07_13.jpg" alt="Figure 7.13: Output for Bayesian optimization for random forest&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.13: Output for Bayesian optimization for random forest</h6>
			<p>The output displays the iterations computed for model evaluation and returns the result at the end of each iteration. You can increase the number of iterations by increasing the value of <strong class="inline">n_iter</strong>. The <strong class="inline">init_points</strong> variable defines the number of randomly chosen points to sample the target function before Bayesian optimization fits the Gaussian process. Additionally, we define the criteria function as <strong class="bold">POI</strong>. The <strong class="inline">eps</strong> parameter is an additional parameter that can be used to tune <strong class="bold">EI</strong> and <strong class="bold">POI</strong>, to balance exploitation against exploration, increasing epsilon will make the optimized hyperparameters more spread out across the whole range. </p>
			<p>Lastly, we also define the kernel, that is, the correlation function for the underlying Gaussian process. This parameter should be a list that specifies the type of correlation function along with the smoothness parameter. Popular choices are square exponential (the default) or <strong class="inline">Matern 5/2</strong>.</p>
			<p>The result of the Bayesian optimization process returned the best value for <strong class="inline">mtry</strong> as 6.12 (this needs to be truncated to 6, as <strong class="inline">mtry</strong> in <strong class="bold">Random Forest</strong> doesn't accept decimal values) and the best minimum node size as 17. We can use these parameter settings in the regular implementation of random forest and evaluate the model results.</p>
			<p>Let's now use the same approach for the XGBoost model on the same use case.</p>
			<h3 id="_idParaDest-309"><a id="_idTextAnchor312"/>Exercise 97: Performing Bayesian Optimization using XGBoost</h3>
			<p>Similar to the previous exercise, we will perform Bayesian optimization for the same dataset and study the output, albeit this time for the XGBoost model. Given that XGBoost has a larger set of hyperparameters to optimize, we would need to provide a range of values for each hyperparameter of interest to the optimization process.</p>
			<p>To perform Bayesian optimization on the XGBoost model, carry out the following steps:</p>
			<ol>
				<li value="1">First, set the <strong class="inline">seed</strong> as <strong class="inline">2019</strong> using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Perform Bayesian optimization for the XGBoost model using the <strong class="inline">xgb_opt</strong> function from the <strong class="inline">MlBayesOpt</strong> package:<p class="snippet">model_xgb_bayesian&lt;- xgb_opt(train, diabetes, test, diabetes,objectfun ='binary:logistic',evalmetric='logloss',eta_range = c(0.1, 1L), max_depth_range = c(2L, 8L),nrounds_range = c(70, 160L), bytree_range = c(0.4, 1L), init_points = 4, n_iter = 10, acq = "poi", eps = 0, optkernel = list(type = "exponential", power =2))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<img src="image/C12624_07_14.jpg" alt="Figure 7.14: Output for Bayesian Optimization using XGBoost&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.14: Output for Bayesian Optimization using XGBoost</h6>
			<p>Like random forest, the <strong class="inline">xgb_opt</strong> function returns the optimal list of hyperparameters with the candidate values. Unlike <strong class="bold">Random Forest</strong>, we have a larger list of hyperparameters for XGBoost. We need to define the range for which we want the Bayesian optimization process to operate for each hyperparameter.</p>
			<p>We can see that the best combination of hyperparameters from Bayesian optimization is different from what we found for <strong class="bold">Random Search Optimization</strong>. Again, the values of <strong class="inline">nrounds</strong> would need to be truncated (as a decimal value won't make sense) when we implement a standard XGBoost Tree model the parameters evaluate above namely: <strong class="inline">nrounds</strong>, <strong class="inline">max_depth</strong>, <strong class="inline">eta</strong>, and <strong class="inline">subsample</strong>.</p>
			<p>In the chapter, we studied various cross-validation techniques to perform model assessment. A small amendment to the k-fold cross-validation helps us perform an improved validation of model performance. Before moving from k-fold to LOOCV, we can instead perform repeated k-fold to get a more robust evaluation of the model. The process repeats cross-validation multiple times where the folds are split in a different way in each repetition. Performing repeated k-fold is a better approach than LOOCV.</p>
			<h3 id="_idParaDest-310"><a id="_idTextAnchor313"/>Activity 12: Performing Repeated K-Fold Cross Validation and Grid Search Optimization</h3>
			<p>In this activity, we will leverage the same dataset (as used in previous exercises), train a random forest model, perform repeated k-fold validation 10 times, and study the model performance. Within each fold iteration, we can try the different grid values of the hyperparameter and have a robust validation for the best model.</p>
			<p>The aim of the activity is to perform repeated k-fold cross-validation and grid search optimization on the same model.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Load the required packages (<strong class="inline">mlbench</strong>, <strong class="inline">caret</strong>, and <strong class="inline">dplyr</strong>).</li>
				<li>Load the <strong class="inline">PimaIndianDiabetes</strong> dataset into memory from <strong class="inline">mlbench</strong> package.</li>
				<li>Set a seed value for reproducibility.</li>
				<li>Define the k-fold validation object using the <strong class="inline">trainControl</strong> function from <strong class="inline">caret</strong> package and define <strong class="inline">method</strong> as <strong class="inline">repeatedcv</strong> instead of <strong class="inline">cv</strong>.</li>
				<li>Define an additional construct in the <strong class="inline">trainControl</strong> function for the number of repeats in the validation of <strong class="inline">repeats = 10</strong>.</li>
				<li>Define the grid for the <strong class="inline">mtry</strong> hyperparameter of a random forest model as <strong class="inline">(3,4,5)</strong>.</li>
				<li>Fit the model with the grid values, cross-validation objects, and random forest classifiers.</li>
				<li>Study the model performance by plotting the accuracy across different values of the hyperparameter.<p>The final output should be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="image/C12624_07_15.jpg" alt="Figure 7.15: Model performance accuracy across different values of the hyperparameter&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.15: Model performance accuracy across different values of the hyperparameter</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 461.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor314"/>Summary</h2>
			<p>In this chapter, you learned a few important aspects of model performance improvement techniques. We started with <strong class="bold">Bias-Variance Trade-off</strong> and understood it impacts a model's performance. We now know that high bias will result in underfitting, whereas high variance will result in overfitting of models, and that achieving one comes at the expense of the other. Therefore, in order to build the best models, we need to strike the ideal balance between bias and variance in machine learning models.</p>
			<p>Next, we explored various types of cross-validation techniques in R that provide ready-to-use functions to implement the same. We studied holdout, k-fold, and hold-one-out validation approaches to cross-validation and understood how we can perform robust assessment of performance of machine learning models. We then studied hyperparameter tuning and explored grid search optimization, random search optimization, and Bayesian optimization techniques in detail. Hyperparameter tuning of machine learning models helped us to develop more generalized models with better performance.</p>
			<p>In the next chapter, we will explore the process of deploying a machine learning model in the cloud.</p>
		</div>
	</body></html>