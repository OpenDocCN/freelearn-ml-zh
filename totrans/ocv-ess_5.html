<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;5.&#xA0;Focusing on the Interesting 2D Features"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Focusing on the Interesting 2D Features</h1></div></div></div><p class="calibre7">In most images, the most useful information is around certain zones that typically correspond to salient points and regions. In most applications, local processing around these salient points is sufficient as long as these points are stable and distinctive. In this chapter, we will cover a basic introduction to the 2D salient points and features offered by OpenCV. It is important to note the difference between detectors and descriptors. <span class="strong"><strong class="calibre8">Detectors</strong></span> only extract interest points (local features) on an image, while descriptors obtain relevant information about the neighborhood of these points. <span class="strong"><strong class="calibre8">Descriptors</strong></span>, as <a id="id322" class="calibre1"/>their name suggests, describe the image by proper features. They describe an interest point in a way that is invariant to change in lighting and to small perspective deformations. This can be used to match them with other descriptors (typically extracted from other images). For this purpose, matchers are used. This, in turn, can be used to detect objects and infer the camera transformation between two images. First, we show the internal structure of the interest points and provide an explanation of the 2D features and descriptor extraction. Finally, the chapter deals with matching, that is, putting 2D features of different images into correspondence.</p></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Focusing on the Interesting 2D Features">
<div class="book" title="Interest points"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec36" class="calibre1"/>Interest points</h1></div></div></div><p class="calibre7">Local<a id="id323" class="calibre1"/> features, also called <a id="id324" class="calibre1"/>interest points, are characterized<a id="id325" class="calibre1"/> by sudden changes of intensity in the region. These local features are usually classified in edges, corners, and blobs. OpenCV encapsulates interesting point information in the <code class="email">KeyPoint</code> class, which contains the following data:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The coordinates of the interest point (the <code class="email">Point2f</code> type)</li><li class="listitem">Diameter of the meaningful keypoint neighborhood</li><li class="listitem">Orientation of the keypoint</li><li class="listitem">Strength of the keypoint, which depends on the keypoint detector that is selected</li><li class="listitem">Pyramid layer (octave) from which the keypoint has been extracted; octaves are used in some descriptors such as <code class="email">SIFT</code>, <code class="email">SURF</code>, <code class="email">FREAK</code>, or <code class="email">BRISK</code></li><li class="listitem">Object ID used to perform clustering</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Feature detectors"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec37" class="calibre1"/>Feature detectors</h1></div></div></div><p class="calibre7">OpenCV handles <a id="id326" class="calibre1"/>several local<a id="id327" class="calibre1"/> feature detector implementations through the <code class="email">FeatureDetector</code> abstract class and its <code class="email">Ptr&lt;FeatureDetector&gt; FeatureDetector::create(const string&amp; detectorType)</code> method or through the algorithm class directly. In the first case, the type of detector is specified (see the following diagram where the detectors used in this chapter are indicated in red color). Detectors and the types of local features that they detect are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">FAST</code> (<code class="email">FastFeatureDetector</code>): This <a id="id328" class="calibre1"/>feature detects corners and blobs</li><li class="listitem"><code class="email">STAR</code> (<code class="email">StarFeatureDetector</code>): This<a id="id329" class="calibre1"/> feature detects edges, corners, and blobs</li><li class="listitem"><code class="email">SIFT</code> (<code class="email">SiftFeatureDetector</code>): This <a id="id330" class="calibre1"/>feature detects corners and blobs (part of the <code class="email">nonfree</code> module)</li><li class="listitem"><code class="email">SURF</code> (<code class="email">SurfFeatureDetector</code>): This <a id="id331" class="calibre1"/>feature detects corners and blobs (part of the <code class="email">nonfree</code> module)</li><li class="listitem"><code class="email">ORB</code> (<code class="email">OrbFeatureDetector</code>): This <a id="id332" class="calibre1"/>feature detects corners and blobs</li><li class="listitem"><code class="email">BRISK</code> (<code class="email">BRISK</code>): This<a id="id333" class="calibre1"/> feature detects corners and blobs</li><li class="listitem"><code class="email">MSER</code> (<code class="email">MserFeatureDetector</code>): This <a id="id334" class="calibre1"/>feature detects blobs</li><li class="listitem"><code class="email">GFTT</code> (<code class="email">GoodFeaturesToTrackDetector</code>): This <a id="id335" class="calibre1"/>feature detects edges and corners</li><li class="listitem"><code class="email">HARRIS</code> (<code class="email">GoodFeaturesToTrackDetector</code>): This <a id="id336" class="calibre1"/>feature detects edges and corners (with the Harris detector enabled)</li><li class="listitem"><code class="email">Dense</code> (<code class="email">DenseFeatureDetector</code>): This <a id="id337" class="calibre1"/>feature detects the features that are distributed densely and regularly on the image</li><li class="listitem"><code class="email">SimpleBlob</code> (<code class="email">SimpleBlobDetector</code>): This<a id="id338" class="calibre1"/> feature detects blobs</li></ul></div><div class="mediaobject"><img src="../images/00028.jpeg" alt="Feature detectors" class="calibre9"/><div class="caption"><p class="calibre13">2D feature detectors in OpenCV</p></div></div><p class="calibre10"> </p><p class="calibre7">We should note that some of these detectors, such as <code class="email">SIFT</code>, <code class="email">SURF</code>, <code class="email">ORB</code>, and <code class="email">BRISK</code>, are also descriptors.</p><p class="calibre7">Keypoint detection<a id="id339" class="calibre1"/> is performed by the <code class="email">void FeatureDetector::detect(const Mat&amp; image, vector&lt;KeyPoint&gt;&amp; keypoints, const Mat&amp; mask)</code> function, which is another method of the <code class="email">FeatureDetector</code> class. The first parameter is the input image where the keypoints will be detected. The second parameter corresponds to the vector where the keypoints will be stored. The last parameter is optional and represents an input mask image in which we can specify where to look for keypoints.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note21" class="calibre1"/>Note</h3><p class="calibre7">Matthieu Labbé has implemented a Qt-based open source application where you can test OpenCV's corner detectors, feature extractors, and matching algorithms in a nice GUI. It is available at <a class="calibre1" href="https://code.google.com/p/find-object/">https://code.google.com/p/find-object/</a>.</p></div><p class="calibre7">The first interest points were historically corners. In 1977, Moravec defined corners as interest points where there is a large intensity variation in several directions (45 degrees). These interest points were used by Moravec to find matching regions in consecutive image frames. Later, in 1988, Harris improved Moravec's algorithm using the Taylor expansion to approximate the shifted intensity variation. Afterwards, other detectors appeared, such as the detector based on<a id="id340" class="calibre1"/> <span class="strong"><strong class="calibre8">difference of Gaussians</strong></span> (<span class="strong"><strong class="calibre8">DoG</strong></span>) and <a id="id341" class="calibre1"/>
<span class="strong"><strong class="calibre8">determinant of the Hessian</strong></span> (<span class="strong"><strong class="calibre8">DoH</strong></span>) (for example, <code class="email">SIFT</code> or <code class="email">SURF</code>, respectively) or the detector based on Moravec's algorithm, but considering continuous intensity values in a pixel neighborhood such as <code class="email">FAST</code> or <code class="email">BRISK</code> (scale-space FAST).</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note22" class="calibre1"/>Note</h3><p class="calibre7">Lu, in her personal blog, <span class="strong"><em class="calibre12">LittleCheeseCake</em></span>, explains some of the most popular detectors and descriptors in detail. The blog is available at <a class="calibre1" href="http://littlecheesecake.me/blog/13804625/feature-detectors-and-descriptors">http://littlecheesecake.me/blog/13804625/feature-detectors-and-descriptors</a>.</p></div></div>

<div class="book" title="Feature detectors">
<div class="book" title="The FAST detector"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec17" class="calibre1"/>The FAST detector</h2></div></div></div><p class="calibre7">The<a id="id342" class="calibre1"/> corner<a id="id343" class="calibre1"/> detector is based on the <a id="id344" class="calibre1"/>
<span class="strong"><strong class="calibre8">Features from Accelerated Segment Test</strong></span> (<span class="strong"><strong class="calibre8">FAST</strong></span>) algorithm. It was designed to be very efficient, targeting real-time applications. The method is based on considering a circle of 16 pixels (neighborhood) around a candidate corner p. The FAST detector will consider p as a corner if there is a set of contiguous pixels in the neighborhood that all are brighter than p+T or darker than p-T, T being a threshold value. This threshold must be properly selected.</p><p class="calibre7">OpenCV implements the <a id="id345" class="calibre1"/>FAST detector in the <code class="email">FastFeatureDetector()</code> class, which is a wrapper class for the <code class="email">FAST()</code> method. To use this class, we must include the <code class="email">features2d.hpp</code> header file in our code.</p><p class="calibre7">Next, we show a code example where the corners are detected using the <code class="email">FAST</code> method with different threshold values. The <code class="email">FASTDetector</code> code example<a id="id346" class="calibre1"/> is shown as follows:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp"
<span class="strong"><strong class="calibre8">#include "opencv2/features2d/features2d.hpp"</strong></span>
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

int main(int argc, char *argv[])
{
    //Load original image and convert to gray scale
    Mat in_img = imread("book.png");
    cvtColor( in_img, in_img, COLOR_BGR2GRAY );
  
    //Create a keypoint vectors
    vector&lt;KeyPoint&gt; keypoints1,keypoints2;
    //FAST detector with threshold value of 80 and 100
    <span class="strong"><strong class="calibre8">FastFeatureDetector detector1(80);</strong></span>
    <span class="strong"><strong class="calibre8">FastFeatureDetector detector2(100);</strong></span>

    //Compute keypoints in in_img with detector1 and detector2
    <span class="strong"><strong class="calibre8">detector1.detect(in_img, keypoints1);</strong></span>
    <span class="strong"><strong class="calibre8">detector2.detect(in_img, keypoints2);</strong></span>

    Mat out_img1, out_img2;
    //Draw keypoints1 and keypoints2
    <span class="strong"><strong class="calibre8">drawKeypoints(in_img,keypoints1,out_img1,Scalar::all(-1),0);</strong></span>
    <span class="strong"><strong class="calibre8">drawKeypoints(in_img,keypoints2,out_img2,Scalar::all(-1),0);</strong></span>

    //Show keypoints detected by detector1 and detector2
    imshow( "out_img1", out_img1 );
    imshow( "out_img2", out_img2 );
    waitKey(0);
    return 0;
}</pre></div><p class="calibre7">The <a id="id347" class="calibre1"/>explanation of the code is given as follows. In this and the following examples, we usually perform the following three steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Create the 2D feature detector.</li><li class="listitem" value="2">Detect keypoints in the image.</li><li class="listitem" value="3">Draw the keypoints obtained.</li></ol><div class="calibre16"/></div><p class="calibre7">In our sample, <code class="email">FastFeatureDetector(int threshold=1, bool nonmaxSuppression= true, type=FastFeatureDetector::TYPE_9_16)</code> is the function where the detector parameters, such as threshold value, non-maximum suppression, and neighborhoods, are defined.</p><p class="calibre7">The following three types of neighborhoods can be selected:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">FastFeatureDetector::TYPE_9_16</code></li><li class="listitem"><code class="email">FastFeatureDetector::TYPE_7_12</code></li><li class="listitem"><code class="email">FastFeatureDetector::TYPE_5_8</code></li></ul></div><p class="calibre7">These neighborhoods define the number of neighbors (16, 12, or 8) and the total number of contiguous pixels (9, 7, or 5) needed to consider the corner (keypoint) valid. An example of <code class="email">TYPE_9_16</code> is shown in the next screenshot.</p><p class="calibre7">In our code, the threshold values <code class="email">80</code> and <code class="email">100</code> have been selected, while the rest of the parameters have their default values, <code class="email">nonmaxSuppression=true</code> and <code class="email">type=FastFeatureDetector::TYPE_9_16</code>, as shown:</p><div class="informalexample"><pre class="programlisting">FastFeatureDetector detector1(80);
FastFeatureDetector detector2(100);</pre></div><p class="calibre7">Keypoints are detected and saved using the <code class="email">void detect(const Mat&amp; image, vector&lt;KeyPoint&gt;&amp; keypoints, const Mat&amp; mask=Mat())</code> function. In our case, we create the following two FAST feature detectors:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">detector1</code> saves its keypoints in the <code class="email">keypoints1</code> vector</li><li class="listitem"><code class="email">detector2</code> saves its keypoints in the <code class="email">keypoints2</code></li></ul></div><p class="calibre7">The <code class="email">void drawKeypoints(const Mat&amp; image, const vector&lt;KeyPoint&gt;&amp; keypoints, Mat&amp; outImage, const Scalar&amp; color=Scalar::all(-1), int flags=DrawMatchesFlags::DEFAULT)</code> function draws the keypoints in the image. The <code class="email">color</code> parameter allows us to define a color of keypoints, and with the <code class="email">Scalar:: all(-1)</code> option, each keypoint will be drawn with a different color.</p><p class="calibre7">The keypoints are<a id="id348" class="calibre1"/> drawn using the two threshold values on the image. We will notice a small difference in the number of keypoints detected. This is due to the threshold value in each case. The following screenshot shows a corner detected in the sample with a threshold value of 80, which is not detected with a threshold value of 100:</p><div class="mediaobject"><img src="../images/00029.jpeg" alt="The FAST detector" class="calibre9"/><div class="caption"><p class="calibre13">Keypoint detected with a threshold value of 80 (in the left-hand side). The same corner is not detected with a threshold value of 100 (in the right-hand side).</p></div></div><p class="calibre10"> </p><p class="calibre7">The difference is due to the fact that the FAST feature detectors are created with the default type, that is, <code class="email">TYPE_9_16</code>. In the example, the p pixel takes a value of 228, so at least nine contiguous pixels must be brighter than p+T or darker than p-T. The following screenshot shows the neighborhood pixel values in this specific keypoint. The condition of nine contiguous pixels is met if we use a threshold value of 80. However, the condition is not met with a threshold value of 100:</p><div class="mediaobject"><img src="../images/00030.jpeg" alt="The FAST detector" class="calibre9"/><div class="caption"><p class="calibre13">Keypoint pixel values and contiguous pixels all darker than p-T (228-80=148) with a threshold value of 80</p></div></div><p class="calibre10"> </p></div></div>

<div class="book" title="Feature detectors">
<div class="book" title="The SURF detector"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec18" class="calibre1"/>The SURF detector</h2></div></div></div><p class="calibre7">The <span class="strong"><strong class="calibre8">Speeded Up Robust Features</strong></span> (<span class="strong"><strong class="calibre8">SURF</strong></span>) detector<a id="id349" class="calibre1"/> is based on <a id="id350" class="calibre1"/>a Hessian matrix to find the interest points. For this purpose, SURF divides the image in different scales (levels and octaves) using second-order Gaussian kernels and approximates these kernels with a simple box filter. This filter box is mostly interpolated in scale and space in order to provide the detector with the scale-invariance properties. SURF is a faster <a id="id351" class="calibre1"/>approximation of the classic <span class="strong"><strong class="calibre8">Scale Invariant Feature Transform</strong></span> (<span class="strong"><strong class="calibre8">SIFT</strong></span>) detector. Both the SURF and SIFT detectors are patented, so OpenCV includes them separately in their <code class="email">nonfree/nonfree.hpp</code> header file.</p><p class="calibre7">The following <code class="email">SURFDetector</code> code shows an example where the keypoints are detected using the SURF detector <a id="id352" class="calibre1"/>with a different number of Gaussian pyramid octaves:</p><div class="informalexample"><pre class="programlisting">//… (omitted for simplicity)
<span class="strong"><strong class="calibre8">#include "opencv2/nonfree/nonfree.hpp"</strong></span>

int main(int argc, char *argv[])
{
    //Load image and convert to gray scale (omitted for
    //simplicity)

    //Create a keypoint vectors
    vector&lt;KeyPoint&gt; keypoints1,keypoints2;
    
    //SURF detector1 and detector2 with 2 and 5 Gaussian pyramid
    //octaves respectively
    <span class="strong"><strong class="calibre8">SurfFeatureDetector detector1(3500, 2, 2, false, false);</strong></span>
    <span class="strong"><strong class="calibre8">SurfFeatureDetector detector2(3500, 5, 2, false, false);</strong></span>

    //Compute keypoints in in_img with detector1 and detector2
    <span class="strong"><strong class="calibre8">detector1.detect(in_img, keypoints1);</strong></span>
    <span class="strong"><strong class="calibre8">detector2.detect(in_img, keypoints2);</strong></span>
    Mat out_img1, out_img2;

    //Draw keypoints1 and keypoints2
    <span class="strong"><strong class="calibre8">drawKeypoints(in_img,keypoints1,out_img1,Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);</strong></span>
    <span class="strong"><strong class="calibre8">drawKeypoints(in_img,keypoints2,out_img2,Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);</strong></span>

//Show the 2 final images (omitted for simplicity)
return 0;
}</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note23" class="calibre1"/>Note</h3><p class="calibre7">In the preceding example (and subsequent ones), some portions of code are not repeated for simplicity because they are the same as in previous examples.</p></div><p class="calibre7">The explanation<a id="id353" class="calibre1"/> of the code is given as follows. <code class="email">SURFFeatureDetector(double hessianThreshold, int nOctaves, int nOctaveLayers, bool extended, bool upright)</code> is the main function used to create a SURF detector where we can define the parameter values of the detector, such as the Hessian threshold, the number of Gaussian pyramid octaves, number of images within each octave of a Gaussian pyramid, number of elements in the descriptor, and the orientation of each feature.</p><p class="calibre7">A high threshold value extracts less keypoints but with more accuracy. A low threshold value extracts more keypoints but with less accuracy. In this case, we have used a large Hessian threshold (<code class="email">3500</code>) to show a reduced number of keypoints in the image. Also, the number of octaves changes for each image (2 and 5, respectively). A larger number of octaves also select keypoints with a larger size. The following screenshot shows the result:</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="The SURF detector" class="calibre9"/><div class="caption"><p class="calibre13">The SURF detector with two Gaussian pyramid octaves (in the left-hand side) and the SURF detector with five Gaussian pyramid octaves (in the right-hand side)</p></div></div><p class="calibre10"> </p><p class="calibre7">Again, we use the <code class="email">drawKeypoints</code> function to draw the keypoints detected, but in this case, as the SURF detector has orientation <a id="id354" class="calibre1"/>properties, the <code class="email">DrawMatchesFlags</code> parameter is defined as <code class="email">DRAW_RICH_KEYPOINTS</code>. Then, the <code class="email">drawKeypoints</code> function draws each keypoint with its size and orientation.</p></div></div>

<div class="book" title="Feature detectors">
<div class="book" title="The ORB detector"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec19" class="calibre1"/>The ORB detector</h2></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre8">Binary Robust Independent Elementary Features</strong></span> (<span class="strong"><strong class="calibre8">BRIEF</strong></span>) <a id="id355" class="calibre1"/>is a descriptor based on binary strings; it<a id="id356" class="calibre1"/> does<a id="id357" class="calibre1"/> not find interest points. The <span class="strong"><strong class="calibre8">Oriented FAST and Rotated BRIEF</strong></span> (<span class="strong"><strong class="calibre8">ORB</strong></span>) detector is a union of the FAST detector and BRIEF descriptor and is considered an alternative to the patented SIFT and SURF detectors. The ORB detector uses the FAST detector with pyramids to detect interest points and then uses the HARRIS algorithm to rank the features and retain the best ones. OpenCV also allows us to use the FAST algorithm to rank the features, but normally, this produces less stable keypoints. The following <code class="email">ORBDetector</code> code shows a simple and clear example of this difference:</p><div class="informalexample"><pre class="programlisting">int main(int argc, char *argv[])
{
    //Load image and convert to gray scale (omitted for
    //simplicity)
    
    //Create a keypoint vectors
    vector&lt;KeyPoint&gt; keypoints1,keypoints2;

    //ORB detector with FAST (detector1) and HARRIS (detector2)
    //score to rank the features
    <span class="strong"><strong class="calibre8">OrbFeatureDetector detector1(300, 1.1f, 2, 31,0, 2, ORB::FAST_SCORE, 31);</strong></span>
    <span class="strong"><strong class="calibre8">OrbFeatureDetector detector2(300, 1.1f, 2, 31,0, 2, ORB::HARRIS_SCORE, 31);</strong></span>
    
    //Compute keypoints in in_img with detector1 and detector2
    detector1.detect(in_img, keypoints1);
    detector2.detect(in_img, keypoints2);
    
    Mat out_img1, out_img2;
    //Draw keypoints1 and keypoints2
    drawKeypoints(in_img,keypoints1,out_img1,Scalar::all(-1), DrawMatchesFlags::DEFAULT);
    drawKeypoints(in_img,keypoints2,out_img2,Scalar::all(-1), DrawMatchesFlags::DEFAULT);

    //Show the 2 final images (omitted for simplicity)
    return 0;
}</pre></div><div class="mediaobject"><img src="../images/00032.jpeg" alt="The ORB detector" class="calibre9"/><div class="caption"><p class="calibre13">The ORB detector with the FAST algorithm to select the 300 best features (in the left-hand side) and the HARRIS detector to select the 300 best features (in the right-hand side)</p></div></div><p class="calibre10"> </p><p class="calibre7">The explanation <a id="id358" class="calibre1"/>of the code is given as follows. The <code class="email">OrbFeatureDetector(int nfeatures=500, float scaleFactor=1.2f, int nlevels=8, int edgeThreshold=31, int firstLevel=0, int WTA_K=2, int scoreType=ORB:: HARRIS_SCORE, int patchSize=31)</code> function is the class constructor where we can specify the maximum number of features to retain the scale, number of levels, and type of detector (<code class="email">HARRIS_SCORE</code> or <code class="email">FAST_SCORE</code>) used to rank the features.</p><p class="calibre7">The following proposed code example shows the difference between the HARRIS and FAST algorithms to rank features; the result is shown in the preceding screenshot:</p><div class="informalexample"><pre class="programlisting">OrbFeatureDetector detector1(300, 1.1f, 2, 31,0, 2, ORB::FAST_SCORE, 31);
OrbFeatureDetector detector2(300, 1.1f, 2, 31,0, 2, ORB::HARRIS_SCORE, 31);</pre></div><p class="calibre7">The HARRIS corner detector<a id="id359" class="calibre1"/> is used more than FAST to rank features, because it rejects edges and provides a reasonable score. The rest of the functions are the same as in the previous detector examples, keypoint detection and drawing.</p></div></div>

<div class="book" title="Feature detectors">
<div class="book" title="The KAZE and AKAZE detectors"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec20" class="calibre1"/>The KAZE and AKAZE detectors</h2></div></div></div><p class="calibre7">The KAZE and <a id="id360" class="calibre1"/>AKAZE detectors <a id="id361" class="calibre1"/>will be included in the upcoming OpenCV 3.0.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip05" class="calibre1"/>Tip</h3><p class="calibre7">OpenCV 3.0 is not yet available. Again, if you want to test this code and use the KAZE and AKAZE features, you can work with the latest version already available in the OpenCV git repository at <a class="calibre1" href="http://code.opencv.org/projects/opencv/repository">http://code.opencv.org/projects/opencv/repository</a>.</p></div><p class="calibre7">The KAZE detector<a id="id362" class="calibre1"/> is a method that can detect 2D features in a nonlinear scale space. This method allows us to keep important image details and remove noise. <span class="strong"><strong class="calibre8">Additive Operator Splitting</strong></span> (<span class="strong"><strong class="calibre8">AOS</strong></span>) schemes<a id="id363" class="calibre1"/> are used for nonlinear <a id="id364" class="calibre1"/>scale space. AOS schemes are efficient, stable, and parallelizable. The algorithm computes the response of a Hessian matrix at multiple scale levels to detect keypoints. On the other hand, the <span class="strong"><strong class="calibre8">Accelerated-KAZE</strong></span> (<span class="strong"><strong class="calibre8">AKAZE</strong></span>) feature detector uses fast explicit diffusion to build a nonlinear scale space.</p><p class="calibre7">Next, in the <code class="email">KAZEDetector</code> code, we see an example of the new KAZE and AKAZE feature detectors:</p><div class="informalexample"><pre class="programlisting">int main(int argc, char *argv[])
{
    //Load image and convert to gray scale (omitted for
    //simplicity)

    //Create a keypoint vectors
    vector&lt;KeyPoint&gt; keypoints1,keypoints2;
    
    //Create KAZE and AKAZE detectors
    <span class="strong"><strong class="calibre8">KAZE detector1(true,true);</strong></span>
    <span class="strong"><strong class="calibre8">AKAZE detector2(cv::AKAZE::DESCRIPTOR_KAZE_UPRIGHT,0,3);</strong></span>

    //Compute keypoints in in_img with detector1 and detector2
    detector1.detect(in_img, keypoints1);
    detector2.detect(in_img, keypoints2,cv::Mat());

    Mat out_img1, out_img2;
    //Draw keypoints1 and keypoints2
    drawKeypoints(in_img,keypoints1,out_img1,Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);
    drawKeypoints(in_img,keypoints2,out_img2,Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);

    //Show the 2 final images (omitted for simplicity)
    return 0;
}</pre></div><p class="calibre7">The <code class="email">KAZE::KAZE(bool extended, bool upright)</code> function is the KAZE class <a id="id365" class="calibre1"/>constructor in which two parameters can be selected: <code class="email">extended</code> and <code class="email">upright</code>. The <code class="email">extended</code> parameter adds the option to select between 64 or 128<a id="id366" class="calibre1"/> descriptors, while the <code class="email">upright</code> parameter allows us to select rotation or no invariant. In this case, we use both parameters with a <code class="email">true</code> value.</p><p class="calibre7">On the other hand, the <code class="email">AKAZE::AKAZE(DESCRIPTOR_TYPE descriptor_type, int descriptor_size=0, int descriptor_channels=3)</code> function is the AKAZE class constructor. This function gets the descriptor type, descriptor size, and the channels as input arguments. For the descriptor type, the following enumeration is applied:</p><div class="informalexample"><pre class="programlisting">enum DESCRIPTOR_TYPE {DESCRIPTOR_KAZE_UPRIGHT = 2, DESCRIPTOR_KAZE = 3, DESCRIPTOR_MLDB_UPRIGHT = 4, DESCRIPTOR_MLDB = 5 };</pre></div><p class="calibre7">The following screenshot shows the results obtained with this example:</p><div class="mediaobject"><img src="../images/00033.jpeg" alt="The KAZE and AKAZE detectors" class="calibre9"/><div class="caption"><p class="calibre13">The KAZE detector (in the left-hand side) and the AKAZE detector (in the right-hand side)</p></div></div><p class="calibre10"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note24" class="calibre1"/>Note</h3><p class="calibre7">Eugene Khvedchenya's <span class="strong"><em class="calibre12">Computer Vision Talks</em></span> blog contains useful reports that compare different keypoints in terms of robustness and efficiency. See the posts at <a class="calibre1" href="http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/">http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/</a> and <a class="calibre1" href="http://computer-vision-talks.com/articles/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/">http://computer-vision-talks.com/articles/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/</a>.</p></div></div></div>
<div class="book" title="Feature descriptor extractors"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec38" class="calibre1"/>Feature descriptor extractors</h1></div></div></div><p class="calibre7">Descriptors<a id="id367" class="calibre1"/> describe local image regions and are invariant to image transformations such as rotation, scale or translation. They provide a <a id="id368" class="calibre1"/>measure and distance function for a small patch around an interest point. Therefore, whenever the similarity between two image patches needs to be estimated, we compute their descriptors and measure their distance. In OpenCV, the basic Mat type is used to represent a collection of descriptors, where each row is a keypoint descriptor.</p><p class="calibre7">There are the<a id="id369" class="calibre1"/> following two possibilities to use a feature descriptor extractor:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The <code class="email">DescriptorExtractor</code> common interface</li><li class="listitem">The algorithm class directly</li></ul></div><p class="calibre7">(See the following diagram where the descriptors used in this chapter are indicated in red color.)</p><p class="calibre7">The common interface <a id="id370" class="calibre1"/>allows us to switch easily between different algorithms. This can be very useful when choosing an algorithm to solve a problem, as the results of each algorithm can be compared with no effort. On the other hand, depending on the algorithm, there are several parameters that can be tweaked only using its class.</p><div class="mediaobject"><img src="../images/00034.jpeg" alt="Feature descriptor extractors" class="calibre9"/><div class="caption"><p class="calibre13">2D feature descriptors in OpenCV</p></div></div><p class="calibre10"> </p><p class="calibre7">The <code class="email">Ptr&lt;DescriptorExtractor&gt; DescriptorExtractor::create(const String&amp; descriptorExtractorType)</code> function creates a new descriptor extractor of the selected type. Descriptors can be grouped in two families: float and binary. Float descriptors store float<a id="id371" class="calibre1"/> values in a vector; this can lead to a high memory<a id="id372" class="calibre1"/> usage. On the other hand, binary descriptors store binary strings, thus enabling faster processing times and a reduced memory footprint. The current implementation supports the following types:</p><div class="book"><ul class="itemizedlist"><li class="listitem">SIFT: This implementation supports the float descriptor</li><li class="listitem">SURF: This implementation supports the float descriptor</li><li class="listitem">BRIEF: This implementation supports the binary descriptor</li><li class="listitem">BRISK: This implementation supports the binary descriptor</li><li class="listitem">ORB: This implementation supports the binary descriptor</li><li class="listitem">FREAK: This implementation supports the binary descriptor</li><li class="listitem">KAZE: This implementation supports the binary descriptor (new in OpenCV 3.0)</li><li class="listitem">AKAZE: This implementation supports the binary descriptor (new in OpenCV 3.0)</li></ul></div><p class="calibre7">The other important function of <code class="email">DescriptorExtractor</code> is <code class="email">void DescriptorExtractor::compute(InputArray image, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors)</code>, which computes the descriptors for a set of keypoints detected in an image on the previous step. There is a variant of the function that accepts an image set.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip06" class="calibre1"/>Tip</h3><p class="calibre7">Note that it is possible to mix feature detectors and descriptor extractors from different algorithms. However, it is recommended that you use both methods from the same algorithm, as they should fit better together.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Descriptor matchers"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec39" class="calibre1"/>Descriptor matchers</h1></div></div></div><p class="calibre7">
<code class="email">DescriptorMatcher</code><a id="id373" class="calibre1"/> is<a id="id374" class="calibre1"/> an abstract base class to match keypoint descriptors that, as happens with <code class="email">DescriptorExtractor</code>, make programs more flexible than using matchers directly. With the <code class="email">Ptr&lt;DescriptorMatcher&gt; DescriptorMatcher::create(const string&amp; descriptorMatcherType)</code> function, we can create a descriptor matcher of the desired type. The following are the supported types:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre8">BruteForce-L1</strong></span>: This is <a id="id375" class="calibre1"/>used for float descriptors. It uses L1 distance and is efficient and fast.</li><li class="listitem"><span class="strong"><strong class="calibre8">BruteForce</strong></span>: This is<a id="id376" class="calibre1"/> used for float descriptors. It uses L2 distance and can be better than L1, but it needs more CPU usage.</li><li class="listitem"><span class="strong"><strong class="calibre8">BruteForce-SL2</strong></span>: This is <a id="id377" class="calibre1"/>used for float descriptors and avoids square root computation from L2, which requires high CPU usage.</li><li class="listitem"><span class="strong"><strong class="calibre8">BruteForce-Hamming</strong></span>: This is<a id="id378" class="calibre1"/> used for binary descriptors and calculates the Hamming distance between the compared descriptors.</li><li class="listitem"><span class="strong"><strong class="calibre8">BruteForce-Hamming(2)</strong></span>: This is <a id="id379" class="calibre1"/>used for binary descriptors (2 bits version).</li><li class="listitem"><span class="strong"><strong class="calibre8">FlannBased</strong></span>: This is <a id="id380" class="calibre1"/>used for float descriptors and is faster than brute force by pre-computing acceleration structures (as in DB engines) at the cost of using more memory.</li></ul></div><p class="calibre7">The <code class="email">void DescriptorMatcher::match(InputArray queryDescriptors, InputArray trainDescriptors, vector&lt;DMatch&gt;&amp; matches, InputArray mask=noArray())</code> and <code class="email">void DescriptorMatcher::knnMatch(InputArray queryDescriptors, InputArray trainDescriptors, vector&lt;vector&lt;DMatch&gt;&gt;&amp; matches, int k, InputArray mask=noArray(), bool compactResult=false)</code> functions give the best k matches for each descriptor, k being 1 for the first function.</p><p class="calibre7">The <code class="email">void DescriptorMatcher::radiusMatch(InputArray queryDescriptors, InputArray trainDescriptors, vector&lt;vector&lt;DMatch&gt;&gt;&amp; matches, float maxDistance, InputArray mask=noArray(), bool compactResult=false)</code> function also finds the matches for each query descriptor but not farther than the specified distance. The major drawback of this method is that the magnitude of this distance is not normalized, and it depends on the feature extractor and descriptor used.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip07" class="calibre1"/>Tip</h3><p class="calibre7">In order to get the best results, we recommend that you use matchers along with descriptors of the same type. Although it is possible to mix binary descriptors with float matchers and the other way around, the results might be inaccurate.</p></div></div>

<div class="book" title="Descriptor matchers">
<div class="book" title="Matching the SURF descriptors"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec21" class="calibre1"/>Matching the SURF descriptors</h2></div></div></div><p class="calibre7">SURF descriptors <a id="id381" class="calibre1"/>belong to the family of oriented gradients descriptors. They encode statistical knowledge about the geometrical shapes present in the patch (via histograms of oriented gradients/Haar-like features). They are considered as a more efficient substitution for SIFT. They are the best known multiscale feature description approaches, and their accuracy has <a id="id382" class="calibre1"/>been widely tested. They have two main drawbacks though:</p><div class="book"><ul class="itemizedlist"><li class="listitem">They are patented</li><li class="listitem">They are slower than binary descriptors</li></ul></div><p class="calibre7">There is a common pipeline in every descriptor matching application that uses the components explained earlier in this chapter. It performs the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Compute interest points in both images.</li><li class="listitem" value="2">Extract descriptors from the two generated interest point sets.</li><li class="listitem" value="3">Use a matcher to find connections between descriptors.</li><li class="listitem" value="4">Filter the results to remove bad matches.</li></ol><div class="calibre16"/></div><p class="calibre7">The following is the <code class="email">matchingSURF</code> example that follows this pipeline:</p><div class="informalexample"><pre class="programlisting">#include &lt;iostream&gt;
#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/nonfree/nonfree.hpp"

using namespace std;
using namespace cv;

int main( int argc, char** argv )
{
    Mat img_orig = imread( argv[1],IMREAD_GRAYSCALE);
    Mat img_fragment = imread( argv[2], IMREAD_GRAYSCALE);
    if(img_orig.empty() || img_fragment.empty())
    {
        cerr &lt;&lt; " Failed to load images." &lt;&lt; endl;
        return -1;
    }

     //Step 1: Detect keypoints using SURF Detector
     vector&lt;KeyPoint&gt; keypoints1, keypoints2;
     <span class="strong"><strong class="calibre8">Ptr&lt;FeatureDetector&gt; detector = FeatureDetector::create("SURF");</strong></span>

     detector-&gt;detect(img_orig, keypoints1);
     detector-&gt;detect(img_fragment, keypoints2);

     //Step 2: Compute descriptors using SURF Extractor
     <span class="strong"><strong class="calibre8">Ptr&lt;DescriptorExtractor&gt; extractor = DescriptorExtractor::create("SURF");</strong></span>
     Mat descriptors1, descriptors2;
     extractor-&gt;compute(img_orig, keypoints1, descriptors1);
     extractor-&gt;compute(img_fragment, keypoints2, descriptors2);

     //Step 3: Match descriptors using a FlannBased Matcher
     <span class="strong"><strong class="calibre8">Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create("FlannBased");</strong></span>
     vector&lt;DMatch&gt; matches12;
     vector&lt;DMatch&gt; matches21;
     vector&lt;DMatch&gt; good_matches;

<span class="strong"><strong class="calibre8">     matcher-&gt;match(descriptors1, descriptors2, matches12);</strong></span>
<span class="strong"><strong class="calibre8">     matcher-&gt;match(descriptors2, descriptors1, matches21);</strong></span>

     //Step 4: Filter results using cross-checking
     for( size_t i = 0; i &lt; matches12.size(); i++ )
     {
         DMatch forward = matches12[i];
         DMatch backward = matches21[forward.trainIdx];
         if( backward.trainIdx == forward.queryIdx )
             good_matches.push_back( forward );
     }

     //Draw the results
     Mat img_result_matches;
     drawMatches(img_orig, keypoints1, img_fragment, keypoints2, good_matches, img_result_matches);
     imshow("Matching SURF Descriptors", img_result_matches);
     waitKey(0);

     return 0;
 }</pre></div><p class="calibre7">The explanation<a id="id383" class="calibre1"/> of the code is given as follows. As we described earlier, following the application pipeline implies performing these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">The first step to be performed is to detect interest points in the input images. In this example, the common interface is used to create a SURF detector with the line <code class="email">Ptr&lt;FeatureDetector&gt; detector = FeatureDetector::create("SURF")</code>.</li><li class="listitem" value="2">After that, the interest points are detected, and a descriptor extractor is created using the common interface <code class="email">Ptr&lt;DescriptorExtractor&gt; extractor = DescriptorExtractor::create( "SURF")</code>. The SURF algorithm is also used to compute the descriptors.</li><li class="listitem" value="3">The next step is to match the descriptors of both images, and for this purpose, a descriptor matcher is created using the common interface, too. The line, <code class="email">Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create("FlannBased")</code>, creates a new matcher based on the Flann algorithm, which is used to match the descriptors in the following way:<div class="informalexample"><pre class="programlisting">matcher-&gt;match(descriptors1, descriptors2, matches12)</pre></div></li><li class="listitem" value="4">Finally, the results are filtered. Note that two matching sets are computed, as a cross-checking filter is performed afterwards. This filtering only stores the matches that appear in both sets when using the input images as query and train images. In the following screenshot, we can see the difference when a filter is used to discard matches:<div class="mediaobject"><img src="../images/00035.jpeg" alt="Matching the SURF descriptors" class="calibre9"/><div class="caption"><p class="calibre13">Results after matching SURF descriptors with and without a filter</p></div></div><p class="calibre14"> </p></li></ol><div class="calibre16"/></div></div></div>

<div class="book" title="Descriptor matchers">
<div class="book" title="Matching the AKAZE descriptors"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec22" class="calibre1"/>Matching the AKAZE descriptors</h2></div></div></div><p class="calibre7">KAZE and AKAZE are <a id="id384" class="calibre1"/>novel descriptors included in the upcoming OpenCV 3.0. According to published tests, both outperform the previous detectors included in the library by improving repeatability and distinctiveness for common 2D image-matching applications. AKAZE is much faster than KAZE while obtaining comparable results, so if speed is critical in an application, AKAZE should be used.</p><p class="calibre7">The <a id="id385" class="calibre1"/>following <code class="email">matchingAKAZE</code> example matches descriptors of this novel algorithm:</p><div class="informalexample"><pre class="programlisting">#include &lt;iostream&gt;
#include "opencv2/core/core.hpp"
#include "opencv2/features2d/features2d.hpp"
#include "opencv2/highgui/highgui.hpp"

using namespace cv;
using namespace std;

int main( int argc, char** argv )
{
  Mat img_orig = imread( argv[1], IMREAD_GRAYSCALE );
  Mat img_cam = imread( argv[2], IMREAD_GRAYSCALE );

  if( !img_orig.data || !img_cam.data )
  {
    cerr &lt;&lt; " Failed to load images." &lt;&lt; endl;
    return -1;
  }

  //Step 1: Detect the keypoints using AKAZE Detector
<span class="strong"><strong class="calibre8">  Ptr&lt;FeatureDetector&gt; detector = FeatureDetector::create("AKAZE");</strong></span>
  std::vector&lt;KeyPoint&gt; keypoints1, keypoints2;

  detector-&gt;detect( img_orig, keypoints1 );
  detector-&gt;detect( img_cam, keypoints2 );

  //Step 2: Compute descriptors using AKAZE Extractor
  Ptr&lt;DescriptorExtractor&gt; extractor = DescriptorExtractor::create("AKAZE");
  Mat descriptors1, descriptors2;

  extractor-&gt;compute( img_orig, keypoints1, descriptors1 );
  extractor-&gt;compute( img_cam, keypoints2, descriptors2 );

  //Step 3: Match descriptors using a BruteForce-Hamming Matcher
<span class="strong"><strong class="calibre8">  Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create("BruteForce-Hamming");</strong></span>
  vector&lt;vector&lt;DMatch&gt; &gt; matches;
  vector&lt;DMatch&gt; good_matches;

<span class="strong"><strong class="calibre8">  matcher.knnMatch(descriptors1, descriptors2, matches, 2);</strong></span>

  //Step 4: Filter results using ratio-test
  float ratioT = 0.6;
  for(int i = 0; i &lt; (int) matches.size(); i++)
  {
      if((matches[i][0].distance &lt; ratioT*(matches[i][1].distance)) &amp;&amp; ((int) matches[i].size()&lt;=2 &amp;&amp; (int) matches[i].size()&gt;0))
      {
          good_matches.push_back(matches[i][0]);
      }
  }

  //Draw the results
  Mat img_result_matches;
  drawMatches(img_orig, keypoints1, img_cam, keypoints2, good_matches, img_result_matches);
  imshow("Matching AKAZE Descriptors", img_result_matches);

  waitKey(0);

  return 0;
}</pre></div><p class="calibre7">The explanation<a id="id386" class="calibre1"/> of the code is given as follows. The first two steps are quite similar to the previous example; the feature detector and descriptor extractor are created through their common interfaces. We only change the string parameter passed to the constructor, as this time, the AKAZE algorithm is used.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note25" class="calibre1"/>Note</h3><p class="calibre7">A BruteForce matcher that uses Hamming distance is used this time, as AKAZE is a binary descriptor.</p></div><p class="calibre7">It is created by executing <code class="email">Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create("BruteForce-Hamming")</code>. The <code class="email">matcher.knnMatch(descriptors1, descriptors2, matches, 2)</code> function computes the matches between the image descriptors. It is noteworthy to mention the last integer parameter, as it is necessary<a id="id387" class="calibre1"/> for the filter processing executed afterwards. This filtering is called Ratio Test, and it computes the goodness of the best match between the goodness of the second best match. To be considered as a good match, this value must be higher than a certain ratio, which can be set in a range of values between 0 and 1. If the ratio tends to be 0, the correspondence between descriptors is stronger. </p><p class="calibre7">In the following screenshot, we can see the output when matching a book cover in an image where the book appears rotated:</p><div class="mediaobject"><img src="../images/00036.jpeg" alt="Matching the AKAZE descriptors" class="calibre9"/><div class="caption"><p class="calibre13">Matching AKAZE descriptors in a rotated image</p></div></div><p class="calibre10"> </p><p class="calibre7">The following screenshot shows the result when the book does not appear in the second image:</p><div class="mediaobject"><img src="../images/00037.jpeg" alt="Matching the AKAZE descriptors" class="calibre9"/><div class="caption"><p class="calibre13">Matching AKAZE descriptors when the train image does not appear</p></div></div><p class="calibre10"> </p></div></div>
<div class="book" title="Summary"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec40" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">In this chapter, we have covered a widely used OpenCV component. Local features are a key part of relevant computer vision algorithms such as object recognition, object tracking, image stitching, and camera calibration. An introduction and several samples have been provided, thus covering interest points detection using different algorithms, extraction of descriptors from interest points, matching descriptors, and filtering the results.</p></div>
<div class="book" title="What else?"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec41" class="calibre1"/>What else?</h1></div></div></div><p class="calibre7">The powerful Bag-of-Words object categorization framework has not been included. This is actually an additional step to what we have covered in this chapter, as extracted descriptors are clustered and used to perform categorization. A complete sample can be found at <code class="email">[opencv_source_code]/samples/cpp/bagofwords_classification.cpp</code>.</p></div></body></html>