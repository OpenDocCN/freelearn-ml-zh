- en: '*Chapter 5*: AWS Services for Data Storing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS provides a wide range of services to store your data safely and securely.
    There are various storage options available on AWS such as block storage, file
    storage, and object storage. It is expensive to manage on-premises data storage
    due to the higher investment in hardware, admin overheads, and managing system
    upgrades. With AWS Storage services, you just pay for what you use, and you don't
    have to manage the hardware. We will also learn about various storage classes
    offered by Amazon S3 for intelligent access of the data and reducing costs. You
    can expect questions in the exam on storage classes. As we continue in this chapter,
    we will master the single-AZ and multi-AZ instances, and **RTO** (**Recovery**
    **Time** **Objective**) and **RPO** (**Recovery** **Point** **Objective**) concepts
    of Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about storing our data securely for further
    analytics purposes by means of the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing data on Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling access on S3 buckets and objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting data on Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing S3 objects at rest and in transit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using other types of data stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational Database Services (RDSes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Failover in Amazon RDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking automatic backup, RDS snapshots, and restore and read replicas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing to Amazon Aurora with multi-master capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing columnar data on Amazon Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon DynamoDB for NoSQL databases as a service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All you will need for this chapter is an AWS account and the AWS CLI configured.
    The steps to configure the AWS CLI for your account are explained in detail by
    Amazon here: [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code examples from Github, here: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-5/](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-5/).'
  prefs: []
  type: TYPE_NORMAL
- en: Storing data on Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**S3** is Amazon''s cloud-based object storage service and it can be accessed
    from anywhere via the internet. It is an ideal storage option for large datasets.
    It is region-based, as your data is stored in a particular region until you move
    the data to a different region. Your data will never leave that region until it
    is configured. In a particular region, data is replicated in the availability
    zones of that region; this makes S3 regionally resilient. If any of the availability
    zones fail in a region, then other availability zones will serve your requests.
    It can be accessed via the AWS Console UI, AWS CLI, or AWS API requests, or via
    standard HTTP methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'S3 has two main components: **buckets** and **objects**.'
  prefs: []
  type: TYPE_NORMAL
- en: Buckets are created in a specific AWS region. Buckets can contain objects, but
    cannot contain other buckets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects have two main attributes. One is **Key**, and the other is **Value**.
    Value is the content being stored, and Key is the name. The maximum size of an
    object can be 5 TB. As per the Amazon S3 documentation available here, [https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html),
    objects also have a version ID, metadata, access control information, and subresources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As per Amazon's docs, S3 provides read-after-write consistency for PUTS of new
    objects, which means that if you put a new object or create a new object and you
    immediately turn around to read the object using its key, then you get the exact
    data that you just uploaded. However, for overwrites and deletes, it behaves in
    an **eventually consistent manner**. This means that if you read an object straight
    after the delete or overwrite operation, then you may read an old copy or a stale
    version of the object. It takes some time to replicate the content of the object
    across three availability zones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A folder structure can be maintained logically by using a prefix. Let''s take
    an example where an image is uploaded into a bucket, `bucket-name-example`, with
    the prefix `folder-name` and the object name as `my-image.jpg`. The entire structure
    looks like this: `/bucket-name-example/folder-name/my-image.jpg`.'
  prefs: []
  type: TYPE_NORMAL
- en: The content of the object can be read by using the bucket name as `bucket-name-example`
    and the key as `/folder-name/my-image.jpg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several storage classes offered by Amazon for the objects being stored
    in S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard Storage**: This is the storage class for frequently accessed objects
    and for quick access. S3 Standard has a millisecond first byte latency and objects
    can be made publicly available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced Redundancy (RR)**: This option provides less redundancy than the
    Standard Storage class. Non-critical and reproducible data can be stored in this
    class. AWS S3 documentation suggests not to use this class as the Standard Storage
    class is more cost-effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard Infrequent Access (IA)**: This option is used when you need data
    to be returned quickly, but not for frequent access. The object size has to be
    a minimum of 128 KB. The minimum storage timeframe is 30 days. If the object is
    deleted before 30 days, you are still charged for 30 days. **Standard IA** objects
    are resilient to the loss of availability zones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One Zone Infrequent Access**: The object of this storage class is stored
    in just one availability zone, which makes it cheaper than **Standard IA**. The
    minimum object size and storage timeframe are the same as **Standard IA**. Objects
    from this storage class are less available and less resilient. This storage class
    is used when you have another copy, or if the data can be recreated. A **One Zone
    IA** storage class should be used for long-lived data that is non-critical and
    replaceable, and where access is infrequent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glacier**: This option is used for long-term archiving and backup. It can
    take anything from minutes to hours to retrieve objects in this storage class.
    The minimum storage timeframe is 90 days. You cannot use the Amazon Glacier API
    to access the objects moved from S3 to Glacier as part of object life cycle management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glacier Deep Archive**: The minimum storage duration of this class is 180
    days. This is the least expensive storage class and has a default retrieval time
    of 12 hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent Tiering**: This storage class is designed to reduce operational
    overheads. Users pay a monitoring fee and AWS selects a storage class between
    Standard (a frequent access tier) and Standard IA (a lower cost infrequent access
    tier) based on the access pattern of an object. This option is designed for long-lived
    data with unknown or unpredictable access patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through sets of rules, the transition between storage classes and deletion of
    the objects can be managed easily, and are referred to as **S3 Lifecycle Configurations**.
    These rules consist of actions. These can be applied to a bucket or a group of
    objects in that bucket defined by prefixes or tags. Actions can either be **Transition
    actions** or **Expiration actions**. Transition actions define the storage class
    transition of the objects following the creation of *a user-defined* number of
    days. Expiration actions configure the deletion of versioned objects, or the deletion
    of delete markers or incomplete multipart uploads. This is very useful for managing
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration is given in *Figure 5.1*. You can find more details available
    here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – A comparison table of S3 Storage classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_05_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – A comparison table of S3 Storage classes
  prefs: []
  type: TYPE_NORMAL
- en: Creating buckets to hold data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's see how to create a bucket, upload an object, and read the object
    using the AWS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step, we will check whether we have any buckets created by using
    the `aws s3 ls` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns nothing here. So, we will create a bucket now by using
    the `mb` argument. Let''s say the bucket name is `demo-bucket-baba` in the `us-east-1`
    region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we have created a bucket now, our next step is to copy a file to our bucket
    using the `cp` argument, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To validate the file upload operation via the AWS console, please log in to
    your AWS account and go to the AWS S3 console to see the same. The AWS S3 console
    lists the result as shown in *Figure 5.2*. The console may have changed by the
    time you're reading this book!![Figure 5.2 – AWS S3 screenshot to list your files
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_05_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'cp command and the --recursive parameter. To achieve this, you will have to
    create two buckets, demo-bucket-baba-copied and demo-bucket-baba-moved. The steps
    are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If all the commands are run successfully, then the original bucket should be
    empty at the end (as all the files have now been moved). NoteIn the certification
    exam, you will not find many questions on bucket- and object-level operations.
    However, it is always better to know the basic operations and the required steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The buckets must be deleted to avoid costs as soon as the hands-on is finished.
    The bucket has to be empty before you supply the `rb` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `demo-bucket-baba-moved` bucket is not empty, so we couldn''t remove the
    bucket. In such scenarios, use the `--force` parameter to delete the entire bucket
    and all its contents, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's take an example of a bucket, `test-bucket`, that has a prefix, `images`.
    This prefix contains four image files named `animal.jpg`, `draw-house.jpg`, `cat.jpg`,
    and `human.jpg`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, to delete the contents inside the images, the command will be as follows:
    **aws s3 rm s3://test-bucket/images –recursive**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bucket should now be empty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we are going to learn about object tags and object metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing between object tags and object metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s compare these two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object Tag**: An object tag is a **key-value** pair. AWS S3 object tags can
    help you filter analytics and metrics, categorize storage, secure objects based
    on certain categorizations, track costs based on certain categorization of objects,
    and much more besides. Object tags can be used to create life cycle rules to move
    objects to cheaper storage tiers. You can have a maximum of 10 tags added to an
    object and 50 tags to a bucket. A tag key can contain 128 Unicode characters,
    while a tag value can contain 256 Unicode characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object Metadata**: Object metadata is descriptive data describing an object.
    It consists of **name-value** pairs. Object metadata is returned as HTTP headers
    on objects. They are of two types: one is **System metadata**, and the other is
    **User-defined metadata**. User-defined metadata is a custom name-value pair added
    to an object by the user. The name must begin with **x-amz-meta**. You can change
    all system metadata such as storage class, versioning, and encryption attributes
    on an object. Further details are available here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Metadata names are case-insensitive, whereas tag names are case-sensitive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, we are going to learn about controlling access to buckets
    and objects on Amazon S3 through different policies, including the resource policy
    and the identity policy.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling access to buckets and objects on Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the object is stored in the bucket, the next major step is to manage access.
    S3 is private by default, and access is given to other users or groups or resources
    via several methods. This means that access to the objects can be managed via
    **Access Control Lists** (**ACLs**), **Public Access Settings**, **Identity Policies**,
    and **Bucket Policies**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some of these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: S3 bucket policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Principal` is rendered `*`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: By default, everything in S3 is private to the owner. If we want to make a prefix
    public to the world, then Resource changes to `arn:aws:s3:::my-bucket/some-prefix/*`,
    and similarly, if it is intended for a specific IAM user or IAM group, then those
    details go to the principal part in the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There can be conditions added to the bucket policy too. Let''s examine a use
    case where the organization wants to keep a bucket public and whitelist particular
    IP addresses. The policy would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'More examples are available in the AWS S3 developer guide, which is available
    here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Block public access** is a separate setting given to the bucket owner to
    avoid any kind of mistakes in bucket policy. In a real-world scenario, the bucket
    can be made public through bucket policy by mistake; to avoid such mistakes, or
    data leaks, AWS has provided this setting. It provides a further level of security,
    irrespective of the bucket policy. You can choose this while creating a bucket,
    or it can be set after creating a bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: '`us-east-1` in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**ACLs** are used to grant high-level permissions, typically for granting access
    to other AWS accounts. ACLs are one of the **Subresources** of a bucket or an
    object. A bucket or object can be made public quickly via ACLs. AWS doesn''t suggest
    doing this, and you shouldn''t expect questions about this on the test. It is
    good to know about this, but it is not as flexible as the **S3 bucket policy**.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's learn about the methods to protect our data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting data on Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to record every version of an object. Along
    with durability, Amazon provides several techniques to secure the data in S3\.
    Some of those techniques involve enabling versioning and encrypting the objects.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning helps you to roll back to a previous version if there is any problem
    with the current object during update, delete, or put operations.
  prefs: []
  type: TYPE_NORMAL
- en: Through encryption, you can control the access of an object. You need the appropriate
    key to read and write an object. We will also learn **Multi Factor Authentication
    (MFA)** for delete operations. Amazon also allows **Cross-Region Replication (CRR)**
    to maintain a copy of an object in another region, which can be used for data
    backup during any disaster, for further redundancy, or for the enhancement of
    data access speed in different regions.
  prefs: []
  type: TYPE_NORMAL
- en: Applying bucket versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now understand how we can enable bucket versioning with the help of
    some hands-on examples. Bucket versioning can be applied while creating a bucket
    from the AWS S3 console:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable versioning on a bucket from the command line, a bucket must be created
    first and then versioning can be enabled, as shown in the following example. In
    this example, I have created a bucket, `version-demo-mlpractice`, and enabled
    versioning through the `put-bucket-versioning` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have not created this bucket with any kind of encryption. So, if you run
    `aws s3api get-bucket-encryption --bucket version-demo-mlpractice`, then it will
    output an error that says the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`put-bucket-encryption` API. The command will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The same can be verified using the following command: `aws s3api get-bucket-encryption
    --bucket version-demo-mlpractice`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will learn more about encryption in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Applying encryption to buckets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You also need to understand how enabling versioning on a bucket would help.
    There are use cases where a file is updated regularly, and versions will be created
    for the same file. To simulate this scenario, try the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will create a file with versions written in it. We will
    overwrite it and retrieve it to check the versions in that file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon retrieval, we got the latest version of the file, in other words, `Version-2`
    in this case. To check each of the versions and the latest one of them, S3 provides
    the `list-object-versions` API, as shown here. From the JSON results, you can
    deduce the latest version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There may be a situation where you have to roll back to the earlier version
    of the current object. In the preceding example, the latest one is *Version-2*.
    You can make any desired version the latest or current version by parsing the
    *VersionId* sub resource to the `get-object` API call and uploading that object
    again. The other way is to delete the current or latest version by passing `versionId`
    to the `–version-id` parameter in the `delete-object` API request. More details
    about the API are available here: [https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you delete an object in a versioning-enabled bucket, it does not delete
    the object from the bucket. It just creates a marker called **DeleteMarker**.
    It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This means that the object is not deleted. You can list it by using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now the bucket has no objects as `version-doc.txt`, and you can verify this
    using the `aws s3 ls` command because that marker became the current version of
    the object with a new ID. If you try to retrieve an object that is deleted, which
    means a delete marker is serving the current version of the object, then you will
    get a `VersionId`, as shown in the following example commands. A simple delete
    request **(without the version ID)** will not delete the delete marker and create
    another delete marker with a unique version ID. So, it''s possible to have multiple
    delete markers for the same object. It is important to note at this point that
    it will consume your storage and you will be billed for it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon listing the bucket now, the older objects can be seen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we have already covered the exam topics and practiced most of the required
    concepts, we should delete the objects in the bucket and then delete the bucket
    to save on costs. This step deletes the versions of the object and, in turn, removes
    the object permanently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, the latest version is deleted by giving the version ID to it, followed
    by the other version ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can clearly see the empty bucket now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'AWS best practices suggest adding another layer of protection through **MFA
    delete**. Accidental bucket deletions can be prevented, and the security of the
    objects in the bucket is ensured. MFA delete can be enabled or disabled via the
    console and CLI. As documented in AWS docs, MFA delete requires two forms of authentication
    together: your security credentials, and the concatenation of a valid serial number,
    a space, and the six-digit code displayed on an approved authentication device.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'CRR helps you to separate data between different geographical regions. A typical
    use case can be business-as-usual activities during a disaster. If a region goes
    down, then another region can support the users if CRR is enabled. This improves
    the availability of the data. Another use case is to reduce latency if the same
    data is used by another compute resource, such as EC2 or AWS Lambda being launched
    in another region. You can also use CRR to copy objects to another AWS account
    that belongs to a different owner. There are a few important points that are worth
    noting down for the certification exam:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to use CRR, versioning has to be enabled on both the source and destination
    bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication is enabled on the source bucket by adding rules. As the source,
    either an entire bucket, or a prefix, or tags can be replicated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypted objects can also be replicated by assigning an appropriate encryption
    key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The destination bucket can be in the same account or in another account. You
    can change the storage type and ownership of the object in the destination bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For CRR, an existing role can be chosen or a new IAM role can be created too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be multiple replication rules on the source bucket, with priority
    accorded to it. Rules with higher priority override rules with lower priority.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you add a replication rule, only new versions an object that are created
    after the rules are enabled get replicated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If versions are deleted from the source bucket, then they are not deleted from
    the destination bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you delete an object from the source bucket, it creates a delete marker
    in said source bucket. That delete marker is not replicated to the destination
    bucket by S3\.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will cover the concept of securing S3 objects.
  prefs: []
  type: TYPE_NORMAL
- en: Securing S3 objects at rest and in transit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned about bucket default encryption, which
    is completely different from object-level encryption. Buckets are not encrypted,
    whereas objects are. A question may arise here: *what is bucket default encryption?*
    We will learn these concepts in this section. Data during transmission can be
    protected by using **Secure Socket Layer (SSL)** or **Transport Layer Security
    (TLS)** for the transfer of the HTTPS requests. The next step is to protect the
    data, where the authorized person can encode and decode the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to have different encryption settings on different objects in
    the same bucket. S3 supports **Client-Side Encryption (CSE)** and **Server-Side
    Encryption (SSE)** for objects at rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client-Side Encryption**: A client uploads the object to S3 via the S3 endpoint.
    In CSE, the data is encrypted by the client before uploading to S3\. Although
    the transit between the user and the S3 endpoint happens in an encrypted channel,
    the data in the channel is already encrypted by the client and can''t be seen.
    In transit, encryption takes place by default through HTTPS. So, AWS S3 stores
    the encrypted object and cannot read the data in any format at any point in time.
    In CSE, the client takes care of encrypting the object content. So, control stays
    with the client in terms of key management and the encryption-decryption process.
    This leads to a huge amount of CPU usage. S3 is only used for storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-Side Encryption**: A client uploads the object to S3 via the S3 endpoint.
    Even though the data in transit is through an encrypted channel that uses HTTPS,
    the objects themselves are not encrypted inside the channel. Once the data hits
    S3, then it is encrypted by the S3 service. In SSE, you trust S3 to perform encryption-decryption,
    object storage, and key management. There are three types of SSE techniques available
    for S3 objects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) SSE-C
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) SSE-S3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) SSE-KMS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`PUT` operation, the user has to provide a key and an object to S3\. S3 encrypts
    the object using the key provided and attaches the hash (a cipher text) to the
    object. As soon as the object is stored, S3 discards the encryption key. This
    generated hash is one-way and cannot be used to generate a new key. When the user
    provides a `GET` operation request along with the decryption key, the hash identifies
    whether the specific key was used for encryption. Then, S3 decrypts and discards
    the key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation, the user just provides the unencrypted object. S3 creates
    a master key to be used for the encryption process. No one can change anything
    on this master key as this is created, rotated internally, and managed by S3 end
    to end. This is a unique key for the object. It uses the AES-256 algorithm by
    default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-Side Encryption with Customer Master Keys stored in AWS Key Management
    Service (SSE-KMS)**: **AWS Key Management Service (KMS)** manages the **Customer
    Master Key (CMK)**. AWS S3 collaborates with AWS KMS and generates an AWS-managed
    CMK. This is the default master key used for **SSE-KMS**. Every time an object
    is uploaded, S3 uses a dedicated key to encrypt that object and that key is a
    **Data Encryption Key (DEK)**. The DEK is generated by KMS using the CMK. S3 is
    provided with both a plain-text version and an encrypted version of the DEK. The
    plain-text version of DEK is used to encrypt the object and then discarded. The
    encrypted version of DEK is stored along with the encrypted object. When you''re
    using SSE-KMS, it is not necessary to use the default CMK that is created by S3\.
    You can create and use a customer managed CMK, which means you can control the
    permission on it as well as the rotation of the key material. So, if you have
    a regulatory board in your organization that is concerned with rotation of the
    key or the separation of roles between encryption users and decryption users,
    then SSE-KMS is the solution. Logging and auditing is also possible on SSE-KMS
    to track the API calls made against keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn about some of the data stores used with EC2
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Using other types of data stores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Elastic Block Store (EBS)** is used to create volumes in an availability
    zone. The volume can only be attached to an EC2 instance in the same availability
    zone. Amazon EBS provides both **SSD (Solid State Drive)** and **HDD (Hard Disk
    Drive)** types of volumes. For SSD-based volumes, the dominant performance attribute
    is **IOPS** (**Input Output Per Second**), and for HDD it is throughput, which
    is generally measured as MiB/s. The following table shown in *Figure 5.3* provides
    an overview of the different volumes and types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Different volumes and their use cases'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_05_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Different volumes and their use cases
  prefs: []
  type: TYPE_NORMAL
- en: '**EBS** is resilient to an availability zone (AZ). If, for some reason, an
    AZ fails, then the volume cannot be accessed. To avoid such scenarios, **snapshots**
    can be created from the EBS volumes and snapshots are stored in S3\. Once the
    snapshot arrives at S3, the data in the snapshot becomes region-resilient. The
    first snapshot is a full copy of data on the volume and, from then onward, snapshots
    are incremental. Snapshots can be used to clone a volume. As the snapshot is stored
    in S3, a volume can be cloned in any AZ in that region. Snapshots can be shared
    between regions and volumes can be cloned from them during disaster recovery.'
  prefs: []
  type: TYPE_NORMAL
- en: AWS KMS manages the CMK. AWS KMS uses an AWS-managed CMK for EBS, or AWS KMS
    can use a customer-managed CMK. CMK is used by EBS when an encrypted volume is
    created. CMK is used to create an encrypted DEK, which is stored with the volume
    on the physical disk. This DEK can only be decrypted using KMS, assuming the entity
    has access to decrypt. When a snapshot is created from the encrypted volume, the
    snapshot is encrypted with the same DEK. Any volume created from this snapshot
    also uses that DEK.
  prefs: []
  type: TYPE_NORMAL
- en: '**Instance Store** volumes are the block storage devices physically connected
    to the EC2 instance. They provide the highest performance, as the **ephemeral
    storage** attached to the instance is from the same host where the instance is
    launched. EBS can be attached to the instance at any time, but the instance store
    must be attached to the instance at the time of its launch; it cannot be attached
    later, once the instance is launched. If there is an issue on the underlying host
    of an EC2 instance, then the same instance will be launched on another host with
    a new instance store volume and the earlier instance store (ephemeral storage)
    and older data is lost. The size and capabilities of the attached volumes depend
    on the instance types and can be found in more detail here: [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic File System (EFS)** provides a network-based filesystem that can
    be mounted within Linux EC2 instances and can be used by multiple instances at
    once. It is an implementation of **NFSv4**. It can be used in general-purpose
    mode, max I/O performance mode (for scientific analysis or parallel computing),
    bursting mode, and provisioned throughput mode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, in the case of instance stores, the data is volatile. As soon as
    the instance is lost, the data is lost from the instance store. That is not the
    case for EFS. EFS is separate from the EC2 instance storage. EFS is a file store
    and is accessed by multiple EC2 instances via mount targets inside a VPC. On-premises
    systems can access EFS storage via hybrid networking to the VPC, such as **VPN**
    or **Direct Connect**. EFS also supports two types of storage classes: Standard
    and Infrequent Access. Standard is used for frequently accessed data. Infrequent
    Access is the cost-effective storage class for long-lived, less frequently accessed
    data. Life cycle policies can be used for the transition of data between storage
    classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: An instance store is preferred for max I/O requirements and if the data is replaceable
    and temporary.
  prefs: []
  type: TYPE_NORMAL
- en: Relational Database Services (RDSes)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most commonly featuring exam topics in AWS exams. You should
    have sufficient knowledge prior to the exam. In this section, we will learn about
    Amazon's RDS.
  prefs: []
  type: TYPE_NORMAL
- en: AWS provides several relational databases as a service to its users. Users can
    run their desired database on EC2 instances, too. The biggest drawback is that
    the instance is only available in one availability zone in a region. The EC2 instance
    has to be administered and monitored to avoid any kind of failure. Custom scripts
    will be required to maintain a data backup over time. Any database major or minor
    version update would result in downtime. Database instances running on an EC2
    instance cannot be easily scaled if the load increases on the database as replication
    is not an easy task.
  prefs: []
  type: TYPE_NORMAL
- en: RDS provides managed database instances that can themselves hold one or more
    databases. Imagine a database server running on an EC2 instance that you do not
    have to manage or maintain. You need only access the server and create databases
    in it. AWS will manage everything else, such as the security of the instance,
    the operating system running on the instance, the database versions, and high
    availability of the database server. RDS supports multiple engines such as MySQL,
    Microsoft SQL Server, MariaDB, Amazon Aurora, Oracle, and PostgreSQL. You can
    choose any of these based on your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of Amazon RDS is a database instance, which can support multiple
    engines and can have multiple databases created by the user. One database instance
    can be accessed only by using the database **CNAME** (CNAME is an alias for a
    canonical name in a domain name system database) of the primary instance. RDS
    uses standard database engines. So, accessing the database using some sort of
    tool in a self-managed database server is the same as accessing Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: As we have now understood the requirements of Amazon RDS, let's understand the
    failover process in Amazon RDS. We will cover what services Amazon offers if something
    goes wrong with the RDS instance.
  prefs: []
  type: TYPE_NORMAL
- en: Managing failover in Amazon RDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDS instances can be **single-AZ** or **multi-AZ**. In multi-AZ, multiple instances
    work together, similar to an active-passive failover design.
  prefs: []
  type: TYPE_NORMAL
- en: For a single-AZ RDS instance, storage can be allocated for that instance to
    use. In a nutshell, a single-AZ RDS instance has one attached block store (EBS
    storage) available in the same availability zone. This makes the databases and
    the storage of the RDS instance vulnerable to availability zone failure. The storage
    allocated to the block storage can be SSD (gp2 or io1) or magnetic. To secure
    the RDS instance, it is advised to use a security group and provide access based
    on requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-AZ is always the best way to design the architecture to avoid any failure
    and keep the applications highly available. With multi-AZ features, a standby
    replica is kept in sync **synchronously** with the primary instance. The standby
    instance has its own storage in the assigned availability zone. A standby replica
    cannot be accessed directly, because all RDS access is via a single database CNAME.
    You can't access the standby unless a failover happens. The standby provides no
    performance benefit, but it does constitute an improvement in terms of availability
    of the RDS instance. It can only happen in the same region, another AZ's subnet
    in the same region inside the VPC. When a multi-AZ RDS instance is online, you
    can take a backup from the standby replica without affecting the performance.
    In a single-AZ instance, availability and performance issues can be significant
    during backup operation.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the workings of multi-AZ, let's take an example of a single-AZ
    instance and expand it to multi-AZ.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an RDS instance running in availability zone `AZ-A` of the
    `us-east-1` region inside a VPC named `db-vpc`. This becomes a primary instance
    in a single-AZ design of an RDS instance. In this case, there will be storage
    allocated to the instance in the *AZ-A* availability zone. Once you opt for multi-AZ
    deployment in another availability zone called *AZ-B*, AWS creates a standby instance
    in availability zone *AZ-B* of the *us-east-1* region inside the *db-vpc* VPC
    and allocates storage for the standby instance in *AZ-B* of the *us-east-1* region.
    Along with that, RDS will enable **synchronous replication** from the primary
    instance to the standby replica. As we have learned earlier, the only way to access
    our RDS instance is via the database CNAME, hence, the access request goes to
    the RDS primary instance. As soon as a write request comes to the endpoint, it
    writes to the primary instance. Then it writes the data to the hardware, which
    is the block storage attached to the primary instance. At the same time, the primary
    instance replicates the same data to the standby instance. Finally, the standby
    instance commits the data to its block storage.
  prefs: []
  type: TYPE_NORMAL
- en: The primary instance writes the data into the hardware and replicates the data
    to the standby instance in parallel, so there is a minimal time lag (almost nothing)
    between the data commit operations in their respective hardware. If an error occurs
    with the primary instance, then RDS detects this and changes the database endpoint
    to the standby instance. The clients accessing the database may experience a very
    short interruption with this. This failover occurs within 60-120 seconds. It does
    not provide a fault-tolerant system because there will be some impact during the
    failover operation.
  prefs: []
  type: TYPE_NORMAL
- en: You should now understand failover management on Amazon RDS. Let's now learn
    about taking automatic RDS backup, using snapshots to restore in the event of
    any failure, and read replicas in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Taking automatic backup, RDS snapshots, and restore and read replicas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how RDS **Automatic Backup** and **Manual Snapshots**
    work. These features come with Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a database that is scheduled to take a backup at 5 A.M. every
    day. If the application fails at 11 A.M., then it is possible to restart the application
    from the backup taken at 11 A.M. with the loss of 6 hours' worth of data. This
    is called 6 hours **RPO (Recovery Point Objective)**. So, RPO is defined as the
    time between the most recent backup and the incident and this defines the amount
    of data loss. If you want to reduce this, then you have to schedule more incremental
    backups, which increases the cost and backup frequency. If your business demands
    a lower RPO value, then the business must spend more on meeting the technical
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, according to our example, an engineer was assigned this task to bring the
    system online as soon as the disaster occurred. The engineer managed to bring
    the database online at 2 P.M. on the same day by adding a few extra hardware components
    to the current system and installed some updated versions of the software. This
    is called 3 hours **RTO (Recovery Time Objective)**. So, RTO is determined as
    the time between the disaster recovery and full recovery. RTO values can be reduced
    by having spare hardware and documenting the restoration process. If the business
    demands a lower RTO value, then your business must spend more money on spare hardware
    and effective system setup to perform the restoration process.
  prefs: []
  type: TYPE_NORMAL
- en: In RDS, RPO and RTO play an important role in the selection of **Automatic Backups**
    and **Manual Snapshots**. Both of these backup services use AWS-managed S3 buckets,
    which means it cannot be visible in the user's AWS S3 console. It is region-resilient
    because the backup is replicated into multiple availability zones in the AWS region.
    In case of a single-AZ RDS instance, the backup happens from the single available
    data store, and for a multi-AZ enabled RDS instance, the backup happens from the
    standby data store (the primary store remains untouched as regards the backup).
  prefs: []
  type: TYPE_NORMAL
- en: The snapshots are manual against RDS instances and these are stored in the AWS-managed
    S3 bucket. The first snapshot of an RDS instance is a full copy of the data and
    the onward snapshots are incremental, reflecting the change in the data. In terms
    of the time taken for the snapshot process, it is higher for the first one and,
    from then on, the incremental backup is quicker. When any snapshot occurs, it
    can impact the performance of the single-AZ RDS instance, but not the performance
    of a multi-AZ RDS instance as this happens on the standby data storage. Manual
    snapshots do not expire, have to be cleared automatically, and they live past
    the termination of an RDS instance. When you delete an RDS instance, it suggests
    making one final snapshot on your behalf and it will contain all the databases
    inside your RDS instance (there is not just a single database in an RDS instance).
    When you restore from a manual snapshot, you restore to a single point in time
    and that affects the RPO.
  prefs: []
  type: TYPE_NORMAL
- en: To automate this entire process, you can choose a time window when these snapshots
    can be taken. This is called automatic backups. These time windows can be managed
    wisely to essentially lower the RPO value of the business. Automatic backups have
    a retention period of 0 to 35 days, with 0 being disabled and the maximum is 35
    days. To quote AWS documentation, retained automated backups contain system snapshots
    and transaction logs from a database instance. They also include database instance
    properties such as allocated storage and a database instance class, which are
    required to restore it to an active instance. Databases generate transaction logs,
    which contain the actual change in data in a particular database. These transaction
    logs are also written to S3 every 5 minutes by RDS. Transaction logs can also
    be replayed on top of the snapshots to restore to a point in time of 5 minutes'
    granularity. Theoretically, the RPO can be a 5-minute point in time.
  prefs: []
  type: TYPE_NORMAL
- en: When you perform a restore, RDS creates a new RDS instance, which means a new
    database endpoint to access the instance. The applications using the instances
    have to point to the new address, which significantly affects the RTO. This means
    that the restoration process is not very fast, which affects the RTO. To minimize
    the RTO during a failure, you may consider replicating the data. With replicas,
    there is a high chance of replicating the corrupted data. The only way to overcome
    this is to have snapshots and an RDS instance can be restored to a particular
    point in time prior to the corruption. **Amazon RDS Read Replicas** are unlike
    the multi-AZ replicas. In multi-AZ RDS instances, the standby replicas cannot
    be used directly for anything unless a primary instance fails, whereas **Read
    Replicas** can be used directly, but only for read operations. Read replicas have
    their own database endpoints and read-heavy applications can directly point to
    this address. They are kept in sync **asynchronously** with the primary instance.
    Read replicas can be created in the same region as the primary instance or in
    a different region. Read replicas in other regions are called **Cross-Region Read
    Replicas** and this improves the global performance of the application.
  prefs: []
  type: TYPE_NORMAL
- en: As per AWS documentation, five direct read replicas are allowed per database
    instance and this helps to scale out the read performances. Read replicas have
    a very low RPO value due to asynchronous replication. They can be promoted to
    a read-write database instance in the case of a primary instance failure. This
    can be done quickly and it offers a fairly low RTO value.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about Amazon's own database engine, Amazon
    Aurora.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to Amazon Aurora with multi-master capabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Aurora is the most reliable relational database engine developed by Amazon
    to deliver speed in a simple and cost-effective manner. Aurora uses a cluster
    of single primary instances and zero or more replicas. Aurora's replicas can give
    you the advantage of both read replicas and multi-AZ instances in RDS. Aurora
    uses a shared cluster volume for storage and is available to all compute instances
    of the cluster (a maximum of 64 TiB). This allows the Aurora cluster to provision
    faster and improves availability and performance. Aurora uses SSD-based storage,
    which provides high IOPS and low latency. Aurora does not ask you to allocate
    storage, unlike other RDS instances; it is based on the storage that you use.
  prefs: []
  type: TYPE_NORMAL
- en: Aurora clusters have multiple endpoints, including **Cluster Endpoint** and
    **Reader Endpoint**. If there are zero replicas, then the cluster endpoint is
    the same as the reader endpoint. If there are replicas available, then the reader
    endpoint is load balanced across the reader endpoints. Cluster endpoints are used
    for reading/writing, while reader endpoints are intended for reading from the
    cluster. If you add more replicas, then AWS manages load balancing under the hood
    for the new replicas.
  prefs: []
  type: TYPE_NORMAL
- en: When failover occurs, the replicas are promoted to read/write mode and this
    takes some time. This can be avoided in a **Multi-Master** mode of an Aurora cluster.
    This allows multiple instances to perform reads and writes at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Storing columnar data on Amazon Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Redshift is not used for real-time transaction use, but it is used for
    data warehouse purposes. It is designed to support huge volumes of data at a petabyte
    scale. It is a column-based database used for analytics purpose, long-term processing,
    tending, and aggregation. **Redshift Spectrum** can be used to query data on S3
    without loading data to the Redshift cluster (a Redshift cluster is required though).
    It's not an OLTP, but an OLAP. **AWS QuickSight** can be integrated with Redshift
    for visualization, with a SQL-like interface that allows you to connect using
    JDBC/ODBC connections for querying the data.
  prefs: []
  type: TYPE_NORMAL
- en: Redshift uses a clustered architecture in one AZ in a VPC with faster network
    connectivity between the nodes. It is not high availability by design as it is
    tightly coupled to the AZ. A Redshift cluster has a **Leader Node**, and this
    node is responsible for all the communication between the client and the computing
    nodes of the cluster, query planning, and aggregation. **Compute Nodes** are responsible
    for running the queries submitted by the leader lode and for storing the data.
    By default, Redshift uses a public network for communicating with external services
    or any AWS services. With **Enhanced VPC Routing**, it can be controlled via customized
    networking settings.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon DynamoDB for NoSQL database as a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon DynamoDB is a NoSQL database-as-a-service product within AWS. It''s
    a fully managed key/value and document database. Accessing DynamoDB is easy via
    its endpoint. The input and output throughputs can be managed or scaled manually
    or in automatic fashion. It also supports data backup, point-in-time recovery,
    and data encryption. We will not cover the DynamoDB table structure or key structure
    in this chapter as this is not required for the certification exam. However, it
    is good to have a basic knowledge of them. For more details, please refer to the
    AWS docs available here: [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html](
    https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about various data storage services from Amazon,
    and how to secure data through various policies and use these services. If you
    are working on machine learning use cases, then you may encounter such scenarios
    where you have to choose an effective data storage service for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the processing of stored data.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To set the region that an S3 bucket is stored in, you must first create the
    bucket and then set the region separately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Is it mandatory to have both the source bucket and destination bucket in the
    same region in order to copy the contents of S3 buckets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By default, objects are private in a bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By WS, a S3 object is immutable and you can only perform put and delete. Rename
    is GET and PUT of the same object with a different name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If a user has stored an unversioned object and a versioned object in the same
    bucket, then the user can only delete the unversioned object. Versioned objects
    cannot be deleted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Versioning applies to a complete bucket and not objects. If versioning is enabled
    on a bucket, then it can only be suspended; it cannot be disabled.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A simple delete request on a delete marker will:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Delete the delete marker
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Create a copy of the delete marker
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Not delete the delete marker, but it will create another delete marker
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Delete the original version of the object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scaling and performance can be improved via RDS multi-AZ instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An RDS multi-AZ has nothing to do with scaling and performance. It is used for
    failover.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What protocol does EFS use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. SMB
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. NFS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. EBS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. HTTP
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which operating systems does EFS support?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Linux only
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Windows only
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Both Windows and Linux
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Neither Windows nor Linux
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Is EFS a private service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Yes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. No, it's a public service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. It's both a private and a public service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. It's neither a private nor a public service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which two of the following are correct?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Multi-AZ:Same Region::Read Replica:Multiple Region
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Multi-AZ:Multiple Region::Read Replica:Same Region
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Multi-AZ:Synchronous Replication::Read Replica:Asynchronous Replication
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Multi-AZ:ASynchronous Replication::Read Replica:Synchronous Replication
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following is correct?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Read replicas are read-only instances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Read replicas are read-write instances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Read replicas are write-only instances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Read replicas are read-only instances, until promoted to read-write.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Where is EFS accessible from?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Inside a VPC only
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Via AWS endpoints
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Anywhere with a public internet connection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Inside a VPC or any on-premises locations connected to that VPC via a hybrid
    network
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which three of the following are true?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Instance store volumes are persistent storage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Instance store volumes are temporary (ephemeral) storage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Data stored on instance store volumes can be lost if a hardware failure occurs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Data stored on instance store volumes can be lost when an EC2 instance restarts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: E. Data stored on instance store volumes can be lost when an EC2 instance stops
    and starts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hardware failure can change the underlying host. So, there is no guarantee of
    the instance store volume. When you stop the instance and start it again, the
    instance store volume is lost due to the change of host. Instance restarting is
    different from stop and start; it means an operating system restart.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to enable encryption at rest using EC2 and EBS, you need to…
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Configure encryption when creating the EBS volume.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Configure encryption using the appropriate operating system's filesystem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Configure encryption using X.509 certificates.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Mount the EBS volume in S3 and then encrypt the bucket using a bucket policy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following is an action you cannot perform on an EBS snapshot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Create an image from a snapshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Create an EBS volume from a snapshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Share a snapshot with another AWS account.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Make an unencrypted copy of an encrypted snapshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With EBS, you need to do the following (choose two).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Create an encrypted volume from a snapshot of another encrypted volume.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Create an encrypted volume from an encrypted snapshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Create an encrypted snapshot from an unencrypted snapshot by creating an
    encrypted copy of the unencrypted snapshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Encrypt an existing volume.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For more on EBS, visit the following link: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_c%20onsiderations](
    https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_c%20onsiderations).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. B
  prefs: []
  type: TYPE_NORMAL
- en: 2\. B
  prefs: []
  type: TYPE_NORMAL
- en: 3\. A
  prefs: []
  type: TYPE_NORMAL
- en: 4\. A
  prefs: []
  type: TYPE_NORMAL
- en: 5\. B
  prefs: []
  type: TYPE_NORMAL
- en: 6\. C
  prefs: []
  type: TYPE_NORMAL
- en: 7\. B
  prefs: []
  type: TYPE_NORMAL
- en: 8\. B
  prefs: []
  type: TYPE_NORMAL
- en: 9\. A
  prefs: []
  type: TYPE_NORMAL
- en: 10\. B
  prefs: []
  type: TYPE_NORMAL
- en: 11\. A, C
  prefs: []
  type: TYPE_NORMAL
- en: 12\. D
  prefs: []
  type: TYPE_NORMAL
- en: 13\. D
  prefs: []
  type: TYPE_NORMAL
- en: 14\. B, C, E
  prefs: []
  type: TYPE_NORMAL
- en: 15\. A
  prefs: []
  type: TYPE_NORMAL
- en: 16\. D
  prefs: []
  type: TYPE_NORMAL
- en: 17\. A, C
  prefs: []
  type: TYPE_NORMAL
