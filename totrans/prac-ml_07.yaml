- en: Chapter 7. Association Rules based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered Decision tree, instance and kernel-based supervised and unsupervised
    learning methods in the previous chapters. We also explored the most commonly
    used algorithms across these learning algorithms in the previous chapters. In
    this chapter, we will cover association rule based learning and, in specific,
    Apriori and FP-Growth algorithms among others. We will learn the basics of this
    technique and get hands-on implementation guidance using Apache Mahout, R, Julia,
    Apache Spark, and Python. The following figure depicts different learning models
    covered in this book. The techniques highlighted in orange will be dealt with
    in detail in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Association Rules based learning](img/B03980_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following topics are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics and core principles of association rules based learning
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core use cases for association rule such as the Market Basket problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key terms such as itemsets, lift, support, confidence and frequent itemsets,
    and rule generation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep dive into association rule based algorithms such as Apriori and FP-Growth;
    comparing and contrasting Apriori and FP-Growth in the context of large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview and purpose of some advanced association rules concepts such as correlation
    and sequential rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample implementation for Apache Mahout, R, Apache Spark, Julia and Python
    (scikit-learn) libraries and modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Association rule-based Machine learning deals with finding frequent patterns,
    associations, and transactions that can be used for classification and prediction
    requirements. The association rule based learning process is as follows: given
    a set of transactions, finding rules and using these rules to predict the occurrence
    of an item based on the occurrences of other items in the transaction is Association
    rule based learning. The following diagram represents the scope of Machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rules based learning](img/B03980_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Association rule – a definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An association rule is a representation of a pattern that describes the probability
    with which an event occurs, given the occurrence of another event. Usually, the
    syntax for association rules follows the *if...then* statements that relate two
    sets of unrelated data from the repository. In short, it helps find the relationship
    between objects that are frequently used together. The goal of association rules
    is to find all the sets of items that have greater support than minimum support
    using the large dataset to predict the rules that have confidence greater than
    the minimum confidence. One of the most common examples where association rule
    is used is the Market Basket example. To elaborate the Market basket example,
    if a customer buys an iPad, he or she is likely to buy an iPad case as well.
  prefs: []
  type: TYPE_NORMAL
- en: Two important criteria are used in association rules, **Support** and **Confidence**.
    Every association rule should have a minimum Confidence and minimum Support at
    the same time. This is usually user-defined.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at what Support, Confidence, and lift measures are. Let's consider
    the same example as explained previously, *If X then Y*. where *X* is buying an
    iPad and *Y* is buying an iPad case.
  prefs: []
  type: TYPE_NORMAL
- en: Then Support is defined as the frequency with which *X* and *Y* are purchased
    together over the total number of purchases or transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Confidence can be defined as the frequency with which *X* and *Y* are purchased
    together over the frequency with which *X* is purchased in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Lift is defined as the Support over the Support for *X* times the Support for
    *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_15.jpg)![Association rule
    – a definition](img/B03980_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Before understanding the significance of these measures, let's look at the terms
    used in this context as an example. A collection of items in a warehouse called
    itemset are represented as *I = { i[1], i[2], …. i[n]}*, a set of all transactions
    where each transaction consists of a subset of itemset is represented as *T =
    { t[1], t[2], …. t[n]}*, where *t*[x] is a subset of *I* with a **Unique Transaction
    Identifier** (**UTI**).
  prefs: []
  type: TYPE_NORMAL
- en: Let's represent items, transactions, and measures using an example now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider five items and five transactions as depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I = {iPad(A), iPad case(B), iPad scratch guard(C), Apple care (D), iPhone
    (E)}*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*T = {{ iPad, iPad case, iPad scratch guard }, { iPad, iPad scratch guard,
    Apple care }, { iPad case, iPad scratch guard, Apple care }, { iPad, Apple care,
    iPhone }, { iPad case, iPad scratch guard, iPhone }}*'
  prefs: []
  type: TYPE_NORMAL
- en: The table below shows the support, confidence and lift values for each of the
    identified rules.
  prefs: []
  type: TYPE_NORMAL
- en: '| # | Rule | Support | Confidence | Lift |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | If iPad (*A*) is purchased, iPhone (*D*) is also purchased | 2/5
    | 2/3 | 10/9 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | If iPad scratch guard(*C*) is purchased, iPad (*A*) is also purchased
    | 2/5 | 2/4 | 5/6 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | If iPad (*A*) is purchased, iPad scratch guard (*C*) is also purchased
    | 2/5 | 2/3 | 5/6 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | If iPad case(*B*) and iPad scratch guard (*C*) are purchased, then
    apple care (*D*) is also purchased | 1/5 | 1/3 | 5/9 |'
  prefs: []
  type: TYPE_TB
- en: 'From these itemsets, based on the support and confidence computations, frequent
    itemset(s) can be determined. The goal of association rule mining is to find the
    rules that satisfy the criteria given here:'
  prefs: []
  type: TYPE_NORMAL
- en: support ≥ minsup (minimum support) threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: confidence ≥ minconf (minimum confidence) threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the steps involved in frequent itemset generation and mining
    association rules:'
  prefs: []
  type: TYPE_NORMAL
- en: List all the possible association rules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the support and confidence for each rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prune the rules that fail to satisfy the minsup and minconf threshold values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach is called the brute force approach and is usually known to be
    computationally prohibitive.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rules originating from the same itemset usually have the same support, but vary
    with confidence. The minimum support (minsup) and the minimum confidence (minconf)
    are the values that are agreed upon during the problem definition statement. For
    example, minimum support and confidence can take percentage values like 75% and
    85% respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid all expensive computations, we can simplify this process into two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequent itemset generation**: This requires generating all the itemsets
    with support ≥ minsup'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rule generation**: From the identified frequent itemsets, generate rules
    with the highest confidence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When there are five items, there are 32 candidate itemsets. The following figure
    depicts the itemset combination for five items: **A**, **B**, **C**, **D**, and
    **E**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The possible number of itemsets and rules, given the number of items is defined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given *d* unique items:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Total number of possible itemsets = 2*^d'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard formula for computing total possible association rules is defined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, if *d* is equivalent to 6, then the *total number of possible itemsets
    = 2*^d *= 64*
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the *total number of possible association rules = 602 rules*
  prefs: []
  type: TYPE_NORMAL
- en: The following graph shows the relationship between the number of items and possible
    association rules.
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule – a definition](img/B03980_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Efficient ways of generating frequent itemsets and association rules determine
    the efficiency of the association rule algorithms. In the next sections, we will
    cover the Apriori and FP-Growth algorithms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Apriori algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will cover the Apriori algorithm step-by-step using an
    example. The Apriori algorithm is as stated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apriori algorithm](img/B03980_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apriori principle—for all the frequent itemsets, the subsets must also be frequent.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the five items (from the example in the previous section)
  prefs: []
  type: TYPE_NORMAL
- en: '*I = {iPad(A), iPad case(B), iPad scratch guard(C), Apple care (D), iPhone
    (E)}*, and the following nine transactions. Let''s assume that the minimum Support
    count is two:'
  prefs: []
  type: TYPE_NORMAL
- en: '| TID | The purpose or meaning in the context of Machine learning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | iPad(*A*), iPad case(*B*), and iPhone(*E*) |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | iPad case(*B*) and Apple care(*D*) |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | iPad case(*B*) and iPad scratch guard(*C*) |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | iPad(*A*), iPad case(*B*), and Apple care(*D*) |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | iPad(*A*) and Apple care(*D*) |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | iPad case(*B*) and iPad scratch guard(*C*) |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | iPad(*A*) and Apple care(*D*) |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | iPad(*A*), iPad case(*B*), iPad scratch guard(*C*), and iPhone (*E*)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | iPad(*A*), iPad case(*B*), and iPad scratch guard(*C*) |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s debug the previous algorithm using the previous datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the number of occurrences for each item from the previous transactions
    (*C*[1]):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Itemset | Support count |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A)}* | 6 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B)}* | 7 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad scratch guard(C)}* | 6 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{Apple care(D)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPhone(E)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Determine Frequent 1—Itemsets (*L*[1]) from *C*[1]:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Itemset | Support count |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A)}* | 6 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B)}* | 7 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad scratch guard(C)}* | 6 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{Apple care(D)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPhone(E)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Generate 2—Itemset candidates (*C*[2]) and scan the dataset for Support count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Itemset | Support count |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), iPad case(B)}* | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), iPad scratch guard(C)}* | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), Apple care(D)}* | 1 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), iPhone(E)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B), iPad scratch guard(C)}* | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B), Apple care(D)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B), iPhone(E)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad scratch guard(C), Apple care(D)}* | 0 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad scratch guard(C), iPhone(E)}* | 1 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{Apple care(D), iPhone(E)}* | 0 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Determine Frequent 2—Itemsets (*L*[2]) from *C*[2]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Itemset | Support count |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), iPad case(B)}* | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), iPad scratch guard(C)}* | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A), Apple care(D)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B), iPad scratch guard(C)}* | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B), Apple care(D)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B), iPhone(E)}* | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Generate 3—Itemset candidates (*C*[3]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, scan the dataset for Support count and frequent 3—Itemset identification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is similar to the previously followed steps, but we will demonstrate how
    pruning can be applied to identify the frequent itemset, based on the Apriori
    principle effectively. First, we identify the possible subset itemsets. We then
    check whether there are any of the subset itemsets that do not belong to the frequent
    itemset list. If not found, we eliminate that 3—Itemset possibility.
  prefs: []
  type: TYPE_NORMAL
- en: '| C3 | Itemset | Possible subset itemsets |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1**✓ | *{A,B,C}* | *{A,B}*✓ | *{A,C}*✓ | *{B,C}*✓ |'
  prefs: []
  type: TYPE_TB
- en: '| **2**✓ | *{A,B,D}* | *{A,B}*✓ | *{A,D}*✓ | *{B,D}*✓ |'
  prefs: []
  type: TYPE_TB
- en: '| **3**✕ | *{A,C,D}* | *{A,C}*✓ | *{A,D}*✓ | *{C,D}*✕ |'
  prefs: []
  type: TYPE_TB
- en: '| **4**✕ | *{B,C,D}* | *{B,C}*✓ | *{B,D}*✓ | *{C,D}*✕ |'
  prefs: []
  type: TYPE_TB
- en: '| **5**✕ | *{B,C,E}* | *{B,C}*✓ | *{B,E}*✕ | *{C,E}*✕ |'
  prefs: []
  type: TYPE_TB
- en: '| **6**✕ | *{B,D,E}* | *{B,D}*✓ | *{B,E}*✕ | *{D,E}*✕ |'
  prefs: []
  type: TYPE_TB
- en: 'In the previous table, the ✕ itemsets are pruned using the Apriori technique,
    and the data from step 4 (*L*[2]) is used. The itemsets are represented using
    the item codes *A*, *B*, *C*, *D*, and *E* instead of the actual names for ease
    of understanding. The 3—itemset candidates can be identified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| C[3] | Itemset | Support Count |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | *{iPad(A), iPad case (B), iPad scratch guard(C)}* | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | *{iPad(A), iPad case (B), Apple care(C)}* | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, the Frequent 3—Itemsets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| L[3] | Itemset | Support Count |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | *{iPad(A), iPad case (B), iPad scratch guard(C)}* | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | *{iPad(A), iPad case (B), Apple care(C)}* | 2 |'
  prefs: []
  type: TYPE_TB
- en: Generate 4— Itemset candidates (*C*[4]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, scan the dataset for the Support count and frequent 3—Itemset identification
    (*L*[4]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, the pruning stops here, as there are no further *C*[3] options
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Apriori algorithm is not efficient as it requires multiple dataset scans.
    However, there are some techniques to improve the efficiency. Some of them are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If a transaction does not contain any frequent item-sets, it is not useful and
    need not participate in the subsequent scans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any itemset that is frequent in the dataset should be frequent in at least one
    partition of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of sampling, to include a subset of the whole data set with a lower
    support threshold, will yield more efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule generation strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s say we have a frequent itemset *{A, B, C, D}*, and the possible candidate
    rules are:'
  prefs: []
  type: TYPE_NORMAL
- en: ABC→D
  prefs: []
  type: TYPE_NORMAL
- en: ABD→C
  prefs: []
  type: TYPE_NORMAL
- en: ACD→B
  prefs: []
  type: TYPE_NORMAL
- en: BCD→A
  prefs: []
  type: TYPE_NORMAL
- en: AB→CD
  prefs: []
  type: TYPE_NORMAL
- en: AC→BD
  prefs: []
  type: TYPE_NORMAL
- en: AD→BC
  prefs: []
  type: TYPE_NORMAL
- en: BC→AD
  prefs: []
  type: TYPE_NORMAL
- en: BD→AC
  prefs: []
  type: TYPE_NORMAL
- en: CD→AB
  prefs: []
  type: TYPE_NORMAL
- en: A→BCD
  prefs: []
  type: TYPE_NORMAL
- en: B→ACD
  prefs: []
  type: TYPE_NORMAL
- en: C→ABD
  prefs: []
  type: TYPE_NORMAL
- en: D→ABC
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard formula is, for every k items in the frequent itemset, *2k-2*
    possible candidate rules can be defined. Only the rules with high confidence can
    be retained. The following figure depicts marking the low confidence rules and
    knocking them off:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rule generation strategy](img/B03980_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Rules for defining appropriate minsup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some important guidelines to be followed for defining the minsup threshold
    for the association rule based mining are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Too high minsup: This will lead to missing itemsets with rare items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Too low minsup: This will result in computational expense as more scans will
    be needed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apriori – the downside
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is now clear that in Apriori algorithm, for every *k* itemsets we will need
    to use *(k-1)* frequent itemsets and when the database scans are done, the pattern
    matching approach is used. The primary bottlenecks are two huge candidate sets
    and multiple database scans. Let's see an example—if there are 10⁴ frequent 1-itemsets,
    then this will result in 10⁷ candidate 2-itemsets. And for every *n* itemsets,
    the longest pattern length, *n + 1* scans are required.
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this would be to avoid the candidate itemset generation completely,
    and one way of solving this is to compress a large dataset or database into a
    compact **frequent pattern tree** (**FP-tree**) that will avoid expensive scans.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways of optimizing the Apriori implementation and here are
    some of the important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach 1**—**Has-based itemset counting**: There is a threshold value set
    for every k itemset bucket, and if the count of the itemset for that itemset is
    lower than the threshold, this bucket will not be processed. This in-turn reduces
    the itemset buckets that are to be considered for processing, thus improving the
    efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approach 2**—**Transaction elimination / counting**: In case a transaction
    does not contain the target k itemset, this transaction does not add value or
    make sense for being processed. So, this approach is about identifying these transactions
    and eliminating them from being processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approach 3**—**Partitioning**: Any itemset that is potentially frequent in
    the dataset will need to be frequent in the partitions of the dataset as well;
    in the absence of which, the itemset could potentially be excluded from being
    processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approach 4**—**Sampling**: This is a simpler way to consider a sample or
    a subset of the bigger universe of data and run the mining process. This would
    reduce the k, and thus the frequent k-itemsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approach 5**—**Dynamic itemset counting**: This is one of the most effective
    methods, and involves including a new itemset only if it is frequent in all its
    subset itemsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although, there are optimization techniques for Apriori; it poses inefficiency
    as a result of expensive scans that are inherent, which will need to be addressed.
    This brings us to the next algorithm of association rule based learning, the **FP-growth**
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: FP-growth algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FP-growth algorithm is an efficient and scalable alternative to mining
    frequent patterns, and thus association rule mining. It addresses most of the
    performance bottlenecks that an Apriori algorithm would undergo. It allows frequent
    itemset generation without having to actually generate the candidate itemsets.
    This algorithm has two steps primarily:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a compact data structure from the database called FP-tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting frequent itemsets directly from the FP-tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s consider the same example we used in the Apriori algorithm. There is
    a total of five items (from the example in the previous section):'
  prefs: []
  type: TYPE_NORMAL
- en: '*I is {iPad(A), iPad case(B), iPad scratch guard(C), Apple care (D), iPhone
    (E)}*, and the following nine transactions. Let''s assume that the minimum support
    count is two:'
  prefs: []
  type: TYPE_NORMAL
- en: '| TID | Transaction Itemsets |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | *iPad(A), iPad case(B), and iPhone(E)* |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | *iPad case(B), Apple care(D)* |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | *iPad case(B), iPad scratch guard(C)* |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | *iPad(A), iPad case(B), and Apple care(D)* |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | *iPad(A), Apple care(D)* |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | *iPad case(B), iPad scratch guard(C)* |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | *iPad(A), Apple care(D)* |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | *iPad(A), iPad case(B), iPad scratch guard(C), and iPhone (E)* |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | *iPad(A), iPad case(B), and iPad scratch guard(C)* |'
  prefs: []
  type: TYPE_TB
- en: 'We will now look at building an FP-tree for this database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identify/calculate the minimum support count. Since it needs to be 30%, the
    minimum support count is calculated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimum support count = 30/100 * 9 = 2.7 ~ 3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the frequency of occurrence for 1-itemset. Additionally, based on
    the support count, add priority:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Itemset | Support count | Priority |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad(A)}* | 6 | 2 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad case(B)}* | 7 | 1 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPad scratch guard(C)}* | 6 | 3 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{Apple care(D)}* | 2 | 4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| *{iPhone(E)}* | 2 | 5 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Order the items for each transaction as per the priority:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| TID | Transaction Itemsets | Re-ordered Itemsets based on priority |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **1** | iPad(*A*), iPad case(*B*), and iPhone(*E*) | iPad case(*B*), iPad(*A*),
    and iPhone(*E*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **2** | iPad case(*B*), Apple care(*D*) | iPad case(*B*), Apple care(*D*)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **3** | iPad case(*B*), iPad scratch guard(*C*) | iPad case(*B*), iPad scratch
    guard(*C*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **4** | iPad(*A*), iPad case(*B*), and Apple care(*D*) | iPad case(*B*),
    iPad(*A*), and Apple care(*D*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **5** | iPad(*A*), Apple care(*D*) | iPad(*A*), Apple care(*D*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **6** | iPad case(*B*), iPad scratch guard(*C*) | iPad case(*B*), iPad scratch
    guard(*C*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **7** | iPad(*A*), Apple care(*D*) | iPad(*A*), Apple care(*D*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **8** | iPad(*A*), iPad case(*B*), iPad scratch guard(*C*), and iPhone (*E*)
    | iPad case(*B*), iPad(*A*), iPad scratch guard(*C*), and iPhone (*E*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **9** | iPad(*A*), iPad case(*B*), and iPad scratch guard(*C*) | iPad case(*B*),
    iPad(*A*), and iPad scratch guard(*C*) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Create FP-tree for transaction for **TID** = **1**, and the ordered itemset
    is iPad case(*B*), iPad(*A*), and iPhone(*E*).![FP-growth algorithm](img/B03980_07_09.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, scan the database for **TID** = **2**, iPad case (*B*) and Apple care(*D*).
    The updated FP-tree will look like this:![FP-growth algorithm](img/B03980_07_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scan all the transactions in the order of L and update the FP-tree accordingly.
    The final FP-tree will be as shown next. Note that every time an item is encountered
    again in the transaction, the count value on the node is incremented.![FP-growth
    algorithm](img/B03980_07_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a conditional FP-tree for each of the transactions and define the conditional
    pattern base.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, generate the frequent patterns. The result for the given dataset is
    shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apriori versus FP-growth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following graph shows the relationship between the algorithms with different
    minsup threshold values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apriori versus FP-growth](img/B03980_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: An article by Prof. Pier Luca Lanzi'
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of the FP-growth algorithm are detailed here:'
  prefs: []
  type: TYPE_NORMAL
- en: The complete information for frequent pattern mining is preserved, without breaking
    the pattern in a long transaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is compacted by eliminating irrelevant information as infrequent itemsets
    are avoided upfront
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FP-growth algorithm works in a divide-and-conquer mode, where the dataset
    is decomposed as per the frequent itemset patterns uncovered so far. This reduces
    searches to the subset of datasets as against the complete database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The candidate itemsets are not generated in this case and hence, will not need
    to be tested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Apriori and FP-growth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing the Apriori
    classifier (source code path `.../chapter7/...` under each of the folders for
    the technology.)
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../mahout/chapter7/aprioriexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../mahout/chapter7/fpgrowthexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../r/chapter7/aprioriexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../r/chapter7/fpgrowthexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../spark/chapter7/aprioriexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../spark/chapter7/fpgrowthexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (Scikit-learn)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../python-scikit-learn/ chapter7/aprioriexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../python-scikit-learn/chapter7/fpgrowthexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../julia/chapter7/aprioriexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the code files folder `.../julia/chapter7/fpgrowthexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned the association rule based learning methods
    and, Apriori and FP-growth algorithms. With a common example, you learned how
    to do frequent pattern mining using Apriori and FP-growth algorithms with a step-by-step
    debugging of the algorithm. We also compared and contrasted the algorithms and
    their performance. We have example implementations for Apriori using Mahout, R,
    Python, Julia, and Spark. In the next chapter, we will cover the Bayesian methods
    and specifically, the Naïve-Bayes algorithm.
  prefs: []
  type: TYPE_NORMAL
