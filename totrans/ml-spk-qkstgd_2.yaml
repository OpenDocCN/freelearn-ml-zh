- en: Setting Up a Local Development Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will install, configure, and deploy a local analytical
    development environment by provisioning a self-contained single-node cluster that
    will allow us to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prototype and develop machine learning models and pipelines in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrate the functionality and usage of Apache Spark's machine learning library,
    `MLlib`, via the Spark Python API (PySpark)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop and test machine learning models on a single-node cluster using small
    sample datasets, and thereafter scale up to multi-node clusters processing much
    larger datasets with little or no code changes required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our single-node cluster will host the following technologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operating system**: CentOS Linux 7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.centos.org/download/](https://www.centos.org/download/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**General Purpose Programming Languages**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java SE Development Kit (JDK) 8 (8u181)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.oracle.com/technetwork/java/javase/downloads/index.html](https://www.oracle.com/technetwork/java/javase/downloads/index.html)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Scala 2.11.x (2.11.12) [https://www.scala-lang.org/download/all.html](https://www.scala-lang.org/download/all.html)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.x (3.6) via Anaconda 5.x (5.2) Python Distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.anaconda.com/download/](https://www.anaconda.com/download/)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**General purpose distributed processing engine**: Apache Spark 2.3.x (2.3.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Distributed streaming platform**: Apache Kafka 2 (2.0.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://kafka.apache.org/downloads](https://kafka.apache.org/downloads)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: CentOS Linux 7 virtual machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, we will assume that you have access to a physical or virtual
    machine provisioned with the CentOS 7 operating system. CentOS 7 is a free Linux
    distribution derived from **Red Hat Enterprise Linux** (**RHEL**). It is commonly
    used, along with its licensed upstream parent, RHEL, as the operating system of
    choice for Linux-based servers, since it is stable and backed by a large active
    community with detailed documentation. All the commands that we will use to install
    the various technologies listed previously will be Linux shell commands to be
    executed on a single CentOS 7 (or RHEL) machine, whether physical or virtual.
    If you do not have access to a CentOS 7 machine, then there are quite a few options
    available to provision a CentOS 7 virtual machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud computing platforms such as **Amazon Web Services** (**AWS**), **Microsoft
    Azure**, and the **Google Cloud Platform** (**GCP**) all allow you to stand up
    virtual machines using a **Pay-As-You-Go** (**PAYG**) pricing model. Often, the
    major cloud computing platforms also provide a free tier for new users with a
    small amount of free capacity in order to trial their services. To learn more
    about these major cloud computing platforms, please visit the following websites:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS: [https://aws.amazon.com/](https://aws.amazon.com/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft Azure: [https://azure.microsoft.com](https://azure.microsoft.com)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCP: [https://cloud.google.com/](https://cloud.google.com/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual Private Server** (**VPS**) hosting companies, such as **Linode**
    and **Digital Ocean**, also provide the ability to provision low-cost CentOS virtual
    machines. These VPS providers often employ a much simpler pricing model consisting
    only of virtual machines of various specifications. To learn more about these
    major VPS providers, please visit the following websites:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linode: [https://www.linode.com/](https://www.linode.com/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Digital Ocean: [https://www.digitalocean.com/](https://www.digitalocean.com/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A common and free option, particularly for local development environments used
    for prototyping and testing, is to provision your own virtual machine hosted by
    your personal physical desktop or laptop. Virtualization software such as **Oracle
    VirtualBox** (open source) and **VMWare Workstation Player** (free for personal
    use) allow you to set up and run virtual machines on your own personal physical
    devices. To learn more about these virtualization software services, please visit
    the following websites:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oracle VirtualBox: [https://www.virtualbox.org/](https://www.virtualbox.org/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VMWare Workstation Player: [https://www.vmware.com/](https://www.vmware.com/uk/products/workstation-player.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the remainder of this chapter, we will assume that you have provisioned
    a 64-bit CentOS 7 machine and that you have either direct desktop access to it,
    or network access to it via both HTTP and SSH protocols. Though the specifications
    of your virtual machine may differ, we would recommend the following minimum virtual
    hardware requirements in order to efficiently run the examples in the remainder
    of this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: CentOS 7 (minimum installation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Virtual CPUs: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory: 8 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage: 20 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, our virtual machine has the following network properties and will
    be referenced hereafter as such. These will be different for your virtual machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static IP address: `192.168.56.10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Netmask: `255.255.255.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully qualified domain name** (**FQDN**): `packt.dev.keisan.io`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the security of your virtual machine, and the subsequent software
    services that it hosts, including big data technologies, is beyond the scope of
    this book. Should you wish to learn more about how to harden your base operating
    system and common software services in order to protect against external attacks,
    we recommend visiting [https://www.cisecurity.org/cis-benchmarks/](https://www.cisecurity.org/cis-benchmarks/)
    as well as the individual software service websites themselves, such as [https://spark.apache.org/docs/latest/security.html](https://spark.apache.org/docs/latest/security.html)
    for Apache Spark security.
  prefs: []
  type: TYPE_NORMAL
- en: Java SE Development Kit 8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Java is a general purpose programming language often used for **object-oriented
    programming** (**OOP**). Many of the distributed technologies that we discussed
    in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*,
    are originally written in Java. As such, th**e Java Development Kit** (**JDK**)
    is required to be installed in our local development environment in order to run
    those software services within **Java Virtual Machines** (**JVM**). Apache Spark
    2.3.x requires Java 8+ in order to run. To install Oracle Java SE Development
    Kit 8, please execute the following shell commands as the Linux *root* user or
    another user with elevated privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands will install JDK 8 and, thereafter, add the location of the
    Java binaries to the global `PATH` variable, allowing any local Linux user to
    run Java-based programs. To check that Java 8 has been installed successfully,
    the following command should return the version of Java installed, demonstrated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Scala 2.11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scala is a general purpose programming language used for both object-oriented
    programming and functional programming. Apache Spark is, in fact, written in the
    Scala programming language. However, as described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*, Spark applications can be written in a variety of languages,
    including Java, Scala, Python, and R. Though the pros and cons of Scala versus
    Python is beyond the scope of this book, Scala is generally faster than Python
    within the context of data analysis and naturally more tightly integrated with
    Spark. Python, however, currently offers a more comprehensive library of advanced
    third-party data science tools and frameworks and is arguably easier to learn
    and use. The code examples provided for this book have been written in Python
    3\. However, this sub-section describes the steps required in order to install
    Scala should you wish to develop Scala-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring to Scala specifically, Apache Spark 2.3.2 requires Scala 2.11.x in
    order to run Scala-based Spark applications. In order to install Scala 2.11.12,
    please execute the following shell commands as the Linux root user or another
    user with elevated privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands will install Scala 2.11.12 and place its binaries in a globally
    accessible location, allowing any local Linux user to run Scala applications,
    whether Spark-based or not. To check that Scala 2.11.12 has been installed successfully,
    the following command should return the version of Scala installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also access the Scala shell and execute interactive Scala commands
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Anaconda 5 with Python 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anaconda is a distribution of the Python general purpose programming language.
    Not only does it contain the Python interpreter, but it also comes bundled with
    a wide range of commonly used Python data science packages out of the box and
    a Python package management system called **conda**, making it quick and easy
    to provision a Python-based data science platform. In fact, we will be taking
    advantage of some of the pre-bundled Python packages in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda 5.2 comes bundled with Python 3.6\. Apache Spark 2.3.x supports both
    branches of Python, namely Python 2 and Python 3\. Specifically, it supports Python
    2.7+ and Python 3.4+. As described earlier, the code examples provided for this
    book have been written in Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install Anaconda 5.2, please execute the following shell commands.
    You may or may not choose to execute these shell commands as the Linux root user.
    If you do not, Anaconda will be installed for the local Linux user running these
    commands and does not require administrator privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that Anaconda 5.2 and, hence, Python 3.6 have been installed successfully,
    the following command should return the version of conda installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also access the Python shell and execute interactive Python commands
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Basic conda commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this sub-section, we will provide some basic conda commands for your reference.
    These commands assume that your virtual machine can access either the internet
    or a local Python repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to upgrade the version of conda and/or Anaconda as a whole, you can
    execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To install or update individual Python packages, you can execute the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in order to list the current Python packages and versions installed
    in your Anaconda distribution, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the conda package management system, please visit [https://conda.io/docs/index.html](https://conda.io/docs/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Additional Python packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following Python packages, which are not already contained within the default
    Anaconda distribution, are required for our local development environment. Please
    execute the following shell commands to install these prerequisite Python packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter Notebook is an open source, web-based application designed for *interactive*
    analytics that comes bundled with the Anaconda distribution. Since it is designed
    for interactive analytics, it is best suited for ad hoc queries, live simulations,
    prototyping, and a means to visualize your data and to look for any trends and
    patterns prior to developing production-ready data science models. **Apache Zeppelin**
    is another example of an open source, web-based notebook used for similar purposes.
    Notebooks such as Jupyter Notebook and Apache Zeppelin tend to support multiple
    kernels, meaning that you can use various general purpose programming languages
    including Python and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: One of the core advantages of notebooks is that they persist both your input
    code and any output data structures and visualizations that your code generates,
    including graphs, charts, and tables. However, they are *not* fully-fledged **integrated
    development environments** (**IDE**). This means that, in general, they should
    not be used for the development of code intended for production-grade data engineering
    or analytical pipelines. This is because they are difficult (but not impossible)
    to manage in version control systems such as Git, as they persist both input code
    and intermediate output structures. As such, they are also difficult to build
    code artifacts from and to deploy automatically using typical DevOps pipelines.
    Therefore, notebooks remain ideally suited for interactive analytics, ad hoc queries,
    and for prototyping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The majority of the code files provided for this book are, in fact, Jupyter
    Notebook files (`.ipynb`), using the Python 3 kernel, so that readers may see
    the output of our models immediately. Should you, in the future, wish to write
    data science code that will eventually be deployed to production-grade systems,
    we strongly recommend writing your code in a proper IDE such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eclipse: [https://www.eclipse.org/ide/](https://www.eclipse.org/ide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IntelliJ IDEA: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyCharm: [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft **Visual Studio Code** (**VS Code**): [https://code.visualstudio.com/](https://code.visualstudio.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Jupyter Notebook is already bundled with the Anaconda
    distribution. However, a few configuration steps are recommended in order to access
    it. Please execute the following shell commands as your local Linux user in order
    to generate a per-user Jupyter Notebook configuration file that you can then edit
    based on per-user preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These commands will configure a per-user Jupyter Notebook instance to listen
    on a dedicated IP address (in our case, `192.168.56.10`) using a designated port
    (in our case `8888`), and to work from a pre-defined base directory in which to
    persist Jupyter Notebook code files (in our case `/data/workspaces/packt/jupyter/notebooks`).
    Note that you should amend these properties based on your specific environment.
  prefs: []
  type: TYPE_NORMAL
- en: Starting Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have desktop-based access to your CentOS virtual machine, the easiest
    way to instantiate a new per-user Jupyter Notebook instance is to execute the
    following shell command as your local Linux user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'However, should you only have SSH or command-line access with no GUI, then
    you should use the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The latter command will stop Jupyter from automatically opening a local browser
    session. In either case, the resultant logs will state the full URL (including
    the security token by default) that can be used to access your instance of Jupyter
    Notebook. The URL should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://192.168.56.10:8888/?token=6ebb5f6a321b478162802a97b8e463a1a053df12fcf9d99c`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please copy and paste this URL into an internet browser supported by Jupyter
    Notebook (Google Chrome, Mozilla Firefox, or Apple Safari). If successful, a screen
    similar to the screenshot illustrated in *Figure 2.1* should be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/883be21b-b3bb-4c6b-88d3-16e3768932ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Jupyter Notebook web session'
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Jupyter Notebook is a web-based application, it is accessible via the
    HTTP protocol at the designated port number. If you are accessing the generated
    URL via a remote internet browser and it cannot connect, then please check your
    firewall settings (and SELinux in the case of CentOS and RHEL) on your virtual
    machine to ensure that access to the designated port number is provisioned from
    your location. For example, the following shell commands executed by the Linux
    *root* user, or another user with elevated privileges, will open port 8888 in
    the CentOS 7 firewall via its public zone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Please contact your system administrator or refer to your cloud platform documentation
    for further network-related information and troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Jupyter Notebook, its configuration, and common troubleshooting,
    please visit [https://jupyter-notebook.readthedocs.io/en/stable/index.html](https://jupyter-notebook.readthedocs.io/en/stable/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark 2.3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The
    Big Data Ecosystem*, Apache Spark is a general purpose distributed processing
    engine that is capable of performing data transformations, advanced analytics,
    machine learning, and graph analytics at scale over petabytes of data. Apache
    Spark can be deployed either in standalone mode (meaning that we utilize its in-built
    cluster manager) or integrated with other third-party cluster managers including
    Apache YARN and Apache Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of our single-node development cluster, we will deploy Apache Spark
    in standalone mode where our single-node will host both the Apache Spark Standalone
    Master server and a single worker node instance. Since Spark software services
    are designed to run in a JVM, it is perfectly acceptable to co-locate both standalone
    master and worker processes on a single node, though in practice, in real-world
    implementations of Apache Spark, clusters can be much larger with multiple worker
    nodes provisioned. Our single-node Apache Spark cluster will still allow us to
    prototype and develop Spark applications and machine learning models that can
    still take advantage of the parallelism offered by a multi-core single machine,
    and thereafter are capable of being deployed to larger clusters and datasets with
    ease.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that we will be installing Apache Spark 2.3.2 direct from its pre-built
    binaries available on the official Apache Spark website at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Nowadays, it is common for distributed technologies such as Spark, Kafka, and
    Hadoop components to be installed together at the same time via consolidated big
    data platforms such as those offered by **Hortonworks Data Platform** (**HDP**),
    **Cloudera**, and **MapR**. The benefits of using consolidated platforms such
    as these include the deployment of individual component versions that have been
    fully tested together and guaranteed to fully integrate with one another, as well
    as web-based installation, monitoring, administration, and support.
  prefs: []
  type: TYPE_NORMAL
- en: Spark binaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please execute the following shell commands as a local Linux user to extract
    the Apache Spark binaries. In our case, we will be installing the Spark binaries
    into `/opt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The resultant Spark parent directory will have the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bin`: Shell scripts for local Spark services, such as `spark-submit`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sbin`: Shell scripts, including starting and stopping Spark services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conf`: Spark configuration files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jars`: Spark library dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`python`: Spark''s Python API, called PySpark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`R`: Spark''s R API, called SparkR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local working directories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each node in a Spark cluster (in our case, just the single node) will generate
    log files as well as local working files, such as when shuffling and serializing
    RDD data. The following commands will create defined local directories in which
    to store these local working outputs, the paths of which you can edit as per your
    preferences and which will be used in later configuration files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Spark configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Configuration can be applied to Spark in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark properties** control application-level settings, including execution
    behavior, memory management, dynamic allocation, scheduling, and security, which
    can be defined in the following order of precedence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Via a Spark configuration programmatic object called `SparkConf` defined in
    your driver program
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Via command-line arguments passed to `spark-submit` or `spark-shell`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Via default options set in `conf/spark-defaults.conf`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environmental variables** control per-machine settings, such as the local
    IP address of the local worker node, and which can be defined in `conf/spark-env.sh`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our case, we will set some basic default Spark properties via `conf/spark-defaults.conf`,
    freeing us to concentrate on the data science content in future chapters. This
    can be achieved by executing the following shell commands (edit the values as
    per your environment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Environmental variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will also set basic environmental variables via `conf/spark-env.sh` as follows
    (edit the values as per your environment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the various Spark configuration options available, including
    an exhaustive list of Spark properties and environmental variables, please visit
    [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html).
  prefs: []
  type: TYPE_NORMAL
- en: Standalone master server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to start the Spark standalone master server, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To check whether this was successful, you can examine the Spark logs as written
    to `SPARK_LOG_DIR`. Spark applications can be submitted to the standalone master
    server at `spark://<Master IP Address>:7077` (port `7077` by default) or `spark://<Master
    IP Address>:6066` using its REST URL in cluster mode (port `6066` by default).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark Master server also provides an out-of-the-box master web **User Interface**
    (**UI**) in which running Spark applications and workers can be monitored and
    performance diagnosed. By default, this master web UI is accessible via HTTP on
    port `8080`, in other words, `http://<Master IP Address>:8080`, the interface
    of which is illustrated in *Figure 2.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/295c2c65-f9a7-4fa9-bf7a-7e6ba03d5cc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Spark standalone master server web UI'
  prefs: []
  type: TYPE_NORMAL
- en: Again, in the event that you cannot access this URL via a remote internet browser,
    you may need to open up port `8080` (by default) in your firewall and/or SELinux
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: Spark worker node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to start our Spark Worker node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, to check whether this was successful, you can examine the Spark logs
    as written to `SPARK_LOG_DIR`. You can also access the Spark Master web UI to
    confirm that the worker has been registered successfully, as illustrated in *Figure
    2.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3a310da-b0c6-493f-9897-8de1c32ee451.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Spark worker successfully registered'
  prefs: []
  type: TYPE_NORMAL
- en: Note that Spark workers also expose a **Worker UI** via HTTP on port 8081 by
    default, in other words, `http://<Worker IP Address>:8081`.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark and Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now integrate Jupyter Notebook with PySpark so that we can write our first
    Spark applications in Python! In the case of our local development environment,
    the easiest way to integrate Jupyter Notebook with PySpark is to set a global
    `SPARK_HOME` environmental variable that points to the directory containing the
    Spark binaries. Thereafter, we can employ the `findspark` Python package, as installed
    earlier, that will append the location of `SPARK_HOME`, and hence the PySpark
    API, to `sys.path` at runtime. Note that `findspark` should not be used for production-grade
    code development—instead, Spark applications should be deployed as code artifacts
    submitted via `spark-submit`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please execute the following shell commands as the Linux root user, or another
    user with elevated privileges, in order to define a global environmental variable
    called `SPARK_HOME` (or, alternatively, add it to your local Linux user''s `.bashrc`
    file, which requires no administrative privileges):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You will need to restart any running Jupyter Notebook instances, and the underlying
    Terminal sessions from which they were spawned, in order for the SPARK_HOME environmental
    variable to be successfully recognized and registered by findspark.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to write our first Spark application in Python! Instantiate
    a Jupyter Notebook instance, access it via your internet browser, and create a
    new Python 3 notebook containing the following code (it may be easier to split
    the following code over separate notebook cells for future ease of reference):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This PySpark application, at a high level, works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required Python dependencies, including `findspark` and `pyspark`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Spark context, which tells the Spark application how to connect to
    the Spark cluster, by instantiating it with a `SparkConf` object that provides
    application-level settings at a higher level of precedence
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the mathematical value of Pi π
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the value of Pi and display it in Jupyter Notebook as a cell output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the Spark context that terminates the Spark application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you access the Spark Master web UI before executing `sc.stop()`, the Spark
    application will be listed under Running Applications, at which time you may view
    its underlying worker and executor log files. If you access the Spark Master web
    UI following execution of `sc.stop()`, the Spark application will be listed under
    Completed Applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this notebook can be downloaded from the GitHub repository accompanying
    this book and is called `chp02-test-jupyter-notebook-with-pyspark.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To finish off our local development environment, we will install Apache Kafka.
    As described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The
    Big Data Ecosystem*, Apache Kafka is a distributed streaming platform. We will
    use Apache Kafka in [Chapter 8](cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml), *Real-Time
    Machine Learning Using Apache Spark*, to develop a real-time analytical model
    by combining it with Spark Streaming and `MLlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Again, for the purposes of our single-node development cluster, Apache Kafka
    will be deployed on the same single node as the Apache Spark software services.
    We will also be installing the version of Apache Kafka 2.0.0 that has been built
    for Scala 2.11.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka binaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After downloading the Kafka release, the first thing we need to do is to extract
    and install the pre-compiled binaries on our single-node cluster. In our case,
    we will be installing the Kafka binaries into `/opt`. Please execute the following
    shell commands as a local Linux user to extract the Apache Kafka binaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Local working directories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with Apache Spark processes, Apache Kafka processes also require their own
    local working directories to persist local data and log files. The following commands
    will create defined local directories in which to store these local working outputs,
    the paths of which you can edit as per your preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Kafka configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will also set some basic configuration as follows (edit the values as per
    your environment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Start the Kafka server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to start Apache Kafka as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Testing Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we can test our Kafka installation by creating a test topic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have created our test topic, let''s start a command-line producer application
    and send some test messages to this topic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s start a command-line consumer application (in another Terminal
    session) to consume these test messages and print them to the console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In fact, if you keep typing new messages in the Terminal running the producer
    application, you will see them immediately being consumed by the consumer application
    and printed to the console in its Terminal!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have installed, configured, and deployed a local analytical
    development environment consisting of a single-node Apache Spark 2.3.2 and Apache
    Kafka 2.0.0 cluster that will also allow us to interactively develop Spark applications
    using Python 3.6 via Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss some of the high-level concepts behind
    common artificial intelligence and machine learning algorithms, as well as introducing
    Apache Spark's machine learning library, `MLlib`!
  prefs: []
  type: TYPE_NORMAL
