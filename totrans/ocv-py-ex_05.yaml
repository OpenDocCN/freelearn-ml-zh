- en: Chapter 5. Extracting Features from an Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how to detect salient points, also known
    as keypoints, in an image. We will discuss why these keypoints are important and
    how we can use them to understand the image content. We will talk about different
    techniques that can be used to detect these keypoints, and understand how we can
    extract features from a given image.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: What are keypoints and why do we care about them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect keypoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use keypoints for image content analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different techniques to detect keypoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a feature extractor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we care about keypoints?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image content analysis refers to the process of understanding the content of
    an image so that we can take some action based on that. Let's take a step back
    and talk about how humans do it. Our brain is an extremely powerful machine that
    can do complicated things very quickly. When we look at something, our brain automatically
    creates a footprint based on the "interesting" aspects of that image. We will
    discuss what interesting means as we move along this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, an interesting aspect is something that''s distinct in that region.
    If we call a point interesting, then there shouldn''t be another point in its
    neighborhood that satisfies the constraints. Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why do we care about keypoints?](img/B04554_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now close your eyes and try to visualize this image. Do you see something specific?
    Can you recollect what''s in the left half of the image? Not really! The reason
    for this is that the image doesn''t have any interesting information. When our
    brain looks at something like this, there''s nothing to make note of. So it tends
    to wander around! Let''s take a look at the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why do we care about keypoints?](img/B04554_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now close your eyes and try to visualize this image. You will see that the
    recollection is vivid and you remember a lot of details about this image. The
    reason for this is that there are a lot of interesting regions in the image. The
    human eye is more sensitive to high frequency content as compared to low frequency
    content. This is the reason we tend to recollect the second image better than
    the first one. To further demonstrate this, let''s look at the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why do we care about keypoints?](img/B04554_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you notice, your eye immediately went to the TV remote, even though it's
    not at the center of the image. We automatically tend to gravitate towards the
    interesting regions in the image because that is where all the information is.
    This is what our brain needs to store in order to recollect it later.
  prefs: []
  type: TYPE_NORMAL
- en: When we build object recognition systems, we need to detect these "interesting"
    regions to create a signature for the image. These interesting regions are characterized
    by keypoints. This is why keypoint detection is critical in many modern computer
    vision systems.
  prefs: []
  type: TYPE_NORMAL
- en: What are keypoints?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know that keypoints refer to the interesting regions in the image,
    let''s dig a little deeper. What are keypoints made of? Where are these points?
    When we say "interesting", it means that something is happening in that region.
    If the region is just uniform, then it''s not very interesting. For example, corners
    are interesting because there is sharp change in intensity in two different directions.
    Each corner is a unique point where two edges meet. If you look at the preceding
    images, you will see that the interesting regions are not completely made up of
    "interesting" content. If you look closely, we can still see plain regions within
    busy regions. For example, consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What are keypoints?](img/B04554_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you look at the preceding object, the interior parts of the interesting regions
    are "uninteresting".
  prefs: []
  type: TYPE_NORMAL
- en: '![What are keypoints?](img/B04554_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, if we were to characterize this object, we would need to make sure that
    we picked the interesting points. Now, how do we define "interesting points"?
    Can we just say that anything that''s not uninteresting can be an interesting
    point? Let''s consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What are keypoints?](img/B04554_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can see that there is a lot of high frequency content in this image
    along the edge. But we cannot call the whole edge "interesting". It is important
    to understand that "interesting" doesn't necessarily refer to color or intensity
    values. It can be anything, as long as it is distinct. We need to isolate the
    points that are unique in their neighborhood. The points along the edge are not
    unique with respect to their neighbors. So, now that we know what we are looking
    for, how do we pick an interesting point?
  prefs: []
  type: TYPE_NORMAL
- en: What about the corner of the table? That's pretty interesting, right? It's unique
    with respect to its neighbors and we don't have anything like that in its vicinity.
    Now this point can be chosen as one of our keypoints. We take a bunch of these
    keypoints to characterize a particular image.
  prefs: []
  type: TYPE_NORMAL
- en: When we do image analysis, we need to convert it into a numerical form before
    we deduce something. These keypoints are represented using a numerical form and
    a combination of these keypoints is then used to create the image signature. We
    want this image signature to represent a given image in the best possible way.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the corners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we know that the corners are "interesting", let's see how we can detect
    them. In computer vision, there is a popular corner detection technique called
    **Harris Corner Detector**. We basically construct a 2x2 matrix based on partial
    derivatives of the grayscale image, and then analyze the eigenvalues. This is
    actually an oversimplification of the actual algorithm, but it covers the gist.
    So, if you want to understand the underlying mathematical details, you can look
    into the original paper by Harris and Stephens at [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf).
    A corner point is a point where both the eigenvalues would have large values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting the corners](img/B04554_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run the Harris corner detector on this image, you will see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting the corners](img/B04554_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, all the black dots correspond to the corners in the image.
    If you notice, the corners at the bottom of the box are not detected. The reason
    for this is that the corners are not sharp enough. You can adjust the thresholds
    in the corner detector to identify these corners. The code to do this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Good Features To Track
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Harris corner detector performs well in many cases, but it misses out on a few
    things. Around six years after the original paper by Harris and Stephens, Shi-Tomasi
    came up with a better corner detector. You can read the original paper at [http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf).
    They used a different scoring function to improve the overall quality. Using this
    method, we can find the 'N' strongest corners in the given image. This is very
    useful when we don't want to use every single corner to extract information from
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you apply the Shi-Tomasi corner detector to the image shown earlier, you
    will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Good Features To Track](img/B04554_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Scale Invariant Feature Transform (SIFT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though corner features are "interesting", they are not good enough to characterize
    the truly interesting parts. When we talk about image content analysis, we want
    the image signature to be invariant to things such as scale, rotation, illumination,
    and so on. Humans are very good at these things. Even if I show you an image of
    an apple upside down that's dimmed, you will still recognize it. If I show you
    a really enlarged version of that image, you will still recognize it. We want
    our image recognition systems to be able to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the corner features. If you enlarge an image, a corner might
    stop being a corner as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale Invariant Feature Transform (SIFT)](img/B04554_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the second case, the detector will not pick up this corner. And, since it
    was picked up in the original image, the second image will not be matched with
    the first one. It's basically the same image, but the corner features based method
    will totally miss it. This means that corner detector is not exactly scale invariant.
    This is why we need a better method to characterize an image.
  prefs: []
  type: TYPE_NORMAL
- en: SIFT is one of the most popular algorithms in all of computer vision. You can
    read David Lowe's original paper at [http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).
    We can use this algorithm to extract keypoints and build the corresponding feature
    descriptors. There is a lot of good documentation available online, so we will
    keep our discussion brief. To identify a potential keypoint, SIFT builds a pyramid
    by downsampling an image and taking the difference of Gaussian. This means that
    we run a Gaussian filter at each level and take the difference to build the successive
    levels in the pyramid. In order to see if the current point is a keypoint, it
    looks at the neighbors as well as the pixels at the same location in neighboring
    levels of the pyramid. If it's a maxima, then the current point is picked up as
    a keypoint. This ensures that we keep the keypoints scale invariant.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how it achieves scale invariance, let's see how it achieves
    rotation invariance. Once we identify the keypoints, each keypoint is assigned
    an orientation. We take the neighborhood around each keypoint and compute the
    gradient magnitude and direction. This gives us a sense of the direction of that
    keypoint. If we have this information, we will be able to match this keypoint
    to the same point in another image even if it's rotated. Since we know the orientation,
    we will be able to normalize those keypoints before making the comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have all this information, how do we quantify it? We need to convert
    it to a set of numbers so that we can do some kind of matching on it. To achieve
    this, we just take the 16x16 neighborhood around each keypoint, and divide it
    into 16 blocks of size 4x4\. For each block, we compute the orientation histogram
    with 8 bins. So, we have a vector of length 8 associated with each block, which
    means that the neighborhood is represented by a vector of size 128 (8x16). This
    is the final keypoint descriptor that will be used. If we extract `N` keypoints
    from an image, then we will have `N` descriptors of length 128 each. This array
    of `N` descriptors characterizes the given image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale Invariant Feature Transform (SIFT)](img/B04554_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you extract the keypoint locations using SIFT, you will see something like
    the following, where the size of the circle indicates the strength of the keypoints,
    and the line inside the circle indicates the orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale Invariant Feature Transform (SIFT)](img/B04554_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we look at the code, it is important to know that SIFT is patented and
    it''s not freely available for commercial use. Following is the code to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also compute the descriptors. OpenCV lets us do it separately or we
    can combine the detection and computation parts in the same step by using the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Speeded Up Robust Features (SURF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though SIFT is nice and useful, it's computationally intensive. This means
    that it's slow and we will have a hard time implementing a real-time system if
    it uses SIFT. We need a system that's fast and has all the advantages of SIFT.
    If you remember, SIFT uses the difference of Gaussian to build the pyramid and
    this process is slow. So, to overcome this, SURF uses a simple box filter to approximate
    the Gaussian. The good thing is that this is really easy to compute and it's reasonably
    fast. There's a lot of documentation available online on SURF at [http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf](http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf).
    So, you can go through it to see how they construct a descriptor. You can refer
    to the original paper at [http://www.vision.ee.ethz.ch/~surf/eccv06.pdf](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf).
    It is important to know that SURF is also patented and it is not freely available
    for commercial use.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the SURF keypoint detector on the earlier image, you will see something
    like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Speeded Up Robust Features (SURF)](img/B04554_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Features from Accelerated Segment Test (FAST)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though SURF is faster than SIFT, it's just not fast enough for a real-time
    system, especially when there are resource constraints. When you are building
    a real-time application on a mobile device, you won't have the luxury of using
    SURF to do computations in real time. We need something that's really fast and
    computationally inexpensive. Hence, Rosten and Drummond came up with FAST. As
    the name indicates, it's really fast!
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of going through all the expensive calculations, they came up with
    a high-speed test to quickly determine if the current point is a potential keypoint.
    We need to note that FAST is just for keypoint detection. Once keypoints are detected,
    we need to use SIFT or SURF to compute the descriptors. Consider the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Features from Accelerated Segment Test (FAST)](img/B04554_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we run the FAST keypoint detector on this image, you will see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Features from Accelerated Segment Test (FAST)](img/B04554_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we clean it up and suppress the unimportant keypoints, it will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Features from Accelerated Segment Test (FAST)](img/B04554_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Binary Robust Independent Elementary Features (BRIEF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though we have FAST to quickly detect the keypoints, we still have to use
    SIFT or SURF to compute the descriptors. We need a way to quickly compute the
    descriptors as well. This is where BRIEF comes into the picture. BRIEF is a method
    for extracting feature descriptors. It cannot detect the keypoints by itself,
    so we need to use it in conjunction with a keypoint detector. The good thing about
    BRIEF is that it's compact and fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary Robust Independent Elementary Features (BRIEF)](img/B04554_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'BRIEF takes the list of input keypoints and outputs an updated list. So if
    you run BRIEF on this image, you will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary Robust Independent Elementary Features (BRIEF)](img/B04554_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Oriented FAST and Rotated BRIEF (ORB)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, now we have arrived at the best combination out of all the combinations
    that we have discussed so far. This algorithm came out of the OpenCV Labs. It's
    fast, robust, and open-source! Both SIFT and SURF algorithms are patented and
    you can't use them for commercial purposes. This is why ORB is good in many ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the ORB keypoint extractor on one of the images shown earlier, you
    will see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Oriented FAST and Rotated BRIEF (ORB)](img/B04554_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the importance of keypoints and why we need
    them. We discussed various algorithms to detect keypoints and compute feature
    descriptors. We will be using these algorithms in all the subsequent chapters
    in various different contexts. The concept of keypoints is central to computer
    vision, and plays an important role in many modern systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss how to stitch multiple images of
    the same scene together to create a panoramic image.
  prefs: []
  type: TYPE_NORMAL
