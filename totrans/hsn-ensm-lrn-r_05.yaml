- en: Chapter 5. The Bare Bones Boosting Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 简洁的增强算法
- en: What do we mean by bare bones boosting algorithms? The boosting algorithm (and
    its variants) is arguably one of the most important algorithms in the machine
    learning toolbox. Any data analyst needs to know this algorithm, and eventually
    the push for higher accuracy invariably drives towards the need for the boosting
    technique. It has been reported on the [www.kaggle.org](http://www.kaggle.org)
    forums that boosting algorithms for complex and voluminous data run for several
    weeks and that most award-winning solutions are based on this. Furthermore, the
    algorithms run on modern graphical device machines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的简洁的增强算法是什么意思？增强算法（及其变体）可以说是机器学习工具箱中最重要的算法之一。任何数据分析师都需要了解这个算法，并且最终追求更高的准确率不可避免地会推动对增强技术的需求。据报道，在[www.kaggle.org](http://www.kaggle.org)论坛上，复杂和大量数据的增强算法运行了数周，而且大多数获奖方案都是基于这个算法。此外，这些算法在现代图形设备机器上运行。
- en: Taking its importance into account, we will study the boosting algorithm in
    detail here. *Bare bones* is certainly not a variant of the boosting algorithm.
    Since the boosting algorithm is one of the very important and vital algorithms,
    we will first state the algorithm and implement it in a rudimentary fashion, which
    will show each step of the algorithm in action.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到其重要性，我们将在下面详细研究增强算法。*简洁的*当然不是增强算法的变体。由于增强算法是非常重要且关键的算法之一，我们将首先陈述该算法并以最基本的方式实现它，这将展示算法的每个步骤是如何运作的。
- en: We will begin with the adaptive boosting algorithm—popularly known and abbreviated
    as the **AdaBoost** algorithm—and using very simple and raw code, we will illustrate
    it for a classification problem. The illustration is carried over a `toy` dataset
    so that the steps can be clearly followed by the reader.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从自适应增强算法开始——通常被称为**AdaBoost**算法——并使用非常简单和原始的代码，我们将展示其在分类问题中的应用。该说明是在一个`toy`数据集上进行的，以便读者可以清楚地跟随步骤。
- en: 'In the next section, we will extend the classification boosting algorithm to
    the regression problem. For this problem, the boosting variant is the famous **gradient
    boosting algorithm**. An interesting nonlinear regression problem will be improvised
    through a series of basic decision trees with a single split, also known as **stumps**.
    The gradient boosting algorithm will be illustrated for the choice of the squared-error
    loss function. Variable importance computation will be clarified for the boosting
    method. The details of the `gbm` package will be discussed in the penultimate
    section of the chapter. The concluding section will carry out a comparison of
    the bagging, random forests, and boosting methods for a spam dataset. This chapter
    consists of the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将扩展分类增强算法到回归问题。对于这个问题，增强变体是著名的**梯度增强算法**。通过一系列具有单个分割的基本决策树，即所谓的**树桩**，将创建一个有趣的非线性回归问题。我们将通过平方误差损失函数的选择来展示梯度增强算法。对于增强方法，我们将阐明变量重要性计算。本章的倒数第二节将讨论`gbm`包的细节。结论部分将对垃圾邮件数据集的袋装、随机森林和增强方法进行比较。本章包括以下主题：
- en: The general boosting algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用增强算法
- en: Adaptive boosting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应提升
- en: Gradient boosting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting based on stumps
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于树桩的梯度提升
- en: Gradient boosting with squared-error loss
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于平方误差损失的梯度提升
- en: Variable importance in the boosting technique
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强技术中的变量重要性
- en: Using the gbm package
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用gbm包
- en: Comparison of bagging, random forests, and boosting algorithms
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袋装、随机森林和增强算法的比较
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following libraries in the chapter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下库：
- en: '`rpart`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: '`gbm`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbm`'
- en: The general boosting algorithm
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用增强算法
- en: The tree-based ensembles in the previous chapters, *Bagging* and *Random Forests*,
    cover an important extension of the decision trees. However, while bagging provides
    greater stability by averaging multiple decision trees, the bias persists. This
    limitation motivated Breiman to sample the covariates at each split point to generate
    an ensemble of "independent" trees and lay the foundation for random forests.
    The trees in the random forests can be developed in parallel, as is the case with
    bagging. The idea of averaging over multiple trees is to ensure the balance between
    the bias and variance trade-off. Boosting is the third most important extension
    of the decision trees, and probably the most effective one. It is again based
    on ensembling homogeneous base learners (in this case, trees), as are the bagging
    and random forests. The design of the boosting algorithm is completely different
    though. It is a *sequential* ensemble method in that the residual/misclassified
    point of the previous learner is improvised in the next run of the algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中介绍的基于树的集成方法，*Bagging* 和 *随机森林*，是决策树的重要扩展。然而，虽然Bagging通过平均多个决策树提供了更大的稳定性，但偏差仍然存在。这种局限性促使Breiman在每个分割点采样协变量以生成一组“独立”的树，并为随机森林奠定了基础。随机森林中的树可以像Bagging一样并行开发。在多个树上进行平均的想法是为了确保偏差和方差权衡之间的平衡。提升是决策树的第三大扩展，也可能是最有效的一种。它同样基于集成同质基学习器（在这种情况下，是树），就像Bagging和随机森林一样。提升算法的设计完全不同。它是一种*顺序*集成方法，因为在算法的下一个运行中，前一个学习者的残差/误分类点被改进。
- en: The general boosting technique consists of a family of algorithms that convert
    weak learners to provide a final strong learner. In the context of classification
    problems, a weak learner is a technique that is better than a random guess. That
    the method converts a weak learning algorithm into a better one is the reason
    it gets the name *boosting*. The boosting technique is designed to deliver a strong
    learner that is closer to the perfect classifier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通用提升技术包括一系列算法，这些算法将弱学习器转换为最终强学习器。在分类问题的背景下，弱学习器是一种比随机猜测更好的技术。该方法将弱学习算法转换为更好的算法，这就是它被称为
    *提升* 的原因。提升技术旨在提供一种接近完美分类器的强学习器。
- en: '| Classifier | Sub-space ![The general boosting algorithm](img/00199.jpeg)
    | Sub-space ![The general boosting algorithm](img/00200.jpeg) | Sub-space ![The
    general boosting algorithm](img/00201.jpeg) | Sub-space ![The general boosting
    algorithm](img/00202.jpeg) | Accuracy |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 子空间 ![通用提升算法](img/00199.jpeg) | 子空间 ![通用提升算法](img/00200.jpeg) | 子空间
    ![通用提升算法](img/00201.jpeg) | 子空间 ![通用提升算法](img/00202.jpeg) | 准确率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ![The general boosting algorithm](img/00203.jpeg) | R | R | R | Q | 0.75
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00203.jpeg) | R | R | R | Q | 0.75 |'
- en: '| ![The general boosting algorithm](img/00204.jpeg) | R | R | Q | R | 0.75
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00204.jpeg) | R | R | Q | R | 0.75 |'
- en: '| ![The general boosting algorithm](img/00205.jpeg) | R | Q | R | R | 0.75
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00205.jpeg) | R | Q | R | R | 0.75 |'
- en: '| ![The general boosting algorithm](img/00206.jpeg) | Q | R | R | R | 0.75
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00206.jpeg) | Q | R | R | R | 0.75 |'
- en: Table 1 A simple classifier scenario
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 简单分类器场景
- en: A broader motivation for boosting can be understood through a simple example.
    Suppose that the random sample of size *n* is drawn as IID from the sample space
    ![The general boosting algorithm](img/00207.jpeg). The distribution of the random
    sample is assumed to be D. Suppose we have *T=4* classifiers in ![The general
    boosting algorithm](img/00208.jpeg), with the classifiers being used for the truth
    function *f*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个简单的例子可以更广泛地理解提升的动机。假设从样本空间 ![通用提升算法](img/00207.jpeg) 中抽取大小为 *n* 的随机样本，作为独立同分布（IID）的样本。随机样本的分布假设为D。假设在
    ![通用提升算法](img/00208.jpeg) 中有 *T=4* 个分类器，分类器用于真值函数 *f*。
- en: 'Consider a hypothetical scenario where the sample space ![The general boosting
    algorithm](img/00209.jpeg) consists of four parts in ![The general boosting algorithm](img/00210.jpeg),
    and the four classifiers perform as indicated in previous table. The idea behind
    the development of the boosting method is to improvise the classifier in a sequential
    manner. That is, combining the classifier is approached one after another and
    not all at the same time. Now, the errors of ![The general boosting algorithm](img/00211.jpeg)
    will be corrected to a new distribution D'' with the errors of the classifier
    being given more weight for the region ![The general boosting algorithm](img/00212.jpeg).
    The classifier ![The general boosting algorithm](img/00213.jpeg) will use the
    distribution D'' and its error zone instances in the region ![The general boosting
    algorithm](img/00214.jpeg) will be given more weight leading to a distribution
    D''''. The boosting method will continue the process for the remaining classifiers
    and give an overall combiner/ensemble. A pseudo-boosting algorithm (see [Chapter
    2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee "Chapter 2. Bootstrapping")
    of Zhou (2012)) is summarized in the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个假设场景，其中样本空间 ![通用提升算法](img/00209.jpeg) 在 ![通用提升算法](img/00210.jpeg) 中由四个部分组成，四个分类器按前表所示执行。提升方法背后的思想是以顺序方式改进分类器。也就是说，分类器是一个接一个地组合，而不是同时全部组合。现在，![通用提升算法](img/00211.jpeg)
    的错误将被纠正到一个新的分布 D'，其中分类器在区域 ![通用提升算法](img/00212.jpeg) 的错误将给予更多权重，导致分布 D''。提升方法将继续对剩余的分类器进行过程，并给出一个总的组合/集成。Zhou（2012）的伪提升算法（见[第2章](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "第2章。引导")）总结如下：
- en: 'Step 0: The initial sample distribution is D, and set D1 = D'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 0：初始样本分布是 D，并设置 D1 = D
- en: 'Steps ![The general boosting algorithm](img/00215.jpeg):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 ![通用提升算法](img/00215.jpeg)：
- en: '![The general boosting algorithm](img/00216.jpeg) Dt'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![通用提升算法](img/00216.jpeg) Dt'
- en: '![The general boosting algorithm](img/00217.jpeg)'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![通用提升算法](img/00217.jpeg)'
- en: '*Dt+1 = Improve Distribution (Dt,*![The general boosting algorithm](img/00218.jpeg)*)*'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dt+1 = 改进分布 (Dt,*![通用提升算法](img/00218.jpeg)*)*'
- en: 'Final step: ![The general boosting algorithm](img/00219.jpeg)'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步：![通用提升算法](img/00219.jpeg)
- en: The two steps of the algorithm in *Improve distribution* and *Combine outputs*
    clearly need implementable actions. In the next section, we will develop the adaptive
    boosting method with a clear numerical illustration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 算法中的两个步骤*改进分布*和*组合输出*显然需要可执行的操作。在下一节中，我们将通过清晰的数值说明来开发自适应提升方法。
- en: Adaptive boosting
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应提升
- en: Schapire and Freund invented the adaptive boosting method. **Adaboost** is a
    popular abbreviation of this technique.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Schapire和Freund发明了自适应提升方法。**Adaboost**是这个技术的流行缩写。
- en: 'The generic adaptive boosting algorithm is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通用自适应提升算法如下：
- en: Initialize the observation weights uniformly:![Adaptive boosting](img/00220.jpeg)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均匀初始化观测权重：![自适应提升](img/00220.jpeg)
- en: 'For *m*, classifier *hm*, from *1* to *m* number of passes over with the data,
    perform the following tasks:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *m*，分类器 *hm* 从 *1* 到 *m* 对数据进行多次遍历，执行以下任务：
- en: Fit a classifier *hm* to the training data using the weights ![Adaptive boosting](img/00221.jpeg)
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用权重 ![自适应提升](img/00221.jpeg) 对训练数据拟合分类器 *hm*。
- en: Compute the error for each classifier as follows:![Adaptive boosting](img/00222.jpeg)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个分类器的误差如下：![自适应提升](img/00222.jpeg)
- en: Compute the *voting power* of the classifier *hm*:![Adaptive boosting](img/00223.jpeg)
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算分类器 *hm* 的*投票权重*：![自适应提升](img/00223.jpeg)
- en: Set ![Adaptive boosting](img/00224.jpeg)![Adaptive boosting](img/00225.jpeg)
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 ![自适应提升](img/00224.jpeg) ![自适应提升](img/00225.jpeg)
- en: Output:![Adaptive boosting](img/00226.jpeg)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：![自适应提升](img/00226.jpeg)
- en: 'Simply put, the algorithm unfolds as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，算法展开如下：
- en: Initially, we start with uniform weights ![Adaptive boosting](img/00227.jpeg)
    for all observations.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始时，我们对所有观测值使用均匀权重 ![自适应提升](img/00227.jpeg)。
- en: In the next step, we calculate the weighted error ![Adaptive boosting](img/00228.jpeg)
    for each of the classifiers under consideration.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们计算考虑的每个分类器的加权误差 ![自适应提升](img/00228.jpeg)。
- en: A classifier (usually stumps, or decision trees with a single split) needs to
    be selected and the practice is to select the classifier with the maximum accuracy.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器（通常是树桩，或单分支的决策树）需要被选择，通常的做法是选择具有最大准确率的分类器。
- en: In *Improve distribution and Combine outputs* case of ties, any accuracy tied
    classifier is selected.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*改进分布和组合输出*的情况下，如果有准确率相同的分类器，则选择任意一个。
- en: 'Next, the misclassified observations are given more weights and the values
    that are correctly classified are down-weighted. An important point needs to be
    recorded here:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，错误分类的观察值被赋予更多的权重，而正确分类的值被降低权重。这里需要记录一个重要点：
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In the weights update step, the sum of weights correctly classified as observations
    will equal the sum of weights of the misclassified observations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重更新步骤中，正确分类为观察值的权重之和将等于错误分类的观察值的权重之和。
- en: The steps from computing the error of the classifier to the weight updating
    step are repeated M number of times, and the voting power of each classifier is
    obtained. For any given observation, we then make the prediction by using the
    predictions across the M classifiers weighted by their respective voting power
    and using the sign function as specified in the algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算分类器的误差到权重更新步骤的步骤会重复M次，从而获得每个分类器的投票权重。对于任何给定的观察值，我们然后使用M个分类器的预测，这些预测根据各自的投票权重加权，并使用算法中指定的符号函数进行预测。
- en: As simplified as the algorithm may be, it is a useful exercise to undertake
    the working of the adaptive boosting method through a toy dataset. The data and
    computational approach is taken from the video of Jessica Noss, available at [https://www.youtube.com/watch?v=gmok1h8wG-Q](https://www.youtube.com/watch?v=gmok1h8wG-Q).
    The illustration of the adaptive boosting algorithm begins now.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管算法可能很简单，但通过玩具数据集来执行自适应提升方法的工作是一个有用的练习。数据和计算方法取自Jessica Noss的视频，可在[https://www.youtube.com/watch?v=gmok1h8wG-Q](https://www.youtube.com/watch?v=gmok1h8wG-Q)找到。自适应提升算法的说明现在开始。
- en: 'Consider a toy data set with five triplet points: two explanatory variables
    and one binary output value. The variables and data can be summarized with ![Adaptive
    boosting](img/00229.jpeg), and here we have the data points as ![Adaptive boosting](img/00230.jpeg),
    ![Adaptive boosting](img/00231.jpeg), ![Adaptive boosting](img/00232.jpeg), ![Adaptive
    boosting](img/00233.jpeg), and ![Adaptive boosting](img/00234.jpeg). The data
    will be first entered in R and then visualized as a preliminary step:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含五个三元点的玩具数据集：两个解释变量和一个二元输出值。变量和数据可以用![自适应提升](img/00229.jpeg)来总结，这里的数据点有![自适应提升](img/00230.jpeg)、![自适应提升](img/00231.jpeg)、![自适应提升](img/00232.jpeg)、![自适应提升](img/00233.jpeg)和![自适应提升](img/00234.jpeg)。数据首先输入到R中，然后作为初步步骤进行可视化：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Adaptive boosting](img/00235.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![自适应提升](img/00235.jpeg)'
- en: 'Figure 1: A simple depiction of the toy dataset'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：玩具数据集的简单表示
- en: Stumps are a particular case of a decision tree that has been mentioned in the
    discussion. Here, we will use the stumps as the base learners. A simple look at
    the preceding diagram helps us to easily find stumps that have an accuracy higher
    than a random guess.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 树桩是决策树的一种特殊情况，已在讨论中提到。在这里，我们将使用树桩作为基学习器。简单看一下前面的图可以帮助我们轻松找到准确率高于随机猜测的树桩。
- en: 'For example, we can put a stump at ![Adaptive boosting](img/00236.jpeg) and
    mark all the observations on the left side as positives and those on the right
    as negatives. In the following program, the points in the green shaded region
    are positives as predicted by the stumps, and those in the red shaded region are
    negatives. Similarly, we can use additional stumps at ![Adaptive boosting](img/00237.jpeg)
    and ![Adaptive boosting](img/00238.jpeg). The predictions can be swapped for the
    same stumps too, thanks to `symmetry()`. Thus, earlier we put the green shaded
    region to the left of ![Adaptive boosting](img/00239.jpeg) and predicted the values
    as positives, and by reversing the order the area on the right side of the stump
    ![Adaptive boosting](img/00240.jpeg) will be marked as positives. A similar classification
    is made for the negatives. The task is repeated at stumps ![Adaptive boosting](img/00241.jpeg)
    and ![Adaptive boosting](img/00242.jpeg). Using the `par`, `plot`, `text`, and
    `rect` graphical functions, we present visual depictions of these base learners
    in the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在![自适应提升](img/00236.jpeg)处放置一个树桩，并将左侧的所有观察值标记为正，右侧的观察值标记为负。在以下程序中，绿色阴影区域的点被树桩预测为正，而红色阴影区域的点被预测为负。同样，我们可以在![自适应提升](img/00237.jpeg)和![自适应提升](img/00238.jpeg)处使用额外的树桩。通过`symmetry()`函数，我们还可以交换相同树桩的预测。因此，我们之前将绿色阴影区域放在![自适应提升](img/00239.jpeg)的左侧，并预测值为正，通过反转顺序，树桩![自适应提升](img/00240.jpeg)右侧的区域将被标记为正。对负值也进行类似的分类。在![自适应提升](img/00241.jpeg)和![自适应提升](img/00242.jpeg)处重复此任务。使用`par`、`plot`、`text`和`rect`图形函数，我们在以下内容中展示这些基学习器的可视化表示：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The result of the preceding R program is shown in the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 前面R程序的结果如下所示：
- en: '![Adaptive boosting](img/00243.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![自适应提升](img/00243.jpeg)'
- en: 'Figure 2: Stump classifiers based on X1'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于X1的树桩分类器
- en: 'Note that a similar classification can be obtained for the variable ![Adaptive
    boosting](img/00244.jpeg) at the points 2, 4, and 6\. Though there is no need
    to give the complete R program for stumps based on ![Adaptive boosting](img/00245.jpeg),
    we simply produce the output in the following diagram. The program is available
    in the code bundle. The stumps based on ![Adaptive boosting](img/00246.jpeg) will
    be ignored in the rest of the discussion:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在点2、4和6处，对于变量![自适应提升](img/00244.jpeg)可以获得类似的分类。尽管不需要给出基于![自适应提升](img/00245.jpeg)的树桩的完整R程序，我们只需在以下图表中生成输出。程序可在代码包中找到。在接下来的讨论中，将忽略基于![自适应提升](img/00246.jpeg)的树桩：
- en: '![Adaptive boosting](img/00247.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![自适应提升](img/00247.jpeg)'
- en: 'Figure 3: Stump classifiers based on X2'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于X2的树桩分类器
- en: 'The choice of the stump based on ![Adaptive boosting](img/00248.jpeg) leads
    to a few misclassifications, and we can see that the observations P1, P4, and
    P3 are correctly classified while P2 and P5 are misclassified. The predictions
    based on this stump can then be put as (1,-1,-1,1,-1). The stump based on ![Adaptive
    boosting](img/00249.jpeg) classifies points P1 and P4 correctly, while P2, P3,
    and P5 are misclassified, and the prediction in vector form here is (1,-1,1,1,-1).
    The six models considered here will be denoted in the R program by M1, M2, …,
    M6, and in terms of the algorithm specified earlier, we have ![Adaptive boosting](img/00250.jpeg).
    Similarly, we have predictions for the other four stumps and we enter them in
    R, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于![自适应提升](img/00248.jpeg)选择的树桩导致了一些误分类，我们可以看到观察点P1、P4和P3被正确分类，而P2和P5被误分类。基于这个树桩的预测可以表示为（1，-1，-1，1，-1）。基于![自适应提升](img/00249.jpeg)的树桩正确分类了点P1和P4，而P2、P3和P5被误分类，这里的向量预测为（1，-1，1，1，-1）。这里考虑的六个模型在R程序中用M1、M2、…、M6表示，根据前面指定的算法，我们有![自适应提升](img/00250.jpeg)。同样，我们还有其他四个树桩的预测，并将它们输入到R中，如下所示：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the predictions given by the six models `M1-M6`, we can compare them with
    the true labels in `y` and see which observations are misclassified in each of
    these models:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用六个模型`M1-M6`给出的预测，我们可以将它们与`y`中的真实标签进行比较，以查看在每个模型中哪些观察值被误分类：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Thus, the values of `TRUE` mean that the column named points is misclassified
    in the row named model. The weights ![Adaptive boosting](img/00251.jpeg) are initialized
    and the weighted errors ![Adaptive boosting](img/00252.jpeg), are computed for
    each of the models in the following R block:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`TRUE`的值表示在名为模型的行中，名为点的列被误分类。初始化权重![自适应提升](img/00251.jpeg)，并在以下R代码块中计算每个模型的加权误差![自适应提升](img/00252.jpeg)：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since the error corresponding to Model 3, or ![Adaptive boosting](img/00253.jpeg),
    is the minimum, we select it first and calculate the voting power ![Adaptive boosting](img/00254.jpeg)
    assignable to it as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对应模型3，或 ![自适应提升](img/00253.jpeg) 的误差是最小的，我们首先选择它，并按照以下方式计算分配给它的投票权重 ![自适应提升](img/00254.jpeg)：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Consequently, the boosting algorithm steps state that ![Adaptive boosting](img/00255.jpeg)
    gives us the required predictions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提升算法步骤表明 ![自适应提升](img/00255.jpeg) 给出了所需的预测：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The central observation, `P3`, remains misclassified and so we proceed to the
    next step.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 中心观察点 `P3` 仍然被错误分类，所以我们继续到下一步。
- en: 'Now we need to update the weights ![Adaptive boosting](img/00256.jpeg) and
    for the classification problem the rule in a simplified form is given using:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要更新权重 ![自适应提升](img/00256.jpeg)，对于分类问题，简化形式的规则如下：
- en: '![Adaptive boosting](img/00257.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![自适应提升](img/00257.jpeg)'
- en: 'Consequently, we need a function that will take the weight of a previous run,
    the error rate, and the misclassifications by the model as inputs and then return
    them as the updated weights that incorporated the preceding formula. We define
    such a function as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要一个函数，它将接受前一次运行的权重、错误率和模型错误分类作为输入，然后返回包含先前公式的更新权重。我们定义这样的函数如下：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will update the weights and calculate the error for each of the six
    models:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将更新权重并计算六个模型中的每个模型的误差：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, models `M1` and `M5` have equal error rates with the new weights, and
    we simply choose Model 1, calculate its voting power, and predict based on the
    updated model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型 `M1` 和 `M5` 使用新的权重具有相等的错误率，我们简单地选择模型1，计算其投票权重，并基于更新后的模型进行预测：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since the point `P3` is still misclassified, we proceed with the iterations
    and apply the cycle once more:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点 `P3` 仍然被错误分类，我们继续迭代并再次应用循环：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now the classification is perfect and after three iterations, we don't have
    any misclassifications or errors. The purpose of the programming in this section
    was to demonstrate the steps in the adaptive boosting algorithm in an elementary
    way. In the next section, we will look at the *gradient boosting* technique.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在分类是完美的，经过三次迭代后，我们没有任何错误分类或错误。本节编程的目的是以基本的方式展示自适应提升算法的步骤。在下一节中，我们将探讨 *梯度提升*
    技术。
- en: Gradient boosting
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: The adaptive boosting method can't be applied to the regression problem since
    it is constructed to address the classification problem. The gradient boosting
    method can be used for both the classification and regression problems with suitable
    loss functions. In fact, the use of gradient boosting methods goes beyond these
    two standard problems. The technique originated from some of Breiman's observations
    and developed into regression problems by Freidman (2000). We will take the rudimentary
    code explanation in the next section without even laying out the algorithm. After
    the setup is clear, we will formally state the boosting algorithm for the squared-error
    loss function in the following subsection and create a new function implementing
    the algorithm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应提升方法不能应用于回归问题，因为它旨在解决分类问题。梯度提升方法可以使用适当的损失函数来解决分类和回归问题。实际上，梯度提升方法的应用不仅限于这两个标准问题。这项技术起源于Breiman的一些观察，并由Friedman（2000）发展成回归问题。在下一节中，我们将对基本代码进行解释，而无需展示算法。在设置清楚之后，我们将在下一个小节中正式陈述针对平方误差损失函数的提升算法，并创建一个新的函数来实现该算法。
- en: 'The following diagram is a depiction of the standard sine wave function. It
    is clearly a nonlinear relationship. Without explicitly using sine transformations,
    we will see the use of the boosting algorithm to learn this function. Of course,
    we need simple regression stumps and we begin with a simple function, `getNode`,
    that will give us the desired split:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示是标准正弦波函数的描述。很明显，这是一个非线性关系。在不显式使用正弦变换的情况下，我们将看到使用提升算法来学习这个函数。当然，我们需要简单的回归树桩，我们从一个简单的函数
    `getNode` 开始，它将给出我们想要的分割：
- en: '![Gradient boosting](img/00258.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升](img/00258.jpeg)'
- en: 'Figure 4: Can boosting work for nonlinear sine data?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：提升算法能否用于非线性正弦数据？
- en: Building it from scratch
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始构建
- en: In the previous section, we used simple classification stumps. In that example,
    a simple visual inspection sufficed to identify the stumps, and we quickly obtained
    12 classification stumps. For the regression problem, we will first define a `getNode`
    function, which is a slight modification of the function defined in Chapter 9
    of Tattar (2017). The required notation is first set up.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了简单的分类树桩。在那个例子中，简单的视觉检查就足以识别树桩，我们很快获得了12个分类树桩。对于回归问题，我们首先定义一个`getNode`函数，这是对Tattar（2017）第9章中定义的函数的轻微修改。首先设置所需的符号。
- en: Suppose we have n pairs of data points ![Building it from scratch](img/00259.jpeg)
    and we are trying to learn the relationship ![Building it from scratch](img/00260.jpeg),
    where the form of *f* is completely unknown to us.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一对n个数据点![从头开始构建](img/00259.jpeg)，我们正在尝试学习![从头开始构建](img/00260.jpeg)之间的关系，其中*f*的形式对我们来说完全未知。
- en: 'For the regression tree, the split criteria are rather straightforward. For
    data split by an x-value, we calculate the sum of mean difference squares of *ys*
    in each of the partitioned part and then add them up. The split criteria are chosen
    as that x-value. This maximizes the sum of the mean difference squares in the
    variable of interest. The R function, `getNode`, implements this thinking:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归树，分割标准相当直接。对于按x值分割的数据，我们计算每个分割部分的*ys*的平均差平方和，然后将它们加起来。分割标准被选为那个x值。这最大化了感兴趣变量中的平均差平方和。R函数`getNode`实现了这种思考：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first step in the `getNode` function is finding the unique values of `x`
    and then sorting them in decrease(ing) order. For the unique values, we calculate
    the sum of squares through a for loop. The first step in the loop is to partition
    the data in right and left parts.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`getNode`函数的第一步是找到`x`的唯一值，然后按降序排序。对于唯一值，我们通过for循环计算平方和。循环的第一步是将数据分割成左右两部分。'
- en: The sum of mean difference squares is calculated in each of the partitions for
    a specific unique value, and then summed up to get the overall residual sum of
    squares.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特定唯一值，在每个分区中计算平均差平方和，然后将它们加起来以得到总的残差平方和。
- en: We then obtain the value of `x`, which leads to the least residual sum of squares.
    The prediction in the partitioned regions is the mean of the y-values in those
    regions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们获取`x`的值，它导致最小的残差平方和。在分割区域中的预测是该区域y值的平均值。
- en: The `getNode` function closes by returning the split value of `x`, and the predictions
    for the right and left partitions. We are now ready to create regression stumps.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`getNode`函数通过返回`x`的分割值和左右分区的预测值来结束。我们现在可以创建回归树桩。'
- en: 'The sine wave data is first easily created and we allow the x-values to range
    in the interval ![Building it from scratch](img/00261.jpeg). The y-value is simply
    the sin function applied on the x vector:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦波数据首先很容易创建，我们允许x值在![从头开始构建](img/00261.jpeg)区间内变化。y值是简单地将正弦函数应用于x向量：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result of the preceding display will be *Figure 1*. We proceed to obtain
    the first split of data and then display the mean of the right and left partitions
    on the graph. The residuals will be from the sine wave and they are also put on
    the same display, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的显示结果将是**图1**。我们继续获取数据的第一次分割，并在图上显示左右分区的平均值。残差将来自正弦波，它们也将放在同一显示中，如下所示：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, our first split point occurs at `x` value of, ![Building it from scratch](img/00262.jpeg)
    here, `3.141593`. The prediction for the right side of the split point is `-0.6353102`
    and for the left side it is `0.6050574`. The predictions are plotted on the same
    display using the segments function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的第一个分割点发生在`x`值为，![从头开始构建](img/00262.jpeg)这里，`3.141593`。分割点右侧的预测值为`-0.6353102`，左侧的预测值为`0.6050574`。预测值使用`segments`函数在同一显示上绘制：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, the predictions are easy to obtain here and a simple `ifelse` function
    helps in calculating them. The deviation from the sine wave is the residuals,
    and we calculate that the first set of residuals and `summary` function give the
    brief of the residual values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，预测很容易获得，简单的`ifelse`函数有助于计算它们。与正弦波之间的偏差是残差，我们计算第一组残差和`summary`函数给出了残差值的简要概述：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first step in the prediction is saved in the `GBFit` object and the difference
    between the fit and predictions is found in the `first_residuals` vector. This
    completes the first iteration of the gradient boosting algorithm. The residuals
    of the first iteration will become the regressand/output variable for the second
    iteration. Using the `getNode` function, we carry out the second iteration, which
    mimics the earlier set of code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的第一步被保存在`GBFit`对象中，拟合与预测之间的差异在`first_residuals`向量中找到。这完成了梯度提升算法的第一轮迭代。第一轮迭代的残差将成为第二轮迭代的回归因变量/输出变量。使用`getNode`函数，我们执行第二轮迭代，这模仿了早期的代码集：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'An important difference here is that we update the prediction not by averaging
    but by adding. Note that we are modeling the residual of the first step and hence
    the remaining part of the residual explained by the next fitting needs to be added
    and not averaged. What is the range of residuals? The reader is advised to compare
    the residual values with earlier iterations. A similar extension is carried out
    for the third iteration:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个重要区别是我们通过累加而不是平均来更新预测。请注意，我们正在模拟第一步的残差，因此下一个拟合所解释的残差剩余部分需要累加而不是平均。残差的范围是多少？建议读者将残差值与早期迭代进行比较。对第三次迭代执行类似的扩展：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'All of the visual display is shown in the following diagram:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的可视化显示在以下图中：
- en: '![Building it from scratch](img/00263.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![从头开始构建](img/00263.jpeg)'
- en: 'Figure 5: Three iterations of the gradient boosting algorithm'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：梯度提升算法的三次迭代
- en: 'Obviously, we can''t keep on carrying out the iterations in a detailed execution
    every time and looping is important. The code is kept in a block and 22 more iterations
    are performed. The output at the end of each iteration is depicted in the diagram
    and we put them all in an external file, `Sine_Wave_25_Iterations.pdf`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们不可能每次都进行详细的执行迭代，循环是非常重要的。代码被保存在一个块中，并执行了22次更多迭代。每次迭代的输出在图中展示，我们将它们全部放入一个外部文件，`Sine_Wave_25_Iterations.pdf`：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Following the 25 iterations, we have an overall fit in `GBFit` and we can plot
    this against the actual y values to see how well the gradient boosting algorithm
    has performed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 经过25次迭代后，我们在`GBFit`中有一个整体拟合，我们可以将其与实际的y值进行比较，以查看梯度提升算法的表现如何：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Building it from scratch](img/00264.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![从头开始构建](img/00264.jpeg)'
- en: 'Figure 6: Gradient fit versus actual sine data'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：梯度拟合与实际正弦数据
- en: The fit is reasonably good for a nonlinear model. The approach was to get a
    clear understanding of the gradient boosting algorithm. A more general form of
    the boosting algorithm is discussed and developed in the next subsection.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非线性模型来说，拟合是相当好的。这种方法是为了清楚地理解梯度提升算法。在下一小节中讨论和开发了提升算法的更一般形式。
- en: Squared-error loss function
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平方误差损失函数
- en: 'Denote the data by ![Squared-error loss function](img/00265.jpeg), and fix
    the number of iterations/trees as a number *B*. Choose a shrinkage factor ![Squared-error
    loss function](img/00266.jpeg) and tree depth *d*. The gradient boosting algorithm
    based on the squared-error loss function is stated here briefly. See Algorithm
    17.2 of Efron and Hastie (2016), as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 用 ![平方误差损失函数](img/00265.jpeg) 表示数据，并将迭代/树的数量固定为一个数字 *B*。选择一个收缩因子 ![平方误差损失函数](img/00266.jpeg)
    和树深度 *d*。基于平方误差损失函数的梯度提升算法在此简要说明。参见Efron和Hastie（2016）的第17.2算法，如下：
- en: Initialize residuals ![Squared-error loss function](img/00267.jpeg) and the
    gradient boosting prediction as ![Squared-error loss function](img/00268.jpeg)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化残差 ![平方误差损失函数](img/00267.jpeg) 和梯度提升预测 ![平方误差损失函数](img/00268.jpeg)
- en: 'For ![Squared-error loss function](img/00269.jpeg):'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![平方误差损失函数](img/00269.jpeg)：
- en: Fit a regression tree of depth *d* for the data ![Squared-error loss function](img/00270.jpeg)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为数据 ![平方误差损失函数](img/00270.jpeg) 拟合深度为 *d* 的回归树
- en: Obtain the predicted values as ![Squared-error loss function](img/00271.jpeg)
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得预测值 ![平方误差损失函数](img/00271.jpeg)
- en: Update the boosting prediction by ![Squared-error loss function](img/00272.jpeg)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 ![平方误差损失函数](img/00272.jpeg) 更新提升预测
- en: Update the residuals ![Squared-error loss function](img/00273.jpeg)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新残差 ![平方误差损失函数](img/00273.jpeg)
- en: Return the sequence of functions ![Squared-error loss function](img/00274.jpeg)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回函数序列 ![平方误差损失函数](img/00274.jpeg)
- en: 'Now, we will define a function `GB_SqEL`, which will implement the gradient
    boosting algorithm driven by the squared-error loss function. The function must
    be provided with five arguments: `y` and `x` will constitute the data, `depth`
    will specify the depth of the trees (that is, the number of splits in the regression
    tree), `iter` for the number of iterations, and `shrinkage` is the ![Squared-error
    loss function](img/00275.jpeg) factor. The `GB_SqEL` function is set up as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个名为`GB_SqEL`的函数，该函数将实现由平方误差损失函数驱动的梯度提升算法。该函数必须提供五个参数：`y`和`x`将构成数据，`depth`将指定树的深度（即回归树中的分割数），`iter`表示迭代次数，`shrinkage`是![平方误差损失函数](img/00275.jpeg)因子。`GB_SqEL`函数的设置如下：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The initialization takes place in specifications, and the line `fit <- y*0`.
    The depth argument of the algorithm is taken in the line `maxdepth=depth`, and
    using the `rpart` function, we create a tree of the necessary depth. The `predict`
    function gives the values of ![Squared-error loss function](img/00276.jpeg) as
    required at each iteration, while `fit+gb_hat` does the necessary update. Note
    that `GB_Hat[,i]` consists of the predicted values at the end of each iteration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化发生在参数设置中，行`fit <- y*0`。算法的深度参数在行`maxdepth=depth`中指定，使用`rpart`函数创建所需深度的树。`predict`函数在每次迭代时提供![平方误差损失函数](img/00276.jpeg)的值，而`fit+gb_hat`执行必要的更新。请注意，`GB_Hat[,i]`包含每个迭代结束时的预测值。
- en: 'We will illustrate the algorithm with the example of Efron and Hastie (2016).
    The data considered is related with Lu Gerig''s disease, or **amyotrophic lateral
    sclerosis** (**ALS**). The dataset has information on 1,822 individuals with the
    ALS disease. The goal is to predict the rate of progression `dFRS` of a functional
    rating score. The study has information on 369 predictors/covariates. Here, we
    will use the `GB_SqEL` function to fit the gradient boosting technique and analyze
    the mean square error as the number of iterations increases. The details and the
    data can be obtained from the source at [https://web.stanford.edu/~hastie/CASI/data.html](https://web.stanford.edu/~hastie/CASI/data.html).
    We will now put the squared-error loss function-driven boosting method into action:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以Efron和Hastie（2016）的例子来说明算法。考虑的数据与Lu Gerig的疾病，或**肌萎缩侧索硬化症**（**ALS**）有关。数据集包含有关1,822名患有ALS疾病的人的信息。目标是预测功能评分的进展率`dFRS`。研究有关于369个预测因子/协变量的信息。在这里，我们将使用`GB_SqEL`函数来拟合梯度提升技术，并随着迭代次数的增加分析均方误差。详细信息和数据可以从[https://web.stanford.edu/~hastie/CASI/data.html](https://web.stanford.edu/~hastie/CASI/data.html)的源文件中获得。现在，我们将平方误差损失函数驱动的提升方法付诸实践：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using the `read.table` function, we import the data from the code bundle into
    the `als` object. The data is available from the source in the `.txt` format.
    The column `testset` indicates whether the observations were marked for training
    purposes or for tests. We select the training observations and also drop the first
    variable `testset` and store it in the object `alst`. The `GB_SqEL` function is
    applied on the `alst` object with appropriate specifications.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`read.table`函数，我们将代码包中的数据导入到`als`对象中。数据以`.txt`格式从源文件中提供。列`testset`表示观察值是为了训练目的还是为了测试。我们选择了训练观察值，并删除了第一个变量`testset`，将其存储在对象`alst`中。对`alst`对象应用了`GB_SqEL`函数，并指定了适当的参数。
- en: 'Following each iteration, we compute the mean-squared error and store it in
    `GB_Hat`, as explained earlier. We can see from the following diagram that as
    the iterations increase, the mean squared error decreases. Here, the algorithm
    stabilizes after nearly 200 iterations:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代之后，我们计算均方误差并将其存储在`GB_Hat`中，如前所述。从以下图中我们可以看出，随着迭代次数的增加，均方误差逐渐减小。在这里，算法在接近200次迭代后稳定下来：
- en: '![Squared-error loss function](img/00277.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![平方误差损失函数](img/00277.jpeg)'
- en: 'Figure 7: Gradient boosting and the MSE by iterations'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：梯度提升和迭代均方误差
- en: In the next section, we will see the use of two powerful R packages.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到两个强大R包的使用。
- en: Using the adabag and gbm packages
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用adabag和gbm包
- en: 'Using the boosting method as an ensemble technique is indeed very effective.
    The algorithm was illustrated for classification and regression problems from
    scratch. Once the understand the algorithm clear and transparent, we can then
    use R packages to deliver results going forward. A host of packages are available
    for implementing the boosting technique. However, we will use the two most popular
    packages `adabag` and `gbm` in this section. First, a look at the options of the
    two functions is in order. The names are obvious and `adabag` implements the adaptive
    boosting methods while `gbm` deals with gradient boosting methods. First, we look
    at the options available in these two functions in the following code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 将提升方法作为集成技术确实非常有效。算法从头开始展示了分类和回归问题。一旦我们清楚地理解了算法，我们就可以使用R包来提供未来的结果。有许多包可用于实现提升技术。然而，在本节中，我们将使用两个最受欢迎的包`adabag`和`gbm`。首先，我们需要查看这两个函数的选项。名称很明显，`adabag`实现了自适应提升方法，而`gbm`处理梯度提升方法。首先，我们查看以下代码中这两个函数可用的选项：
- en: '![Using the adabag and gbm packages](img/00278.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![使用adabag和gbm包](img/00278.jpeg)'
- en: The boosting and gbm functions
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 提升和gbm函数
- en: The formula is the usual argument. The argument `mfinal` in `adabag` and `n.trees`
    in `gbm` allows the specification of the number of trees or iterations. The boosting
    function gives the option of `boos`, which is the bootstrap sample of the training
    set drawn using the weights for each observation on that iteration. Gradient boosting
    is a more generic algorithm that is capable of handling more than the regression
    structure. It can be used for classification problems as well. The option of `distribution`
    in the `gbm` function gives those options. Similarly, one can see here that the
    `gbm` function offers a host of other options. We will neither undertake the daunting
    task of explaining them all nor apply them to complex datasets. The two datasets
    that were used to explain and elaborate adaptive and gradient boosting algorithms
    will be continued with the `boosting` and `gbm` functions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 公式是常规的参数。在`adabag`中的`mfinal`参数和`gbm`中的`n.trees`参数允许指定树的数量或迭代次数。提升函数提供了`boos`选项，这是使用每个观测值的权重在该迭代中抽取的培训集的bootstrap样本。梯度提升是一个更通用的算法，能够处理比回归结构更多的内容。它可以用于分类问题。`gbm`函数中的`distribution`选项提供了这些选项。同样，在这里可以看到`gbm`函数提供了许多其他选项。我们既不会承担解释所有这些选项的艰巨任务，也不会将它们应用于复杂的数据集。用于解释和阐述自适应和梯度提升算法的两个数据集将使用`boosting`和`gbm`函数继续进行。
- en: 'The toy datasets need to be changed and we will replicate them multiple times
    over so that we have enough observations for running the `boosting` and `gbm`
    functions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 需要更改玩具数据集，我们将多次复制它们，以便我们有足够的观测值来运行`boosting`和`gbm`函数：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `maxdepth=1` function ensures that we are using only stumps as the base
    classifiers. It is easily seen that the boosting function works perfectly, as
    all the observations are correctly classified.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxdepth=1`函数确保我们只使用树桩作为基础分类器。很容易看出提升函数工作得非常完美，因为所有观测都被正确分类。'
- en: 'As with the `boosting` function, we need to have more data points. We increase
    this with the `seq` function and using the `distribution="gaussian"` option, we
    ask the `gbm` function to fit the regression boosting technique:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与`boosting`函数一样，我们需要更多的数据点。我们通过`seq`函数增加这些数据点，并使用`distribution="gaussian"`选项，要求`gbm`函数拟合回归提升技术：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using the plot functions, we make a comparison of the fit with the gradient
    boosting method. The following diagram suggests that the fit has been appropriate.
    However, the plots also show that something is not quite correct with the story
    either. The function approximation at ![Using the adabag and gbm packages](img/00279.jpeg)
    and ![Using the adabag and gbm packages](img/00280.jpeg) by the boosting method
    leaves a lot to be desired, and the actual versus predicted plot suggests discontinuity/poor
    performance at 0\. However, we will not delve too far into these issues:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用绘图函数，我们比较了梯度提升方法的拟合情况。以下图表表明拟合是适当的。然而，图表也显示故事中有些地方不太对劲。提升方法在![使用adabag和gbm包](img/00279.jpeg)和![使用adabag和gbm包](img/00280.jpeg)处的函数近似留下了很多遗憾，实际与预测的图表表明在0处存在不连续/性能不佳的问题。然而，我们不会对这些问题深入探讨：
- en: '![Using the adabag and gbm packages](img/00281.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![使用adabag和gbm包](img/00281.jpeg)'
- en: 'Figure 8: Sine wave approximation by using the gbm function'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：使用gbm函数进行的正弦波近似
- en: Next, we will discuss the concept of variable importance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论变量重要性的概念。
- en: Variable importance
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量重要性
- en: Boosting methods essentially use trees as base learners, and hence the idea
    of variable importance gets carried over here the same as with trees, bagging,
    and random forests. We simply add the importance of the variables across the trees
    as we do with bagging or random forests.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法本质上使用树作为基学习器，因此变量重要性的概念在这里与树、袋装和随机森林相同。我们只需像在袋装或随机森林中那样，将变量在树之间的重要性相加。
- en: 'For a boosting fitted object from the `adabag` package, the variable importance
    is extracted as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自`adabag`包的提升拟合对象，变量重要性提取如下：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This means that the boosting method has not used the `x2` variable at all.
    For the gradient boosting objects, the importance is given by the `summary` function:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着提升方法根本未使用`x2`变量。对于梯度提升对象，变量重要性由`summary`函数给出：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It is now apparent that we only have one variable and so it is important to
    explain the regressand and we certainly did not require some software to tell
    us. Of course, it is useful in complex cases. Comparisons are for different ensembling
    methods based on trees. Let us move on to the next section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很明显，我们只有一个变量，因此解释回归因变量很重要，我们当然不需要某些软件来告诉我们。当然，在复杂情况下是有用的。比较是基于树的集成方法的。让我们继续下一节。
- en: Comparing bagging, random forests, and boosting
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较袋装、随机森林和提升
- en: 'We carried out comparisons between the bagging and random forest methods in
    the previous chapter. Using the `gbm` function, we now add boosting accuracy to
    the earlier analyses:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们进行了袋装和随机森林方法的比较。现在，我们使用`gbm`函数将提升准确率添加到早期分析中：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The boosting accuracy of `0.9457` is higher than the random forest accuracy
    of `0.9436`. Further fine tuning, to be explored in the next chapter, will help
    in improving the accuracy. The variable importance is also easily obtained using
    the `summary` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 提升准确率`0.9457`高于随机森林准确率`0.9436`。进一步的微调，将在下一章中探讨，将有助于提高准确率。变量重要性也可以通过`summary`函数轻松获得。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Boosting is yet another ramification of decision trees. It is a sequential iteration
    technique where the error from a previous iteration is targeted with more impunity.
    We began with the important adaptive boosting algorithm and used very simple toy
    data to illustrate the underpinnings. The approach was then extended to the regression
    problem and we illustrated the gradient boosting method with two different approaches.
    The two packages `adabag` and `gbm` were briefly elaborated on and the concept
    of variable importance was emphasized yet again. For the spam dataset, we got
    more accuracy with boosting and hence the deliberations of the boosting algorithm
    are especially more useful.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是决策树的另一种推论。它是一种迭代技术，通过更无顾忌地针对前一次迭代的误差。我们从重要的自适应提升算法开始，并使用非常简单的玩具数据来说明其基础。然后，该方法被扩展到回归问题，我们通过两种不同的方法说明了梯度提升方法。对`adabag`和`gbm`这两个包进行了简要阐述，并再次强调了变量重要性的概念。对于垃圾邮件数据集，我们通过提升方法获得了更高的准确率，因此提升算法的讨论特别有用。
- en: The chapter considered different variants of the boosting algorithm. However,
    we did not discuss why it works at all. In the next chapter, these aspects will
    be covered in more detail.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本章考虑了提升算法的不同变体。然而，我们没有讨论它为什么有效。在下一章中，这些方面将更详细地介绍。
