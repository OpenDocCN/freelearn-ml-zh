- en: Chapter 5. The Bare Bones Boosting Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What do we mean by bare bones boosting algorithms? The boosting algorithm (and
    its variants) is arguably one of the most important algorithms in the machine
    learning toolbox. Any data analyst needs to know this algorithm, and eventually
    the push for higher accuracy invariably drives towards the need for the boosting
    technique. It has been reported on the [www.kaggle.org](http://www.kaggle.org)
    forums that boosting algorithms for complex and voluminous data run for several
    weeks and that most award-winning solutions are based on this. Furthermore, the
    algorithms run on modern graphical device machines.
  prefs: []
  type: TYPE_NORMAL
- en: Taking its importance into account, we will study the boosting algorithm in
    detail here. *Bare bones* is certainly not a variant of the boosting algorithm.
    Since the boosting algorithm is one of the very important and vital algorithms,
    we will first state the algorithm and implement it in a rudimentary fashion, which
    will show each step of the algorithm in action.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with the adaptive boosting algorithm—popularly known and abbreviated
    as the **AdaBoost** algorithm—and using very simple and raw code, we will illustrate
    it for a classification problem. The illustration is carried over a `toy` dataset
    so that the steps can be clearly followed by the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will extend the classification boosting algorithm to
    the regression problem. For this problem, the boosting variant is the famous **gradient
    boosting algorithm**. An interesting nonlinear regression problem will be improvised
    through a series of basic decision trees with a single split, also known as **stumps**.
    The gradient boosting algorithm will be illustrated for the choice of the squared-error
    loss function. Variable importance computation will be clarified for the boosting
    method. The details of the `gbm` package will be discussed in the penultimate
    section of the chapter. The concluding section will carry out a comparison of
    the bagging, random forests, and boosting methods for a spam dataset. This chapter
    consists of the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The general boosting algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting based on stumps
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting with squared-error loss
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable importance in the boosting technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the gbm package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of bagging, random forests, and boosting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following libraries in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rpart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gbm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The general boosting algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tree-based ensembles in the previous chapters, *Bagging* and *Random Forests*,
    cover an important extension of the decision trees. However, while bagging provides
    greater stability by averaging multiple decision trees, the bias persists. This
    limitation motivated Breiman to sample the covariates at each split point to generate
    an ensemble of "independent" trees and lay the foundation for random forests.
    The trees in the random forests can be developed in parallel, as is the case with
    bagging. The idea of averaging over multiple trees is to ensure the balance between
    the bias and variance trade-off. Boosting is the third most important extension
    of the decision trees, and probably the most effective one. It is again based
    on ensembling homogeneous base learners (in this case, trees), as are the bagging
    and random forests. The design of the boosting algorithm is completely different
    though. It is a *sequential* ensemble method in that the residual/misclassified
    point of the previous learner is improvised in the next run of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The general boosting technique consists of a family of algorithms that convert
    weak learners to provide a final strong learner. In the context of classification
    problems, a weak learner is a technique that is better than a random guess. That
    the method converts a weak learning algorithm into a better one is the reason
    it gets the name *boosting*. The boosting technique is designed to deliver a strong
    learner that is closer to the perfect classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '| Classifier | Sub-space ![The general boosting algorithm](img/00199.jpeg)
    | Sub-space ![The general boosting algorithm](img/00200.jpeg) | Sub-space ![The
    general boosting algorithm](img/00201.jpeg) | Sub-space ![The general boosting
    algorithm](img/00202.jpeg) | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ![The general boosting algorithm](img/00203.jpeg) | R | R | R | Q | 0.75
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![The general boosting algorithm](img/00204.jpeg) | R | R | Q | R | 0.75
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![The general boosting algorithm](img/00205.jpeg) | R | Q | R | R | 0.75
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![The general boosting algorithm](img/00206.jpeg) | Q | R | R | R | 0.75
    |'
  prefs: []
  type: TYPE_TB
- en: Table 1 A simple classifier scenario
  prefs: []
  type: TYPE_NORMAL
- en: A broader motivation for boosting can be understood through a simple example.
    Suppose that the random sample of size *n* is drawn as IID from the sample space
    ![The general boosting algorithm](img/00207.jpeg). The distribution of the random
    sample is assumed to be D. Suppose we have *T=4* classifiers in ![The general
    boosting algorithm](img/00208.jpeg), with the classifiers being used for the truth
    function *f*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a hypothetical scenario where the sample space ![The general boosting
    algorithm](img/00209.jpeg) consists of four parts in ![The general boosting algorithm](img/00210.jpeg),
    and the four classifiers perform as indicated in previous table. The idea behind
    the development of the boosting method is to improvise the classifier in a sequential
    manner. That is, combining the classifier is approached one after another and
    not all at the same time. Now, the errors of ![The general boosting algorithm](img/00211.jpeg)
    will be corrected to a new distribution D'' with the errors of the classifier
    being given more weight for the region ![The general boosting algorithm](img/00212.jpeg).
    The classifier ![The general boosting algorithm](img/00213.jpeg) will use the
    distribution D'' and its error zone instances in the region ![The general boosting
    algorithm](img/00214.jpeg) will be given more weight leading to a distribution
    D''''. The boosting method will continue the process for the remaining classifiers
    and give an overall combiner/ensemble. A pseudo-boosting algorithm (see [Chapter
    2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee "Chapter 2. Bootstrapping")
    of Zhou (2012)) is summarized in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 0: The initial sample distribution is D, and set D1 = D'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Steps ![The general boosting algorithm](img/00215.jpeg):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The general boosting algorithm](img/00216.jpeg) Dt'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The general boosting algorithm](img/00217.jpeg)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '*Dt+1 = Improve Distribution (Dt,*![The general boosting algorithm](img/00218.jpeg)*)*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Final step: ![The general boosting algorithm](img/00219.jpeg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two steps of the algorithm in *Improve distribution* and *Combine outputs*
    clearly need implementable actions. In the next section, we will develop the adaptive
    boosting method with a clear numerical illustration.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Schapire and Freund invented the adaptive boosting method. **Adaboost** is a
    popular abbreviation of this technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generic adaptive boosting algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the observation weights uniformly:![Adaptive boosting](img/00220.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For *m*, classifier *hm*, from *1* to *m* number of passes over with the data,
    perform the following tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a classifier *hm* to the training data using the weights ![Adaptive boosting](img/00221.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the error for each classifier as follows:![Adaptive boosting](img/00222.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the *voting power* of the classifier *hm*:![Adaptive boosting](img/00223.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set ![Adaptive boosting](img/00224.jpeg)![Adaptive boosting](img/00225.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Output:![Adaptive boosting](img/00226.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simply put, the algorithm unfolds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we start with uniform weights ![Adaptive boosting](img/00227.jpeg)
    for all observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we calculate the weighted error ![Adaptive boosting](img/00228.jpeg)
    for each of the classifiers under consideration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A classifier (usually stumps, or decision trees with a single split) needs to
    be selected and the practice is to select the classifier with the maximum accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Improve distribution and Combine outputs* case of ties, any accuracy tied
    classifier is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, the misclassified observations are given more weights and the values
    that are correctly classified are down-weighted. An important point needs to be
    recorded here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the weights update step, the sum of weights correctly classified as observations
    will equal the sum of weights of the misclassified observations.
  prefs: []
  type: TYPE_NORMAL
- en: The steps from computing the error of the classifier to the weight updating
    step are repeated M number of times, and the voting power of each classifier is
    obtained. For any given observation, we then make the prediction by using the
    predictions across the M classifiers weighted by their respective voting power
    and using the sign function as specified in the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: As simplified as the algorithm may be, it is a useful exercise to undertake
    the working of the adaptive boosting method through a toy dataset. The data and
    computational approach is taken from the video of Jessica Noss, available at [https://www.youtube.com/watch?v=gmok1h8wG-Q](https://www.youtube.com/watch?v=gmok1h8wG-Q).
    The illustration of the adaptive boosting algorithm begins now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a toy data set with five triplet points: two explanatory variables
    and one binary output value. The variables and data can be summarized with ![Adaptive
    boosting](img/00229.jpeg), and here we have the data points as ![Adaptive boosting](img/00230.jpeg),
    ![Adaptive boosting](img/00231.jpeg), ![Adaptive boosting](img/00232.jpeg), ![Adaptive
    boosting](img/00233.jpeg), and ![Adaptive boosting](img/00234.jpeg). The data
    will be first entered in R and then visualized as a preliminary step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Adaptive boosting](img/00235.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A simple depiction of the toy dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Stumps are a particular case of a decision tree that has been mentioned in the
    discussion. Here, we will use the stumps as the base learners. A simple look at
    the preceding diagram helps us to easily find stumps that have an accuracy higher
    than a random guess.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can put a stump at ![Adaptive boosting](img/00236.jpeg) and
    mark all the observations on the left side as positives and those on the right
    as negatives. In the following program, the points in the green shaded region
    are positives as predicted by the stumps, and those in the red shaded region are
    negatives. Similarly, we can use additional stumps at ![Adaptive boosting](img/00237.jpeg)
    and ![Adaptive boosting](img/00238.jpeg). The predictions can be swapped for the
    same stumps too, thanks to `symmetry()`. Thus, earlier we put the green shaded
    region to the left of ![Adaptive boosting](img/00239.jpeg) and predicted the values
    as positives, and by reversing the order the area on the right side of the stump
    ![Adaptive boosting](img/00240.jpeg) will be marked as positives. A similar classification
    is made for the negatives. The task is repeated at stumps ![Adaptive boosting](img/00241.jpeg)
    and ![Adaptive boosting](img/00242.jpeg). Using the `par`, `plot`, `text`, and
    `rect` graphical functions, we present visual depictions of these base learners
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding R program is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adaptive boosting](img/00243.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Stump classifiers based on X1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a similar classification can be obtained for the variable ![Adaptive
    boosting](img/00244.jpeg) at the points 2, 4, and 6\. Though there is no need
    to give the complete R program for stumps based on ![Adaptive boosting](img/00245.jpeg),
    we simply produce the output in the following diagram. The program is available
    in the code bundle. The stumps based on ![Adaptive boosting](img/00246.jpeg) will
    be ignored in the rest of the discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adaptive boosting](img/00247.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Stump classifiers based on X2'
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the stump based on ![Adaptive boosting](img/00248.jpeg) leads
    to a few misclassifications, and we can see that the observations P1, P4, and
    P3 are correctly classified while P2 and P5 are misclassified. The predictions
    based on this stump can then be put as (1,-1,-1,1,-1). The stump based on ![Adaptive
    boosting](img/00249.jpeg) classifies points P1 and P4 correctly, while P2, P3,
    and P5 are misclassified, and the prediction in vector form here is (1,-1,1,1,-1).
    The six models considered here will be denoted in the R program by M1, M2, …,
    M6, and in terms of the algorithm specified earlier, we have ![Adaptive boosting](img/00250.jpeg).
    Similarly, we have predictions for the other four stumps and we enter them in
    R, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the predictions given by the six models `M1-M6`, we can compare them with
    the true labels in `y` and see which observations are misclassified in each of
    these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the values of `TRUE` mean that the column named points is misclassified
    in the row named model. The weights ![Adaptive boosting](img/00251.jpeg) are initialized
    and the weighted errors ![Adaptive boosting](img/00252.jpeg), are computed for
    each of the models in the following R block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the error corresponding to Model 3, or ![Adaptive boosting](img/00253.jpeg),
    is the minimum, we select it first and calculate the voting power ![Adaptive boosting](img/00254.jpeg)
    assignable to it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Consequently, the boosting algorithm steps state that ![Adaptive boosting](img/00255.jpeg)
    gives us the required predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The central observation, `P3`, remains misclassified and so we proceed to the
    next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to update the weights ![Adaptive boosting](img/00256.jpeg) and
    for the classification problem the rule in a simplified form is given using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adaptive boosting](img/00257.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, we need a function that will take the weight of a previous run,
    the error rate, and the misclassifications by the model as inputs and then return
    them as the updated weights that incorporated the preceding formula. We define
    such a function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will update the weights and calculate the error for each of the six
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, models `M1` and `M5` have equal error rates with the new weights, and
    we simply choose Model 1, calculate its voting power, and predict based on the
    updated model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the point `P3` is still misclassified, we proceed with the iterations
    and apply the cycle once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now the classification is perfect and after three iterations, we don't have
    any misclassifications or errors. The purpose of the programming in this section
    was to demonstrate the steps in the adaptive boosting algorithm in an elementary
    way. In the next section, we will look at the *gradient boosting* technique.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The adaptive boosting method can't be applied to the regression problem since
    it is constructed to address the classification problem. The gradient boosting
    method can be used for both the classification and regression problems with suitable
    loss functions. In fact, the use of gradient boosting methods goes beyond these
    two standard problems. The technique originated from some of Breiman's observations
    and developed into regression problems by Freidman (2000). We will take the rudimentary
    code explanation in the next section without even laying out the algorithm. After
    the setup is clear, we will formally state the boosting algorithm for the squared-error
    loss function in the following subsection and create a new function implementing
    the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is a depiction of the standard sine wave function. It
    is clearly a nonlinear relationship. Without explicitly using sine transformations,
    we will see the use of the boosting algorithm to learn this function. Of course,
    we need simple regression stumps and we begin with a simple function, `getNode`,
    that will give us the desired split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient boosting](img/00258.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Can boosting work for nonlinear sine data?'
  prefs: []
  type: TYPE_NORMAL
- en: Building it from scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we used simple classification stumps. In that example,
    a simple visual inspection sufficed to identify the stumps, and we quickly obtained
    12 classification stumps. For the regression problem, we will first define a `getNode`
    function, which is a slight modification of the function defined in Chapter 9
    of Tattar (2017). The required notation is first set up.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have n pairs of data points ![Building it from scratch](img/00259.jpeg)
    and we are trying to learn the relationship ![Building it from scratch](img/00260.jpeg),
    where the form of *f* is completely unknown to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the regression tree, the split criteria are rather straightforward. For
    data split by an x-value, we calculate the sum of mean difference squares of *ys*
    in each of the partitioned part and then add them up. The split criteria are chosen
    as that x-value. This maximizes the sum of the mean difference squares in the
    variable of interest. The R function, `getNode`, implements this thinking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The first step in the `getNode` function is finding the unique values of `x`
    and then sorting them in decrease(ing) order. For the unique values, we calculate
    the sum of squares through a for loop. The first step in the loop is to partition
    the data in right and left parts.
  prefs: []
  type: TYPE_NORMAL
- en: The sum of mean difference squares is calculated in each of the partitions for
    a specific unique value, and then summed up to get the overall residual sum of
    squares.
  prefs: []
  type: TYPE_NORMAL
- en: We then obtain the value of `x`, which leads to the least residual sum of squares.
    The prediction in the partitioned regions is the mean of the y-values in those
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: The `getNode` function closes by returning the split value of `x`, and the predictions
    for the right and left partitions. We are now ready to create regression stumps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sine wave data is first easily created and we allow the x-values to range
    in the interval ![Building it from scratch](img/00261.jpeg). The y-value is simply
    the sin function applied on the x vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding display will be *Figure 1*. We proceed to obtain
    the first split of data and then display the mean of the right and left partitions
    on the graph. The residuals will be from the sine wave and they are also put on
    the same display, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our first split point occurs at `x` value of, ![Building it from scratch](img/00262.jpeg)
    here, `3.141593`. The prediction for the right side of the split point is `-0.6353102`
    and for the left side it is `0.6050574`. The predictions are plotted on the same
    display using the segments function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the predictions are easy to obtain here and a simple `ifelse` function
    helps in calculating them. The deviation from the sine wave is the residuals,
    and we calculate that the first set of residuals and `summary` function give the
    brief of the residual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step in the prediction is saved in the `GBFit` object and the difference
    between the fit and predictions is found in the `first_residuals` vector. This
    completes the first iteration of the gradient boosting algorithm. The residuals
    of the first iteration will become the regressand/output variable for the second
    iteration. Using the `getNode` function, we carry out the second iteration, which
    mimics the earlier set of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'An important difference here is that we update the prediction not by averaging
    but by adding. Note that we are modeling the residual of the first step and hence
    the remaining part of the residual explained by the next fitting needs to be added
    and not averaged. What is the range of residuals? The reader is advised to compare
    the residual values with earlier iterations. A similar extension is carried out
    for the third iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the visual display is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building it from scratch](img/00263.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Three iterations of the gradient boosting algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, we can''t keep on carrying out the iterations in a detailed execution
    every time and looping is important. The code is kept in a block and 22 more iterations
    are performed. The output at the end of each iteration is depicted in the diagram
    and we put them all in an external file, `Sine_Wave_25_Iterations.pdf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the 25 iterations, we have an overall fit in `GBFit` and we can plot
    this against the actual y values to see how well the gradient boosting algorithm
    has performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Building it from scratch](img/00264.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Gradient fit versus actual sine data'
  prefs: []
  type: TYPE_NORMAL
- en: The fit is reasonably good for a nonlinear model. The approach was to get a
    clear understanding of the gradient boosting algorithm. A more general form of
    the boosting algorithm is discussed and developed in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Squared-error loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Denote the data by ![Squared-error loss function](img/00265.jpeg), and fix
    the number of iterations/trees as a number *B*. Choose a shrinkage factor ![Squared-error
    loss function](img/00266.jpeg) and tree depth *d*. The gradient boosting algorithm
    based on the squared-error loss function is stated here briefly. See Algorithm
    17.2 of Efron and Hastie (2016), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize residuals ![Squared-error loss function](img/00267.jpeg) and the
    gradient boosting prediction as ![Squared-error loss function](img/00268.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For ![Squared-error loss function](img/00269.jpeg):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a regression tree of depth *d* for the data ![Squared-error loss function](img/00270.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain the predicted values as ![Squared-error loss function](img/00271.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the boosting prediction by ![Squared-error loss function](img/00272.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the residuals ![Squared-error loss function](img/00273.jpeg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the sequence of functions ![Squared-error loss function](img/00274.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will define a function `GB_SqEL`, which will implement the gradient
    boosting algorithm driven by the squared-error loss function. The function must
    be provided with five arguments: `y` and `x` will constitute the data, `depth`
    will specify the depth of the trees (that is, the number of splits in the regression
    tree), `iter` for the number of iterations, and `shrinkage` is the ![Squared-error
    loss function](img/00275.jpeg) factor. The `GB_SqEL` function is set up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The initialization takes place in specifications, and the line `fit <- y*0`.
    The depth argument of the algorithm is taken in the line `maxdepth=depth`, and
    using the `rpart` function, we create a tree of the necessary depth. The `predict`
    function gives the values of ![Squared-error loss function](img/00276.jpeg) as
    required at each iteration, while `fit+gb_hat` does the necessary update. Note
    that `GB_Hat[,i]` consists of the predicted values at the end of each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will illustrate the algorithm with the example of Efron and Hastie (2016).
    The data considered is related with Lu Gerig''s disease, or **amyotrophic lateral
    sclerosis** (**ALS**). The dataset has information on 1,822 individuals with the
    ALS disease. The goal is to predict the rate of progression `dFRS` of a functional
    rating score. The study has information on 369 predictors/covariates. Here, we
    will use the `GB_SqEL` function to fit the gradient boosting technique and analyze
    the mean square error as the number of iterations increases. The details and the
    data can be obtained from the source at [https://web.stanford.edu/~hastie/CASI/data.html](https://web.stanford.edu/~hastie/CASI/data.html).
    We will now put the squared-error loss function-driven boosting method into action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Using the `read.table` function, we import the data from the code bundle into
    the `als` object. The data is available from the source in the `.txt` format.
    The column `testset` indicates whether the observations were marked for training
    purposes or for tests. We select the training observations and also drop the first
    variable `testset` and store it in the object `alst`. The `GB_SqEL` function is
    applied on the `alst` object with appropriate specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following each iteration, we compute the mean-squared error and store it in
    `GB_Hat`, as explained earlier. We can see from the following diagram that as
    the iterations increase, the mean squared error decreases. Here, the algorithm
    stabilizes after nearly 200 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Squared-error loss function](img/00277.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Gradient boosting and the MSE by iterations'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see the use of two powerful R packages.
  prefs: []
  type: TYPE_NORMAL
- en: Using the adabag and gbm packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the boosting method as an ensemble technique is indeed very effective.
    The algorithm was illustrated for classification and regression problems from
    scratch. Once the understand the algorithm clear and transparent, we can then
    use R packages to deliver results going forward. A host of packages are available
    for implementing the boosting technique. However, we will use the two most popular
    packages `adabag` and `gbm` in this section. First, a look at the options of the
    two functions is in order. The names are obvious and `adabag` implements the adaptive
    boosting methods while `gbm` deals with gradient boosting methods. First, we look
    at the options available in these two functions in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the adabag and gbm packages](img/00278.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The boosting and gbm functions
  prefs: []
  type: TYPE_NORMAL
- en: The formula is the usual argument. The argument `mfinal` in `adabag` and `n.trees`
    in `gbm` allows the specification of the number of trees or iterations. The boosting
    function gives the option of `boos`, which is the bootstrap sample of the training
    set drawn using the weights for each observation on that iteration. Gradient boosting
    is a more generic algorithm that is capable of handling more than the regression
    structure. It can be used for classification problems as well. The option of `distribution`
    in the `gbm` function gives those options. Similarly, one can see here that the
    `gbm` function offers a host of other options. We will neither undertake the daunting
    task of explaining them all nor apply them to complex datasets. The two datasets
    that were used to explain and elaborate adaptive and gradient boosting algorithms
    will be continued with the `boosting` and `gbm` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toy datasets need to be changed and we will replicate them multiple times
    over so that we have enough observations for running the `boosting` and `gbm`
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `maxdepth=1` function ensures that we are using only stumps as the base
    classifiers. It is easily seen that the boosting function works perfectly, as
    all the observations are correctly classified.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the `boosting` function, we need to have more data points. We increase
    this with the `seq` function and using the `distribution="gaussian"` option, we
    ask the `gbm` function to fit the regression boosting technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the plot functions, we make a comparison of the fit with the gradient
    boosting method. The following diagram suggests that the fit has been appropriate.
    However, the plots also show that something is not quite correct with the story
    either. The function approximation at ![Using the adabag and gbm packages](img/00279.jpeg)
    and ![Using the adabag and gbm packages](img/00280.jpeg) by the boosting method
    leaves a lot to be desired, and the actual versus predicted plot suggests discontinuity/poor
    performance at 0\. However, we will not delve too far into these issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the adabag and gbm packages](img/00281.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Sine wave approximation by using the gbm function'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the concept of variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: Variable importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting methods essentially use trees as base learners, and hence the idea
    of variable importance gets carried over here the same as with trees, bagging,
    and random forests. We simply add the importance of the variables across the trees
    as we do with bagging or random forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a boosting fitted object from the `adabag` package, the variable importance
    is extracted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the boosting method has not used the `x2` variable at all.
    For the gradient boosting objects, the importance is given by the `summary` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It is now apparent that we only have one variable and so it is important to
    explain the regressand and we certainly did not require some software to tell
    us. Of course, it is useful in complex cases. Comparisons are for different ensembling
    methods based on trees. Let us move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing bagging, random forests, and boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We carried out comparisons between the bagging and random forest methods in
    the previous chapter. Using the `gbm` function, we now add boosting accuracy to
    the earlier analyses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The boosting accuracy of `0.9457` is higher than the random forest accuracy
    of `0.9436`. Further fine tuning, to be explored in the next chapter, will help
    in improving the accuracy. The variable importance is also easily obtained using
    the `summary` function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is yet another ramification of decision trees. It is a sequential iteration
    technique where the error from a previous iteration is targeted with more impunity.
    We began with the important adaptive boosting algorithm and used very simple toy
    data to illustrate the underpinnings. The approach was then extended to the regression
    problem and we illustrated the gradient boosting method with two different approaches.
    The two packages `adabag` and `gbm` were briefly elaborated on and the concept
    of variable importance was emphasized yet again. For the spam dataset, we got
    more accuracy with boosting and hence the deliberations of the boosting algorithm
    are especially more useful.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter considered different variants of the boosting algorithm. However,
    we did not discuss why it works at all. In the next chapter, these aspects will
    be covered in more detail.
  prefs: []
  type: TYPE_NORMAL
