- en: Chapter 5. The Bare Bones Boosting Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 简洁的增强算法
- en: What do we mean by bare bones boosting algorithms? The boosting algorithm (and
    its variants) is arguably one of the most important algorithms in the machine
    learning toolbox. Any data analyst needs to know this algorithm, and eventually
    the push for higher accuracy invariably drives towards the need for the boosting
    technique. It has been reported on the [www.kaggle.org](http://www.kaggle.org)
    forums that boosting algorithms for complex and voluminous data run for several
    weeks and that most award-winning solutions are based on this. Furthermore, the
    algorithms run on modern graphical device machines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的简洁的增强算法是什么意思？增强算法（及其变体）可以说是机器学习工具箱中最重要的算法之一。任何数据分析师都需要了解这个算法，并且最终追求更高的准确率不可避免地会推动对增强技术的需求。据报道，在[www.kaggle.org](http://www.kaggle.org)论坛上，复杂和大量数据的增强算法运行了数周，而且大多数获奖方案都是基于这个算法。此外，这些算法在现代图形设备机器上运行。
- en: Taking its importance into account, we will study the boosting algorithm in
    detail here. *Bare bones* is certainly not a variant of the boosting algorithm.
    Since the boosting algorithm is one of the very important and vital algorithms,
    we will first state the algorithm and implement it in a rudimentary fashion, which
    will show each step of the algorithm in action.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到其重要性，我们将在下面详细研究增强算法。*简洁的*当然不是增强算法的变体。由于增强算法是非常重要且关键的算法之一，我们将首先陈述该算法并以最基本的方式实现它，这将展示算法的每个步骤是如何运作的。
- en: We will begin with the adaptive boosting algorithm—popularly known and abbreviated
    as the **AdaBoost** algorithm—and using very simple and raw code, we will illustrate
    it for a classification problem. The illustration is carried over a `toy` dataset
    so that the steps can be clearly followed by the reader.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从自适应增强算法开始——通常被称为**AdaBoost**算法——并使用非常简单和原始的代码，我们将展示其在分类问题中的应用。该说明是在一个`toy`数据集上进行的，以便读者可以清楚地跟随步骤。
- en: 'In the next section, we will extend the classification boosting algorithm to
    the regression problem. For this problem, the boosting variant is the famous **gradient
    boosting algorithm**. An interesting nonlinear regression problem will be improvised
    through a series of basic decision trees with a single split, also known as **stumps**.
    The gradient boosting algorithm will be illustrated for the choice of the squared-error
    loss function. Variable importance computation will be clarified for the boosting
    method. The details of the `gbm` package will be discussed in the penultimate
    section of the chapter. The concluding section will carry out a comparison of
    the bagging, random forests, and boosting methods for a spam dataset. This chapter
    consists of the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将扩展分类增强算法到回归问题。对于这个问题，增强变体是著名的**梯度增强算法**。通过一系列具有单个分割的基本决策树，即所谓的**树桩**，将创建一个有趣的非线性回归问题。我们将通过平方误差损失函数的选择来展示梯度增强算法。对于增强方法，我们将阐明变量重要性计算。本章的倒数第二节将讨论`gbm`包的细节。结论部分将对垃圾邮件数据集的袋装、随机森林和增强方法进行比较。本章包括以下主题：
- en: The general boosting algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用增强算法
- en: Adaptive boosting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应提升
- en: Gradient boosting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting based on stumps
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于树桩的梯度提升
- en: Gradient boosting with squared-error loss
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于平方误差损失的梯度提升
- en: Variable importance in the boosting technique
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强技术中的变量重要性
- en: Using the gbm package
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用gbm包
- en: Comparison of bagging, random forests, and boosting algorithms
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袋装、随机森林和增强算法的比较
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following libraries in the chapter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下库：
- en: '`rpart`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: '`gbm`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbm`'
- en: The general boosting algorithm
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用增强算法
- en: The tree-based ensembles in the previous chapters, *Bagging* and *Random Forests*,
    cover an important extension of the decision trees. However, while bagging provides
    greater stability by averaging multiple decision trees, the bias persists. This
    limitation motivated Breiman to sample the covariates at each split point to generate
    an ensemble of "independent" trees and lay the foundation for random forests.
    The trees in the random forests can be developed in parallel, as is the case with
    bagging. The idea of averaging over multiple trees is to ensure the balance between
    the bias and variance trade-off. Boosting is the third most important extension
    of the decision trees, and probably the most effective one. It is again based
    on ensembling homogeneous base learners (in this case, trees), as are the bagging
    and random forests. The design of the boosting algorithm is completely different
    though. It is a *sequential* ensemble method in that the residual/misclassified
    point of the previous learner is improvised in the next run of the algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中介绍的基于树的集成方法，*Bagging* 和 *随机森林*，是决策树的重要扩展。然而，虽然Bagging通过平均多个决策树提供了更大的稳定性，但偏差仍然存在。这种局限性促使Breiman在每个分割点采样协变量以生成一组“独立”的树，并为随机森林奠定了基础。随机森林中的树可以像Bagging一样并行开发。在多个树上进行平均的想法是为了确保偏差和方差权衡之间的平衡。提升是决策树的第三大扩展，也可能是最有效的一种。它同样基于集成同质基学习器（在这种情况下，是树），就像Bagging和随机森林一样。提升算法的设计完全不同。它是一种*顺序*集成方法，因为在算法的下一个运行中，前一个学习者的残差/误分类点被改进。
- en: The general boosting technique consists of a family of algorithms that convert
    weak learners to provide a final strong learner. In the context of classification
    problems, a weak learner is a technique that is better than a random guess. That
    the method converts a weak learning algorithm into a better one is the reason
    it gets the name *boosting*. The boosting technique is designed to deliver a strong
    learner that is closer to the perfect classifier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通用提升技术包括一系列算法，这些算法将弱学习器转换为最终强学习器。在分类问题的背景下，弱学习器是一种比随机猜测更好的技术。该方法将弱学习算法转换为更好的算法，这就是它被称为
    *提升* 的原因。提升技术旨在提供一种接近完美分类器的强学习器。
- en: '| Classifier | Sub-space ![The general boosting algorithm](img/00199.jpeg)
    | Sub-space ![The general boosting algorithm](img/00200.jpeg) | Sub-space ![The
    general boosting algorithm](img/00201.jpeg) | Sub-space ![The general boosting
    algorithm](img/00202.jpeg) | Accuracy |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 子空间 ![通用提升算法](img/00199.jpeg) | 子空间 ![通用提升算法](img/00200.jpeg) | 子空间
    ![通用提升算法](img/00201.jpeg) | 子空间 ![通用提升算法](img/00202.jpeg) | 准确率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ![The general boosting algorithm](img/00203.jpeg) | R | R | R | Q | 0.75
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00203.jpeg) | R | R | R | Q | 0.75 |'
- en: '| ![The general boosting algorithm](img/00204.jpeg) | R | R | Q | R | 0.75
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00204.jpeg) | R | R | Q | R | 0.75 |'
- en: '| ![The general boosting algorithm](img/00205.jpeg) | R | Q | R | R | 0.75
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00205.jpeg) | R | Q | R | R | 0.75 |'
- en: '| ![The general boosting algorithm](img/00206.jpeg) | Q | R | R | R | 0.75
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ![通用提升算法](img/00206.jpeg) | Q | R | R | R | 0.75 |'
- en: Table 1 A simple classifier scenario
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 简单分类器场景
- en: A broader motivation for boosting can be understood through a simple example.
    Suppose that the random sample of size *n* is drawn as IID from the sample space
    ![The general boosting algorithm](img/00207.jpeg). The distribution of the random
    sample is assumed to be D. Suppose we have *T=4* classifiers in ![The general
    boosting algorithm](img/00208.jpeg), with the classifiers being used for the truth
    function *f*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个简单的例子可以更广泛地理解提升的动机。假设从样本空间 ![通用提升算法](img/00207.jpeg) 中抽取大小为 *n* 的随机样本，作为独立同分布（IID）的样本。随机样本的分布假设为D。假设在
    ![通用提升算法](img/00208.jpeg) 中有 *T=4* 个分类器，分类器用于真值函数 *f*。
- en: 'Consider a hypothetical scenario where the sample space ![The general boosting
    algorithm](img/00209.jpeg) consists of four parts in ![The general boosting algorithm](img/00210.jpeg),
    and the four classifiers perform as indicated in previous table. The idea behind
    the development of the boosting method is to improvise the classifier in a sequential
    manner. That is, combining the classifier is approached one after another and
    not all at the same time. Now, the errors of ![The general boosting algorithm](img/00211.jpeg)
    will be corrected to a new distribution D'' with the errors of the classifier
    being given more weight for the region ![The general boosting algorithm](img/00212.jpeg).
    The classifier ![The general boosting algorithm](img/00213.jpeg) will use the
    distribution D'' and its error zone instances in the region ![The general boosting
    algorithm](img/00214.jpeg) will be given more weight leading to a distribution
    D''''. The boosting method will continue the process for the remaining classifiers
    and give an overall combiner/ensemble. A pseudo-boosting algorithm (see [Chapter
    2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee "Chapter 2. Bootstrapping")
    of Zhou (2012)) is summarized in the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 0: The initial sample distribution is D, and set D1 = D'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Steps ![The general boosting algorithm](img/00215.jpeg):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The general boosting algorithm](img/00216.jpeg) Dt'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The general boosting algorithm](img/00217.jpeg)'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '*Dt+1 = Improve Distribution (Dt,*![The general boosting algorithm](img/00218.jpeg)*)*'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Final step: ![The general boosting algorithm](img/00219.jpeg)'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two steps of the algorithm in *Improve distribution* and *Combine outputs*
    clearly need implementable actions. In the next section, we will develop the adaptive
    boosting method with a clear numerical illustration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive boosting
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Schapire and Freund invented the adaptive boosting method. **Adaboost** is a
    popular abbreviation of this technique.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The generic adaptive boosting algorithm is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the observation weights uniformly:![Adaptive boosting](img/00220.jpeg)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For *m*, classifier *hm*, from *1* to *m* number of passes over with the data,
    perform the following tasks:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a classifier *hm* to the training data using the weights ![Adaptive boosting](img/00221.jpeg)
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the error for each classifier as follows:![Adaptive boosting](img/00222.jpeg)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the *voting power* of the classifier *hm*:![Adaptive boosting](img/00223.jpeg)
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set ![Adaptive boosting](img/00224.jpeg)![Adaptive boosting](img/00225.jpeg)
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Output:![Adaptive boosting](img/00226.jpeg)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simply put, the algorithm unfolds as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we start with uniform weights ![Adaptive boosting](img/00227.jpeg)
    for all observations.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we calculate the weighted error ![Adaptive boosting](img/00228.jpeg)
    for each of the classifiers under consideration.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A classifier (usually stumps, or decision trees with a single split) needs to
    be selected and the practice is to select the classifier with the maximum accuracy.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器（通常是树桩，或单分支的决策树）需要被选择，通常的做法是选择具有最大准确率的分类器。
- en: In *Improve distribution and Combine outputs* case of ties, any accuracy tied
    classifier is selected.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*改进分布和组合输出*的情况下，如果有准确率相同的分类器，则选择任意一个。
- en: 'Next, the misclassified observations are given more weights and the values
    that are correctly classified are down-weighted. An important point needs to be
    recorded here:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，错误分类的观察值被赋予更多的权重，而正确分类的值被降低权重。这里需要记录一个重要点：
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In the weights update step, the sum of weights correctly classified as observations
    will equal the sum of weights of the misclassified observations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重更新步骤中，正确分类为观察值的权重之和将等于错误分类的观察值的权重之和。
- en: The steps from computing the error of the classifier to the weight updating
    step are repeated M number of times, and the voting power of each classifier is
    obtained. For any given observation, we then make the prediction by using the
    predictions across the M classifiers weighted by their respective voting power
    and using the sign function as specified in the algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算分类器的误差到权重更新步骤的步骤会重复M次，从而获得每个分类器的投票权重。对于任何给定的观察值，我们然后使用M个分类器的预测，这些预测根据各自的投票权重加权，并使用算法中指定的符号函数进行预测。
- en: As simplified as the algorithm may be, it is a useful exercise to undertake
    the working of the adaptive boosting method through a toy dataset. The data and
    computational approach is taken from the video of Jessica Noss, available at [https://www.youtube.com/watch?v=gmok1h8wG-Q](https://www.youtube.com/watch?v=gmok1h8wG-Q).
    The illustration of the adaptive boosting algorithm begins now.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管算法可能很简单，但通过玩具数据集来执行自适应提升方法的工作是一个有用的练习。数据和计算方法取自Jessica Noss的视频，可在[https://www.youtube.com/watch?v=gmok1h8wG-Q](https://www.youtube.com/watch?v=gmok1h8wG-Q)找到。自适应提升算法的说明现在开始。
- en: 'Consider a toy data set with five triplet points: two explanatory variables
    and one binary output value. The variables and data can be summarized with ![Adaptive
    boosting](img/00229.jpeg), and here we have the data points as ![Adaptive boosting](img/00230.jpeg),
    ![Adaptive boosting](img/00231.jpeg), ![Adaptive boosting](img/00232.jpeg), ![Adaptive
    boosting](img/00233.jpeg), and ![Adaptive boosting](img/00234.jpeg). The data
    will be first entered in R and then visualized as a preliminary step:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含五个三元点的玩具数据集：两个解释变量和一个二元输出值。变量和数据可以用![自适应提升](img/00229.jpeg)来总结，这里的数据点有![自适应提升](img/00230.jpeg)、![自适应提升](img/00231.jpeg)、![自适应提升](img/00232.jpeg)、![自适应提升](img/00233.jpeg)和![自适应提升](img/00234.jpeg)。数据首先输入到R中，然后作为初步步骤进行可视化：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Adaptive boosting](img/00235.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![自适应提升](img/00235.jpeg)'
- en: 'Figure 1: A simple depiction of the toy dataset'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：玩具数据集的简单表示
- en: Stumps are a particular case of a decision tree that has been mentioned in the
    discussion. Here, we will use the stumps as the base learners. A simple look at
    the preceding diagram helps us to easily find stumps that have an accuracy higher
    than a random guess.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 树桩是决策树的一种特殊情况，已在讨论中提到。在这里，我们将使用树桩作为基学习器。简单看一下前面的图可以帮助我们轻松找到准确率高于随机猜测的树桩。
- en: 'For example, we can put a stump at ![Adaptive boosting](img/00236.jpeg) and
    mark all the observations on the left side as positives and those on the right
    as negatives. In the following program, the points in the green shaded region
    are positives as predicted by the stumps, and those in the red shaded region are
    negatives. Similarly, we can use additional stumps at ![Adaptive boosting](img/00237.jpeg)
    and ![Adaptive boosting](img/00238.jpeg). The predictions can be swapped for the
    same stumps too, thanks to `symmetry()`. Thus, earlier we put the green shaded
    region to the left of ![Adaptive boosting](img/00239.jpeg) and predicted the values
    as positives, and by reversing the order the area on the right side of the stump
    ![Adaptive boosting](img/00240.jpeg) will be marked as positives. A similar classification
    is made for the negatives. The task is repeated at stumps ![Adaptive boosting](img/00241.jpeg)
    and ![Adaptive boosting](img/00242.jpeg). Using the `par`, `plot`, `text`, and
    `rect` graphical functions, we present visual depictions of these base learners
    in the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The result of the preceding R program is shown in the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Adaptive boosting](img/00243.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Stump classifiers based on X1'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a similar classification can be obtained for the variable ![Adaptive
    boosting](img/00244.jpeg) at the points 2, 4, and 6\. Though there is no need
    to give the complete R program for stumps based on ![Adaptive boosting](img/00245.jpeg),
    we simply produce the output in the following diagram. The program is available
    in the code bundle. The stumps based on ![Adaptive boosting](img/00246.jpeg) will
    be ignored in the rest of the discussion:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Adaptive boosting](img/00247.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Stump classifiers based on X2'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the stump based on ![Adaptive boosting](img/00248.jpeg) leads
    to a few misclassifications, and we can see that the observations P1, P4, and
    P3 are correctly classified while P2 and P5 are misclassified. The predictions
    based on this stump can then be put as (1,-1,-1,1,-1). The stump based on ![Adaptive
    boosting](img/00249.jpeg) classifies points P1 and P4 correctly, while P2, P3,
    and P5 are misclassified, and the prediction in vector form here is (1,-1,1,1,-1).
    The six models considered here will be denoted in the R program by M1, M2, …,
    M6, and in terms of the algorithm specified earlier, we have ![Adaptive boosting](img/00250.jpeg).
    Similarly, we have predictions for the other four stumps and we enter them in
    R, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the predictions given by the six models `M1-M6`, we can compare them with
    the true labels in `y` and see which observations are misclassified in each of
    these models:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Thus, the values of `TRUE` mean that the column named points is misclassified
    in the row named model. The weights ![Adaptive boosting](img/00251.jpeg) are initialized
    and the weighted errors ![Adaptive boosting](img/00252.jpeg), are computed for
    each of the models in the following R block:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since the error corresponding to Model 3, or ![Adaptive boosting](img/00253.jpeg),
    is the minimum, we select it first and calculate the voting power ![Adaptive boosting](img/00254.jpeg)
    assignable to it as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对应模型3，或 ![自适应提升](img/00253.jpeg) 的误差是最小的，我们首先选择它，并按照以下方式计算分配给它的投票权重 ![自适应提升](img/00254.jpeg)：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Consequently, the boosting algorithm steps state that ![Adaptive boosting](img/00255.jpeg)
    gives us the required predictions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提升算法步骤表明 ![自适应提升](img/00255.jpeg) 给出了所需的预测：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The central observation, `P3`, remains misclassified and so we proceed to the
    next step.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 中心观察点 `P3` 仍然被错误分类，所以我们继续到下一步。
- en: 'Now we need to update the weights ![Adaptive boosting](img/00256.jpeg) and
    for the classification problem the rule in a simplified form is given using:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要更新权重 ![自适应提升](img/00256.jpeg)，对于分类问题，简化形式的规则如下：
- en: '![Adaptive boosting](img/00257.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![自适应提升](img/00257.jpeg)'
- en: 'Consequently, we need a function that will take the weight of a previous run,
    the error rate, and the misclassifications by the model as inputs and then return
    them as the updated weights that incorporated the preceding formula. We define
    such a function as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要一个函数，它将接受前一次运行的权重、错误率和模型错误分类作为输入，然后返回包含先前公式的更新权重。我们定义这样的函数如下：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will update the weights and calculate the error for each of the six
    models:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将更新权重并计算六个模型中的每个模型的误差：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, models `M1` and `M5` have equal error rates with the new weights, and
    we simply choose Model 1, calculate its voting power, and predict based on the
    updated model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型 `M1` 和 `M5` 使用新的权重具有相等的错误率，我们简单地选择模型1，计算其投票权重，并基于更新后的模型进行预测：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since the point `P3` is still misclassified, we proceed with the iterations
    and apply the cycle once more:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点 `P3` 仍然被错误分类，我们继续迭代并再次应用循环：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now the classification is perfect and after three iterations, we don't have
    any misclassifications or errors. The purpose of the programming in this section
    was to demonstrate the steps in the adaptive boosting algorithm in an elementary
    way. In the next section, we will look at the *gradient boosting* technique.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在分类是完美的，经过三次迭代后，我们没有任何错误分类或错误。本节编程的目的是以基本的方式展示自适应提升算法的步骤。在下一节中，我们将探讨 *梯度提升*
    技术。
- en: Gradient boosting
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: The adaptive boosting method can't be applied to the regression problem since
    it is constructed to address the classification problem. The gradient boosting
    method can be used for both the classification and regression problems with suitable
    loss functions. In fact, the use of gradient boosting methods goes beyond these
    two standard problems. The technique originated from some of Breiman's observations
    and developed into regression problems by Freidman (2000). We will take the rudimentary
    code explanation in the next section without even laying out the algorithm. After
    the setup is clear, we will formally state the boosting algorithm for the squared-error
    loss function in the following subsection and create a new function implementing
    the algorithm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应提升方法不能应用于回归问题，因为它旨在解决分类问题。梯度提升方法可以使用适当的损失函数来解决分类和回归问题。实际上，梯度提升方法的应用不仅限于这两个标准问题。这项技术起源于Breiman的一些观察，并由Friedman（2000）发展成回归问题。在下一节中，我们将对基本代码进行解释，而无需展示算法。在设置清楚之后，我们将在下一个小节中正式陈述针对平方误差损失函数的提升算法，并创建一个新的函数来实现该算法。
- en: 'The following diagram is a depiction of the standard sine wave function. It
    is clearly a nonlinear relationship. Without explicitly using sine transformations,
    we will see the use of the boosting algorithm to learn this function. Of course,
    we need simple regression stumps and we begin with a simple function, `getNode`,
    that will give us the desired split:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示是标准正弦波函数的描述。很明显，这是一个非线性关系。在不显式使用正弦变换的情况下，我们将看到使用提升算法来学习这个函数。当然，我们需要简单的回归树桩，我们从一个简单的函数
    `getNode` 开始，它将给出我们想要的分割：
- en: '![Gradient boosting](img/00258.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升](img/00258.jpeg)'
- en: 'Figure 4: Can boosting work for nonlinear sine data?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：提升算法能否用于非线性正弦数据？
- en: Building it from scratch
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始构建
- en: In the previous section, we used simple classification stumps. In that example,
    a simple visual inspection sufficed to identify the stumps, and we quickly obtained
    12 classification stumps. For the regression problem, we will first define a `getNode`
    function, which is a slight modification of the function defined in Chapter 9
    of Tattar (2017). The required notation is first set up.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了简单的分类树桩。在那个例子中，简单的视觉检查就足以识别树桩，我们很快获得了12个分类树桩。对于回归问题，我们首先定义一个`getNode`函数，这是对Tattar（2017）第9章中定义的函数的轻微修改。首先设置所需的符号。
- en: Suppose we have n pairs of data points ![Building it from scratch](img/00259.jpeg)
    and we are trying to learn the relationship ![Building it from scratch](img/00260.jpeg),
    where the form of *f* is completely unknown to us.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一对n个数据点![从头开始构建](img/00259.jpeg)，我们正在尝试学习![从头开始构建](img/00260.jpeg)之间的关系，其中*f*的形式对我们来说完全未知。
- en: 'For the regression tree, the split criteria are rather straightforward. For
    data split by an x-value, we calculate the sum of mean difference squares of *ys*
    in each of the partitioned part and then add them up. The split criteria are chosen
    as that x-value. This maximizes the sum of the mean difference squares in the
    variable of interest. The R function, `getNode`, implements this thinking:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归树，分割标准相当直接。对于按x值分割的数据，我们计算每个分割部分的*ys*的平均差平方和，然后将它们加起来。分割标准被选为那个x值。这最大化了感兴趣变量中的平均差平方和。R函数`getNode`实现了这种思考：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first step in the `getNode` function is finding the unique values of `x`
    and then sorting them in decrease(ing) order. For the unique values, we calculate
    the sum of squares through a for loop. The first step in the loop is to partition
    the data in right and left parts.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`getNode`函数的第一步是找到`x`的唯一值，然后按降序排序。对于唯一值，我们通过for循环计算平方和。循环的第一步是将数据分割成左右两部分。'
- en: The sum of mean difference squares is calculated in each of the partitions for
    a specific unique value, and then summed up to get the overall residual sum of
    squares.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特定唯一值，在每个分区中计算平均差平方和，然后将它们加起来以得到总的残差平方和。
- en: We then obtain the value of `x`, which leads to the least residual sum of squares.
    The prediction in the partitioned regions is the mean of the y-values in those
    regions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们获取`x`的值，它导致最小的残差平方和。在分割区域中的预测是该区域y值的平均值。
- en: The `getNode` function closes by returning the split value of `x`, and the predictions
    for the right and left partitions. We are now ready to create regression stumps.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`getNode`函数通过返回`x`的分割值和左右分区的预测值来结束。我们现在可以创建回归树桩。'
- en: 'The sine wave data is first easily created and we allow the x-values to range
    in the interval ![Building it from scratch](img/00261.jpeg). The y-value is simply
    the sin function applied on the x vector:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦波数据首先很容易创建，我们允许x值在![从头开始构建](img/00261.jpeg)区间内变化。y值是简单地将正弦函数应用于x向量：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result of the preceding display will be *Figure 1*. We proceed to obtain
    the first split of data and then display the mean of the right and left partitions
    on the graph. The residuals will be from the sine wave and they are also put on
    the same display, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的显示结果将是**图1**。我们继续获取数据的第一次分割，并在图上显示左右分区的平均值。残差将来自正弦波，它们也将放在同一显示中，如下所示：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, our first split point occurs at `x` value of, ![Building it from scratch](img/00262.jpeg)
    here, `3.141593`. The prediction for the right side of the split point is `-0.6353102`
    and for the left side it is `0.6050574`. The predictions are plotted on the same
    display using the segments function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的第一个分割点发生在`x`值为，![从头开始构建](img/00262.jpeg)这里，`3.141593`。分割点右侧的预测值为`-0.6353102`，左侧的预测值为`0.6050574`。预测值使用`segments`函数在同一显示上绘制：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, the predictions are easy to obtain here and a simple `ifelse` function
    helps in calculating them. The deviation from the sine wave is the residuals,
    and we calculate that the first set of residuals and `summary` function give the
    brief of the residual values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，预测很容易获得，简单的`ifelse`函数有助于计算它们。与正弦波之间的偏差是残差，我们计算第一组残差和`summary`函数给出了残差值的简要概述：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first step in the prediction is saved in the `GBFit` object and the difference
    between the fit and predictions is found in the `first_residuals` vector. This
    completes the first iteration of the gradient boosting algorithm. The residuals
    of the first iteration will become the regressand/output variable for the second
    iteration. Using the `getNode` function, we carry out the second iteration, which
    mimics the earlier set of code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'An important difference here is that we update the prediction not by averaging
    but by adding. Note that we are modeling the residual of the first step and hence
    the remaining part of the residual explained by the next fitting needs to be added
    and not averaged. What is the range of residuals? The reader is advised to compare
    the residual values with earlier iterations. A similar extension is carried out
    for the third iteration:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'All of the visual display is shown in the following diagram:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![Building it from scratch](img/00263.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Three iterations of the gradient boosting algorithm'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, we can''t keep on carrying out the iterations in a detailed execution
    every time and looping is important. The code is kept in a block and 22 more iterations
    are performed. The output at the end of each iteration is depicted in the diagram
    and we put them all in an external file, `Sine_Wave_25_Iterations.pdf`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Following the 25 iterations, we have an overall fit in `GBFit` and we can plot
    this against the actual y values to see how well the gradient boosting algorithm
    has performed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Building it from scratch](img/00264.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Gradient fit versus actual sine data'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The fit is reasonably good for a nonlinear model. The approach was to get a
    clear understanding of the gradient boosting algorithm. A more general form of
    the boosting algorithm is discussed and developed in the next subsection.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Squared-error loss function
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Denote the data by ![Squared-error loss function](img/00265.jpeg), and fix
    the number of iterations/trees as a number *B*. Choose a shrinkage factor ![Squared-error
    loss function](img/00266.jpeg) and tree depth *d*. The gradient boosting algorithm
    based on the squared-error loss function is stated here briefly. See Algorithm
    17.2 of Efron and Hastie (2016), as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Initialize residuals ![Squared-error loss function](img/00267.jpeg) and the
    gradient boosting prediction as ![Squared-error loss function](img/00268.jpeg)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For ![Squared-error loss function](img/00269.jpeg):'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a regression tree of depth *d* for the data ![Squared-error loss function](img/00270.jpeg)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain the predicted values as ![Squared-error loss function](img/00271.jpeg)
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the boosting prediction by ![Squared-error loss function](img/00272.jpeg)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the residuals ![Squared-error loss function](img/00273.jpeg)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the sequence of functions ![Squared-error loss function](img/00274.jpeg)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will define a function `GB_SqEL`, which will implement the gradient
    boosting algorithm driven by the squared-error loss function. The function must
    be provided with five arguments: `y` and `x` will constitute the data, `depth`
    will specify the depth of the trees (that is, the number of splits in the regression
    tree), `iter` for the number of iterations, and `shrinkage` is the ![Squared-error
    loss function](img/00275.jpeg) factor. The `GB_SqEL` function is set up as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The initialization takes place in specifications, and the line `fit <- y*0`.
    The depth argument of the algorithm is taken in the line `maxdepth=depth`, and
    using the `rpart` function, we create a tree of the necessary depth. The `predict`
    function gives the values of ![Squared-error loss function](img/00276.jpeg) as
    required at each iteration, while `fit+gb_hat` does the necessary update. Note
    that `GB_Hat[,i]` consists of the predicted values at the end of each iteration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'We will illustrate the algorithm with the example of Efron and Hastie (2016).
    The data considered is related with Lu Gerig''s disease, or **amyotrophic lateral
    sclerosis** (**ALS**). The dataset has information on 1,822 individuals with the
    ALS disease. The goal is to predict the rate of progression `dFRS` of a functional
    rating score. The study has information on 369 predictors/covariates. Here, we
    will use the `GB_SqEL` function to fit the gradient boosting technique and analyze
    the mean square error as the number of iterations increases. The details and the
    data can be obtained from the source at [https://web.stanford.edu/~hastie/CASI/data.html](https://web.stanford.edu/~hastie/CASI/data.html).
    We will now put the squared-error loss function-driven boosting method into action:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using the `read.table` function, we import the data from the code bundle into
    the `als` object. The data is available from the source in the `.txt` format.
    The column `testset` indicates whether the observations were marked for training
    purposes or for tests. We select the training observations and also drop the first
    variable `testset` and store it in the object `alst`. The `GB_SqEL` function is
    applied on the `alst` object with appropriate specifications.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Following each iteration, we compute the mean-squared error and store it in
    `GB_Hat`, as explained earlier. We can see from the following diagram that as
    the iterations increase, the mean squared error decreases. Here, the algorithm
    stabilizes after nearly 200 iterations:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Squared-error loss function](img/00277.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Gradient boosting and the MSE by iterations'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see the use of two powerful R packages.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Using the adabag and gbm packages
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the boosting method as an ensemble technique is indeed very effective.
    The algorithm was illustrated for classification and regression problems from
    scratch. Once the understand the algorithm clear and transparent, we can then
    use R packages to deliver results going forward. A host of packages are available
    for implementing the boosting technique. However, we will use the two most popular
    packages `adabag` and `gbm` in this section. First, a look at the options of the
    two functions is in order. The names are obvious and `adabag` implements the adaptive
    boosting methods while `gbm` deals with gradient boosting methods. First, we look
    at the options available in these two functions in the following code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the adabag and gbm packages](img/00278.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: The boosting and gbm functions
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The formula is the usual argument. The argument `mfinal` in `adabag` and `n.trees`
    in `gbm` allows the specification of the number of trees or iterations. The boosting
    function gives the option of `boos`, which is the bootstrap sample of the training
    set drawn using the weights for each observation on that iteration. Gradient boosting
    is a more generic algorithm that is capable of handling more than the regression
    structure. It can be used for classification problems as well. The option of `distribution`
    in the `gbm` function gives those options. Similarly, one can see here that the
    `gbm` function offers a host of other options. We will neither undertake the daunting
    task of explaining them all nor apply them to complex datasets. The two datasets
    that were used to explain and elaborate adaptive and gradient boosting algorithms
    will be continued with the `boosting` and `gbm` functions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The toy datasets need to be changed and we will replicate them multiple times
    over so that we have enough observations for running the `boosting` and `gbm`
    functions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `maxdepth=1` function ensures that we are using only stumps as the base
    classifiers. It is easily seen that the boosting function works perfectly, as
    all the observations are correctly classified.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the `boosting` function, we need to have more data points. We increase
    this with the `seq` function and using the `distribution="gaussian"` option, we
    ask the `gbm` function to fit the regression boosting technique:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using the plot functions, we make a comparison of the fit with the gradient
    boosting method. The following diagram suggests that the fit has been appropriate.
    However, the plots also show that something is not quite correct with the story
    either. The function approximation at ![Using the adabag and gbm packages](img/00279.jpeg)
    and ![Using the adabag and gbm packages](img/00280.jpeg) by the boosting method
    leaves a lot to be desired, and the actual versus predicted plot suggests discontinuity/poor
    performance at 0\. However, we will not delve too far into these issues:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the adabag and gbm packages](img/00281.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Sine wave approximation by using the gbm function'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the concept of variable importance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Variable importance
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting methods essentially use trees as base learners, and hence the idea
    of variable importance gets carried over here the same as with trees, bagging,
    and random forests. We simply add the importance of the variables across the trees
    as we do with bagging or random forests.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'For a boosting fitted object from the `adabag` package, the variable importance
    is extracted as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This means that the boosting method has not used the `x2` variable at all.
    For the gradient boosting objects, the importance is given by the `summary` function:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It is now apparent that we only have one variable and so it is important to
    explain the regressand and we certainly did not require some software to tell
    us. Of course, it is useful in complex cases. Comparisons are for different ensembling
    methods based on trees. Let us move on to the next section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Comparing bagging, random forests, and boosting
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We carried out comparisons between the bagging and random forest methods in
    the previous chapter. Using the `gbm` function, we now add boosting accuracy to
    the earlier analyses:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The boosting accuracy of `0.9457` is higher than the random forest accuracy
    of `0.9436`. Further fine tuning, to be explored in the next chapter, will help
    in improving the accuracy. The variable importance is also easily obtained using
    the `summary` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is yet another ramification of decision trees. It is a sequential iteration
    technique where the error from a previous iteration is targeted with more impunity.
    We began with the important adaptive boosting algorithm and used very simple toy
    data to illustrate the underpinnings. The approach was then extended to the regression
    problem and we illustrated the gradient boosting method with two different approaches.
    The two packages `adabag` and `gbm` were briefly elaborated on and the concept
    of variable importance was emphasized yet again. For the spam dataset, we got
    more accuracy with boosting and hence the deliberations of the boosting algorithm
    are especially more useful.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The chapter considered different variants of the boosting algorithm. However,
    we did not discuss why it works at all. In the next chapter, these aspects will
    be covered in more detail.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
