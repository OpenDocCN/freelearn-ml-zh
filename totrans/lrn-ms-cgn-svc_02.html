<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Analyzing Images to Recognize a Face"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Analyzing Images to Recognize a Face</h1></div></div></div><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>"We can use the Computer Vision API to prove to our clients the reliability of the data, so they can be confident making important business decisions based on that information."</em></span>
</p><p>- Leendert de Voogd, CEO of Vigiglobe</p></blockquote></div><p>In the previous chapter, you were briefly introduced to Microsoft Cognitive Services. Throughout this chapter, we will dive into image-based APIs from the vision API. We will learn how to perform image analysis. Moving on, we will dive deeper into the Face API, which we briefly looked at in the previous chapter, and we will learn how you can identify people. Next, we will learn how to use the Face API to recognize emotions in faces. Finally, we will learn about the different ways to moderate content.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Analyzing images to identify content, metadata, and adult ratings.</li><li class="listitem" style="list-style-type: disc">Recognizing celebrities in images and reading text in images.</li><li class="listitem" style="list-style-type: disc">Diving into the Face API:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learning to find the likelihood of two faces belonging to the same person</li><li class="listitem" style="list-style-type: disc">Grouping faces based on visual similarities and searching similar faces</li><li class="listitem" style="list-style-type: disc">Identifying a person from a face</li><li class="listitem" style="list-style-type: disc">Recognizing emotions</li></ul></div></li><li class="listitem" style="list-style-type: disc">Content moderation.</li></ul></div><div class="section" title="Analyze an image using the Computer Vision API"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Analyze an image using the Computer Vision API</h1></div></div></div><p>The Computer Vision API <a class="indexterm" id="id84"/>allows us to process an image and retrieve information about it. It relies on advanced algorithms to analyze the content of <a class="indexterm" id="id85"/>the image in different ways, based on our needs.</p><p>Throughout<a class="indexterm" id="id86"/> this section, we will learn how to take advantage of this API. We will look at the different ways to analyze an image through standalone examples. Some of the features we will cover will also be incorporated into our<a class="indexterm" id="id87"/> end-to-end application in a later chapter.</p><p>Calling any of the APIs will return one of the following response codes:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Code</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">200</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Information of the extracted features in JSON format.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">400</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Typically, this means bad request. It may be an invalid image URL, an image that is too small or too large, an invalid image format, or any other errors to do with the request body.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">415</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Unsupported media type.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">500</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Possible errors may include a failure to process the image, image processing timing out, or an internal server error.</p>
</td></tr></tbody></table></div><div class="section" title="Setting up a chapter example project"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec10"/>Setting up a chapter example project</h2></div></div></div><p>Before we<a class="indexterm" id="id88"/> go into the specifics of the API, we need to create an example project for this chapter. This project will contain all of the examples, which will not be put into the end-to-end application at this stage:</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>If you have not already done so, sign up for an API key for Computer Vision by visiting <a class="ulink" href="https://portal.azure.com">https://portal.azure.com</a>.</p></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a new project in Visual Studio using the template we created in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>.</li><li class="listitem">Right-click on the project and choose <span class="strong"><strong>Manage NuGet Packages</strong></span>. Search for the <code class="literal">Microsoft.ProjectOxford.Vision</code> package and install it into the project, as shown in the following screenshot:<div class="mediaobject"><img alt="Setting up a chapter example project" src="graphics/B12373_02_01.jpg"/></div></li><li class="listitem">Create <a class="indexterm" id="id89"/>the following <code class="literal">UserControls</code> files and add them into the <code class="literal">ViewModel</code> folder:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">CelebrityView.xaml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DescriptionView.xaml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ImageAnalysisView.xaml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">OcrView.xaml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ThumbnailView.xaml</code></li></ul></div></li><li class="listitem">Also, add the corresponding <code class="literal">ViewModel</code> instances from the following list into the <code class="literal">ViewModel</code> folder:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">CelebrityViewModel.cs</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DescriptionViewModel.cs</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ImageAnalysisViewModel.cs</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">OcrViewModel.cs</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ThumbnailViewModel.cs</code></li></ul></div></li></ol></div><p>Go through the newly created <code class="literal">ViewModel</code> instances and make sure that all classes are public.</p><p>We will switch between the different views using a <code class="literal">TabControl</code> tag. Open the <code class="literal">MainView.xaml</code> file and add the following in the precreated <code class="literal">Grid</code> tag:</p><div class="informalexample"><pre class="programlisting">    &lt;TabControl x: Name = "tabControl"
                   HorizontalAlignment = "Left"
                   VerticalAlignment = "Top"
                   Width = "810" Height = "520"&gt;
        &lt;TabItem Header="Analysis" Width="100"&gt;
            &lt;controls:ImageAnalysisView /&gt;
        &lt;/TabItem&gt;
        &lt;TabItem Header="Description" Width="100"&gt;
            &lt;controls:DescriptionView /&gt;
        &lt;/TabItem&gt;
        &lt;TabItem Header="Celebs" Width="100"&gt;
            &lt;controls:CelebrityView /&gt;
        &lt;/TabItem&gt;
        &lt;TabItem Header="OCR" Width="100"&gt;
            &lt;controls:OcrView /&gt;
        &lt;/TabItem&gt;
        &lt;TabItem Header="Thumbnail" Width="100"&gt;
            &lt;controls:ThumbnailView /&gt;
        &lt;/TabItem&gt;
    &lt;/TabControl&gt;</pre></div><p>This<a class="indexterm" id="id90"/> will add a tab bar at the top of the application that will allow you to navigate between the different views.</p><p>Next, we will add the properties and members required in our <code class="literal">MainViewModel.cs</code> file.</p><p>The following is the variable used to access the Computer Vision API:</p><div class="informalexample"><pre class="programlisting">    private IVisionServiceClient _visionClient;</pre></div><p>The following code declares a private variable holding the <code class="literal">CelebrityViewModel</code> object. It also declares the <code class="literal">public</code> property that we use to access the <code class="literal">ViewModel</code> in our <code class="literal">View</code>:</p><div class="informalexample"><pre class="programlisting">    private CelebrityViewModel _celebrityVm;
    public CelebrityViewModel CelebrityVm
    {
        get { return _celebrityVm; }
        set
        {
            _celebrityVm = value;
            RaisePropertyChangedEvent("CelebrityVm");
        }
    }</pre></div><p>Following the same pattern, add properties for the rest of the created <code class="literal">ViewModel</code> instances.</p><p>With all the properties in place, create the <code class="literal">ViewModel</code> instances in our constructor using the following code:</p><div class="informalexample"><pre class="programlisting">    public MainViewModel()
    {
        _visionClient = new VisionServiceClient("VISION_API_KEY_HERE", "ROOT_URI");

        CelebrityVm = new CelebrityViewModel(_visionClient);
        DescriptionVm = new DescriptionViewModel(_visionClient);
        ImageAnalysisVm= new ImageAnalysisViewModel(_visionClient);
        OcrVm = new OcrViewModel(_visionClient);
        ThumbnailVm = new ThumbnailViewModel(_visionClient);
    }</pre></div><p>Note how<a class="indexterm" id="id91"/> we first create the <code class="literal">VisionServiceClient</code> object with the API key that we signed up for earlier and the root URI, as described in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>. This is then injected into all the <code class="literal">ViewModel</code> instances to be used there.</p><p>This should now compile and present you with the application shown in the following screenshot:</p><div class="mediaobject"><img alt="Setting up a chapter example project" src="graphics/B12373_02_02.jpg"/></div></div><div class="section" title="Generic image analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/>Generic image analysis</h2></div></div></div><p>We<a class="indexterm" id="id92"/> start enabling generic <a class="indexterm" id="id93"/>image analysis by adding a UI to the <code class="literal">ImageAnalysis.xaml</code> file. All the Computer Vision example UIs will be built in the same manner.</p><p>The UI should have two columns, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    &lt;Grid.ColumnDefinitions&gt;
        &lt;ColumnDefinition Width="*" /&gt;
        &lt;ColumnDefinition Width="*" /&gt;
    &lt;/Grid.ColumnDefinitions&gt;</pre></div><p>The first one will contain the image selection, while the second one will display our results.</p><p>In the <a class="indexterm" id="id94"/>left-hand column, we create a vertically oriented <code class="literal">StackPanel</code> label. To this, we add a label and a <code class="literal">ListBox</code> label. The list box will display a list of visual features that we can add to our analysis query. Note how we have a <code class="literal">SelectionChanged</code> event hooked up in the <code class="literal">ListBox</code> label in<a class="indexterm" id="id95"/> the following code. This will be added behind the code, and will be covered shortly:</p><div class="informalexample"><pre class="programlisting">    &lt;StackPanel Orientation="Vertical"Grid.Column="0"&gt;

    &lt;TextBlock Text="Visual Features:"
               FontWeight="Bold"
               FontSize="15"
               Margin="5, 5" Height="20" /&gt;

    &lt;ListBox: Name = "VisualFeatures"
          ItemsSource = "{Binding ImageAnalysisVm.Features}"
          SelectionMode = "Multiple" Height="150" Margin="5, 0, 5, 0"
          SelectionChanged = "VisualFeatures_SelectionChanged" /&gt;</pre></div><p>The list box will be able to select multiple items, and the items will be gathered in the <code class="literal">ViewModel</code>.</p><p>In the same stack panel, we also add a button element and an image element. These will allow us to browse for an image, show it, and analyze it. Both the <code class="literal">Button</code> command and the image source are bound to the corresponding properties in the <code class="literal">ViewModel</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    &lt;Button Content = "Browse and analyze"
            Command = "{Binding ImageAnalysisVm.BrowseAndAnalyzeImageCommand}"
            Margin="5, 10, 5, 10" Height="20" Width="120"
            HorizontalAlignment="Right" /&gt;
       
    &lt;Image Stretch = "Uniform"
           Source="{Binding ImageAnalysisVm.ImageSource}"
           Height="280" Width="395" /&gt;
    &lt;/StackPanel&gt;</pre></div><p>We also add another vertically oriented stack panel. This will be placed in the right-hand column. It contains a title label, as well as a textbox, bound to the analysis result in our <code class="literal">ViewModel</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    &lt;StackPanel Orientation= "Vertical"Grid.Column="1"&gt;
        &lt;TextBlock Text="Analysis Results:"
                   FontWeight = "Bold"
                   FontSize="15" Margin="5, 5" Height="20" /&gt;
        &lt;TextBox Text = "{Binding ImageAnalysisVm.AnalysisResult}"
                 Margin="5, 0, 5, 5" Height="485" /&gt;
    &lt;/StackPanel&gt;</pre></div><p>Next, we want to add our <code class="literal">SelectionChanged</code> event handler to our code-behind. Open the <code class="literal">ImageAnalysisView.xaml.cs</code> file and add the following:</p><div class="informalexample"><pre class="programlisting">    private void VisualFeatures_SelectionChanged(object sender, SelectionChangedEventArgs e) {
        var vm = (MainViewModel) DataContext;
        vm.ImageAnalysisVm.SelectedFeatures.Clear();</pre></div><p>The <a class="indexterm" id="id96"/>first line of the function will give us the current <code class="literal">DataContext</code>, which is the <code class="literal">MainViewModel</code> class. We<a class="indexterm" id="id97"/> access the <code class="literal">ImageAnalysisVm</code> property, which is our <code class="literal">ViewModel</code>, and clear the selected visual features list.</p><p>From there, we loop through the selected items from our list box. All items will be added to the <code class="literal">SelectedFeatures</code> list in our <code class="literal">ViewModel</code>:</p><div class="informalexample"><pre class="programlisting">        foreach(VisualFeature feature in VisualFeatures.SelectedItems)
        {
            vm.ImageAnalysisVm.SelectedFeatures.Add(feature);
        }
    }</pre></div><p>Open the <code class="literal">ImageAnalysisViewModel.cs</code> file. Make sure that the class inherits the <code class="literal">ObservableObject</code> class.</p><p>Declare a <code class="literal">private</code> variable, as follows:</p><div class="informalexample"><pre class="programlisting">    private IVisionServiceClient _visionClient;    </pre></div><p>This will be used to access the Computer Vision API, and it is initialized through the constructor.</p><p>Next, we declare a private variable and the corresponding property for our list of visual features, as follows:</p><div class="informalexample"><pre class="programlisting">    private List&lt;VisualFeature&gt; _features=new List&lt;VisualFeature&gt;();
    public List&lt;VisualFeature&gt; Features {
        get { return _features; }
        set {
            _features = value;
            RaisePropertyChangedEvent("Features");
        }
    }</pre></div><p>In a similar manner, create a <code class="literal">BitmapImage</code> variable and property called <code class="literal">ImageSource</code>. Create a list of <code class="literal">VisualFeature</code> types called <code class="literal">SelectedFeatures</code> and a string called <code class="literal">AnalysisResult</code>.</p><p>We<a class="indexterm" id="id98"/> also need to declare the property for our button, as follows:</p><div class="informalexample"><pre class="programlisting">    public ICommandBrowseAndAnalyzeImageCommand {get; private set;}</pre></div><p>With that in place, we create our constructor, as follows:</p><div class="informalexample"><pre class="programlisting">    public ImageAnalysisViewModel(IVisionServiceClientvisionClient) {
        _visionClient = visionClient;
        Initialize();
    }</pre></div><p>The <a class="indexterm" id="id99"/>constructor takes one parameter, the <code class="literal">IVisionServiceClient</code> object, which we have created in our <code class="literal">MainViewModel</code> file. It assigns that parameter to the variable that we created earlier. Then we call an <code class="literal">Initialize</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">    private void Initialize() {
        Features = Enum.GetValues(typeof(VisualFeature))
                       .Cast&lt;VisualFeature&gt;().ToList();

        BrowseAndAnalyzeImageCommand = new DelegateCommand(BrowseAndAnalyze);
    }</pre></div><p>In the <code class="literal">Initialize</code> function, we fetch all the values from the <code class="literal">VisualFeature</code> variable of the <code class="literal">enum</code> type. These values are added to the features list, which is displayed in the UI. We also created our button, and now that we have done so, we need to create the corresponding action, as follows:</p><div class="informalexample"><pre class="programlisting">    private async void BrowseAndAnalyze(object obj)
    {
        var openDialog = new Microsoft.Win32.OpenFileDialog();

        openDialog.Filter = "JPEG Image(*.jpg)|*.jpg";
        bool? result = openDialog.ShowDialog();

        if (!(bool)result) return;

        string filePath = openDialog.FileName;

        Uri fileUri = new Uri(filePath);
        BitmapImage image = new BitmapImage(fileUri);

        image.CacheOption = BitmapCacheOption.None;
        image.UriSource = fileUri;

        ImageSource = image;</pre></div><p>The <a class="indexterm" id="id100"/>first lines of the preceding code are similar to what we did in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>. We open a file browser and get the selected image.</p><p>With an image selected, we run an analyze on it, as follows:</p><div class="informalexample"><pre class="programlisting">    try {
        using (StreamfileStream = File.OpenRead(filePath)) {
            AnalysisResult analysisResult = await  _visionClient.AnalyzeImageAsync(fileStream, SelectedFeatures);</pre></div><p>We <a class="indexterm" id="id101"/>call the <code class="literal">AnalyzeImageAsync</code> function of our <code class="literal">_visionClient</code>. This function has four overloads, all of which are quite similar. In our case, we pass on the image as a <code class="literal">Stream</code> type and the <code class="literal">SelectedFeatures</code> list, containing the <code class="literal">VisualFeatures</code> variable to analyze.</p><p>The request parameters<a class="indexterm" id="id102"/> are as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Image (required)</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Can be uploaded in the form of a raw image binary or URL.</li><li class="listitem" style="list-style-type: disc">Can be JPEG, PNG, GIF, or BMP.</li><li class="listitem" style="list-style-type: disc">File size must be less than 4 MB.</li><li class="listitem" style="list-style-type: disc">Image dimensions must be at least 50 x 50 pixels.</li></ul></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Visual features (optional)</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>A list indicating the visual feature types to return. It can include categories, tags, descriptions, faces, image types, color, and whether or not it is adult content.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Details (optional)</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>A list indicating what domain-specific details to return.</p>
</td></tr></tbody></table></div><p>The response to this request is the <code class="literal">AnalysisResult</code> string.</p><p>We then check to see if the result is <code class="literal">null</code>. If it is not, we call a function to parse it and assign the result to our <code class="literal">AnalysisResult</code> string, as follows:</p><div class="informalexample"><pre class="programlisting">    if (analysisResult != null)
        AnalysisResult = PrintAnalysisResult(analysisResult);</pre></div><p>Remember to close the <code class="literal">try</code> clause and finish the method with the corresponding <code class="literal">catch</code> clause.</p><p>The <code class="literal">AnalysisResult</code> string contains data according to the visual features requested in the API call.</p><p>Data in the<a class="indexterm" id="id103"/> <code class="literal">AnalysisResult</code> variable is described in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Visual feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Categories</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Images are categorized according to a defined taxonomy. This includes everything from animals, buildings, and outdoors, to people.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Tags</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Images are tagged with a list of words related to the content.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Description</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This contains a full sentence describing the image.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Faces</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This detects faces in images and contains face coordinates, gender, and age.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>ImageType</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This detects whether an image is clipart or a line drawing.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Color</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This contains information about dominant colors, accent colors, and whether or not the image is in black and white.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Adult</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This detects whether an image is pornographic in nature and whether or not it is racy.</p>
</td></tr></tbody></table></div><p>To<a class="indexterm" id="id104"/> retrieve data, for <a class="indexterm" id="id105"/>example for categories, you can use the following:</p><div class="informalexample"><pre class="programlisting">    if (analysisResult.Description != null) {
        result.AppendFormat("Description: {0}\n", analysisResult.Description.Captions[0].Text);
        result.AppendFormat("Probability: {0}\n\n", analysisResult.Description.Captions[0].Confidence);
    }</pre></div><p>A successful call would present us with the following result:</p><div class="mediaobject"><img alt="Generic image analysis" src="graphics/B12373_02_03.jpg"/></div><p>Sometimes, you<a class="indexterm" id="id106"/> may only <a class="indexterm" id="id107"/>be interested in the image description. In such cases, it is wasteful to ask for the kind of full analysis that we have just done. By calling the following function, you will get an array of descriptions:</p><div class="informalexample"><pre class="programlisting">    AnalysisResultdescriptionResult = await _visionClient.DescribeAsync(ImageUrl, NumberOfDescriptions);</pre></div><p>In this call, we have specified a URL for the image and the number of descriptions to return. The first parameter must always be included, but it may be an image upload instead of a URL. The second parameter is optional, and in cases where it is not provided, it defaults to one.</p><p>A <a class="indexterm" id="id108"/>successful query will <a class="indexterm" id="id109"/>result in an <code class="literal">AnalysisResult</code> object, which is the same as the one that was described in the preceding code. In this case, it will only contain the request ID, image metadata, and an array of captions. Each caption contains an image description and the confidence of that description being correct.</p><p>We will add this form of image analysis to our smart-house application in a later chapter.</p></div><div class="section" title="Recognizing celebrities using domain models"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Recognizing celebrities using domain models</h2></div></div></div><p>One <a class="indexterm" id="id110"/>of the features of the Computer Vision API is the ability to recognize domain-specific content. At the time of writing, the API only supports celebrity recognition, where it is able to recognize around 200,000 celebrities.</p><p>For this example, we choose to use an image from the internet. The UI will then need a textbox to input the URL. It will need a button to load the image and perform the domain analysis. There should be an image element to see the image and a textbox to output the result.</p><p>The corresponding <code class="literal">ViewModel</code> should have two <code class="literal">string</code> properties for the URL and the analysis result. It should have a <code class="literal">BitmapImage</code> property for the image and an <code class="literal">ICommand</code> property for our button.</p><p>Add a <code class="literal">private</code> variable for the <code class="literal">IVisionServiceClient</code> type at the start of the <code class="literal">ViewModel</code>, as follows:</p><div class="informalexample"><pre class="programlisting">    private IVisionServiceClient _visionClient;</pre></div><p>This should be assigned in the constructor, which will take a parameter of the <code class="literal">IVisionServiceClient</code> type.</p><p>As we need a URL to fetch an image from the internet, we need to initialize the <code class="literal">Icommand</code> property with both an action and a predicate. The latter checks whether the URL property is set or not, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    public CelebrityViewModel(IVisionServiceClient visionClient) {
        _visionClient = visionClient;
        LoadAndFindCelebrityCommand = new DelegateCommand(LoadAndFindCelebrity, CanFindCelebrity);
    }</pre></div><p>The <code class="literal">LoadAndFindCelebrity</code> load creates a <code class="literal">Uri</code> with the given URL. Using this, it creates a <code class="literal">BitmapImage</code> and assigns this to <code class="literal">ImageSource</code>, the <code class="literal">BitmapImage</code> property, as shown in the following code. The image should be visible in the UI:</p><div class="informalexample"><pre class="programlisting">    private async void LoadAndFindCelebrity(object obj) {
        UrifileUri = new Uri(ImageUrl);
        BitmapImage image = new BitmapImage(fileUri);

        image.CacheOption = BitmapCacheOption.None;
        image.UriSource = fileUri;

        ImageSource = image;</pre></div><p>We <a class="indexterm" id="id111"/>call the <code class="literal">AnalyzeImageInDomainAsync</code> type with the given URL, as shown in the following code. The first parameter we pass in is the image URL. Alternatively, this could have been an image that was opened as a <code class="literal">Stream</code> type:</p><div class="informalexample"><pre class="programlisting">    try {
        AnalysisInDomainResultcelebrityResult = await _visionClient.AnalyzeImageInDomainAsync(ImageUrl, "celebrities");

        if (celebrityResult != null)
            Celebrity = celebrityResult.Result.ToString();
    }</pre></div><p>The second parameter is the domain model name, which is in a <code class="literal">string</code> format. As an alternative, we could have used a specific <code class="literal">Model</code> object, which can be retrieved by calling the following:</p><div class="informalexample"><pre class="programlisting">    VisionClient.ListModelsAsync();</pre></div><p>This would return an array of <code class="literal">Models</code>, which we can display and select from. As there is only one available at this time, there is no point in doing so.</p><p>The result from <code class="literal">AnalyzeImageInDomainAsync</code> is an object of the <code class="literal">AnalysisInDomainResult </code>type. This object will contain the request ID, metadata of the image, and the result, containing an array of celebrities. In our case, we simply output the entire result array. Each item in this array will contain the name of the celebrity, the confidence of a match, and the face rectangle in the image. Do try it in the example code provided.</p></div><div class="section" title="Utilizing optical character recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Utilizing optical character recognition</h2></div></div></div><p>For <a class="indexterm" id="id112"/>some <a class="indexterm" id="id113"/>tasks, <span class="strong"><strong>optical character recognition</strong></span> (<span class="strong"><strong>OCR</strong></span>) can be very useful. Say that you took a photo of a receipt. Using OCR, you can read the amount from the photo itself and have it automatically added to accounting.</p><p>OCR will<a class="indexterm" id="id114"/> detect text in images and extract machine-readable characters. It will automatically detect language. Optionally, the API will detect image orientation and correct it before reading the text.</p><p>To<a class="indexterm" id="id115"/> specify a language, you need to<a class="indexterm" id="id116"/> use the <span class="strong"><strong>BCP-47</strong></span> language code. At the time of writing, the following languages are supported: simplified Chinese, traditional Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Turkish, Arabic, Romanian, Cyrillic Serbian, Latin Serbian, and Slovak.</p><p>In the code example, the UI will have an image element. It will also have a button to load the image and detect text. The result will be printed to a textbox element.</p><p>The <code class="literal">ViewModel</code> will need a <code class="literal">string</code> property for the result, a <code class="literal">BitmapImage</code> property for the image, and an <code class="literal">ICommand</code> property for the button.</p><p>Add a <code class="literal">private</code> variable to the <code class="literal">ViewModel</code> for the Computer Vision API, as follows:</p><div class="informalexample"><pre class="programlisting">    private IVisionServiceClient _visionClient;</pre></div><p>The constructor should have one parameter of the <code class="literal">IVisionServiceClient</code> type, which should be assigned to the preceding variable.</p><p>Create a function as a command for our button. Call it <code class="literal">BrowseAndAnalyze</code> and have it accept <code class="literal">object</code> as the parameter. Then, open a file browser and find an image to analyze. With the image selected, we run the OCR analysis, as follows:</p><div class="informalexample"><pre class="programlisting">    using (StreamfileStream = File.OpenRead(filePath)) {
        OcrResultsanalysisResult = await _visionClient.RecognizeTextAsync (fileStream);

        if(analysisResult != null)
            OcrResult = PrintOcrResult(analysisResult);
    }</pre></div><p>With the image opened as a <code class="literal">Stream</code> type, we call the <code class="literal">RecognizeTextAsync</code> method. In this case, we pass on the image as a <code class="literal">Stream</code> type, but we could just as easily have passed on a URL to an image.</p><p>Two more parameters may be specified in this call. First, you can specify the language of the text. The default is unknown, which means that the API will try to detect the language automatically. Second, you can specify whether or not the API should detect the orientation of the image. The default is set to <code class="literal">false</code>.</p><p>If the call succeeds, it will return data in the form of an <code class="literal">OcrResults</code> object. We send this result to a function, the <code class="literal">PrintOcrResult</code> function, where we will parse it and print the text, as follows:</p><div class="informalexample"><pre class="programlisting">    private string PrintOcrResult(OcrResultsocrResult)
    {
        StringBuilder result = new StringBuilder();

        result.AppendFormat("Language is {0}\n", ocrResult.Language);
        result.Append("The words are:\n\n");</pre></div><p>First, we <a class="indexterm" id="id117"/>create a <code class="literal">StringBuilder</code> object, which will hold all the text. The first content we add to it is the language of the text in the image, as follows:</p><div class="informalexample"><pre class="programlisting">        foreach(var region in ocrResult.Regions) { 
            foreach(var line in region.Lines) { 
                foreach(var text in line.Words) { 
                    result.AppendFormat("{0} ", text.Text);
                }
                result.Append("\n");
            }
            result.Append("\n\n");
        }</pre></div><p>The <a class="indexterm" id="id118"/>result has an array, which contains the <code class="literal">Regions</code> property. Each item represents recognized text, and each region contains multiple lines. The <code class="literal">line</code> variables are arrays, where each item represents recognized text. Each line contains an array of the <code class="literal">Words</code> property. Each item in this array represents a recognized word.</p><p>With all the words appended to the <code class="literal">StringBuilder</code> function, we return it as a string. This will then be printed in the UI, as shown in the following screenshot:</p><div class="mediaobject"><img alt="Utilizing optical character recognition" src="graphics/B12373_02_04.jpg"/></div><p>The <a class="indexterm" id="id119"/>result also contains the orientation and angle of the text. Combining this with the <a class="indexterm" id="id120"/>bounding box, also included, you can mark each word in the original image.</p></div><div class="section" title="Generating image thumbnails"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/>Generating image thumbnails</h2></div></div></div><p>In<a class="indexterm" id="id121"/> today's world, we, as developers, have<a class="indexterm" id="id122"/> to consider different screen sizes when displaying images. The Computer Vision API offers some help with this by providing the ability to generate thumbnails.</p><p>Thumbnail generation, in itself, is not that big a deal. What makes the API clever is that it analyzes the image and determines the region of interest.</p><p>It will <a class="indexterm" id="id123"/>also generate smart cropping coordinates. This means that if the specified aspect ratio differs from the original, it will crop the image, with a focus on the interesting regions.</p><p>In the <a class="indexterm" id="id124"/>example code, the UI consists of two image elements and one button. The first image is the image in its original size. The second is for the generated thumbnail, which we specify to be 250 x 250 pixels in size.</p><p>The <code class="literal">View</code> model will need the corresponding properties, two <code class="literal">BitmapImages</code> methods to act as image sources, and one <code class="literal">ICommand</code> property for our button command.</p><p>Define a private variable in the <code class="literal">ViewModel</code>, as follows:</p><div class="informalexample"><pre class="programlisting">    private IVisionServiceClient _visionClient;</pre></div><p>This will be our API access point. The constructor should accept an <code class="literal">IVisionServiceClient</code> object, which should be assigned to the preceding variable.</p><p>For the <code class="literal">ICommand</code> property, we create a function, <code class="literal">BrowseAndAnalyze</code>, accepting an <code class="literal">object</code> parameter. We do not need to check whether we can execute the command. We will browse for an image each time.</p><p>In the <code class="literal">BrowseAndAnalyze</code> function, we open a file dialog and select an image. When we have the image file path, we can generate our thumbnail, as follows:</p><div class="informalexample"><pre class="programlisting">    using (StreamfileStream = File.OpenRead(filePath))
    {
        byte[] thumbnailResult = await _visionClient.GetThumbnailAsync(fileStream, 250, 250);

        if(thumbnailResult != null &amp;&amp;thumbnailResult.Length != 0)
            CreateThumbnail(thumbnailResult);
    }</pre></div><p>We open the image file so that we have a <code class="literal">Stream</code> type. This stream is the first parameter in our call to the <code class="literal">GetThumbnailAsync</code> method. The next two parameters indicate the width and height that we want for our thumbnail.</p><p>By default, the API call will use smart cropping, so we do not have to specify it. If we have a case where we do not want smart cropping, we could add a <code class="literal">bool</code> variable as the fourth parameter.</p><p>If the call succeeds, we get a <code class="literal">byte</code> array back. This is the image data. If it contains data, we pass it on to a new function, <code class="literal">CreateThumbnail</code>, to create a <code class="literal">BitmapImage</code> object from it, as follows:</p><div class="informalexample"><pre class="programlisting">    private void CreateThumbnail(byte[] thumbnailResult)
    {
        try {
            MemoryStreamms = new MemoryStream(thumbnailResult);
            ms.Seek(0, SeekOrigin.Begin);</pre></div><p>To <a class="indexterm" id="id125"/>create an image from a <code class="literal">byte</code> array, we create a <code class="literal">MemoryStream</code> object from it. We make sure that we start at the beginning of the array.</p><p>Next, we<a class="indexterm" id="id126"/> create a <code class="literal">BitmapImage</code> object and begin to initialize it. We specify the <code class="literal">CacheOption</code> and set the <code class="literal">StreamSource</code> to the <code class="literal">MemoryStream</code> variables we created earlier. Finally, we stop the <code class="literal">BitmapImage</code> initialization and assign the image to our <code class="literal">Thumbnail</code> property, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">        BitmapImage image = new BitmapImage();
        image.BeginInit();
        image.CacheOption = BitmapCacheOption.None;
        image.StreamSource = ms;
        image.EndInit();

        Thumbnail = image;   </pre></div><p>Close up the <code class="literal">try</code> clause and add the corresponding <code class="literal">catch</code> clause. You should now be able to generate thumbnails.</p></div></div></div>
<div class="section" title="Diving deep into the Face API"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Diving deep into the Face API</h1></div></div></div><p>The <a class="indexterm" id="id127"/>Face API has two main features. The first one is face detection and the other is face recognition.</p><p>Face detection allows us to detect up to 64 faces in one image. We have already seen the basic usage. The features of face recognition are implied in its name: using it, we can detect whether two <a class="indexterm" id="id128"/>faces belong to the same person. We can find similar faces, or one in particular, and we can group similar faces. We will learn how to do all of this in the following sections.</p><p>When calling any of the APIs, it will respond with one of the following responses:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Code</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">200</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Successful call. It returns an array containing data related to the API call.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">400</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Request body is invalid. This can be a number of errors, depending on the API call. Typically, the request code is invalid.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">401</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Access denied because of an invalid subscription key. The key may be wrong or the account/subscription plan may be blocked.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">403</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Out of call volume data. You have made all the available calls to the API for this month.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">415</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Invalid media type.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">429</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Rate limit is exceeded. You will need to wait a period of time (less than one minute in the free preview) before you try again.</p>
</td></tr></tbody></table></div><div class="section" title="Retrieving more information from the detected faces"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/>Retrieving more information from the detected faces</h2></div></div></div><p>In <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>, we learned the very basic form of face detection. In the example, we retrieved a <code class="literal">Face</code> array. This contained information <a class="indexterm" id="id129"/>on all faces that were found in an image. In that specific example, we obtained information about the face rectangle, face ID, face landmarks, and age.</p><p>When calling the API, there are <a class="indexterm" id="id130"/>four request parameters, as shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">image</code>
</p>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The image in which to search for faces. It will either be in the form of a URL or binary data.</li><li class="listitem" style="list-style-type: disc">Supported formats are JPEG, PNG, GIF, and BMP.</li><li class="listitem" style="list-style-type: disc">The maximum file size is 4 MB.</li><li class="listitem" style="list-style-type: disc">The size of detectable faces is between 36 x 36 pixels and 4096 x 4096 pixels.</li></ul></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">return FaceId</code> (optional)</p>
</td><td style="text-align: left" valign="top">
<p>Boolean value. This specifies whether the response should include the face ID or not.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">return FaceLandmarks</code> (optional)</p>
</td><td style="text-align: left" valign="top">
<p>Boolean value. This specifies whether the response should include <code class="literal">FaceLandmarks</code> in detected faces.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">return FaceAttributes</code> (optional)</p>
</td><td style="text-align: left" valign="top">
 <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">String value. This is a comma-separated string containing all face attributes that are to be analyzed.</li><li class="listitem" style="list-style-type: disc">Supported attributes are age, gender, head pose, smile, facial hair, emotion, and glasses.</li><li class="listitem" style="list-style-type: disc">These attributes are still experimental, and should be treated as such.</li></ul></div>
</td></tr></tbody></table></div><p>If a face is successfully discovered, it will expire in 24 hours. When calling other parts of the Face API, you are often required to have a face ID as an input. In those cases, we need to detect a face first, followed by the call to the API we wish to use, using the detected face as a parameter.</p><p>Using this<a class="indexterm" id="id131"/> knowledge, I challenge you to play around with the example in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>. Draw a rectangle around the face. Mark the eyes in the image.</p></div><div class="section" title="Deciding whether two faces belong to the same person"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/>Deciding whether two faces belong to the same person</h2></div></div></div><p>To <a class="indexterm" id="id132"/>decide whether two faces belong to the same person, we are going to call the <code class="literal">Verify</code> function of the API. The API allows us to detect when two faces are of the same person, which is<a class="indexterm" id="id133"/> called <span class="strong"><strong>face-to-face verification</strong></span>. Detecting whether a face belongs to a specific person is <a class="indexterm" id="id134"/>called <span class="strong"><strong>face-to-person verification</strong></span>.</p><p>The UI will consist of three button elements, two image elements, and one text block element. Two of the buttons will be used to browse for images, which are then shown in each image element. The last button will run the verification. The text block will output the result.</p><p>Lay out the UI how you want and bind the different elements to properties in the <code class="literal">ViewModel</code>, as we have done previously. In the <code class="literal">ViewModel</code>, there should be two <code class="literal">BitmapImage</code> properties for the image elements. There should be one <code class="literal">string</code> property, containing the verification result. Finally, there should be three <code class="literal">ICommand</code> properties, one for each of our buttons.</p><p>Remember to add the UI to the <code class="literal">MainView.xaml</code> file as a new <code class="literal">TabItem</code>. In addition, add the <code class="literal">ViewModel</code> to the <code class="literal">MainViewModel.cs</code> file, where you will also need to add a new variable for the <code class="literal">FaceServiceClient</code> variable. This should be created with the Face API key, which we signed up for in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Microsoft Cognitive Services">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Microsoft Cognitive Services</em></span>.</p><p>In the <code class="literal">ViewModel</code>, we need to declare the following three <code class="literal">private</code> variables:</p><div class="informalexample"><pre class="programlisting">    private FaceServiceClient _faceServiceClient;
    private Guid _faceId1 = Guid.Empty;
    private Guid _faceId2 = Guid.Empty;</pre></div><p>We have seen the first one before; it will access the Face API. The two <code class="literal">Guid</code> variables will be assigned when we have run the face detection.</p><p>The constructor accepts one parameter, which is our <code class="literal">FaceServiceClient</code> object. This is assigned to the previously created variable, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    public FaceVerificationViewModel (FaceServiceClientfaceServiceClient)
    {    
        _faceServiceClient = faceServiceClient;
        Initialize();
    }</pre></div><p>From <a class="indexterm" id="id135"/>the constructor, we call the <code class="literal">Initialize</code> function to create the <code class="literal">DelegateCommand</code> properties, as follows:</p><div class="informalexample"><pre class="programlisting">    private void Initialize()
    {
        BrowseImage1Command = new DelegateCommand(BrowseImage1);
        BrowseImage2Command = new DelegateCommand(BrowseImage2);
        VerifyImageCommand = new DelegateCommand(VerifyFace, CanVerifyFace);
    }</pre></div><p>The browse commands do not need to be disabled at any point, so we just pass on the command function, as follows:</p><div class="informalexample"><pre class="programlisting">    private async void BrowseImage1(object obj) {
        Image1Source = await BrowseImageAsync(1);
    }</pre></div><p>Both functions will look similar. We call another function to browse for an image and detect a face. To separate each image, we pass on the image number.</p><p>The <code class="literal">BrowseImageAsync</code> function will accept an <code class="literal">int</code> type as a parameter. It returns a <code class="literal">BitmapImage</code> object, which we assign to the <code class="literal">BitmapImage</code> property bound to our UI. The first part opens a browse dialog and returns the selected image. We will jump in when we have the image and the path to that image.</p><p>We open the image as a <code class="literal">Stream</code> object. The <code class="literal">Stream</code> object is used in the API call to detect faces. When we call the API, we can use the default call, as it will return the value we are interested in, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    try {
        using (Stream fileStream = File.OpenRead(filePath)) {
            Face[] detectedFaces = await  _faceServiceClient.DetectAsync(fileStream);</pre></div><p>When the detection process has completed, we check to see which image this is and assign the <code class="literal">FaceId</code> parameter to the correct <code class="literal">Guid</code> variable using the following code. For this example, we are assuming that there will be only one face per image:</p><div class="informalexample"><pre class="programlisting">            if (imagenumber == 1)
                _faceId1 = detectedFaces[0].FaceId;
            else
                _faceId2 = detectedFaces[0].FaceId;
        }
    }</pre></div><p>Finish off the function by adding catch clauses as you see fit. You also need to create and return a <code class="literal">BitmapImage</code> parameter from the selected image.</p><p>Before<a class="indexterm" id="id136"/> the button for the face verification is enabled, we perform a check to see if both face IDs have been set using the following code:</p><div class="informalexample"><pre class="programlisting">    private bool CanVerifyFace(object obj)
    {
        return !_faceId1.Equals(Guid.Empty) &amp;&amp;! _faceId2.Equals(Guid.Empty);
    }</pre></div><p>The <code class="literal">VerifyFace</code> function is not a complex one, as you can see in the following code:</p><div class="informalexample"><pre class="programlisting">    private async void VerifyFace(object obj) {
        try {
            VerifyResultverificationResult = await  _faceServiceClient.VerifyAsync(_faceId1, _faceId2);</pre></div><p>With the face IDs set, we can make a call to the <code class="literal">VerifyAsync</code> function of the API. We pass on the face IDs as parameters and get a <code class="literal">VerifyResult</code> object in return. We use this object to provide the output, as follows:</p><div class="informalexample"><pre class="programlisting">            FaceVerificationResult = $"The two provided faces is identical: {verificationResult.IsIdentical}, with confidence: {verificationResult.Confidence}";
        }</pre></div><p>A successful call will return a code <code class="literal">200</code> response. The response data is a <code class="literal">bool</code> type variable, <code class="literal">isIdentical</code>, and a number, <code class="literal">confidence</code>:</p><div class="mediaobject"><img alt="Deciding whether two faces belong to the same person" src="graphics/B12373_02_05.jpg"/></div><p>At the<a class="indexterm" id="id137"/> time of writing, the <code class="literal">NuGet</code> package for the Face API only allows for face-to-face verification. If we were calling directly to the REST API, we could have utilized face-to-person verification as well.</p><p>To use face-to-person verification, only one image is required. You will need to pass on the face ID for that image. You will also need to pass on a person group ID, and a person ID. These are to specify a specific person group to search in and a certain person within that group. We will cover person groups and persons later in this chapter.</p></div><div class="section" title="Finding similar faces"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Finding similar faces</h2></div></div></div><p>Using<a class="indexterm" id="id138"/> the Face API, you can find faces similar to a provided face. The API allows for two search modes. Match person mode is the default mode. This will match faces to the same person, according to an internal same-person threshold. The other is match face mode, which will ignore the same-person threshold. This returns matches that are similar, but the similarity may be low.</p><p>In the example code provided, we have three buttons in our UI: one for generating a face list, another for adding faces to the list, and, finally, one to find similar faces. We need a textbox to specify a name for the face list. For convenience, we add a list box, outputting the persisted face IDs from the face list. We also add an image element to show the image we are checking, and a textbox outputting the result.</p><p>In the corresponding <code class="literal">ViewModel</code>, we need to add a <code class="literal">BitmapImage</code> property for the image element. We need two <code class="literal">string</code> properties: one for our face-list name and one for the API call result. To get data to our list box, we need an <code class="literal">ObservableCollection</code> property containing <code class="literal">Guids</code>. The buttons need to be hooked up to individual <code class="literal">ICommand</code> properties.</p><p>We declare two <code class="literal">private</code> variables at the start of the <code class="literal">ViewModel</code>, as shown in the following code. The first one is a <code class="literal">bool</code> variable to indicate whether or not the face list already exists. The other is used to access the Face API:</p><div class="informalexample"><pre class="programlisting">    private bool _faceListExists = false;
    private FaceServiceClient _faceServiceClient;</pre></div><p>The constructor should accept the <code class="literal">FaceServiceClient</code> parameter, which it assigns to the preceding variable. It will then call an <code class="literal">Initialize</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">    private async void Initialize()
    {
        FaceListName = "Chapter2";

        CreateFaceListCommand = new DelegateCommand(CreateFaceListAsync, CanCreateFaceList);
        FindSimilarFaceCommand = new DelegateCommand(FindSimilarFace);
        AddExampleFacesToListCommand = new DelegateCommand(AddExampleFacesToList, CanAddExampleFaces);</pre></div><p>First, we initialize the <code class="literal">FaceListName</code> property to <code class="literal">Chapter2</code>. Next, we create the command objects, specifying actions and predicates.</p><p>We finish the <code class="literal">Initialize</code> function by calling two functions, as shown in the following code. One checks whether the face list exists, while the second updates the list of face IDs:</p><div class="informalexample"><pre class="programlisting">        await DoesFaceListExistAsync();
        UpdateFaceGuidsAsync();
    }</pre></div><p>To check whether a given face list exists, we first need to get a list of all face lists. We do this by calling the <code class="literal">ListFaceListsAsync</code> method, which will return a <code class="literal">FaceListMetadata</code> array. We<a class="indexterm" id="id139"/> make sure that the result has data before we loop through the array, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private async Task DoesFaceListExistAsync()
    {
        FaceListMetadata[] faceLists = await _faceServiceClient.ListFaceListsAsync();</pre></div><p>Each <code class="literal">FaceListMetadata</code> array, from the resultant array, contains a face-list ID, a name of the face list, and user-provided data. For this example, we are just interested in the name. If the face-list name that we have specified is the name of any face list returned, we set the <code class="literal">_faceListExists</code> parameter to <code class="literal">true</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    foreach (FaceListMetadatafaceList in faceLists) {
        if (faceList.Name.Equals(FaceListName)) {
            _faceListExists = true;
            break;
        }
    }</pre></div><p>If the face list exists, we can update the list of face IDs.</p><p>To get the faces in a face list, we need to get the face list first. This is done with a call to the Face API's function, the <code class="literal">GetFaceListAsync</code> method. This requires the face-list ID to be passed as a parameter. The face-list ID needs to be in lowercase or digits, and can contain a maximum of 64 characters. For the sake of simplicity, we use the face-list name as the face ID, as follows:</p><div class="informalexample"><pre class="programlisting">    private async void UpdateFaceGuidsAsync() {
        if (!_faceListExists) return;

        try { 
            FaceListfaceList = await _faceServiceClient.GetFaceListAsync(FaceListName.ToLower());</pre></div><p>The result of this API call is a <code class="literal">FaceList</code> object, containing the face-list ID and face-list name. It also contains user-provided data and an array of persisted faces.</p><p>We check whether we have any data and then get the array of persisted faces. Looping through this array, we are able to get the <code class="literal">PersistedFaceId</code> parameter (as a <code class="literal">guid</code> variable) and user-provided data of each item. The persisted face ID is added to the <code class="literal">FaceIds ObservableCollection</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">        if (faceList == null) return;

        PersonFace[] faces = faceList.PersistedFaces;

        foreach (PersonFace face in faces) {
            FaceIds.Add(face.PersistedFaceId);
        }</pre></div><p>Finish<a class="indexterm" id="id140"/> the function by adding the corresponding <code class="literal">catch</code> clause.</p><p>If the face list does not exist and we have specified a face-list name, then we can create a new face list, as follows:</p><div class="informalexample"><pre class="programlisting">    private async void CreateFaceListAsync(object obj) {
        try {
            if (!_faceListExists) {
                await _faceServiceClient.CreateFaceListAsync (
FaceListName.ToLower(), FaceListName, string.Empty);
                await DoesFaceListExistAsync();
            }
        }</pre></div><p>First, we check to see that the face list does not exist. Using the <code class="literal">_faceServiceClient</code> parameter, you are required to pass on a face-list ID, a face-list name, and user data. As seen previously, the face-list ID needs to be lowercase characters or digits.</p><p>Using the REST API, the user parameter is optional, and as such, you would not have to provide it.</p><p>After we have created a face list, we want to ensure that it exists. We do this by a call to the previously created <code class="literal">DoesFaceListExistAsync</code> function. Add the <code class="literal">catch</code> clause to finish the function.</p><p>If the named face list exists, we can add faces to this list. Add the <code class="literal">AddExampleFacesToList</code> function. It should accept <code class="literal">object</code> as a parameter. I will leave the details of adding the images up to you. In the provided example, we get a list of images from a given directory and loop through it.</p><p>With the file path of a given image, we open the image as a <code class="literal">Stream</code>. To optimize it for our similarity operation, we find the <code class="literal">FaceRectangle</code> parameter in an image. As there should be only one face per image in the face list, we select the first element in the <code class="literal">Face</code> array, as follows:</p><div class="informalexample"><pre class="programlisting">    using (StreamfileStream = File.OpenRead(image))
    {
        Face[] faces = await _faceServiceClient.DetectAsync(fileStream);
        FaceRectanglefaceRectangle = faces[0].FaceRectangle;</pre></div><p>Adding the face to the face list is as simple as calling the <code class="literal">AddFaceToFaceListAsync</code> function. We need to specify the face-list ID and the image. The image may come from a <code class="literal">Stream</code> (as in our case) or a URL. Optionally, we can add user data and the face rectangle of the image, as follows:</p><div class="informalexample"><pre class="programlisting">AddPersistedFaceResult addFacesResult = await _faceServiceClient.AddFaceToFaceListAsync(FaceListName.ToLower(), fileStream, null, faceRectangle);
UpdateFaceGuidsAsync();</pre></div><p>The <a class="indexterm" id="id141"/>result of the API call is an <code class="literal">AddPersistedFaceResult</code> variable. This contains the persisted face ID, which is different from a face ID in the <code class="literal">DetectAsync</code> call. A face added to a face list will not expire until it is deleted.</p><p>We finish the function by calling the <code class="literal">UpdateFaceGuidsAsync</code> method.</p><p>Finally, we create our <code class="literal">FindSimilarFace</code> function, also accepting <code class="literal">object</code> as a parameter. To be able to search for similar faces, we need a face ID (the <code class="literal">Guid</code> variable) from the <code class="literal">DetectAsync</code> method. This can be called with a local image or from a URL. The example code opens a file browser and allows the user to browse for an image.</p><p>With the face ID, we can search for similar faces, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    try {
        SimilarPersistedFace[] similarFaces = await _faceServiceClient.FindSimilarAsync (findFaceGuid, FaceListName.ToLower(), 3);</pre></div><p>We call the <code class="literal">FindSimilarAsync</code> function. The first parameter is the face ID of the face we specified. The next parameter is the face-list ID, and the final parameter is the number of candidate faces returned. The default for this is 20, so it is often best to specify a number.</p><p>Instead of using a face list to find similar faces, you can use an array of the <code class="literal">Guid</code> variable. That array should contain face IDs retrieved from the <code class="literal">DetectAsync</code> method.</p><p>At the time of writing, the NuGet API package only supports match person mode. If you are using the REST API directly, you can specify the mode as a parameter.</p><p>Depending on the mode selected, the result will contain either the face ID or the persisted face ID of similar faces. It will also contain the confidence of the similarity of the given face.</p><p>To delete a face from the face list, call the following function in the Face API:</p><div class="informalexample"><pre class="programlisting">    DeleteFaceFromFaceListAsync(FACELISTID, PERSISTEDFACEID)</pre></div><p>To delete a face list, call the following function in the Face API:</p><div class="informalexample"><pre class="programlisting">    DeleteFaceListAsync(FACELISTID)</pre></div><p>To update a face list, call the following function in the Face API:</p><div class="informalexample"><pre class="programlisting">    UpdateFaceListAsync(FACELISTID, FACELISTNAME, USERDATA)</pre></div></div><div class="section" title="Grouping similar faces"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Grouping similar faces</h2></div></div></div><p>If you <a class="indexterm" id="id142"/>have several images of faces, one thing you may want to do is group the faces. Typically, you will want to group faces based on similarity, which is a feature the Face API provides.</p><p>By providing the API with a list of face IDs, it will respond with one or more groups. One group consists of faces that are similar looking. Usually, this means that the faces belong to the same person. Faces that cannot find any similar counterparts are placed in a group we'll call <code class="literal">MessyGroup</code>.</p><p>Create a new <code class="literal">View</code> called <code class="literal">FaceGroupingView.xaml</code>. The <code class="literal">View</code> should have six image elements, with corresponding titles and textboxes for face IDs. It should also have a button for our group command and a textbox to output the grouping result.</p><p>In the corresponding <code class="literal">FaceGroupingViewModel.xaml</code> <code class="literal">View</code> model, you should add the <code class="literal">BitmapImage</code> properties for all images. You should also add the <code class="literal">string</code> properties for the face IDs and one for the result. There is also a need for an <code class="literal">ICommand</code> property.</p><p>At the start of the <code class="literal">ViewModel</code>, we declare some <code class="literal">private</code> variables, as follows:</p><div class="informalexample"><pre class="programlisting">    private FaceServiceClient _faceServiceClient;
    private List&lt;string&gt; _imageFiles = new List&lt;string&gt;();
    private List&lt;Guid&gt; _faceIds = new List&lt;Guid&gt;();</pre></div><p>The first one is used to access the Face API. The second one contains a list of strings that in turn contain the location of our images. The last list contains the detected face IDs.</p><p>The constructor accepts a parameter of the <code class="literal">FaceServiceClient</code> type. It assigns it to the corresponding variable and calls the <code class="literal">Initialize</code> function. This creates our <code class="literal">ICommand</code> object and calls a function to add our images to the application.</p><p>In the function that adds images, we add hardcoded image paths to our <code class="literal">_imageFiles</code> list. For this example, we add six. Using a <code class="literal">for</code> loop, we generate each <code class="literal">BitmapImage</code> property. When we have an image, we want to detect faces in it:</p><div class="informalexample"><pre class="programlisting">    try {
        using (Stream fileStream = File.OpenRead(_imageFiles[i])) {
            Face[] faces = await
            _faceServiceClient.DetectAsync(fileStream);</pre></div><p>We do not need any more data than the generated face ID, which we know is stored for 24 hours after detection:</p><div class="informalexample"><pre class="programlisting">            _faceIds.Add(faces[0].FaceId);
            CreateImageSources(image, i, faces[0].FaceId);
        }
    }</pre></div><p>Assuming that there is only one face per image, we add that face ID to our <code class="literal">_faceIds</code> list. The image, face ID, and current iteration number in the loop are passed on to a new function, <code class="literal">CreateImageSources</code>. This function contains a <code class="literal">switch</code> case based on the iteration number. Based on the number, we assign the image and face ID to the corresponding image and image ID property. This is then shown in the UI.</p><p>We have a <a class="indexterm" id="id143"/>button to group the images. To group the images, we call the Face API's <code class="literal">GroupAsync</code> method, passing on an array of face IDs, as shown in the following code. The array of face IDs must contain at least two elements, and it cannot contain more than 1,000 elements:</p><div class="informalexample"><pre class="programlisting">    private async void GroupFaces(object obj) {
        try {
            GroupResultfaceGroups = await _faceServiceClient.GroupAsync(_faceIds.ToArray());</pre></div><p>The response is a <code class="literal">GroupResult</code> type, which may contain one or more groups, as well as the messy group. We check to see whether there is a response and then we parse it, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">            if (faceGroups != null)
                FaceGroupingResult = ParseGroupResult(faceGroups);
        }</pre></div><p>Before looking at the <code class="literal">ParseGroupResult</code> method, add the corresponding <code class="literal">catch</code> clause and close-up <code class="literal">GroupFaces</code> function.</p><p>When parsing the results, we first create a <code class="literal">StringBuilder</code> class to hold our text. Then we get the <code class="literal">groups</code> from the result. A group is an array of face IDs of the images in that group. All groups are stored in a list, and we append the number of groups to the <code class="literal">StringBuilder</code> class, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">private string ParseGroupResult(GroupResultfaceGroups) {
   StringBuilder result = new StringBuilder();
   List&lt;Guid[]&gt;groups = faceGroups.Groups;
   result.AppendFormat("There are {0} group(s)\n", groups.Count);</pre></div><p>We loop through the list of groups. Inside this loop, we loop through each item in the group. For the sake of readability, we have a helper function to find the image name from the ID. It finds the index in our <code class="literal">_faceIds</code> list. This is then used in the image name, so if the index is <code class="literal">2</code>, the image name would be <code class="literal">Image 3</code>. For this to give the intended effect, you must have placed the images in a logical order, as follows:</p><div class="informalexample"><pre class="programlisting">            result.Append("Groups:\t");

            foreach(Guid[] guid in groups)
            {
                foreach(Guid id in guid)
                {
                    result.AppendFormat("{0} - ", GetImageName(id));
                }
                result.Append("\n");
            }</pre></div><p>The <code class="literal">GroupResult</code> method may also contain a <code class="literal">MessyGroup</code> array. This is an array of <code class="literal">Guid</code> variables <a class="indexterm" id="id144"/>containing the face IDs in that group. We loop through this array and append the image name, the same way we did with the regular groups, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">            result.Append("Messy group:\t");

            Guid[] messyGroup = faceGroups.MessyGroup;
            foreach(Guidguid in messyGroup)
            {
                result.AppendFormat("{0} - ", GetImageName(guid));
            }</pre></div><p>We end the function by returning the <code class="literal">StringBuilder</code> function's text, which will output it to the screen, as follows:</p><div class="informalexample"><pre class="programlisting">            return result.ToString();
        }</pre></div><p>Make sure that the <code class="literal">ViewModel</code> instances have been created in the <code class="literal">MainViewModel.cs</code> file. Also, make sure that the <code class="literal">View</code> has been added as a <code class="literal">TabItem</code> property in the <code class="literal">MainView.xaml</code> file. Compile and test the application.</p><p>If you are using the sample images provided, you may end up with something like the following:</p><div class="mediaobject"><img alt="Grouping similar faces" src="graphics/B12373_02_06.jpg"/></div></div></div>
<div class="section" title="Adding identification to our smart-house application"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Adding identification to our smart-house application</h1></div></div></div><p>As a<a class="indexterm" id="id145"/> part of our smart-house application, we want the application to recognize who we are. Doing so opens up the opportunity to get responses and actions from the application, tailored to you.</p><div class="section" title="Creating our smart-house application"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Creating our smart-house application</h2></div></div></div><p>Create a<a class="indexterm" id="id146"/> new project for the smart-house application, based on the MVVM template we created earlier.</p><p>With the new project created, add the <code class="literal">Microsoft.ProjectOxford.Face</code> NuGet package.</p><p>As we will be building this application throughout this book, we will start small. In the <code class="literal">MainView.xaml</code> file, add a <code class="literal">TabControl</code> property containing two items. The two items should be two user controls, one called the <code class="literal">AdministrationView.xaml</code> file and the other called the <code class="literal">HomeView.xaml</code> file.</p><p>The administration control will be where we administer different parts of the application. The home control will be the starting point and the main control to use.</p><p>Add corresponding <code class="literal">ViewModel</code> instances to the <code class="literal">Views</code>. Make sure they are declared and created in <code class="literal">MainViewModel.cs</code>, as we have seen throughout this chapter. Make sure that the application compiles and runs before moving on.</p></div><div class="section" title="Adding people to be identified"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Adding people to be identified</h2></div></div></div><p>Before we<a class="indexterm" id="id147"/> can go on to identify a person, we need to have something to identify them from. To identify a person, we need a <code class="literal">PersonGroup</code> property. This is<a class="indexterm" id="id148"/> a group that contains several <code class="literal">Persons</code> properties.</p><div class="section" title="Creating a view"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec26"/>Creating a view</h3></div></div></div><p>In the <a class="indexterm" id="id149"/>administration control, we will execute several operations in this regard. The UI should contain two textbox elements, two list box elements, and six buttons. The two textbox elements will allow us to input a name for the person group and a name for the person. One list box will list all person groups that we have available. The other will list all the persons in any given group.</p><p>We have buttons for each of the operations that we want to execute, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Add person group</li><li class="listitem" style="list-style-type: disc">Delete person group</li><li class="listitem" style="list-style-type: disc">Train person group</li><li class="listitem" style="list-style-type: disc">Add person</li><li class="listitem" style="list-style-type: disc">Delete person</li><li class="listitem" style="list-style-type: disc">Add person face</li></ul></div><p>The <code class="literal">View</code> model <a class="indexterm" id="id150"/>should have two <code class="literal">ObservableCollection</code> properties: one of a <code class="literal">PersonGroup</code> type and the other of a <code class="literal">Person</code> type. We should also add three <code class="literal">string</code> properties. One will be for our person group name, the other for our person name. The last will hold some status text. We also want a <code class="literal">PersonGroup</code> property for the selected person group. Finally, we want a <code class="literal">Person</code> property holding the selected person.</p><p>In our <code class="literal">View</code> model, we want to add a <code class="literal">private</code> variable for the <code class="literal">FaceServiceClient</code> method, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private FaceServiceClient _faceServiceClient;</pre></div><p>This should be assigned in the constructor, which should accept a parameter of a <code class="literal">FaceServiceClient</code> type. It should also call an initialization function, which will initialize six <code class="literal">ICommand</code> properties. These maps to the buttons, created earlier. The initialization function should call the <code class="literal">GetPersonGroups</code> function to list all person groups available, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private async void GetPersonGroups() {
        try {
            PersonGroup[] personGroups = await
            _faceServiceClient.ListPersonGroupsAsync();</pre></div><p>The <code class="literal">ListPersonGroupsAsync</code> function does not take any parameters, and returns a <code class="literal">PersonGroup</code> array if successfully executed, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">            if(personGroups == null || personGroups.Length == 0)
            {
                StatusText = "No person groups found.";
                return;
            }

            PersonGroups.Clear();

            foreach (PersonGrouppersonGroup in personGroups)
            {
                PersonGroups.Add(personGroup); 
            }
        }</pre></div><p>We then check to see whether the array contains any elements. If it does, we clear out the existing <code class="literal">PersonGroups</code> list. Then we loop through each item of the <code class="literal">PersonGroup</code> array and add them to the <code class="literal">PersonGroups</code> list.</p><p>If no <a class="indexterm" id="id151"/>person groups exist, we can add a new one by filling in a name. The name you fill in here will also be used as a person group ID. This means that it can include numbers and English lowercase letters, the "-" character (hyphen), and the "_" character (underscore). The maximum length is 64 characters. When it is filled in, we can add a person group.</p></div><div class="section" title="Adding person groups"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec27"/>Adding person groups</h3></div></div></div><p>First, we <a class="indexterm" id="id152"/>call the <code class="literal">DoesPersonGroupExistAsync</code> function, specifying <code class="literal">PersonGroupName</code> as a parameter, as shown in the following code. If this is <code class="literal">true</code>, then the name we have given already exists, and as such, we are not allowed to add it. Note how we call the <code class="literal">ToLower</code> function on the name. This is so we are sure that the ID is in lowercase:</p><div class="informalexample"><pre class="programlisting">    private async void AddPersonGroup(object obj) {
        try {
            if(await DoesPersonGroupExistAsync(PersonGroupName.ToLower())) {
                StatusText = $"Person group {PersonGroupName} already exist";
                return;
            }</pre></div><p>If the person group does not exist, we call the <code class="literal">CreatePersonGroupAsync</code> function, as shown in the following code. Again, we specify the <code class="literal">PersonGroupName</code> as lowercase in the first parameter. This represents the ID of the group. The second parameter indicates the name we want. We end the function by calling the <code class="literal">GetPersonGroups</code> function again, so we get the newly added group in our list:</p><div class="informalexample"><pre class="programlisting">            await _faceServiceClient.CreatePersonGroupAsync (PersonGroupName.ToLower(), PersonGroupName);
            StatusText = $"Person group {PersonGroupName} added";
            GetPersonGroups();
        }</pre></div><p>The <code class="literal">DoesPersonGroupExistAsync</code> function makes one API call. It tries to call the <code class="literal">GetPersonGroupAsync</code> function, with the person group ID specified as a parameter. If the resultant <code class="literal">PersonGroup</code> list is anything but <code class="literal">null</code>, we return <code class="literal">true</code>.</p><p>To delete a person group, a group must be selected as follows:</p><div class="informalexample"><pre class="programlisting">    private async void DeletePersonGroup(object obj)
    {
        try
        {
            await _faceServiceClient.DeletePersonGroupAsync (SelectedPersonGroup.PersonGroupId);
            StatusText = $"Deleted person group {SelectedPersonGroup.Name}";

            GetPersonGroups();
        }</pre></div><p>The API <a class="indexterm" id="id153"/>call to the <code class="literal">DeletePersonGroupAsync</code> function requires a person group ID as a parameter. We get this from the selected person group. If no exception is caught, then the call has completed successfully, and we call the <code class="literal">GetPersonGroups</code> function to update our list.</p><p>When a person group is selected from the list, we make sure that we call the <code class="literal">GetPersons</code> function. This will update the list of persons, as follows:</p><div class="informalexample"><pre class="programlisting">    private async void GetPersons()
    {
        if (SelectedPersonGroup == null)
            return;

        Persons.Clear();

        try
        {
            Person[] persons = await _faceServiceClient.GetPersonsAsync(SelectedPersonGroup.PersonGroupId);</pre></div><p>We make sure the selected person group is not <code class="literal">null</code>. If it is not, we clear our <code class="literal">persons</code> list. The API call to the <code class="literal">GetPersonsAsync</code> function requires a person group ID as a parameter. A successful call will result in a <code class="literal">Person</code> array.</p><p>If the resultant array contains any elements, we loop through it. Each <code class="literal">Person</code> object is added to our <code class="literal">persons</code> list, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">            if (persons == null || persons.Length == 0)
            {
                StatusText = $"No persons found in {SelectedPersonGroup.Name}.";
                return;
            }

            foreach (Person person in persons)
            {
                Persons.Add(person);
            }
        }</pre></div></div><div class="section" title="Adding new persons"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec28"/>Adding new persons</h3></div></div></div><p>If no <a class="indexterm" id="id154"/>persons exist, we can add new ones. To add a new one, a person group must be selected, and a name of the person must be filled in. With this in place, we can click on the <span class="strong"><strong>Add</strong></span> button:</p><div class="informalexample"><pre class="programlisting">    private async void AddPerson(object obj)
    {
        try
        {
            CreatePersonResultpersonId = await _faceServiceClient.CreatePersonAsync(SelectedPersonGroup.PersonGroupId, PersonName);
            StatusText = $"Added person {PersonName} got ID: {personId.PersonId.ToString()}";
               
            GetPersons();
        }</pre></div><p>The API call to the <code class="literal">CreatePersonAsync</code> function requires a person group ID as the first parameter. The next parameter is the name of the person. Optionally, we can add user data as a third parameter. In this case, it should be a string. When a new person has been created, we update the <code class="literal">persons</code> list by calling the <code class="literal">GetPersons</code> function again.</p><p>If we have selected a person group and a person, then we will be able to delete that person, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private async void DeletePerson(object obj)
    {
        try
        {
            await _faceServiceClient.DeletePersonAsync (SelectedPersonGroup.PersonGroupId, SelectedPerson.PersonId);

            StatusText = $"Deleted {SelectedPerson.Name} from {SelectedPersonGroup.Name}";

            GetPersons();
        }</pre></div><p>To delete a person, we make a call to the <code class="literal">DeletePersonAsync</code> function. This requires the person group ID of the person group the person lives in. It also requires the ID of the person we want to delete. If no exceptions are caught, then the call succeeded, and we call the <code class="literal">GetPersons</code> function to update our person list.</p><p>Our administration control now looks similar to the following screenshot:</p><div class="mediaobject"><img alt="Adding new persons" src="graphics/B12373_02_07.jpg"/></div></div><div class="section" title="Associating faces with a person"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec29"/>Associating faces with a person</h3></div></div></div><p>Before <a class="indexterm" id="id155"/>we can identify a person, we need to associate faces with that person. With a given person group and person selected, we can add faces. To do so, we open a file dialog. When we have an image file, we can add the face to the person, as follows:</p><div class="informalexample"><pre class="programlisting">        using (StreamimageFile = File.OpenRead(filePath))
        {
            AddPersistedFaceResultaddFaceResult = await _faceServiceClient.AddPersonFaceAsync(
            SelectedPersonGroup.PersonGroupId,
            SelectedPerson.PersonId, imageFile);

            if (addFaceResult != null)
            {
                StatusText = $"Face added for {SelectedPerson.Name}. Remember to train the person group!";
            }
        }</pre></div><p>We open the image file as a <code class="literal">Stream</code>. This file is passed on as the third parameter in our call to the <code class="literal">AddPersonFaceAsync</code> function. Instead of a stream, we could have passed a URL to an image.</p><p>The first <a class="indexterm" id="id156"/>parameter in the call is the person group ID of the group in which the person lives. The next parameter is the person ID.</p><p>Some optional parameters to include are user data in the form of a string and a <code class="literal">FaceRectangle</code> parameter for the image. The <code class="literal">FaceRectangle</code> parameter is required if there is more than one face in the image.</p><p>A successful call will result in an <code class="literal">AddPersistedFaceResult</code> object. This contains the persisted face ID for the person.</p><p>Each person can have a maximum of 248 faces associated with it. The more faces you can add, the more likely it is that you will receive a solid identification later. The faces that you add should from slightly different angles.</p></div><div class="section" title="Training the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec30"/>Training the model</h3></div></div></div><p>With<a class="indexterm" id="id157"/> enough faces associated with the persons, we need to train the person group. This is a task that is required after any change to a person or person group.</p><p>We can train a person group when one has been selected, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private async void TrainPersonGroup(object obj)
    {
        try
        {
            await _faceServiceClient.TrainPersonGroupAsync(
SelectedPersonGroup.PersonGroupId);</pre></div><p>The call to the <code class="literal">TrainPersonGroupAsync</code> function takes a person group ID as a parameter, as shown in the following code. It does not return anything, and it may take a while to execute:</p><div class="informalexample"><pre class="programlisting">            while(true)
            {
                TrainingStatustrainingStatus = await _faceServiceClient.GetPersonGroupTrainingStatusAsync (SelectedPersonGroup.PersonGroupId);</pre></div><p>We want to ensure that the training completed successfully. To do so, we call the <code class="literal">GetPersonGroupTrainingStatusAsync</code> function inside a <code class="literal">while</code> loop. This call requires a person group ID, and a successful call results in a <code class="literal">TrainingStatus</code> object, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">                if(trainingStatus.Status != Status.Running)
                {
                    StatusText = $"Person group finished with status: {trainingStatus.Status}";
                    break;
                }

                StatusText = "Training person group...";
                await Task.Delay(1000);
            }
        }</pre></div><p>We <a class="indexterm" id="id158"/>check the status and we show the result if it is not running. If the training is still running, we wait for one second and run the check again.</p><p>When the training has succeeded, we are ready to identify people.</p></div><div class="section" title="Additional functionality"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec31"/>Additional functionality</h3></div></div></div><p>There <a class="indexterm" id="id159"/>are a few API calls that we have not looked at, which will be mentioned briefly in the following bullet list:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To update a person group, call the following; this function does not return anything:<div class="informalexample"><pre class="programlisting">        UpdatePersonGroupAsync(PERSONGROUPID, NEWNAME, USERDATA)</pre></div></li><li class="listitem" style="list-style-type: disc">To get a person's face, call the following:<div class="informalexample"><pre class="programlisting">        GetPersonFaceAsync(PERSONGROUPID, PERSONID, PERSISTEDFACEID)</pre></div><p>A successful call returns the persisted face ID and user-provided data.</p></li><li class="listitem" style="list-style-type: disc">To delete a person's face, call the following; this call does not return anything:<div class="informalexample"><pre class="programlisting">        DeletePersonFaceAsync(PERSONGROUPID, PERSONID, PERSISTEDFACeID)</pre></div></li><li class="listitem" style="list-style-type: disc">To update a person, call the following; this call does not return anything:<div class="informalexample"><pre class="programlisting">        UpdatePersonAsync(PERSONGROUPID, PERSONID, NEWNAME, USERDATA)</pre></div></li><li class="listitem" style="list-style-type: disc">To update a person's face, call the following; this call does not return anything:<div class="informalexample"><pre class="programlisting">        UpdatePersonFaceAsync(PERSONGROUID, PERSONID, PERSISTEDFACEID, USERDATA)</pre></div></li></ul></div></div></div><div class="section" title="Identifying a person"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Identifying a person</h2></div></div></div><p>To <a class="indexterm" id="id160"/>identify a person, we are first going to upload an image. Open the <code class="literal">HomeView.xaml</code> file and add a <code class="literal">ListBox</code> element to the UI. This will contain the person groups to choose from when identifying a person. We will need to add a button element to find an image, upload it, and identify the person. A <code class="literal">TextBox</code> element is added to show the working response. For our own convenience, we also add an image element to show the image we are using.</p><p>In the <code class="literal">View</code> model, add an <code class="literal">ObservableCollection</code> property of a <code class="literal">PersonGroup</code> type. We need to add a property for the selected <code class="literal">PersonGroup</code> type. Also, add a <code class="literal">BitmapImage</code> property for our image, and a string property for the response. We will also need an <code class="literal">ICommand</code> property for our button.</p><p>Add a <code class="literal">private</code> variable for the <code class="literal">FaceServiceClient</code> type, as follows:</p><div class="informalexample"><pre class="programlisting">    private FaceServiceClient _faceServiceClient;</pre></div><p>This will be assigned in our constructor, which should accept a parameter of a <code class="literal">FaceServiceClient</code> type. From the constructor, call on the <code class="literal">Initialize</code> function to initialize everything, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private void Initialize()
    {
        GetPersonGroups();
        UploadOwnerImageCommand = new DelegateCommand(UploadOwnerImage,CanUploadOwnerImage);
    }</pre></div><p>First, we call the <code class="literal">GetPersonGroups</code> function to retrieve all the person groups. This function makes a call to the <code class="literal">ListPersonGroupsAsync</code> API, which we saw earlier. The result is added to our <code class="literal">PersonGroup</code> list's <code class="literal">ObservableCollection</code> parameter.</p><p>Next, we create our <code class="literal">ICommand</code> object. The <code class="literal">CanUploadOwnerImage</code> function will return <code class="literal">true</code> if we have selected an item from the <code class="literal">PersonGroup</code> list. If we have not, it will return <code class="literal">false</code>, and we will not be able to identify anyone.</p><p>In the <code class="literal">UploadOwnerImage</code> function, we first browse to an image and then load it. With an image loaded and a file path available, we can start to identify the person in the image, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    using (StreamimageFile = File.OpenRead(filePath))
    {
        Face[] faces = await _faceServiceClient.DetectAsync(imageFile);
        Guid[] faceIds = faces.Select(face =&gt;face.FaceId).ToArray();</pre></div><p>We open the image as a <code class="literal">Stream</code> type, as shown in the following code. Using this, we detect faces in the image. From the detected faces, we get all the face IDs in an array:</p><div class="informalexample"><pre class="programlisting">        IdentifyResult[] personsIdentified = await _faceServiceClient.IdentifyAsync (SelectedPersonGroup.PersonGroupId,
faceIds, 1);</pre></div><p>The <a class="indexterm" id="id161"/>array of face IDs will be sent as the second parameter to the <code class="literal">IdentifyAsync</code> API call. Remember that when we detect a face, it is stored for 24 hours. Proceeding to use the corresponding face ID will make sure that the service knows which face to use for identification.</p><p>The first parameter used is the ID of the person group we have selected. The last parameter in the call is the number of candidates returned. As we do not want to identify more than one person at a time, we specify one. Because of this, we should ensure that there is only one face in the image we upload.</p><p>A successful API call will result in an array of the <code class="literal">IdentifyResult</code> parameter, as shown in the following code. Each item in this array will contain candidates:</p><div class="informalexample"><pre class="programlisting">    foreach(IdentifyResultpersonIdentified in personsIdentified) { 
        if(personIdentified.Candidates.Length == 0) {
            SystemResponse = "Failed to identify you.";
            break;
        }
        GuidpersonId = personIdentified.Candidates[0].PersonId;</pre></div><p>We loop through the array of results, as shown in the following code. If we do not have any candidates, we just break out of the loop. If, however, we do have candidates, we get the <code class="literal">PersonId</code> parameter of the first candidate (we asked for only one candidate earlier, so this is okay):</p><div class="informalexample"><pre class="programlisting">        Person person = await faceServiceClient.GetPersonAsync(
SelectedPersonGroup.PersonGroupId, personId);

        if(person != null) {
            SystemResponse = $"Welcome home, {person.Name}";
            break;
        }
    }
}</pre></div><p>With the <code class="literal">personId</code> parameter, we get a single <code class="literal">Person</code> object, using the API to call the <code class="literal">GetPersonAsync</code> function. If the call is successful, we print a welcome message to the correct person (as shown in the following screenshot) and break out of the loop:</p><div class="mediaobject"><img alt="Identifying a person" src="graphics/B12373_02_08.jpg"/></div></div></div>
<div class="section" title="Knowing your mood using the Face API"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Knowing your mood using the Face API</h1></div></div></div><p>The Face API <a class="indexterm" id="id162"/>allows you to recognize emotions from faces.</p><p>Research has shown that there are some key emotions that can be classified as cross-cultural. These are happiness, sadness, surprise, anger, fear, contempt, disgust, and neutral. All <a class="indexterm" id="id163"/>of these are detected by the API, which allows your applications to respond in a more personalized way by knowing the user's mood.</p><p>We will <a class="indexterm" id="id164"/>learn how to recognize emotions from images so that our smart-house application can know our mood.</p><div class="section" title="Getting images from a web camera"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Getting images from a web camera</h2></div></div></div><p>Imagine <a class="indexterm" id="id165"/>that there are several cameras around your house. The smart-house application can see what your mood is at any time. By knowing this, it can utilize the mood to better predict your needs.</p><p>We are going to add web-camera capabilities to our application. If you do not have a web camera, you can follow along, but load images using the techniques we have already seen.</p><p>First we need to add a NuGet package to <a class="indexterm" id="id166"/>our smart-house application. Search for <code class="literal">OpenCvSharp3-AnyCPU</code> and install the package by <span class="strong"><strong>shimat</strong></span>. This is a package that allows for the processing of images, and is utilized by the next dependency we are going to add.</p><p>In the example code provided, there is a project called <code class="literal">VideoFrameAnalyzer</code>. This is a project written by Microsoft that allows us to grab frame-by-frame images from a web camera. Using this, we are able to analyze emotions in our application. The use case we will execute is as follows:</p><div class="mediaobject"><img alt="Getting images from a web camera" src="graphics/B12373_02_09.jpg"/></div><p>In our <code class="literal">HomeView.xaml</code> file, add two new buttons. One will be to start the web camera while the other will be to stop it.</p><p>In the <a class="indexterm" id="id167"/>corresponding <code class="literal">View</code> model, add two <code class="literal">ICommand</code> properties for each of the buttons. Also add the following <code class="literal">private</code> members:</p><div class="informalexample"><pre class="programlisting">    private FrameGrabber&lt;CameraResult&gt; _frameGrabber;
    private static readonly ImageEncodingParam[] s_jpegParams = {
        new ImageEncodingParam(ImwriteFlags.JpegQuality, 60)
    };</pre></div><p>The first one is a <code class="literal">FrameGrabber</code> object, which is from the <code class="literal">VideoFrameAnalyzer</code> project. The <code class="literal">static</code> member is an array of parameters for images, and is used when fetching web camera images. Additionally, we need to add a <code class="literal">CameraResult</code> class, which should be within the <code class="literal">ViewModel</code> file.</p><p>We initialize the <code class="literal">EmotionScores</code> to <code class="literal">null</code>, as shown in the following code. This is done so that new emotion scores always will be assigned from the most resent analysis result:</p><div class="informalexample"><pre class="programlisting">    internal class CameraResult {
        public EmotionScores EmotionScores { get; set; } = null;
    }</pre></div><p>Add an initialization of the <code class="literal">_frameGrabber</code> member in the constructor and add the following in the <code class="literal">Initialization</code> function:</p><div class="informalexample"><pre class="programlisting">    _frameGrabber.NewFrameProvided += OnNewFrameProvided;</pre></div><p>Each time a new frame is provided from the camera, an event is raised.</p><p>When we receive new frames, we want to create a <code class="literal">BitmapImage</code> from it to show it in the UI. To do so requires us to invoke the action from the current dispatcher, as the event is triggered from a background thread, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">    private void OnNewFrameProvided(object sender, FrameGrabber&lt;CameraResult&gt;.NewFrameEventArgs e) {          
        Application.Current.Dispatcher.Invoke(() =&gt; {
            BitmapSource bitmapSource = e.Frame.Image.ToBitmapSource();

            JpegBitmapEncoder encoder = new JpegBitmapEncoder();
            MemoryStream memoryStream = new MemoryStream();
            BitmapImage image = new BitmapImage();</pre></div><p>We get the <code class="literal">BitmapSource</code> of the <code class="literal">Frame</code> and create some required variables.</p><p>Using the <code class="literal">encoder</code> we created, we add the <code class="literal">bitmapSource</code> and save it to the <code class="literal">memoryStream</code>, as follows:</p><div class="informalexample"><pre class="programlisting">    encoder.Frames.Add(BitmapFrame.Create(bitmapSource));
    encoder.Save(memoryStream);</pre></div><p>This <code class="literal">memoryStream</code> is then assigned to the <code class="literal">BitmapImage</code> we created, as shown in the following code. This <a class="indexterm" id="id168"/>is in turn assigned to the <code class="literal">ImageSource</code>, which will show the frame in the UI:</p><div class="informalexample"><pre class="programlisting">    memoryStream.Position = 0;
    image.BeginInit(); 
    image.CacheOption = BitmapCacheOption.OnLoad;
    image.StreamSource = memoryStream;
    image.EndInit();

    memoryStream.Close(); 
    ImageSource = image;</pre></div><p>As this event will be triggered a lot, we will get a fluent stream in the UI, and it will seem like it is a direct video feed.</p><p>In our <code class="literal">Initialization</code> function, we will also need to create our <code class="literal">ICommand</code> for the buttons, as follows:</p><div class="informalexample"><pre class="programlisting">    StopCameraCommand = new DelegateCommand(StopCamera);
    StartCameraCommand = new DelegateCommand(StartCamera, CanStartCamera);</pre></div><p>To be able to start the camera, we need to have selected a person group, and we need to have at least one camera available:</p><div class="informalexample"><pre class="programlisting">    private bool CanStartCamera(object obj) {
        return _frameGrabber.GetNumCameras() &gt; 0 &amp;&amp; SelectedPersonGroup != null;
    }</pre></div><p>To start a camera, we need to specify which camera to use and how often we want to trigger an analysis using the following code:</p><div class="informalexample"><pre class="programlisting">    private async void StartCamera(object obj) {
        _frameGrabber.TriggerAnalysisOnInterval(TimeSpan.FromSeconds(5));
        await _frameGrabber.StartProcessingCameraAsync();
    }</pre></div><p>If no camera is specified in <code class="literal">StartProcessingCameraAsync</code>, the first one available is chosen by default.</p><p>We will get back to the analysis part of this process soon.</p><p>To stop the camera, we run the following command:</p><div class="informalexample"><pre class="programlisting">    private async void StopCamera(object obj) {
        await _frameGrabber.StopProcessingAsync();
    }</pre></div></div><div class="section" title="Letting the smart house know your mood"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Letting the smart house know your mood</h2></div></div></div><p>We<a class="indexterm" id="id169"/> now have a video from the web camera available for our use.</p><p>In the <code class="literal">FrameGrabber</code> class, there is a <code class="literal">Func</code>, which will be used for analysis functions. We <a class="indexterm" id="id170"/>need to create the function that will be passed on this that will enable emotions to be recognized.</p><p>Create a new function, <code class="literal">EmotionAnalysisAsync</code>, that accepts a <code class="literal">VideoFrame</code> as a parameter. The return type should be <code class="literal">Task&lt;CameraResult&gt;</code> and the function should be marked as <code class="literal">async</code>.</p><p>The <code class="literal">frame</code> we get as a parameter is used to create a <code class="literal">MemoryStream</code> containing the current frame. This will be in the JPG file format. We will find a face in this image, and we want to ensure that we specify that we want emotion attributes using the following code:</p><div class="informalexample"><pre class="programlisting">private async Task&lt;CameraResult&gt; EmotionAnalysisAsync (VideoFrame frame) {
   MemoryStream jpg = frame.Image.ToMemoryStream(".jpg", s_jpegParams); 
   try {
      Face[] face = await _faceServiceClient.DetectAsync(jpg, true, false, new List&lt;FaceAttributeType&gt;
         { FaceAttributeType.Emotion });
      EmotionScores emotions = face.First()?.FaceAttributes?.Emotion;</pre></div><p>A successful call will result in an object containing all the emotion scores, as shown in the following code. The scores are what we want to return:</p><div class="informalexample"><pre class="programlisting">    return new CameraResult {
        EmotionScores = emotions
    };</pre></div><p>Catch any exceptions that may be thrown, returning <code class="literal">null</code> when they are.</p><p>We need to assign the <code class="literal">Initialize</code> function to the <code class="literal">Func</code>. We also need to add an event handler each time we have a new result.</p><p>When a new result is obtained, we grab the <code class="literal">EmotionScore</code> that is received, as shown in the following code. If it is <code class="literal">null</code> or does not contain any elements, then we do not want to do anything else:</p><div class="informalexample"><pre class="programlisting">    _frameGrabber.NewResultAvailable += OnResultAvailable;
    _frameGrabber.AnalysisFunction = EmotionAnalysisAsync;
    private void OnResultAvailable(object sender, FrameGrabber&lt;CameraResult&gt;.NewResultEventArgs e)
    {
        var analysisResult = e.Analysis.EmotionScores; 
        if (analysisResult == null)
            return;</pre></div><p>In the <a class="indexterm" id="id171"/>following code, we parse the emotion scores in <code class="literal">AnalyseEmotions</code>, which we will look at in a bit:</p><div class="informalexample"><pre class="programlisting">        string emotion = AnalyseEmotions(analysisResult);

        Application.Current.Dispatcher.Invoke(() =&gt; {
            SystemResponse = $"You seem to be {emotion} today.";
        });
    }</pre></div><p>Using the result from <code class="literal">AnalyseEmotions</code>, we print a string to the result to indicate the current mood. This will need to be invoked from the current dispatcher, as the event has been triggered in another thread.</p><p>To get <a class="indexterm" id="id172"/>the current mood in a readable format, we parse the emotion scores in <code class="literal">AnalyseEmotions</code> as follows:</p><div class="informalexample"><pre class="programlisting">    private string AnalyseEmotions(Scores analysisResult) {
        string emotion = string.Empty;
        var sortedEmotions = analysisResult.ToRankedList();
        string currentEmotion = sortedEmotions.First().Key;</pre></div><p>With the <code class="literal">Scores</code> we get, we call a <code class="literal">ToRankedList</code> function. This will return a list of <code class="literal">KeyValuePair</code>, containing each emotion, along with the corresponding confidence. The first one will be the most likely, the second will be the second most likely, and so on. We only care about the most likely one, so we select it.</p><p>With the top emotion score selected, we use a <code class="literal">switch</code> statement to find the correct emotion. This is returned and printed to the result, as follows:</p><div class="informalexample"><pre class="programlisting">        switch(currentEmotion)
        { 
            case "Anger":
                emotion = "angry";
                break;
            case "Contempt":
                emotion = "contempt";
                break;
            case "Disgust":
                emotion = "disgusted";
                break;
            case "Fear":
                emotion = "scared";
                break;
            case "Happiness":
                emotion = "happy";
                break;
            case "Neutral":
                default:
                emotion = "neutral";
                break;
            case "Sadness":
                emotion = "sad";
                break;
            case "Suprise":
                emotion = "suprised";
                break;
            }
            return emotion;
        }</pre></div><p>The last piece of the puzzle is to make sure that the analysis is being executed at a specified interval. In the <code class="literal">StartCamera</code> function, add the following line, just before calling <code class="literal">StartProcessingCamera</code>:</p><div class="informalexample"><pre class="programlisting">    _frameGrabber.TriggerAnalysisOnInterval(TimeSpan.FromSeconds(5));</pre></div><p>This will<a class="indexterm" id="id173"/> trigger an emotion analysis to be called every fifth second.</p><p>When I <a class="indexterm" id="id174"/>have a smile on my face, the application now knows that I am happy and can provide further interaction accordingly. If we compile and run the example, we should get results like those shown in the following screenshots:</p><div class="mediaobject"><img alt="Letting the smart house know your mood" src="graphics/B12373_02_10.jpg"/></div><p>As my<a class="indexterm" id="id175"/> mood changes to neutral, the application detects this as well:</p><div class="mediaobject"><img alt="Letting the smart house know your mood" src="graphics/B12373_02_11.jpg"/></div></div></div>
<div class="section" title="Automatically moderating user content"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Automatically moderating user content</h1></div></div></div><p>Using the content <a class="indexterm" id="id176"/>moderator API, we can add monitoring to<a class="indexterm" id="id177"/> user-generated content. The API is created to assist with flags and to assess and filter offensive and unwanted content.</p><div class="section" title="Types of content moderation APIs"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Types of content moderation APIs</h2></div></div></div><p>We will <a class="indexterm" id="id178"/>quickly go through the key features of the moderation APIs in this section.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>A reference to <a class="indexterm" id="id179"/>the documentation for all APIs can be found at <a class="ulink" href="https://docs.microsoft.com/nb-no/azure/cognitive-services/content-moderator/api-reference">https://docs.microsoft.com/nb-no/azure/cognitive-services/content-moderator/api-reference</a>.</p></div></div><div class="section" title="Image moderation"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec32"/>Image moderation</h3></div></div></div><p>The<a class="indexterm" id="id180"/> image moderation API allows <a class="indexterm" id="id181"/>you to moderate images for adult and inappropriate content. It can also extract textual content and detect faces in images.</p><p>When using <a class="indexterm" id="id182"/>the API to evaluate inappropriate content, the API will take an image as input. Based on the image, it will return a Boolean value, indicating whether the image is appropriate or not. It will also contain a corresponding confidence score between 0 and 1. The Boolean value is set based on a set of default thresholds.</p><p>If the image contains any text, the API will use OCR to extract the text. It will then look for the same adult or racy content as text moderation, which we will get to shortly.</p><p>Some content-based applications may not want to display any personally identifiable information, in which case it can be wise to detect faces in images. Based on the information retrieved in the face-detection evaluation, you can ensure that no user content contains images of people.</p></div><div class="section" title="Text moderation"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec33"/>Text moderation</h3></div></div></div><p>Using<a class="indexterm" id="id183"/> the text moderation API, you can <a class="indexterm" id="id184"/>screen text against custom and shared lists of text. It is able to detect personally identifiable information and profanity in text. In this case, personally identifiable information is the presence of information such as email addresses, phone numbers, and mailing addresses.</p><p>When you submit a text to be moderated, the API can detect the language used, if it is not stated. Screening text will automatically correct any misspelled words (to catch deliberately misspelled words). The results will contain the location of profanities and personal identifiable information in the text, as well as the original text, autocorrected text, and the <a class="indexterm" id="id185"/>language. Using these results, you can moderate content appropriately.</p></div></div><div class="section" title="Moderation tools"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Moderation tools</h2></div></div></div><p>There<a class="indexterm" id="id186"/> are three ways to moderate content, enabled <a class="indexterm" id="id187"/>by the content moderator:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Human moderation</strong></span>: Using teams and <a class="indexterm" id="id188"/>community <a class="indexterm" id="id189"/>to manually moderate all content</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Automated moderation</strong></span>: Utilizing machine<a class="indexterm" id="id190"/> learning <a class="indexterm" id="id191"/>and AI to moderate at scale with no human interaction</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hybrid moderation</strong></span>: A combination <a class="indexterm" id="id192"/>of the preceding<a class="indexterm" id="id193"/> two, where people typically occasionally do reviews</li></ul></div><p>The common scenario used is the last one. This is where machine learning is used to automate the moderation process and teams of people can review the moderation. Microsoft have created a review tool to ease this process. This allows you to see through all the items for review in a web browser while using the APIs in your application. We will look into this tool in the following section.</p><div class="section" title="Using the"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec34"/>Using the</h3></div></div></div><p>l, head over to <a class="ulink" href="https://contentmoderator.cognitive.microsoft.com/">https://contentmoderator.cognitive.microsoft.com/</a>. From here, you can sign in using your <a class="indexterm" id="id194"/>Microsoft account. On your first sign-in, you will need to register by adding<a class="indexterm" id="id195"/> your name to the account. You will then go on to create a <span class="emphasis"><em>review team</em></span>, as shown in the following screenshot:</p><div class="mediaobject"><img alt="Using the" src="graphics/B12373_02_12.jpg"/></div><p>You can <a class="indexterm" id="id196"/>do this by selecting the region and entering a team name. You can optionally enter the email addresses of other people who should be part of the team. Click on <span class="strong"><strong>Create Team</strong></span>.</p><p>Once in, you will be presented with the following dashboard:</p><div class="mediaobject"><img alt="Using the" src="graphics/B12373_02_13.jpg"/></div><p>You will be presented with the total number of images and textual content that are for review. You will also be presented with the total number of completed and pending reviews. The dashboard also lists the users that have completed reviews, as well as any tags used for content.</p><p>By selecting the <span class="strong"><strong>Try</strong></span> option in the menu, you have the option to upload images or text to execute moderation online. Do this by either uploading an image or entering sample text in the textbox. Once done, you can select the <span class="strong"><strong>Review</strong></span> option, where you will be presented with the following screen:</p><div class="mediaobject"><img alt="Using the" src="graphics/B12373_02_14.jpg"/></div><p>If the given <a class="indexterm" id="id197"/>content is either adult content or racist, you can click on the <span class="strong"><strong>a</strong></span> or <span class="strong"><strong>r</strong></span> buttons, respectively. For text, any profanities will be displayed. Once you are done marking reviews, click on <span class="strong"><strong>Next</strong></span>. This will go through a process of moderating the given content.</p></div><div class="section" title="Other tools"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec35"/>Other tools</h3></div></div></div><p>Apart from the APIs <a class="indexterm" id="id198"/>and the review tool, there are two other tools you can use, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>List manager API</strong></span>: Using <a class="indexterm" id="id199"/>custom lists of images and text to moderate pre-identified content that you don't wish to scan for repeatedly</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Workflow API</strong></span>: Using<a class="indexterm" id="id200"/> this API, you can define conditional logic and actions to specify the policies used by your specific content</li></ul></div><p>To use any of these APIs, or to <a class="indexterm" id="id201"/>use the moderator APIs, you can make calls to specific REST APIs. To do so, you will need to use an API key and a base URL. These settings can be found under <span class="strong"><strong>Settings | Credentials</strong></span> on the review tool website, as shown in the following screenshot:</p><div class="mediaobject"><img alt="Other tools" src="graphics/B12373_02_15.jpg"/></div></div></div></div>
<div class="section" title="Building your own image classifiers"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Building your own image classifiers</h1></div></div></div><p>The <span class="strong"><strong>Custom Vision</strong></span> service <a class="indexterm" id="id202"/>allows you to build your own<a class="indexterm" id="id203"/> image classifiers. There might be cases where you require special images to use the image APIs. Such cases may be from a factory, where the equipment you need to recognize is not very available. You can start to build a prototype, using as little</p><div class="section" title="Building a classifier"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Building a classifier</h2></div></div></div><p>To <a class="indexterm" id="id204"/>build a classifier, you will need to create a new project. Doing so will allow you to specify what category the images will be in. You will also select the classification type and project type.</p><p>Moving on, you will need to upload images. This can be done through the web page or through a REST API. All images must be tagged so that the classifier will recognize similar images later.</p><p>Once all images (at least 50) are uploaded, you must train your model. Once the training is complete, you will be presented with a precision percentage per tag. This is a measurement of the accuracy of the model.</p></div><div class="section" title="Improving the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Improving the model</h2></div></div></div><p>On the website, you<a class="indexterm" id="id205"/> can test your models. Doing so will allow you to upload images, which will be classified by the model. If it turns out that the model performs poorly, you can improve the model.</p><p>Improving the model involves uploading more images. Some general guidelines to improve the model are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Have enough images</li><li class="listitem" style="list-style-type: disc">Make sure that the balance between tags is good (so that there is an equal number of images per tag)</li><li class="listitem" style="list-style-type: disc">Use a diverse set of images for training</li><li class="listitem" style="list-style-type: disc">Use images that have been used for prediction</li><li class="listitem" style="list-style-type: disc">Inspect the predictions</li></ul></div></div><div class="section" title="Using the trained model"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec28"/>Using the trained model</h2></div></div></div><p>Once<a class="indexterm" id="id206"/> you are happy with the model, you can use it for predictions. The model can be used in one of the two following ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">With a REST API</li><li class="listitem" style="list-style-type: disc">Export it to a model file</li></ul></div><p>The first choice involves uploading an image. Calling the generated endpoint for your model, along with the image data, will result in a prediction. The result will contain the predicted tags, ordered by their probability.</p><p>The second choice allows you to run the prediction offline. This means that you can utilize different frameworks, such as TensorFlow, CoreML, and ONNX, for different platforms. How to use the model with these frameworks is beyond the scope of this book. The downside of using an offline model is that the accuracy may suffer a bit compared to the online version.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Summary</h1></div></div></div><p>In this chapter, we took a deep dive into a big part of the vision APIs. You first learned how to get good descriptions of images. Next, you learned how to recognize celebrities and text in images, and you learned how to generate thumbnails. Following this, we moved on to the Face API, where we got more information about detected faces. We found out how to verify whether two faces were the same. After this, you learned how to find similar faces and group similar faces. Then we added identification to our smart-house application, allowing it to know who we are. We also added the ability to recognize emotions in faces. We took a quick look into the content moderator to see how you can add automatic moderation to user-generated content. Finally, we briefly looked at the Custom Vision service, and how you can use it to generate specific prediction models.</p><p>The next chapter will continue with the final vision API. We will focus on videos, learning what the video indexer API has to offer.</p></div></body></html>