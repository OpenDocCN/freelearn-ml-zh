<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Detecting Edges and Applying Image Filters</h1></div></div></div><p>In this chapter, we are going to see how to apply cool visual effects to images. We will learn how to use fundamental image processing operators. We are going to discuss edge detection and how we can use image filters to apply various effects on photos.</p><p>By the end of this chapter, you will know:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is 2D convolution and how to use it</li><li class="listitem" style="list-style-type: disc">How to blur an image</li><li class="listitem" style="list-style-type: disc">How to detect edges in an image</li><li class="listitem" style="list-style-type: disc">How to apply motion blur to an image</li><li class="listitem" style="list-style-type: disc">How to sharpen and emboss an image</li><li class="listitem" style="list-style-type: disc">How to erode and dilate an image</li><li class="listitem" style="list-style-type: disc">How to create a vignette filter</li><li class="listitem" style="list-style-type: disc">How to enhance image contrast<div><div><h3 class="title"><a id="tip02"/>Tip</h3><p>
<strong>Downloading the example code</strong>
</p><p>You can download the example code files from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div></li></ul></div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec18"/>2D convolution</h1></div></div></div><p>Convolution is a<a id="id72" class="indexterm"/> fundamental operation in image processing. We basically apply a mathematical operator to each pixel and change its value in some way. To apply this mathematical operator, we use another <a id="id73" class="indexterm"/>matrix called a <strong>kernel</strong>. The kernel is usually<a id="id74" class="indexterm"/> much smaller in size than the input image. For each pixel in the image, we take the kernel and place it on top such that the center of the kernel coincides with the pixel under consideration. We then multiply each value in the kernel matrix with the corresponding values in the image, and then sum it up. This is the new value that will be substituted in this position in the output image.</p><p>Here, the kernel is called the "image filter" and the process of applying this kernel to the given image<a id="id75" class="indexterm"/> is called "image filtering". The output obtained after applying the kernel to the image is called the filtered image. Depending on the values in the kernel, it performs different functions like blurring, detecting edges, and so on. The following figure should help you visualize the image filtering operation:</p><div><img src="img/B04554_02_01.jpg" alt="2D convolution"/></div><p>Let's start with the <a id="id76" class="indexterm"/>simplest case which is identity kernel. This kernel doesn't really change the input image. If we consider a 3x3 identity kernel, it looks something like the following:</p><div><img src="img/B04554_02_02.jpg" alt="2D convolution"/></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Blurring</h1></div></div></div><p>Blurring refers to averaging the pixel values within a neighborhood. This is also called a <strong>low pass filter</strong>. A low pass filter is a filter that allows low frequencies and blocks higher frequencies. Now, the <a id="id77" class="indexterm"/>next question that comes to our mind is—What does "frequency" mean in an image? Well, in this context, frequency refers to the rate of change of pixel values. So we can say that the sharp edges would be high frequency <a id="id78" class="indexterm"/>content because the pixel values change rapidly in that region. Going by that logic, plain areas would be low frequency content. Going by this definition, a low pass filter would try to smoothen the edges.</p><p>A simple way to build a low pass filter is by uniformly averaging the values in the neighborhood of a pixel. We can choose the size of the kernel depending on how much we want to smoothen the image, and it will correspondingly have different effects. If you choose a bigger size, then you will be averaging over a larger area. This tends to increase the smoothening effect. Let's see what a 3x3 low pass filter kernel looks like:</p><div><img src="img/B04554_02_03.jpg" alt="Blurring"/></div><p>We are dividing the matrix by 9 because we want the values to sum up to <code class="literal">1</code>. This is called <strong>normalization</strong>, and it's important because we don't want to artificially increase the intensity value at that pixel's location. So you should normalize the kernel before applying it to an image. Normalization<a id="id79" class="indexterm"/> is a really important concept, and it is used in a variety of scenarios, so you should read a couple of tutorials online to get a good grasp on it.</p><p>Here is the code to apply this low pass filter to an image:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')
rows, cols = img.shape[:2]

kernel_identity = np.array([[0,0,0], [0,1,0], [0,0,0]])
kernel_3x3 = np.ones((3,3), np.float32) / 9.0
kernel_5x5 = np.ones((5,5), np.float32) / 25.0

cv2.imshow('Original', img)

output = cv2.filter2D(img, -1, kernel_identity)
cv2.imshow('Identity filter', output)

output = cv2.filter2D(img, -1, kernel_3x3)
cv2.imshow('3x3 filter', output)

output = cv2.filter2D(img, -1, kernel_5x5)
cv2.imshow('5x5 filter', output)

cv2.waitKey(0)</pre></div><p>If you run the preceding code, you will see something like this:</p><div><img src="img/B04554_02_04.jpg" alt="Blurring"/></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec20"/>The size of the kernel versus the blurriness</h2></div></div></div><p>In the preceding code, we <a id="id80" class="indexterm"/>are generating different kernels in the code which are <code class="literal">kernel_identity</code>, <code class="literal">kernel_3x3</code>, and <code class="literal">kernel_5x5</code>. We use the function, <code class="literal">filter2D</code>, to apply these kernels to the input image. If you look at the images carefully, you can see that they keep getting blurrier as we increase the kernel size. The reason for this is because when we increase the kernel size, we are averaging over a larger area. This tends to have a larger blurring effect.</p><p>An alternative way of doing this would be by using the OpenCV function, <code class="literal">blur</code>. If you don't want to generate the kernels yourself, you can just use this function directly. We can call it using the following line of code:</p><div><pre class="programlisting">output = cv2.blur(img, (3,3))</pre></div><p>This will apply the 3x3 kernel to the input and give you the output directly.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Edge detection</h1></div></div></div><p>The process of edge<a id="id81" class="indexterm"/> detection involves detecting sharp edges in the image and producing a binary image as the output. Typically, we draw white lines on a black background to indicate those edges. We can think of edge detection as a high pass filtering operation. A high pass filter allows <a id="id82" class="indexterm"/>high frequency content to pass through and blocks the low frequency content. As we discussed earlier, edges are high frequency content. In edge detection, we want to retain these edges and discard everything else. Hence, we should build a kernel that is the equivalent of a high pass filter.</p><p>Let's start with a simple edge detection filter known as the <code class="literal">Sobel</code> filter. Since edges can occur in both horizontal and vertical directions, the <code class="literal">Sobel</code> filter is composed of the following two kernels:</p><div><img src="img/B04554_02_05.jpg" alt="Edge detection"/></div><p>The kernel on the left detects horizontal edges and the kernel on the right detects vertical edges. OpenCV provides a function to directly apply the <code class="literal">Sobel</code> filter to a given image. Here is the code to use Sobel filters to detect edges:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input_shapes.png', cv2.IMREAD_GRAYSCALE)
rows, cols = img.shape

sobel_horizontal = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5)
sobel_vertical = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5)

cv2.imshow('Original', img)
cv2.imshow('Sobel horizontal', sobel_horizontal)
cv2.imshow('Sobel vertical', sobel_vertical)

cv2.waitKey(0)</pre></div><p>The output will look something like the following:</p><div><img src="img/B04554_02_06.jpg" alt="Edge detection"/></div><p>In the preceding figure, the image in the middle is the output of horizontal edge detector, and the image on the right is the vertical edge detector. As we can see here, the <code class="literal">Sobel</code> filter detects edges in either a horizontal or vertical direction and it doesn't give us a holistic view of all the edges. To overcome this, we can use the <code class="literal">Laplacian</code> filter. The advantage of<a id="id83" class="indexterm"/> using this filter is that it uses double derivative in both directions. You can call the function using the following line:</p><div><pre class="programlisting">laplacian = cv2.Laplacian(img, cv2.CV_64F)</pre></div><p>The output will look something like the following screenshot:</p><div><img src="img/B04554_02_07.jpg" alt="Edge detection"/></div><p>Even though the <code class="literal">Laplacian</code> kernel worked great in this case, it doesn't always work well. It gives rise to a lot of noise in the output, as shown in the screenshot that follows. This is where the <code class="literal">Canny edge</code> detector comes in handy:</p><div><img src="img/B04554_02_08.jpg" alt="Edge detection"/></div><p>As we can see in<a id="id84" class="indexterm"/> the above images, the <code class="literal">Laplacian</code> kernel gives rise to a noisy output and this is not exactly useful. To overcome this problem, we use the <code class="literal">Canny edge</code> detector. To use the <code class="literal">Canny edge</code> detector, we can use the following function:</p><div><pre class="programlisting">canny = cv2.Canny(img, 50, 240)</pre></div><p>As we can see, the quality of the Canny edge detector is much better. It takes two numbers as arguments to indicate the thresholds. The second argument is called the low threshold value, and the third argument is called the high threshold value. If the gradient value is above the high threshold value, it is marked as a strong edge. The Canny Edge Detector starts tracking the edge from this point and continues the process until the gradient value<a id="id85" class="indexterm"/> falls below the low threshold value. As you increase these thresholds, the weaker edges will be ignored. The output image will be cleaner and sparser. You can play around with the thresholds and see what happens as you increase or decrease<a id="id86" class="indexterm"/> their values. The overall formulation is quite deep. You can learn more about it at <a class="ulink" href="http://www.intelligence.tuc.gr/~petrakis/courses/computervision/canny.pdf">http://www.intelligence.tuc.gr/~petrakis/courses/computervision/canny.pdf</a>
</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Motion blur</h1></div></div></div><p>When we apply the<a id="id87" class="indexterm"/> motion blurring effect, it will look like you captured the picture while moving in a particular direction. For example, you can make an image look like it was captured from a moving car.</p><p>The input and output images will look like the following ones:</p><div><img src="img/B04554_02_09.jpg" alt="Motion blur"/></div><p>Following is the code to achieve this motion blurring effect:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')
cv2.imshow('Original', img)

size = 15

# generating the kernel
kernel_motion_blur = np.zeros((size, size))
kernel_motion_blur[int((size-1)/2), :] = np.ones(size)
kernel_motion_blur = kernel_motion_blur / size

# applying the kernel to the input image
output = cv2.filter2D(img, -1, kernel_motion_blur)

cv2.imshow('Motion Blur', output)
cv2.waitKey(0)</pre></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Under the hood</h2></div></div></div><p>We are<a id="id88" class="indexterm"/> reading the image as usual. We are then constructing a motion <code class="literal">blur</code> kernel. A motion blur kernel averages the pixel values in a particular direction. It's like a directional low pass filter. A 3x3 horizontal motion-blurring kernel would look this:</p><div><img src="img/B04554_02_10.jpg" alt="Under the hood"/></div><p>This will blur the image in a horizontal direction. You can pick any direction and it will work accordingly. The amount of blurring will depend on the size of the kernel. So, if you want to make the image blurrier, just pick a bigger size for the kernel. To see the full effect, we have taken a 15x15 kernel in the preceding code. We then use <code class="literal">filter2D</code> to apply this kernel to the input image, to obtain the motion-blurred output.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Sharpening</h1></div></div></div><p>Applying the <code class="literal">sharpening</code> filter will sharpen<a id="id89" class="indexterm"/> the edges in the image. This filter is very useful when we want to enhance the edges in an image that's not crisp. Here are some images to give you an idea of what the image <code class="literal">sharpening</code> process looks like:</p><div><img src="img/B04554_02_11.jpg" alt="Sharpening"/></div><p>As you can see in the preceding figure, the level of sharpening depends on the type of kernel we use. We have a lot of freedom to customize the kernel here, and each kernel will give you a different kind of sharpening. To just sharpen an image, like we are doing in the top right image in the preceding picture, we would use a kernel like this:</p><div><img src="img/B04554_02_12.jpg" alt="Sharpening"/></div><p>If we want to<a id="id90" class="indexterm"/> do excessive sharpening, like in the bottom left image, we would use the following kernel:</p><div><img src="img/B04554_02_13.jpg" alt="Sharpening"/></div><p>But the problem with these two kernels is that the output image looks artificially enhanced. If we want our images to look more natural, we would use an <code class="literal">Edge Enhancement</code> filter. The underlying concept remains the same, but we use an approximate Gaussian kernel to build this filter. It will help us smoothen the image when we enhance the edges, thus making the image look more natural.</p><p>Here is the code to achieve the effects applied in the preceding screenshot:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')
cv2.imshow('Original', img)

# generating the kernels
kernel_sharpen_1 = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
kernel_sharpen_2 = np.array([[1,1,1], [1,-7,1], [1,1,1]])
kernel_sharpen_3 = np.array([[-1,-1,-1,-1,-1],
                             [-1,2,2,2,-1],
                             [-1,2,8,2,-1],
                             [-1,2,2,2,-1],
                             [-1,-1,-1,-1,-1]]) / 8.0

# applying different kernels to the input image
output_1 = cv2.filter2D(img, -1, kernel_sharpen_1)
output_2 = cv2.filter2D(img, -1, kernel_sharpen_2)
output_3 = cv2.filter2D(img, -1, kernel_sharpen_3)

cv2.imshow('Sharpening', output_1)
cv2.imshow('Excessive Sharpening', output_2)
cv2.imshow('Edge Enhancement', output_3)
cv2.waitKey(0)</pre></div><p>If you noticed, in the<a id="id91" class="indexterm"/> preceding code, we didn't divide the first two kernels by a normalizing factor. The reason is because the values inside the kernel already sum up to 1, so we are implicitly dividing the matrices by 1.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Understanding the pattern</h2></div></div></div><p>You must have noticed a<a id="id92" class="indexterm"/> common pattern in image filtering code examples. We build a kernel and then use <code class="literal">filter2D</code> to get the desired output. That's exactly what's happening in this code example as well! You can play around with the values inside the kernel and see if you can get different visual effects. Make sure that you normalize the kernel before applying it, or else the image will look too bright because you are artificially increasing the pixel values in the image.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Embossing</h1></div></div></div><p>An embossing filter will<a id="id93" class="indexterm"/> take an image and convert it into an embossed image. We basically take each pixel and replace it with a shadow or a highlight. Let's say we are dealing with a relatively plain region in the image. Here, we need to replace it with plain gray color because there's not much information there. If there is a lot of contrast in a particular region, we will replace it with a white pixel (highlight), or a dark pixel (shadow), depending on the direction in which we are embossing.</p><p>This is what it will look like:</p><div><img src="img/B04554_02_14.jpg" alt="Embossing"/></div><p>Let's take a<a id="id94" class="indexterm"/> look at the code and see how to do this:</p><div><pre class="programlisting">import cv2
import numpy as np

img_emboss_input = cv2.imread('input.jpg')

# generating the kernels
kernel_emboss_1 = np.array([[0,-1,-1],
                            [1,0,-1],
                            [1,1,0]])
kernel_emboss_2 = np.array([[-1,-1,0],
                            [-1,0,1],
                            [0,1,1]])
kernel_emboss_3 = np.array([[1,0,0],
                            [0,0,0],
                            [0,0,-1]])

# converting the image to grayscale
gray_img = cv2.cvtColor(img_emboss_input,cv2.COLOR_BGR2GRAY)

# applying the kernels to the grayscale image and adding the offset
output_1 = cv2.filter2D(gray_img, -1, kernel_emboss_1) + 128
output_2 = cv2.filter2D(gray_img, -1, kernel_emboss_2) + 128
output_3 = cv2.filter2D(gray_img, -1, kernel_emboss_3) + 128

cv2.imshow('Input', img_emboss_input)
cv2.imshow('Embossing - South West', output_1)
cv2.imshow('Embossing - South East', output_2)
cv2.imshow('Embossing - North West', output_3)
cv2.waitKey(0)</pre></div><p>If you run the preceding code, you will see that the output images are embossed. As we can see from the kernels above, we are just replacing the current pixel value with the difference of the<a id="id95" class="indexterm"/> neighboring pixel values in a particular direction. The embossing effect is achieved by offsetting all the pixel values in the image by <code class="literal">128</code>. This operation adds the highlight/shadow effect to the picture.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Erosion and dilation</h1></div></div></div><p>
<strong>Erosion</strong> and <strong>dilation</strong> are <a id="id96" class="indexterm"/>morphological image processing operations. Morphological image processing basically deals with modifying geometric structures in the image. These<a id="id97" class="indexterm"/> operations are primarily defined for binary images, but we can also use them on grayscale images. Erosion basically strips out the outermost layer of pixels in a structure, where as dilation adds an extra layer of pixels on a structure.</p><p>Let's see what these operations look like:</p><div><img src="img/B04554_02_15.jpg" alt="Erosion and dilation"/></div><p>Following is<a id="id98" class="indexterm"/> the code to<a id="id99" class="indexterm"/> achieve this:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.png', 0)

kernel = np.ones((5,5), np.uint8)

img_erosion = cv2.erode(img, kernel, iterations=1)
img_dilation = cv2.dilate(img, kernel, iterations=1)

cv2.imshow('Input', img)
cv2.imshow('Erosion', img_erosion)
cv2.imshow('Dilation', img_dilation)

cv2.waitKey(0)</pre></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Afterthought</h2></div></div></div><p>OpenCV provides functions to directly erode and dilate an image. They are called erode and dilate, respectively. The interesting<a id="id100" class="indexterm"/> thing to note is the third argument in these two<a id="id101" class="indexterm"/> functions. The number of iterations will determine how much you want to erode/dilate a given image. It basically applies the operation successively to the resultant image. You can take a sample image and play around with this parameter to see what the results look like.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec25"/>Creating a vignette filter</h1></div></div></div><p>Using all the<a id="id102" class="indexterm"/> information we have, let's see if we can create a nice <code class="literal">vignette</code> filter. The output will look something like the following:</p><div><img src="img/B04554_02_16.jpg" alt="Creating a vignette filter"/></div><p>Here is the code to achieve this effect:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')
rows, cols = img.shape[:2]

# generating vignette mask using Gaussian kernels
kernel_x = cv2.getGaussianKernel(cols,200)
kernel_y = cv2.getGaussianKernel(rows,200)
kernel = kernel_y * kernel_x.T
mask = 255 * kernel / np.linalg.norm(kernel)
output = np.copy(img)

# applying the mask to each channel in the input image
for i in range(3):
    output[:,:,i] = output[:,:,i] * mask

cv2.imshow('Original', img)
cv2.imshow('Vignette', output)
cv2.waitKey(0)</pre></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec24"/>What's happening underneath?</h2></div></div></div><p>The <code class="literal">Vignette</code> filter basically focuses the brightness on a particular part of the image and the other parts look faded. In order to achieve this, we need to filter out each channel in the image using a Gaussian <a id="id103" class="indexterm"/>kernel. OpenCV provides a function to do this, which is called <code class="literal">getGaussianKernel</code>. We need to build a 2D kernel whose size matches the size of the image. The second parameter of the function, <code class="literal">getGaussianKernel</code>, is interesting. It is the standard deviation of the Gaussian and it controls the radius of the bright central region. You can play around with this parameter and see how it affects the output.</p><p>Once we build the 2D kernel, we need to build a mask by normalizing this kernel and scaling it up, as shown in the following line:</p><div><pre class="programlisting"> mask = 255 * kernel / np.linalg.norm(kernel)</pre></div><p>This is an important step because if you don't scale it up, the image will look black. This happens because all the pixel values will be close to <code class="literal">0</code> after you superimpose the mask on the input image. After this, we iterate through all the color channels and apply the mask to each channel.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec25"/>How do we move the focus around?</h2></div></div></div><p>We now know how to create a <code class="literal">vignette</code> filter that focuses on the center of the image. Let's say we want to <a id="id104" class="indexterm"/>achieve the same <code class="literal">vignette</code> effect, but we want to focus on a different region in the image, as shown in the following figure:</p><div><img src="img/B04554_02_17.jpg" alt="How do we move the focus around?"/></div><p>All we need to do is build a bigger Gaussian kernel and make sure that the peak coincides with the region of interest. Following is the code to achieve this:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')
rows, cols = img.shape[:2]

# generating vignette mask using Gaussian kernels
kernel_x = cv2.getGaussianKernel(int(1.5*cols),200)
kernel_y = cv2.getGaussianKernel(int(1.5*rows),200)
kernel = kernel_y * kernel_x.T
mask = 255 * kernel / np.linalg.norm(kernel)
mask = mask[int(0.5*rows):, int(0.5*cols):]
output = np.copy(img)

# applying the mask to each channel in the input image
for i in range(3):
    output[:,:,i] = output[:,:,i] * mask

cv2.imshow('Input', img)
cv2.imshow('Vignette with shifted focus', output)

cv2.waitKey(0)</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec26"/>Enhancing the contrast in an image</h1></div></div></div><p>Whenever we<a id="id105" class="indexterm"/> capture images in low-light conditions, the images turn out to be dark. This typically happens when you capture images in the evening or in a dimly lit room. You must have seen this happen many times! The reason this happens is because the pixel values tend to concentrate near 0 when we capture the images under such conditions. When this happens, a lot of details in the image are not clearly visible to the human eye. The human eye likes contrast, and so we need to adjust the contrast to make the image look nice and pleasant. A lot of cameras and photo applications implicitly do this already. We use <a id="id106" class="indexterm"/>a process called <strong>Histogram Equalization</strong> to achieve this.</p><p>To give an example, this is what it looks like before and after contrast enhancement:</p><div><img src="img/B04554_02_18.jpg" alt="Enhancing the contrast in an image"/></div><p>As we can see here, the input image on the left is really dark. To rectify this, we need to adjust the pixel <a id="id107" class="indexterm"/>values so that they are spread across the entire spectrum of values, that is, between 0 and <code class="literal">255</code>.</p><p>Following is the code for adjusting the pixel values:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg', 0)

# equalize the histogram of the input image
histeq = cv2.equalizeHist(img)

cv2.imshow('Input', img)
cv2.imshow('Histogram equalized', histeq)
cv2.waitKey(0)</pre></div><p>Histogram equalization is applicable to grayscale images. OpenCV provides a function, <code class="literal">equalizeHist</code>, to achieve this effect. As we can see here, the code is pretty straightforward, where we read the image and equalize its histogram to adjust the contrast of the image.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec26"/>How do we handle color images?</h2></div></div></div><p>Now that we<a id="id108" class="indexterm"/> know how to equalize the histogram of a grayscale image, you might be wondering how to handle color images. The thing about histogram equalization is that it's a nonlinear process. So, we cannot just separate out the three channels in an RGB image, equalize the histogram separately, and combine them later to form the <a id="id109" class="indexterm"/>output image. The concept of histogram equalization is only applicable to the intensity values in the image. So, we have to make sure not to modify the color information when we do this.</p><p>In order to handle the histogram equalization of color images, we need to convert it to a color space where intensity is separated from the color information. YUV is a good example of such a color space. Once we convert it to YUV, we just need to equalize the Y-channel and combine it with the other two channels to get the output image.</p><p>Following is an example of what it looks like:</p><div><img src="img/B04554_02_19.jpg" alt="How do we handle color images?"/></div><p>Here is the code<a id="id110" class="indexterm"/> to achieve histogram equalization for color<a id="id111" class="indexterm"/> images:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')

img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)

# equalize the histogram of the Y channel
img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])

# convert the YUV image back to RGB format
img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)

cv2.imshow('Color input image', img)
cv2.imshow('Histogram equalized', img_output)

cv2.waitKey(0)</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec27"/>Summary</h1></div></div></div><p>In this chapter, we learned how to use image filters to apply cool visual effects to images. We discussed the fundamental image processing operators and how we can use them to build various things. We learnt how to detect edges using various methods. We understood the importance of 2D convolution and how we can use it in different scenarios. We discussed how to smoothen, motion-blur, sharpen, emboss, erode, and dilate an image. We learned how to create a vignette filter, and how we can change the region of focus as well. We discussed contrast enhancement and how we can use histogram equalization to achieve it. In the next chapter, we will discuss how to cartoonize a given image.</p></div></div>
</body></html>