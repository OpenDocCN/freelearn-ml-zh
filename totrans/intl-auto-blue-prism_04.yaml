- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reviewing Predictions and Human in the Loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest differences between RPA and IA lies in how previously certain
    automated work outcomes can become uncertain. We’re no longer 100% confident that
    the Digital Worker has taken the correct course of action due to uncertainty in
    the ML prediction. The main way to get a sense of how well the ML algorithm is
    performing, and to reduce the overall risk levels of IA, is to *perform a partial
    manual review of the ML predictions*. This chapter discusses two different ways
    to design this manual verification through adding **human in the loop** (**HITL**)
    into the automated process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why should we review predictions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does HITL mean in the context of IA?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What criteria can be used to trigger human intervention?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we share prediction data between prediction reviewers and Blue Prism?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download and import the three `.bprelease` files in the `ch4` folder on GitHub:
    [https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/tree/main/ch4](https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/tree/main/ch4)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Example 2* of this chapter also requires that the [*Chapter 3*](B18416_03.xhtml#_idTextAnchor048),
    *Example 3* Release is imported. There is also an Excel file in the GitHub folder,
    which will be downloaded as part of *Example 2*.'
  prefs: []
  type: TYPE_NORMAL
- en: Why should we review predictions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automation enthusiasts don’t like the idea of having to review ML predictions.
    After all, isn’t the purpose of combining ML with RPA to further remove people
    from being involved in the business process? ML models, outside the context of
    IA, are like any other software. They can’t be used in perpetuity. They require
    monitoring and updating as ML models are known to *drift* over time, leading to
    poor accuracy. Drift can happen for many reasons, such as changes in the characteristics
    of the underlying input data, or exposure to new data that the model wasn’t trained
    on. Not only can reviewing predictions help detect drift but there are also some
    IA-specific reasons to review predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Reduce business risk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When moving a Process into production for the first time, it’s common to closely
    monitor the processing and to only allow a fraction of the total work volume to
    be worked. These practices are ways to manage the risks inherent to deploying
    something new into production.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, introducing ML increases the risk levels of an automated process.
    Despite this, we continue to pursue IA because we believe that the benefits of
    using ML outweigh these risks. Reviewing ML predictions and correcting them if
    necessary is one of the most direct ways of mitigating this increased business
    risk on an ongoing, operational, case-by-case basis.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways of minimizing the business risks of IA, for example, by
    deliberately choosing processes where human review is guaranteed to occur downstream,
    or by choosing use cases where incorrect outcomes are tolerated. However, these
    types of risk mitigations tend to determine which IA processes get greenlit. They
    can’t be used to control risk on an operational level after a Process is put into
    production.
  prefs: []
  type: TYPE_NORMAL
- en: Stay ahead of regulatory concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automation and AI are front and center in the minds of regulators and lawmakers
    globally. Countries such as Brazil, China, and the United States are actively
    developing legislation to govern the use of AI in society. For example, in October
    2023, the Biden administration issued an executive order centered on how to manage
    the risks of AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Europe is leading the way in defining legislation with two relevant frameworks:
    the **EU AI Act**, which is currently in development, and the **General Data Protection
    Regulation** (**GDPR**), which is already in effect. The **EU AI Act** categorizes
    AI use cases into four risk categories: minimal, limited, high, and unacceptable.
    Guidelines exist to define which use cases fall into each category and there’s
    corresponding legislation to cover each of the levels. For example, under the
    *unacceptable* risk category, the use of AI to find victims of crime is explicitly
    disallowed. Article 14 of the Act discusses human oversight. Systems that are
    high-risk must be designed in a way such that they can be *overseen by natural
    persons during the period in which the AI system is in use.* While very few use
    cases would be classified as high-risk under the EU AI Act, it’s worth incorporating
    human oversight into our IA solution design as a precautionary measure.'
  prefs: []
  type: TYPE_NORMAL
- en: Article 22 of the **GDPR** states that EU citizens have the right *not to be
    subject to a decision based solely on automated processing* and the right to *obtain
    human intervention*. While the EU AI Act and GDPR are only applicable to the EU,
    it’s possible that other countries will adopt similar provisions in their own
    AI legislation. If we design our Processes such that ML predictions can be manually
    reviewed (i.e., human oversight), we can mitigate regulatory risks that may arise
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the EU AI Act and the GDPR imply that there should be mechanisms designed
    into our IA Processes to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Allow for ML prediction to be verified by a human
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable ML decision-making either completely or selectively when requested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter focuses on the first point, which is a form of HITL. The second
    point will be covered in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: What does HITL mean in the context of IA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HITL broadly means that humans should interact with automation in some way.
    Under traditional RPA, human input might be needed for many reasons. For example,
    there might be two-factor authentication that needs human input to log in to a
    website.
  prefs: []
  type: TYPE_NORMAL
- en: 'As IA means the addition of ML into an automated process, human interaction
    in this context refers specifically to someone *viewing the results of an ML prediction,
    correcting it if necessary, and making the reviewed predictions available to BP*.
    Adding HITL to do this greatly affects our process design. It requires determining
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What criteria should trigger human intervention in the automated process?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What user interface should be used to display predictions and to receive feedback
    on whether the prediction is correct? For example, this could be achieved through
    email, a shared spreadsheet, a website, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether reviewing predictions needs to be done in near real-time to meet service
    level agreements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When presented with a prediction to review, the reviewer must decide on the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to accept or change the prediction result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to continue or suspend automated processing of the work case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to these questions will be illustrated through the examples of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What criteria can be used to trigger human intervention?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that an ML prediction will give you a *predicted value*, whether that
    be a number, a label, an image, or generated text, and a *confidence score*. A
    well-informed decision to intervene will usually take both factors into account,
    but there are still reasons to intervene solely based on one or the other.
  prefs: []
  type: TYPE_NORMAL
- en: Random sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest way to decide which work case should be manually reviewed is through
    **random sampling**:'
  prefs: []
  type: TYPE_NORMAL
- en: To use random sampling, choose a *target percentage* that you want to route
    for manual verification, for example, 10%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a random number between 1 and 100, inclusive, after an ML prediction
    is made. If the random number is under the chosen target (10 in this case), automated
    processing will stop, pending human review of the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During review, the reviewer can correct the prediction and allow the work case
    to continue, or suspend automated processing altogether.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In one real-life use case, the IA team chose a random sampling target of 25%.
    The main purpose behind randomly sampling 25% of the cases for manual review was
    to keep track of prediction accuracy. Although random sampling wasn’t explicitly
    called out as being used to reduce risk, it can absolutely be used for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Random sampling doesn’t use the prediction or the confidence score in any way.
    This is the *major difference between random sampling and thresholding*, a different
    HITL trigger that is discussed later in this chapter. Notice that if we set the
    target percentage to 100%, we effectively make it so that every prediction must
    be reviewed by a person.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how random sampling can be implemented through an example. The four
    high-level steps that we will go through are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Examine the contents of the example’s Release file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understand how to generate random numbers in BP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understand how Deferrals, Choices, and Statuses can define the flow of the Process
    that uses Random Sampling to trigger HITL review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example to see the result in the Control Room.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example 1 – random sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of random sampling is simple. First, generate a random number. Next,
    check to see whether the random number meets your criteria for human review. Despite
    this simple logic, the implementation of random sampling in a BP process requires
    using numerous BP features. In this example, we’ll look at an implementation of
    random sampling that uses a single Work Queue, Statuses, and Deferrals.
  prefs: []
  type: TYPE_NORMAL
- en: This example has already been developed, based on BP’s Process template. Our
    goal is to examine and understand the design elements used to implement random
    sampling for ML predictions for human review. The first step is to import the
    Release file.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the .bprelease contents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s examine *Example 1*’s `.bprelease` contents before starting. Verify that
    one Process, one Work Queue, and one Environment Variable have been imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Example 1’s .bprelease file contents](img/B18416_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Example 1’s .bprelease file contents
  prefs: []
  type: TYPE_NORMAL
- en: The Environment Variable named `Ch4 Example 1 Random Sampling Target` defaults
    to **33.33**, meaning that roughly one-third of the ML predictions in this Process
    will be chosen for manual review.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s open the Process and examine the BP Stages related to random sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Generating random numbers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we’ll be seeing how the random sampling logic has been implemented.
    BP doesn’t have a direct method to generate random numbers. There are, broadly,
    three ways to generate random numbers in BP. The first is to download a release
    on the DX that has random number functionality (see [https://digitalexchange.blueprism.com/dx/entry/3439/solution/utility---mathnet---random](https://digitalexchange.blueprism.com/dx/entry/3439/solution/utility---mathnet---random)).
    This DX asset requires a separate `.dll` file to be downloaded from NuGet. The
    second method is to create a new Object and add random number generation code
    into a *Code* Stage. The third way, which is used in this example, is through
    PowerShell’s `Get-Random` function. Note that I’m only using PowerShell to keep
    the `.bprelease` content simple. The preferred solution would be to have it in
    an Object.
  prefs: []
  type: TYPE_NORMAL
- en: Open **Example 1 – Random Sampling** Process in *Ch4* Group in the Process Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find the Block named `Main Page`. This is where all of the major steps of the
    Process should belong. Let’s focus our attention on the `03 Random Sampling` Page,
    which contains the logic to perform random sampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Random sampling in the Work Block on the Main Page](img/B18416_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Random sampling in the Work Block on the Main Page
  prefs: []
  type: TYPE_NORMAL
- en: Open the `03 Random Sampling` Page. Double-click the `Get-Random` function and
    passing in minimum and maximum values of `1.0` and `100.0`. The added decimal
    `.0` is important because it allows for decimal numbers to be generated. If omitted,
    `Get-Random` returns integer values. Decimals are needed since our random sampling
    target is also a decimal number, 33.33.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Using PowerShell’s Get-Random to generate random numbers](img/B18416_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Using PowerShell’s Get-Random to generate random numbers
  prefs: []
  type: TYPE_NORMAL
- en: 'Double-click on the *Require Manual Review?* Decision Stage. Here, we compare
    the random number generated from PowerShell to the value of the Environment Variable.
    If the comparison is `Main Page`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The criteria to determine whether manual review is needed](img/B18416_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – The criteria to determine whether manual review is needed
  prefs: []
  type: TYPE_NORMAL
- en: We’ve finished examining the random sampling logic. Next, let’s see how a Work
    Queue Item can be prevented from working while we wait for the HITL review to
    complete.
  prefs: []
  type: TYPE_NORMAL
- en: Deferrals, Choices, and Statuses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Main Page` contains three important Stages that make up the logic needed
    to prevent Work Queue Items from proceeding while they’re pending review. We’ll
    be examining these three Stages – *Check Status*, *Manual Review*, and *Defer
    Item for Manual Review* – in this section of the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The Stages that prevent Items from proceeding while reviews
    are pending](img/B18416_04_5..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The Stages that prevent Items from proceeding while reviews are
    pending
  prefs: []
  type: TYPE_NORMAL
- en: Return to the `Main Page` and double-click the *Manual Review?* Decision Stage.
    This Stage checks the flag that was set on the `03 Random Sampling` Page. If manual
    review is required, execution continues to the `Defer Item for Manual` `Review`
    Page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the `Defer Item for Manual Review` Page. Only two things happen here.
    First, the *Status* of the current Work Queue Item is set to **Manual Review Required**.
    Next, the Work Queue Item is deferred for 10 minutes. The 10-minute deferral time
    is arbitrary and can be tuned to your liking:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – The Defer Item for Manual Review Page sets the Item Status and
    defers](img/B18416_04_6..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – The Defer Item for Manual Review Page sets the Item Status and
    defers
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to the `Main Page` and double-click the *Check Status* Choice Stage.
    In the context of this example, the Choice Stage is primarily used to continually
    defer items with the **Manual Review Required** Status until the Status changes
    to **Manual** **Review Complete**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Status checks stop deferred Items from progressing if they haven’t
    been reviewed](img/B18416_04_7..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Status checks stop deferred Items from progressing if they haven’t
    been reviewed
  prefs: []
  type: TYPE_NORMAL
- en: 'Also see that the third Choice always sets the Manual Review Flag to **True**
    so that deferred Work Queue Items that haven’t been reviewed are deferred again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Deferral continues until the Status changes to Manual Review
    Complete](img/B18416_04_8..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Deferral continues until the Status changes to Manual Review Complete
  prefs: []
  type: TYPE_NORMAL
- en: If the Manual Review Flag is **False** (the randomly generated number was above
    or equal to 33.33), or if the Work Queue Item Status is changed to **Manual Review
    Complete**, automated processing can continue.
  prefs: []
  type: TYPE_NORMAL
- en: Run the Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let’s run the Process once. The Process loads nine items into the Work
    Queue named [*Chapter 5*](B18416_05.xhtml#_idTextAnchor075) *Example 1 Queue*.
    Since our sampling rate is 33.33%, we expect to see roughly three Items deferred
    for HITL review each time the Process is run:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the Process named **Example 1 – Random Sampling** in the *Ch4* Group from
    the Control Room. You will see the PowerShell window pop up each time that a prediction
    is made.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on **Chapter 4** **Example 1 Queue** in the *Ch4* Group under **Queue
    Management**. Hopefully, you’ll see that some of the Work Queue Items are **Completed**
    while those randomly selected for manual review have been deferred, with the Status
    set to **Manual Review Required**. The Items with a **Manual Review Required**
    Status also have a deferral time, which is 10 minutes from now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Check that the Items needing manual review have been deferred](img/B18416_04_9..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Check that the Items needing manual review have been deferred
  prefs: []
  type: TYPE_NORMAL
- en: We’ve completed *Example 1*. Here, we saw how Statuses, Choices, and Deferrals
    can be used to implement a *random sampling* strategy that prevents Items from
    being worked until they are manually reviewed. We also saw how to generate random
    numbers through PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: One point that we didn’t cover in *Example 1* was how to actually change the
    Status of an Item so that it can be picked up for processing again after a deferral.
    This will be discussed in *Example 3* of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: An alternate strategy to random sampling to choose which cases need human review
    involves using the predicted values and confidence scores. This is called **thresholding**,
    which is discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thresholding uses the *confidence score* and sometimes the *label* to determine
    which predictions need manual review. The idea is to choose a threshold value
    for the confidence score. If a prediction’s confidence score is *under* the threshold,
    the case is flagged for manual review, and if it’s *above,* automated processing
    can continue.
  prefs: []
  type: TYPE_NORMAL
- en: There are many examples of thresholding being used in real-life IA. In one case,
    a company decided to have three ranges of thresholds. The first threshold range
    of 0% to 90% was chosen to stop automated processing completely in favor of manual
    processing. Confidence scores between 90% and 99.5% were flagged for human review.
    Work Items with scores above 99.5% were allowed to continue IA without any review.
    This is an example of how we can have not just one threshold, but many.
  prefs: []
  type: TYPE_NORMAL
- en: Not only can there be multiple thresholds, but different labels can have their
    own thresholds as well. In a different real-life IA use case, there was an ML
    algorithm that predicted between over 70 classes. Some of these classes were predicted
    with high accuracy (> 90%) whereas other classes were predicted with lower accuracy
    (<75%). The IA team decided to set the confidence score threshold to higher values
    for predictions falling into the low-accuracy classes in order to make human validation
    more likely. For predictions into the highly accurate labels, the confidence score
    threshold was set to 0, allowing for fully automated processing as nothing would
    be flagged for review.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing threshold values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Thresholding requires the IA team to decide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Which labels (numerical ranges for regression) require thresholding?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many threshold intervals are required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What the cutoff points of the thresholds should be?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no simple guidelines on how to set the threshold values, as they’re
    closely tied to how much risk the business can accept. A reasonable way to set
    this value is to perform experiments to estimate the number of incorrect predictions
    that result from setting the threshold at certain levels. Then, quantify the impact
    of getting an incorrect prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we have an NLP model that extracts invoice IDs from invoices.
    After testing the model on 10 invoices, we have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **#** | **Confidence Score** | **Invoice** **ID correct?** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 87 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 57 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 46 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 67 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 78 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 93 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 72 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 53 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 62 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 43 | No |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – The results of trying to extract invoice IDs from 10 invoices
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the model has correctly extracted the invoice ID 6/10 times (60% of
    the time) for this small sample set. The IA team has decided to use *thresholding*
    and improve accuracy by manually reviewing predictions with low confidence scores.
    Predictions with a confidence score above the threshold will be processed automatically
    by BP without review. *Which threshold value is appropriate for the* *confidence
    score?*
  prefs: []
  type: TYPE_NORMAL
- en: 'If manual review *always* results in the correct invoice ID being extracted
    (note that this isn’t always the case), choosing different threshold values gives
    us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Confidence score** **threshold** | **# of predictions under the threshold
    requiring** **manual review** | **# of predictions above the threshold that are
    processed** **without review** | **Accuracy of cases processed** **without review**
    | **Accuracy of all cases including** **manual review** |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | 0 | 10 | 6/10 = 60% | 6/10 = 60% |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 1 | 9 | 6/9 = 66.7% | 7/10 = 70% |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 2 | 8 | 6/8 = 75% | 8/10 = 80% |'
  prefs: []
  type: TYPE_TB
- en: '| 55 | 3 | 7 | 5/7 = 71% | 8/10 = 80% |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 4 | 6 | 5/6 = 83.3% | 9/10 = 90% |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | 5 | 5 | 4/5 = 80% | 9/10 = 90% |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | 6 | 4 | 4/4 = 100% | 10/10 = 100% |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 7 | 3 | 3/3 = 100% | 10/10 = 100% |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 8 | 2 | 2/2 = 100% | 10/10 = 100% |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – The accuracies of the automated portion and the overall accuracy
    of different threshold levels for the confidence score
  prefs: []
  type: TYPE_NORMAL
- en: Having a high threshold value for the confidence score generally improves accuracy,
    but also results in more cases being flagged for manual review. Look at the threshold
    value of *80* in the preceding table. We can see that there’s perfect accuracy
    based on the 10 samples (column 5), but we need to manually review 8/10 of the
    predictions (column 2).
  prefs: []
  type: TYPE_NORMAL
- en: Setting a low threshold allows for more cases to be processed by BP automatically;
    however, the number of incorrect predictions that get actioned on also increases.
    For example, if the threshold is set to *45*, only one prediction needs to be
    reviewed (column 2), but 30% of the cases are processed based on an incorrect
    prediction (column 5).
  prefs: []
  type: TYPE_NORMAL
- en: Setting the threshold value is a *balancing act between the desired accuracy,
    the impact of working a case based on an incorrect prediction, and the number
    of cases you want to process without manual intervention*. We’ve already seen
    how the overall accuracy is related to the number of automated cases through *Table
    4.2*. Next, we’ll discuss the impact of allowing automation to work a case based
    on an incorrect prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of automating cases with incorrect ML predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the example of extracting invoice IDs, the inherent riskiness of continuing
    processing depends on many factors:'
  prefs: []
  type: TYPE_NORMAL
- en: How likely can an incorrectly extracted invoice match a genuine invoice ID?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will manual review occur after the automation is complete so that ML prediction
    mistakes are caught further downstream?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How difficult is it to revise an incorrect invoice ID in the systems where it
    is persisted? Does it need to be done in many different places?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the invoice ID be submitted to third-party systems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The business and IA teams will need to come up with an assessment of how much
    manual review is needed based on the criteria listed, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In a multi-label classification case, each label can also have its own degree
    of riskiness. For example, if the NLP model for invoices also extracts numerical
    values, incorrectly labeling a number as a *price* is likely riskier than incorrectly
    labeling a number as the invoice *year*.
  prefs: []
  type: TYPE_NORMAL
- en: The selected threshold values should be carefully considered by assessing the
    risks of using an incorrect prediction to continue processing. Different types
    of incorrect predictions may have different risk profiles, meaning that *accuracy
    is insufficient to determine the threshold* in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we should have an understanding of how thresholding works and how to potentially
    choose a threshold value for the confidence score. Next, let’s implement thresholding
    in BP through an example. In the following example, we will perform three high-level
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Examine the contents of the example’s Release file and download an Excel file
    that stores the threshold labels and values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See how the thresholding logic is implemented in BP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example to see the result in the Control Room.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example 2 – thresholding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that we’re working for a company that sells products through e-commerce
    platforms such as Amazon, eBay, and Walmart. Our company uses RPA to collect user
    reviews that are left on those marketplace websites. As part of an initiative
    to improve customer relations and product quality, the company wants to determine
    whether user feedback is positive or not. If the feedback is negative, the comments
    will be sent to the customer service and product development teams for analysis
    and follow-up.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the Object created in [*Chapter 3*](B18416_03.xhtml#_idTextAnchor048),
    *Example 3* does something similar to this. It uses ML.NET to detect *toxic* versus
    non toxic sentiment for user reviews. A random sampling scheme isn’t a good choice
    here, since we want to place more emphasis on reviewing negative rather than positive
    sentiment. We’ll instead use thresholding, to review potentially negative reviews
    more often than positive ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of thresholds, the IA team together with the business users have done
    some testing to determine the threshold ranges and have chosen the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Label** | **Threshold** **Interval 1** | **Threshold** **Interval 2** |'
  prefs: []
  type: TYPE_TB
- en: '| Non Toxic | 0 <= Confidence Score < 75Manual review | 75 <= Confidence Score
    <= 100Allow automated processing |'
  prefs: []
  type: TYPE_TB
- en: '| Toxic | Always Manual Review |  |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – The selected thresholds
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the BP Process’s high-level layout is more or less identical
    to the random sampling process in our previous example. The difference lies mostly
    in how we handle the threshold values so our attention will be placed there.
  prefs: []
  type: TYPE_NORMAL
- en: Examine the .bprelease contents and download an Excel file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s familiarize ourselves with the contents of the Release. There is one Process,
    one Work Queue, and one Environment Variable. The Environment Variable named `Ch4
    Example 2 Thresholds Excel File Full Path` stores the path to the Excel file that
    contains the threshold values shown in *Table 4.3*. By default, it is set to `C:/Users/Public/Ch4
    Example 2 Thresholds.xlsx`. We will need to download this Excel file from GitHub
    if it hasn’t been already.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The Release file contents](img/B18416_04_10..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The Release file contents
  prefs: []
  type: TYPE_NORMAL
- en: Download the Excel file from [https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch4/Ch4%20Example%202%20Thresholds.xlsx](https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch4/Ch4%20Example%202%20Thresholds.xlsx).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the Excel file into the `C:\Users\Public` folder and ensure that the file
    is named `Ch4 Example` `2 Thresholds.xlsx`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the Excel file. This file stores all of the thresholds for the different
    labels. According to the thresholds chosen in *Table 4.3*, the threshold is set
    to *75* for the **Non Toxic** label and *100* for the **Toxic** label. *100* means
    that everything will require manual review. While you can use Environment Variables
    to store threshold values, it can get difficult to manage if there are multiple
    labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.11 – The threshold values stored in the Excel file](img/B18416_04_11..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The threshold values stored in the Excel file
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s view the Process in the Process Studio and examine the BP stages
    related to thresholding. The portions of the Process related to Deferrals and
    Statuses won’t be covered as they were already discussed in *Example 1* of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we’ll look at the areas of the BP Process that are relevant to the thresholding
    logic. This includes a Page that converts the threshold Excel file into a Collection,
    a Global Data Item, and a Decision Stage that checks the threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: Open **Example 2 – Thresholding** Process in the *Ch4* Group in the Process
    Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See that there’s a Page named `Convert Thresholds to Collection`. This Page
    maps the thresholds saved in the Excel file into a Collection. This `Threshold`
    Collection is stored on the `Main Page` as a Global Data Item:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12 – The Excel thresholds are converted into a Collection](img/B18416_04_12..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – The Excel thresholds are converted into a Collection
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `03 Thresholding` Page. There are two things to notice. First, we’re
    filtering the `Threshold` Collection by the label that was predicted for this
    Work Queue Item run. This allows us to set thresholds on a per-label basis. Next,
    there’s a *Decision* Stage right before the End Stage that contains the logic
    to determine whether this Item needs manual review:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The thresholding logic in the Decision Stage](img/B18416_04_13..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The thresholding logic in the Decision Stage
  prefs: []
  type: TYPE_NORMAL
- en: Run the Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Example 2* loads 10 static user reviews into the Work Queue. There is one
    toxic user review, which is correctly labeled as toxic. There’s another user review
    that is predicted as non toxic, but at low confidence, necessitating HITL review.
    In total, 2 of the 10 reviews should be deferred as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the Process named **Example 2 – Thresholding** under the *Ch4* Group in
    the *Control Room*. Wait for the Session to finish.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Chapter 4** **Example 2 Queue** under **Queue Management**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See that two of the Items are deferred due to needing manual review:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Check that the Items needing manual review have been deferred](img/B18416_04_14..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Check that the Items needing manual review have been deferred
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now seen how we can design *random sampling* and *thresholding* into a
    BP Process. Both methods use the same overall design structure, which includes
    using Status and Defer. Choosing between random sampling and thresholding is a
    business decision rather than a technical one.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we’ve conveniently ignored how reviewers can submit reviewed predictions
    back into BP. This is the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How can we share prediction data between prediction reviewers and BP?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the two previous examples, we didn’t show how corrected predictions can
    be fed back into BP so that the cases can resume. To enable this, we need a place
    to share data between BP and the reviewer. There are four common ways to achieve
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: By copying the prediction data to a shared folder location
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By uploading the prediction data through programmatic or API calls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By storing the prediction data in a database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By sending data through email or other messaging services
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding list isn’t exhaustive; for instance, an FTP could be used. However,
    using a shared folder and API calls are the most common ways to share the data
    needed to correct predictions between BP and the reviewer.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to consider whether there’s an SLA concern, meaning that reviews
    must be completed within a certain timeframe to ensure that the rest of the processing
    can complete on time. If that is the case, we may want to choose a review method
    with inherent notification, such as email or instant messaging.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing predictions through shared folders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sharing data between humans and digital workers through *shared folders* is
    a common RPA practice. However, when shared folders are used to review ML predictions,
    the design becomes more complex, as there are more factors to consider. Some examples
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What *file format* (user interface) is most appropriate? Examples include plain
    text files, Excel files, HTML documents, and so on. This depends on how much and
    what kind of data needs to be presented to the reviewer. If we’re verifying whether
    a block of text has been labeled correctly, a text file might be good enough.
    If we have tabular input data, such as shop ID, product ID, price, and so on,
    Excel might be more appropriate. If we need to display an image to the reviewer,
    we can consider using a web-based interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How many predictions* belong in each file? Is it just one, or can we batch
    reviews together? This depends on the volume of review cases, the complexity of
    the review process, SLAs, and the number of people available to review predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does BP know whether the review has been *completed*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore these design considerations by expanding **Example 1 – Random
    Sampling** Process to include shared folders as the interface to transfer prediction
    data between the reviewers and BP.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – copying data through shared folders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In **Example 1 – Random Sampling** Process, we predict whether a customer will
    churn based on their characteristics, such as job category, and the number of
    points they have in the loyalty plan. Since this is tabular data, we’ll choose
    to present this data for review through *Excel*.
  prefs: []
  type: TYPE_NORMAL
- en: With a random sampling rate of 33.33%, many predictions will need to be reviewed.
    The business also believes that reviewing a single case is not straightforward.
    Based on these two factors, the team has decided to *batch predictions into five
    per Excel file* so that the review work can be spread among multiple reviewers,
    while not overwhelming any one reviewer by having them to do too many reviews
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal for this example is to see how we can build a prediction review scheme
    that involves using shared folders and batching multiple predictions into Excel
    files. At a high level, we’ll be going through the following six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Examine the contents of the example’s Release file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See how predictions needing review are first written into an Excel file in a
    temporary folder, and how the Excel file can be moved to a “ready to review” folder
    once the batch size is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See how reviewers can review predictions in the Excel file, and inform BP that
    the review is complete by copying the file to a “completed review” folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understand the BP logic to recognize and further work on Items that have completed
    human review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See how to copy leftover reviews in the temporary folder if the full batch size
    isn’t met before the Session ends.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example to see the result in the Control Room.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the .bprelease contents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This example is pre-built and equivalent to the Process in *Example 1 – Random
    Sampling* with a number of additions. The important additions will be highlighted
    as we go through this example together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that you’ve imported one Process, one Work Queue, and five Environment
    Variables. One of the Environment Variables, **Ch4 Example 3 Random Sampling Target**,
    is the same as in *Example 1*. It stores the percentage of predictions (**33.33**)
    that we want to review. **Ch4 Example 3 Review Batch Size** stores the number
    of predictions we want to write into each Excel file (**5**). Three of the other
    Environment Variables are file paths and are used to facilitate the data sharing
    between BP and the reviewer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – The Release file contents](img/B18416_04_15..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – The Release file contents
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to the shared folder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two of the five Environment Variables are file paths used to store the Excel
    files that will be presented to the reviewer. **Ch4 Example 3 To Review Folder
    Path** is a file share folder that *should be accessible to the reviewers*. The
    Process will copy Excel files into this folder, indicating that predictions need
    to be reviewed. Reviewers should regularly check this folder to see whether there
    are any Excel files within.
  prefs: []
  type: TYPE_NORMAL
- en: '`Ch4 Example 3 Temporary Review Folder Path` is a folder that’s needed due
    to the complications introduced by batching multiple reviews into a single Excel
    file. Reviewers don’t need access to this folder, so it can be any *local folder*
    that BP can write to. The purpose of this temporary folder is to prevent reviewers
    from opening and locking the Excel files before all five reviews are written to
    the file. Each prediction will first be written to this temporary path until it
    reaches the batch size, after which the Excel file will be moved to a folder that
    is accessible to the reviewer.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When writing the prediction to the Excel file, we include all of the input data
    that is fed into the ML algorithm. In addition, we also need to provide the *Item
    ID* so that we can map this prediction back to the Item in the BP Work Queue.
    Reviewers need to be informed to not modify the *Item ID* that appears in the
    spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at the Process diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: Open **Example 3 - Random Sampling with Shared Folders** in the *Ch4* Group
    in the Process Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `Defer Item for Manual` `Review` Page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the Block named *Do not Reset to Initial Value*. Writing into the same
    Excel file between different Work Queue Items for batching requires two Data Items
    that do not reset to their Initial Value when the Page is rerun. **Current Batch
    Count** keeps track of how many predictions have been written into the current
    Excel file. **Excel File Name** stores a unique filename that contains both the
    name of the Runtime Resource and the timestamp. Including the name of the Runtime
    Resource in the Excel filename is an important part of the logic design, as it
    allows the Digital Worker to know which files it has created in the shared folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Two Data Items that don’t reset their values](img/B18416_04_16..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Two Data Items that don’t reset their values
  prefs: []
  type: TYPE_NORMAL
- en: 'Examine the Block named `Writing the Temporary Excel file`. The logic inside
    the Block writes one prediction to a *temporary* file. This temporary file should
    be inaccessible to the reviewer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Writing one prediction to a temporary file](img/B18416_04_17..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Writing one prediction to a temporary file
  prefs: []
  type: TYPE_NORMAL
- en: The *Yes* path of the *Create New Excel File?* Decision Stage is used to create
    a new Excel file. The *No* path is for writing into an existing Excel file for
    batching reviews together. These are done in the *temporary* folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the four Stages that immediately follow the *Writing the Temporary
    Excel File* Block. After writing the temporary file to the shared folder, we increment
    the `Current Batch Count` and check to see whether the desired batch size (stored
    as an Environment Variable) has been reached. If so, we move the file from `Ch4
    Example 3 Temporary Review Folder Path` to `Ch4 Example 3 To Review Folder Path`,
    which is the shared folder that is accessible to the reviewer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Move the file from the temporary local folder to a reviewer-accessible
    one](img/B18416_04_18..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Move the file from the temporary local folder to a reviewer-accessible
    one
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what the reviewer does
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section of the example explains what reviewers are expected to do. We
    won’t be performing any actions here. Reviewers should regularly monitor **Ch4
    Example 3 To Review Folder Path** for new Excel files. Reviewers should open the
    file and change the **label** column (column I in the following figure) to the
    correct label. They can leave the label the same if they believe that the prediction
    is accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Reviewers should open the Excel file and edit the label column](img/B18416_04_19..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Reviewers should open the Excel file and edit the label column
  prefs: []
  type: TYPE_NORMAL
- en: After editing and saving the Excel file, reviewers should cut/paste the file
    into the path specified by the **Ch4 Example 3 Completed Review Folder Path**
    Environment Variable. The BP Process will check this folder, loop through all
    of the Excel files and their rows, and change each of the Item’s Statuses to allow
    processing to continue.
  prefs: []
  type: TYPE_NORMAL
- en: If we need to allow a Work Queue Item to bypass HITL review, we can change its
    *Status* manually in the Control Room and modify its *Deferral* time to an earlier
    **DateTime**.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for reviewed predictions and updating Item Statuses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s understand the logic that checks for completed reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the `Main Page` of the Process. See that there’s a Page named `Check
    for Reviewed Predictions` that appears before **Get** **Next Item**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Checking for completed reviews happens before the Work Queue
    Item processing](img/B18416_04_20..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Checking for completed reviews happens before the Work Queue Item
    processing
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Check for Reviewed Predictions` Page. This Page loops through all
    of the Excel files in **Ch4 Example 3 Completed Review Folder** and reads all
    of the rows. It then tries to *lock* each Item based on the Item ID so that its
    *Status* can be updated. After updating the Status, the Item is unlocked, allowing
    it to be picked up in the Work Queue again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate the four Stages shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Lock the Item, update the Status, and unlock it](img/B18416_04_21..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Lock the Item, update the Status, and unlock it
  prefs: []
  type: TYPE_NORMAL
- en: If we successfully update the *Status* of an Item, a **Rewrite Excel Needed**
    Flag is set, indicating that the Process will need to remove some rows (reviewed
    predictions) from the Excel file. Removing a row from the completed Excel file
    means that the Item can continue the processing steps post-review.
  prefs: []
  type: TYPE_NORMAL
- en: If we’re unable to lock the Item, we set a different Flag, `Failed to Lock`,
    saying that we encountered a problem with this Excel file. Any Items that we fail
    to lock are added to a new Collection for tracking purposes. The Items that we
    fail to update the Status for will not be deleted from the Excel file so that
    they can be checked again on future Session runs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Store the items that fail to lock into a Collection](img/B18416_04_22..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Store the items that fail to lock into a Collection
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the two Stages shown in the following figure. If the **Failed to Lock**
    flag indicates that we’ve successfully updated *all* of the Item Statuses for
    this Excel file, delete the Excel file from the folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Delete the file if we successfully update the Statuses of all
    five predictions](img/B18416_04_23..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Delete the file if we successfully update the Statuses of all
    five predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the six Stages shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Remove Excel rows where the Status is updated successfully](img/B18416_04_24..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Remove Excel rows where the Status is updated successfully
  prefs: []
  type: TYPE_NORMAL
- en: If the Process fails to update the Status for some of the Items, overwrite the
    Excel file so that it contains only those failed rows (which were separately saved
    in *Figure 4**.22*). This removes the predictions from the Excel file where the
    Status was successfully updated. On the next Session run, this `Check for Reviewed
    Predictions` Page will be executed, and the Process will attempt to update the
    Status of those Items again.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up files in the temporary folder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It’s possible for Excel files to get stuck in `Close` `Down` page:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Close Down` Page. See that there’s a `Move Temporary Files` Page right
    before End.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `Move Temporary Files` Page. This Page loops through all of the files
    in the temporary folder and moves them only if they were created by this specific
    Runtime Resource. Files in this folder that are created by other Runtime Resources
    will not be moved. This is achieved by *checking for the full Runtime Resource
    name in the Excel* *files’ filenames*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.25 – The Move Temporary Files Page](img/B18416_04_25..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – The Move Temporary Files Page
  prefs: []
  type: TYPE_NORMAL
- en: Run the Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve now seen an implementation of HITL review with BP using Excel and shared
    folders. Next, we need to test the Process. When run, 30 customers will be randomly
    generated and the algorithm predicts whether they will churn or not. At a random
    sampling rate of 33.33%, we expect roughly 10 predictions to need manual review.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When testing *Example 3*, do *not* mix between running in the Control Room and
    the Process Studio. This is because the Process Studio appends `_DEBUG` to the
    end of the Runtime Resource name. In the `Move Temporary Files` and `Defer Item
    for Manual Review` Pages, we use the Runtime Resource name in the Excel filename.
    If we run the Process once in the Process Studio and once in the Control Room,
    the Runtime Resource names will not match up and the `Move Temporary Files` Page
    will not move some of the Excel files to the review folder.
  prefs: []
  type: TYPE_NORMAL
- en: Start **Example 3 – Random Sampling with Shared Folders** Process in the *Ch4*
    Group from the Control Room. Wait for the Session to complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the Control Room and click on **Chapter 4** **Example 3 Queue**. There
    should be numerous Items that are waiting for manual review. Make a note of the
    number of Items that need review. In the following figure, there are seven:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Count the Items waiting for manual review](img/B18416_04_26..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Count the Items waiting for manual review
  prefs: []
  type: TYPE_NORMAL
- en: Open the folder corresponding to **Ch4 Example 3 To Review Folder Path** in
    Windows. The number of Excel files in this folder should be the *(number of items
    that need review)/5*, rounded up. For example, if you have seven Items that need
    manual review, we should have two Excel files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open each Excel file and ensure that the cumulative number of rows in each file
    (excluding the header) equals the number of Items that need review. Based on the
    screen capture in *step 2*, our first file should have five rows and our second
    file should have two.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cut and paste all of the Excel files in **Ch5 Example 3 To Review Folder Path**
    to **Ch5 Example 3 Completed Review Folder Path**. This simulates completing all
    of the ML prediction reviews.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visit the **System** area of BP. Change the **Ch4 Example 3 Random Sampling
    Target** Environment Variable from 33.33 to 0 and click **Apply**. This turns
    off random sampling for the next Session run, meaning that none of the 30 new
    Items will need to be manually reviewed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for the Deferral time to pass for all of the deferred Work Queue Items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Process again from the Control Room. A new batch of 30 customers will
    be created and predicted against. Wait for the Session to finish.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the Control Room and click on **Chapter 4** **Example 3 Queue**. All 60
    Work Queue Items should be marked as Complete, with no items needing manual review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open each of the folders that correspond to **Ch4 Example 3 To Review Folder
    Path**, **Ch4 Example 3 Temporary Review Folder Path**, and **Ch4 Example 3 Completed
    Review Folder** in Windows. All three folders should be empty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, we showed how to implement HITL reviews using shared folders
    in BP. Due to the tabular format of the prediction input data, we chose Excel
    as the interface to review predictions. Multiple prediction reviews were batched
    together and saved into a single file. We also designed a shared folder structure
    that prevents the Digital Worker and the human reviewer from opening the same
    Excel files concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing, reviewers manually move Excel files into a *completed* folder.
    BP reads the completed Excel files and changes the Status of Work Queue Items
    so that they can be picked up for processing again. Finally, we also saw how we
    can set the random sampling Environment Variable to 0, turning off random sampling
    altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key difference between RPA and IA lies in IA’s use of ML algorithms. This
    shifts our process from having deterministic outcomes to non-deterministic ones.
    As automated outcomes become less predictable, our business risk increases as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to the idea of bringing humans back into the loop to verify and correct
    a portion (not necessarily all) of the ML predictions. This gives us an ongoing
    measure of how accurate the ML predictions are – given that accuracy is known
    to drift over time. It also gives us a way to operationally reduce risk and hedge
    against future regulatory risk.
  prefs: []
  type: TYPE_NORMAL
- en: There are two primary methods used in real life to determine which predictions
    need reviewing. The first is random sampling, where a fixed percentage of all
    predictions is chosen for review. Random sampling doesn’t look at the predicted
    values in any way. The next is thresholding, which looks at the confidence score
    and sometimes the predicted label. If a prediction’s confidence score is under
    the threshold value, it’s marked for review. If the confidence score is above,
    automated processing continues.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an appropriate threshold is a complex topic. A single label can have
    multiple threshold intervals. We can also have different thresholds for different
    labels. A reasonable way to set the threshold values is through experimentation
    and discussion with the business users. Threshold values will never be perfect
    – trade-offs between the desired accuracy, the impact of working a case based
    on an incorrect prediction, and the number of cases you want to process without
    manual intervention must be made.
  prefs: []
  type: TYPE_NORMAL
- en: We can design other schemes to determine which cases need human review. For
    example, we could have a formula that changes the random sampling rate based on
    the number of pending items in the Work Queue if we have a fixed number of reviewers
    with SLAs to meet. However, I haven’t encountered anything more complicated than
    standard random sampling and thresholding being used after examining over 100
    real-life IA use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of both random sampling and thresholding were provided, and their design
    elements were discussed. In our design, we used Statuses, Deferrals, shared folders,
    and a single Work Queue to achieve an end-to-end HITL solution for reviewing ML
    predictions. In the next chapter, we will move away from the *single*-Work Queue
    design and discuss how and why we might want to design an IA process with *multiple*
    Work Queues.
  prefs: []
  type: TYPE_NORMAL
