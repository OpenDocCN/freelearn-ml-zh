- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LightGBM MLOps with AWS SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B16690_08.xhtml#_idTextAnchor134), *Machine Learning Pipelines
    and MLOps with LightGBM*, we built an end-to-end ML pipeline using scikit-learn.
    We also looked at encapsulating the pipeline within a REST API and deployed our
    API to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will look at developing and deploying a pipeline using **Amazon
    SageMaker**. SageMaker is a complete set of production services for developing,
    hosting, monitoring, and maintaining ML solutions provided by **Amazon Web** **Services**
    (**AWS**).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll expand our capabilities with ML pipelines by looking at advanced topics
    such as detecting bias in a trained model and automating deployment to fully scalable,
    serverless web endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following main topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to AWS and SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model explainability and bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an end-to-end pipeline with SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dives deep into building ML models and pipelines using Amazon SageMaker.
    You need access to an Amazon account, and you must also configure a payment method.
    Note that running the example code for this chapter will incur costs on AWS. The
    complete notebooks and scripts for this chapter are available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9).
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to AWS and SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides a high-level overview of AWS and delves into SageMaker,
    AWS’ ML offering.
  prefs: []
  type: TYPE_NORMAL
- en: AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS is one of the leading players in the global cloud computing marketplace.
    AWS offers many cloud-based products and services, including databases, **machine
    learning** (**ML**), analytics, networking, storage, developer tools, and enterprise
    applications. The idea behind AWS is to offer businesses an affordable and scalable
    solution to their computing needs, regardless of their size or industry.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of AWS is elasticity, meaning servers and services can be stopped
    and started quickly and at will, scaling from zero machines to thousands. The
    elasticity of the services goes hand in hand with its primary pricing model of
    pay-as-you-go, meaning customers only pay for the services and resources they
    use without any upfront costs or long-term contracts. This elasticity and pricing
    allow businesses to scale computing needs as needed, on an ad hoc and granular
    level, and then only pay for what they use. This approach has transformed how
    businesses scale IT resources and applications, enabling them to react quickly
    to changing business needs without incurring the heavy costs traditionally associated
    with hardware and software procurement and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is the global reach of AWS. AWS services are available in
    many regions across the globe. Regions are geographically separated, and each
    region is further divided into availability zones. The region-zone setup allows
    users to create globally distributed and redundant infrastructure to maximize
    resilience and architect for disaster recovery. The regional data centers also
    allow users to create servers and services close to end users, minimizing latency.
  prefs: []
  type: TYPE_NORMAL
- en: Core services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core AWS services provide computing, networking, and storage capability.
    AWS’s compute services include **Amazon Elastic Compute Cloud** (**EC2**), which
    offers configurable virtual machines to customers, and **AWS Lambda**, a serverless
    compute platform that allows you to run code without the need to provision and
    manage servers. In ML, both EC2 instances and Lambda functions are often used
    to train and validate or serve models via API endpoints. The elastic nature of
    EC2 servers allows ML engineers to scale up training servers to many thousands,
    which can significantly speed up training or parameter-tuning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: AWS’s storage and database services, such as **Amazon Simple Storage Service**
    (**S3**) and **Amazon RDS** (**Relational Database Service**), offer reliable,
    scalable, and secure data storage solutions. These services manage storage infrastructure
    and offer high-level features such as backups, patch management, and vertical
    and horizontal scaling. S3 is a widely used service for data engineering and ML.
    S3 offers low-cost, highly redundant secure storage that scales beyond exabytes.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also offers data warehousing solutions with **Amazon Redshift**. Large enterprises
    frequently use Redshift as a warehouse or the basis of a data lake, meaning it’s
    often a data source for ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also offers networking services to help businesses meet complex networking
    and isolation needs. **AWS Direct Connect** allows customers to set up a dedicated
    network connection from a customer’s site to the AWS cloud. Routing and name servers
    can be managed with Amazon Route 53, a flexible and scalable **Domain Name System**
    (**DNS**) service.
  prefs: []
  type: TYPE_NORMAL
- en: However, chief among the network services is **Amazon Virtual Private Cloud**
    (**VPC**). VPCs offer customers the ability to configure completely isolated virtual
    networks. Customers can granularly configure subnetworks, routing tables, address
    ranges, gateways, and security groups. VPCs allow users to isolate their environment
    and cloud resources and control inbound and outbound traffic for increased security
    and privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A critical piece of any infrastructure equation is security. In terms of security,
    AWS provides a highly secure, scalable, and flexible cloud computing environment.
    AWS’s security services, including **AWS Identity and Access Management** (**IAM**)
    and **Amazon Security Hub**, help customers protect their data and applications
    by implementing robust security measures.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also complies with multiple international and industry-specific compliance
    standards, such as GDPR, HIPAA, and ISO 27001\. Further, in terms of data governance,
    AWS makes it easy to comply with data residency and privacy requirements. Due
    to the regional structure of AWS, data can remain resident in specific countries,
    while engineers have access to the full suite of AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS also offers services focused on ML and **artificial intelligence** (**AI**).
    Among these are many fully managed services for specific ML tasks. **AWS Comprehend**
    offers many **natural language processing** (**NLP**) services, such as document
    processing, named entity recognition, and sentiment analysis. **Amazon Lookout**
    is a service for anomaly detection in equipment, metrics, or images. Further,
    **Amazon Rekognition** offers services for machine vision use cases such as image
    classification and facial recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Of particular interest to us is **Amazon SageMaker**, a complete ML platform
    that allows us to create, train, and deploy ML models in the Amazon cloud. The
    following section discusses SageMaker in detail.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon SageMaker** is an end-to-end ML platform that allows data scientists
    to work with data and develop, train, deploy, and monitor ML models. SageMaker
    is fully managed, so there is no need to provision or manage servers.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary appeal of Amazon SageMaker lies in its comprehensive nature as a
    platform. It encompasses all aspects of the ML process, including data labeling,
    model building, training, tuning, deployment, management, and monitoring. By taking
    care of these aspects, SageMaker allows developers and data scientists to focus
    on the core ML tasks instead of managing the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: As we have discussed, the ML life cycle starts with data gathering, which often
    requires manual data labeling. For this, SageMaker provides a service called **SageMaker
    Ground Truth**. This service makes it easy to annotate ML datasets efficiently.
    It can significantly reduce the time and costs typically associated with data
    labeling by using automated labeling workflows, and it also offers a workforce
    for manual data labeling tasks. Further, SageMaker also provides the **Data Wrangler**
    service, which helps with data preparation and **exploratory data analysis** (**EDA**).
    Data Wrangler provides functionality to query data from S3, Redshift, and other
    platforms and then cleanse, visualize, and understand the data from a single visual
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker provides a fully managed service for the model training phase that
    can handle large-scale, distributed model training via **Training Jobs**. The
    service is designed to be flexible and adaptable, allowing users to optimize their
    ML models as needed. Users only need to specify the location of their data, typically
    S3 and the ML algorithm, and SageMaker takes care of the rest of the training
    process. The model training service fully leverages the elastic nature of the
    underlying AWS infrastructure: many servers can be created quickly to perform
    training jobs and discarded after training is complete to save costs.'
  prefs: []
  type: TYPE_NORMAL
- en: This paradigm also extends to hyperparameter tuning. To simplify hyperparameter
    optimization, SageMaker provides an automatic model-tuning feature. Many tuning
    algorithms are provided, such as Optuna or FLAML, and tuning can be run across
    multiple servers.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker also has support for a more fully AutoML experience via **SageMaker
    Autopilot**. Autopilot is a service that enables automatic model creation. A user
    only needs to provide the raw data and set the target; then, Autopilot automatically
    explores different solutions to find the best model. Autopilot provides complete
    visibility into the process so that data scientists can understand how the model
    is created and make any necessary adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: Once a model has been trained and optimized, it must be deployed. SageMaker
    simplifies this process by providing a one-click deployment process. Users can
    quickly deploy their models to production with auto-scaling capabilities without
    worrying about the underlying infrastructure. This deployment autoscaling capability
    allows users to set metrics-based policies that increase or decrease backing servers.
    For instance, the deployment can be scaled up if the number of invocations within
    a period exceeds a specific threshold. SageMaker ensures the high availability
    of models and allows for A/B testing of models to compare different variants and
    decide on the best one. SageMaker also supports multi-model endpoints, allowing
    users to deploy multiple models on a single endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker also provides capabilities to monitor the model’s performance
    and conduct analysis once deployed. **SageMaker Model Monitor** monitors the quality
    of deployed models continuously (for real-time endpoints) or in batches (for asynchronous
    jobs). Alerts can be defined to notify the user if metric thresholds are exceeded.
    Model Monitor can monitor data drift and model drift based on metrics such as
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, SageMaker is both a platform within AWS and a software SDK. The SDK
    is available in both Python and R. The SageMaker SDK provides a range of built-in
    algorithms and frameworks, including support for the most popular algorithms in
    the ML community, such as XGBoost, TensorFlow, PyTorch, and MXNet. It also supports
    a marketplace where users can choose from a vast collection of algorithm and model
    packages shared by AWS and other SageMaker users.
  prefs: []
  type: TYPE_NORMAL
- en: A noteworthy part of SageMaker that simplifies one of the most important aspects
    of model development (bias and fairness) is **SageMaker Clarify**.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Clarify
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon SageMaker Clarify is a tool that provides greater transparency into ML
    models. SageMaker Clarify aims to assist in understanding how ML models make predictions,
    thereby enabling model explainability and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary features of SageMaker Clarify is its capacity to provide
    model interpretability. It helps developers understand the relationships between
    the input data and the model’s predictions. The service generates feature attributions
    that show how each feature in the dataset influences predictions, which can be
    critical in many domains, especially those where it’s vital to understand the
    reasoning behind a model’s prediction. In addition to providing insight into individual
    predictions, SageMaker Clarify offers global explanatory capabilities. It measures
    the importance of input features on a model’s predictions in aggregate across
    the whole dataset. Feature impact analysis allows developers and data scientists
    to understand the overall behavior of a model, helping them interpret how different
    features drive model predictions on a global level.
  prefs: []
  type: TYPE_NORMAL
- en: Further, Clarify can help identify potential bias in trained models. The service
    includes pre-training and post-training bias metrics that help us understand if
    a model favors certain groups unfairly. It’s best practice to check all new models
    for bias, but it is also imperative in regulated industries such as finance or
    healthcare, where biased predictions can have severe consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Clarify provides model interpretability by using an advanced technique known
    as **SHapley Additive** **exPlanations** (**SHAP**).
  prefs: []
  type: TYPE_NORMAL
- en: SHAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SHAP is a game theoretic approach to interpreting the output of any ML model
    [1]. SHAP aims to provide an understanding of the impact of individual features
    on a model’s overall prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, SHAP values assess the effect of a particular feature value by
    contrasting it with a baseline value for that feature, highlighting its contribution
    to the prediction. A SHAP value is a fair contribution allocation from each feature
    to the prediction for each instance. SHAP values are rooted in cooperative game
    theory, representing a solution to the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: “*Given the difference a feature makes in predicting an outcome, what portion
    of that difference is attributable to* *each feature?*”
  prefs: []
  type: TYPE_NORMAL
- en: These values are calculated using the concept of Shapley values from game theory.
    A Shapley value determines the significance of a feature by contrasting a model’s
    predictions with the presence and absence of that feature. Yet, as the sequence
    in which a model encounters features can affect its prediction, Shapley values
    consider all possible orderings. Then, it assigns an importance value to a feature
    so that it equals the average marginal contribution of that feature across all
    possible coalitions.
  prefs: []
  type: TYPE_NORMAL
- en: There are several advantages to using SHAP for model interpretation. First,
    it offers consistency in interpretation. If the contribution of a feature changes,
    the attributed importance of that feature changes proportionally.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, SHAP guarantees local accuracy, which means the sum of the SHAP values
    for all features would equal the difference between the prediction and the average
    prediction for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great way to visualize SHAP values is by using SHAP summary plots. These
    plots provide a bird’s-eye view of feature importance and what is driving it.
    They plot all the SHAP values for a feature on a graph for easy visualization.
    Each point on the graph represents a SHAP value for a feature and an instance.
    The position on the Y-axis is determined by the feature and on the X-axis by the
    SHAP value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Local explanation example for the Census Income dataset. Bars
    indicate SHAP values or the relative importance of each feature in predicting
    this specific instance](img/B16690_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Local explanation example for the Census Income dataset. Bars indicate
    SHAP values or the relative importance of each feature in predicting this specific
    instance
  prefs: []
  type: TYPE_NORMAL
- en: In the context of SageMaker Clarify, the service generates a set of SHAP values
    for each instance in your dataset when you run a clarification job. SageMaker
    Clarify can also provide global feature importance measures by aggregating SHAP
    values across the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP values can help you understand complex model behavior, highlight potential
    issues, and improve your model over time. For example, by examining SHAP values,
    you might discover that a specific feature has a more significant effect on your
    model’s predictions than expected, prompting you to explore why this might happen.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at AWS and, more specifically, what the AWS ML family
    of services, SageMaker, offers. The functionality available in SageMaker, such
    as model explainability, bias detection, and monitoring, are components we have
    yet to implement in our ML pipelines. In the next section, we’ll look at building
    a complete end-to-end LightGBM-based ML pipeline, including these crucial steps,
    using SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Building a LightGBM ML pipeline with Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we’ll use for our case study of building a SageMaker pipeline is
    the Census Income dataset from *Chapter 4*, *Comparing LightGBM, XGBoost, and
    Deep Learning*. This dataset is also available as a SageMaker sample dataset,
    so it’s easy to work with on SageMaker if you are getting started.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline we’ll build consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training and tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bias and explainability checks using Clarify.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model registration within SageMaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model deployment using an AWS Lambda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s a graph showing the complete pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – SageMaker ML pipeline for Census Income classification](img/B16690_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – SageMaker ML pipeline for Census Income classification
  prefs: []
  type: TYPE_NORMAL
- en: Our approach is to create the entire pipeline using a Jupyter Notebook running
    in SageMaker Studio. The sections that follow explain and go through the code
    for each pipeline step, starting with setting up the SageMaker session.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a SageMaker session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps assume you have already created an AWS account and set
    up a SageMaker domain to get started. If not, the following documentation can
    be referenced to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prerequisites: [https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Onboarding to a SageMaker domain: [https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We must initialize the SageMaker session and create S3, SageMaker, and SageMaker
    Runtime clients via `boto3` to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will use Amazon S3 to store our training data, source code, and all data
    and artifacts created by the pipeline, such as the serialized model. Our data
    and artifacts are split into a read bucket and a separate write bucket. This is
    a standard best practice as it separates the concerns for data storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker sessions have the concept of a default S3 bucket for the session.
    If no default bucket name is supplied, one is generated, and the bucket is created
    for you. Here, we’re grabbing a reference to the bucket. This is our output or
    write bucket. The read bucket is a bucket we’ve created previously that stores
    our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The source code, configuration, and output of each of the steps in the pipeline
    are captured in folders within our S3 write bucket. It’s useful to create variables
    for each S3 URI to avoid errors when repeatedly referring to data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'SageMaker needs us to specify the compute instance types we want to use when
    running the jobs for training, processing, Clarify, and prediction. In our example,
    we’re using `m5.large` instances. Most EC2 instance types can be used with SageMaker.
    However, a few special instance types that support GPUs and deep learning frameworks
    are also available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: SageMaker uses standard EC2 instances for training but runs specific Docker
    images on the instances to provide ML functionality. Amazon SageMaker provides
    many prebuilt Docker images for various ML frameworks and stacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker SDK also provides a function to search for images that are compatible
    with the instance type we need within the AWS region we are using. We can search
    for an image using `retrieve`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We specify `None` for the framework parameter as we manage the LightGBM installation
    ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parameterize our pipeline, we must define SageMaker workflow parameters
    from the `sagemaker.workflow.parameters` package. Wrappers are available for various
    parameter types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With our pipeline parameters, S3 data paths, and other configuration variables
    set, we can move on to creating our pipeline’s preprocessing step.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Setting up our preprocessing step has two parts: creating a Python script that
    performs the preprocessing and creating a processor that is added to the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script we’ll be using is a regular Python script with a main function.
    We’ll use scikit-learn to do our preprocessing. The preprocessing script hasn’t
    been entirely reproduced here but is available in our source code repository.
    Notably, when the pipeline executes the step, the data is retrieved from S3 and
    added to a local staging directory on the preprocessing instance. From here, we
    can read the data using standard pandas tooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, after processing is complete, we can write the results to a local
    directory, from which SageMaker retrieves it and uploads it to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With a preprocessing script defined, we need to upload it to S3 for the pipeline
    to be able to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define the preprocessing step as follows. First, we must create an `SKLearnProcessor`
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`SKLearnProcessor` handles the processing task for jobs that require scikit-learn.
    We specify the scikit-learn framework version and the instance type and count
    we defined earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The processor is then added to `ProcessingStep` for use in the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`inputs` and `outputs` are defined using the `ProcessingInput` and `ProcessingOutput`
    wrappers, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`ProcessingStep` takes our scikit-learn processor and the inputs and outputs
    for the data. The `ProcessingInput` instances define the S3 source and local directory
    destination to facilitate copying the data (these are the same local directories
    our preprocessing script uses). Similarly, the `ProcessingOutput` instances take
    the local directory source and S3 destinations. We also set job arguments, which
    are passed to the preprocessing script as CLI arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: Having set up the preprocessing step, we can move on to training.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We define a training script in the same way as a preprocessing script: a Python
    script with a main function that uses our standard Python tools, such as scikit-learn,
    to train a LightGBM model. However, we also need to install the LightGBM library
    itself.'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to installing the library is building it into a Docker image
    and using it as our training image in SageMaker. This is the canonical way of
    managing environments in SageMaker. However, it entails significant work and includes
    the long-term need to maintain the image over time. Alternatively, if we only
    need to install a handful of dependencies, we can do that directly from our training
    script, as shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must define a helper function to install packages and then use it to install
    LightGBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This also has the advantage that we install the latest version (or a specific
    version) every time we run training.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the package installed, the rest of the training script trains a standard
    `LGBMClassifier` on the data prepared by the preprocessing step. We can set up
    or train data and parameters from the arguments to the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must do standard scikit-learn cross-validation scoring, fit the model
    to the data, and output the training and validation scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown here, the script accepts CLI arguments to set hyperparameters. This
    is used by the hyperparameter tuning step to set parameters during the optimization
    phase. We can use Python’s `ArgumentParser` for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can also see that we log training and validation F1 scores, allowing SageMaker
    and CloudWatch to pull the data from logs for reporting and evaluation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to write out the results of the training in a JSON document.
    The results can then be used in subsequent pipeline processes and are shown as
    output from the job in the SageMaker interface. The JSON document is stored on
    disk, along with the serialized model file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As with the preprocessing step, the results are written to a local directory,
    where SageMaker picks them up and copies them to S3.
  prefs: []
  type: TYPE_NORMAL
- en: With the script defined, we can create the tuning step in the pipeline, which
    trains the model and tunes hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must define a SageMaker `Estimator` that, similar to `SKLearnProcessor`,
    encapsulates the configuration for training, including a reference to the script
    (on S3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then define our SageMaker `HyperparameterTuner`, which performs the
    actual hyperparameter tuning. Similar to Optuna or FLAML, we must specify valid
    ranges for the hyperparameters using SageMaker wrappers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`HyperparameterTuner` can be set up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'SageMaker supports many strategies for hyperparameter tuning, including Hyperband
    tuning. More information can be found in the documentation for hyperparameter
    tuning: [https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.xhtml).
    Here, we used random search, with a maximum job size of 20\. It’s here that AWS’
    elastic infrastructure can be used to significant benefit. If we increase the
    training instance count, SageMaker automatically distributes the training job
    across all machines. Provisioning additional machines has some overhead and increases
    cost, but it can also majorly reduce tuning time if we run thousands of trials.'
  prefs: []
  type: TYPE_NORMAL
- en: The tuner’s metric definitions define regular expressions that are used to pull
    the results metrics from the logs, as we showed in the training script earlier.
    The parameter optimization framework optimizes relative to the metrics defined
    here, minimizing or maximizing the metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the hyperparameter tuner defined, we can create a `TuningStep` for inclusion
    into the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The pipeline steps we’ve defined thus far prepare data and produce a trained
    model that’s serialized to S3\. The pipeline’s next step is to create a SageMaker
    `Model` that wraps the model and is used for the evaluation, bias, and inference
    steps. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `Model` instance encapsulates all the necessary configurations to deploy
    and run the model. We can see that `model_data` is taken from the top-performing
    model resulting from the tuning step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline steps we’ve defined so far will produce processed data and train
    a tuned model. The layout for the processed data in S3 is shown in *Figure 9**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – S3 directory layout for the results of the processing jobs](img/B16690_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – S3 directory layout for the results of the processing jobs
  prefs: []
  type: TYPE_NORMAL
- en: We could proceed to the deployment step if we needed to. However, we will follow
    best practice and add quality gates to our pipeline that check the model’s performance
    and bias and produce insights into its function.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation, bias, and explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve seen the general pattern of adding steps to a SageMaker pipeline:
    set up the configuration using SageMaker’s configuration classes and then create
    the relevant pipeline step.'
  prefs: []
  type: TYPE_NORMAL
- en: Bias configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To add bias checks to our pipeline, we must create the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`BiasConfig` describes which facets (features) we want to check for bias. We’ve
    selected `Sex` and `Age`, which are always essential facets to check when working
    with demographic data.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ModeLBiasCheckConfig` wraps the data configuration, model configuration, and
    bias confirmation for the bias check step. It also sets the method to use for
    the bias check. Here, we use the **difference in positive proportions in predicted**
    **labels** (**DPPL**).'
  prefs: []
  type: TYPE_NORMAL
- en: The DPPL is a metric that’s used to gauge if a model predicts outcomes differently
    for varying facets of data. The DPPL is calculated as the difference between the
    proportion of positive predictions for facet “a” and facet “d.” It helps assess
    whether there’s bias in the model predictions after training by comparing them
    with the initial bias present in the dataset. For instance, if a model predicting
    eligibility for a home loan predicts positive outcomes for 70% of male applicants
    (facet “a”) and 60% for female applicants (facet “d”), the 10% difference could
    indicate bias against facet “d.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The DPPL formula is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: DPPL = q a ′ − q d ′
  prefs: []
  type: TYPE_NORMAL
- en: Here, q a ′ is the predicted proportion of facet “a” receiving a positive outcome,
    and q d ′  is the analogous proportion for facet “d.” For binary and multicategory
    facet labels, normalized DPPL values fall between [-1, 1], while continuous labels
    vary over the interval (-∞, +∞). A positive DPPL value suggests a higher proportion
    of positive predictions for facet “a” versus “d,” indicating a positive bias.
    Conversely, a negative DPPL indicates a higher proportion of positive predictions
    for facet “d,” signifying a negative bias. A DPPL near zero points to a relatively
    equal proportion of positive predictions for both facets, with a value of zero
    implying perfect demographic parity.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add the bias check to the pipeline using `ClarifyCheckStep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Explainability configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The configuration for explainability is very similar. Instead of creating `BiasConfig`,
    we must create `SHAPConfig`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Alongside `SHAPConfig`, we must create `ModelExplainabilityCheckConfig` to
    calculate the SHAP values and create an explainability report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything is then combined using `ClarifyCheckStep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we also need to evaluate our model against test data. The evaluation
    script is very similar to the training script, except it pulls the tuned model
    from S3 for scoring. The script consists of a main function with two steps. First,
    we must bootstrap the trained model and perform the scoring (in our case, calculating
    the F1 score):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must output the results to a JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation JSON is used for reporting and subsequent steps that rely on
    the evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and monitoring the LightGBM model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now ready to add our pipeline’s final steps for supporting deployment.
    The deployment part of the pipeline consists of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Registering the model in SageMaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A conditional check to validate that the model evaluation surpasses a minimum
    threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying a model endpoint using an AWS Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To deploy our model, we first need to register our model in SageMaker’s **Model
    Registry**.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker’s Model Registry is a central repository where you can manage and
    deploy your models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Model Registry provides the following core functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model versioning**: Every time a model is trained and registered, it’s assigned
    a version in the Model Registry. This helps you keep track of different iterations
    of your models, which is useful when you need to compare model performance, roll
    back to previous versions, or maintain reproducibility in your ML projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approval workflow**: The Model Registry supports an approval workflow, where
    models can be marked as “Pending Manual Approval,” “Approved,” or “Rejected.”
    This allows teams to effectively manage the life cycle of their models and ensure
    that only approved models are deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model catalog**: The Model Registry acts as a catalog where all your models
    are centrally stored and accessible. Each model in the registry has metadata associated
    with it, such as the training data used, hyperparameters, and performance metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While registering our model, we attach the metrics that were calculated from
    our evaluation step. These metrics are also used for model drift detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of drift are possible: **data drift** and **model drift**.'
  prefs: []
  type: TYPE_NORMAL
- en: Data drift refers to a change in the statistical distribution of the incoming
    data compared to our model’s training data. For example, if the training data
    had a male/female split of 60% to 40%, but the data used for prediction is skewed
    to 80% male and 20% female, it’s possible that drift occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift is a phenomenon where the statistical properties of the target variable,
    which the model tries to predict, change over time in unforeseen ways, causing
    model performance to degrade.
  prefs: []
  type: TYPE_NORMAL
- en: Both data and model drift can occur due to environmental changes, societal behaviors,
    product usage, or other factors not accounted for during model training.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker supports continuous monitoring of drift. SageMaker calculates the
    statistical distribution of both incoming data and the predictions we are making.
    Both are compared against the distributions present in the training data. Should
    drift be detected, SageMaker can produce alerts to AWS CloudWatch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can configure our metrics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for the drift metrics, we must set up `DriftCheckBaselines`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must create a model registration step with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Model validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The conditional check uses the evaluation data from the evaluation step to
    determine whether the model is suitable for deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created `ConditionStep` and compared the F1 score against a threshold
    of `0.9`. Deployment can proceed if the model has an F1 score higher than the
    threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment with AWS Lambda
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The deployment script is a standard AWS Lambda script in Python that defines
    a `lambda_handler` function that obtains a client connection to SageMaker and
    proceeds to create the model endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Notably, the Lambda function does not serve requests for the model. It only
    creates the model endpoint within SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In SageMaker, an **endpoint** is a web service to get predictions from your
    models. Once a model is trained and the training job is complete, you need to
    deploy the model to make real-time or batch predictions. Deployment in SageMaker
    parlance means setting up an endpoint – a hosted, production-ready model.
  prefs: []
  type: TYPE_NORMAL
- en: An endpoint in SageMaker is a scalable and secure RESTful API that you can use
    to send real-time inference requests to your models. Your applications can access
    an endpoint to make predictions directly via the REST API or AWS SDKs. It can
    scale instances up and down as needed, providing flexibility and cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker also supports multi-model endpoints, which can deploy multiple models
    on a single endpoint. This feature can significantly save on costs if many models
    are used infrequently or are not resource-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Lambda script defined, it can be incorporated into the pipeline using
    `LambdaStep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A model endpoint incurs cost as soon as it’s deployed for the duration of its
    deployment. Once you run your pipeline, an endpoint is created as a result. If
    you are only experimenting with or testing your pipeline, you should delete the
    endpoint once you’re done.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running the pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All of our pipeline steps are now in place, which means we can create the pipeline
    itself. The `Pipeline` construct takes the name and parameters we’ve already defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We must also pass all the steps we’ve defined as a list parameter and finally
    upsert the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the pipeline is done by calling the `start` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note the conditions we defined here. When running the pipeline for the first
    time, we must skip the model bias and explainability checks while registering
    new bias and explainability baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Both checks require an existing baseline to run (otherwise, there is no data
    to check against). Once baselines have been established, we can disable skipping
    the checks in subsequent runs.
  prefs: []
  type: TYPE_NORMAL
- en: More information on the model life cycle and creating baselines can be found
    at [https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the pipeline is executed, you can view the execution graph to see the
    status of each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Successful execution of the LightGBM Census Income pipeline](img/B16690_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Successful execution of the LightGBM Census Income pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the model itself registered in the Model Registry once the
    pipeline completes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – SageMaker Model Registry showing the approved Census Income
    model and the related endpoint](img/B16690_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – SageMaker Model Registry showing the approved Census Income model
    and the related endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'The bias and explainability reports can be viewed when a model is selected.
    *Figure 9**.6* shows the bias report for the model that was created by the pipeline.
    We can see a slight imbalance in the DPPL for sex, but less than the class imbalance
    in the training data. The report indicates there isn’t strong evidence for bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Bias report for the Census Income model. We can see a slight
    imbalance in the DPPL but less than the class imbalance in the training data](img/B16690_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Bias report for the Census Income model. We can see a slight imbalance
    in the DPPL but less than the class imbalance in the training data
  prefs: []
  type: TYPE_NORMAL
- en: 'The explainability report, as shown in *Figure 9**.7*, shows the importance
    of each feature in terms of SHAP values. Here, we can see that the **Capital Gain**
    and **Country** features are dominant regarding importance to predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Explainability report for the Census Income model showing the
    dominant importance of the Capital Gain and Country features](img/B16690_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Explainability report for the Census Income model showing the dominant
    importance of the Capital Gain and Country features
  prefs: []
  type: TYPE_NORMAL
- en: The bias and explainability reports can also be downloaded in PDF format, which
    can easily be shared with business or non-technical stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using the endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, our deployed model is not very useful if we can’t make any predictions
    using it. We can make predictions with the deployed model via REST calls or the
    Python SDK. Here is an example of using the Python SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We obtain a SageMaker `Predictor` using the endpoint name and the session. Then,
    we can call `predict`, passing a NumPy array (obtained from a test DataFrame in
    this case).
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have created a complete, end-to-end, production-ready pipeline
    using SageMaker. Our pipeline includes data preprocessing, automatic model tuning,
    bias validation, drift detection, and a fully scalable deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced AWS and Amazon SageMaker as a platform for building
    and deploying ML solutions. An overview of the SageMaker service was given, including
    the Clarify service, which provides advanced features such as model bias checks
    and explainability.
  prefs: []
  type: TYPE_NORMAL
- en: We then proceeded to build a complete ML pipeline with the SageMaker service.
    The pipeline includes all steps of the ML life cycle, including data preparation,
    model training, tuning, model evaluation, bias checks, explainability reports,
    validation against test data, and deployment to cloud-native, scalable infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Specific examples were given to build each step within the pipeline, emphasizing
    full automation, looking to enable straightforward retraining and constant monitoring
    of data and model processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter looks at another MLOps platform called **PostgresML**. PostgresML
    offers ML capabilities on top of a staple of the server landscape: the Postgres
    database.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *S. M. Lundberg and S.-I. Lee, A Unified Approach to Interpreting
    Model Predictions, in Advances in Neural Information Processing Systems 30, I.
    Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R.
    Garnett, Eds., Curran Associates, Inc., 2017,* *p. 4765–4774.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *R. P. Moro and P. Cortez, Bank* *Marketing, 2012.* |'
  prefs: []
  type: TYPE_TB
