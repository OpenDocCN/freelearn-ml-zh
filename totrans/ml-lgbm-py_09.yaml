- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: LightGBM MLOps with AWS SageMaker
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS SageMaker进行LightGBM MLOps
- en: In [*Chapter 8*](B16690_08.xhtml#_idTextAnchor134), *Machine Learning Pipelines
    and MLOps with LightGBM*, we built an end-to-end ML pipeline using scikit-learn.
    We also looked at encapsulating the pipeline within a REST API and deployed our
    API to the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B16690_08.xhtml#_idTextAnchor134)“使用LightGBM的机器学习管道和MLOps”中，我们使用scikit-learn构建了一个端到端的ML管道。我们还探讨了将管道封装在REST
    API中，并将我们的API部署到云端。
- en: This chapter will look at developing and deploying a pipeline using **Amazon
    SageMaker**. SageMaker is a complete set of production services for developing,
    hosting, monitoring, and maintaining ML solutions provided by **Amazon Web** **Services**
    (**AWS**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨使用**Amazon SageMaker**开发和部署管道。SageMaker是Amazon Web Services（**AWS**）提供的一套完整的用于开发、托管、监控和维护ML解决方案的生产服务。
- en: We’ll expand our capabilities with ML pipelines by looking at advanced topics
    such as detecting bias in a trained model and automating deployment to fully scalable,
    serverless web endpoints.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过查看高级主题，如检测训练模型中的偏差和自动化部署到完全可扩展的无服务器Web端点，来扩展我们的ML管道功能。
- en: 'The following main topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: An introduction to AWS and SageMaker
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS和SageMaker简介
- en: Model explainability and bias
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可解释性和偏差
- en: Building an end-to-end pipeline with SageMaker
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker构建端到端管道
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter dives deep into building ML models and pipelines using Amazon SageMaker.
    You need access to an Amazon account, and you must also configure a payment method.
    Note that running the example code for this chapter will incur costs on AWS. The
    complete notebooks and scripts for this chapter are available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了使用Amazon SageMaker构建ML模型和管道。您需要访问一个Amazon账户，并且您还必须配置一种支付方式。请注意，运行本章的示例代码将在AWS上产生费用。本章的完整笔记本和脚本可在[https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9)找到。
- en: An introduction to AWS and SageMaker
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS和SageMaker简介
- en: This section provides a high-level overview of AWS and delves into SageMaker,
    AWS’ ML offering.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了AWS的高级概述，并深入探讨了SageMaker，AWS的ML服务。
- en: AWS
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS
- en: AWS is one of the leading players in the global cloud computing marketplace.
    AWS offers many cloud-based products and services, including databases, **machine
    learning** (**ML**), analytics, networking, storage, developer tools, and enterprise
    applications. The idea behind AWS is to offer businesses an affordable and scalable
    solution to their computing needs, regardless of their size or industry.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: AWS是全球云计算市场的主要参与者之一。AWS提供许多基于云的产品和服务，包括数据库、**机器学习**（**ML**）、分析、网络、存储、开发工具和企业应用程序。AWS背后的理念是为企业提供一种经济实惠且可扩展的解决方案，以满足其计算需求，无论其规模或行业如何。
- en: A key advantage of AWS is elasticity, meaning servers and services can be stopped
    and started quickly and at will, scaling from zero machines to thousands. The
    elasticity of the services goes hand in hand with its primary pricing model of
    pay-as-you-go, meaning customers only pay for the services and resources they
    use without any upfront costs or long-term contracts. This elasticity and pricing
    allow businesses to scale computing needs as needed, on an ad hoc and granular
    level, and then only pay for what they use. This approach has transformed how
    businesses scale IT resources and applications, enabling them to react quickly
    to changing business needs without incurring the heavy costs traditionally associated
    with hardware and software procurement and maintenance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AWS的一个关键优势是弹性，这意味着服务器和服务可以快速随意地停止和启动，从零台机器扩展到数千台。服务的弹性与其主要定价模式“按使用付费”相辅相成，这意味着客户只需为使用的服务和资源付费，无需任何预付成本或长期合同。这种弹性和定价允许企业根据需要按需和细粒度地扩展计算需求，然后只为他们使用的付费。这种方法已经改变了企业扩展IT资源和应用程序的方式，使他们能够快速响应不断变化的企业需求，而无需承担与硬件和软件采购和维护相关的传统高昂成本。
- en: Another advantage is the global reach of AWS. AWS services are available in
    many regions across the globe. Regions are geographically separated, and each
    region is further divided into availability zones. The region-zone setup allows
    users to create globally distributed and redundant infrastructure to maximize
    resilience and architect for disaster recovery. The regional data centers also
    allow users to create servers and services close to end users, minimizing latency.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Core services
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core AWS services provide computing, networking, and storage capability.
    AWS’s compute services include **Amazon Elastic Compute Cloud** (**EC2**), which
    offers configurable virtual machines to customers, and **AWS Lambda**, a serverless
    compute platform that allows you to run code without the need to provision and
    manage servers. In ML, both EC2 instances and Lambda functions are often used
    to train and validate or serve models via API endpoints. The elastic nature of
    EC2 servers allows ML engineers to scale up training servers to many thousands,
    which can significantly speed up training or parameter-tuning tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: AWS’s storage and database services, such as **Amazon Simple Storage Service**
    (**S3**) and **Amazon RDS** (**Relational Database Service**), offer reliable,
    scalable, and secure data storage solutions. These services manage storage infrastructure
    and offer high-level features such as backups, patch management, and vertical
    and horizontal scaling. S3 is a widely used service for data engineering and ML.
    S3 offers low-cost, highly redundant secure storage that scales beyond exabytes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: AWS also offers data warehousing solutions with **Amazon Redshift**. Large enterprises
    frequently use Redshift as a warehouse or the basis of a data lake, meaning it’s
    often a data source for ML solutions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: AWS also offers networking services to help businesses meet complex networking
    and isolation needs. **AWS Direct Connect** allows customers to set up a dedicated
    network connection from a customer’s site to the AWS cloud. Routing and name servers
    can be managed with Amazon Route 53, a flexible and scalable **Domain Name System**
    (**DNS**) service.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: However, chief among the network services is **Amazon Virtual Private Cloud**
    (**VPC**). VPCs offer customers the ability to configure completely isolated virtual
    networks. Customers can granularly configure subnetworks, routing tables, address
    ranges, gateways, and security groups. VPCs allow users to isolate their environment
    and cloud resources and control inbound and outbound traffic for increased security
    and privacy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Security
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A critical piece of any infrastructure equation is security. In terms of security,
    AWS provides a highly secure, scalable, and flexible cloud computing environment.
    AWS’s security services, including **AWS Identity and Access Management** (**IAM**)
    and **Amazon Security Hub**, help customers protect their data and applications
    by implementing robust security measures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: AWS also complies with multiple international and industry-specific compliance
    standards, such as GDPR, HIPAA, and ISO 27001\. Further, in terms of data governance,
    AWS makes it easy to comply with data residency and privacy requirements. Due
    to the regional structure of AWS, data can remain resident in specific countries,
    while engineers have access to the full suite of AWS services.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS also offers services focused on ML and **artificial intelligence** (**AI**).
    Among these are many fully managed services for specific ML tasks. **AWS Comprehend**
    offers many **natural language processing** (**NLP**) services, such as document
    processing, named entity recognition, and sentiment analysis. **Amazon Lookout**
    is a service for anomaly detection in equipment, metrics, or images. Further,
    **Amazon Rekognition** offers services for machine vision use cases such as image
    classification and facial recognition.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Of particular interest to us is **Amazon SageMaker**, a complete ML platform
    that allows us to create, train, and deploy ML models in the Amazon cloud. The
    following section discusses SageMaker in detail.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon SageMaker** is an end-to-end ML platform that allows data scientists
    to work with data and develop, train, deploy, and monitor ML models. SageMaker
    is fully managed, so there is no need to provision or manage servers.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The primary appeal of Amazon SageMaker lies in its comprehensive nature as a
    platform. It encompasses all aspects of the ML process, including data labeling,
    model building, training, tuning, deployment, management, and monitoring. By taking
    care of these aspects, SageMaker allows developers and data scientists to focus
    on the core ML tasks instead of managing the infrastructure.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: As we have discussed, the ML life cycle starts with data gathering, which often
    requires manual data labeling. For this, SageMaker provides a service called **SageMaker
    Ground Truth**. This service makes it easy to annotate ML datasets efficiently.
    It can significantly reduce the time and costs typically associated with data
    labeling by using automated labeling workflows, and it also offers a workforce
    for manual data labeling tasks. Further, SageMaker also provides the **Data Wrangler**
    service, which helps with data preparation and **exploratory data analysis** (**EDA**).
    Data Wrangler provides functionality to query data from S3, Redshift, and other
    platforms and then cleanse, visualize, and understand the data from a single visual
    interface.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker provides a fully managed service for the model training phase that
    can handle large-scale, distributed model training via **Training Jobs**. The
    service is designed to be flexible and adaptable, allowing users to optimize their
    ML models as needed. Users only need to specify the location of their data, typically
    S3 and the ML algorithm, and SageMaker takes care of the rest of the training
    process. The model training service fully leverages the elastic nature of the
    underlying AWS infrastructure: many servers can be created quickly to perform
    training jobs and discarded after training is complete to save costs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker为模型训练阶段提供了一个完全管理的服务，可以通过**Training Jobs**处理大规模、分布式模型训练。该服务旨在灵活和适应性强，使用户能够根据需要优化他们的机器学习模型。用户只需指定其数据的位置，通常是S3和机器学习算法，SageMaker就会负责其余的训练过程。模型训练服务充分利用了底层AWS基础设施的弹性：可以快速创建多个服务器来执行训练任务，训练完成后丢弃以节省成本。
- en: This paradigm also extends to hyperparameter tuning. To simplify hyperparameter
    optimization, SageMaker provides an automatic model-tuning feature. Many tuning
    algorithms are provided, such as Optuna or FLAML, and tuning can be run across
    multiple servers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范式也扩展到了超参数调整。为了简化超参数优化，SageMaker提供了一个自动模型调优功能。提供了许多调优算法，例如Optuna或FLAML，并且可以在多个服务器上运行调优。
- en: SageMaker also has support for a more fully AutoML experience via **SageMaker
    Autopilot**. Autopilot is a service that enables automatic model creation. A user
    only needs to provide the raw data and set the target; then, Autopilot automatically
    explores different solutions to find the best model. Autopilot provides complete
    visibility into the process so that data scientists can understand how the model
    is created and make any necessary adjustments.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker还通过**SageMaker Autopilot**支持更全面的AutoML体验。Autopilot是一种服务，它能够实现自动模型创建。用户只需提供原始数据并设置目标；然后，Autopilot会自动探索不同的解决方案以找到最佳模型。Autopilot提供了对整个过程的完全可见性，以便数据科学家可以了解模型是如何创建的，并做出任何必要的调整。
- en: Once a model has been trained and optimized, it must be deployed. SageMaker
    simplifies this process by providing a one-click deployment process. Users can
    quickly deploy their models to production with auto-scaling capabilities without
    worrying about the underlying infrastructure. This deployment autoscaling capability
    allows users to set metrics-based policies that increase or decrease backing servers.
    For instance, the deployment can be scaled up if the number of invocations within
    a period exceeds a specific threshold. SageMaker ensures the high availability
    of models and allows for A/B testing of models to compare different variants and
    decide on the best one. SageMaker also supports multi-model endpoints, allowing
    users to deploy multiple models on a single endpoint.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过训练和优化，就必须部署。SageMaker通过提供一键部署过程简化了这一过程。用户可以快速将模型部署到生产环境中，并具有自动扩展功能，无需担心底层基础设施。这种部署自动扩展功能允许用户设置基于指标的策略，以增加或减少后端服务器。例如，如果一段时间内的调用次数超过特定阈值，则可以扩展部署。SageMaker确保模型的高可用性，并允许进行A/B测试以比较不同版本并决定最佳版本。SageMaker还支持多模型端点，允许用户在单个端点上部署多个模型。
- en: Amazon SageMaker also provides capabilities to monitor the model’s performance
    and conduct analysis once deployed. **SageMaker Model Monitor** monitors the quality
    of deployed models continuously (for real-time endpoints) or in batches (for asynchronous
    jobs). Alerts can be defined to notify the user if metric thresholds are exceeded.
    Model Monitor can monitor data drift and model drift based on metrics such as
    accuracy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker还提供了监控模型性能和分析部署后模型的功能。**SageMaker Model Monitor**持续监控已部署模型的品质（对于实时端点）或批量监控（对于异步作业）。可以定义警报，当指标阈值超过时通知用户。Model
    Monitor可以根据如准确度等指标监控数据漂移和模型漂移。
- en: Finally, SageMaker is both a platform within AWS and a software SDK. The SDK
    is available in both Python and R. The SageMaker SDK provides a range of built-in
    algorithms and frameworks, including support for the most popular algorithms in
    the ML community, such as XGBoost, TensorFlow, PyTorch, and MXNet. It also supports
    a marketplace where users can choose from a vast collection of algorithm and model
    packages shared by AWS and other SageMaker users.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，SageMaker 既是 AWS 中的一个平台，也是一个软件 SDK。SDK 提供了 Python 和 R 两种语言版本。SageMaker SDK
    提供了一系列内置算法和框架，包括对机器学习社区中最受欢迎的算法的支持，如 XGBoost、TensorFlow、PyTorch 和 MXNet。它还支持一个市场，用户可以从
    AWS 和其他 SageMaker 用户共享的大量算法和模型包中选择。
- en: A noteworthy part of SageMaker that simplifies one of the most important aspects
    of model development (bias and fairness) is **SageMaker Clarify**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 中的一个值得关注的部分，简化了模型开发中最重要的一环（偏差和公平性），就是 **SageMaker Clarify**。
- en: SageMaker Clarify
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker Clarify
- en: Amazon SageMaker Clarify is a tool that provides greater transparency into ML
    models. SageMaker Clarify aims to assist in understanding how ML models make predictions,
    thereby enabling model explainability and fairness.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Clarify 是一个提供更多机器学习模型透明度的工具。SageMaker Clarify 的目标是帮助理解机器学习模型如何进行预测，从而实现模型可解释性和公平性。
- en: One of the primary features of SageMaker Clarify is its capacity to provide
    model interpretability. It helps developers understand the relationships between
    the input data and the model’s predictions. The service generates feature attributions
    that show how each feature in the dataset influences predictions, which can be
    critical in many domains, especially those where it’s vital to understand the
    reasoning behind a model’s prediction. In addition to providing insight into individual
    predictions, SageMaker Clarify offers global explanatory capabilities. It measures
    the importance of input features on a model’s predictions in aggregate across
    the whole dataset. Feature impact analysis allows developers and data scientists
    to understand the overall behavior of a model, helping them interpret how different
    features drive model predictions on a global level.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify 的一个主要特性是其提供模型可解释性的能力。它帮助开发者理解输入数据与模型预测之间的关系。该服务生成特征归因，显示数据集中每个特征如何影响预测，这在许多领域都至关重要，尤其是在需要理解模型预测背后的推理时。除了提供对单个预测的洞察外，SageMaker
    Clarify 还提供全局解释能力。它衡量输入特征对模型预测的整体重要性，在整个数据集上汇总。特征影响分析允许开发者和数据科学家理解模型的总体行为，帮助他们从全局层面解释不同特征如何驱动模型预测。
- en: Further, Clarify can help identify potential bias in trained models. The service
    includes pre-training and post-training bias metrics that help us understand if
    a model favors certain groups unfairly. It’s best practice to check all new models
    for bias, but it is also imperative in regulated industries such as finance or
    healthcare, where biased predictions can have severe consequences.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Clarify 可以帮助识别训练模型中的潜在偏差。该服务包括预训练和后训练偏差指标，帮助我们了解模型是否不公平地偏向某些群体。检查所有新模型是否存在偏差是最佳实践，但在金融或医疗保健等受监管行业中，偏差预测可能具有严重后果，因此这一点至关重要。
- en: Clarify provides model interpretability by using an advanced technique known
    as **SHapley Additive** **exPlanations** (**SHAP**).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Clarify 通过使用一种称为 **SHapley Additive **exPlanations**（SHAP）的先进技术来提供模型可解释性。
- en: SHAP
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SHAP
- en: SHAP is a game theoretic approach to interpreting the output of any ML model
    [1]. SHAP aims to provide an understanding of the impact of individual features
    on a model’s overall prediction.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 是一种基于博弈论的解释任何机器学习模型输出的方法 [1]。SHAP 的目标是提供对单个特征对模型整体预测影响的了解。
- en: 'Essentially, SHAP values assess the effect of a particular feature value by
    contrasting it with a baseline value for that feature, highlighting its contribution
    to the prediction. A SHAP value is a fair contribution allocation from each feature
    to the prediction for each instance. SHAP values are rooted in cooperative game
    theory, representing a solution to the following question:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，SHAP 值通过将该特定特征值与该特征的基线值进行对比来评估其影响，突出其对预测的贡献。SHAP 值是每个实例中每个特征对预测的公平贡献分配。SHAP
    值植根于合作博弈论，代表了对以下问题的解决方案：
- en: “*Given the difference a feature makes in predicting an outcome, what portion
    of that difference is attributable to* *each feature?*”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: “*考虑到一个特征在预测结果中的差异，这部分差异中有多少可以归因于* *每个特征？*”
- en: These values are calculated using the concept of Shapley values from game theory.
    A Shapley value determines the significance of a feature by contrasting a model’s
    predictions with the presence and absence of that feature. Yet, as the sequence
    in which a model encounters features can affect its prediction, Shapley values
    consider all possible orderings. Then, it assigns an importance value to a feature
    so that it equals the average marginal contribution of that feature across all
    possible coalitions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值是使用博弈论中的 Shapley 值概念计算的。Shapley 值通过对比模型预测与该特征存在和不存在的情况来确定特征的重要性。然而，由于模型遇到特征序列可能会影响其预测，Shapley
    值考虑了所有可能的顺序。然后，它为特征分配一个重要性值，使得该特征在所有可能的联盟中的平均边际贡献等于这个值。
- en: There are several advantages to using SHAP for model interpretation. First,
    it offers consistency in interpretation. If the contribution of a feature changes,
    the attributed importance of that feature changes proportionally.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SHAP 进行模型解释有几个优点。首先，它提供了解释的一致性。如果一个特征的影响发生变化，该特征的归因重要性也会成比例地变化。
- en: Secondly, SHAP guarantees local accuracy, which means the sum of the SHAP values
    for all features would equal the difference between the prediction and the average
    prediction for the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，SHAP 保证局部准确性，这意味着所有特征的 SHAP 值之和将等于预测与数据集平均预测之间的差异。
- en: 'A great way to visualize SHAP values is by using SHAP summary plots. These
    plots provide a bird’s-eye view of feature importance and what is driving it.
    They plot all the SHAP values for a feature on a graph for easy visualization.
    Each point on the graph represents a SHAP value for a feature and an instance.
    The position on the Y-axis is determined by the feature and on the X-axis by the
    SHAP value:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SHAP 摘要图可视化 SHAP 值是一个很好的方法。这些图提供了一个俯瞰特征重要性和驱动因素的全景。它们在图上绘制了每个特征的所有 SHAP 值，以便于可视化。图上的每个点代表一个特征和一个实例的
    SHAP 值。Y 轴上的位置由特征决定，X 轴上的位置由 SHAP 值决定：
- en: '![Figure 9.1 – Local explanation example for the Census Income dataset. Bars
    indicate SHAP values or the relative importance of each feature in predicting
    this specific instance](img/B16690_09_01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 人口普查收入数据集的本地解释示例。条形表示 SHAP 值或每个特征在预测此特定实例时的相对重要性](img/B16690_09_01.jpg)'
- en: Figure 9.1 – Local explanation example for the Census Income dataset. Bars indicate
    SHAP values or the relative importance of each feature in predicting this specific
    instance
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 人口普查收入数据集的本地解释示例。条形表示 SHAP 值或每个特征在预测此特定实例时的相对重要性
- en: In the context of SageMaker Clarify, the service generates a set of SHAP values
    for each instance in your dataset when you run a clarification job. SageMaker
    Clarify can also provide global feature importance measures by aggregating SHAP
    values across the entire dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SageMaker Clarify 的上下文中，当你运行一个澄清作业时，该服务会为你的数据集中的每个实例生成一组 SHAP 值。SageMaker
    Clarify 还可以通过在整个数据集上聚合 SHAP 值来提供全局特征重要性度量。
- en: SHAP values can help you understand complex model behavior, highlight potential
    issues, and improve your model over time. For example, by examining SHAP values,
    you might discover that a specific feature has a more significant effect on your
    model’s predictions than expected, prompting you to explore why this might happen.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 值可以帮助你理解复杂的模型行为，突出潜在问题，并随着时间的推移改进你的模型。例如，通过检查 SHAP 值，你可能会发现某个特定特征对你的模型预测的影响比预期的更大，这会促使你探索为什么会发生这种情况。
- en: In this section, we looked at AWS and, more specifically, what the AWS ML family
    of services, SageMaker, offers. The functionality available in SageMaker, such
    as model explainability, bias detection, and monitoring, are components we have
    yet to implement in our ML pipelines. In the next section, we’ll look at building
    a complete end-to-end LightGBM-based ML pipeline, including these crucial steps,
    using SageMaker.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 AWS 以及更具体地，AWS 机器学习服务家族中的 SageMaker 提供了什么。SageMaker 中可用的功能，如模型可解释性、偏差检测和监控，是我们尚未在我们的机器学习管道中实现的部分。在下一节中，我们将探讨使用
    SageMaker 构建一个完整的端到端 LightGBM 机器学习管道，包括这些关键步骤。
- en: Building a LightGBM ML pipeline with Amazon SageMaker
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 构建 LightGBM 机器学习管道
- en: The dataset we’ll use for our case study of building a SageMaker pipeline is
    the Census Income dataset from *Chapter 4*, *Comparing LightGBM, XGBoost, and
    Deep Learning*. This dataset is also available as a SageMaker sample dataset,
    so it’s easy to work with on SageMaker if you are getting started.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline we’ll build consists of the following steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training and tuning.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model evaluation.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bias and explainability checks using Clarify.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model registration within SageMaker.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model deployment using an AWS Lambda.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s a graph showing the complete pipeline:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – SageMaker ML pipeline for Census Income classification](img/B16690_09_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – SageMaker ML pipeline for Census Income classification
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Our approach is to create the entire pipeline using a Jupyter Notebook running
    in SageMaker Studio. The sections that follow explain and go through the code
    for each pipeline step, starting with setting up the SageMaker session.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a SageMaker session
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps assume you have already created an AWS account and set
    up a SageMaker domain to get started. If not, the following documentation can
    be referenced to do so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Prerequisites: [https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.xhtml)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Onboarding to a SageMaker domain: [https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.xhtml)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We must initialize the SageMaker session and create S3, SageMaker, and SageMaker
    Runtime clients via `boto3` to get started:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use Amazon S3 to store our training data, source code, and all data
    and artifacts created by the pipeline, such as the serialized model. Our data
    and artifacts are split into a read bucket and a separate write bucket. This is
    a standard best practice as it separates the concerns for data storage.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker sessions have the concept of a default S3 bucket for the session.
    If no default bucket name is supplied, one is generated, and the bucket is created
    for you. Here, we’re grabbing a reference to the bucket. This is our output or
    write bucket. The read bucket is a bucket we’ve created previously that stores
    our training data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The source code, configuration, and output of each of the steps in the pipeline
    are captured in folders within our S3 write bucket. It’s useful to create variables
    for each S3 URI to avoid errors when repeatedly referring to data, like so:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'SageMaker needs us to specify the compute instance types we want to use when
    running the jobs for training, processing, Clarify, and prediction. In our example,
    we’re using `m5.large` instances. Most EC2 instance types can be used with SageMaker.
    However, a few special instance types that support GPUs and deep learning frameworks
    are also available:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: SageMaker uses standard EC2 instances for training but runs specific Docker
    images on the instances to provide ML functionality. Amazon SageMaker provides
    many prebuilt Docker images for various ML frameworks and stacks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker SDK also provides a function to search for images that are compatible
    with the instance type we need within the AWS region we are using. We can search
    for an image using `retrieve`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We specify `None` for the framework parameter as we manage the LightGBM installation
    ourselves.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'To parameterize our pipeline, we must define SageMaker workflow parameters
    from the `sagemaker.workflow.parameters` package. Wrappers are available for various
    parameter types:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With our pipeline parameters, S3 data paths, and other configuration variables
    set, we can move on to creating our pipeline’s preprocessing step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing step
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Setting up our preprocessing step has two parts: creating a Python script that
    performs the preprocessing and creating a processor that is added to the pipeline.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The script we’ll be using is a regular Python script with a main function.
    We’ll use scikit-learn to do our preprocessing. The preprocessing script hasn’t
    been entirely reproduced here but is available in our source code repository.
    Notably, when the pipeline executes the step, the data is retrieved from S3 and
    added to a local staging directory on the preprocessing instance. From here, we
    can read the data using standard pandas tooling:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Similarly, after processing is complete, we can write the results to a local
    directory, from which SageMaker retrieves it and uploads it to S3:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With a preprocessing script defined, we need to upload it to S3 for the pipeline
    to be able to use it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can define the preprocessing step as follows. First, we must create an `SKLearnProcessor`
    instance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`SKLearnProcessor` handles the processing task for jobs that require scikit-learn.
    We specify the scikit-learn framework version and the instance type and count
    we defined earlier.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The processor is then added to `ProcessingStep` for use in the pipeline:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`inputs` and `outputs` are defined using the `ProcessingInput` and `ProcessingOutput`
    wrappers, as shown here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`ProcessingStep` takes our scikit-learn processor and the inputs and outputs
    for the data. The `ProcessingInput` instances define the S3 source and local directory
    destination to facilitate copying the data (these are the same local directories
    our preprocessing script uses). Similarly, the `ProcessingOutput` instances take
    the local directory source and S3 destinations. We also set job arguments, which
    are passed to the preprocessing script as CLI arguments.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Having set up the preprocessing step, we can move on to training.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Model training and tuning
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We define a training script in the same way as a preprocessing script: a Python
    script with a main function that uses our standard Python tools, such as scikit-learn,
    to train a LightGBM model. However, we also need to install the LightGBM library
    itself.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to installing the library is building it into a Docker image
    and using it as our training image in SageMaker. This is the canonical way of
    managing environments in SageMaker. However, it entails significant work and includes
    the long-term need to maintain the image over time. Alternatively, if we only
    need to install a handful of dependencies, we can do that directly from our training
    script, as shown here.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'We must define a helper function to install packages and then use it to install
    LightGBM:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This also has the advantage that we install the latest version (or a specific
    version) every time we run training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'With the package installed, the rest of the training script trains a standard
    `LGBMClassifier` on the data prepared by the preprocessing step. We can set up
    or train data and parameters from the arguments to the script:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we must do standard scikit-learn cross-validation scoring, fit the model
    to the data, and output the training and validation scores:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As shown here, the script accepts CLI arguments to set hyperparameters. This
    is used by the hyperparameter tuning step to set parameters during the optimization
    phase. We can use Python’s `ArgumentParser` for this purpose:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can also see that we log training and validation F1 scores, allowing SageMaker
    and CloudWatch to pull the data from logs for reporting and evaluation purposes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to write out the results of the training in a JSON document.
    The results can then be used in subsequent pipeline processes and are shown as
    output from the job in the SageMaker interface. The JSON document is stored on
    disk, along with the serialized model file:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As with the preprocessing step, the results are written to a local directory,
    where SageMaker picks them up and copies them to S3.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: With the script defined, we can create the tuning step in the pipeline, which
    trains the model and tunes hyperparameters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'We must define a SageMaker `Estimator` that, similar to `SKLearnProcessor`,
    encapsulates the configuration for training, including a reference to the script
    (on S3):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can then define our SageMaker `HyperparameterTuner`, which performs the
    actual hyperparameter tuning. Similar to Optuna or FLAML, we must specify valid
    ranges for the hyperparameters using SageMaker wrappers:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`HyperparameterTuner` can be set up as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'SageMaker supports many strategies for hyperparameter tuning, including Hyperband
    tuning. More information can be found in the documentation for hyperparameter
    tuning: [https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.xhtml).
    Here, we used random search, with a maximum job size of 20\. It’s here that AWS’
    elastic infrastructure can be used to significant benefit. If we increase the
    training instance count, SageMaker automatically distributes the training job
    across all machines. Provisioning additional machines has some overhead and increases
    cost, but it can also majorly reduce tuning time if we run thousands of trials.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The tuner’s metric definitions define regular expressions that are used to pull
    the results metrics from the logs, as we showed in the training script earlier.
    The parameter optimization framework optimizes relative to the metrics defined
    here, minimizing or maximizing the metric.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'With the hyperparameter tuner defined, we can create a `TuningStep` for inclusion
    into the pipeline:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The pipeline steps we’ve defined thus far prepare data and produce a trained
    model that’s serialized to S3\. The pipeline’s next step is to create a SageMaker
    `Model` that wraps the model and is used for the evaluation, bias, and inference
    steps. This can be done as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `Model` instance encapsulates all the necessary configurations to deploy
    and run the model. We can see that `model_data` is taken from the top-performing
    model resulting from the tuning step.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline steps we’ve defined so far will produce processed data and train
    a tuned model. The layout for the processed data in S3 is shown in *Figure 9**.3*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – S3 directory layout for the results of the processing jobs](img/B16690_09_03.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – S3 directory layout for the results of the processing jobs
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: We could proceed to the deployment step if we needed to. However, we will follow
    best practice and add quality gates to our pipeline that check the model’s performance
    and bias and produce insights into its function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation, bias, and explainability
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve seen the general pattern of adding steps to a SageMaker pipeline:
    set up the configuration using SageMaker’s configuration classes and then create
    the relevant pipeline step.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Bias configuration
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To add bias checks to our pipeline, we must create the following configuration:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`BiasConfig` describes which facets (features) we want to check for bias. We’ve
    selected `Sex` and `Age`, which are always essential facets to check when working
    with demographic data.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '`ModeLBiasCheckConfig` wraps the data configuration, model configuration, and
    bias confirmation for the bias check step. It also sets the method to use for
    the bias check. Here, we use the **difference in positive proportions in predicted**
    **labels** (**DPPL**).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The DPPL is a metric that’s used to gauge if a model predicts outcomes differently
    for varying facets of data. The DPPL is calculated as the difference between the
    proportion of positive predictions for facet “a” and facet “d.” It helps assess
    whether there’s bias in the model predictions after training by comparing them
    with the initial bias present in the dataset. For instance, if a model predicting
    eligibility for a home loan predicts positive outcomes for 70% of male applicants
    (facet “a”) and 60% for female applicants (facet “d”), the 10% difference could
    indicate bias against facet “d.”
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The DPPL formula is represented as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: DPPL = q a ′ − q d ′
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Here, q a ′ is the predicted proportion of facet “a” receiving a positive outcome,
    and q d ′  is the analogous proportion for facet “d.” For binary and multicategory
    facet labels, normalized DPPL values fall between [-1, 1], while continuous labels
    vary over the interval (-∞, +∞). A positive DPPL value suggests a higher proportion
    of positive predictions for facet “a” versus “d,” indicating a positive bias.
    Conversely, a negative DPPL indicates a higher proportion of positive predictions
    for facet “d,” signifying a negative bias. A DPPL near zero points to a relatively
    equal proportion of positive predictions for both facets, with a value of zero
    implying perfect demographic parity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add the bias check to the pipeline using `ClarifyCheckStep`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Explainability configuration
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The configuration for explainability is very similar. Instead of creating `BiasConfig`,
    we must create `SHAPConfig`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Alongside `SHAPConfig`, we must create `ModelExplainabilityCheckConfig` to
    calculate the SHAP values and create an explainability report:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Everything is then combined using `ClarifyCheckStep`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Evaluation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we also need to evaluate our model against test data. The evaluation
    script is very similar to the training script, except it pulls the tuned model
    from S3 for scoring. The script consists of a main function with two steps. First,
    we must bootstrap the trained model and perform the scoring (in our case, calculating
    the F1 score):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we must output the results to a JSON file:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The evaluation JSON is used for reporting and subsequent steps that rely on
    the evaluation metrics.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and monitoring the LightGBM model
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now ready to add our pipeline’s final steps for supporting deployment.
    The deployment part of the pipeline consists of three steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Registering the model in SageMaker.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A conditional check to validate that the model evaluation surpasses a minimum
    threshold.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying a model endpoint using an AWS Lambda function.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model registration
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To deploy our model, we first need to register our model in SageMaker’s **Model
    Registry**.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker’s Model Registry is a central repository where you can manage and
    deploy your models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'The Model Registry provides the following core functionality:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Model versioning**: Every time a model is trained and registered, it’s assigned
    a version in the Model Registry. This helps you keep track of different iterations
    of your models, which is useful when you need to compare model performance, roll
    back to previous versions, or maintain reproducibility in your ML projects.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approval workflow**: The Model Registry supports an approval workflow, where
    models can be marked as “Pending Manual Approval,” “Approved,” or “Rejected.”
    This allows teams to effectively manage the life cycle of their models and ensure
    that only approved models are deployed.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model catalog**: The Model Registry acts as a catalog where all your models
    are centrally stored and accessible. Each model in the registry has metadata associated
    with it, such as the training data used, hyperparameters, and performance metrics.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While registering our model, we attach the metrics that were calculated from
    our evaluation step. These metrics are also used for model drift detection.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of drift are possible: **data drift** and **model drift**.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Data drift refers to a change in the statistical distribution of the incoming
    data compared to our model’s training data. For example, if the training data
    had a male/female split of 60% to 40%, but the data used for prediction is skewed
    to 80% male and 20% female, it’s possible that drift occurred.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Model drift is a phenomenon where the statistical properties of the target variable,
    which the model tries to predict, change over time in unforeseen ways, causing
    model performance to degrade.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Both data and model drift can occur due to environmental changes, societal behaviors,
    product usage, or other factors not accounted for during model training.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker supports continuous monitoring of drift. SageMaker calculates the
    statistical distribution of both incoming data and the predictions we are making.
    Both are compared against the distributions present in the training data. Should
    drift be detected, SageMaker can produce alerts to AWS CloudWatch.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'We can configure our metrics as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, for the drift metrics, we must set up `DriftCheckBaselines`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we must create a model registration step with the following code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Model validation
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The conditional check uses the evaluation data from the evaluation step to
    determine whether the model is suitable for deployment:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, we created `ConditionStep` and compared the F1 score against a threshold
    of `0.9`. Deployment can proceed if the model has an F1 score higher than the
    threshold.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Deployment with AWS Lambda
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The deployment script is a standard AWS Lambda script in Python that defines
    a `lambda_handler` function that obtains a client connection to SageMaker and
    proceeds to create the model endpoint:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Notably, the Lambda function does not serve requests for the model. It only
    creates the model endpoint within SageMaker.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In SageMaker, an **endpoint** is a web service to get predictions from your
    models. Once a model is trained and the training job is complete, you need to
    deploy the model to make real-time or batch predictions. Deployment in SageMaker
    parlance means setting up an endpoint – a hosted, production-ready model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: An endpoint in SageMaker is a scalable and secure RESTful API that you can use
    to send real-time inference requests to your models. Your applications can access
    an endpoint to make predictions directly via the REST API or AWS SDKs. It can
    scale instances up and down as needed, providing flexibility and cost-effectiveness.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker also supports multi-model endpoints, which can deploy multiple models
    on a single endpoint. This feature can significantly save on costs if many models
    are used infrequently or are not resource-intensive.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Lambda script defined, it can be incorporated into the pipeline using
    `LambdaStep`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: A model endpoint incurs cost as soon as it’s deployed for the duration of its
    deployment. Once you run your pipeline, an endpoint is created as a result. If
    you are only experimenting with or testing your pipeline, you should delete the
    endpoint once you’re done.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running the pipeline
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All of our pipeline steps are now in place, which means we can create the pipeline
    itself. The `Pipeline` construct takes the name and parameters we’ve already defined:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We must also pass all the steps we’ve defined as a list parameter and finally
    upsert the pipeline:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Executing the pipeline is done by calling the `start` method:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note the conditions we defined here. When running the pipeline for the first
    time, we must skip the model bias and explainability checks while registering
    new bias and explainability baselines.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Both checks require an existing baseline to run (otherwise, there is no data
    to check against). Once baselines have been established, we can disable skipping
    the checks in subsequent runs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: More information on the model life cycle and creating baselines can be found
    at [https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.xhtml).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the pipeline is executed, you can view the execution graph to see the
    status of each step:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Successful execution of the LightGBM Census Income pipeline](img/B16690_09_04.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Successful execution of the LightGBM Census Income pipeline
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the model itself registered in the Model Registry once the
    pipeline completes:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – SageMaker Model Registry showing the approved Census Income
    model and the related endpoint](img/B16690_09_05.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – SageMaker Model Registry showing the approved Census Income model
    and the related endpoint
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The bias and explainability reports can be viewed when a model is selected.
    *Figure 9**.6* shows the bias report for the model that was created by the pipeline.
    We can see a slight imbalance in the DPPL for sex, but less than the class imbalance
    in the training data. The report indicates there isn’t strong evidence for bias:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Bias report for the Census Income model. We can see a slight
    imbalance in the DPPL but less than the class imbalance in the training data](img/B16690_09_06.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Bias report for the Census Income model. We can see a slight imbalance
    in the DPPL but less than the class imbalance in the training data
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The explainability report, as shown in *Figure 9**.7*, shows the importance
    of each feature in terms of SHAP values. Here, we can see that the **Capital Gain**
    and **Country** features are dominant regarding importance to predictions:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Explainability report for the Census Income model showing the
    dominant importance of the Capital Gain and Country features](img/B16690_09_07.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Explainability report for the Census Income model showing the dominant
    importance of the Capital Gain and Country features
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The bias and explainability reports can also be downloaded in PDF format, which
    can easily be shared with business or non-technical stakeholders.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using the endpoint
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, our deployed model is not very useful if we can’t make any predictions
    using it. We can make predictions with the deployed model via REST calls or the
    Python SDK. Here is an example of using the Python SDK:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We obtain a SageMaker `Predictor` using the endpoint name and the session. Then,
    we can call `predict`, passing a NumPy array (obtained from a test DataFrame in
    this case).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have created a complete, end-to-end, production-ready pipeline
    using SageMaker. Our pipeline includes data preprocessing, automatic model tuning,
    bias validation, drift detection, and a fully scalable deployment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced AWS and Amazon SageMaker as a platform for building
    and deploying ML solutions. An overview of the SageMaker service was given, including
    the Clarify service, which provides advanced features such as model bias checks
    and explainability.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: We then proceeded to build a complete ML pipeline with the SageMaker service.
    The pipeline includes all steps of the ML life cycle, including data preparation,
    model training, tuning, model evaluation, bias checks, explainability reports,
    validation against test data, and deployment to cloud-native, scalable infrastructure.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Specific examples were given to build each step within the pipeline, emphasizing
    full automation, looking to enable straightforward retraining and constant monitoring
    of data and model processes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter looks at another MLOps platform called **PostgresML**. PostgresML
    offers ML capabilities on top of a staple of the server landscape: the Postgres
    database.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *S. M. Lundberg and S.-I. Lee, A Unified Approach to Interpreting
    Model Predictions, in Advances in Neural Information Processing Systems 30, I.
    Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R.
    Garnett, Eds., Curran Associates, Inc., 2017,* *p. 4765–4774.* |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *R. P. Moro and P. Cortez, Bank* *Marketing, 2012.* |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
