- en: The Automated Classification of Lithofacies Formation Using ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the idea of building an end-to-end cloud-based
    machine learning system to identify **lithofacies** based on well log measurements.
    This is a crucial step in all drilling applications. First, we will start by introducing
    the problem and the dataset. Next, we will explain the types of preprocessing
    and post processing needed for such a use case. Finally, a complete solution will
    be built using machine learning services, Python, and IBM Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding lithofacies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding lithofacies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sedimentary rock that has been formed through the deposition and solidification
    of sediment transported by water, ice, and wind is usually deposited in layers.
    The geological properties of these layers depend upon a number of forces such
    as tectonics, sea level, sediment supply, physical and biological processes of
    sediment transport and deposition, and climate. The result of these forces and
    interactions yield what is known as a **geometric arrangement**, making up the
    stratigraphic architecture of an area. The arrangement or internal anatomy of
    the sediment bodies within the architecture is identified through lithofacies
    analysis and the interpretation of the depositional environments.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering and interpreting this information is a critical component in the work
    of oil and gas, groundwater, and mineral and geothermal exploration (as well as
    being a significant share of environmental and geotechnical study).
  prefs: []
  type: TYPE_NORMAL
- en: Depositional environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previously mentioned depositional environments are created by the various
    physical and biological processes of transporting and depositing sediments. These
    processes result in various distributions of grain size andbiogenic sedimentary
    structures that characterize (or classify) the deposited sediment through a direct
    relationship to the depositional force that produced them.
  prefs: []
  type: TYPE_NORMAL
- en: Relating the features found in an environmental structure back to the forces
    that created them is the basic method used by geologists to interpret the depositional
    environment of the sedimentary sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Lithofacies formation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the very first steps in the progression of **litho****facies analysis**
    (and, therefore, lithofacies formation), is the description and interpretation
    of available and conventional core data.
  prefs: []
  type: TYPE_NORMAL
- en: An important outcome of core descripting is the subdivision of cores into lithofacies,
    defined as **classifications of a sedimentary sequence** based on lithology (the
    study of the characteristics of rock), grain size, physical and biogenic sedimentary
    structures, and the stratification that relates to the depositional processes
    that produced them.
  prefs: []
  type: TYPE_NORMAL
- en: Lithofacies and lithofacies associations (groups of related lithofacies) are
    the basic units for the interpretation of depositional environments.
  prefs: []
  type: TYPE_NORMAL
- en: Our use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope that after reading the preceding sections of this chapter, you have already
    formulated the idea that a critical component in evaluating opportunities for
    drilling applications is lithology and lithofacies formation.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal, in this project, is to use machine learning to interpret core data
    and identify lithofacies (that is, classify bodies of rock or rock types into
    mappable units of a designated stratigraphic unit) based upon its physical characteristics,
    composition, formation, or various other attributes, obtained in well logging
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will explore the well training data and plot the
    learning in various forms.
  prefs: []
  type: TYPE_NORMAL
- en: Well logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well logging, sometimes referred to as **borehole logging**, is the practice
    of making a detailed record (or a well log) of the geological formations penetrated
    by a borehole or a well. This log may be established either on a visual inspection
    of the samples brought to the surface (called **geological logs**) or on the physical
    measurements made by instruments lowered into the hole (called geophysical logs).
  prefs: []
  type: TYPE_NORMAL
- en: Geophysical well logs such as, drilling, completing, producing, or abandoning
    can be done during any phase of a well's history.
  prefs: []
  type: TYPE_NORMAL
- en: Log ASCII Standard (LAS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thankfully, there is a commonly acceptable format in which well logs are expected
    to be.
  prefs: []
  type: TYPE_NORMAL
- en: LAS is an industry-standard file format used in all oil-and-gas and water well
    industries to log and store well log information and data. A single LAS file can
    only contain data for one well. But in that one well, it can contain any number
    of datasets (called **curves**). Common curves found in an LAS file may include
    natural gamma, travel time, resistivity logs, and other possible information.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on LAS files, you can refer to this paper: [https://www.bcogc.ca/node/11400/download](https://www.bcogc.ca/node/11400/download).'
  prefs: []
  type: TYPE_NORMAL
- en: Wow! Although not rocket science, the data is not a simple relational table.
    Preliminary work for this exercise will be to better understand the specifics
    of the data provided.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our goal is to implement a machine learning algorithm in Python
    using `scikit-learn`, one of the most popular machine learning tools for Python,
    based upon a sample well drilling log dataset for the task of training a classifier
    to distinguish between different types of lithofacies.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we are told that the training log dataset was created from sample
    logs, and based upon research defining eight different lithofacies, along with
    various log measurements, such as gamma-ray, neutron porosity, **photoelectric
    factor** (**PeF**), and resistivity.
  prefs: []
  type: TYPE_NORMAL
- en: We also know that in this file, we'll have six lithofacies data points (`GCR`,
    `NPHI`, `PE`, `PEF`, `ILD`, and `ILM`), along with an ID and the lithofacies type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a portion of the top section of an actual well
    logging file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b68a5bb4-aef2-4d70-b9eb-4c9fba0ae94a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot is a peek at a portion of our file, which we will
    use for training our machine learning model. This file excludes the top LAS-formatted
    section headers and is simply a continuous list of curves or well-logging measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/752d0e9e-ecd2-4c1e-aab9-c49d7f42b6bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading the data asset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will assume that you have created a new IBM Watson Studio project, and therefore,
    you can proceed to add a data asset (our well sample log file) to it so that we
    can work with the data. We have loaded data files in our previous chapters, but
    here is a quick refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: From the new project's Assets page, click on Add to project | DATA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Load pane that opens, browse to the file. Remember that you must stay
    on the page until the load is complete
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IBM Watson then saves the files in the object storage that is associated with
    your project, and they are listed as data assets on the Assets page of your project
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data asset annotations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you build your assets within IBM Watson Studio, it is highly recommended
    that you take the time to annotate your data assets at the time that you load
    them. This will allow you to quickly search for and locate those assets for collaboration
    with others and to use within other projects. You can accomplish this by simply
    adding a description and one or more tags to your asset.
  prefs: []
  type: TYPE_NORMAL
- en: 'A tag is metadata that simplifies searching for your assets. A tag consists
    of one string containing spaces, letters, numbers, underscores, dashes, and the
    # and @ symbols. You can create multiple tags on the same asset by using a comma
    to separate the individual tag values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the project page, under Data assets, you can click on the asset name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e8b5376-5a96-4eb0-8dd4-1e9ed26ccde4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, click inside the Tags box to add tags, and you can manually assign business
    terms and tags to the data, as well as a description, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/7c3c91a5-584b-4920-a47e-31e7b1b1b8d8.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another method for developing a good understanding of what your data offers
    is to create a profile of the data, using the profile feature within IBM Watson
    Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The profile of a data asset includes generated metadata and statistics about
    the textual content of the data so that you can *see* how it is made up. You can
    create a profile on the asset''s Profile page in the project, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72826041-99d2-4e0b-b965-b78f7bd2d3ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click on Create Profile, IBM Watson reviews the data and generates
    the visuals that you can scroll though and easily examine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd9d5d20-4461-4fa1-8ff0-7f8174f3c5a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending upon the size and complexity of the data, generating the profile may
    take a few minutes. The good news is that after it is created, it is saved with
    the file so that it doesn't have to be generated again.
  prefs: []
  type: TYPE_NORMAL
- en: Using a notebook and Python instead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rather than using the profiler, you can use visualizations within an IBM Watson
    notebook to present data visually to identify patterns, gain insights, and make
    decisions based upon your project's objectives or assumptions. As we've seen in
    earlier chapters, many open source visualization libraries, such as `matplotlib`,
    are already pre-installed on IBM Watson Studio for you, and all you have to do
    is import them.
  prefs: []
  type: TYPE_NORMAL
- en: You can install other third-party and open source visualization libraries and
    packages in the same manner, or take advantage of other IBM visualization libraries
    and tools, such as **Brunel**, to create interactive graphs with simple code and
    SPSS models to create interactive tables and charts to help evaluate and improve
    a predictive analytics model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will use a notebook and Python commands to show
    the various ways to analyze and condition data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, since we assume that we already have a new IBM Watson Studio project
    created, we can go ahead and add a new notebook to the project (from your project,
    click on Add to Project | Notebook, just as we did in prior chapters, just be
    sure to specify the language as Python). Let''s take a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load and open the file, then print the first five records (from the file). Recall
    that to accomplish this, there is no coding required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You simply click on Insert to code and then Insert pandas DataFrame for our
    file in the Files | Data Asset pane:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/92e6b2c1-df3d-46a9-b1cd-b71b74030a96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This automatically generates the following code in our notebook''s first cell,
    which will load our data file into a pandas DataFrame object (`df_data_1`) and
    then print the first five records of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1af1fbbd-c4f8-4b1f-8815-dc265de72dca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding code generates the following output for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eda02023-ffa5-42dc-8dd5-5174db5f5310.png)'
  prefs: []
  type: TYPE_IMG
- en: From this review, we can see that each row of the dataset represents one lithofacies,
    and they are each represented by several features that are in our table's columns
    (as shown in the preceding screenshot).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `print` and `.shape` functions of Python, we see that we have `180`
    lithofacies (the number of records in the file) and `8` features in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/376a8c2b-3641-4b56-97ce-8310c444ed2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use the `.unique()` function to demonstrate that we have eight
    different types of lithofacies in our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/476c4681-fd03-4a3d-82d8-c2f3c0f52aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we can use the `.size()` function to see how each lithofacies is represented
    within the file. The data seems pretty balanced between `22` and `25`, with the
    `Mdst/Mdst-Wkst` lithofacies being the most unbalanced with `16`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c7e0eae-94a9-41af-b0ef-da03eef5df28.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use Python to create a bar chart based upon the lithofacies size; this
    makes it a bit easier to understand as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f50fce8-81b5-4861-b250-b5ab91541912.png)'
  prefs: []
  type: TYPE_IMG
- en: Box plotting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often used in performing explanatory data analysis, a box plot is a type of
    graph that is used to show and understand the shape of a distribution, its central
    value, and its variability. Seeing a box plot for each numeric variable in our
    well log data will give you a better idea of the distribution of the input variables
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e5ec820-fac0-4fc1-83cb-515e7d0c3bbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **histogram** is used to graphically recap and display the distribution of
    data points within a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following Python code, we can now try a histogram for each numeric
    input value within our data (`GCR`, `ILD`, `ILM`, `NPHI`, `PE`, and `PEF`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5a9c946-0b60-4950-9387-3220a8d696c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once a histogram is generated from the data, the first question that is usually
    asked is whether the shape of the histogram is normal. A characteristic of a **normal
    distribution** (of data), *s*, is that it is symmetrical. This means that if the
    distribution is cut in half, each side will be the mirror of the other, forming
    a bell-shaped curve, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eef7aa30-8519-4100-9dc4-e4eeb72d413e.png)'
  prefs: []
  type: TYPE_IMG
- en: From our generated histograms, perhaps the **NPHI** data point comes the closest
    to showing a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The scatter matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **scatter matrix** is another common analysis tool as it include several pairwise
    scatter plots of variables presented in a matrix format. It is also used to verify
    if variables are correlated and whether the correlation is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be used to experiment with this type of visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e242789-03cf-443d-b1ab-3fce75e9faf4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A scatter plot attempts to reveal relationships or associations between variables
    (called a correlation*)*. Refer to the following link to learn more about scatter
    plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mste.illinois.edu/courses/ci330ms/youtsey/scatterinfo.html](https://mste.illinois.edu/courses/ci330ms/youtsey/scatterinfo.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the scatter plot generated from our log data (shown in the preceding
    screenshot), I really don't see any specific or direct correlations between the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may continue performing a deep dive into the data, perform
    some reforming or aggregations, or even perhaps go back to the original source
    (of the data) and request additional or new data.
  prefs: []
  type: TYPE_NORMAL
- en: In the interest of time, for this exercise, we will assume that we will use
    what data we have and move on to creating and testing various modeling algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Training the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-learn` library can be used to code machine learning classifier and
    is the only Python library which has four-step modeling pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following link for more information about `sckit-learn`: [http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf](http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The coding process of implementing the `scikit-learn` model applies to various
    classifiers within `sklearn`, such as decision trees, **k-nearest neighbors**
    (**KNN**), and more. We will look at a few of these classifiers here, using our
    well logging data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in using Scikit to build a model is to create training and test
    datasets and apply scaling, using the following lines of Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have created a training dataset, we can proceed with building our
    various types of machine learning models using that data. Typically, in a particular
    machine learning project, you will have some idea as to the type of machine learning
    algorithm that you'll want to use, but perhaps not. Either way, you want to verify
    the performance of your selected algorithm(s).
  prefs: []
  type: TYPE_NORMAL
- en: The following sections show the Python commands which with to create models
    based using the `scikit-learn` module.
  prefs: []
  type: TYPE_NORMAL
- en: Building a logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regression analysis** is used to understand which of the independent variables
    (our features: `GCR`, `NPHI`, `PE`, `ILD`, and `ILM`) are related to the dependent
    variable; that is, the type of lithofacies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of Python code create a logistic regression classifier
    model and print its accuracy statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b36a60b-1c77-4299-bfe9-c73918b2f3bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a KNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The KNN algorithm is a simple, supervised machine learning algorithm that can
    also be used for classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of Python code create a KNN classifier model and print
    its accuracy statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09803695-03fa-409a-8f59-43877547ab38.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a Gaussian Naive Bayes model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the class variable, all Naive Bayes classifiers infer that the value of
    a particular feature in the data is independent of the value of any other feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of Python code create a **Gaussian Naive Bayes** (**GaussianNB**)
    classifier model and print its accuracy statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6db00de6-f0d7-42d8-b1fa-7968c9ee3111.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a support vector machine model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **support vector machine** (**SVM**) is a supervised learning model with associated
    learning algorithms that analyze data used for classification and regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of Python code create an SVM classifier model and print
    its accuracy statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38e0acc3-c204-4d0f-be26-03f071d7d277.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a decision tree model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a decision support tool that uses a tree-like model of decisions
    and their possible consequences, including chance event outcomes, resource costs,
    and utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of Python code create a decision tree classifier mode and
    print its accuracy statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbbae996-3329-4140-b918-69c59eade65b.png)'
  prefs: []
  type: TYPE_IMG
- en: Summing them up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, if we were working a real project, this step or phase (of the project)
    would include a much deeper review of each model's performance results and, perhaps,
    would require the decision to even return back to the data exploration and transformation
    phase. However, for the sake of time, we'll move forward.
  prefs: []
  type: TYPE_NORMAL
- en: Since we've now seen some simple ways for creating and, at least, superficially
    judging each model's performance (as far as accuracy), we will move on to the
    last section of this chapter and look at an example of visualizing a selected
    model using Python commands.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reviewing the outputs printed after each model build, we should notice that
    the decision tree model has one of the best results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A disclaimer of sorts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, we would spend much more time evaluating and verifying the performance
    of a selected model (and continually training it), but again, you get the general
    idea (there is plenty of due diligence work to do!), and our goals are more around
    demonstrating the steps in building an end-to-end machine learning solution using
    IBM Watson Studio and its resources.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, we will now use some Python code to create some visualizations
    of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree algorithms are very commonly-used supervised learning algorithm
    models for classification and regression tasks. In this section, we will show
    how you can visualize decision tree classifiers to better understand their logic.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree classifiers build a sequence of simple if/else rulings on data
    through the use of which they can then predict the target value.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are usually simpler to interpret because of their structure and
    the ability we have to visualize the modeled tree, using modules such as the `sklearn
    export_graphviz` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following standard Python code can be used to visualize the decision tree
    model that we previously built in our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The sample code can be found at [https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084](https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the code executed in our notebook and the graphical
    output it creates (although this is difficult to fit into a single screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cedfd34-c220-42ff-8706-cd2c1f2b139c.png)'
  prefs: []
  type: TYPE_IMG
- en: Very seldom does one allow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a real-world use case in the evaluation of well
    drilling logs to classify lithofacies.
  prefs: []
  type: TYPE_NORMAL
- en: We loaded a sample file and performed various profiling and visualization exercises
    comparing Watson profiles, as well as using Python commands within a notebook.
    Finally, we used specialized Python libraries to build various types of models
    and then presented a graph of a supervised machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build a cloud-based, multibiometric identity authentication
    platform.
  prefs: []
  type: TYPE_NORMAL
