<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Sentiment Analysis of Twitter Data"><div class="book" id="1UU542-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Sentiment Analysis of Twitter Data</h1></div></div></div><div class="blockquote"><table border="0" cellspacing="0" cellpadding="0" class="blockquote2" summary="Block quote"><tr class="calibre22"><td valign="top" class="calibre23"> </td><td valign="top" class="calibre23"><p class="calibre24"><span><em class="calibre25">"He who molds the public sentiment... makes statutes and decisions possible or impossible to make."</em></span></p></td><td valign="top" class="calibre23"> </td></tr><tr class="calibre22"><td valign="top" class="calibre23"> </td><td colspan="2" valign="top" class="calibre26">--<span><span><em class="calibre25">Abraham Lincoln.</em></span></span></td></tr></table></div><p class="calibre8">What people think matters not only to politicians and celebrities but also to most of us social beings. This need to know opinions about ourselves has affected people for a long time and is aptly summarized by the preceding famous quote. The opinion bug not only affects our own outlook, it affects the way we use products and services as well. As discussed while learning about market basket analysis and recommender engines (see <a class="calibre1" title="Chapter 3. Predicting Customer Shopping Trends with Market Basket Analysis" href="part0026_split_000.html#OPEK1-973e731d75c2419489ee73e3a0cf4be8">Chapter 3</a>, <span class="strong"><em class="calibre10">Predicting Customer Shopping Trends with Market Basket Analysis</em></span> and <a class="calibre1" title="Chapter 4. Building a Product Recommendation System" href="part0032_split_000.html#UGI01-973e731d75c2419489ee73e3a0cf4be8">Chapter 4</a>, <span class="strong"><em class="calibre10">Building a Product Recommendation System</em></span> respectively), our behavior can be approximated or predicted by observing the behavior of a group of people with similar characteristics such as price sensitivity, color preferences, brand loyalty, and so on. We also discussed in the earlier chapters that, for a long time, we have asked our friends and relatives for their opinions before making that next big purchase. While those opinions are important to us at an individual level, there are far more valuable insights we can derive from such information.</p><p class="calibre8">To say that the advent of the World Wide Web has simply accelerated and widened our circle would be an understatement. Without being repetitive, it is worth mentioning that the web has opened new doors for analyzing human behavior.</p><p class="calibre8">In the previous chapter, social networks were the object of discussion. We not only used social networks as tools to derive insights but we also discussed the fact that these platforms satisfy our inherent curiosity about what others are thinking or doing. Social networks provide us all with a platform where we can voice our opinions and be heard. The <span class="strong"><em class="calibre10">be heard</em></span> aspect of it is a little tricky to define and handle. For instance, our opinions and feedback (assuming they are genuine) about someone or something on these platforms will certainly be heard by the people in our circles (directly or indirectly), but they may or may not be heard by the people or organizations they are intended for. Nevertheless, such opinions or feedback do impact the people connected to them and their behavior from then on. This impact of opinions and our general curiosity about what people think, coupled with more such use cases, is the motivation for this chapter.</p><p class="calibre8">In this chapter, we will:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Learn about sentiment Analysis and its key concepts</li><li class="listitem">Look into the applications and challenges presented by sentiment analysis</li><li class="listitem">Understand the different approaches to perform opinion mining</li><li class="listitem">Apply the concepts of sentiment analysis on Twitter data</li></ul></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Sentiment Analysis of Twitter Data">
<div class="book" title="Understanding Sentiment Analysis"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec60" class="calibre1"/>Understanding Sentiment Analysis</h1></div></div></div><p class="calibre8">The fact that <a id="id564" class="calibre1"/>Internet-based companies and their CEOs feature as some of the most profitable entities in the global economy says a lot about how the world is being driven by technology and shaped by the Internet. Unlike any other medium, the Internet has become ubiquitous and has penetrated every aspect of our lives. It is no surprise that we are using and relying on the Internet and Internet-based solutions for advice and recommendations, apart from using it for many other purposes.</p><p class="calibre8">As we saw in the previous chapters, the relationship between the Internet and domains such as e-commerce and financial institutions goes way too deep. But our use of and trust in the online world doesn't stop there. Be it about booking a table at the new restaurant in your neighborhood or deciding which movie to see tonight, we take help from the Internet to know what opinions others have, or what others have to share, before we make the final call. As we will see later, such decision aids are not just limited to the commerce platforms but also apply to many other domains.</p><p class="calibre8">Opinion mining or sentiment analysis (as it is widely and interchangeably known) is the process of automatically identifying the subjectivity in text using natural language processing, text analytics, and computational linguistics. Sentiment analysis aims to identify the positive, negative, or neutral opinion, sentiment, or attitude of the speaker using said techniques. Sentiment analysis (henceforth used interchangeably with opinion mining) finds its application in areas from commerce to service domains across the world.</p></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Sentiment Analysis of Twitter Data">
<div class="book" title="Understanding Sentiment Analysis">
<div class="book" title="Key concepts of sentiment analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec77" class="calibre1"/>Key concepts of sentiment analysis</h2></div></div></div><p class="calibre8">We will now examine the <a id="id565" class="calibre1"/>key terms and concepts related to sentiment analysis. These terms and concepts will help us formalize our discussions in the coming sections.</p><div class="book" title="Subjectivity"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec24" class="calibre1"/>Subjectivity</h3></div></div></div><p class="calibre8">Opinions or sentiments are <a id="id566" class="calibre1"/>one's own expression of views and beliefs. Furthermore, subjectivity (or subjective text) expresses our sentiments about entities such as products, people, governments, and so on. For instance, a subjective sentence could be <span class="strong"><em class="calibre10">I love to use Twitter</em></span>, which shows a person's love towards a particular social network, while an objective sentence would be <span class="strong"><em class="calibre10">Twitter is a social network</em></span>. The second example simply states a fact. Sentiment analysis revolves around subjective texts or subjectivity classification. It is also important to understand that not all subjective texts express sentiment. For example, <span class="strong"><em class="calibre10">I just created my Twitter account</em></span>.</p></div><div class="book" title="Sentiment polarity"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec25" class="calibre1"/>Sentiment polarity</h3></div></div></div><p class="calibre8">Once we have a piece of <a id="id567" class="calibre1"/>text which is subjective in nature (and expresses some sentiment), the next task is to classify it into one of the sentiment classes of positive or negative (sometimes neutral is also considered). The task may also involve placing the text's sentiment on a continuous (or discrete) scale of polarities, thus defining the degree of positivity (or sentiment polarity). The sentiment polarity classification may deal with a different set of classes depending upon the context. For example, in a rating system for movies, sentiment polarities may be defined as liked versus disliked, or in a debate the views may be classified as for versus against.</p></div><div class="book" title="Opinion summarization"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec26" class="calibre1"/>Opinion summarization</h3></div></div></div><p class="calibre8">Opinion classification or <a id="id568" class="calibre1"/>sentiment extraction from a piece of text is an important task in the process of sentiment analysis. This is often followed by a summarization of sentiments. To draw insights or conclusions from different texts related to the same topic (say, reviews of a given movie), it is important to aggregate (or summarize) the sentiments into a consumable form to draw conclusions (whether the movie is a blockbuster or a dud). This may involve the use of visualizations to infer the overall sentiment.</p></div><div class="book" title="Feature extraction"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec27" class="calibre1"/>Feature extraction</h3></div></div></div><p class="calibre8">As we have <a id="id569" class="calibre1"/>seen across the chapters, feature identification and extraction is what makes or breaks a machine learning algorithm. It is the most important factor after the data itself. Let us look at some of the feature sets utilized in solving the problem of sentiment analysis:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">TF-IDF</strong></span>: Information Retrieval makes heavy use of <span class="strong"><strong class="calibre9">Term Frequency-Inverse </strong></span><a id="id570" class="calibre1"/><span class="strong"><strong class="calibre9">Document Frequency</strong></span> (<span class="strong"><strong class="calibre9">tf-idf</strong></span>) to enable quick information retrieval and analysis. In the context of tf-idf, a piece of text is represented as a feature vector containing words as its constituents. Recent research has also shown that, in the context of sentiment analysis, the presence of a word improves the performance and accuracy as compared to the frequency of the word.<div class="note" title="Note"><h3 class="title2"><a id="note28" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Source</strong></span>:</p><p class="calibre8">Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference <a id="id571" class="calibre1"/>on <span class="strong"><strong class="calibre9">Empirical Methods in Natural Language Processing</strong></span> (<span class="strong"><strong class="calibre9">EMNLP</strong></span>), pages 79–86, 2002.</p></div><p class="calibre20">TF-IDF is given as: <span class="strong"><img src="../images/00239.jpeg" alt="Feature extraction" class="calibre16"/></span></p><p class="calibre20">Where,</p><p class="calibre20"><code class="email">tf(t,d)</code> is the term frequency of term <code class="email">t</code> in document <code class="email">d</code>.</p><p class="calibre20"><code class="email">idf(t,D)</code> is the inverse document frequency for term <code class="email">t</code> in document set <code class="email">D</code>.</p><p class="calibre20">For example, we have the following screenshots of two documents with their terms and their corresponding frequencies:</p><div class="mediaobject"><img src="../images/00240.jpeg" alt="Feature extraction" class="calibre11"/></div><p class="calibre21"> </p><p class="calibre20">In its simplest form, <code class="email">TF-IDF</code> for the term <code class="email">Twitter</code> can be given as:</p><div class="mediaobject"><img src="../images/00241.jpeg" alt="Feature extraction" class="calibre11"/></div><p class="calibre21"> </p><p class="calibre20">Different weight schemes can be used for calculating <code class="email">tfidf</code>; the preceding example uses log with base 10 to calculate <code class="email">idf</code>.</p></li><li class="listitem"><code class="email">n-Grams</code>: Computational linguistics and probability consider a text corpus as a contiguous sequence of terms, which may be phonemes, letters, words, and so on. The n-gram-<a id="id572" class="calibre1"/>based modeling techniques find their roots in information theory, where the likelihood of the next character or word is based upon the <span class="strong"><em class="calibre10">n</em></span> previous terms. Depending upon the value of <code class="email">n</code>, the feature vector or model is termed as unigram (for <code class="email">n=1</code>), bigram (for <code class="email">n=2</code>), trigram (for <code class="email">n=3</code>), and so on. n-grams are particularly useful with out-of-vocabulary words and approximate matches. For example, considering a sequence of words, a sentence such as <span class="strong"><em class="calibre10">A chapter on sentiment analysis</em></span> would have bigrams such as <span class="strong"><em class="calibre10">a chapter</em></span>, <span class="strong"><em class="calibre10">chapter on</em></span>, <span class="strong"><em class="calibre10">on sentiment</em></span>, <span class="strong"><em class="calibre10">sentiment analysis</em></span>, and so on.<div class="note" title="Note"><h3 class="title2"><a id="note29" class="calibre1"/>Note</h3><p class="calibre8">Interesting work by Google on using <a id="id573" class="calibre1"/>n-grams: <a class="calibre1" href="http://googleresearch.blogspot.in/2006/08/all-our-n-gram-are-belong-to-you.html">http://googleresearch.blogspot.in/2006/08/all-our-n-gram-are-belong-to-you.html</a>.</p></div></li><li class="listitem"><span class="strong"><strong class="calibre9">Parts of Speech</strong></span> (<span class="strong"><strong class="calibre9">POS</strong></span>): Understanding and making use of the underlying structure of the <a id="id574" class="calibre1"/>language for analysis has obvious advantages. POS are rules of language which are used to create sentences, paragraphs and documents. In its simplest form, adjectives are usually pretty good indicators of subjectivity (not always, though). A number of approaches make use of the polarity of adjectives while classifying subjective texts. Using phrases containing adjectives has been shown to improve performance even further. Research into using other parts of speech, such as verbs and nouns, along with adjectives has also shown positive results.<div class="note" title="Note"><h3 class="title2"><a id="note30" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Reference</strong></span>:</p><p class="calibre8">Peter Turney. Thumbs up or thumbs down? Semantic orientation applied to unsupervised <a id="id575" class="calibre1"/>classification of reviews. In Proceedings of the <span class="strong"><strong class="calibre9">Association for Computational Linguistics</strong></span> (<span class="strong"><strong class="calibre9">ACL</strong></span>), pages 417–424, 2002.</p></div><p class="calibre20">The following example shows the parts of speech (adjectives, nouns, and so on) tagged in a sample sentence, for example, <span class="strong"><em class="calibre10">We saw the yellow dog</em></span>:</p><div class="mediaobject"><img src="../images/00242.jpeg" alt="Feature extraction" class="calibre11"/><div class="caption"><p class="calibre18">Source: <a class="calibre1" href="http://www.nltk.org/">http://www.nltk.org/</a></p></div></div><p class="calibre21"> </p></li><li class="listitem"><span class="strong"><strong class="calibre9">Negation</strong></span>: In the case of sentiment analysis, negation plays an important role. For example, sentences such as <span class="strong"><em class="calibre10">I like oranges</em></span> and <span class="strong"><em class="calibre10">I don't like oranges</em></span> differ only in the word <span class="strong"><em class="calibre10">don't</em></span>, but the negation term flips the polarity of sentences to opposite classes (positive and negative respectively). Negation may be used as a secondary feature set <a id="id576" class="calibre1"/>where the original feature vector is generated as is, but is flipped in polarity later on based on the negation term. There are other variants to this approach as well, and they show an improvement in the results as compared to approaches which do not take into account the effects of negation.</li><li class="listitem"><span class="strong"><strong class="calibre9">Topic specific features</strong></span>: Topic plays an important role in setting the context. Since sentiment analysis is about the speaker's opinion, the subjectivity is influenced by the topic being discussed. Extensive research has gone into analyzing the effects of and relationship between the topic and sentiment of the text corpus.</li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note31" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Reference</strong></span>:</p><p class="calibre8">Tony Mullen and Nigel Collier. Sentiment analysis using support vector machines with diverse information sources. In Proceedings of the Conference on <span class="strong"><strong class="calibre9">Empirical </strong></span><a id="id577" class="calibre1"/>
<span class="strong"><strong class="calibre9">Methods in Natural Language Processing</strong></span> (<span class="strong"><strong class="calibre9">EMNLP</strong></span>), pages 412–418, July 2004. Poster paper.</p></div></div></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Sentiment Analysis of Twitter Data">
<div class="book" title="Understanding Sentiment Analysis">
<div class="book" title="Approaches"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec78" class="calibre1"/>Approaches</h2></div></div></div><p class="calibre8">Now that we have a basic understanding of the key concepts from the world of sentiment analysis, let us look at <a id="id578" class="calibre1"/>different approaches for tackling this problem.</p><p class="calibre8">Sentiment analysis is mostly performed at the following two levels of abstraction:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Document level</strong></span>: At this level of abstraction, the task is to analyze a given document to determine <a id="id579" class="calibre1"/>whether its overall sentiment is positive or negative (or neutral in certain cases). The basic assumption is that the whole document expresses opinions related to a single entity. For example, given a product review, the system analyzes it to determine whether the review is positive or negative.</li><li class="listitem"><span class="strong"><strong class="calibre9">Sentence level</strong></span>: Sentence level analysis is a more granular form of sentiment analysis. This level <a id="id580" class="calibre1"/>of granularity counters the fact that not all sentences in a document are subjective and thus makes better use of subjectivity classification to determine the sentiment on a per sentence basis.</li></ul></div><p class="calibre8">Pretty much like other machine learning techniques, sentiment analysis can also be tackled using supervised and unsupervised methods:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Supervised Approach</strong></span>: Research on sentiment analysis has been going on for quite some time. While the earlier research was constrained by the availability of labeled <a id="id581" class="calibre1"/>datasets and performed rather shallow analysis, modern day supervised learning approaches for sentiment analysis have seen a boost, both in terms of systems utilizing these techniques as well as the overall performance of such systems due to the availability of labeled datasets. Datasets such as WordNet, SentiWordNet, SenticNet, newswire, Epinions, and so on enormously assist researchers in improving supervised algorithms by providing datasets with polar words, documents classified into categories, user opinions, and so on. Algorithms such as <a id="id582" class="calibre1"/><span class="strong"><strong class="calibre9">Naïve Bayes</strong></span>, <span class="strong"><strong class="calibre9">Support Vector Machines</strong></span> (<span class="strong"><strong class="calibre9">SVM</strong></span>), as discussed in <a class="calibre1" title="Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics" href="part0046_split_000.html#1BRPS1-973e731d75c2419489ee73e3a0cf4be8">Chapter 6</a>, <span class="strong"><em class="calibre10">Credit Risk Detection and Prediction – Predictive Analytics</em></span>, and <span class="strong"><strong class="calibre9">Maximum-Entropy-based</strong></span> classification algorithms are classic examples of supervised learning approaches.</li><li class="listitem"><span class="strong"><strong class="calibre9">Unsupervised Approach</strong></span>: Unsupervised algorithms for sentiment analysis usually begin with building or learning a sentiment lexicon and then determining the polarity of the text input. Lexicon generation has been done through techniques such as linguistic heuristics, bootstrapping, and so on. One famous approach was detailed by Turney in his 2002 paper where he describes unsupervised sentiment analysis using some fixed syntactic patterns which were based on POS.<div class="note" title="Note"><h3 class="title2"><a id="note32" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Reference</strong></span>:</p><p class="calibre8"><span class="strong"><strong class="calibre9">Linguistic heuristics</strong></span>: Vasileios Hatzivassiloglou and Kathleen McKeown. Predicting the semantic orientation of adjectives. In Proceedings of the Joint ACL/EACL Conference, pages 174–181, 1997.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Bootstrapping</strong></span>: Ellen Riloff and Janyce Wiebe. Learning extraction patterns for subjective expressions. In Proceedings of the Conference on <span class="strong"><strong class="calibre9">Empirical Methods in Natural Language Processing</strong></span> (<span class="strong"><strong class="calibre9">EMNLP</strong></span>), 2003.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Turney</strong></span>: Peter Turney. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the <span class="strong"><strong class="calibre9">Association for Computational Linguistics</strong></span> (<span class="strong"><strong class="calibre9">ACL</strong></span>), pages 417–424, 2002.</p></div></li></ul></div></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Sentiment Analysis of Twitter Data">
<div class="book" title="Understanding Sentiment Analysis">
<div class="book" title="Applications"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec79" class="calibre1"/>Applications</h2></div></div></div><p class="calibre8">As we have been discussing <a id="id583" class="calibre1"/>throughout, our reliance upon online opinions is something of a surprise. We knowingly or unknowingly check these opinions or are influenced by them before buying products, downloading software, selecting apps, or choosing a restaurant. Sentiment analysis or opinion mining finds its application in many areas; they can be summarized into the following broad categories:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Online and Offline Commerce</strong></span>: Customer preferences can make or break brands in an instant. For a product to be a hot seller, everything including pricing, packaging, and marketing has to be perfect. Customers form opinions about all aspects related to products, and thus affect their sales. It is not just the case with online commerce where customers check product reviews on multiple websites or blogs before making the actual purchase, but word of mouth and other such factors affect customer opinions in the world of offline commerce as well. Sentiment analysis thus forms an important factor which brands or companies track and analyze to be on top of the game. Analysis of social media content such as tweets, Facebook posts, blogs, and so on provide brands with an insight into how customers perceive their product. In certain cases, brands roll out specific marketing campaigns to set the general sentiment or hype about a product.</li><li class="listitem"><span class="strong"><strong class="calibre9">Governance</strong></span>: In a world where most activities have online counterparts, governments are no exceptions. There have been projects by various governments across the globe which have made use of sentiment analysis in matters of policy making and security (by analyzing and monitoring any increase in hostile or negative communications). Sentiment Analysis has also been used by analysts to determine or predict outcomes of elections as well. Tools such as <span class="strong"><em class="calibre10">eRuleMaking</em></span> have sentiment analysis as a key component.</li></ul></div><p class="calibre8">Apart from the aforementioned two categories, opinion mining acts as an augmenting technology in fields such as recommendation engines and general prediction systems. For example, opinion mining may be used in conjunction with recommender engines to exclude products from recommendation lists which have opinions or sentiments below certain thresholds. Sentiment analysis may also find innovative use in predicting whether an upcoming movie will be a blockbuster or not based on sentiments related to the star cast, production <a id="id584" class="calibre1"/>house, topic of the movie, and so on.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Sentiment Analysis of Twitter Data">
<div class="book" title="Understanding Sentiment Analysis">
<div class="book" title="Challenges"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch08lvl2sec80" class="calibre1"/>Challenges</h2></div></div></div><p class="calibre8">To understand the <a id="id585" class="calibre1"/>opinions and/or sentiments of others is an inherently difficult task. To be able to handle such a problem algorithmically is equally hard. The following are some of the challenges faced while performing Sentiment Analysis:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Understanding and Modeling Natural Language Constructs</strong></span>: Sentiment analysis is inherently a <span class="strong"><strong class="calibre9">natural language processing</strong></span> (<span class="strong"><strong class="calibre9">NLP</strong></span>) problem, albeit a restricted <a id="id586" class="calibre1"/>one. Even though sentiment analysis is a restricted form of NLP, involving classification into positive, negative or neutral, it still faces issues like coreference resolution, word sense disambiguation, and negation handling to name a few. Advancements in NLP, as well as Sentiment Analysis, in recent years have helped in overcoming these issues to a certain extent, yet there is a long way to travel before we will be able to model the rules of a natural language perfectly.</li><li class="listitem"><span class="strong"><strong class="calibre9">Sarcasm</strong></span>: Sentiments can be expressed in pretty subtle ways. It is not just negative sentiments; positive sentiments can also be nicely disguised within sarcastic sentences. Since understanding sarcasm is a trick only a few can master, it is not easy to model sarcasm and identify sentiment correctly. For example, the comment <span class="strong"><em class="calibre10">Such a simple to use product, you just need to read 300 pages from the manual</em></span>, contains only positive words yet has a negative flavor to it which is not easy to model.</li><li class="listitem"><span class="strong"><strong class="calibre9">Review and reviewer quality</strong></span>: Opinions vary from person to person. Some of us may present our opinions very strongly while others may not. Another issue is that everyone has an opinion, whether they know about a subject or not. This creates a problem of review and reviewer quality, which may affect overall analysis. For example, a person who is a casual reader may not be the most apt person to ask for a review of a new book. Similarly, it may not be advisable to get a new author's book reviewed by a critic. Both extremes may result in biased outcomes or incorrect insights.</li><li class="listitem"><span class="strong"><strong class="calibre9">Opinion data size and skew</strong></span>: The web has loads and loads of blogs and sites which provide users with a platform to voice and share opinions on everything possible on and beyond the planet. Still, the opinion data at a granular level is an issue. As we discussed in the previous chapter, the amount of data related to a particular context (say a brand or a person) is so limited that it affects the overall analysis. Moreover, the data available is sometimes <a id="id587" class="calibre1"/>skewed in favor of (or against) entities due to prejudices, incorrect facts, or rumors.</li></ul></div></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Sentiment analysis upon Tweets"><div class="book" id="1VSLM2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec61" class="calibre1"/>Sentiment analysis upon Tweets</h1></div></div></div><p class="calibre8">Now that we are <a id="id588" class="calibre1"/>equipped with the key terms and concepts from the world of Sentiment Analysis, let us put our theory to the test. We have seen some major application areas for Sentiment Analysis and the challenges faced, in general, to perform such analytics. In this section we will perform Sentiment Analysis categorized into:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Polarity analysis</strong></span>: This will involve the scoring and aggregation of sentiment polarity using a labeled list of positive and negative words.</li><li class="listitem"><span class="strong"><strong class="calibre9">Classification-based analysis</strong></span>: In this approach we will make use of R's rich libraries to perform classification based on labeled tweets available for public usage. We will also discuss their performance and accuracy.</li></ul></div><p class="calibre8">R has a very robust library for the extraction and manipulation of information from Twitter called <code class="email">TwitteR</code>. As we saw in the previous chapter, we first need to create an application using Twitter's application management console before we can use TwitteR or any other library for sentiment analysis. For this chapter, we will be reusing the application from the previous chapter (keep your application keys and secrets handy). Also, in the coming sections, we will be utilizing our code from previous chapters in a more structured format to enable reuse and to follow <code class="email">#bestCodingPractices</code>.</p><p class="calibre8">Before we begin our analysis, let us first restructure our existing code and write some helper functions, which will come in handy later on. As we know, data from Twitter can be extracted using search terms or from a user's timeline. The following two helper functions help us to do exactly the same tasks in a reusable fashion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#extract search tweets</strong></span>
<span class="strong"><strong class="calibre9">extractTweets &lt;- function(searchTerm,tweetCount){</strong></span>
<span class="strong"><strong class="calibre9">  # search term tweets</strong></span>
<span class="strong"><strong class="calibre9">  tweets = searchTwitter(searchTerm,n=tweetCount)</strong></span>
<span class="strong"><strong class="calibre9">  tweets.df = twListToDF(tweets)</strong></span>
<span class="strong"><strong class="calibre9">  tweets.df$text &lt;- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  return(tweets.df)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>

<span class="strong"><strong class="calibre9">#extract timeline tweets</strong></span>
<span class="strong"><strong class="calibre9">extractTimelineTweets &lt;- function(username,tweetCount){</strong></span>
<span class="strong"><strong class="calibre9">  # timeline tweets</strong></span>
<span class="strong"><strong class="calibre9">  twitterUser &lt;- getUser(username)</strong></span>
<span class="strong"><strong class="calibre9">  tweets = userTimeline(twitterUser,n=tweetCount)</strong></span>
<span class="strong"><strong class="calibre9">  tweets.df = twListToDF(tweets)</strong></span>
<span class="strong"><strong class="calibre9">  tweets.df$text &lt;- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  return(tweets.df)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">The function <code class="email">extractTweets</code> takes the <code class="email">search</code> term and number of tweets to be extracted as inputs and <a id="id589" class="calibre1"/>returns the data in a data frame which contains text converted to UTF8 encoding. Similarly, the function <code class="email">extractTimelineTweets</code> takes the username and number of tweets as inputs and returns data in a data frame with the text converted to UTF8 encoding. Therefore, the preceding two functions will help us to extract tweets multiple times (based on different <code class="email">search</code> terms or users) without rewriting the same lines of code again and again.</p><p class="calibre8">Continuing with the same theme, we will write another helper function to clean and transform our data set. As we saw in the previous chapter, R's <code class="email">tm</code> library provides us with various utility functions to quickly clean and transform text corpus. In this function, we will make use of <code class="email">tm_map</code> to transform our tweets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># clean and transform tweets</strong></span>
<span class="strong"><strong class="calibre9">transformTweets &lt;- function(tweetDF){</strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- Corpus(VectorSource(tweetDF$text))</strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- tm_map(tweetCorpus, tolower)</strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- tm_map(tweetCorpus, removePunctuation)</strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- tm_map(tweetCorpus, removeNumbers)</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  # remove URLs</strong></span>
<span class="strong"><strong class="calibre9">  removeURL &lt;- function(x) gsub("http://[[:alnum:]]*", "", x)</strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- tm_map(tweetCorpus, removeURL) </strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  # remove stop words</strong></span>
<span class="strong"><strong class="calibre9">  twtrStopWords &lt;- c(stopwords("english"),'rt','http','https')</strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- tm_map(tweetCorpus, removeWords, twtrStopWords)</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  tweetCorpus &lt;- tm_map(tweetCorpus, PlainTextDocument)</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  #convert back to dataframe</strong></span>
<span class="strong"><strong class="calibre9">  tweetDataframe &lt;- data.frame(text=unlist(sapply(tweetCorpus, </strong></span>
<span class="strong"><strong class="calibre9">                    `[`, "content")), stringsAsFactors=F)</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  #split each doc into words</strong></span>
<span class="strong"><strong class="calibre9">  splitText &lt;- function(x) {</strong></span>
<span class="strong"><strong class="calibre9">    word.list = str_split(x, '\\s+')</strong></span>
<span class="strong"><strong class="calibre9">    words = unlist(word.list)</strong></span>
<span class="strong"><strong class="calibre9">  }</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  # attach list of words to the data frame</strong></span>
<span class="strong"><strong class="calibre9">  tweetDataframe$wordList = sapply(</strong></span>
<span class="strong"><strong class="calibre9">                    tweetDataframe$text,</strong></span>
<span class="strong"><strong class="calibre9">                    function(text) splitText(text))</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  return (tweetDataframe)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">In addition to the usual transformations, such as stop word removal, change to lower case, punctuation <a id="id590" class="calibre1"/>removal, and so on, the function <code class="email">transformTweets</code> tokenizes each tweet at word level and attaches the list of words in each tweet to the object. Also, the function returns the transformed tweets in a data frame for further manipulation.</p></div>

<div class="book" title="Sentiment analysis upon Tweets">
<div class="book" title="Polarity analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec81" class="calibre1"/>Polarity analysis</h2></div></div></div><p class="calibre8">Polarity, as discussed in the <a id="id591" class="calibre1"/>section <span class="strong"><em class="calibre10">Key Concepts</em></span>, is the positive, negative or neutral classification of the piece of text in consideration. The class labels <a id="id592" class="calibre1"/>may change depending upon the context (liked versus disliked or favorable versus unfavorable). Polarity may also have a degree attached to it which places the analyzed text on a continuous (or discrete) scale of polarities (say from <code class="email">-5</code> to <code class="email">5</code>). This degree of polarity helps us analyze the extent (or degree) of positivity (or negativity) in the text. This is particularly useful in comparative studies as we have the opportunity to view analyzed text with reference to certain benchmarks.</p><p class="calibre8">In this section, we will analyze tweets and score each of them based on the polar words identified in each of the tweets. The simple and easy-to-code algorithm is outlined in the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Extract tweets based on selected search terms or Twitter handles.</li><li class="listitem" value="2">Clean and transform tweets into a suitable format for ease of analysis. Tokenize tweets into a constituent list of words.</li><li class="listitem" value="3">Load the list of positive and negative words to be used for polar word identification.</li><li class="listitem" value="4">For each tweet, count the number of positive and negative words that match the list of positive and negative words obtained in the preceding step 3.</li><li class="listitem" value="5">Assign a polarity score to each tweet based on the difference between positive and negative matches in the preceding step.</li></ol><div class="calibre14"/></div><p class="calibre8">The preceding <a id="id593" class="calibre1"/>steps are represented diagrammatically as follows:</p><div class="mediaobject"><img src="../images/00243.jpeg" alt="Polarity analysis" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Once each tweet in the dataset has been scored, we may aggregate the scores to understand the overall sentiment distribution related to the search terms or Twitter handle. Positive values define a positive sentiment; larger numbers denote a greater degree of positivity, and similarly for negative sentiments. A neutral stance is represented by a score of 0. For example, <span class="strong"><em class="calibre10">This car is amazingly fast and beautiful</em></span> has a greater degree of positivity than <span class="strong"><em class="calibre10">This is a nice car</em></span>, though both are positive sentences.</p><p class="calibre8">Let us use this <a id="id594" class="calibre1"/>algorithm to analyze sentiments using search terms and Twitter handles. As discussed previously, opinion mining has become essential, not just for brands but for governments as well. Every entity out there wants to gauge how its target audience feels about it and its initiatives, and governments are no exception. Of late, the Indian Government has been utilizing Twitter and other social media platforms effectively to reach its audience and make them aware about its initiatives and policies. One such initiative is the recently launched Make in India initiative. Consider a scenario where one is tasked with analyzing the effectiveness of and public opinion related to such an initiative. To analyze public opinion, which changes dynamically over time, Twitter would be a good choice. So, to analyze sentiments for the Make in India initiative, let us analyze some tweets.</p><p class="calibre8">As previously outlined, we start by connecting to Twitter and extracting tweets related to the search term <span class="strong"><em class="calibre10">Make In India</em></span>. This is followed by the preprocessing step, where we remove stop words, URLs, and so on to transform the tweets into a usable format. We also tokenize each tweet into a list of constituent words for use in the coming steps. Once our dataset is ready and in a consumable format, we load the precompiled list of positive and negative words. The list is available from <a class="calibre1" href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a>.</p><p class="calibre8">We first write a reusable <code class="email">analyzeTrendSentiments</code> function which takes the search term and number of tweets to be extracted as inputs. It makes use of the functions <code class="email">extractTweets</code> and <a id="id595" class="calibre1"/>
<code class="email">transformTweets</code> to get the job done:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">analyzeTrendSentiments &lt;- function(search,tweetCount){ </strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  #extract tweets</strong></span>
<span class="strong"><strong class="calibre9">  tweetsDF &lt;- extractTweets(search,tweetCount)</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  # transformations</strong></span>
<span class="strong"><strong class="calibre9">  transformedTweetsDF &lt;- transformTweets(tweetsDF)</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  #score the words  </strong></span>
<span class="strong"><strong class="calibre9">  transformedTweetsDF$sentiScore = sapply(transformedTweetsDF$wordList,function(wordList) scoreTweet(wordList))</strong></span>

<span class="strong"><strong class="calibre9">  transformedTweetsDF$search &lt;- search</strong></span>
<span class="strong"><strong class="calibre9">  </strong></span>
<span class="strong"><strong class="calibre9">  return(transformedTweetsDF) </strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">We then use the <a id="id596" class="calibre1"/>function <code class="email">analyzeTrendSentiments</code> to get a data frame consisting of tweets scored using a precompiled list of polar words. We use <code class="email">twitteR</code>, <code class="email">ggplot2</code>, <code class="email">stringr</code> and <code class="email">tm</code> libraries as well:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">library(twitteR)</strong></span>
<span class="strong"><strong class="calibre9">library(stringr)</strong></span>
<span class="strong"><strong class="calibre9">library(tm)</strong></span>
<span class="strong"><strong class="calibre9">library(ggplot2)</strong></span>

<span class="strong"><strong class="calibre9">consumerSecret = "XXXXXXXXXX"</strong></span>
<span class="strong"><strong class="calibre9">consumerKey = "XXXXXXXXXXXXXXXXXXXXXXXXX"</strong></span>

<span class="strong"><strong class="calibre9">setup_twitter_oauth(consumer_key = consumerKey,consumer_secret = consumerSecret)</strong></span>

<span class="strong"><strong class="calibre9"># list of positive/negative words from opinion lexicon</strong></span>
<span class="strong"><strong class="calibre9">pos.words = scan(file= 'positive-words.txt', what='character', comment.char=';')</strong></span>

<span class="strong"><strong class="calibre9">neg.words = scan(file= 'negative-words.txt', what='character', comment.char=';')</strong></span>

<span class="strong"><strong class="calibre9">#extract 1500 tweets on the given topic</strong></span>
<span class="strong"><strong class="calibre9">makeInIndiaSentiments &lt;- analyzeTrendSentiments("makeinindia",1500)</strong></span>

<span class="strong"><strong class="calibre9">#plot the aggregated scores on a histogram</strong></span>
<span class="strong"><strong class="calibre9">qplot(makeInIndiaSentiments $sentiScore)</strong></span>
</pre></div><p class="calibre8">In the last chapter, we learned and used different visualizations to grasp the insights hidden in our analysis. Continuing with the same thought process, we generate a histogram of aggregated <a id="id597" class="calibre1"/>scores. The visualization looks like <a id="id598" class="calibre1"/>this:</p><div class="mediaobject"><img src="../images/00244.jpeg" alt="Polarity analysis" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The histogram is easy to interpret. It shows the tweets distributed across a polarity scale on the <span class="strong"><em class="calibre10">x</em></span>-axis and frequency of tweets on the <span class="strong"><em class="calibre10">y</em></span>-axis. The results show a normal distribution with a general tilt towards the positive side. It seems the initiative is getting a positive response from its audience.</p><p class="calibre8">Going a bit deeper into the analysis itself, let us analyze the sentiments for the same search term and see how the opinions change over time.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note33" class="calibre1"/>Note</h3><p class="calibre8">The tweets for this analysis were extracted on the day the initiative was launched as well as a day later. Your results may vary due to the dynamic nature of Twitter. You may observe a difference in outcomes across other examples in this chapter as well. We urge you to be creative and try out other trending topics while working through <a id="id599" class="calibre1"/>examples from this chapter.</p></div><p class="calibre8">The output looks like this:</p><div class="mediaobject"><img src="../images/00245.jpeg" alt="Polarity analysis" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The preceding two histograms show a shift in opinions over the course of two days. If you were following the news at the time, in one of the events for this initiative a sudden fire broke out and burnt the whole stage. The graph on top is based upon tweets after the fire broke out while the graph labeled <span class="strong"><strong class="calibre9">makeinindia_yday </strong></span>refers to the tweets from the day before. Though the <a id="id600" class="calibre1"/>shift in sentiments isn't drastic, it is clearly visible that the shift has been more towards the positive side (some tweets are even hitting a score of 6+). Could this be because tweeple started praising the emergency teams and police for their quick action in preventing casualties? Well, it looks like Twitter isn't just about people ranting on random stuff!</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note34" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">World leaders</strong></span></p><p class="calibre8">Twitter has caught the frenzy of celebrities and politicians alike. As a quick exercise, try analyzing tweets from the twitter handles of world leaders such as <code class="email">@potus</code>, <code class="email">@pmoindia</code>, and <code class="email">@number10gov</code> to see what kind of opinions our leaders project through Twitter. Don't be surprised if their timelines are neutral...oops, diplomatic!</p></div><div class="mediaobject"><img src="../images/00246.jpeg" alt="Polarity analysis" class="calibre11"/></div><p class="calibre12"> </p></div></div>

<div class="book" title="Sentiment analysis upon Tweets">
<div class="book" title="Classification-based algorithms"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec82" class="calibre1"/>Classification-based algorithms</h2></div></div></div><p class="calibre8">A classification <a id="id601" class="calibre1"/>problem requires the labeling of input data into required classes based on some defined characteristics of each class (see <a class="calibre1" title="Chapter 2. Let's Help Machines Learn" href="part0022_split_000.html#KVCC1-973e731d75c2419489ee73e3a0cf4be8">Chapter 2</a>, <span class="strong"><em class="calibre10">Let's Help Machines Learn</em></span>, for details). In the case of sentiment analysis, the classes are positive and negative (or neutral in certain cases). We have learned about different classification algorithms and seen how they are used across domains to solve categorization and classification problems in the previous chapters, and sentiment analysis is yet another domain where these algorithms are highly useful.</p><p class="calibre8">In this section, we will perform opinion mining using classification algorithms such as SVM and boosting. We will also touch upon ensemble methods and see how they help to improve performance. Note that, for this section, we will concentrate only on the positive and negative polarities, but the approach is generic enough to be easily extended to include the neutral polarity as well.</p><div class="book" title="Labeled dataset"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec28" class="calibre1"/>Labeled dataset</h3></div></div></div><p class="calibre8">Since this is a supervised <a id="id602" class="calibre1"/>learning approach, we require labeled data for training and testing the performance of our algorithms. For the purpose of this chapter, we will utilize a labeled dataset from <a class="calibre1" href="http://www.sentiment140.com/">http://www.sentiment140.com/</a>. It contains tweets labeled as 0, 2, and 4 for negative, neutral and positive sentiments, respectively. There are various attributes such as <code class="email">tweet ID</code>, <code class="email">date</code>, <code class="email">search query</code>, <code class="email">username</code>, and the <code class="email">tweet text</code>, apart from the sentiment label. For our case we will be considering only the tweet text and its corresponding label.</p><div class="mediaobject"><img src="../images/00247.jpeg" alt="Labeled dataset" class="calibre11"/></div><p class="calibre12"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note35" class="calibre1"/>Note</h3><p class="calibre8">Another source of labeled <a id="id603" class="calibre1"/>tweets is available at <a class="calibre1" href="https://github.com/guyz/twitter-sentiment-dataset">https://github.com/guyz/twitter-sentiment-dataset</a>. This source makes use of a python script to download around 5000 labeled tweets keeping Twitter API guidelines in mind.</p></div><p class="calibre8">Before we get into the algorithm-specific details, let us look into the labeled dataset and perform the initial steps <a id="id604" class="calibre1"/>of collecting and transforming our data into the required forms. We will make use of libraries such as <code class="email">caret</code> and <code class="email">RTextTools</code> for these steps.</p><p class="calibre8">As mentioned previously, the dataset contains polarities labeled as 0, 2, and 4 for negative, neutral, and positive. We will load the <code class="email">csv</code> file in R and apply a quick transformation to change the labels to positive and negative. Once the polarities have been transformed into intelligible names, we will filter out the rows of data containing neutral sentiments. Also, we will keep only the columns for polarity and tweet text, and remove the rest.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># load labeled dataset</strong></span>
<span class="strong"><strong class="calibre9">labeledDSFilePath = "labeled_tweets.csv"</strong></span>
<span class="strong"><strong class="calibre9">labeledDataset = read.csv(labeledDSFilePath, header = FALSE)</strong></span>

<span class="strong"><strong class="calibre9"># transform polarity labels</strong></span>
<span class="strong"><strong class="calibre9">labeledDataset$V1 = sapply(labeledDataset$V1, </strong></span>
<span class="strong"><strong class="calibre9">    function(x) </strong></span>
<span class="strong"><strong class="calibre9">      if(x==4) </strong></span>
<span class="strong"><strong class="calibre9">        x &lt;- "positive" </strong></span>
<span class="strong"><strong class="calibre9">      else if(x==0) </strong></span>
<span class="strong"><strong class="calibre9">        x&lt;-"negative" </strong></span>
<span class="strong"><strong class="calibre9">      else x&lt;- "none")</strong></span>

<span class="strong"><strong class="calibre9">#select required columns only</strong></span>
<span class="strong"><strong class="calibre9">requiredColumns &lt;- c("V1","V6")</strong></span>

<span class="strong"><strong class="calibre9"># extract only positive/negative labeled tweets </strong></span>
<span class="strong"><strong class="calibre9">tweets&lt;-as.matrix(labeledDataset[labeledDataset$V1 </strong></span>
<span class="strong"><strong class="calibre9">      %in% c("positive","negative")</strong></span>
<span class="strong"><strong class="calibre9">      ,requiredColumns])</strong></span>
</pre></div><p class="calibre8">The <code class="email">tweets</code> object is now available as a matrix with each row representing a tweet, and with columns referring to polarity and tweet text. Before we transform this matrix into the formats required by the classification algorithms, we need to split our data into training and testing datasets (see <a class="calibre1" title="Chapter 2. Let's Help Machines Learn" href="part0022_split_000.html#KVCC1-973e731d75c2419489ee73e3a0cf4be8">Chapter 2</a>, <span class="strong"><em class="calibre10">Let's Help Machines Learn</em></span>, for more on this). Since both the training and testing datasets should contain a good enough distribution of samples of all classes for the purposes of training and testing, we use the <code class="email">createDataPartition</code> <a id="id605" class="calibre1"/>function available from the <code class="email">caret</code> package. For our use case, we split our data into 70/30 training and testing datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">indexes &lt;- createDataPartition(tweets[,1], p=0.7, list = FALSE)</strong></span>

<span class="strong"><strong class="calibre9">train.data &lt;- tweets[indexes,]</strong></span>
<span class="strong"><strong class="calibre9">test.data &lt;- tweets[-indexes,]</strong></span>
</pre></div><p class="calibre8">We perform a quick check to see how our data is split across the positive and negative classes in our original dataset, and the training and testing datasets. You will see the result in the following screenshot:</p><p class="calibre8"><span class="strong"><img src="../images/00248.jpeg" alt="Labeled dataset" class="calibre16"/></span></p><p class="calibre8">As we can see, <code class="email">createDataPartition</code> has done a nice job of maintaining a similar sentiment distribution across the training and testing datasets.</p><p class="calibre8">Next in the line of transformations is the Document Term Matrix transformation. As we have seen in <a class="calibre1" title="Chapter 7. Social Media Analysis – Analyzing Twitter Data" href="part0059_split_000.html#1O8H61-973e731d75c2419489ee73e3a0cf4be8">Chapter 7</a>, <span class="strong"><em class="calibre10">Social Media Analysis – Analyzing Twitter Data</em></span>, a document term matrix transforms a given dataset into rows representing the documents and columns of terms (words/sentences). Unlike the previous chapter, where we used the <code class="email">tm</code> library's <code class="email">DocumentTermMatrix</code> function for transformation and applied various transformations using <code class="email">tm_map</code>, for the current use case we will use the <code class="email">create_matrix</code> function from the <code class="email">RTextTools</code> library. This function is an abstraction over <code class="email">tm</code>'s corresponding functions. We will also assign weights to each of the terms using <code class="email">tfidf</code> as our feature. The <code class="email">create_matrix</code> method also helps us take care of splitting sentences into words, stop words and number removal, and stemming them as well. Here's how you do it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">train.dtMatrix &lt;- create_matrix(train.data[,2], </strong></span>
<span class="strong"><strong class="calibre9">                        language="english" , </strong></span>
<span class="strong"><strong class="calibre9">                        removeStopwords=TRUE, </strong></span>
<span class="strong"><strong class="calibre9">                        removeNumbers=TRUE,</strong></span>
<span class="strong"><strong class="calibre9">                        stemWords=TRUE,</strong></span>
<span class="strong"><strong class="calibre9">                        weighting = tm::weightTfIdf)</strong></span>
<span class="strong"><strong class="calibre9">            </strong></span>

<span class="strong"><strong class="calibre9">            </strong></span>
<span class="strong"><strong class="calibre9">test.dtMatrix &lt;- create_matrix(test.data[,2], </strong></span>
<span class="strong"><strong class="calibre9">                               language="english" , </strong></span>
<span class="strong"><strong class="calibre9">                               removeStopwords=TRUE, </strong></span>
<span class="strong"><strong class="calibre9">                               removeNumbers=TRUE,</strong></span>
<span class="strong"><strong class="calibre9">                               stemWords=TRUE,</strong></span>
<span class="strong"><strong class="calibre9">                               weighting = tm::weightTfIdf,</strong></span>
<span class="strong"><strong class="calibre9">                               originalMatrix=train.dtMatrix)</strong></span>

<span class="strong"><strong class="calibre9">test.data.size &lt;- nrow(test.data)</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note36" class="calibre1"/>Note</h3><p class="calibre8">The <code class="email">create_matrix</code> method in <code class="email">RTextTools v1.4.2</code> has a small bug which prevents weight assignment when using <code class="email">originalMatrix</code> option. The following small <a id="id606" class="calibre1"/>hack can be used to fix the issue till the library gets updated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">&gt;  trace("create_matrix",edit=T) </strong></span>
</pre></div><p class="calibre8">Scroll to line 42 and update  Acronym to  acronym.</p><p class="calibre8">Check the following links for more details and alternate ways of handling this issue:</p><p class="calibre8"><a class="calibre1" href="https://github.com/timjurka/RTextTools/issues/4">https://github.com/timjurka/RTextTools/issues/4</a></p><p class="calibre8"><a class="calibre1" href="http://stackoverflow.com/questions/16630627/recreate-same-document-term-matrix-with-new-data">http://stackoverflow.com/questions/16630627/recreate-same-document-term-matrix-with-new-data</a></p></div><p class="calibre8">Now that we have both the training and testing datasets in the <code class="email">DocumentTermMatrix</code> format, we can proceed towards the classification algorithms and let our machines learn and build sentiment classifiers!</p></div><div class="book" title="Support Vector Machines"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec29" class="calibre1"/>Support Vector Machines</h3></div></div></div><p class="calibre8"><span class="strong"><strong class="calibre9">Support Vector </strong></span><a id="id607" class="calibre1"/>
<span class="strong"><strong class="calibre9">Machines</strong></span>, or <span class="strong"><strong class="calibre9">SVM</strong></span> as they are commonly known, are one of the most versatile classes of supervised learning algorithms for classification. An SVM builds a model in such a way that the data points belonging to different classes are separated by a clear gap, which is optimized such that the distance of separation is the maximum possible. The samples on the margins are called the support <a id="id608" class="calibre1"/>vectors, which are separated by a hyperplane (see <a class="calibre1" title="Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics" href="part0046_split_000.html#1BRPS1-973e731d75c2419489ee73e3a0cf4be8">Chapter 6</a>, <span class="strong"><em class="calibre10">Credit Risk Detection and Prediction – Predictive Analytics </em></span>for more details).</p><p class="calibre8">Since our current use case for sentiment analysis is also a binary (positive and negative) classification problem, SVM helps us build a model using the training dataset, which separates tweets into positive and negative sentiment classes, respectively.</p><p class="calibre8">We will use <code class="email">e1071</code> library's <code class="email">svm</code> function to build a sentiment classifier. We start off with the default values for the SVM classifier available from the library and follow the same iterative procedure we did in <a class="calibre1" title="Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics" href="part0046_split_000.html#1BRPS1-973e731d75c2419489ee73e3a0cf4be8">Chapter 6</a>, <span class="strong"><em class="calibre10">Credit Risk Detection and Prediction – Predictive Analytics</em></span>, to finally arrive at the best classifier. The following snippet of code builds a <a id="id609" class="calibre1"/>sentiment classifier using the default values and then prints a confusion matrix, along with other statistics for evaluation, as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">svm.model &lt;- svm(train.dtMatrix, as.factor(train.data[,1]))</strong></span>

<span class="strong"><strong class="calibre9">## view inital model details</strong></span>
<span class="strong"><strong class="calibre9">summary(svm.model)</strong></span>

<span class="strong"><strong class="calibre9">## predict and evaluate results</strong></span>
<span class="strong"><strong class="calibre9">svm.predictions &lt;- predict(svm.model, test.dtMatrix)</strong></span>

<span class="strong"><strong class="calibre9">true.labels &lt;- as.factor(test.data[,1])</strong></span>

<span class="strong"><strong class="calibre9">confusionMatrix(data=svm.predictions, reference=true.labels, positive="positive")</strong></span>
</pre></div><p class="calibre8">The confusion matrix generated as follows shows that the classifier has just <span class="strong"><em class="calibre10">50% accuracy</em></span>, which is as bad as a coin toss, with no predictions for negative sentiments whatsoever! It seems like the classifier couldn't infer or learn much from the training dataset.</p><p class="calibre8"><span class="strong"><img src="../images/00249.jpeg" alt="Support Vector Machines" class="calibre16"/></span></p><p class="calibre8">To build a better-performing model, we will now go under the hood and tweak some parameters. The <code class="email">svm</code> implementation from <code class="email">e1071</code> provides us with a wonderful utility called <code class="email">tune</code> to <a id="id610" class="calibre1"/>obtain the optimized values of <a id="id611" class="calibre1"/>hyperparameters using a grid search over the given parameter ranges:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">## hyperparameter optimizations</strong></span>

<span class="strong"><strong class="calibre9"># run grid search</strong></span>
<span class="strong"><strong class="calibre9">cost.weights &lt;- c(0.1, 10, 100)</strong></span>
<span class="strong"><strong class="calibre9">gamma.weights &lt;- c(0.01, 0.25, 0.5, 1)</strong></span>
<span class="strong"><strong class="calibre9">tuning.results &lt;- tune(svm, train.dtMatrix, as.factor(train.data[,1]), kernel="radial", </strong></span>
<span class="strong"><strong class="calibre9">                       ranges=list(cost=cost.weights, gamma=gamma.weights))</strong></span>

<span class="strong"><strong class="calibre9"># view optimization results</strong></span>
<span class="strong"><strong class="calibre9">print(tuning.results)</strong></span>

<span class="strong"><strong class="calibre9"># plot results</strong></span>
<span class="strong"><strong class="calibre9">plot(tuning.results, cex.main=0.6, cex.lab=0.8,xaxs="i", yaxs="i")</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note38" class="calibre1"/>Note</h3><p class="calibre8">In the code snippet above, we have utilized the <span class="strong"><strong class="calibre9">radial bias kernel</strong></span> (or <span class="strong"><strong class="calibre9">rbf</strong></span> for short) for hyperparameter optimization. The motivation for using rbf was due to its better performance with <a id="id612" class="calibre1"/>respect to <span class="strong"><em class="calibre10">specificity</em></span> and <span class="strong"><em class="calibre10">sensitivity</em></span> even though the overall accuracy was comparable to <span class="strong"><em class="calibre10">linear</em></span> kernels. We urge our readers to try out linear kernels and observe the difference in the overall results. Please note that, for text classification, linear kernels usually perform better than other kernels, not only in terms of accuracy but in performance as well</p></div><p class="calibre8">The <a id="id613" class="calibre1"/>parameter-tuning results in optimized <a id="id614" class="calibre1"/>values for hyperparameters <code class="email">cost</code> and <code class="email">gamma</code> as <code class="email">10</code> and <code class="email">0.01</code>, respectively; the following plot confirms the same (darkest region corresponds to best values).</p><div class="mediaobject"><img src="../images/00250.jpeg" alt="Support Vector Machines" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The following snippet of code uses the best model to predict and prepare a confusion matrix, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># get best model and evaluate predictions</strong></span>
<span class="strong"><strong class="calibre9">svm.model.best = tuning.results$best.model</strong></span>

<span class="strong"><strong class="calibre9">svm.predictions.best &lt;- predict(svm.model.best, test.dtMatrix)</strong></span>

<span class="strong"><strong class="calibre9">confusionMatrix(data=svm.predictions.best, reference=true.labels, positive="positive")</strong></span>
</pre></div><p class="calibre8">The following confusion matrix shows the predictions from a much improved model. From a mere <a id="id615" class="calibre1"/>50% accuracy to a comfortable <a id="id616" class="calibre1"/>80% and above is a good leap. Let us check the ROC curves for this model to confirm that the accuracy is indeed good enough:</p><p class="calibre8"><span class="strong"><img src="../images/00251.jpeg" alt="Support Vector Machines" class="calibre16"/></span></p><p class="calibre8">To prepare the ROC curves, we will reuse our utility script <code class="email">performance_plot_utils.R</code> from <a class="calibre1" title="Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics" href="part0046_split_000.html#1BRPS1-973e731d75c2419489ee73e3a0cf4be8">Chapter 6</a>, <span class="strong"><em class="calibre10">Credit Risk Detection and Prediction – Predictive Analytics</em></span>, and pass the predictions from the optimized model to it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># plot best model evaluation metric curves</strong></span>
<span class="strong"><strong class="calibre9">svm.predictions.best &lt;- predict(svm.model.best, test.dtMatrix, decision.values = T)</strong></span>

<span class="strong"><strong class="calibre9">svm.prediction.values &lt;- attributes(svm.predictions.best)</strong></span>
<span class="strong"><strong class="calibre9">$decision.values</strong></span>

<span class="strong"><strong class="calibre9">predictions &lt;- prediction(svm.prediction.values, true.labels)</strong></span>

<span class="strong"><strong class="calibre9">par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre9">plot.roc.curve(predictions, title.text="SVM ROC Curve")</strong></span>
<span class="strong"><strong class="calibre9">plot.pr.curve(predictions, title.text="SVM Precision/Recall Curve")</strong></span>
</pre></div><p class="calibre8">The ROC curves <a id="id617" class="calibre1"/>generated using the preceding code snippet are as follows:</p><div class="mediaobject"><img src="../images/00252.jpeg" alt="Support Vector Machines" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The ROC curves also confirm a well-learned model with an AUC of 0.89. We can therefore use this model to classify tweets into positive or negative classes. We encourage readers to try out ROC-b<a id="id618" class="calibre1"/>ased optimizations and observe if there are any further improvements in the model.</p></div><div class="book" title="Ensemble methods"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec30" class="calibre1"/>Ensemble methods</h3></div></div></div><p class="calibre8">Supervised <a id="id619" class="calibre1"/>Machine Learning algorithms, in a nutshell, are about <a id="id620" class="calibre1"/>learning the underlying functions or patterns which help us predict accurately (within certain bounds) based on historic data. Over the course of this book, we have come across many such algorithms and, although R makes it easy to code and test these, it is worth mentioning that learning a highly accurate function or pattern is not an easy task. Building highly complex models leads us to issues of overfitting and underfitting, to name a few. Amidst all this confusion, it is to be noted that it is always easy to learn simple rules and functions.</p><p class="calibre8">For example, to classify an email as spam or not spam there are multiple rules which a machine learning algorithm would have to learn, rules such as:</p><div class="book"><ul class="itemizedlist"><li class="listitem">E-mails containing text such as <span class="strong"><em class="calibre10">buy now</em></span> are spam</li><li class="listitem">E-mails <a id="id621" class="calibre1"/>containing more than five hyperlinks are spam</li><li class="listitem">E-mails from contacts in the address book are not spam</li></ul></div><p class="calibre8">And many more such rules. Given a training dataset, say <code class="email">T</code> of labeled emails, a machine learning algorithm (specifically a classification algorithm) will generate a classifier, <code class="email">C</code>, which is a hypothesis of an underlying function or pattern. We then use this classifier <code class="email">C</code> to predict the labels for new emails.</p><p class="calibre8">On the other hand, an ensemble of classifiers is defined as a set of classifiers whose outputs are combined in some way to classify new examples. The main discovery in the field of machine learning-based ensembles is that ensembles perform much better than the individual classifiers they are made of.</p><p class="calibre8">A necessary and <a id="id622" class="calibre1"/>sufficient condition for ensembles to be better than their constituents is that they should be <span class="strong"><em class="calibre10">accurate</em></span> and <span class="strong"><em class="calibre10">diverse</em></span>. A classifier is termed <span class="strong"><em class="calibre10">accurate</em></span> if its predictions are better than random guessing (see weak learners ). While two classifiers are termed as <span class="strong"><em class="calibre10">diverse</em></span> if they make different errors on the same data points.</p><p class="calibre8">We can define a <span class="strong"><strong class="calibre9">weak learner</strong></span> as a learner whose predictions and decisions are at least better than random guessing. Weak learners are also termed as base learners or meta learners.</p><p class="calibre8">The following block diagram visualizes the concept of ensemble classifiers:</p><div class="mediaobject"><img src="../images/00253.jpeg" alt="Ensemble methods" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">As seen in the <a id="id623" class="calibre1"/>preceding block diagram, the training dataset is split into <span class="strong"><em class="calibre10">n</em></span> datasets (the splitting or generation of such datasets is dependent upon the ensemble-ing methodology) upon which weak learners (the same or different weak learners, again, depends upon the ensemble methodology) build models. These models are then combined based on weighted or unweighted voting to prepare a final model, which is used for classification. The mathematical proofs of why ensembles work are fairly <a id="id624" class="calibre1"/>involved and beyond the scope of this <a id="id625" class="calibre1"/>book.</p><div class="book" title="Boosting"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch08lvl4sec06" class="calibre1"/>Boosting</h4></div></div></div><p class="calibre8">There are various ways of constructing ensemble classifiers (or regressors) and boosting is one of them. Boosting <a id="id626" class="calibre1"/>came out as an answer by Robert Schapire in his <a id="id627" class="calibre1"/>pioneering paper in 1990 entitled <span class="strong"><em class="calibre10">The Strength of Weak Learnability</em></span>, where he elegantly describes the boosting ensemble while answering questions posed by Kearns and Valiant in their paper published in 1989, which talks about multiple weak learners that can create a single strong learner.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note39" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><em class="calibre10">The Strength of Weak Learnability</em></span>: <a class="calibre1" href="http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf">http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf</a>.</p><p class="calibre8"><span class="strong"><strong class="calibre9">Kearns and Valiant: </strong></span>Cryptographic limitations on learning Boolean Learning and finite automata: <a class="calibre1" href="http://dl.acm.org/citation.cfm?id=73049">http://dl.acm.org/citation.cfm?id=73049</a></p></div><p class="calibre8">The original algorithm for boosting was revised by Freund and Schapire, and termed as <span class="strong"><strong class="calibre9">AdaBoost</strong></span> or <span class="strong"><strong class="calibre9">Adaptive </strong></span><a id="id628" class="calibre1"/>
<span class="strong"><strong class="calibre9">Boosting</strong></span>. This algorithm was practically implementable and empirically improves generalization performance. The algorithm can be mathematically presented as follows:</p><div class="mediaobject"><img src="../images/00254.jpeg" alt="Boosting" class="calibre11"/><div class="caption"><p class="calibre18">Source: <a class="calibre1" href="https://www.cs.princeton.edu/picasso/mats/schapire02boosting_schapire.pdf">https://www.cs.princeton.edu/picasso/mats/schapire02boosting_schapire.pdf</a></p></div></div><p class="calibre12"> </p><p class="calibre8">Here:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">X</strong></span> is the training set</li><li class="listitem"><span class="strong"><strong class="calibre9">Y</strong></span> is the label set</li><li class="listitem"><span class="strong"><strong class="calibre9">D<sub class="calibre19">t</sub>(i)</strong></span> is the weight distribution on training example <span class="strong"><strong class="calibre9">i</strong></span> on iteration <span class="strong"><strong class="calibre9">t</strong></span></li><li class="listitem"><span class="strong"><strong class="calibre9">h<sub class="calibre19">t</sub></strong></span> is the classifier obtained in iteration <span class="strong"><strong class="calibre9">t</strong></span></li><li class="listitem"><span class="strong"><strong class="calibre9">α</strong></span> is the strength parameter or weight of <span class="strong"><strong class="calibre9">h<sub class="calibre19">t</sub></strong></span></li><li class="listitem"><span class="strong"><strong class="calibre9">H</strong></span> is the final or combined classifier.</li></ul></div><p class="calibre8">In simple words, boosting, in general, begins by initially assigning equal weights to all training examples. It <a id="id629" class="calibre1"/>then iterates over the hypothesis space to learn a hypothesis <span class="strong"><strong class="calibre9">h<sub class="calibre19">t</sub></strong></span> on the weighted examples. After each such hypothesis is learned, the <a id="id630" class="calibre1"/>weights are adjusted in such a manner that the weights of the examples that are correctly classified are reduced. This update to weights helps weak learners, in coming iterations, to concentrate more on wrongly classified data points. Finally, each of the learned hypotheses is then passed through a weighted voting to come up with a final model, <span class="strong"><strong class="calibre9">H</strong></span>.</p><p class="calibre8">Now that we have an overview of ensemble methods and boosting in general, let us use the boosting implementation available from the <code class="email">RTextTools</code> library in R to classify tweets as positive or negative.</p><p class="calibre8">We will reuse the training-testing document term matrices <code class="email">train.dtMatrix</code> and <code class="email">test.dtMatrix</code>, and container objects <code class="email">train.container</code> and <code class="email">test.container</code>, which we created for the SVM-based classification.</p><p class="calibre8">For building a classifier based on a boosting ensemble, <code class="email">RTextTools</code> provides an easy-to-use utility function called <code class="email">train_model</code>. It uses <span class="strong"><em class="calibre10">LogitBoosting</em></span> internally to build a classifier. We use <code class="email">500</code> iterations for building our boosting ensemble.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">boosting.model &lt;- train_model(train.container, "BOOSTING"</strong></span>
<span class="strong"><strong class="calibre9">            , maxitboost=500)</strong></span>
<span class="strong"><strong class="calibre9">boosting.classify &lt;- classify_model(test.container, boosting.model)</strong></span>
</pre></div><p class="calibre8">We then prepare a confusion matrix to see how our classifier performs on the test dataset.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">predicted.labels &lt;- boosting.classify[,1]</strong></span>
<span class="strong"><strong class="calibre9">true.labels &lt;- as.factor(test.data[,1])</strong></span>

<span class="strong"><strong class="calibre9">confusionMatrix(data = predicted.labels, </strong></span>
<span class="strong"><strong class="calibre9">                reference = true.labels, </strong></span>
<span class="strong"><strong class="calibre9">                positive = "positive")</strong></span>
</pre></div><p class="calibre8">The following confusion <a id="id631" class="calibre1"/>matrix shows that our boosting-based classifier <a id="id632" class="calibre1"/>works with an accuracy of 78.5%, which is fairly good given the fact that we did not perform any performance tuning. Compare this to the initial iteration of SVM where we got a dismal accuracy of just over 50%.</p><p class="calibre8"><span class="strong"><img src="../images/00255.jpeg" alt="Boosting" class="calibre16"/></span></p><p class="calibre8">As mentioned earlier, ensemble methods (specifically boosting) have improved generalized <a id="id633" class="calibre1"/>performance, that is, they help achieve close to 0 training errors without overfitting on the training data. To understand and evaluate our Boosting classifier on <a id="id634" class="calibre1"/>these parameters, we will use a model-evaluation technique called <span class="strong"><strong class="calibre9">Cross-validation</strong></span>.</p></div><div class="book" title="Cross-validation"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch08lvl4sec07" class="calibre1"/>Cross-validation</h4></div></div></div><p class="calibre8">Cross-validation is a <a id="id635" class="calibre1"/>model-evaluation technique which is used to evaluate <a id="id636" class="calibre1"/>the generalization performance of a model. It is <a id="id637" class="calibre1"/>also termed <span class="strong"><strong class="calibre9">rotational estimation</strong></span>. Cross-validation is a better measure to validate a model for generalization compared to residual methods because, for conventional validation techniques, the error (such as <span class="strong"><strong class="calibre9">Root </strong></span><a id="id638" class="calibre1"/>
<span class="strong"><strong class="calibre9">Mean Square Error</strong></span>/<span class="strong"><strong class="calibre9">RMSE</strong></span>) for the training set and testing set does not properly represent the model's performance. Cross-validation can be performed using:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Holdout method</strong></span>: The simplest cross-validation technique. Data is split into training and testing sets. The <a id="id639" class="calibre1"/>model is fitted on the training set, and then the testing set (which the model hasn't seen so far) is used to calculate the mean absolute test error. This accumulated error is used to evaluate the model. This technique suffers from high variance due to its dependency on how the training-testing division was done.</li><li class="listitem"><span class="strong"><strong class="calibre9">K-fold cross validation method</strong></span>: This is an improvement over the holdout method. The <a id="id640" class="calibre1"/>dataset is divided into <span class="strong"><em class="calibre10">k</em></span> subsets and then the holdout method is applied k times using <span class="strong"><em class="calibre10">1</em></span> of the <span class="strong"><em class="calibre10">k</em></span> subsets as test and the rest, <span class="strong"><em class="calibre10">k-1</em></span>, as training sets. This method has a lower variance due to the fact that each data point gets to be in the test set once and in the training set <span class="strong"><em class="calibre10">k-1</em></span> times. The disadvantage is that more computation time is required due to the number of iterations. An extreme form of K-fold cross validation is the Leave-Out One cross-validation method where all data points except one are used for training. The process is repeated <span class="strong"><em class="calibre10">N</em></span> (size of dataset) times.</li></ul></div><p class="calibre8">We can easily perform K-fold cross validation on our boosting classifier using the <code class="email">cross_validate</code> function. In general, 10-fold cross validation is used:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># Cross validation</strong></span>
<span class="strong"><strong class="calibre9">N=10</strong></span>
<span class="strong"><strong class="calibre9">set.seed(42)</strong></span>
<span class="strong"><strong class="calibre9">cross_validate(train.container,N,"BOOSTING"</strong></span>
<span class="strong"><strong class="calibre9">    , maxitboost=500)</strong></span>
</pre></div><p class="calibre8">The preceding code snippet produces the following cross validation summary:</p><p class="calibre8"><span class="strong"><img src="../images/00256.jpeg" alt="Cross-validation" class="calibre16"/></span></p><p class="calibre8">The results show that the classifier has generalized well enough, and has an overall mean accuracy of 97.8%.</p><p class="calibre8">Boosting is one of the <a id="id641" class="calibre1"/>methods to construct ensemble classifiers based on weak learners. Methods such as bagging, bayes optimal classifier, bucketing, and stacking are some of the variants with their own pros and cons.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note40" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Constructing ensembles</strong></span></p><p class="calibre8"><code class="email">RTextTools</code> is a <a id="id642" class="calibre1"/>robust library which provides functions such as <code class="email">train_models</code> and <code class="email">classify_models</code> to prepare ensembles by combining various base learners. It also provides tools for generating analysis for evaluating the performance of such ensembles in a very detailed manner. Check out the detailed explanation at <a class="calibre1" href="https://journal.r-project.org/archive/2013-1/collingwood-jurka-boydstun-etal.pdf">https://journal.r-project.org/archive/2013-1/collingwood-jurka-boydstun-etal.pdf</a>.</p></div></div></div></div></div>
<div class="book" title="Summary" id="20R681-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec62" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Twitter is a goldmine for data science, with interesting patterns and insights spread all across it. Its constant flow of user-generated content, coupled with unique, interest-based relationships, present opportunities to understand human dynamics up close. Sentiments Analysis is one such field where Twitter provides the right set of ingredients to understand what and how we present and share opinions about products, brands, people, and so on.</p><p class="calibre8">Throughout this chapter, we have looked at the basics of Sentiment Analysis, key terms, and areas of application. We have also looked into the various challenges posed while performing sentiment analysis. We have looked at various commonly-used feature extraction methods such as tf-idf, Ngrams, POS, negation, and so on for performing sentiment analysis (or textual analysis in general). We have built on our code base from the previous chapter to streamline and structure utility functions for reuse. We have performed polarity analysis using Twitter search terms and have seen how public opinion about certain campaigns can be easily tracked and analyzed. We then moved on to supervised learning algorithms for classification, where we used SVM and Boosting to build sentiment classifiers using libraries such as <code class="email">caret</code>, <code class="email">RTextTools</code>, <code class="email">ROCR</code>, <code class="email">e1071</code> and so on. Before closing the final chapter we also briefly touched upon the highly researched and widely used field of ensemble methods, and also learned about cross-validation-based model evaluation.</p><p class="calibre8">There are many other algorithms and analysis techniques which can be applied to extract even more interesting insights from Twitter and other sources on the Internet. Throughout this chapter (and this book), we have merely attempted to address the tip of a huge iceberg! Data science is not just about applying algorithms to solve a problem or derive insights. It requires creative thinking and a lot of due diligence apart from domain understanding, feature engineering, and collecting data to try and solve problems which are as yet unknown.</p><p class="calibre8">To sum up things, ponder upon this quote by Donald Rumsfeld:</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre17"><span class="strong"><em class="calibre10">"There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don't know. But there are also unknown unknowns. There are things we don't know we don't know."</em></span></p></blockquote></div><p class="calibre8">Data science is a journey of learning the knowns and exploring the unknown unknowns, and machine learning is a powerful tool to help accomplish it. <code class="email">#KeepMining</code>!</p></div></body></html>