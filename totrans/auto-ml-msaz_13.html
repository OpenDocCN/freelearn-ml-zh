<html><head></head><body>
		<div id="_idContainer119">
			<h1 id="_idParaDest-147"><em class="italic"><a id="_idTextAnchor151"/>Chapter 10</em>: Creating End-to-End AutoML Solutions</h1>
			<p>Now that you have created <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) pipelines, you can learn how to use them in other Azure products outside of the <strong class="bold">Azure Machine Learning</strong> <strong class="bold">Service</strong> (<strong class="bold">AMLS</strong>). Perhaps the most useful is Azure Data Factory. </p>
			<p><strong class="bold">Azure Data Factory</strong> (<strong class="bold">ADF</strong>) is Azure's premier code-free data orchestration tool. You can use ADF to pull data from on-premise sources into the Azure cloud, to run ML pipelines, and push data out of Azure by creating an <strong class="bold">Azure Data Factory pipeline </strong>(<strong class="bold">ADF pipeline</strong>). ADF pipelines are an integral part of creating end-to-end ML solutions and are the end goal of any non-real-time AutoML project.</p>
			<p>You will begin this chapter by learning how to connect AMLS to ADF. Once you have accomplished this task, you will learn how to schedule an ML pipeline using the parallel pipeline you created in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>. </p>
			<p>Next, you will learn how to pull data from your local machine and load it into the Azure cloud using ADF. Finally, you will pull everything you have together to create an end-to-end AutoML solution, creating an ADF pipeline for scoring incoming data, and another ADF pipeline for retraining AutoML models.</p>
			<p>By the end of this chapter, you will be able to integrate AMLS with ADF to create ADF pipelines and be able to fashion complete end-to-end AutoML solutions, from ingesting and scoring data, to retraining ML models. This is an invaluable, in-demand skillset that will set you apart from your peers. </p>
			<p>If you're already a trained data scientist, you will acquire software engineering skills that are rare in your field. If you're a trained engineer, you will learn how to incorporate ML into a familiar field you already understand.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Connecting AMLS to ADF</li>
				<li>Scheduling a machine learning pipeline in ADF</li>
				<li>Transferring data using ADF</li>
				<li>Automating an end-to-end scoring solution</li>
				<li>Automating an end-to-end training solution</li>
			</ul>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor152"/>Technical requirements</h1>
			<p>In this chapter, you will create an ADF resource and use the ML pipeline objects you created in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>. As such, you will need a working internet connection, an Azure account, and access to your AMLS workspace. </p>
			<p>With your Azure account, you will also need permissions to create a service principal in Azure Active Directory. If you're using a personal Azure account, you should have this access. If you're using a work account, speak with your Azure administrator for this level of permission. </p>
			<p>The following are the prerequisites for the chapter:</p>
			<ul>
				<li>Have access to the internet</li>
				<li>Have a web browser, preferably Google Chrome or Microsoft Edge Chromium</li>
				<li>Have a Microsoft Azure account</li>
				<li>Have created an AMLS workspace</li>
				<li>Have created the <strong class="source-inline">compute-cluster</strong> compute cluster in <a href="B16595_02_ePub.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with Azure Machine Learning Service</em></li>
				<li>Understand how to navigate to the Jupyter environment from an Azure compute instance as demonstrated in <a href="B16595_04_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 4</em></a>, <em class="italic">Building an AutoML Regression Solution</em></li>
				<li>Have trained and registered the <strong class="source-inline">Iris-Multi-Classification-AutoML</strong> ML model in <a href="B16595_05_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 5</em></a>, <em class="italic">Building an AutoML Classification Solution</em></li>
				<li>Have created all three of the ML pipelines in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>. The three ML pipelines are <em class="italic">Iris-Scoring-Pipeline</em>, <em class="italic">Iris-Parallel-Scoring-Pipeline</em>, and <em class="italic">Iris-AutoML-Training-Pipeline</em>. </li>
				<li>Have necessary permissions to create service principals in Azure Active Directory. If you're using a personal account, you will have these permissions.</li>
			</ul>
			<p>The code for this chapter is available here: <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter10">https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter10</a>.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor153"/>Connecting AMLS to ADF</h1>
			<p>ADF is a <a id="_idIndexMarker603"/>code-free <a id="_idIndexMarker604"/>data orchestration and transformation tool. With it, you can create ADF pipelines that can copy data into Azure, transform data, run ML pipelines, and push data back onto certain on-premise databases and file shares. It's incredibly easy to make and schedule ADF pipelines using ADF's code-free pipeline editing tool. As you create an ADF pipeline with the drag and drop interface, you're actually writing JSON code, which ADF uses to execute jobs.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout"><strong class="bold">Azure Synapse Analytics</strong>, Microsoft<a id="_idIndexMarker605"/> Azure's premier data warehousing and integrated analytics service, also has a feature nearly identical to ADF pipelines: <strong class="bold">Azure Synapse pipelines</strong>. Anything<a id="_idIndexMarker606"/> that you do in this chapter with ADF pipelines you can also achieve with Azure Synapse pipelines using a very similar interface.</p>
			<p>In this section, you will create an ADF resource and connect it to AMLS. You will do this using a <strong class="bold">linked service</strong>, an object similar to a connection string that ADF requires to connect to other Azure and non-Azure services and data stores. Linked services require authentication, and AMLS requires service principal authentication. </p>
			<p>A <strong class="bold">service principal</strong> is a security identity that Azure uses to grant permissions across Azure resources. Once you grant your service principal access to both ADF and AMLS, it's easy to <a id="_idIndexMarker607"/>connect<a id="_idIndexMarker608"/> them together and start running ML pipelines.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor154"/>Creating an ADF</h2>
			<p>An ADF can be created by using the GUI or using Azure <a id="_idIndexMarker609"/>PowerShell through the Azure<strong class="bold"> Command Line Interface</strong> (<strong class="bold">CLI</strong>) that you used in <a href="B16595_02_ePub.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with Azure Machine Learning Service</em>. <strong class="bold">PowerShell</strong> is a series of cmdlets for managing Azure <a id="_idIndexMarker610"/>resources through the CLI. Here, you will first learn how to create an ADF resource using the Azure portal GUI. Then, you will learn how to create an ADF resource through PowerShell. </p>
			<p>To create an ADF resource using the GUI, do the following: </p>
			<ol>
				<li>Navigate to the Azure portal at <a href="https://portal.azure.com">https://portal.azure.com</a>.</li>
				<li>Click on <strong class="bold">Create a resource</strong> in the top-left corner.</li>
				<li>In the search box, type in <strong class="source-inline">Data Factory</strong> and click <strong class="bold">Data Factory</strong> from the drop-down box.</li>
				<li>Click on <strong class="bold">Create</strong>, the blue box in the top-left corner under <strong class="bold">Data Factory</strong>. </li>
				<li>Now fill out the <strong class="bold">Data Factory creation</strong> form. Begin by selecting the same <strong class="bold">Resource group</strong> that holds your AMLS workspace. If you used the suggested <strong class="bold">Resource group</strong> in <a href="B16595_02_ePub.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with Azure Machine Learning Service</em>, this will be <strong class="bold">auto-ml-example-resource-group</strong>.</li>
				<li>Select the same Azure <strong class="bold">Region</strong> that holds your AMLS workspace. If you used the suggested Azure <strong class="bold">Region</strong> in <a href="B16595_02_ePub.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with Azure Machine Learning Service</em>, this will be <strong class="bold">North Central US</strong>.</li>
				<li>Give your ADF a name in the <strong class="bold">Name </strong>field. As it has to be globally unique across Microsoft Azure, try <strong class="source-inline">automl-adf</strong> followed by a number string. The following screenshot shows what your completed settings should look like. Leave <strong class="bold">Version</strong> as <strong class="bold">V2</strong>:<div id="_idContainer101" class="IMG---Figure"><img src="image/Figure_10.1_B16595.jpg" alt="Figure 10.1 – Data factory settings"/></div><p class="figure-caption">Figure 10.1 – Data factory settings</p></li>
				<li>Click the <strong class="bold">Git configuration</strong> tab <a id="_idIndexMarker611"/>and check the <strong class="bold">Configure Git later</strong> box.</li>
				<li>Click <strong class="bold">Review + create</strong> and hit <strong class="bold">Create</strong>. Your data factory is now created.</li>
			</ol>
			<p>Another way to create an ADF resource is through PowerShell. Follow these steps:</p>
			<ol>
				<li value="1">Navigate to the Azure portal at <a href="https://portal.azure.com">https://portal.azure.com</a>.</li>
				<li>Click the computer screen icon at the top right of your screen as shown in the following screenshot. When you hover over the icon, the words <strong class="bold">Cloud Shell</strong> will appear:<div id="_idContainer102" class="IMG---Figure"><img src="image/Figure_10.2_B16595.jpg" alt="Figure 10.2 – Navigating to PowerShell "/></div><p class="figure-caption">Figure 10.2 – Navigating to PowerShell</p></li>
				<li>Select <strong class="bold">PowerShell</strong> from <a id="_idIndexMarker612"/>the drop-down box.</li>
				<li>Type in the following code: <p class="source-code">$DataFactory =\</p><p class="source-code">Set-AzDataFactoryV2 -ResourceGroupName 'auto-ml-example-resource-group' -location 'northcentralus' -Name 'automl-adf713'</p><p><strong class="source-inline">ResourceGroupName</strong> sets your resource group. <strong class="source-inline">Set-AzDataFactoryV2</strong> sets your version, while <strong class="source-inline">location</strong> sets your Azure region. <strong class="source-inline">Name</strong> gives your ADF a name. Your data factory has now been created.</p></li>
			</ol>
			<p>Now that you have created an ADF, the next step is to create a service principal and give it access to both your ADF and AMLS workspace. This will grant ADF access to use ML pipelines.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor155"/>Creating a service principal and granting access</h2>
			<p>Service principals <a id="_idIndexMarker613"/>are security identities that<a id="_idIndexMarker614"/> Azure uses to grant Azure resources access to other Azure resources. You can use service principal authentication across many areas of Azure, including AMLS. In order to connect AMLS to ADF, however, a service principal is required. To create one using the Azure portal, you must first navigate to <strong class="bold">Azure Active Directory</strong>, Azure's premier identity and authentication service, by following these steps:</p>
			<ol>
				<li value="1">Navigate to the Azure portal at <a href="https://portal.azure.com">https://portal.azure.com</a>.</li>
				<li>Search for <strong class="source-inline">Azure Active Directory</strong> in the top search bar and click <strong class="bold">Azure Active Directory</strong> under the <strong class="bold">Services</strong> heading.</li>
				<li>Click <strong class="bold">App registrations</strong> on the left-hand side of the screen.</li>
				<li>Click <strong class="bold">New registration</strong> at the top left of the screen.</li>
				<li>Give the service principal a name, <strong class="source-inline">adf-service-principal</strong>, and click <strong class="bold">Register</strong>. Leave all other settings as is.</li>
				<li>You will now be taken to a page with all of the information about your service principal. Copy <strong class="bold">Application (client) ID</strong> and paste it in Notepad or a similar text editor. You will need this ID later.</li>
				<li>Click <strong class="bold">Certificates &amp; secrets</strong> on the left-hand side of the screen. This will let you create a password for your service principal.</li>
				<li>Click <strong class="bold">New client secret</strong>. <em class="italic">Secret</em> is another word for a password in Azure.</li>
				<li>Give the secret a name, <strong class="source-inline">ADF-Secret</strong>, and set it so that it never expires as shown in <em class="italic">Figure 10.3</em>. Click <strong class="bold">Add</strong>:<div id="_idContainer103" class="IMG---Figure"><img src="image/Figure_10.3_B16595.jpg" alt="Figure 10.3 – Naming your service principal secret "/></div><p class="figure-caption">Figure 10.3 – Naming your service principal secret</p></li>
				<li>Copy the <strong class="bold">Value</strong> field <a id="_idIndexMarker615"/>of your<a id="_idIndexMarker616"/> secret. This is your password and you will only be able to see it for a short time before it disappears. Once it vanishes, you will never be able to see it again. Paste it into Notepad or a similar text editor, as you will need it later when creating your ADF linked service. </li>
				<li>With your service principal created, you must now grant it access to both AMLS and ADF. Navigate to the front page of the Azure portal at <a href="https://portal.azure.com">https://portal.azure.com</a>.</li>
				<li>Open your AMLS resource by clicking the <strong class="bold">Machine Learning</strong> icon near the top of your screen under <strong class="bold">Azure Services</strong>. You should see it if you have recently used AMLS, as shown in <em class="italic">Figure 10.4</em>:<div id="_idContainer104" class="IMG---Figure"><img src="image/Figure_10.4_B16595.jpg" alt="Figure 10.4 – Azure services panel "/></div><p class="figure-caption">Figure 10.4 – Azure services panel</p></li>
				<li>Click the <a id="_idIndexMarker617"/>name of your AMLS workspace to access the resource. </li>
				<li>Click <strong class="bold">Access Control (IAM)</strong> on the left-hand panel.</li>
				<li>Click <strong class="bold">Add role assignments</strong>.</li>
				<li>Select <strong class="bold">Contributor</strong> for <strong class="bold">Role</strong>, assign access to <strong class="bold">User, group, or service principal</strong>, search for <strong class="source-inline">adf-service-principal</strong> and click it as shown in <em class="italic">Figure 10.5</em>. Then, click <strong class="bold">Save</strong>:<div id="_idContainer105" class="IMG---Figure"><img src="image/Figure_10.5_B16595.jpg" alt="Figure 10.5 – Granting permission to your service principal "/></div><p class="figure-caption">Figure 10.5 – Granting permission to your service principal</p></li>
				<li>You have<a id="_idIndexMarker618"/> now granted your service <a id="_idIndexMarker619"/>principal access to AMLS. Now, you must do the same thing for ADF. Begin by navigating to the front page of the Azure portal by clicking <strong class="bold">Home</strong> in the top-left corner of your screen.</li>
				<li>Click the <strong class="bold">Data factories</strong> icon at the top of your screen under <strong class="bold">Azure Services</strong>. If you have used ADF lately, you will see this icon.</li>
				<li>Click the name of the data factory you made to open up the resource.</li>
				<li>Repeat <em class="italic">steps 14-16</em> for ADF. These<a id="_idIndexMarker620"/> steps <a id="_idIndexMarker621"/>are identical to AMLS. You have now granted your service principal access to both ADF and AMLS.</li>
			</ol>
			<h3>Creating an ADF resource through the Azure CLI</h3>
			<p>You can also<a id="_idIndexMarker622"/> create a<a id="_idIndexMarker623"/> service principal and grant it access to ADF and AMLS through the Azure CLI with the following steps:</p>
			<ol>
				<li value="1">Open up the Azure CLI by clicking the computer screen icon at the top right of your screen and select <strong class="bold">Bash</strong> from the drop-down menu.</li>
				<li>Type in the following code to create your service principal, assigning it a name:<p class="source-code">az ad sp create-for-rbac --name adf-service-principal</p></li>
				<li>Copy both the <strong class="source-inline">appid</strong> and <strong class="source-inline">password</strong> fields. You will never see the value for your password again, so make sure to copy it. This maps to <strong class="bold">Application (client) ID</strong> and the secret.</li>
				<li>Use the following code to grant your service principal access to all resources in the resource group containing both AMLS and ADF. Change to match your resource group if necessary. Use <strong class="source-inline">assignee</strong> to pass in your service principal application ID. Use <strong class="source-inline">role</strong> to pass in the correct level of access, in this case, <strong class="bold">Contributor</strong>. Use <strong class="source-inline">resource-group</strong> to assign access to the correct <strong class="bold">Resource group</strong>:<p class="source-code">az role assignment create --assignee "your-service-principal-id" --role "Contributor" --resource-group "auto-ml-example-resource-group"</p></li>
			</ol>
			<p>With your service principal created and granted access to the appropriate resources, your next step is<a id="_idIndexMarker624"/> to open ADF<a id="_idIndexMarker625"/> and create the linked service. Make sure you have Notepad open as you will need both the service principal ID as well as the secret you created. Having these readily available will make the next set of steps easy.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor156"/>Creating a linked service to connect ADF with AMLS</h2>
			<p>Moving forward, now <a id="_idIndexMarker626"/>you'll get a chance to<a id="_idIndexMarker627"/> open up ADF and get used to its interface. ADF is designed largely as a code-free platform, but everything you create is also written as JSON files under the hood. To link ADF with AMLS, follow these steps:</p>
			<ol>
				<li value="1">Navigate to the Azure portal at <a href="https://portal.azure.com">https://portal.azure.com</a>.</li>
				<li>Click the <strong class="bold">Data factories</strong> icon at the top of your screen under <strong class="bold">Azure Services</strong>. If you have used ADF lately, you will see this icon. If not, use the search bar at the top of your screen.</li>
				<li>Click the name of the data factory you made that starts with the name <strong class="source-inline">autol-adf</strong>.</li>
				<li>Click <strong class="bold">Author &amp; Monitor</strong> in the middle of the screen. You should now see the ADF user interface, shown in <em class="italic">Figure 10.6</em>:<div id="_idContainer106" class="IMG---Figure"><img src="image/Figure_10.6_B16595.jpg" alt="Figure 10.6 – ADF UI "/></div><p class="figure-caption">Figure 10.6 – ADF UI</p></li>
				<li>Click the<a id="_idIndexMarker628"/> toolbox icon on the left-hand<a id="_idIndexMarker629"/> side. When you hover over the icon for a few seconds, the word <strong class="bold">Manage</strong> will appear to indicate the section you're navigating to.</li>
				<li>Click <strong class="bold">Linked Services</strong> under <strong class="bold">Connections</strong> in the top-left corner.</li>
				<li>Click <strong class="bold">Create linked service</strong>.</li>
				<li>Click <strong class="bold">Compute</strong> and select <strong class="bold">Azure Machine Learning</strong> as shown in <em class="italic">Figure 10.7</em>:<div id="_idContainer107" class="IMG---Figure"><img src="image/Figure_10.7_B16595.jpg" alt="Figure 10.7 – Create an Azure ML linked service "/></div><p class="figure-caption">Figure 10.7 – Create an Azure ML linked service</p></li>
				<li>Click <strong class="bold">Continue</strong>.</li>
				<li>Now it's <a id="_idIndexMarker630"/>time to fill out the linked <a id="_idIndexMarker631"/>service creation form. Begin by giving your linked service a name such as <strong class="source-inline">AMLS Linked Service</strong>.</li>
				<li>Select <strong class="bold">AutoResolveIntegrationRuntime</strong> from the dropdown under <strong class="bold">Connect via integration runtime</strong>. An <strong class="bold">integration runtime</strong> is simply the compute that<a id="_idIndexMarker632"/> ADF uses under the hood to move data and run<a id="_idIndexMarker633"/> your jobs. The <strong class="bold">Azure integration runtime</strong> (<strong class="bold">Azure IR</strong>) is a serverless, elastic compute fully managed by ADF.</li>
				<li>Select your Azure subscription from the drop-down box under <strong class="bold">Azure subscription</strong>.</li>
				<li>Select your AMLS workspace from the drop-down box under <strong class="bold">Azure Machine Learning workspace name</strong>.</li>
				<li>Paste your <a id="_idIndexMarker634"/>service principal <a id="_idIndexMarker635"/>application (client) ID into the <strong class="bold">Service principal ID</strong> textbox. You copied this ID into a text editor earlier.</li>
				<li>Paste your service principal secret into the <strong class="bold">Service principal key</strong> textbox.</li>
				<li>Click <strong class="bold">Test connection</strong>.</li>
				<li>If the result of the test is <strong class="bold">Connection successful</strong>, click <strong class="bold">Create</strong>.</li>
				<li>Your linked service has been created. Hover over your new linked service and click the <strong class="bold">{ }</strong> icon to view the underlying JSON code.</li>
			</ol>
			<p>You have successfully connected your ADF to AMLS. In this short section, you have learned a lot. Not only have you learned how to create an ADF, but you have also learned how to create service principals, grant access, and create linked services. With this infrastructure created, you can now learn how to easily run and schedule ML pipelines inside of an ADF pipeline.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor157"/>Scheduling a machine learning pipeline in ADF</h1>
			<p>Perhaps ADF's<a id="_idIndexMarker636"/> best feature is its ease of use. By <a id="_idIndexMarker637"/>clicking and dragging objects across a screen, you can easily orchestrate a flow of seamless data ingestion, transformation, and ML through an ADF pipeline. Moreover, with a few more clicks, you can schedule that ADF pipeline to run whenever you want. Gaining this skill will enable you to create code-free data orchestration runs quickly and easily. </p>
			<p>First, you will schedule and run the simplest ML pipeline you created in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>, the <strong class="source-inline">Iris-Scoring-Pipeline</strong>. To do so, follow these steps:</p>
			<ol>
				<li value="1">Navigate to your ADF resource and click <strong class="bold">Author &amp; Monitor</strong>.</li>
				<li>Click the pen icon on the left-hand side. When you hover over this icon, the word <strong class="bold">Author</strong> will appear to indicate which section you're navigating to.</li>
				<li>Click the blue cross icon next to the search box under <strong class="bold">Factory Resources</strong> in the top-left corner. When you hover over this icon, the words <strong class="bold">Add new resource</strong> will appear.</li>
				<li>Click <strong class="bold">Pipeline</strong> from the resulting dropdown as shown in <em class="italic">Figure 10.8</em>:<div id="_idContainer108" class="IMG---Figure"><img src="image/Figure_10.8_B16595.jpg" alt="Figure 10.8 – Creating your first ADF pipeline "/></div><p class="figure-caption">Figure 10.8 – Creating your first ADF pipeline</p></li>
				<li>Click <strong class="bold">Machine Learning</strong> under <strong class="bold">Activities</strong> in <a id="_idIndexMarker638"/>the <a id="_idIndexMarker639"/>center-left of the screen.</li>
				<li>Click and drag the blue flask icon onto the canvas as shown in <em class="italic">Figure 10.9</em>. This is the <strong class="bold">Machine Learning Executive Pipeline</strong> activity:<div id="_idContainer109" class="IMG---Figure"><img src="image/Figure_10.9_B16595.jpg" alt="Figure 10.9 – Machine Learning Execute Pipeline activity "/></div><p class="figure-caption">Figure 10.9 – Machine Learning Execute Pipeline activity</p></li>
				<li>Click <strong class="bold">General</strong> underneath<a id="_idIndexMarker640"/> the canvas <a id="_idIndexMarker641"/>and give your activity a new name, <strong class="source-inline">Execute Iris Scoring Pipeline</strong>.</li>
				<li>Click <strong class="bold">Settings</strong> underneath the canvas.</li>
				<li>Select <strong class="bold">AMLS Linked Service</strong> from the first drop-down box to connect your linked service.</li>
				<li>Select <strong class="bold">Iris-Scoring-Pipeline</strong> from the <strong class="bold">Machine Learning pipeline name</strong> drop-down box.</li>
				<li>Select a pipeline ID from the <strong class="bold">Machine Learning pipeline ID</strong> drop-down box. There should only be one pipeline ID unless you published multiple ML pipelines with the same name.</li>
				<li>Click the<a id="_idIndexMarker642"/> abacus icon in the top right-hand <a id="_idIndexMarker643"/>corner to open the <strong class="bold">Properties</strong> box. Rename your ADF pipeline from <strong class="source-inline">pipeline1</strong> to <strong class="source-inline">Iris Scoring Pipeline</strong>. Spaces are allowed.</li>
				<li>Click <strong class="bold">Publish All</strong> near the top left of your screen. Then, click <strong class="bold">Publish</strong> in the bottom right-hand corner to create your ADF pipeline.<p class="callout-heading">Important tip</p><p class="callout">In order to save your work in ADF, you need to publish your changes. Make sure you publish multiple times as you're developing new ADF pipelines.</p></li>
				<li>To schedule your newly created ADF pipeline, click <strong class="bold">Add trigger</strong> near the top of your screen. </li>
				<li>Click <strong class="bold">New/Edit</strong> from the resulting dropdown.</li>
				<li>Click the <strong class="bold">Choose Trigger</strong> dropdown and select <strong class="bold">+ New</strong>.<p>Like the AML <a id="_idIndexMarker644"/>Python SDK, there are multiple types of <a id="_idIndexMarker645"/>triggers within ADF. Execute <strong class="bold">schedule-based triggers</strong> on a timetable. Execute <strong class="bold">event-based triggers</strong> when a file is created or deleted in an Azure blob container. </p><p><strong class="bold">Tumbling window triggers</strong> are <a id="_idIndexMarker646"/>similar to schedule-based triggers, but they are more advanced and have options such as retrying failed runs and backfilling past time periods. For this exercise, create a simple schedule-based trigger. </p></li>
				<li>Give your trigger a name such as <strong class="source-inline">Once Monthly</strong>.</li>
				<li>Select <strong class="bold">Schedule</strong> under <strong class="bold">Type</strong>. </li>
				<li>Pick a start date and the appropriate time zone from the respective drop-down boxes.</li>
				<li>Under <strong class="bold">Recurrence</strong>, type in <strong class="source-inline">1</strong> and select <strong class="bold">Month</strong> from the drop-down box.</li>
				<li>Click <strong class="bold">OK</strong> twice.</li>
				<li>Click <strong class="bold">Publish All</strong> near<a id="_idIndexMarker647"/> the top left of your<a id="_idIndexMarker648"/> screen, followed by <strong class="bold">Publish</strong> in the bottom right-hand corner. You are now finished. Your completed ADF pipeline should look something like <em class="italic">Figure 10.10</em>. To see the JSON code, click the <strong class="bold">{ }</strong> icon at the top right-hand side of your screen:</li>
			</ol>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Figure_10.10_B16595.jpg" alt="Figure 10.10 – Your first completed ADF pipeline "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Your first completed ADF pipeline</p>
			<p>While you have created and scheduled your first ADF pipeline, it does not mean much. After all, while <a id="_idIndexMarker649"/>this ADF pipeline will trigger your ML<a id="_idIndexMarker650"/> pipeline on a monthly basis, you still need to automate the ingestion of new data. Thankfully, data ingestion is where ADF excels. You will see just how easy transferring data is using ADF in the next section. No matter where the data lies, ADF can pull it in. </p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor158"/>Transferring data using ADF</h1>
			<p>Moving data <a id="_idIndexMarker651"/>from on-premise to the cloud and from the cloud to on-premise is a key skill for any data engineer or data scientist. ADF <a id="_idIndexMarker652"/>accomplishes this task with the <strong class="bold">Copy data</strong> activity. This is ADF's most basic and most powerful function. </p>
			<p>In this section, first, you will <a id="_idIndexMarker653"/>download a <strong class="bold">self-hosted integration runtime</strong> (<strong class="bold">SHIR</strong>) to your local machine, allowing your computer to serve as a compute resource to load data into Azure. Then, you will create a linked service for your <strong class="bold">Azure storage account</strong> and your local PC. </p>
			<p>Next, you will download a file from the GitHub repository and save it to your PC. Finally, you will create a <strong class="bold">Copy data</strong> activity in ADF that will take data from your PC and put it into the same Azure blob container that's connected to your AML datastore. </p>
			<p>Going through<a id="_idIndexMarker654"/> these exercises will give you the data engineering skills that will allow you to create an end-to-end solution in the next section. </p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor159"/>Installing a self-hosted integration runtime</h2>
			<p>Before you<a id="_idIndexMarker655"/> can copy data into Azure, you first need to install a SHIR on your local machine. Begin with the following steps: </p>
			<ol>
				<li value="1">Navigate to your ADF resource and click <strong class="bold">Author &amp; Monitor</strong>.</li>
				<li>Click the toolbox icon on the left-hand side. When you hover over this icon, the word <strong class="bold">Manage</strong> will appear to indicate which section you're navigating to.</li>
				<li>Click <strong class="bold">Integration runtimes</strong> under <strong class="bold">Connections</strong>.</li>
				<li>Click <strong class="bold">+New</strong> at the top center of your screen.</li>
				<li>Select <strong class="bold">Azure, Self-Hosted</strong> and click <strong class="bold">Continue</strong> as shown in <em class="italic">Figure 10.11</em>:<div id="_idContainer111" class="IMG---Figure"><img src="image/Figure_10.11_B16595.jpg" alt="Figure 10.11 – Selecting the right integration runtime to install a SHIR "/></div><p class="figure-caption">Figure 10.11 – Selecting the right integration runtime to install a SHIR</p></li>
				<li>Select <strong class="bold">Self-Hosted</strong> and click <strong class="bold">Continue</strong>.</li>
				<li>Give your <a id="_idIndexMarker656"/>new SHIR a name, <strong class="source-inline">IntegrationRuntime</strong>, and click <strong class="bold">Create</strong>.</li>
				<li>On the next screen, you will see two options to install the SHIR. Choose <strong class="bold">Option 1: Express setup</strong> by clicking <strong class="bold">Click here to launch the express setup for this computer</strong>.<p class="callout-heading">Important note</p><p class="callout">If you are using your work machine, ask your IT security organization for permission before installing the SHIR. It does open up a connection between your machine and the public-facing Azure cloud.</p></li>
				<li>This will download the SHIR installation file to your compute. Open the file and click <strong class="bold">Yes</strong>. Installation should take between 5 and 10 minutes.</li>
				<li>Once the <a id="_idIndexMarker657"/>installation is finished, click <strong class="bold">Close</strong>. Your SHIR should now appear as shown in <em class="italic">Figure 10.12</em>:</li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_10.12_B16595.jpg" alt="Figure 10.12 – Self-hosted integration runtime "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Self-hosted integration runtime</p>
			<p>With an SHIR installed on your local machine, you're now able to move data from your PC directly into Azure using ADF. Just as you created a linked service for AMLS, next you will create a linked service for Azure Blob storage.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor160"/>Creating an Azure Blob storage linked service</h2>
			<p>To create a<a id="_idIndexMarker658"/> linked service connecting ADF to <strong class="bold">Azure Blob storage</strong>, follow these steps:</p>
			<ol>
				<li value="1">Click <strong class="bold">Linked services</strong> under <strong class="bold">Connections</strong> in the top-left corner of your screen.</li>
				<li>Click <strong class="bold">+New</strong> at the top center of your screen.</li>
				<li>Select <strong class="bold">Azure Blob Storage</strong> and click <strong class="bold">Continue</strong> to see the linked service creation form.</li>
				<li>Give your linked service a name such as <strong class="source-inline">AMLSDatastoreLink</strong>.</li>
				<li>Select <strong class="bold">AutoResolveIntegrationRuntime</strong>.</li>
				<li>Select your <a id="_idIndexMarker659"/>Azure subscription from the drop-down box under <strong class="bold">Azure subscription</strong>.</li>
				<li>Select your storage account from the drop-down box under <strong class="bold">Storage account name</strong>. This should be the storage account linked to your AMLS workspace; if you followed the naming convention, it should begin with <strong class="source-inline">automlexamplew</strong> followed by a string of numbers.</li>
				<li>Click <strong class="bold">Test connection</strong>.</li>
				<li>If your test was successful, click <strong class="bold">Create</strong>.</li>
				<li>Download <strong class="source-inline">Iris_Scoring_Data_for_ADF.csv</strong> from the GitHub repository:<p><a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Chapter10/Iris_Scoring_Data_for_ADF.csv">https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Chapter10/Iris_Scoring_Data_for_ADF.csv</a></p></li>
				<li>Create a folder on your PC called <strong class="source-inline">Iris</strong>. Move <strong class="source-inline">Iris_Scoring_Data_for_ADF.csv</strong> there.</li>
			</ol>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor161"/>Creating a linked service to your PC</h2>
			<p>Next, it's time<a id="_idIndexMarker660"/> to create a linked service to your PC. This linked <a id="_idIndexMarker661"/>service will make use of your SHIR. Use the following steps:</p>
			<ol>
				<li value="1">Click <strong class="bold">Linked Services</strong> and <strong class="bold">+New</strong> as you did to create the other linked services. </li>
				<li>Click <strong class="bold">File</strong>, select <strong class="bold">File System</strong>, and click <strong class="bold">Continue</strong> to see the linked service creation form.</li>
				<li>Give your linked service a name such as <strong class="source-inline">LocalPCLink</strong>. </li>
				<li>Select your SHIR from the drop-down box under <strong class="bold">Connect via integration runtime</strong>.</li>
				<li>Under <strong class="bold">Host</strong>, type in the full file path, starting with your disk drive, to the <strong class="source-inline">Iris</strong> folder.</li>
				<li>Fill in the<a id="_idIndexMarker662"/> username and password that you use to log <a id="_idIndexMarker663"/>into your PC. To find your username, search for <strong class="bold">System Information</strong> in your PC's search bar and click it; your user name can be found under <strong class="bold">System Summary</strong>.</li>
				<li>Click <strong class="bold">Test connection</strong>.</li>
				<li>If your test was successful, click <strong class="bold">Create</strong>.</li>
			</ol>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor162"/>Creating an ADF pipeline to copy data</h2>
			<p>With an SHIR, Azure<a id="_idIndexMarker664"/> Blob storage linked service, and a<a id="_idIndexMarker665"/> linked service connecting to your local PC, you are now ready to build an ADF pipeline using the <strong class="bold">Copy data</strong> activity with the following steps:</p>
			<ol>
				<li value="1">Click the pen icon on the left-hand side of your ADF.</li>
				<li>Click the blue cross icon next to the search box under <strong class="bold">Factory Resources</strong> in the top-left corner. When you hover over this icon, the words <strong class="bold">Add new resource</strong> will appear.</li>
				<li>Click <strong class="bold">Copy Data tool</strong> from the resulting dropdown as shown in <em class="italic">Figure 10.13</em>:<div id="_idContainer113" class="IMG---Figure"><img src="image/Figure_10.13_B16595.jpg" alt="Figure 10.13 – Copy Data tool "/></div><p class="figure-caption">Figure 10.13 – Copy Data tool</p></li>
				<li>Under <strong class="bold">Task name</strong>, enter <strong class="source-inline">Copy Iris Data to Azure</strong> and click <strong class="bold">Next</strong>.</li>
				<li>You now have <a id="_idIndexMarker666"/>to <a id="_idIndexMarker667"/>select your source datastore. Select the linked service to your PC, <strong class="bold">LocalPCLink</strong>, and click <strong class="bold">Next</strong>.</li>
				<li>Choose the data you wish to transfer into Azure by clicking <strong class="bold">Browse</strong>, selecting <strong class="source-inline">Iris_Scoring_Data_for_ADF.csv</strong>, and clicking <strong class="bold">Choose</strong>.</li>
				<li>Click <strong class="bold">Next</strong>.</li>
				<li>Under <strong class="bold">File format settings</strong>, check the box for <strong class="bold">First row as header</strong> and click <strong class="bold">Next</strong>.</li>
				<li>You now have to select your destination datastore. Select the linked service to your Azure storage account, <strong class="bold">AMLSDatastoreLink</strong>.</li>
				<li>Under <strong class="bold">File name</strong>, type <strong class="source-inline">Input_Folder/Iris_Scoring_Data.csv</strong> and click <strong class="bold">Next</strong>. This will write your data to a file called <strong class="source-inline">Iris_Scoring_Data.csv</strong> in a folder called <strong class="source-inline">Input_Folder</strong>. The folder will be created if it does not exist.</li>
				<li>Under <strong class="bold">File format settings</strong>, check the box for <strong class="bold">Add header to file</strong> and click <strong class="bold">Next</strong>.</li>
				<li>Click <strong class="bold">Next</strong> under <strong class="bold">Settings</strong> without<a id="_idIndexMarker668"/> changing any <a id="_idIndexMarker669"/>of the defaults.</li>
				<li>Click <strong class="bold">Next</strong> under <strong class="bold">Summary</strong>. Your ADF pipeline will now be created and run.</li>
				<li>Click <strong class="bold">Finish</strong>. You will now be transported to the main ADF pipeline authoring tool.</li>
				<li>Click <strong class="source-inline">Copy Iris Data to Azure</strong> under <strong class="bold">Pipelines</strong>. You will notice that your <strong class="bold">Copy data</strong> activity will be poorly named.</li>
				<li>Click your activity and rename it <strong class="source-inline">Copy Iris Data from PC</strong> as shown in <em class="italic">Figure 10.14</em>:<div id="_idContainer114" class="IMG---Figure"><img src="image/Figure_10.14_B16595.jpg" alt="Figure 10.14 – Finished ADF pipeline with the Copy data activity "/></div><p class="figure-caption">Figure 10.14 – Finished ADF pipeline with the Copy data activity</p></li>
				<li>Save <a id="_idIndexMarker670"/>your<a id="_idIndexMarker671"/> change to the pipeline by clicking <strong class="bold">Publish All</strong> and then <strong class="bold">Publish</strong>.</li>
			</ol>
			<p>You have now successfully transferred data from your PC into Azure using ADF. This is a foundational skill for data engineering and allows you to transfer all sorts of data into the cloud. Like the ADF pipeline you created in the previous section for executing ML pipelines, you can schedule this to run on any time schedule you wish.</p>
			<p>We are now going to bring together all of the skills you have learned in this chapter to write a truly productionalizable ADF pipeline. This pipeline will ingest data from your computer, score it, and then write results back to your local machine. Even though it's not a trivial task by any means, you will be able to churn out pipeline after pipeline by the end of this chapter.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor163"/>Automating an end-to-end scoring solution</h1>
			<p>Ultimately, the end<a id="_idIndexMarker672"/> goal of any AutoML project is to create an automated scoring solution. Data gets pulled in from a source, scored automatically using the model you trained, and the results get stored in a location of your choice. By combining everything you've learned in the previous three sections, you can accomplish this task easily. </p>
			<p>You will begin this section by opening up AMLS, creating a new dataset, and slightly altering your existing <strong class="source-inline">Iris-Scoring-Pipeline</strong>. Then, after republishing your pipeline with a new name, you will combine it with the <strong class="bold">Copy data</strong> activity you created to load data into Azure. </p>
			<p>Next, you will create another Copy Data activity to transfer your results from Azure to your PC and schedule the job to run once a week on Mondays. This is a very common pattern in ML, and it's one you can accomplish without any code at all using ADF. </p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor164"/>Editing an ML pipeline to score new data</h2>
			<p>First, you need <a id="_idIndexMarker673"/>to create a new ML <a id="_idIndexMarker674"/>pipeline by editing <strong class="source-inline">Iris-Scoring-Pipeline</strong> you created in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>, with the following steps:</p>
			<ol>
				<li value="1">Access your AML studio at <a href="https://ml.azure.com">https://ml.azure.com</a>.</li>
				<li>Use the GUI to create a new dataset as you did in <a href="B16595_03_ePub.xhtml#_idTextAnchor044"><em class="italic">Chapter 3</em></a>, <em class="italic">Training Your First AutoML Model</em>. Begin by clicking <strong class="bold">Datasets</strong> under <strong class="bold">Assets</strong> on the left-hand panel.</li>
				<li>After clicking <strong class="bold">Create dataset</strong>, select <strong class="bold">From datastore</strong> and name your new dataset <strong class="source-inline">Iris Local Scoring Data</strong>.</li>
				<li>Select <strong class="bold">workplaceblobstore</strong> as your datastore and navigate to <strong class="source-inline">Iris_Data.csv</strong> in <strong class="source-inline">Input_Folder</strong>. This is the data you copied over from your PC.</li>
				<li>Finish creating the dataset, making sure to set the option under the <strong class="bold">Column headers</strong> dropdown to <strong class="bold">Use headers from the first file</strong> in order to pull in column names.</li>
				<li>With your dataset created, navigate to your <strong class="bold">Compute instance</strong> and open up your <strong class="source-inline">machine-learning-pipeline</strong> Python notebook in Jupyter.</li>
				<li>Click <strong class="bold">File</strong> at the top left and select <strong class="bold">Make a Copy</strong> from the drop-down box.</li>
				<li>Rename your copied notebook to <strong class="source-inline">machine-learning-pipeline-local-scoring</strong>.</li>
				<li>Delete the <a id="_idIndexMarker675"/>cells that <a id="_idIndexMarker676"/>make <strong class="source-inline">Iris</strong> data and register it as a dataset.</li>
				<li>Rename your Python script from <strong class="source-inline">Iris_Scoring.py</strong> to <strong class="source-inline">Iris_Scoring_Local.py</strong> in the first line of your script that writes the file as follows:<p class="source-code">%%writefile Scoring_Scripts/Iris_Scoring_Local.py </p></li>
				<li>In <strong class="source-inline">Iris_Scoring_Local.py</strong>, retrieve your <strong class="source-inline">Iris Local Scoring Data</strong> dataset instead of the <strong class="source-inline">Iris Scoring</strong> dataset as shown in the following line of code:<p class="source-code">dataset =\</p><p class="source-code">Dataset.get_by_name(ws,'Iris Local Scoring Data')</p></li>
				<li>When configuring your ML pipeline step, replace <strong class="source-inline">Iris_Scoring.py</strong> with <strong class="source-inline">Iris_Scoring_Local.py</strong> as shown in the following code:<p class="source-code">scoring_step =\</p><p class="source-code">PythonScriptStep(name='iris-scoring-step',\</p><p class="source-code">                script_name= 'Iris_Scoring_Local.py',\</p><p class="source-code">                source_directory='Scoring_Scripts',\</p><p class="source-code">                arguments=[],\</p><p class="source-code">                inputs=[],\</p><p class="source-code">               compute_target=compute_target,\</p><p class="source-code">                runconfig=run_config,\</p><p class="source-code">                allow_reuse=False)</p></li>
				<li>In <strong class="source-inline">Iris_Scoring_Local.py</strong>, retrieve your <strong class="source-inline">Iris Local Scoring Data</strong> dataset instead of the <strong class="source-inline">Iris Scoring</strong> dataset in the following line of code:<p class="source-code">dataset =\</p><p class="source-code">Dataset.get_by_name(ws,'Iris Local Scoring Data')</p></li>
				<li>Rename your<a id="_idIndexMarker677"/> published<a id="_idIndexMarker678"/> pipeline from <strong class="source-inline">Iris-Scoring-Pipeline</strong> to <strong class="source-inline">Iris-Local-Scoring-Pipeline</strong> in the following lines of code:<p class="source-code">published_pipeline =\</p><p class="source-code">pipeline_run.publish_pipeline( name='Iris-Local-Scoring-Pipeline',\</p><p class="source-code">       description='Pipeline that Scores Iris Data', version= '1.0')</p></li>
				<li>Run all cells in your notebook to create your new ML pipeline, <strong class="source-inline">Iris-Local-Scoring-Pipeline</strong>. This will take a few minutes. You have now created an ML pipeline that scores data loaded into Azure from your PC.</li>
			</ol>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor165"/>Creating an ADF pipeline to run your ML pipeline</h2>
			<p>With a <a id="_idIndexMarker679"/>new<a id="_idIndexMarker680"/> ML <a id="_idIndexMarker681"/>pipeline created, you are now ready to create a new ADF pipeline that automates the end-to-end scoring process using these steps:</p>
			<ol>
				<li value="1">Open up ADF and click the pen icon on the left-hand side to open the ADF pipeline authoring tool.</li>
				<li>Click <strong class="bold">Pipelines</strong> under <strong class="bold">Factory Resources</strong> to see your existing ADF pipelines. Click the three dots next to <strong class="source-inline">Copy Iris Data to Azure</strong> and select <strong class="bold">Clone</strong> from the drop-down box.</li>
				<li>Rename your new pipeline to <strong class="source-inline">End-to-End Iris Scoring</strong>.</li>
				<li>Click <strong class="bold">Machine Learning</strong> under <strong class="bold">Activities</strong> and drag a <strong class="bold">Machine Learning Execute Pipeline</strong> activity onto the canvas. Connect it to your <strong class="source-inline">Copy Iris Data from PC</strong> activity by clicking the green square, holding your mouse button so an<a id="_idIndexMarker682"/> arrow appears, and connecting <a id="_idIndexMarker683"/>the arrow to your <strong class="bold">Machine Learning Execute Pipeline</strong> activity <a id="_idIndexMarker684"/>as shown in <em class="italic">Figure 10.15</em>:<div id="_idContainer115" class="IMG---Figure"><img src="image/Figure_10.15_B16595.jpg" alt="Figure 10.15 – Connecting activities in an ADF pipeline"/></div><p class="figure-caption">Figure 10.15 – Connecting activities in an ADF pipeline</p></li>
				<li>Configure your <strong class="bold">Machine Learning Execute Pipeline</strong> activity as you did earlier in this chapter. Rename your activity to <strong class="source-inline">Score Iris Data</strong> by selecting the activity and opening the <strong class="bold">General</strong> tab. <p>Then, after opening the <strong class="bold">Settings</strong> tab, set your linked service to <strong class="bold">AMLS Linked Service</strong>. Subsequently set your <strong class="bold">Machine Learning pipeline name</strong> field to <strong class="source-inline">Iris-Scoring-Local-Pipeline</strong> and select the only <strong class="bold">Machine Learning pipeline ID</strong> that appears in the drop-down box.</p></li>
				<li>Click <strong class="bold">Publish All</strong> and <strong class="bold">Publish</strong> to save your work.</li>
				<li>Click <strong class="bold">Move &amp; transform</strong> under <strong class="bold">Activities</strong> and drag a new Copy Data activity onto <a id="_idIndexMarker685"/>your <a id="_idIndexMarker686"/>canvas. Connect it to<a id="_idIndexMarker687"/> the end of your ADF pipeline.</li>
				<li>After selecting the new activity, click <strong class="bold">General</strong> and rename it to <strong class="source-inline">Copy Results to PC</strong>.</li>
				<li>Click <strong class="bold">Source</strong> and click <strong class="bold">+New</strong> to begin the creation of a new input data file.</li>
				<li>Select <strong class="bold">Azure Blob Storage</strong> as your source destination and <strong class="bold">DelimitedText</strong> as your file format. Click <strong class="bold">Continue</strong>.</li>
				<li>Fill out the ADF dataset creation form. Name this object <strong class="source-inline">ScoringResults</strong>. Select <strong class="bold">AMLSDatastoreLink</strong> as your linked service. Navigate to <strong class="source-inline">Iris_Predictions.csv</strong> in <strong class="source-inline">Output_Folder</strong> on your AML datastore. When finished, the form should match <em class="italic">Figure 10.16</em>. Click <strong class="bold">OK</strong>:<div id="_idContainer116" class="IMG---Figure"><img src="image/Figure_10.16_B16595.jpg" alt="Figure 10.16 – ADF dataset creation form "/></div><p class="figure-caption">Figure 10.16 – ADF dataset creation form</p></li>
				<li>Click <strong class="bold">Sink</strong> and <a id="_idIndexMarker688"/>click <strong class="bold">+New</strong> to<a id="_idIndexMarker689"/> begin the <a id="_idIndexMarker690"/>creation of a new output data file.</li>
				<li>Select <strong class="bold">Azure Blob Storage</strong> as your source destination and <strong class="bold">DelimitedText</strong> as your file format. Click <strong class="bold">Continue</strong>.</li>
				<li>Click <strong class="bold">File,</strong> select <strong class="bold">File system</strong>, and click <strong class="bold">Continue</strong>.</li>
				<li>Select <strong class="bold">DelimitedText</strong> as your file format. Click <strong class="bold">Continue</strong>.</li>
				<li>Fill out the ADF dataset creation form. Name this object <strong class="source-inline">ScoringLocalOutput</strong>. Select <strong class="bold">LocalPCLink</strong> as your linked service. Check the box for <strong class="bold">First row as header</strong>. Click <strong class="bold">OK</strong>. This will save your file in the same folder on your PC as your input data.</li>
				<li>Click <strong class="bold">Sink</strong> and<a id="_idIndexMarker691"/> click <strong class="bold">Open</strong>. This will open a new<a id="_idIndexMarker692"/> tab where you can edit the destination <a id="_idIndexMarker693"/>data.</li>
				<li>Edit <strong class="bold">File path</strong>, specifying <strong class="source-inline">Iris_Scoring_Results.csv</strong> as the filename.</li>
				<li>Click <strong class="bold">Publish All</strong> and <strong class="bold">Publish</strong> to save your work.</li>
			</ol>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor166"/>Adding a trigger to your ADF pipeline</h2>
			<p>The last step in<a id="_idIndexMarker694"/> creating an ADF pipeline is <a id="_idIndexMarker695"/>adding a trigger to automate pipeline runs:</p>
			<ol>
				<li value="1">Next, add a trigger as you did earlier in the chapter. Click <strong class="bold">Add trigger</strong> and select <strong class="bold">New/Edit</strong>.</li>
				<li>Click <strong class="bold">Choose trigger</strong> and select <strong class="bold">+New</strong>.</li>
				<li>Name your trigger <strong class="source-inline">Monday Trigger</strong> and set it to run once a week on Mondays at 11:00 A.M. Make sure you set your time zone to your local time zone. Click <strong class="bold">OK</strong> twice.</li>
				<li>Click <strong class="bold">Publish All</strong> and <strong class="bold">Publish</strong> to save your work. Test your new ADF pipeline by clicking <strong class="bold">Trigger (1)</strong> and <strong class="bold">Trigger Now</strong>. Your pipeline should run successfully as shown in <em class="italic">Figure 10.17</em>:</li>
			</ol>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/Figure_10.17_B16595.jpg" alt="Figure 10.17 – Successful end-to-end scoring pipeline "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – Successful end-to-end scoring pipeline</p>
			<p>You have now created a fully automated AutoML scoring solution that will pull in data from your local PC every Monday at 11:00 A.M. and produce a scoring file. In a real situation, this solution would pull data from a database that gets updated on a routine basis. </p>
			<p>This technique is applicable to any ML project; you can use custom trained models, vision models, AutoML models, or any other type of ML model. This pattern is reusable for any batch scoring scenario, and it is the most common deployment scenario across all industries. Practice it.</p>
			<p>With an automated <a id="_idIndexMarker696"/>scoring solution in your<a id="_idIndexMarker697"/> toolkit, your final task is to craft an automated training solution. ML models, for many reasons, often need to be retrained and should be retrained when new data becomes available. By using the same techniques and patterns you used in this section, this will be an easy task.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor167"/>Automating an end-to-end training solution</h1>
			<p>Like any other <a id="_idIndexMarker698"/>ML model, once an AutoML model is deployed and runs for a few months, it can benefit from being retrained. There are many reasons for this, in order of importance:</p>
			<ul>
				<li>ML models break if the pattern between your input data and target column changes. This often happens due to extraneous factors such as changes in consumer behavior. When the pattern breaks, you need to retrain your model to retain performance.</li>
				<li>ML models perform better the more relevant data you feed them. Therefore, as your data grows, you should periodically retrain models. </li>
				<li>Retraining models on a consistent basis means that they're less likely to break if patterns change slowly over time. Consequently, it's best practice to retrain as data is acquired.</li>
			</ul>
			<p>In this section, you are going to put your skills to the test. You will be given a set of instructions similar<a id="_idIndexMarker699"/> to when you created an end-to-end scoring solution. However, this time, there will be significantly less guidance. If you find yourself lost, carefully reread the instructions throughout this chapter. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor168"/>Creating a pipeline to copy data into Azure</h2>
			<p>First, you <a id="_idIndexMarker700"/>need to create an<a id="_idIndexMarker701"/> ADF pipeline to copy data from your PC into Azure:</p>
			<ol>
				<li value="1">Download <strong class="source-inline">Iris_Training_Data_for_ADF.csv</strong> from the GitHub repository and put it in your <strong class="source-inline">Iris</strong> folder on your PC:<p><a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Chapter10/Iris_Training_Data_for_ADF.csv">https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Chapter10/Iris_Training_Data_for_ADF.csv</a></p></li>
				<li>Create a new ADF pipeline called <strong class="source-inline">End-to-End Iris Training</strong>.</li>
				<li>Within the pipeline, create a <strong class="bold">Copy data</strong> activity called <strong class="source-inline">Copy Iris Training Data from PC</strong> where you copy <strong class="source-inline">Iris_Training_Data_for_ADF.csv</strong> into <strong class="source-inline">Input_Folder</strong> on your Azure storage account. <p>Refer to the <strong class="source-inline">Copy Iris Data from PC</strong> activity you created in the <em class="italic">Automating an end-to-end scoring solution</em> section. </p></li>
				<li>Run this pipeline once to move data into Azure.</li>
			</ol>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor169"/>Editing an ML pipeline to train with new data </h2>
			<p>Next, copy and <a id="_idIndexMarker702"/>edit<a id="_idIndexMarker703"/> the <strong class="source-inline">automl-training-pipeline</strong> you created in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>:</p>
			<ol>
				<li value="1">Open AMLS and create a new dataset called <strong class="source-inline">Iris Local Training Data</strong>.</li>
				<li>Open your Jupyter notebook called <strong class="source-inline">automl-training-pipeline</strong>. Make a copy and rename it <strong class="source-inline">automl-local-training-pipeline</strong>.</li>
				<li>Replace the <strong class="source-inline">Iris Training</strong> dataset with the <strong class="source-inline">Iris Local Training Data</strong> dataset within the ML pipeline. Run and publish the ML pipeline with the name <strong class="source-inline">Iris-AutoML-Training-Local-Pipeline</strong>.</li>
			</ol>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor170"/>Adding a Machine Learning Execute Pipeline activity to your ADF pipeline</h2>
			<p>Finally, you'll<a id="_idIndexMarker704"/> add<a id="_idIndexMarker705"/> an <a id="_idIndexMarker706"/>activity to your ADF pipeline to execute the ML pipeline you just created as follows:</p>
			<ol>
				<li value="1">In ADF, add a <strong class="bold">Machine Learning Execute Pipeline</strong> activity to your <strong class="bold">End-to-End Iris Training</strong> pipeline. Name this activity <strong class="source-inline">Retrain Iris Model</strong>.</li>
				<li>Add a trigger called <strong class="source-inline">Tuesday Trigger</strong> to your <strong class="bold">End-to-End Iris Training</strong> pipeline. Schedule this trigger to run every Tuesday at 6:00 A.M. local time.</li>
				<li>Publish your changes to save your work. Your finished pipeline should be a two-step process resembling <em class="italic">Figure 10.18</em>:</li>
			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Figure_10.18_B16595.jpg" alt="Figure 10.18 – Completed retraining pipeline "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.18 – Completed retraining pipeline</p>
			<p>That's it! You've created a solution that will automatically retrain models on a weekly basis with new data. In the real world, this would be pulling from a database that is routinely updated<a id="_idIndexMarker707"/> instead <a id="_idIndexMarker708"/>of from <a id="_idIndexMarker709"/>your local PC. </p>
			<p>Remember that your ML pipeline for training automatically registers models, and your ML pipeline for scoring automatically reuses the latest version of your ML model. As such, there's no need to manually update either of the pipelines from this point on.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor171"/>Summary</h1>
			<p>Automating ML solutions in an end-to-end fashion is no easy task and if you've made it this far, feel proud. Most modern data science organizations can easily train models. Very few can implement reliable, automated, end-to-end solutions as you have done in this chapter. </p>
			<p>You should now feel confident in your ability to design end-to-end AutoML solutions. You can train models with AutoML and create ML pipelines to score data and retrain models. You can easily ingest data into Azure and transfer it out of Azure with ADF. Furthermore, you can tie everything together and create ADF pipelines that seamlessly ingest data, score data, train data, and push results to wherever you'd like. You can now create end-to-end ML solutions.</p>
			<p><a href="B16595_11_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 11</em></a>, <em class="italic">Implementing a Real-Time Scoring Solution</em>, will cement your ML knowledge by teaching you how to score data in real time using Azure Kubernetes Service within AMLS. Adding real-time scoring to your batch-scoring skillset will make you a more complete applied ML expert, able to tackle a wide variety of problems.</p>
		</div>
</body></html>