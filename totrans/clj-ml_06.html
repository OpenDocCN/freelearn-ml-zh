<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Building Support Vector Machines"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Building Support Vector Machines</h1></div></div></div><p>In this chapter, we will explore <span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVMs</strong></span>).<a id="id548" class="indexterm"/> We will study several SVM implementations in Clojure that can be used to build and train an SVM using some given training data.</p><p>SVMs are supervised learning models that are used for both regression and classification. In this chapter, however, we will focus on the problem of classification within the context of SVMs. SVMs find applications in text mining, chemical classification, and image and handwriting recognition. Of course, we should not overlook the fact that the overall performance of a machine learning model mostly depends on the amount and nature of the training data and is also affected by which machine learning model we use to model the available data.</p><p>In the simplest form, an SVM separates and predicts two classes of data by estimating the optimal vector plane or <span class="strong"><strong>hyperplane</strong></span><a id="id549" class="indexterm"/> between these two classes represented in vector space. A <span class="strong"><strong>hyperplane</strong></span> can be simply defined as a plane that has one less dimension than the ambient space. For a three-dimensional space, we would obtain a two-dimensional hyperplane.</p><p>A basic SVM is a non-probabilistic binary classifier that uses linear classification. In addition to linear classification, SVMs can also be used to perform nonlinear classification over several classes. An interesting aspect of SVMs is that the estimated vector plane will have a substantially large and distinct gap between the classes of input values. Due to this, SVMs often have a good generalization performance and also implement a kind of automatic complexity control to avoid overfitting. Hence, SVMs are also called <span class="strong"><strong>large margin classifiers</strong></span><a id="id550" class="indexterm"/>. In this chapter, we will also study how SVMs achieve this large margin between classes of input data, when compared to other classifiers. Another interesting fact about SVMs is that they scale very well with the number of features being modeled and thus, SVMs are often used in machine learning problems that deal with a large number of features.</p><div class="section" title="Understanding large margin classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Understanding large margin classification</h1></div></div></div><p>As we previously mentioned,<a id="id551" class="indexterm"/> SVMs classify input data across large margins. Let's examine how this is achieved. We use our definition of a logistic classification model, which we previously described in <a class="link" href="ch03.html" title="Chapter 3. Categorizing Data">Chapter 3</a>, <span class="emphasis"><em>Categorizing Data</em></span>, as a basis for reasoning about SVMs.</p><p>We can use the logistic or <span class="emphasis"><em>sigmoid</em></span> function to separate two classes of input values, as we described in <a class="link" href="ch03.html" title="Chapter 3. Categorizing Data">Chapter 3</a>, <span class="emphasis"><em>Categorizing Data</em></span>. This function can be formally defined as a function of an input variable <span class="emphasis"><em>X</em></span> as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_01.jpg" alt="Understanding large margin classification"/></div><p>In the preceding equation, the output variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_02.jpg" alt="Understanding large margin classification"/></span> depends not only on the variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_03.jpg" alt="Understanding large margin classification"/></span>, but also on the coefficient <span class="inlinemediaobject"><img src="graphics/4351OS_06_04.jpg" alt="Understanding large margin classification"/></span>. The variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_03.jpg" alt="Understanding large margin classification"/></span> is analogous to the vector of input values in our model, and the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_04.jpg" alt="Understanding large margin classification"/></span> is the parameter vector of the model. For binary classification, the value of <span class="emphasis"><em>Y</em></span> must exist in the range of 0 and 1. Also, the class of a set of input values is determined by whether the output variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_02.jpg" alt="Understanding large margin classification"/></span> is closer to 0 or 1. For these values of <span class="emphasis"><em>Y</em></span>, the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_05.jpg" alt="Understanding large margin classification"/></span> is either much greater than or much less than 0. This can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_06.jpg" alt="Understanding large margin classification"/></div><p>For <span class="inlinemediaobject"><img src="graphics/4351OS_06_07.jpg" alt="Understanding large margin classification"/></span> sample with input values <span class="inlinemediaobject"><img src="graphics/4351OS_06_08.jpg" alt="Understanding large margin classification"/></span> and output values <span class="inlinemediaobject"><img src="graphics/4351OS_06_09.jpg" alt="Understanding large margin classification"/></span>, we define the cost function <span class="inlinemediaobject"><img src="graphics/4351OS_06_10.jpg" alt="Understanding large margin classification"/></span> as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_12.jpg" alt="Understanding large margin classification"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>Note that the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_11.jpg" alt="Understanding large margin classification"/></span> represents the output variable calculated from the estimated model.</p></div></div><p>For a logistic classification model, <span class="inlinemediaobject"><img src="graphics/4351OS_06_11.jpg" alt="Understanding large margin classification"/></span> is the value of logistic function when applied to a set of input values <span class="inlinemediaobject"><img src="graphics/4351OS_06_08.jpg" alt="Understanding large margin classification"/></span>. We can simplify and<a id="id552" class="indexterm"/> expand the summation term <span class="inlinemediaobject"><img src="graphics/4351OS_06_13.jpg" alt="Understanding large margin classification"/></span> in the cost function defined in the preceding equation, as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_14.jpg" alt="Understanding large margin classification"/></div><p>It's obvious that the cost function shown in the preceding expression depends on the two logarithmic terms in the expression. Thus, we can represent the cost function as a function of these two logarithmic terms, represented by the terms <span class="inlinemediaobject"><img src="graphics/4351OS_06_15.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span>. Now, let's assume the two terms as shown the following equation:</p><div class="mediaobject"><img src="graphics/4351OS_06_17.jpg" alt="Understanding large margin classification"/></div><p>Both the functions <span class="inlinemediaobject"><img src="graphics/4351OS_06_15.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span> are composed using the logistic function. A classifier that models the logistic function must be trained such that these two functions are minimized over all possible values of the parameter vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_04.jpg" alt="Understanding large margin classification"/></span>. We can use the <span class="strong"><strong>hinge-loss</strong></span> function to approximate the desired behavior of a linear classifier that uses the logistic function (for more information, refer to "Are Loss Functions All the Same?"). We will now study the hinge-loss function by comparing it to the logistic function. The following diagram depicts how the <span class="inlinemediaobject"><img src="graphics/4351OS_06_15.jpg" alt="Understanding large margin classification"/></span> function must vary with respect to the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_18.jpg" alt="Understanding large margin classification"/></span> and how it can be modeled using the logistic and hinge-loss functions:</p><div class="mediaobject"><img src="graphics/image1.jpg" alt="Understanding large margin classification"/></div><p>In the plot shown in<a id="id553" class="indexterm"/> the preceding diagram, the logistic function is represented as a smooth curve. The function is seen to decrease rapidly before a given point and then decreases at a lower rate. In this example, the point at which this change of rate of the logistic function occurs is found to be <span class="emphasis"><em>x = 0</em></span>. The hinge-loss function approximates this by using two line segments that meet at the point <span class="emphasis"><em>x = 0</em></span>. Interestingly, both these functions model a behavior that changes at a rate that is inversely proportional to the input value <span class="emphasis"><em>x</em></span>. Similarly, we can approximate the effect of the <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span> function using the hinge-loss function as follows:</p><div class="mediaobject"><img src="graphics/image2.jpg" alt="Understanding large margin classification"/></div><p>Note that the <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span> function is directly proportional to the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_18.jpg" alt="Understanding large margin classification"/></span>. Thus, we can achieve the classification ability of the logistic function by<a id="id554" class="indexterm"/> modelling the hinge-loss function and a classifier built using the hinge-loss function will perform equally well as a classifier using the logistic function.</p><p>As seen in the preceding diagram, the hinge-loss function only changes its value at the point <span class="inlinemediaobject"><img src="graphics/4351OS_06_21.jpg" alt="Understanding large margin classification"/></span>. This applies to both the functions <span class="inlinemediaobject"><img src="graphics/4351OS_06_15.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span>. Thus, we can use the hinge loss function to separate two classes of data depending on whether the value of <span class="inlinemediaobject"><img src="graphics/4351OS_06_23.jpg" alt="Understanding large margin classification"/></span> is greater or less than 0. In this case, there's virtually no margin of separation between these two classes. To improve the margin of classification, we can modify the hinge-loss function such that its value is greater than 0 only when <span class="inlinemediaobject"><img src="graphics/4351OS_06_24.jpg" alt="Understanding large margin classification"/></span> or <span class="inlinemediaobject"><img src="graphics/4351OS_06_25.jpg" alt="Understanding large margin classification"/></span>. </p><p>The modified hinge-loss functions can be plotted for the two classes of data as follows. The following<a id="id555" class="indexterm"/> plot describes the case where <span class="inlinemediaobject"><img src="graphics/4351OS_06_25.jpg" alt="Understanding large margin classification"/></span>:</p><div class="mediaobject"><img src="graphics/image3.jpg" alt="Understanding large margin classification"/></div><p>Similarly, the modified hinge-loss function for the case <span class="inlinemediaobject"><img src="graphics/4351OS_06_24.jpg" alt="Understanding large margin classification"/></span> can be illustrated by the following plot:</p><div class="mediaobject"><img src="graphics/image4.jpg" alt="Understanding large margin classification"/></div><p>Note that the <span class="emphasis"><em>hinge</em></span> occurs at <span class="emphasis"><em>-1</em></span> in the case of <span class="inlinemediaobject"><img src="graphics/4351OS_06_24.jpg" alt="Understanding large margin classification"/></span>.</p><p>If we substitute the<a id="id556" class="indexterm"/> hinge-loss functions in place of the <span class="inlinemediaobject"><img src="graphics/4351OS_06_15.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span> functions, we arrive at an optimization problem of SVMs (for more information, refer to "Support-vector networks"), which can be formally written as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_28.jpg" alt="Understanding large margin classification"/></div><p>In the preceding equation, the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_29.jpg" alt="Understanding large margin classification"/></span> is the regularization parameter. Also, when <span class="inlinemediaobject"><img src="graphics/4351OS_06_30.jpg" alt="Understanding large margin classification"/></span>, the behavior of the SVM is affected more by the <span class="inlinemediaobject"><img src="graphics/4351OS_06_15.jpg" alt="Understanding large margin classification"/></span> function than the <span class="inlinemediaobject"><img src="graphics/4351OS_06_16.jpg" alt="Understanding large margin classification"/></span> function, and vice versa when <span class="inlinemediaobject"><img src="graphics/4351OS_06_31.jpg" alt="Understanding large margin classification"/></span>. In some contexts, the regularization parameter <span class="inlinemediaobject"><img src="graphics/4351OS_06_29.jpg" alt="Understanding large margin classification"/></span> of the model is added to the optimization problem as a constant <span class="emphasis"><em>C</em></span>, where <span class="emphasis"><em>C</em></span> is analogous to <span class="inlinemediaobject"><img src="graphics/4351OS_06_32.jpg" alt="Understanding large margin classification"/></span>. This representation of the optimization problem can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_33.jpg" alt="Understanding large margin classification"/></div><p>As we only deal with<a id="id557" class="indexterm"/> two classes of data in which <span class="inlinemediaobject"><img src="graphics/4351OS_06_09.jpg" alt="Understanding large margin classification"/></span> is either 0 or 1, we can rewrite the optimization problem described previously, as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_34.jpg" alt="Understanding large margin classification"/></div><p>Let's try to visualize the behavior of an SVM on some training data. Suppose we have two input variables <span class="inlinemediaobject"><img src="graphics/4351OS_06_35.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_36.jpg" alt="Understanding large margin classification"/></span> in our training data. The input values and their classes can represented by the following plot diagram:</p><div class="mediaobject"><img src="graphics/image5.jpg" alt="Understanding large margin classification"/></div><p>In the preceding plot diagram, the two classes in the training data are represented as circles and squares. A linear classifier will attempt to partition these sample values into two distinct classes and will produce a decision boundary that can be represented by any one of the lines in the preceding plot diagram. Of course, the classifier should strive to minimize the overall error of the formulated model, while also finding a model that generalizes the data well. An <a id="id558" class="indexterm"/>SVM will also attempt to partition the sample data into two classes just as any other classification model. However, the SVM manages to determine a hyperplane of separation that is observed to have the largest possible margin between the two classes of input data.</p><p>This behavior of an SVM can be illustrated using the following plot:</p><div class="mediaobject"><img src="graphics/4351OS_06_38_a.jpg" alt="Understanding large margin classification"/></div><p>As shown in the preceding plot diagram, an SVM will determine the optimal hyperplane that separates the two <a id="id559" class="indexterm"/>classes of data with the maximum possible margin between these two classes. From the optimization problem of an SVM, which we previously described, we can prove that the equation of the hyperplane of separation estimated by the SVM is as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_40.jpg" alt="Understanding large margin classification"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>Note that in the preceding equation, the constant <span class="inlinemediaobject"><img src="graphics/4351OS_06_39.jpg" alt="Understanding large margin classification"/></span> is simply the y-intercept of the hyperplane.</p></div></div><p>To understand more about how an SVM achieves this large margin of separation, we need to use some elementary vector arithmetic. Firstly, we can define the length of a given vector as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_41.jpg" alt="Understanding large margin classification"/></div><div class="mediaobject"><img src="graphics/4351OS_06_42.jpg" alt="Understanding large margin classification"/></div><p>Another operation that is often used to describe SVMs is the inner product of the two vectors. The inner product of two given vectors can be formally defined as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_43.jpg" alt="Understanding large margin classification"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>Note that the inner product of two vectors only exists if the two vectors are of the same length.</p></div></div><p>As shown in the<a id="id560" class="indexterm"/> preceding equation, the inner product <span class="inlinemediaobject"><img src="graphics/4351OS_06_44.jpg" alt="Understanding large margin classification"/></span> of the two vectors <span class="inlinemediaobject"><img src="graphics/4351OS_06_45.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_46.jpg" alt="Understanding large margin classification"/></span> is equal to the dot product of the transpose of <span class="inlinemediaobject"><img src="graphics/4351OS_06_45.jpg" alt="Understanding large margin classification"/></span> and the vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_46.jpg" alt="Understanding large margin classification"/></span>. Another way to represent the inner product of two vectors is in terms of the projection of one vector onto another, which is given as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_47.jpg" alt="Understanding large margin classification"/></div><p>Note that the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_48.jpg" alt="Understanding large margin classification"/></span> is equivalent to the vector product <span class="inlinemediaobject"><img src="graphics/4351OS_06_49.jpg" alt="Understanding large margin classification"/></span> of the vector V and the transpose of the vector U. Since the expression <span class="inlinemediaobject"><img src="graphics/4351OS_06_50.jpg" alt="Understanding large margin classification"/></span> is equivalent to the product <span class="inlinemediaobject"><img src="graphics/4351OS_06_51.jpg" alt="Understanding large margin classification"/></span> of the vectors, we can rewrite the optimization problem, which we described earlier in terms of the projection of the input variables onto the output variable. This can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_52.jpg" alt="Understanding large margin classification"/></div><p>Hence, an SVM attempts to minimize the squared sum of the elements in the parameter vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_04.jpg" alt="Understanding large margin classification"/></span> while ensuring that the optimal hyperplane that separates the two<a id="id561" class="indexterm"/> classes of data is present in between the two planes and <span class="inlinemediaobject"><img src="graphics/4351OS_06_53.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_54.jpg" alt="Understanding large margin classification"/></span>. These two planes are called the <span class="strong"><strong>support vectors</strong></span> of the SVM. Since we must minimize the values of the elements in the parameter vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_04.jpg" alt="Understanding large margin classification"/></span>, the projection <span class="inlinemediaobject"><img src="graphics/4351OS_06_55.jpg" alt="Understanding large margin classification"/></span> must be large enough to ensure that <span class="inlinemediaobject"><img src="graphics/4351OS_06_56.jpg" alt="Understanding large margin classification"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_06_57.jpg" alt="Understanding large margin classification"/></span>:</p><div class="mediaobject"><img src="graphics/4351OS_06_58.jpg" alt="Understanding large margin classification"/></div><p>Thus, the SVM will ensure that the projection of the input variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_08.jpg" alt="Understanding large margin classification"/></span> onto the output variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_09.jpg" alt="Understanding large margin classification"/></span> is as large as possible. This implies that the SVM will find the largest possible margin between the two classes of input values in the training data.</p><div class="section" title="Alternative forms of SVMs"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec11"/>Alternative forms of SVMs</h2></div></div></div><p>We will now describe a<a id="id562" class="indexterm"/> couple of alternative forms <a id="id563" class="indexterm"/>to represent an SVM. The remainder of this section can be safely skipped, but the reader is advised to know these forms as they also widely used notations of SVMs.</p><p>If <span class="inlinemediaobject"><img src="graphics/4351OS_06_59.jpg" alt="Alternative forms of SVMs"/></span> is the normal to hyperplane estimated by an SVM, we can represent this hyperplane of separation using the following equation:</p><div class="mediaobject"><img src="graphics/4351OS_06_61.jpg" alt="Alternative forms of SVMs"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>Note that in the preceding equation, the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_60.jpg" alt="Alternative forms of SVMs"/></span> is the y-intercept of the hyperplane and is analogous to the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_39.jpg" alt="Alternative forms of SVMs"/></span> in the equation of the hyperplane that we previously described.</p></div></div><p>The two peripheral support vectors of this hyperplane have the following equations:</p><div class="mediaobject"><img src="graphics/4351OS_06_62.jpg" alt="Alternative forms of SVMs"/></div><p>We can use the expression <span class="inlinemediaobject"><img src="graphics/4351OS_06_63.jpg" alt="Alternative forms of SVMs"/></span> to determine the class of a given set <a id="id564" class="indexterm"/>of input values. If the value of this expression is less than or equal to -1, then we can say <a id="id565" class="indexterm"/>that the input values belong to one of the two classes of data. Similarly, if the value of the expression <span class="inlinemediaobject"><img src="graphics/4351OS_06_63.jpg" alt="Alternative forms of SVMs"/></span> is greater than or equal to 1, the input values are predicted to belong to the second class. This can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_64.jpg" alt="Alternative forms of SVMs"/></div><p>The two inequalities described in the preceding equation can be combined into a single inequality, as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_65.jpg" alt="Alternative forms of SVMs"/></div><p>Thus, we can concisely rewrite the optimization problem of SVMs as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_66.jpg" alt="Alternative forms of SVMs"/></div><p>In the constrained problem defined in the preceding equation, we use the normal <span class="inlinemediaobject"><img src="graphics/4351OS_06_59.jpg" alt="Alternative forms of SVMs"/></span> instead<a id="id566" class="indexterm"/> of the parameter vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_04.jpg" alt="Alternative forms of SVMs"/></span> to parameterize the optimization problem. By using Lagrange multipliers <span class="inlinemediaobject"><img src="graphics/4351OS_06_67.jpg" alt="Alternative forms of SVMs"/></span>, we can express the optimization problem as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_68.jpg" alt="Alternative forms of SVMs"/></div><p>This form of the optimization problem of an SVM is known as the <span class="strong"><strong>primal form</strong></span><a id="id567" class="indexterm"/>. Note that in practice, only a few of the Lagrange multipliers will have a value greater than 0. Also, this solution can be <a id="id568" class="indexterm"/>expressed as a linear combination of the input vectors <span class="inlinemediaobject"><img src="graphics/4351OS_06_08.jpg" alt="Alternative forms of SVMs"/></span> and the output variable <span class="inlinemediaobject"><img src="graphics/4351OS_06_09.jpg" alt="Alternative forms of SVMs"/></span>, as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_69.jpg" alt="Alternative forms of SVMs"/></div><p>We can also express the optimization problem of an SVM in the <span class="emphasis"><em>dual form</em></span>, which is a constrained representation that can be described as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_70.jpg" alt="Alternative forms of SVMs"/></div><p>In the constrained problem<a id="id569" class="indexterm"/> described in the preceding<a id="id570" class="indexterm"/> equation, the function <span class="inlinemediaobject"><img src="graphics/4351OS_06_71.jpg" alt="Alternative forms of SVMs"/></span> is called the <span class="strong"><strong>kernel function</strong></span><a id="id571" class="indexterm"/> and we will discuss more about the role of this function in SVMs in the later sections of this chapter.</p></div></div></div>
<div class="section" title="Linear classification using SVMs"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Linear classification using SVMs</h1></div></div></div><p>As we previously described, SVMs can be used to perform linear classification over two distinct classes. An SVM<a id="id572" class="indexterm"/> will attempt to find a hyperplane that separates the two classes such that the estimated hyperplane describes the maximum achievable margin of separation between the two classes in our model.</p><p>For example, an estimated hyperplane between two classes of data can be visualized using the following plot diagram:</p><div class="mediaobject"><img src="graphics/4351OS_06_72.jpg" alt="Linear classification using SVMs"/></div><p>As depicted in the graph shown in the preceding plot diagram, the circles and crosses are used to represent the two<a id="id573" class="indexterm"/> classes of input values in the sample data. The line represents the estimated hyperplane of an SVM.</p><p>In practice, it's often more efficient to use an implemented SVM rather than implement our own SVM. There are several libraries that implement SVMs that have been ported to multiple programming languages. One such library is <span class="strong"><strong>LibLinear</strong></span><a id="id574" class="indexterm"/> (<a class="ulink" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">http://www.csie.ntu.edu.tw/~cjlin/liblinear/</a>), which implements a linear classifier using an SVM. The Clojure wrapper for LibLinear is <code class="literal">clj-liblinear</code> (<a class="ulink" href="https://github.com/lynaghk/clj-liblinear">https://github.com/lynaghk/clj-liblinear</a>) and we will now explore how we can use this library to easily build a linear classifier.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>The <code class="literal">clj-liblinear</code> library can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">[clj-liblinear "0.1.0"]</pre></div><p>For the example that will follow, the namespace declaration should look similar to the following declaration:</p><div class="informalexample"><pre class="programlisting">(ns my-namespace
  (:use [clj-liblinear.core :only [train predict]]))</pre></div></div></div><p>Firstly, let's generate some training data, such that we have two classes of input values. For this example, we will model two input variables, as follows:</p><div class="informalexample"><pre class="programlisting">(def training-data
  (concat
   (repeatedly
    500 #(hash-map :class 0
                   :data {:x (rand)
                          :y (rand)}))
   (repeatedly
    500 #(hash-map :class 1
                   :data {:x (- (rand))
                          :y (- (rand))}))))</pre></div><p>Using the <code class="literal">repeatedly</code> function<a id="id575" class="indexterm"/> as shown in the preceding code, we generate two sequences of maps. Each map in these two sequences contains the keys <code class="literal">:class</code> and <code class="literal">:data</code>. The value of the <code class="literal">:class</code> key represents the class of category of the input values and the value of the <code class="literal">:data</code> key is itself another map with the keys <code class="literal">:x</code> and <code class="literal">:y</code>. The values of the keys <code class="literal">:x</code> and <code class="literal">:y</code> represent <a id="id576" class="indexterm"/>the two input variables in our training data. These values for the input variables are randomly generated using the <code class="literal">rand</code> function. The training data is generated such that the class of a set of input values is <code class="literal">0</code> if both the input values are positive, and the class of a set of input values is <code class="literal">1</code> if both the input values are negative. As shown in the preceding code, a total of a 1,000 samples are generated for two classes as two sequences using the <code class="literal">repeatedly</code> function, and then combined into a single sequence using the <code class="literal">concat</code> function. We can inspect some of these input values in the REPL, as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (first training-data)
{:class 0,
 :data {:x 0.054125811753944264, :y 0.23575052637986382}}
user&gt; (last training-data)
{:class 1,
 :data {:x -0.8067872409710037, :y -0.6395480020409928}}</pre></div><p>We can create and train an SVM using the training data we've generated. To do this, we use the <code class="literal">train</code> function. The <code class="literal">train</code> function accepts two arguments, which include a sequence of input values and a sequence of output values. Both sequences are assumed to be in the same order. For the purpose of classification, the output variable can be set to the class of a given set of input values as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn train-svm []
  (train
   (map :data training-data)
   (map :class training-data)))</pre></div><p>The <code class="literal">train-svm</code> function<a id="id577" class="indexterm"/> defined in the preceding code will instantiate and train an SVM with the <code class="literal">training-data</code> sequence. Now, we can use the trained SVM to perform classification using the <code class="literal">predict</code> function<a id="id578" class="indexterm"/>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">user&gt; (def svm (train-svm))
#'user/svm
user&gt; (predict svm {:x 0.5 :y 0.5})
0.0
user&gt; (predict svm {:x -0.5 :y 0.5})
0.0
user&gt; (predict svm {:x -0.4 :y 0.4})
0.0
user&gt; (predict svm {:x -0.4 :y -0.4})
1.0
user&gt; (predict svm {:x 0.5 :y -0.5})
1.0</pre></div><p>The <code class="literal">predict</code> function requires<a id="id579" class="indexterm"/> two parameters, which are an instance of an SVM and a set of input values.</p><p>As shown in the preceding code, we use the <code class="literal">svm</code> variable to represent a trained SVM. We then pass the <code class="literal">svm</code> variable to the <code class="literal">predict</code> function, along with a new set of input values whose class we intend to predict. It's observed that the output of the <code class="literal">predict</code> function agrees with the training data. Interestingly, the classifier predicts the class of any set of input values as <code class="literal">0</code> as long as the input value <code class="literal">:y</code> is positive, and conversely the class of a set of input values whose <code class="literal">:y</code> feature is negative is predicted as <code class="literal">1</code>.</p><p>In the previous example, we used an SVM to perform classification. However, the output variable of the trained SVM was always a number. Thus, we could also use the <code class="literal">clj-liblinear</code> library in the same way as described in the preceding code to train a regression model.</p><p>The <code class="literal">clj-liblinear</code> library also supports more complex types for the features of an SVM, such as vectors, maps, and sets. We will now demonstrate how we can train a classifier that uses sets as input variables, instead of plain numbers as shown in the previous example. Suppose we have a stream of tweets from a given user's Twitter feed. Assume that the user will manually classify these tweets into a specific category, which is selected from a set of predefined categories. This processed sequence of tweets can be represented as follows:</p><div class="informalexample"><pre class="programlisting">(def tweets
  [{:class 0 :text "new lisp project released"}
   {:class 0 :text "try out this emacs package for common lisp"}
   {:class 0 :text "a tutorial on guile scheme"}

   {:class 1 :text "update in javascript library"}
   {:class 1 :text "node.js packages are now supported"}
   {:class 1 :text "check out this jquery plugin"}

   {:class 2 :text "linux kernel news"}
   {:class 2 :text "unix man pages"}
   {:class 2 :text "more about linux software"}])</pre></div><p>The tweets vector defined in the preceding code contains several maps, each of which have the keys <code class="literal">:class</code> and <code class="literal">:text</code>. The <code class="literal">:text</code> key contains the text of a tweet, and we will train an SVM using the value contained by the <code class="literal">:text</code> keyword. But we can't use the text in verbatim, since some<a id="id580" class="indexterm"/> words might be repeated in a tweet. Also, we need some way of dealing with the case of the letters in this text. Let's define a function to convert this text into a set as follows:</p><div class="informalexample"><pre class="programlisting">(defn extract-words [text]
  (-&gt;&gt; #" "
       (split text)
       (map lower-case)
       (into #{})))</pre></div><p>The <code class="literal">extract-words</code> function<a id="id581" class="indexterm"/> defined in the preceding code will convert any string, represented by the parameter <code class="literal">text</code>, into a set of words that are all in lower case. To create a set, we use the <code class="literal">(into #{})</code> form. By definition, this set will not contain any duplicate values. Note the use of the <code class="literal">-&gt;&gt;</code> threading macro in the definition of the <code class="literal">extract-words</code> function.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>In the <code class="literal">extract-words</code>  function, the <code class="literal">-&gt;&gt;</code> form can be equivalently written as <code class="literal">(into #{} (map lower-case (split text #" ")))</code>.</p></div></div><p>We can inspect the behavior of the <code class="literal">extract-words</code> function in the REPL, as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (extract-words "Some text to extract some words")
#{"extract" "words" "text" "some" "to"}</pre></div><p>Using the <code class="literal">extract-words</code> function<a id="id582" class="indexterm"/>, we can effectively train an SVM with a set of strings as a feature variable. As we mentioned earlier, this can be done using the <code class="literal">train</code> function<a id="id583" class="indexterm"/>, as follows:</p><div class="informalexample"><pre class="programlisting">(defn train-svm []
  (train (-&gt;&gt; tweets
              (map :text)
              (map extract-words))
         (map :class tweets)))</pre></div><p>The <code class="literal">train-svm</code> function<a id="id584" class="indexterm"/> defined in the preceding code will create and train an SVM with the processed training data in the tweets variable using the <code class="literal">train</code> and <code class="literal">extract-word</code>s functions. We now need to compose the <code class="literal">predict</code> and <code class="literal">extract-words</code> functions in the following code so that we can predict the class of a given tweet:</p><div class="informalexample"><pre class="programlisting">(defn predict-svm [svm text]
  (predict
    svm (extract-words text)))</pre></div><p>The <code class="literal">predict-svm</code> function<a id="id585" class="indexterm"/> defined in the preceding code can be used to classify a given tweet. We can verify the predicted<a id="id586" class="indexterm"/> classes of the SVM for some arbitrary tweets in the REPL, as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (def svm (train-svm))
#'user/svm
user&gt; (predict-svm svm "a common lisp tutorial")
0.0
user&gt; (predict-svm svm "new javascript library")
1.0
user&gt; (predict-svm svm "new linux kernel update")
2.0</pre></div><p>In conclusion, the <code class="literal">clj-liblinear</code> library allows us to easily build and train an SVM with most Clojure data types. The only restriction that is imposed by this library is that the training data must be linearly separable into the classes of our model. We will study how we can build more complex classifiers in the following sections of this chapter.</p></div>
<div class="section" title="Using kernel SVMs"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Using kernel SVMs</h1></div></div></div><p>In some cases, the available<a id="id587" class="indexterm"/> training data is not linearly separable and we would not be able to model the data using linear classification. Thus, we need to use different models to fit nonlinear data. As described in <a class="link" href="ch04.html" title="Chapter 4. Building Neural Networks">Chapter 4</a>, <span class="emphasis"><em>Building Neural Networks</em></span>, ANNs can be used to model this kind of data. In this section, we will describe how we can fit an SVM on nonlinear data using kernel functions. An SVM that incorporates kernel function is termed as a <span class="strong"><strong>kernel support vector machine</strong></span>. Note that, in this section, the terms SVM and kernel SVM are used interchangeably. A kernel SVM will classify data based on a nonlinear decision boundary, and the nature of the decision boundary depends on the kernel function that is used by the SVM. To illustrate this behavior, a kernel SVM will classify the training data into two classes as described by the following plot diagram:</p><div class="mediaobject"><img src="graphics/4351OS_06_73.jpg" alt="Using kernel SVMs"/></div><p>The concept of using kernel functions in SVMs is actually based on mathematical transformation. The role <a id="id588" class="indexterm"/>of the kernel function in an SVM is to transform the input variables in the training data such that the transformed features are linearly separable. Since an SVM linearly partitions the input data based on a large margin, this large gap of separation between the two classes of data will also be observable in a nonlinear space.</p><p>The kernel function is written as <span class="inlinemediaobject"><img src="graphics/4351OS_06_71.jpg" alt="Using kernel SVMs"/></span>, where <span class="inlinemediaobject"><img src="graphics/4351OS_06_08.jpg" alt="Using kernel SVMs"/></span> is a vector of input values from the training data and <span class="inlinemediaobject"><img src="graphics/4351OS_06_74.jpg" alt="Using kernel SVMs"/></span> is the transformed vector of <span class="inlinemediaobject"><img src="graphics/4351OS_06_75.jpg" alt="Using kernel SVMs"/></span>. The function <span class="inlinemediaobject"><img src="graphics/4351OS_06_71.jpg" alt="Using kernel SVMs"/></span> represents the similarity of these two vectors and is equivalent to the inner product of these two vectors in the transformed space. If the input vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_75.jpg" alt="Using kernel SVMs"/></span> has a given class, then the class of the vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_74.jpg" alt="Using kernel SVMs"/></span> is the same as that of the vector <span class="inlinemediaobject"><img src="graphics/4351OS_06_75.jpg" alt="Using kernel SVMs"/></span> when the kernel function of these two vectors has a value close to 1, that is, when <span class="inlinemediaobject"><img src="graphics/4351OS_06_76.jpg" alt="Using kernel SVMs"/></span>. A kernel function can be mathematically expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_77.jpg" alt="Using kernel SVMs"/></div><p>In the preceding equation, the function <span class="inlinemediaobject"><img src="graphics/4351OS_06_78.jpg" alt="Using kernel SVMs"/></span> performs the transformation from a nonlinear space <span class="inlinemediaobject"><img src="graphics/4351OS_06_79.jpg" alt="Using kernel SVMs"/></span> into a linear space <span class="inlinemediaobject"><img src="graphics/4351OS_06_46.jpg" alt="Using kernel SVMs"/></span>. Note that the explicit representation of <span class="inlinemediaobject"><img src="graphics/4351OS_06_78.jpg" alt="Using kernel SVMs"/></span> is not required, and it's enough to know that <span class="inlinemediaobject"><img src="graphics/4351OS_06_46.jpg" alt="Using kernel SVMs"/></span> is an inner product space. Although we are free to choose any arbitrary kernel function to model the given training<a id="id589" class="indexterm"/> data, we must strive to reduce the problem of minimizing the cost function of the formulated SVM model. Thus, the kernel function is generally selected such that calculating the SVM's decision boundary only requires determining the dot products of vectors in the transformed feature space <span class="inlinemediaobject"><img src="graphics/4351OS_06_46.jpg" alt="Using kernel SVMs"/></span>.</p><p>A common choice for the kernel function of an SVM is the <span class="strong"><strong>polynomial kernel function</strong></span><a id="id590" class="indexterm"/>, also called the <span class="strong"><strong>polynomic kernel function</strong></span>,<a id="id591" class="indexterm"/> which models the training data as polynomials of the original feature variables. As the reader may recall from <a class="link" href="ch05.html" title="Chapter 5. Selecting and Evaluating Data">Chapter 5</a>, <span class="emphasis"><em>Selecting and Evaluating Data</em></span>, we have discussed how polynomial features can greatly improve the performance of a given machine learning model. The polynomial kernel function can be thought of as an extension of this concept that applies to SVMs. This function can be formally expressed as follows.</p><div class="mediaobject"><img src="graphics/4351OS_06_80.jpg" alt="Using kernel SVMs"/></div><p>In the preceding equation, the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_81.jpg" alt="Using kernel SVMs"/></span> represents the highest degree of the polynomial features. Also, when (the constant) <span class="inlinemediaobject"><img src="graphics/4351OS_06_82.jpg" alt="Using kernel SVMs"/></span>, the kernel is termed to be<a id="id592" class="indexterm"/> <span class="strong"><strong>homogenous</strong></span>.</p><p>Another widely used kernel function is the <span class="strong"><strong>Gaussian kernel function</strong></span><a id="id593" class="indexterm"/>. Most readers who are adept in linear algebra will need no introduction to the Gaussian function. <a id="id594" class="indexterm"/>It's important to know that this function represents a normal distribution of data in which the data points are closer to the mean of the data. </p><p>In the context of SVMs, the Gaussian kernel function can be used to represent a model in which one of the two classes in the training data has values for the input variables that are close to an arbitrary mean value. The Gaussian kernel function can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_06_83.jpg" alt="Using kernel SVMs"/></div><p>In the Gaussian kernel function defined in the preceding equation, the term <span class="inlinemediaobject"><img src="graphics/4351OS_06_84.jpg" alt="Using kernel SVMs"/></span> represents the variance of the training data and represents the <span class="emphasis"><em>width</em></span> of the Gaussian kernel.</p><p>Another popular choice for the kernel function is the <span class="strong"><strong>string kernel function</strong></span><a id="id595" class="indexterm"/> that operates<a id="id596" class="indexterm"/> on string values. By the term <span class="emphasis"><em>string</em></span>, we mean a finite sequence of symbols. The string kernel function essentially<a id="id597" class="indexterm"/> measures the similarity between two given strings. If both the strings passed to the string kernel function are the same, the value returned by this function will be <code class="literal">1</code>. Thus, the string kernel function is useful in modeling data where the features are represented as strings.</p><div class="section" title="Sequential minimal optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec12"/>Sequential minimal optimization</h2></div></div></div><p>The optimization problem<a id="id598" class="indexterm"/> of an SVM can be solved using <span class="strong"><strong>Sequential Minimal Optimization</strong></span> (<span class="strong"><strong>SMO</strong></span>). The optimization problem of an SVM is the numerical optimization of the cost<a id="id599" class="indexterm"/> function across several dimensions in order to reduce the overall error of the trained SVM. In practice, this must be done through numerical optimization techniques. A complete discussion of the SMO algorithm is beyond the scope of this book. However, we must note that this algorithm solves the optimization problem by a <span class="emphasis"><em>divide-and-conquer</em></span> technique. Essentially, SMO divides the optimization problem of multiple dimensions into several smaller two-dimensional problems that can be solved analytically (for more information, refer to <span class="emphasis"><em>Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines</em></span>).</p><p>
<span class="strong"><strong>LibSVM</strong></span><a id="id600" class="indexterm"/> is a popular library that implements SMO to train an SVM. The <code class="literal">svm-clj</code> library is a Clojure wrapper for LibSVM and we will now explore how we can use this library to formulate an SVM model.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>The <code class="literal">svm-clj</code> library can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">[svm-clj "0.1.3"]</pre></div><p>For the example that will follow, the namespace declaration should look similar to the following declaration:</p><div class="informalexample"><pre class="programlisting">(ns my-namespace
  (:use svm.core))</pre></div></div></div><p>This example will use a simplified version of the <span class="strong"><strong>SPECT Heart</strong></span> dataset<a id="id601" class="indexterm"/> (<a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/SPECT+Heart">http://archive.ics.uci.edu/ml/datasets/SPECT+Heart</a>). This dataset describes the diagnosis of several heart disease patients using <span class="strong"><strong>Single Proton Emission Computed Tomography</strong></span> (<span class="strong"><strong>SPECT</strong></span>) images.<a id="id602" class="indexterm"/> The original dataset contains a total of 267 samples, in which each sample<a id="id603" class="indexterm"/> has 23 features. The output variable of the dataset describes a positive or negative diagnosis of a given patient, which is<a id="id604" class="indexterm"/> represented using either +1 or -1, respectively.</p><p>For this example, the training data is stored in a file named <code class="literal">features.dat</code>. This file must be placed in the <code class="literal">resources/</code> directory of the Leiningen project to make it available for use. This file contains several input features and the class of these input values. Let's have a look at one of the following sample values in this file:</p><div class="informalexample"><pre class="programlisting">+1 2:1 3:1 4:-0.132075 5:-0.648402 6:1 7:1 8:0.282443 9:1 10:0.5 11:1 12:-1 13:1</pre></div><p>As shown in the preceding line of code, the first value <code class="literal">+1</code> denotes the class of the sample and the other values represent the input variables. Note that the indexes of the input variables are also given. Also, the value of the first feature in the preceding sample is <code class="literal">0</code>, as it is not mentioned using a <code class="literal">1:</code> key. From the preceding line, it's clear that each sample will have a maximum of 12 features. All sample values must conform to this format, as dictated by LibSVM.</p><p>We can train an SVM using this sample data. To do this, we use the <code class="literal">train-model</code> function from the <code class="literal">svm-clj</code> library. Also, since we must first load the sample data from the file, we will need to first call the <code class="literal">read-dataset</code> function as well using the following code:</p><div class="informalexample"><pre class="programlisting">(def dataset (read-dataset "resources/features.dat"))

(def model (train-model dataset))</pre></div><p>The trained SVM represented by the model variable as defined in the preceding code can now be used to predict the class of a set of input values. The <code class="literal">predict</code> function can be used for this purpose. For simplicity, we will use a sample value from the dataset variable itself as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (def feature (last (first dataset)))
#'user/feature
user&gt; feature
{1 0.708333, 2 1.0, 3 1.0, 4 -0.320755, 5 -0.105023,
 6 -1.0, 7 1.0, 8 -0.4198, 9 -1.0, 10 -0.2258, 12 1.0, 13 -1.0}
user&gt; (feature 1)
0.708333
user&gt; (predict model feature)
1.0</pre></div><p>As shown in the REPL output in the preceding code, <code class="literal">dataset</code> can be treated as a sequence of maps. <a id="id605" class="indexterm"/>Each map contains a single key that represents the value of the output variable in the sample. The value of this key in the <code class="literal">dataset</code> map is another map that represents the input variables of the given<a id="id606" class="indexterm"/> sample. Since the <code class="literal">feature</code> variable represents a map, we can call it as a function, as shown by the <code class="literal">(feature 1)</code> call in the preceding code.</p><p>The predicted value agrees with the actual value of the output variable, or the class, of a given set of input values. In conclusion, the <code class="literal">svm-clj</code> library provides us with a simple and concise implementation of an SVM.</p></div><div class="section" title="Using kernel functions"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec13"/>Using kernel functions</h2></div></div></div><p>As we have mentioned earlier,<a id="id607" class="indexterm"/> we can choose a kernel function for an SVM when we need to fit some nonlinear data. We will now demonstrate how this<a id="id608" class="indexterm"/> is achieved in practice using the <code class="literal">clj-ml</code> library. Since this library has already been discussed in the previous chapters, we will not focus on the complete training of an SVM, but rather on how we can create an SVM that uses kernel functions.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note39"/>Note</h3><p>For the example that will follow, the namespace declaration should look similar to the following declaration:</p><div class="informalexample"><pre class="programlisting">(ns my-namespace
  (:use [clj-ml classifiers kernel-functions]))</pre></div></div></div><p>The function, <code class="literal">make-kernel-function</code>, from the <code class="literal">clj-ml.kernel-functions</code> namespace is used to create kernel functions that can be used for SVMs. For example, we can create a polynomial kernel function by passing the :<code class="literal">polynomic</code> keyword to this function, as follows:</p><div class="informalexample"><pre class="programlisting">(def K (make-kernel-function :polynomic {:exponent 3}))</pre></div><p>As shown in the preceding line, the polynomial kernel function defined by the variable <code class="literal">K</code> has a polynomial degree of <code class="literal">3</code>. Similarly, we can also create a string kernel function using the <code class="literal">:string</code> keyword, as follows:</p><div class="informalexample"><pre class="programlisting">(def K (make-kernel-function :string))</pre></div><p>There are several such kernel functions available in the <code class="literal">clj-ml</code> library and the reader is encouraged to explore more kernel functions in this library. The documentation for this namespace is available at <a class="ulink" href="http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html">http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html</a>. We can create an SVM using the <code class="literal">make-classifier</code> function by specifying the <code class="literal">:support-vector-machine</code> and <code class="literal">:smo</code> keywords; and the kernel function <a id="id609" class="indexterm"/>with the keyword option <code class="literal">:kernel-function</code>, as follows:</p><div class="informalexample"><pre class="programlisting">(def classifier
  (make-classifier :support-vector-machine :smo
                   :kernel-function K))</pre></div><p>We can now train the SVM<a id="id610" class="indexterm"/> represented by the variable classifier as we have done in the previous chapters. The <code class="literal">clj-ml</code> library, thus, allows us to create SVMs that exhibit a given kernel function.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Summary</h1></div></div></div><p>In this chapter, we have explored SVMs and how they can be used to fit both linear and nonlinear data. The following are the other topics that we have covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have examined how SVMs are capable of large margin classification and the various forms of the optimization problem of SVMs</li><li class="listitem" style="list-style-type: disc">We have discussed how we can use kernel functions and SMO to train an SVM with nonlinear sample data</li><li class="listitem" style="list-style-type: disc">We have also demonstrated how we can use several Clojure libraries to build and train SVMs</li></ul></div><p>We will shift our focus to unsupervised learning in the next chapter and we will explore clustering techniques to model these types of machine learning problems.</p></div></body></html>