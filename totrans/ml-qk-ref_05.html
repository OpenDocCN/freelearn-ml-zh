<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Time Series Analysis</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will take a look at time series analysis and learn several ways of <span>observing and capturing an event at different points in time. We will introduce the concept of white noise and learn about its detection in a series. </span></p>
<p><span>We will take the time series data and compute the differences between the consecutive observations, which will lead to the formation of a new series. These concepts will help us deep dive into time series analysis and help us build a deeper understanding around it.</span></p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul>
<li class="h1">Introduction to time series analysis</li>
<li class="h1">White noise</li>
<li class="h1">Random walk</li>
<li class="h1">Autoregression</li>
<li class="h1">Autocorrelation</li>
<li class="h1">Stationarity</li>
<li class="h1">Differencing</li>
<li class="h1">AR model</li>
<li class="h1">Moving average model</li>
<li class="h1">Autoregressive integrated moving average</li>
<li class="h1">Optimization of parameters</li>
<li class="h1">Anomaly detection</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to time series analysis</h1>
                </header>
            
            <article>
                
<p>There are several occasions when we might try to observe and capture an event at different points in time. Often, we would end up drawing a correlation or association between adjacent observations that cannot be handled by an approach that deals with data that is independent and identically distributed. The approach that takes all of this into consideration in a mathematical and statistical manner is called <strong>time series analysis</strong>.</p>
<p>Time series analysis has been used in a number of fields, such as the automotive, banking, and retail industries, product development, and so on. There is no boundary for its use, and so analysts and data scientists are exploring this area to the hilt in order to derive the maximum benefit for organizations.</p>
<p>In this section, we will go through a few of the concepts around time series analysis that will lay the foundation for a deeper understanding in the future. Once we have established this foundation, we will jump into modeling.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">White noise</h1>
                </header>
            
            <article>
                
<p>A simple series with a collection of uncorrelated random variables with a mean of zero and a standard deviation of <em>σ<sup>2</sup></em> is called <strong>white noise</strong>. In this, variables are independent and identically distributed. All values have the same variance of <em><span>σ</span><sup>2</sup></em>. In this case, the series is drawn from Gaussian distribution, and is called <strong>Gaussian white noise</strong>.</p>
<p>When the series turns out to be white noise, it implies that the nature of the series is totally random and there is no association within the series. As a result, the model can't be developed, and prediction is not possible in this scenario.</p>
<p>However, when we typically build a time series model with a nonwhite noise series, we try to attain a white noise phenomenon within the residuals or errors. In simple terms, whenever we try to build a model, the motive is to extract the maximum amount of information from the series so that no more information exists in the variable. Once we build a model, noise will always be part of it. The equation is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y<sub>t </sub>= X<sub>t </sub>+ Error</em></p>
<p>So the error series should be totally random in nature, which implies that it is white noise. If we have got these errors as white noise, then we can go ahead and say that we have extracted all the information possible from the series.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detection of white noise in a series</h1>
                </header>
            
            <article>
                
<p>We can detect white noise by using the following tools:</p>
<ul>
<li><strong>Line plot</strong>: Once we have a line plot, we can have an idea of whether the series has a constant mean and variance</li>
<li><strong>Autocorrelation plot</strong>: Having a correlation plot can give us an inkling as to whether there is an association among lagged variables</li>
<li><span><strong>Summary</strong>: Checking the mean and variance of the series against the mean and variance of meaningful contiguous blocks of values in the series</span></li>
</ul>
<p>Let's do this in Python:</p>
<ol>
<li>First, we will import all the required libraries as follows:</li>
</ol>
<pre style="padding-left: 60px">from random import gauss<br/>from random import seed<br/>from pandas import Series<br/>from pandas.tools.plotting import autocorrelation_plot<br/>from matplotlib import pyplot</pre>
<ol start="2">
<li><span>Next, we will set up the white noise series for us to analyze, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">seed(1000)<br/>#creating white noise series<br/>series = [gauss(0.0, 1.0) for i in range(500)]<br/>series = Series(series)</pre>
<ol start="3">
<li>Let's take the summary or statistic of it using the following code:</li>
</ol>
<pre style="padding-left: 60px">print(series.describe())</pre>
<p style="padding-left: 60px">We will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c34dd0d9-d796-4882-816c-dee770bd9a29.png" style="width:11.17em;height:11.08em;"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">Here, we can see that the mean is approaching zero and the standard deviation is close to 1.</p>
<ol start="4">
<li>Let's make a line plot now to check out the trend, using the following code:</li>
</ol>
<pre style="padding-left: 60px">series.plot()<br/>pyplot.show()</pre>
<p style="padding-left: 60px">We will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/50887a2b-3c88-4223-9981-e0c20423e3af.png"/></p>
<p style="padding-left: 60px">The line plot looks totally random, and no trend can be observed here.</p>
<ol start="5">
<li>It's time to make an autocorrelation plot. Let's set one up using the following code:</li>
</ol>
<pre style="padding-left: 60px">autocorrelation_plot(series)<br/>pyplot.show()</pre>
<p style="padding-left: 60px">We will get the following output:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e38f049f-3af8-46b0-9f7a-97db972126ff.png"/></p>
<p>Even in an <strong>autocorrelation</strong> function plot, the correlation breaches the band of our confidence level. This tells us that it is a white noise series.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random walk</h1>
                </header>
            
            <article>
                
<p>Random walk is a time series model where the current observation is equal to the previous observations with a random modification. It can be described in the following manner:</p>
<p class="CDPAlignCenter CDPAlign"><em><span> x<sub>t</sub></span><span class="MathJax"><span class="math"><span><span class="mrow"><span class="mo">= x<sub>t-1 </sub></span></span></span></span><span class="MJX_Assistive_MathML">+ w<sub>t</sub></span></span></em></p>
<p class="CDPAlignLeft CDPAlign">In the preceding formula, <em>w<sub>t</sub></em><em> </em>is a white noise series.</p>
<p>Sometimes, we might come across a series that reflects irregular growth. In these cases, the strategy to predict the next level won't be the correct one. Rather, <span>it might be better to try to predict the change </span><span>that occurs from one period to the next—that is, it may be better to look at the first difference of the series in order to find out a significant pattern. The following figure shows a random walk pattern:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ff0118d8-02d9-447b-91bf-e85f8320c801.png" style="width:20.50em;height:23.75em;"/></p>
<p><span>In each time period, going from left to right, the value of the variable takes an independent random step up or down, which is called a <strong>random walk</strong>.</span></p>
<p>It can also be described in the following way:</p>
<p class="CDPAlignCenter CDPAlign"><em>y(t)= b<sub>o </sub>+ b<sub>1</sub>*x<sub>t-1</sub> + w<sub>t</sub></em></p>
<p class="CDPAlignLeft CDPAlign">The following list explains the preceding formula:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><em>y(t)</em>: Next value in the series</li>
<li class="CDPAlignLeft CDPAlign"><em><span>b</span><sub>o</sub></em>: Coefficient, which, if set to a number other than zero, means that the random walk comes along with a drift</li>
<li class="CDPAlignLeft CDPAlign"><em><span>b</span><sub>1</sub></em>: Coefficient, which is set to 1</li>
<li class="CDPAlignLeft CDPAlign"><em>w<sub>t</sub></em><strong>:</strong> White noise</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoregression</h1>
                </header>
            
            <article>
                
<p>An autoregression is a time series model that typically uses the previous values of the same series as an explanatory factor for the regression in order to predict the next value. Let's say that we have measured and kept track of a metric over time, called <em>y<sub>t</sub></em>, which is measured at time <em>t</em> w<span>hen this value is regressed on previous values from that same time series. For example, <em>y<sub>t</sub></em></span><span><em> </em>on </span><em><span class="mjx-chtml MathJax_CHTML"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">y<sub>t-1</sub></span></span></span></span></span></span></span></em><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1724339d-2858-4f5d-a54f-5a0f39cad697.png" style="width:10.67em;height:1.25em;"/></p>
<p>As shown in the preceding equation, the previous value <em>y<sub>t-1</sub></em> has become the predictor here and <em>y<sub>t</sub></em> is the response value that is to be predicted. Also, <em>ε<sub>t</sub></em> is normally distributed with a mean of zero and variance of 1. The order of the autoregression model is defined by the number of previous values that are being used by the model to determine the next value. Therefore, the preceding equation is a first-order autoregression, or <strong>AR(1)</strong>. If we have to generalize it, a <span class="mjx-chtml MathJax_CHTML"><span class="MJX_Assistive_MathML"><em>k</em><sup><em>th</em> </sup></span></span>order autoregression, written as <strong>AR(k)</strong>, is a multiple linear regression in which the value of the series at any time (<em>t</em>) is a (linear) function of the values at times <span class="mjx-chtml MathJax_CHTML"><span class="mjx-math"><span class="mjx-mrow"><em><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">−</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">1</span></span></em><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">, </span></span><em><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">−</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">2</span></span></em><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">, </span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">…</span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">,<em> </em></span></span></span></span><em><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">−</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">k.</span></span></span></span></em></span></p>
<p>The following list shows what the following values means for an AR(1) model:</p>
<ul>
<li>When <em>β<sub>1 </sub>= 0</em>,<span> </span><span class="math inline"><span class="mjx-chtml MathJax_CHTML"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I"><em>y</em><em>t</em>, it<sub> </sub></span></span></span></span></span></span></span></span>is equivalent to white noise</li>
<li><span>When</span> <em><span>β<sub>1 </sub>= 1</span></em><span> </span>and <em><span>β<sub>0</sub>= 0</span></em>, <em><span>y<sub>t</sub></span></em><span>, it </span>is equivalent to a random walk</li>
<li><span>When</span> <span><em>β<sub>1 </sub>= 1</em> and <em>β<sub>0 </sub>≠ 0</em>, <em>y<sub>t</sub></em></span>, it is equivalent to a random walk with drift</li>
<li><span>When</span> <em><span>β<sub>1 </sub>&lt; 1</span></em>, <em><span>y<sub>t</sub></span></em><span>, it </span>tends to oscillate between positive and negative values</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autocorrelation</h1>
                </header>
            
            <article>
                
<p>Autocorrelation is a measure of the correlation between the lagged values of a time series. For example, <em>r<sub>1</sub></em> is the autocorrelation between <em><span>y</span><sub>t </sub></em>and <em><span>y</span><sub>t-1;</sub></em> similarly, <em>r<sub>2 </sub></em>is the autocorrelation between <em><span>y</span><sub>t </sub></em><span>and </span><em><span>y</span><sub>t-2</sub></em>. This can be summarized in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a10d9b15-19a3-45ce-bb47-e2c67c544a52.png" style="width:14.83em;height:3.42em;"/></p>
<p>In the preceding formula, <em>T</em> is the length of the time series.</p>
<p class="mce-root"/>
<p>For example, say that we have the correlation coefficients, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/46e209e2-3a8f-4332-a0e4-6df905f03963.png"/></p>
<p>To plot it, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ae116bfd-56ab-48d8-bc93-ff04987c4eda.png" style="width:32.58em;height:10.00em;"/></p>
<p>The following are some observations from this autocorrelation function plot:</p>
<ul>
<li><strong>r<sub>4</sub></strong> is higher than other lags, which is mostly because of a seasonal pattern</li>
<li>The blue lines are the indicators of whether correlations are significantly different from zero</li>
<li>Autocorrelation at lag 0 is always 1</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stationarity</h1>
                </header>
            
            <article>
                
<p>A common assumption for a few of the time series models is that data has to be stationary. Let's look at what stationarity means regarding time series.</p>
<p>A stationary process is one for which the mean, variance, and autocorrelation structure doesn't change over time. What this means is that the data doesn't have a trend (increasing or decreasing).</p>
<p>We can describe this by using the following formulas:</p>
<p class="CDPAlignCenter CDPAlign"><em>E(x<sub>t</sub>)= μ</em>, for all <em>t</em></p>
<p class="CDPAlignCenter CDPAlign"><em>E(x<sub>t</sub><sup>2</sup>)= σ</em><sup><em>2</em></sup>, for all <em>t</em></p>
<p class="CDPAlignCenter CDPAlign"><em>cov(x<sub>t</sub>,x<sub>k</sub>)= cov(x<sub>t+s</sub>, x<sub>k+s</sub>)</em>, for all <em>t</em>, <em>k</em>, and s</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detection of stationarity</h1>
                </header>
            
            <article>
                
<p>There are multiple methods that can help us in figuring out whether the data is stationary, listed as follows:</p>
<ul>
<li><strong>Plotting the data</strong>: Having a plot of the data with respect to the time variable can help us to see whether it has got a trend. We know from the definition of stationarity that a trend in the data means that there is no constant mean and variance. Let's do this in Python. For this example, we are using international airline passenger data.</li>
</ul>
<p style="padding-left: 60px">First, let's load all the required libraries, as follows:</p>
<pre style="padding-left: 60px"> <span>from pandas import Series</span><br/><span>from matplotlib import pyplot</span><br/><span>%matplotlib inline<br/><br/></span>data = Series.from_csv('AirPassengers.csv', header=0)<br/>  series.plot()<br/>  pyplot.show()</pre>
<p style="padding-left: 60px">We will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-978 image-border" src="assets/25b20f75-7749-4a04-b737-86f0c5e65b7e.png" style="width:31.50em;height:21.33em;"/></p>
<p style="padding-left: 60px">It is quite clear from the plot that there is an increasing trend here and that it would vindicate our hypothesis that it is a non-stationary series.</p>
<p class="mce-root"/>
<ul>
<li><strong>Dividing the data set and computing the summary</strong>: The next method would be to divide the data series into two parts and compute the mean and variance. By doing this, we will be able to figure out whether the mean and variance are constant. Let's do this by using the following code:</li>
</ul>
<pre style="padding-left: 60px">X = data.values<br/>partition =int(len(X) / 2)<br/>X1, X2 = X[0:partition], X[partition:]<br/>mean1, mean2 =np.nanmean(X1),np.nanmean(X2)<br/>var1, var2 = np.nanvar(X1), np.nanvar(X2)<br/>print('mean1=%f, mean2=%f' % (mean1, mean2))<br/>print('variance1=%f, variance2=%f' % (var1, var2))</pre>
<p style="padding-left: 60px">The output is as follows:</p>
<pre style="padding-left: 60px">mean1=182.902778, mean2=377.694444 variance1=2244.087770, variance2=7367.962191</pre>
<p style="padding-left: 60px">We can see that the mean and variance of series 1 and series 2 are not equal, and so we can conclude that the series is not stationary.</p>
<ul>
<li><strong>Augmented Dickey-Fuller test</strong>: The augmented Dickey-Fuller test is a statistical test that tends to give an indication with a certain level of confidence as to whether the series is stationary. A statistical test takes the data and tests our hypothesis about the data using its assumption and process. Eventually, it yields the result with a certain degree of confidence, which helps us in taking the decision.</li>
</ul>
<p style="padding-left: 60px">This test is nothing but the unit root test, which tries to find out whether the time series is influenced by the trend. It makes use of the <strong>autoregressive</strong> (<strong>AR</strong>) model and optimizes the information criterion at different lag values.</p>
<p>Here, the null hypothesis is as follows:</p>
<ul>
<li><strong><em>H<sub>o</sub></em></strong>: The time series has got the unit root, which implies that the series is nonstationary</li>
</ul>
<p style="padding-left: 60px">The alternate hypothesis is as follows:</p>
<ul>
<li><strong><em>H<sub>1</sub></em></strong>: The time series doesn't have a unit root and, as such, it is stationary</li>
</ul>
<p class="mce-root"/>
<p>As we know from the rules of hypothesis testing, if we have chosen a significance level of 5% for the test, then the result would be interpreted as follows:</p>
<p class="CDPAlignLeft CDPAlign">If <em>p-value &gt;0.05</em> =&gt;, then we fail to reject the null hypothesis. That is, the series is nonstationary.</p>
<p class="CDPAlignLeft CDPAlign">If <em>p-value &lt;0.05</em> =&gt;, then the null hypothesis is rejected which means that the series is stationary.</p>
<p>Let's perform this in Python:</p>
<ol>
<li>First, we will load the libraries, as follows:</li>
</ol>
<pre style="padding-left: 60px">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pylab as plt<br/>%matplotlib inline<br/>from matplotlib.pylab import rcParams<br/>rcParams['figure.figsize'] = 25, 6</pre>
<ol start="2">
<li>Next, we load the data and time plot as follows:</li>
</ol>
<pre style="padding-left: 60px">data = pd.read_csv('AirPassengers.csv')<br/>print(data.head())<br/>print('\n Data Types:')<br/>print(data.dtypes)</pre>
<p style="padding-left: 60px">The output can be seen in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a4fd26e4-ca81-4934-9e9c-41d4270d9dc6.png" style="width:13.25em;height:12.50em;"/></p>
<ol start="3">
<li>We then parse the data as follows:</li>
</ol>
<pre style="padding-left: 60px">dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')<br/>data = pd.read_csv('./data/AirPassengers.csv', parse_dates=['Month'], index_col='Month',date_parser=dateparse)<br/>print(data.head())</pre>
<p style="padding-left: 60px">We then get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/981dfc7c-4087-4ad9-adef-0e20d45ce0ff.png" style="width:13.00em;height:8.83em;"/></p>
<pre style="padding-left: 60px">ts= data["#Passengers"]<br/>ts.head()</pre>
<p style="padding-left: 60px">From this, we get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c73847fc-b69f-407c-89f7-00a57ed3637d.png" style="width:17.17em;height:8.33em;"/></p>
<ol start="4">
<li>Then we plot the graph, as follows:</li>
</ol>
<pre style="padding-left: 60px">plt.plot(ts)</pre>
<p style="padding-left: 60px">The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1013 image-border" src="assets/ab3a758d-f7b4-49a8-a902-8698cdd85b52.png" style="width:119.83em;height:30.08em;"/></p>
<ol start="5">
<li><span><span>Let's create a function to perform a stationarity test using the following code:</span></span></li>
</ol>
<pre style="padding-left: 60px">from statsmodels.tsa.stattools import adfuller<br/>def stationarity_test(timeseries):<br/> dftest = adfuller(timeseries, autolag='AIC')<br/> dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])<br/> for key,value in dftest[4].items():<br/>               dfoutput['Critical Value (%s)'%key] = value<br/> print(dfoutput)<br/><br/>stationarity_test(ts)</pre>
<p style="padding-left: 60px">The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5eac6c3a-a6f6-4d71-b144-8c49b23ac8ba.png" style="width:21.17em;height:9.50em;"/></p>
<p>Since <em>p-value &gt; 0.05</em> and the <em>t</em>-statistic is greater than all the critical values (1%,5%,10%), <em>tt</em> implies that the series is nonstationary as we failed to reject the null hypothesis.</p>
<p>So what can be done if the data is nonstationary? We use differencing to make the nonstationary data into stationary data.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AR model</h1>
                </header>
            
            <article>
                
<p>An AR model is a part of the stochastic process, wherein specific lagged values of <em>y<sub>t</sub></em> are used as predictor variables and regressed on <em>y<sub>t </sub></em>in order to estimate its values. Lagged values are values of the series of the previous period that tend to have an impact on the current value of the series. Let's look at an example. Say we have to assess and predict tomorrow's weather. We would start by thinking of what today's weather is and what yesterday's weather was, as this will help us in predicting whether it will be rainy, bright and sunny, or cloudy. Subconsciously, we are also cognizant of the fact that the weather of the previous day might have an association with today's weather. This is what we call an <strong>AR model</strong>.</p>
<p>This has a degree of uncertainty that results in less accuracy in the prediction of future values. The formula is the same as the formula for a series with <em>p</em> lag, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2a650119-f373-4979-bbe5-e04705f4def8.png" style="width:18.42em;height:1.33em;"/></p>
<p>In the previous equation,<em> ω</em> is the white noise term and <em>α</em> is the coefficient, which can't be zero. The aggregated equation appears as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/32d5bfdb-35ea-4877-a922-ed08417f2ab5.png" style="width:9.42em;height:3.25em;"/></p>
<p><span>Occasionally, we might talk about the order of a model. For example, we might describe an AR model as being of order <em>p</em></span><span>. In this case,</span><strong> </strong><span>the <em>p</em> represents the number of lagged variables used within the model. For example, an AR(2) model or </span>second-order<em><span> </span></em><span>AR model looks like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/de8924d2-fec8-4208-99eb-c3ee056e35a1.png" style="width:15.42em;height:1.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Moving average model</h1>
                </header>
            
            <article>
                
<p>A <strong>moving average model</strong> (<strong>MA</strong>) is a linear combination of historic white noise error terms. Let's have a look at the equation of the model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/69f46d0d-cecc-4cfe-9d4e-337c2cb62b95.png" style="width:17.50em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/399ae830-4338-4610-8ede-adeabdc9eb61.png" style="width:11.25em;height:3.17em;"/></p>
<p>Here, <em>ω</em> is the white noise with <em>E(ω<sub>t</sub>)=0</em> and variance =<em> σ<sup>2</sup></em>. </p>
<p>In order to find out the order of the AR model, we need to plot a partial autocorrelation function plot, and then look for the lag where the upper confidence level has been crossed for the first time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoregressive integrated moving average</h1>
                </header>
            
            <article>
                
<p>An <strong>autoregressive integrated moving average</strong> (<strong>ARIMA</strong>) model is a combination of the following elements:</p>
<ul>
<li><strong>Autoregressive operator</strong>: We have already learned what this means; just to reiterate, it is the lags of the stationarized series. It is denoted by <em>p</em>, which is nothing but the number of autoregressive terms. The PACF plot yields this component.</li>
<li><strong>Integration operator</strong>: A<span> series that needs to be differenced to be made stationary is said to be an integrated version of a stationary series. It is denoted by <em>d</em>, which is the amount of differencing that is needed to transform the nonstationary time series into a stationary one. This is done by subtracting the observation from the current period from the previous one. If this has been done only once to the series, it is called <strong>first differenced</strong>. This process eliminates the trend out of the series that is growing at a constant rate. In this case, the series is growing at an increasing rate, and the differenced series needs another round of differencing, which is called <strong>second differencing</strong>.</span></li>
<li><strong>Moving average operator</strong>: The lags of the forecasted errors, which is denoted by <em>q</em>. It is the number of lagged forecast errors in the equation. The ACF plot would yield this component.</li>
</ul>
<p>The ARIMA model can only be applied on stationary series. Therefore, before applying it, the stationarity condition has to be checked in the series. The ADF test can be performed to establish this.</p>
<p>The equation of ARIMA turns looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/627d8248-1c5b-4643-85e4-01d8b54e1444.png" style="width:29.92em;height:1.42em;"/></p>
<p>The first part of the equation (before the <em>-</em> sign) is the autoregressive section, and the second part (after the <em>-</em> sign)is the MA section. </p>
<p>We can go ahead and add a seasonal component in ARIMA as well, which would be ARIMA <em>(p,d,q)(p,d,q)<sub>s</sub></em>. While adding it, we need to perform seasonal differencing, which means subtracting the current observation from the seasonal lag.</p>
<p>Let's plot ACF and PACF in order to find out the <em>p</em> and <em>q </em><span>parameters</span><span>.</span></p>
<p>Here, we take the number of lags as 20 and use the <kbd>statsmodel.tsa.stattools </kbd><span>library</span><span> </span><span>to import the </span><kbd>acf</kbd> <span>and</span> <kbd>pacf</kbd> <span>functions, as follows:</span></p>
<pre>from statsmodels.tsa.stattools import acf,pacf<br/>lag_acf= acf(ts_log_dif,nlags=20)<br/>lag_pacf = pacf(ts_log_dif, nlags=20,method="ols")</pre>
<p>Now we will plot with the help of <kbd>matplotlib</kbd> using the following code:</p>
<pre class="mce-root"><br/>plt.subplot(121) <br/>plt.plot(lag_acf)<br/>plt.axhline(y=0,linestyle='--',color='gray')<br/>plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')<br/>plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')<br/>plt.title('Autocorrelation Function')</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-977 image-border" src="assets/daafb80f-b167-4394-8fc0-c8668dffea4d.png" style="width:40.50em;height:22.17em;"/></p>
<p>Here, we are <span>measuring the correlation between the time series with a lagged version of itself. For instance, at lag 5, ACF would compare the series at time instant <em>t1</em>, <em>t2</em> with the series at instant <em>t<sub>1-5</sub></em>, <em>…</em>, <em>t<sub>2-5</sub></em>. It is a plot of the coefficients of the correlation with its lagged values.</span></p>
<p><span>If we look at the preceding plot carefully, we will see that the upper confidence level line has been crossed at lag 2. Therefore, the order of MA would be <em>2</em> and <em>q=2</em>.</span></p>
<p>A partial correlation between the series and lagged values is plotted, and it gives us a <strong>partial auto correlation functional</strong> (<strong>PACF</strong>) <span>plot</span>. It's a very interesting term. If we go on and compute the correlation between a <em>Y </em>variable and <em>X</em>3 while we know that <em>Y</em> has a separation association with <em>X</em>1 and <em>X</em>2, the partial correlation addresses that portion of the correlation that is not explained by their correlations with <em>X</em>1 and <em>X</em>2.</p>
<p>Here, the partial correlation is the square root (reduction in variance by adding a variable (here, <em>X</em>3) while regressing <em>Y</em> on the other variables (here <em>X</em>1, <em>X</em>2)).</p>
<p>In the case of a time series, partial autocorrelation between Y &amp; lagged value Y<sub>t-3</sub> will be the value that is not explained by a correlation between <em>Y</em> and <em>Y</em><sub><em>t</em>-1</sub> and <em>Y</em><sub><em>t</em>-2</sub>, as shown in the following code:</p>
<pre>#Plot PACF:<br/>plt.subplot(122)<br/>plt.plot(lag_pacf)<br/>plt.axhline(y=0,linestyle='--',color='gray')<br/>plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')<br/>plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')<br/>plt.title('Partial Autocorrelation Function')<br/>plt.tight_layout()</pre>
<p>We will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3736fa84-2505-4317-a0b0-04f9398d23e1.png" style="width:49.00em;height:22.67em;"/></p>
<p>If we look at the preceding plot carefully, we will see that the upper confidence level line has been crossed at lag 2. Therefore, the order of AR would be 2 and <em>p=2</em>.</p>
<p>Let's try out an AR model that is of the order <em>(p=2, d=1, q=0)</em>. The <em>d</em> value has been taken as 1, since it is a case of single differencing. The residual sum of the square has been calculated as well to judge how good the model is and compare it with others, as shown in the following code:</p>
<pre>from statsmodels.tsa.arima_model import ARIMA<br/>model1 = ARIMA(ts_log, order=(2, 1, 0)) <br/>results_AR = model1.fit(disp=-1) <br/>plt.plot(ts_log_dif)<br/>plt.plot(results_AR.fittedvalues, color='red')<br/>plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_dif)**2))</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6302a46b-427d-4c32-9ccd-cd4093856311.png"/></p>
<p>Now, we can have a look at the model summary that depicts the coefficients of AR1 and AR2 using the following code:</p>
<pre>results_AR.summary()</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8386ccb1-d258-419f-bfc0-97f4a536f099.png" style="width:29.58em;height:24.83em;"/></p>
<p>Now, let's build an MA model of the order <em>(p=0,d=1,q=2)</em> using the following code:</p>
<pre>model2 = ARIMA(ts_log, order=(0, 1, 2)) <br/>results_MA = model2.fit(disp=-1) <br/>plt.plot(ts_log_dif)<br/>plt.plot(results_MA.fittedvalues, color='red')<br/>plt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_dif)**2))</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58645090-67c5-41f9-8af7-fa7641911d6b.jpg"/></p>
<p>Now, let's combine these two models and build an ARIMA model using the following code:</p>
<pre>model3 = ARIMA(ts_log, order=(2, 1, 2))  
results_ARIMA = model.fit(disp=-1)  
plt.plot(ts_log_dif)
plt.plot(results_ARIMA.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_dif)**2))</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/74a2b07b-767b-4f40-93b7-80652cb8c0dc.png"/></p>
<p>We can experience a dip in the value of RSS from the AR model to ARIMA. Now <strong>RSS= 1.0292</strong>:</p>
<pre>results_ARIMA.summary()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can see the coefficients of AR1, AR2, MA1, and MA2, and, if we go by <em>p </em>values, we can see that all these parameters are significant, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7ef067a9-23ce-4363-9c3d-23505461dc85.png"/></p>
<p>Let's turn the predicted values into a series using the following code:</p>
<pre>predictions_ARIMA_dif= pd.Series(results_ARIMA.fittedvalues, copy=True)<br/>print(predictions_ARIMA_dif.head())</pre>
<p>We will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4798ec69-cfe0-4653-bcdb-0f09b00e7917.png" style="width:10.75em;height:7.75em;"/></p>
<p><span>The way to convert the differencing to log scale is to add these differences consecutively to the base number. An easy way to do this is to first determine the cumulative sum at the index and then add it to the base number. The cumulative sum can be found using the following code:</span></p>
<pre>predictions_ARIMA_dif_cumsum = predictions_ARIMA_dif.cumsum()<br/>print(predictions_ARIMA_dif_cumsum.head())</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>From this, we will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44bc962a-af3f-4b2f-beaa-329bc598f642.png" style="width:11.33em;height:7.92em;"/></p>
<p><span>We will create a series with all values as the base number and add the differences to it in order to add to the base series, as follows:</span></p>
<pre>predictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)<br/>predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_dif_cumsum,fill_value=0)<br/>predictions_ARIMA_log.head()</pre>
<p>The following shows the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4362a7c6-a3ea-48c2-a6bb-82810c433dbb.png" style="width:11.83em;height:7.92em;"/></p>
<p><span><span>Let's now find out the forecast using the following code:</span></span></p>
<pre>predictions_ARIMA = np.exp(predictions_ARIMA_log)<br/>plt.plot(ts)<br/>plt.plot(predictions_ARIMA)<br/>plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)/len(ts)))</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1016 image-border" src="assets/b958ef4c-0375-49af-8a7e-6d8dcc293d1d.png" style="width:119.83em;height:31.08em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization of parameters</h1>
                </header>
            
            <article>
                
<p>Let's look at how to optimize the parameters of the models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AR model</h1>
                </header>
            
            <article>
                
<pre>import statsmodels.tsa.api as smtsa<br/>aic=[] <br/>for ari in range(1, 3): <br/> obj_arima = smtsa.ARIMA(ts_log_diff, order=(ari,2,0)).fit(maxlag=30, method='mle', trend='nc') <br/> aic.append([ari,2,0, obj_arima.aic])<br/>print(aic)</pre>
<pre>[[1, 2, 0, -76.46506473849644], [2, 2, 0, -116.1112196485397]]</pre>
<p>Therefore, our model parameters are <kbd>p=2</kbd>, <kbd>d=2</kbd>, and <kbd>q=0</kbd> in this scenario for the AR model, as the AIC for this combination is the least.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ARIMA model</h1>
                </header>
            
            <article>
                
<p>Even for the ARIMA model, we can optimize the parameters by using the following code:</p>
<pre>import statsmodels.tsa.api as smtsa<br/>aic=[] <br/>for ari in range(1, 3): <br/>     for maj in range(1,3): <br/>        arima_obj = smtsa.ARIMA(ts_log, order=(ari,1,maj)).fit(maxlag=30, method='mle', trend='nc') <br/>        aic.append([ari,1, maj, arima_obj.aic])<br/>print(aic)</pre>
<p>The following is the output you get by executing the preceding code:</p>
<pre>[[1, 1, 1, -242.6262079840165], [1, 1, 2, -248.8648292320533], [2, 1, 1, -251.46351037666676], [2, 1, 2, -279.96951163008583]]</pre>
<p>The combination with the least <strong>Akaike information criterion</strong> (<span><strong>AIC</strong>)</span> should be chosen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Anomaly detection</h1>
                </header>
            
            <article>
                
<p>Anomalies are essentially abnormal patterns in a series that are irregular deviations from the expected behavior. For example, many of us have watched a cricket match. One form of getting out in this game is to be caught out, and before the ball travels straight to the hands of a fielder, it has to touch the bat of a batsman. If the stadium is very noisy, sometimes it is too difficult for anyone to judge whether the ball has touched the bat or not. To solve this problem, umpires use a device called the <strong>snickometer</strong> to help them make the call. The snickometer uses the sound from the stump mic to generate a plot of the mic's sound. If the plot is a straight line, then the ball did not make contact with the bat; otherwise, the plot will show a spike. Therefore, a spike is a sign of an anomaly. Another example of an anomaly could be the detection of a malignant tumor in a scan.</p>
<p>Anomaly detection is a technique that we can use to figure out aberrant behavior. An anomaly can also be called an <strong>outlier</strong>. The following list shows several different anomalies:</p>
<ul>
<li><strong>Point anomalies</strong>: A point anomaly is a point that breaches the boundary of a threshold that has been assigned to keep the whole system in check. There is often a system in place to send an alert when this boundary has been breached by a point anomaly. For example, fraud detection in the financial industries can use point anomaly detection to check whether a transaction has taken place from a different city to the card holder's usual location.</li>
<li><strong>Contextual anomalies</strong>: Context-specific observations are called <strong>contextual anomalies</strong>. For example, it is commonplace to have lots of traffic on weekdays, but a holiday falling on a Monday may make it look like an anomaly.</li>
<li><strong>Collective anomalies</strong>: A set of collective data instances helps in detecting anomalies. Say that someone is unexpectedly trying to copy data form a remote machine to a local host. In this case, this anomaly would be flagged as a potential cyber attack.</li>
</ul>
<p>In this section, we will focus on contextual anomalies and try to detect them with the help of a simple moving average.</p>
<p>First, let's load all the required libraries as follows:</p>
<pre>import numpy as np # vectors and matrices<br/>import pandas as pd # tables and data manipulations<br/>import matplotlib.pyplot as plt # plots<br/>import seaborn as sns # more plots<br/>from sklearn.metrics import mean_absolute_error<br/>import warnings # `do not disturb` mode<br/>warnings.filterwarnings('ignore')<br/>%matplotlib inline</pre>
<p>Next, we read the dataset using the following code. We are keeping the same dataset—namely, <kbd>AirPassenger.csv</kbd>:</p>
<pre>data = pd.read_csv('AirPassengers.csv', index_col=['Month'], parse_dates=['Month'])<br/> plt.figure(figsize=(20, 10))<br/> plt.plot(ads)<br/> plt.title('Trend')<br/> plt.grid(True)<br/> plt.show()</pre>
<p>We get the output as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1017 image-border" src="assets/f9d68dc2-afa6-4892-a030-d439b726a98c.png" style="width:96.67em;height:49.17em;"/></p>
<p>Now we will write a function and create a threshold for detecting the anomalies using the following code:</p>
<pre>def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):<br/>       rolling_mean = series.rolling(window=window).mean()<br/>       plt.figure(figsize=(15,5))<br/>       plt.title("Moving average\n window size = {}".format(window))<br/>       plt.plot(rolling_mean, "g", label="Rolling mean trend")<br/>      # Plot confidence intervals for smoothed values<br/>      if plot_intervals:<br/>          mae = mean_absolute_error(series[window:], rolling_mean[window:])<br/>          deviation = np.std(series[window:] - rolling_mean[window:])<br/>          lower_bond = rolling_mean - (mae + scale * deviation)<br/>          upper_bond = rolling_mean + (mae + scale * deviation)<br/>          plt.plot(upper_bond, "r--", label="Upper Bond / Lower Bond")<br/>          plt.plot(lower_bond, "r--") <br/> # Having the intervals, find abnormal values<br/>      if plot_anomalies:<br/>         anomalies = pd.DataFrame(index=series.index, columns=series.columns)<br/>         anomalies[series&lt;lower_bond] = series[series&lt;lower_bond]<br/>         anomalies[series&gt;upper_bond] = series[series&gt;upper_bond]<br/>         plt.plot(anomalies, "ro", markersize=10) <br/>         plt.plot(series[window:], label="Actual values")<br/>         plt.legend(loc="upper left")<br/>         plt.grid(True)</pre>
<p>Now, let's introduce anomalies to the series using the following:</p>
<pre class="mce-root">data_anomaly = data.copy()<br/>data_anomaly.iloc[-20] = data_anomaly.iloc[-20] * 0.2</pre>
<p>Now, let's plot it to detect the anomalies <span>introduced </span>using the following code:</p>
<pre>plotMovingAverage(data_anomaly, 4, plot_intervals=True, plot_anomalies=True)</pre>
<p>The following diagram shows the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1018 image-border" src="assets/7890f46f-ae72-4d05-99ed-1ff3692503b3.png" style="width:73.42em;height:27.83em;"/></p>
<p>Now, the introduced anomaly can be seen after 1959 as a dip in the number of travelers. It should be noted, however, that this is one of the simpler methods. ARIMA and Holt-Winters can also be used in this scenario.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about time series analysis and white noise. We were introduced to the concepts of random walk, autoregression, autocorrelation, and stationarity, which describes how to <span>figure out whether data is stationary</span>. </p>
<p>We also learned about differencing, taking the time series data and computing the differences between consecutive observations that lead to the formation of a new series. This chapter also talked about the AR model, which is <span>a part of a stochastic process wherein the specific lagged values of <em>y</em></span><em><sub>t</sub></em><span><em> </em>are used as</span> <span>predictor variables and regressed on <em>y</em></span><em><sub>t </sub></em><span>in order to </span><span>estimate the values. We also learned two optimization parameters, namely the AR model and ARIMA model</span><span>. </span></p>
<p>In the next chapter, we will learn about <span>natural language processing.</span></p>


            </article>

            
        </section>
    </body></html>