<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Creating Ensembles and Multiclass Classification</h1>
            </header>

            <article>
                
<div class="packt_quote"><span>"This is how you win ML competitions: you take other people's work and ensemble them together."<br/></span><span>                                                                                        - Vitaly Kuznetsov, NIPS2014</span></div>
<p>You may have already realized that we have discussed ensemble learning. It is defined by <a href="http://www.scholarpedia.org/article/Main_Page" target="_blank">www.scholarpedia.org</a> as "the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem". In random forest and gradient boosting, we combined the "votes" of hundreds or thousands of trees to make a prediction. Thus, by definition, those models are ensembles. This methodology can be extended to any learner to create ensembles, which some refer to as meta-ensembles or meta-learners. We will look at one of these methods referred to as "stacking". In this methodology, we will produce a number of classifiers and use their predicted class probabilities as input features to another classifier. This method <em>can</em> result in improved predictive accuracy. In the previous chapters, we focused on classification problems focused on binary outcomes. We will now look at methods to predict those situations where the data consists of more than two outcomes, a very common situation in real-world data sets. I have to confess that the application of these methods in R is some of the most interesting and enjoyable applications I have come across.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Ensembles</h1>
            </header>

            <article>
                
<p>The quote at the beginning of this chapter mentions using ensembles to win machine learning competitions. However, they do have practical applications. I've provided a definition of what ensemble modeling is, but why does it work? To demonstrate this, I've co-opted an example, from the following blog, which goes into depth at a number of ensemble methods:<br/>
<a href="http://mlwave.com/kaggle-ensembling-guide/">http://mlwave.com/kaggle-ensembling-guide/</a>.</p>
<p>As I write this chapter, we are only a couple of days away from Super Bowl 51, the Atlanta Falcons versus the New England Patriots. Let's say we want to review our probability of winning a friendly wager where we want to take the Patriots minus the points (3 points as of this writing). Assume that we have been following three expert prognosticators that all have the same probability of predicting that the Patriots will cover the spread (60%). Now, if we favor any one of the so-called experts, it is clear we have a 60% chance to win. However, let's see what creating an ensemble of their predictions can do to increase our chances of profiting and humiliating friends and family.</p>
<p>Start by calculating the probability of each possible outcome for the experts picking New England. If all three pick New England, we have 0.6 x 0.6 x 0.6, or a 21.6% chance, that all three are correct. If any two of the three pick New England then we have (0.6 x 0.6 x 0.3) x 3 for a total of 43.2%. By using majority voting, if at least two of the three pick New England, then our probability of winning becomes almost 65%. </p>
<p>This is a rather simplistic example but representative nonetheless. In machine learning, it can manifest itself by incorporating the predictions from several average or even weak learners to improve overall accuracy. The diagram that follows shows how this can be accomplished:</p>
<div class="CDPAlignCenter CDPAlign"><img height="156" width="414" class="image-border" src="assets/image_ens_01.png"/></div>
<p>In this graphic, we build three different classifiers and use their predicted probabilities as inputs to a fourth and different classifier in order to make predictions on the test data. Let's see how to apply this with R.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business and data understanding</h1>
            </header>

            <article>
                
<p>We are are going to visit our old nemesis the Pima Diabetes data once again. It has proved to be quite a challenge with most classifiers producing accuracy rates in the mid-70s. We've looked at this data in <a href="a7511867-5362-4215-a7dd-bbdc162740d1.xhtml" target="_blank">Chapter 5</a>, <em>More Classification Techniques - K-Nearest Neighbors and Support Vector Machines </em>and <a href="4dd3d3b8-f4cc-4ddd-b062-34d9684311cd.xhtml" target="_blank">Chapter 6</a>, <em>Classification and Regression Trees</em> so we can skip over the details. There are a number of R packages to build ensembles, and it is not that difficult to build your own code. In this iteration, we are going to attack the problem with the <kbd>caret</kbd> and <kbd>caretEnsemble</kbd> packages.  Let's get the packages loaded and the data prepared, including creating the train and test sets using the <kbd>createDataPartition()</kbd> function from caret:</p>
<pre>
<strong>    &gt; library(MASS)</strong><br/><br/><strong>    </strong><strong>&gt; library(caretEnsemble)</strong><br/><br/><strong>    &gt; library(caTools)</strong><br/><br/><strong>    &gt; pima &lt;- rbind(Pima.tr, Pima.te)</strong><br/><br/><strong>    &gt; set.seed(502)</strong><br/><br/><strong>    &gt; split &lt;- createDataPartition(y = pima$type, p = 0.75, list = F)</strong><br/><br/><strong>    &gt; train &lt;- pima[split, ]</strong><br/><br/><strong>    &gt; test &lt;- pima[-split, ]</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling evaluation and selection</h1>
            </header>

            <article>
                
<p>As we've done in prior chapters, the first recommended task when utilizing caret functions is to build the object that specifies how model training is going to happen. This is done with the <kbd>trainControl()</kbd> function. We are going to create a five-fold cross-validation and save the final predictions (the probabilities). It is recommended that you also index the resamples so that each base model trains on the same folds. Also, notice in the function that I specified upsampling. Why? Well, notice that the ratio of "Yes" versus "No" is 2 to 1:</p>
<pre>
<strong>    &gt; table(train$type)</strong><br/><br/><strong>     No Yes </strong><br/><strong>    267 133</strong>
</pre>
<p>This ratio is not necessarily imbalanced, but I want to demonstrate something here. In many data sets, the outcome of interest is a rare event. As such, you can end up with a classifier that is highly accurate but does a horrible job at predicting the outcome of interest, which is to say it doesn't predict any true positives. To balance the response, you can upsample the minority class, downsample the majority class, or create "synthetic data". We will focus on synthetic data in the next exercise, but here, let's try upsampling. In upsampling, for each cross-validation fold, the minority class is randomly sampled with replacement to match the number of observations in the majority class. Here is our function:</p>
<pre>
<strong>&gt; control &lt;- trainControl(method = "cv",</strong><br/><strong>  number = 5,</strong><br/><strong>  savePredictions = "final",</strong><br/><strong>  classProbs = T,</strong><br/><strong>  index=createResample(train$type, 5),</strong><br/><strong>  sampling = "up",</strong><br/><strong>  summaryFunction = twoClassSummary)</strong>
</pre>
<p>We can now train our models using the <kbd>caretList()</kbd> function. You can use any model in the function that is supported by the caret package. A list of models is available with their corresponding hyperparameters here:</p>
<p><a href="https://rdrr.io/cran/caret/man/models.html">https://rdrr.io/cran/caret/man/models.html</a></p>
<p>In this example, we will train three models: </p>
<ul>
<li>Classification tree - <kbd>"rpart"</kbd></li>
<li>Multivariate Adaptive Regression Splines - <kbd>"earth"</kbd></li>
<li>K-Nearest Neighbors - <kbd>"knn"</kbd></li>
</ul>
<p>Let's put this all together:</p>
<pre>
<strong>    &gt; set.seed(2)<br/></strong><br/><strong>    &gt; models &lt;- caretList(</strong><br/><strong>      type ~ ., data = train,</strong><br/><strong>      trControl = control,</strong><br/><strong>      metric = "ROC",</strong><br/><strong>      methodList = c("rpart", "earth", "knn")<br/></strong><strong>      )</strong>
</pre>
<p>Not only are the models built, but the parameters for each model are tuned according to caret's rules. You can create your own tune grids for each model by incorporating the <kbd>caretModelSpec()</kbd> function, but for demonstration purposes, we will let the function do that for us. You can examine the results by calling the model object. This is the abbreviated output:</p>
<pre>
<strong>    &gt; models</strong><br/><strong>    ...</strong><br/><strong>    Resampling results across tuning parameters:</strong><br/><br/><strong>            cp       ROC      Sens      Spec </strong><br/><strong>    0.03007519 0.7882347 0.8190343 0.6781714</strong><br/><strong>    0.04010025 0.7814718 0.7935024 0.6888857</strong><br/><strong>    0.36090226 0.7360166 0.8646440 0.6073893</strong>
</pre>
<p>A trick to effective ensembles is that the base models are not highly correlated. This is a subjective statement, and there is no hard rule of correlated predictions. One should experiment with the results and substitute a model if deemed necessary. Let's look at our results:</p>
<pre>
<strong>    &gt; modelCor(resamples(models))</strong><br/><strong>              rpart     earth       knn</strong><br/><strong>    rpart 1.0000000 0.9589931 0.7191618</strong><br/><strong>    earth 0.9589931 1.0000000 0.8834022</strong><br/><strong>    knn   0.7191618 0.8834022 1.0000000</strong>
</pre>
<p>The classification tree and earth models are highly correlated. This may be an issue, but let's progress by creating our the fourth classifier, the stacking model, and examining the results. To do this, we will capture the predicted probabilities for "Yes" on the test set in a dataframe:</p>
<pre>
<strong>    &gt; model_preds &lt;- lapply(models, predict, newdata=test, type="prob")<br/></strong><br/><strong>    &gt; model_preds &lt;- lapply(model_preds, function(x) x[,"Yes"])<br/></strong><br/><strong>    &gt; model_preds &lt;- data.frame(model_preds)</strong>
</pre>
<p>We now stack these models for a final prediction with <kbd>caretStack()</kbd>. This will be a simple logistic regression based on five bootstrapped samples:</p>
<pre>
<strong>    &gt; stack &lt;- caretStack(models, method = "glm",</strong><br/><strong>      metric = "ROC",</strong><br/><strong>      trControl = trainControl(</strong><br/><strong>      method = "boot",</strong><br/><strong>      number = 5,</strong><br/><strong>      savePredictions = "final",</strong><br/><strong>      classProbs = TRUE,</strong><br/><strong>      summaryFunction = twoClassSummary</strong><br/><strong>      ))</strong>
</pre>
<p>You can examine the final model as such:</p>
<pre>
<strong>    &gt; summary(stack)</strong><br/><br/><strong>    Call:</strong><br/><strong>    NULL</strong><br/><br/><strong>    Deviance Residuals: </strong><br/><strong>        Min      1Q  Median     3Q    Max </strong><br/><strong>    -2.1029 -0.6268 -0.3584 0.5926 2.3714 </strong><br/><br/><strong>    Coefficients:</strong><br/><strong>               Estimate  Std. Error  z value   Pr(&gt;|z|) </strong><br/><strong>    (Intercept)  2.2212      0.2120   10.476   &lt; 2e-16 ***</strong><br/><strong>    rpart       -0.8529      0.3947   -2.161   0.03071 * </strong><br/><strong>    earth       -3.0984      0.4250   -7.290   3.1e-13 ***</strong><br/><strong>    knn         -1.2626      0.3524   -3.583   0.00034 ***</strong>
</pre>
<p>Even though <kbd>rpart</kbd> and earth were highly correlated, their coefficients are both significant and we can probably keep both in the analysis. We can now compare the individual model results with the <kbd>ensembled</kbd> learner:</p>
<pre>
<strong>    &gt; prob &lt;- 1-predict(stack, newdata = test, type = "prob")</strong><br/><br/><strong>    &gt; model_preds$ensemble &lt;- prob</strong><br/><br/><strong>    &gt; colAUC(model_preds, test$type)</strong><br/><strong>                   rpart     earth       knn  ensemble</strong><br/><strong>    No vs. Yes 0.7413481 0.7892562 0.7652376 0.8001033</strong>
</pre>
<p>What we see with the <kbd>colAUC()</kbd> function is the individual model AUCs and the AUC of the stacked/ensemble. The ensemble has led to a slight improvement over only using MARS from the earth package. So in this example, we see how creating an ensemble via model stacking can indeed increase predictive power. Can you build a better ensemble given this data? What other sampling or classifiers would you try? With that, let's move on to multi-class problems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Multiclass classification</h1>
            </header>

            <article>
                
<p>There are a number of approaches to learning in multiclass problems. Techniques such as random forest and discriminant analysis will deal with multiclass while some techniques and/or packages will not, for example, generalized linear models, <kbd>glm()</kbd>, in base R. As of this writing, the <kbd>caretEnsemble</kbd> package, unfortunately, will not work with multiclasses. However, the Machine Learning in R (<kbd>mlr</kbd>) package does support multiple classes and also ensemble methods. If you are familiar with sci-kit Learn for Python, one could say that <kbd>mlr</kbd> endeavors to provide the same functionality for R. The mlr and the caret-based packages are quickly turning into my favorites for almost any business problem. I intend to demonstrate how powerful the package is on a multiclass problem, then conclude by showing how to do an ensemble on the <kbd>Pima</kbd> data.</p>
<p>For the multiclass problem, we will look at how to tune a random forest and then examine how to take a GLM and turn it into a multiclass learner using the "one versus rest" technique. This is where we build a binary probability prediction for each class versus all the others, then ensemble them together to predict an observation's final class. The technique allows you to extend any classifier method to multiclass problems, and it can often outperform multiclass learners.</p>
<p>One quick note: don't confuse the terminology of multiclass and multilabel. In the former, an observation can be assigned to one and only one class, while in the latter, it can be assigned to multiple classes. An example of this is text that could be labeled both politics and humor. We will not cover multilabel problems in this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business and data understanding</h1>
            </header>

            <article>
                
<p>We are once again going to visit our wine data set that we used in <a href="f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml" target="_blank">Chapter 8</a>, <em>Cluster Analysis</em>. If you recall, it consists of 13 numeric features and a response of three possible classes of wine. Our task is to predict those classes. I will include one interesting twist and that is to artificially increase the number of observations. The reasons are twofold. First, I want to fully demonstrate the resampling capabilities of the <kbd>mlr</kbd> package, and second, I wish to cover a synthetic sampling technique. We utilized upsampling in the prior section, so synthetic is in order.</p>
<p>Our first task is to load the package libraries and bring the data:</p>
<pre>
<strong>    &gt; library(mlr)<br/></strong><br/><strong>    &gt; library(ggplot2)<br/></strong><br/><strong>    &gt; library(HDclassif)<br/></strong><br/><strong>    &gt; library(DMwR)<br/></strong><br/><strong>    &gt; library(reshape2)<br/><br/>    &gt; library(corrplot)<br/></strong><br/><strong>    &gt; data(wine)<br/></strong><br/><strong>    &gt; table(wine$class)</strong><br/><br/><strong>     1  2  3 </strong><br/><strong>    59 71 48</strong>
</pre>
<p>We have 178 observations, plus the response labels are numeric (1, 2 and 3). Let's more than double the size of our data. The algorithm used in this example is <strong>Synthetic Minority Over-Sampling Technique</strong> (<strong>SMOTE</strong>). In the prior example, we used upsampling where the minority class was sampled WITH REPLACEMENT until the class size matched the majority. With <kbd>SMOTE</kbd>, take a random sample of the minority class and compute/identify the k-nearest neighbors for each observation and randomly generate data based on those neighbors. The default nearest neighbors in the <kbd>SMOTE()</kbd> function from the <kbd>DMwR</kbd> package is 5 (k = 5).  The other thing you need to consider is the percentage of minority oversampling. For instance, if we want to create a minority class double its current size, we would specify <kbd>"percent.over = 100"</kbd> in the function. The number of new samples for each case added to the current minority class is percent over/100, or one new sample for each observation. There is another parameter for percent over, and that controls the number of majority classes randomly selected for the new dataset.</p>
<p>Here is the application of the technique, first starting by structuring the classes to a factor, otherwise the function will not work:</p>
<pre>
<strong>    &gt; wine$class &lt;- as.factor(wine$class)</strong><br/><br/><strong>    &gt; set.seed(11)</strong><br/><br/><strong>    &gt; df &lt;- SMOTE(class ~ ., wine, perc.over = 300, perc.under = 300)</strong><br/><br/><strong>    &gt; table(df$class)</strong><br/><br/><strong>      1   2   3 </strong><br/><strong>    195 237 192</strong>
</pre>
<p>Voila! We have created a dataset of 624 observations. Our next endeavor will involve a visualization of the number of features by class. I am a big fan of boxplots, so let's create boxplots for the first four inputs by class. They have different scales, so putting them into a dataframe with mean 0 and standard deviation of 1 will aid the comparison:</p>
<pre>
<strong>    &gt; wine.scale &lt;- data.frame(scale(wine[, 2:5]))<br/></strong><br/><strong>    &gt; wine.scale$class &lt;- wine$class<br/></strong><br/><strong>    &gt; wine.melt &lt;- melt(wine.scale, id.var="class")<br/></strong><br/><strong>    &gt; ggplot(data = wine.melt, aes( x = class, y = value)) +</strong><br/><strong>      geom_boxplot() +</strong><br/><strong>      facet_wrap( ~ variable, ncol = 2)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="275" width="404" class="image-border" src="assets/image_ens_02.png"/></div>
<p>Recall from <a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml" target="_blank">Chapter 3</a>, <em>Logistic Regression and Discriminant Analysis</em> that a dot on the boxplot is considered an outlier. So, what should we do with them? There are a number of things to do:</p>
<ul>
<li>Nothing--doing nothing is always an option</li>
<li>Delete the outlying observations</li>
<li>Truncate the observations either within the current feature or create a separate feature of truncated values</li>
<li>Create an indicator variable per feature that captures whether an observation is an outlier</li>
</ul>
<p>I've always found outliers interesting and usually look at them closely to determine why they occur and what to do with them. We don't have that kind of time here, so let me propose a simple solution and code around truncating the outliers. Let's create a function to identify each outlier and reassign a high value (&gt; 99th percentile) to the 75th percentile and a low value (&lt; 1 percentile) to the 25th percentile. You could do median or whatever, but I've found this to work well.</p>
<div class="packt_infobox">You could put these code excerpts into the same function, but I've done in this fashion for simplification and understanding.</div>
<p>These are our outlier functions:</p>
<pre>
<strong>    &gt; outHigh &lt;- function(x) {</strong><br/><strong>      x[x &gt; quantile(x, 0.99)] &lt;- quantile(x, 0.75)</strong><br/><strong>      x</strong><br/><strong>      }</strong><br/><br/><strong>    &gt; outLow &lt;- function(x) {</strong><br/><strong>      x[x &lt; quantile(x, 0.01)] &lt;- quantile(x, 0.25)</strong><br/><strong>      x</strong><br/><strong>      }</strong>
</pre>
<p>Now we execute the function on the original data and create a new dataframe:</p>
<pre>
    <strong>&gt; wine.trunc &lt;- data.frame(lapply(wine[, -1], outHigh))</strong><br/><br/><strong>    &gt; wine.trunc &lt;- data.frame(lapply(wine.trunc, outLow))</strong><br/><br/><strong>    &gt; wine.trunc$class &lt;- wine$class</strong>
</pre>
<p>A simple comparison of a truncated feature versus the original is in order.  Let's try that with <kbd>V3</kbd>:</p>
<pre>
<strong>    &gt; boxplot(wine.trunc$V3 ~ wine.trunc$class)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="264" width="389" class="image-border" src="assets/image_ens_03.png"/></div>
<p>So that worked out well. Now it's time to look at correlations:</p>
<pre>
<strong>    &gt; c &lt;- cor(wine.trunc[, -14])</strong><br/><br/><strong>    &gt; corrplot.mixed(c, upper = "ellipse")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="476" class="image-border" src="assets/image_ens_04.png"/></div>
<p>We see that <span class="packt_screen">V6</span> and <span class="packt_screen">V7</span> are the highest correlated features, and we see a number above 0.5. In general, this is not a problem with non-linear based learning methods, but we will account for this in our GLM by incorporating an L2 penalty (ridge regression). </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Model evaluation and selection</h1>
            </header>

            <article>
                
<p>We will begin by creating our training and testing sets, then create a random forest classifier as our base model. After evaluating its performance, we will move on and try the one-versus-rest classification method and see how it performs. We split our data 70/30. Also, one of the unique things about the <kbd>mlr</kbd> package is its requirement to put your training data into a "task" structure, specifically a classification task. Optionally, you can place your test set in a task as well.</p>
<p>A full list of models is available here, plus you can also utilize your own:</p>
<p><a href="https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html">https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html</a></p>
<pre>
<strong>    &gt; library(caret) #if not already loaded<br/></strong><br/><strong>    &gt; set.seed(502)<br/></strong><br/><strong>    &gt; split &lt;- createDataPartition(y = df$class, p = 0.7, list = F)<br/></strong><br/><strong>    &gt; train &lt;- df[split, ]<br/></strong><br/><strong>    &gt; test &lt;- df[-split, ]<br/><br/>    &gt; wine.task &lt;- makeClassifTask(id = "wine", data = train, target = <br/>      "class")<br/></strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random forest</h1>
            </header>

            <article>
                
<p>With our training data task created, you have a number of functions to explore it. Here is the abbreviated output that looks at its structure:</p>
<pre>
<strong>    &gt; str(getTaskData(wine.task))</strong><br/><strong>    'data.frame': 438 obs. of 14 variables:</strong><br/><strong>     $ class: Factor w/ 3 levels "1","2","3": 1 2 1 2 2 1 2 1 1 2 ...</strong><br/><strong>     $ V1 : num 13.6 11.8 14.4 11.8 13.1 ...</strong>
</pre>
<p>There are many ways to use <kbd>mlr</kbd> in your analysis, but I recommend creating your resample object. Here we create a resampling object to help us in tuning the number of trees for our random forest, consisting of three subsamples:</p>
<pre>
<strong>    &gt; rdesc &lt;- makeResampleDesc("Subsample", iters = 3)</strong>
</pre>
<p>The next object establishes the grid of trees for tuning with the minimum number of trees, 750, and the maximum of 2000. You can also establish a number of multiple parameters like we did with the <kbd>caret</kbd> package. Your options can be explored using calling help for the function with <kbd>makeParamSet</kbd>:</p>
<pre>
<strong>    &gt; param &lt;- makeParamSet(<br/>        makeDiscreteParam("ntree", values = c(750, 1000, 1250, 1500, <br/>         1750, 2000))<br/>    )</strong>
</pre>
<p>Next, create a control object, establishing a numeric grid:</p>
<pre>
<strong>    &gt; ctrl &lt;- makeTuneControlGrid()</strong>
</pre>
<p>Now, go ahead and tune the hyperparameter for the optimal number of trees. Then, call up both the optimal number of trees and the associated out-of-sample error:</p>
<pre>
<strong>    &gt; tuning &lt;- tuneParams("classif.randomForest", task = wine.task,</strong><br/><strong>        resampling = rdesc, par.set = param,</strong><br/><strong>        control = ctrl)<br/><br/>    &gt; tuning$x<br/>    $ntree<br/>    [1] 1250<br/><br/>    &gt; tuning$y<br/>    mmce.test.mean <br/>    0.01141553 <br/></strong>
</pre>
<p>The optimal number of trees is 1,250 with a mean misclassification error of 0.01 percent, almost perfect classification. It is now a simple matter of setting this parameter for training as a wrapper around the <kbd>makeLearner()</kbd> function. Notice that I set the predict type to probability as the default is the predicted class:</p>
<pre>
<strong>    &gt; rf &lt;- setHyperPars(makeLearner("classif.randomForest",</strong><br/><strong>      predict.type = "prob"), par.vals = tuning$x)</strong>
</pre>
<p>Now we train the model:</p>
<pre>
<strong>    &gt; fitRF &lt;- train(rf, wine.task)</strong>
</pre>
<p>You can see the confusion matrix on the train data:</p>
<pre>
<strong>    &gt; fitRF$learner.model<br/>              OOB estimate of error rate: 0%<br/>    Confusion matrix:<br/>       1   2  3  class.error<br/>    1 72   0   0           0<br/>    2  0  97   0           0<br/>    3  0   0 101           0<br/></strong>
</pre>
<p>Then, evaluate its performance on the test set, both error and accuracy (1 - error). With no test task, you specify <kbd>newdata = test</kbd>, otherwise if you did create a test task, just use <kbd>test.task</kbd>:</p>
<pre>
<strong>    &gt; predRF &lt;- predict(fitRF, newdata = test)</strong><br/><br/><strong>    &gt; getConfMatrix(predRF)</strong><br/><strong>           predicted</strong><br/><strong>    true     1  2  3  -SUM-</strong><br/><strong>      1     58  0  0      0</strong><br/><strong>      2      0 71  0      0</strong><br/><strong>      3      0  0 57      0</strong><br/><strong>      -SUM-  0  0  0      0</strong><br/><br/><strong>    &gt; performance(predRF, measures = list(mmce, acc))</strong><br/><strong>     mmce acc </strong><br/><strong>      0   1</strong>
</pre>
<p>Well, that was just too easy as we are able to predict each class with no error. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Ridge regression</h1>
            </header>

            <article>
                
<p>For demonstration purposes, let's still try our ridge regression on a one-versus-rest method. To do this, create a <kbd>MulticlassWrapper</kbd> for a binary classification method. The <kbd>classif.penalized.ridge</kbd> method is from the <kbd>penalized</kbd> package, so make sure you have it installed:</p>
<pre>
<strong>    &gt; ovr &lt;- makeMulticlassWrapper("classif.penalized.ridge", <br/>      mcw.method = "onevsrest")</strong>
</pre>
<p>Now let's go ahead and create a wrapper for our classifier that creates a bagging resample of 10 iterations (it is the default) with replacement, sampling 70% of the observations and all of the input features:</p>
<pre>
<strong>    &gt; bag.ovr = makeBaggingWrapper(ovr, bw.iters = 10, #default of 10</strong><br/><strong>      bw.replace = TRUE, #default</strong><br/><strong>      bw.size = 0.7,</strong><br/><strong>      bw.feats = 1)</strong>
</pre>
<p>This can now be used to train our algorithm. Notice in the code I put <kbd>mlr::</kbd> before <kbd>train()</kbd>. The reason is that caret also has a <kbd>train()</kbd> function, so we are specifying we want <kbd>mlr</kbd> train function and not caret's. Sometimes, if this is not done when both packages are loaded, you will end up with an egregious error:</p>
<pre>
<strong>    &gt; set.seed(317)</strong><br/><strong>    &gt; fitOVR &lt;- mlr::train(bag.ovr, wine.task)<br/>    &gt; predOVR &lt;- predict(fitOVR, newdata = test)<br/></strong>
</pre>
<p>Let's see how it did:</p>
<pre>
<strong>    &gt; head(data.frame(predOVR))</strong><br/><strong>       truth response</strong><br/><strong>    60     2        2</strong><br/><strong>    78     2        2</strong><br/><strong>    79     2        2</strong><br/><strong>    49     1        1</strong><br/><strong>    19     1        1</strong><br/><strong>    69     2        2</strong><br/><br/><strong>    &gt; getConfMatrix(predOVR)</strong><br/><strong>           predicted</strong><br/><strong>    true     1  2  3  -SUM-</strong><br/><strong>      1     58  0  0      0</strong><br/><strong>      2      0 71  0      0</strong><br/><strong>      3      0  0 57      0</strong><br/><strong>      -SUM-  0  0  0      0</strong>
</pre>
<p>Again, it is just too easy. However, don't focus on the accuracy as much as the methodology of creating your classifier, tuning any parameters, and implementing a resampling strategy. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">MLR's ensemble</h1>
            </header>

            <article>
                
<p>Here is something we haven't found too easy: the <kbd>Pima</kbd> diabetes classification. Like caret, you can build ensemble models, so let's give that a try. I will also show how to incorporate <kbd>SMOTE</kbd> into the learning process instead of creating a separate dataset.</p>
<p>First, make sure you run the code from the beginning of this chapter to create the train and test sets. I'll pause here and let you take care of that.</p>
<p>Great, now let's create the training task as before:</p>
<pre>
<strong>    &gt; pima.task &lt;- makeClassifTask(id = "pima", data = train, target = <br/>      "type")</strong>
</pre>
<p>The <kbd>smote()</kbd> function here is a little different from what we did before. You just have to specify the rate of minority oversample and the k-nearest neighbors. We will double our minority class (Yes) based on the three nearest neighbors:</p>
<pre>
<strong>    &gt; pima.smote &lt;- smote(pima.task, rate = 2, nn = 3)<br/><br/>    &gt; str(getTaskData(pima.smote))<br/>    'data.frame': 533 obs. of 8 variables:<br/></strong>
</pre>
<p>We now have 533 observations instead of the 400 originally in train. To accomplish our ensemble stacking, we will create three base models (random forest, quadratic discriminant analysis, and L1 penalized GLM). This code puts them together as the base models, learners if you will, and ensures we have probabilities created for use as input features:</p>
<pre>
<strong>    &gt; base &lt;- c("classif.randomForest", "classif.qda", classif.glmnet")<br/></strong><br/><strong>    &gt; learns &lt;- lapply(base, makeLearner)<br/></strong><br/><strong>    &gt; learns &lt;- lapply(learns, setPredictType, "prob")</strong>
</pre>
<p>The stacking model will simply be a GLM, with coefficients tuned by cross-validation. The package default is five folds:</p>
<pre>
<strong>    &gt; sl &lt;- makeStackedLearner(base.learners = learns,</strong><br/><strong>      super.learner = "classif.logreg",</strong><br/><strong>      predict.type = "prob",</strong><br/><strong>      method = "stack.cv")</strong>
</pre>
<p>We can now train the base and stacked models. You can choose to incorporate resampling and tuning wrappers as you see fit, just like we did in the prior sections. In this case, we will just stick with the defaults. Training and prediction on the test set works the same way also:</p>
<pre>
<strong>    &gt; slFit &lt;- mlr::train(sl, pima.smote)</strong><br/><br/><strong>    &gt; predFit &lt;- predict(slFit, newdata = test)</strong><br/><br/><strong>    &gt; getConfMatrix(predFit)</strong><br/><strong>           predicted</strong><br/><strong>    true        No Yes -SUM-</strong><br/><strong>     No        70  18    18</strong><br/><strong>     Yes       15  29    15</strong><br/><strong>     -SUM-     15  18    33</strong><br/><br/><strong>    &gt; performance(predFit, measures = list(mmce, acc, auc))</strong><br/><strong>      mmce    acc         auc</strong><br/><strong>      0.25   0.75   0.7874483</strong>
</pre>
<p>All that effort and we just achieved 75% accuracy and a slightly inferior <kbd>AUC</kbd> to the ensemble built using <kbd>caretEnsemble</kbd>, granted we used different base learners. So, that begs the question as before, can you improve on these results? Please let me know your results.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we looked at the very important machine learning methods of creating an ensemble model by stacking and then multiclass classification. In stacking, we used base models (learners) to create predicted probabilities that were used on input features to another model (super learner) to make our final predictions. Indeed, the stacked method showed slight improvement over the individual base models. As for multiclass methods, we worked on using a multiclass classifier as well as taking a binary classification method and applying it to a multiclass problem using the one-versus-all technique. As a side task, we also incorporated two sampling techniques (upsampling and Synthetic Minority Oversampling Technique) to balance the classes. Also significant was the utilization of two very powerful R packages, <kbd>caretEnsemble</kbd> and <kbd>mlr</kbd>. These methods and packages are powerful additions to an R machine learning practitioner.</p>
<p>Up next, we are going to delve into the world of time series and causality. In my opinion, time series analysis is one of the most misunderstood and neglected areas of machine learning. The next chapter should get you on your way to help our profession close that gap.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>