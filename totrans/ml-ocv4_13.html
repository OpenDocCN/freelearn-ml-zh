<html><head></head><body><div><h1 class="header-title">Ensemble Methods for Classification</h1>
                
            
            
                
<p>So far, we have looked at a number of interesting machine learning algorithms, from classic methods such as linear regression to more advanced techniques such as deep neural networks. At various points, we pointed out that every algorithm has its own strengths and weaknesses—and we took note of how to spot and overcome these weaknesses.</p>
<p>However, wouldn't it be great if we could simply stack together a bunch of average classifiers to form a much stronger <strong>ensemble</strong> of classifiers?</p>
<p>In this chapter, we will do just that. Ensemble methods are techniques that bind multiple different models together in order to solve a shared problem. Their use has become a common practice in competitive machine learning—making ...</p></div>



  
<div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You can refer to the code for this chapter from the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10</a>.</p>
<p>Here is a short summary of the software and hardware requirements:</p>
<ul>
<li>OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>Python version 3.6 (any Python version 3.x will be fine).</li>
<li>Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any OS—macOS, Windows, or Linux-based—with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided with the book.</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding ensemble methods</h1>
                
            
            
                
<p>The goal of ensemble methods is to combine the predictions of several individual estimators built with a given learning algorithm in order to solve a shared problem. Typically, an ensemble consists of two major components:</p>
<ul>
<li>A set of models</li>
<li>A set of decision rules that govern how the results of these models are combined into a single output</li>
</ul>
<p>The idea behind ensemble methods has much to do with the <em>wisdom of the crowd</em> concept. Rather than the opinion of a single expert, we consider the collective opinion of a group of individuals. In the context of machine learning, these individuals would be classifiers or regressors. The idea is that if we just ask a large enough number of classifiers, one of them ought to get ...</p></div>



  
<div><h1 class="header-title">Understanding averaging ensembles</h1>
                
            
            
                
<p>Averaging methods have a long history in machine learning and are commonly applied to fields such as molecular dynamics and audio signal processing. Such ensembles are typically seen as exact replicas of a given system.</p>
<p>An averaging ensemble is essentially a collection of models that train on the same dataset. Their results are then aggregated in a number of ways.</p>
<p>One common method involves creating multiple model configurations that take different parameter subsets as input. Techniques that take this approach are referred to collectively as bagging methods.</p>
<p>Bagging methods come in many different flavors. However, they typically only differ in the way they draw random subsets of the training set:</p>
<ul>
<li>Pasting methods draw random subsets of the samples without replacement of data samples.</li>
<li>Bagging methods draw random subsets of the samples with replacement of data samples.</li>
<li>Random subspace methods draw random subsets of the features but train on all data samples.</li>
<li>Random patch methods draw random subsets of both samples and features.</li>
</ul>
<p>Averaging ensembles can be used to reduce the variability of a model's performance.</p>
<p>In scikit-learn, bagging methods can be realized using the <kbd>BaggingClassifier</kbd> and <kbd>BaggingRegressor</kbd> meta-estimators. These are meta-estimators because they allow us to build an ensemble from any other base estimator.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing a bagging classifier</h1>
                
            
            
                
<p>We can, for instance, build an ensemble from a collection of 10 <em>k</em>-NN classifiers as follows:</p>
<pre>In [1]: from sklearn.ensemble import BaggingClassifier...     from sklearn.neighbors import KNeighborsClassifier...     bag_knn = BaggingClassifier(KNeighborsClassifier(),...                                 n_estimators=10)</pre>
<p>The <kbd>BaggingClassifier</kbd> class provides a number of options to customize the ensemble:</p>
<ul>
<li><kbd>n_estimators</kbd>: As shown in the preceding code, this specifies the number of base estimators in the ensemble.</li>
<li><kbd>max_samples</kbd>: This denotes the number (or fraction) of samples to draw from the dataset to train each base estimator. We can set <kbd>bootstrap=True</kbd> to sample with replacement (effectively implementing bagging), or we can set <kbd>bootstrap=False</kbd> to implement ...</li></ul></div>



  
<div><h1 class="header-title">Implementing a bagging regressor</h1>
                
            
            
                
<p>Similarly, we can use the <kbd>BaggingRegressor</kbd> class to form an ensemble of regressors.</p>
<p>For example, we could build an ensemble of decision trees to predict housing prices from the Boston dataset of <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>.</p>
<p>In the following steps, you will learn how to use a bagging regressor for forming an ensemble of regressors:</p>
<ol>
<li>The syntax is almost identical to setting up a bagging classifier:</li>
</ol>
<pre style="padding-left: 60px">In [7]: from sklearn.ensemble import BaggingRegressor<br/>...     from sklearn.tree import DecisionTreeRegressor<br/>...     bag_tree = BaggingRegressor(DecisionTreeRegressor(),<br/>...                                 max_features=0.5, n_estimators=10, <br/>...                                 random_state=3)</pre>
<ol start="2">
<li>Of course, we need to load and split the dataset as we did for the breast cancer dataset:</li>
</ol>
<pre style="padding-left: 60px">In [8]: from sklearn.datasets import load_boston<br/>...     dataset = load_boston()<br/>...     X = dataset.data<br/>...     y = dataset.target<br/>In [9]: from sklearn.model_selection import train_test_split<br/>...     X_train, X_test, y_train, y_test = train_test_split(<br/>...         X, y, random_state=3<br/>...     )</pre>
<ol start="3">
<li>Then, we can fit the bagging regressor on <kbd>X_train</kbd> and score it on <kbd>X_test</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In [10]: bag_tree.fit(X_train, y_train)<br/>...      bag_tree.score(X_test, y_test)<br/>Out[10]: 0.82704756225081688</pre>
<p>In the preceding example, we find a performance boost of roughly 5%, from 77.3% accuracy for a single decision tree to 82.7% accuracy.</p>
<p>Of course, we wouldn't just stop here. Nobody said the ensemble needs to consist of 10 individual estimators, so we are free to explore different-sized ensembles. On top of that, the <kbd>max_samples</kbd> and <kbd>max_features</kbd> parameters allow for a great deal of customization.</p>
<p>A more sophisticated version of bagged decision trees is called random forests, which we will talk about later in this chapter.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding boosting ensembles</h1>
                
            
            
                
<p>Another approach to building ensembles is through boosting. Boosting models use multiple individual learners in sequence to iteratively boost the performance of the ensemble.</p>
<p>Typically, the learners used in boosting are relatively simple. A good example is a decision tree with only a single node—a decision stump. Another example could be a simple linear regression model. The idea is not to have the strongest individual learners, but quite the opposite—we want the individuals to be weak learners so that we get a superior performance only when we consider a large number of individuals.</p>
<p>At each iteration of the procedure, the training set is adjusted so that the next classifier is applied to the data points that ...</p></div>



  
<div><h1 class="header-title">Weak learners</h1>
                
            
            
                
<p>Weak learners are classifiers that are only slightly correlated with the actual classification; they can be somewhat better than the random predictions. On the contrary, strong learners are arbitrarily well correlated with the correct classification.</p>
<p class="mce-root">The idea here is that you don't use just one but a broad set of weak learners, each one slightly better than random. Many instances of the weak learners can be pooled using boosting, bagging, and so on together to create a strong ensemble classifier. The benefit is that the final classifier will not lead to <em>overfitting</em> on your training data.  </p>
<p>For example, AdaBoost fits a sequence of weak learners on different weighted training data. It starts by predicting the training dataset and gives equal weight to each observation/sample. If the first learner prediction is incorrect, then it gives higher weight to the observation/sample that has been mispredicted. Since it is an iterative process, it continues to add learners until a limit is reached in the number of models or accuracy.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing a boosting classifier</h1>
                
            
            
                
<p>For example, we can build a boosting classifier from a collection of 10 decision trees as follows:</p>
<pre>In [11]: from sklearn.ensemble import GradientBoostingClassifier...      boost_class = GradientBoostingClassifier(n_estimators=10,...                                               random_state=3)</pre>
<p>These classifiers support both binary and multiclass classification.</p>
<p>Similar to the <kbd>BaggingClassifier</kbd> class, the <kbd>GradientBoostingClassifier</kbd> class provides a number of options to customize the ensemble:</p>
<ul>
<li><kbd>n_estimators</kbd>: This denotes the number of base estimators in the ensemble. A large number of estimators typically results in better performance.</li>
<li><kbd>loss</kbd>: This denotes the loss function (or cost function) to be optimized. Setting <kbd>loss='deviance'</kbd> implements logistic regression ...</li></ul></div>



  
<div><h1 class="header-title">Implementing a boosting regressor</h1>
                
            
            
                
<p>Implementing a boosted regressor follows the same syntax as the boosted classifier:</p>
<pre>In [15]: from sklearn.ensemble import GradientBoostingRegressor<br/>...      boost_reg = GradientBoostingRegressor(n_estimators=10,<br/>...                                            random_state=3)</pre>
<p>We have seen earlier that a single decision tree can achieve 79.3% accuracy on the Boston dataset. A bagged decision tree classifier made of 10 individual regression trees achieved 82.7% accuracy. But how does a boosted regressor compare?</p>
<p>Let's reload the Boston dataset and split it into training and test sets. We want to make sure we use the same value for <kbd>random_state</kbd> so that we end up training and testing on the same subsets of the data:</p>
<pre>In [16]: dataset = load_boston()<br/>...      X = dataset.data<br/>...      y = dataset.target<br/>In [17]: X_train, X_test, y_train, y_test = train_test_split(<br/>...          X, y, random_state=3<br/>...     )</pre>
<p>As it turns out, the boosted decision tree ensemble actually performs worse than the previous code:</p>
<pre>In [18]: boost_reg.fit(X_train, y_train)<br/>...      boost_reg.score(X_test, y_test)<br/>Out[18]: 0.71991199075668488</pre>
<p>This result might be confusing at first. After all, we used 10 times more classifiers than we did for the single decision tree. Why would our numbers get worse?</p>
<p>You can see this is a good example of an expert classifier being smarter than a group of weak learners. One possible solution is to make the ensemble larger. In fact, it is customary to use in the order of 100 weak learners in a boosted ensemble:</p>
<pre>In [19]: boost_reg = GradientBoostingRegressor(n_estimators=100)</pre>
<p>Then, when we retrain the ensemble on the Boston dataset, we get a test score of 89.8%:</p>
<pre>In [20]: boost_reg.fit(X_train, y_train)<br/>...      boost_reg.score(X_test, y_test)<br/>Out[20]: 0.89984081091774459</pre>
<p>What happens when you increase the number to <kbd>n_estimators=500</kbd>? There's a lot more we could do by playing with the optional parameters.</p>
<p>As you can see, boosting is a powerful procedure that allows you to get massive performance improvements by combining a large number of relatively simple learners.</p>
<p>A specific implementation of boosted decision trees is the AdaBoost algorithm, which we will talk about later in this chapter.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding stacking ensembles</h1>
                
            
            
                
<p>All the ensemble methods we have seen so far share a common design philosophy: to fit multiple individual classifiers to the data and incorporate their predictions with the help of some simple decision rules (such as averaging or boosting) into a final prediction.</p>
<p>Stacking ensembles, on the other hand, build ensembles with hierarchies. Here, individual learners are organized into multiple layers where the output of one layer of learners is used as training data for a model at the next layer. This way, it is possible to successfully blend hundreds of different models.</p>
<p>Unfortunately, discussing stacking ensembles in detail is beyond the scope of this book.</p>
<p>However, these models can be very powerful, as seen, ...</p></div>



  
<div><h1 class="header-title">Combining decision trees into a random forest</h1>
                
            
            
                
<p>A popular variation of bagged decision trees are the so-called random forests. These are essentially a collection of decision trees, where each tree is slightly different from the others. In contrast to bagged decision trees, each tree in a random forest is trained on a slightly different subset of data features.</p>
<p>Although a single tree of unlimited depth might do a relatively good job of predicting the data, it is also prone to overfitting. The idea behind random forests is to build a large number of trees, each of them trained on a random subset of data samples and features. Because of the randomness of the procedure, each tree in the forest will overfit the data in a slightly different way. The effect of overfitting can then be reduced by averaging the predictions of the individual trees.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding the shortcomings of decision trees</h1>
                
            
            
                
<p>The effect of overfitting the dataset, which a decision tree often falls victim to, is best demonstrated through a simple example.</p>
<p>For this, we will return to the <kbd>make_moons</kbd> function from scikit-learn's <kbd>datasets</kbd> module, which we previously used in <a href="790a10c4-635a-40da-ae5f-13946bc0e9fd.xhtml" target="_blank">Chapter 8</a>, <em>Discovering Hidden Structures with Unsupervised Learning</em>, to organize data into two interleaving half circles. Here, we choose to generate 100 data samples belonging to two half circles, in combination with some Gaussian noise with a standard deviation of <kbd>0.25</kbd>:</p>
<pre>In [1]: from sklearn.datasets import make_moons...     X, y = make_moons(n_samples=100, noise=0.25,...                       random_state=100)</pre>
<p>We can visualize this data using matplotlib and the <kbd>scatter</kbd></p></div>



  
<div><h1 class="header-title">Implementing our first random forest</h1>
                
            
            
                
<p>In OpenCV, random forests can be built using the <kbd>RTrees_create</kbd> function from the <kbd>ml</kbd> module:</p>
<pre>In [7]: import cv2<br/>...     rtree = cv2.ml.RTrees_create()</pre>
<p>The tree object provides a number of options, the most important of which are the following:</p>
<ul>
<li><kbd>setMaxDepth</kbd>: This sets the maximum possible depth of each tree in the ensemble. The actual obtained depth may be smaller if other termination criteria are met first.</li>
<li><kbd>setMinSampleCount</kbd>: This sets the minimum number of samples that a node can contain for it to get split.</li>
<li><kbd>setMaxCategories</kbd>: This sets the maximum number of categories allowed. Setting the number of categories to a smaller value than the actual number of classes in the data leads to subset estimation.</li>
<li><kbd>setTermCriteria</kbd>: This sets the termination criteria of the algorithm. This is also where you set the number of trees in the forest.</li>
</ul>
<p>Although we might have hoped for a <kbd>setNumTrees</kbd> method to set the number of trees in the forest (kind of the most important parameter of them all, no?), we instead need to rely on the <kbd>setTermCriteria</kbd> method. Confusingly, the number of trees is conflated with <kbd>cv2.TERM_CRITERA_MAX_ITER</kbd>, which is usually reserved for the number of iterations that an algorithm is run for, not for the number of estimators in an ensemble.</p>
<p>We can specify the number of trees in the forest by passing an integer, <kbd>n_trees</kbd>, to the <kbd>setTermCriteria</kbd> method. Here, we also want to tell the algorithm to quit once the score does not increase by at least <kbd>eps</kbd> from one iteration to the next:</p>
<pre>In [8]: n_trees = 10<br/>...     eps = 0.01<br/>...     criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,<br/>...                 n_trees, eps)<br/>...     rtree.setTermCriteria(criteria)</pre>
<p>Then, we are ready to train the classifier on the data from the preceding code:</p>
<pre>In [9]: rtree.train(X_train.astype(np.float32), cv2.ml.ROW_SAMPLE,<br/>                    y_train);</pre>
<p>The test labels can be predicted with the <kbd>predict</kbd> method:</p>
<pre>In [10]: _, y_hat = rtree.predict(X_test.astype(np.float32))</pre>
<p>Using scikit-learn's <kbd>accuracy_score</kbd>, we can evaluate the model on the test set:</p>
<pre>In [11]: from sklearn.metrics import accuracy_score<br/>...      accuracy_score(y_test, y_hat)<br/>Out[11]: 0.83999999999999997</pre>
<p>After training, we can pass the predicted labels to the <kbd>plot_decision_boundary</kbd> function:</p>
<pre>In [12]: plot_decision_boundary(rtree, X_test, y_test)</pre>
<p>This will produce the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-996 image-border" src="img/637629d2-7e17-4c9a-9a00-a58b68d8f480.png" style="width:44.67em;height:27.17em;" width="851" height="517"/></p>
<p>The preceding image shows the decision landscape of a random forest classifier.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing a random forest with scikit-learn</h1>
                
            
            
                
<p>Alternatively, we can implement random forests using scikit-learn:</p>
<pre>In [13]: from sklearn.ensemble import RandomForestClassifier...      forest = RandomForestClassifier(n_estimators=10, random_state=200)</pre>
<p>Here, we have a number of options to customize the ensemble:</p>
<ul>
<li><kbd>n_estimators</kbd>: This specifies the number of trees in the forest.</li>
<li><kbd>criterion</kbd>: This specifies the node-splitting criterion. Setting <kbd>criterion='gini'</kbd> implements the Gini impurity, whereas setting <kbd>criterion='entropy'</kbd> implements information gain.</li>
<li><kbd>max_features</kbd>: This specifies the number (or fraction) of features to consider at each node split.</li>
<li><kbd>max_depth</kbd>: This specifies the maximum depth of each tree.</li>
<li><kbd>min_samples</kbd>: This specifies the minimum number ...</li></ul></div>



  
<div><h1 class="header-title">Implementing extremely randomized trees</h1>
                
            
            
                
<p>Random forests are already pretty arbitrary. But what if we wanted to take the randomness to its extreme?</p>
<p>In extremely randomized trees (see the <kbd>ExtraTreesClassifier</kbd> and <kbd>ExtraTreesRegressor</kbd> classes), the randomness is taken even further than in random forests. Remember how decision trees usually choose a threshold for every feature so that the purity of the node split is maximized? Extremely randomized trees, on the other hand, choose these thresholds at random. The best one of these randomly generated thresholds is then used as the splitting rule.</p>
<p>We can build an extremely randomized tree as follows:</p>
<pre>In [16]: from sklearn.ensemble import ExtraTreesClassifier<br/>...      extra_tree = ExtraTreesClassifier(n_estimators=10, random_state=100)</pre>
<p>To illustrate the difference between a single decision tree, a random forest, and extremely randomized trees, let's consider a simple dataset, such as the Iris dataset:</p>
<pre>In [17]: from sklearn.datasets import load_iris<br/>...      iris = load_iris()<br/>...      X = iris.data[:, [0, 2]]<br/>...      y = iris.target<br/>In [18]: X_train, X_test, y_train, y_test = train_test_split(<br/>...          X, y, random_state=100<br/>...      )</pre>
<p>We can then fit and score the tree object the same way we did before:</p>
<pre>In [19]: extra_tree.fit(X_train, y_train)<br/>...      extra_tree.score(X_test, y_test)<br/>Out[19]: 0.92105263157894735</pre>
<p>For comparison, using a random forest would have resulted in the same performance:</p>
<pre>In [20]: forest = RandomForestClassifier(n_estimators=10,<br/>                                        random_state=100)<br/>...      forest.fit(X_train, y_train)<br/>...      forest.score(X_test, y_test)<br/>Out[20]: 0.92105263157894735</pre>
<p>In fact, the same is true for a single tree:</p>
<pre>In [21]: tree = DecisionTreeClassifier()<br/>...      tree.fit(X_train, y_train)<br/>...      tree.score(X_test, y_test)<br/>Out[21]: 0.92105263157894735</pre>
<p>So, what's the difference between them? To answer this question, we have to look at the decision boundaries. Fortunately, we have already imported our <kbd>plot_decision_boundary</kbd> helper function in the preceding section, so all we need to do is pass the different classifier objects to it.</p>
<p>We will build a list of classifiers, where each entry in the list is a tuple that contains an index, a name for the classifier, and the classifier object:</p>
<pre class="mce-root">In [22]: classifiers = [<br/>...          (1, 'decision tree', tree),<br/>...          (2, 'random forest', forest),<br/>...          (3, 'extremely randomized trees', extra_tree)<br/>...      ]</pre>
<p>Then, it's easy to pass the list of classifiers to our helper function such that the decision landscape of every classifier is drawn in its own subplot:</p>
<pre class="mce-root">In [23]: for sp, name, model in classifiers:<br/>...      plt.subplot(1, 3, sp)<br/>...      plot_decision_boundary(model, X_test, y_test)<br/>...      plt.title(name)<br/>...      plt.axis('off')<br/><br/></pre>
<p>The result looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-998 image-border" src="img/84874c28-7c2e-4c95-ba4a-139c4b2d6901.png" style="width:93.25em;height:34.83em;" width="1119" height="418"/></p>
<p>Now the differences between the three classifiers become clearer. We see the single tree drawing by far the simplest decision boundaries, splitting the landscape using horizontal decision boundaries. The random forest is able to more clearly separate the cloud of data points in the lower-left of the decision landscape. However, only extremely randomized trees were able to corner the cloud of data points toward the center of the landscape from all sides.</p>
<p>Now that we know about all the different variations of tree ensembles, let's move on to a real-world dataset.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using random forests for face recognition</h1>
                
            
            
                
<p>A popular dataset that we haven't talked much about yet is the Olivetti faces dataset.</p>
<p>The Olivetti face dataset was collected in 1990 by AT&amp;T Laboratories Cambridge. The dataset comprises facial images of 40 distinct subjects, taken at different times and under different lighting conditions. In addition, subjects varied their facial expressions (open/closed eyes, smiling/not smiling) and their facial details (glasses/no glasses).</p>
<p>Images were then quantized into 256 grayscale levels and stored as unsigned 8-bit integers. Because there are 40 distinct subjects, the dataset comes with 40 distinct target labels. Recognizing faces thus constitutes an example of a multiclass classification task.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Loading the dataset</h1>
                
            
            
                
<p>Like many other classic datasets, the Olivetti face dataset can be loaded using scikit-learn:</p>
<pre>In [1]: from sklearn.datasets import fetch_olivetti_faces<br/>...     dataset = fetch_olivetti_faces()<br/>In [2]: X = dataset.data<br/>...     y = dataset.target</pre>
<p>Although the original images consisted of 92 x 112 pixel images, the version available through scikit-learn contains images downscaled to <em>64 x 64</em> pixels.</p>
<p>To get a sense of the dataset, we can plot some example images. Let's pick eight indices from the dataset in random order:</p>
<pre class="mce-root">In [3]: import numpy as np<br/>...     np.random.seed(21)<br/>...     idx_rand = np.random.randint(len(X), size=8)</pre>
<p>We can plot these example images using matplotlib, but we need to make sure we reshape the column vectors to 64 x 64-pixel images before plotting:</p>
<pre>In [4]: import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>...     for p, i in enumerate(idx_rand):<br/>...         plt.subplot(2, 4, p + 1)<br/>... plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')<br/>...         plt.axis('off')</pre>
<p class="mce-root"/>
<p>The preceding code produces the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-999 image-border" src="img/f68b62ef-a7ec-476b-8467-87c6fc2ba850.png" style="width:41.92em;height:23.00em;" width="1155" height="633"/></p>
<p>You can see how all the faces are taken against a dark background and are portrait. The facial expressions vary drastically from image to image, making this an interesting classification problem. Try not to laugh at some of them!</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Preprocessing the dataset</h1>
                
            
            
                
<p>Before we can pass the dataset to the classifier, we need to preprocess it following the best practices from <a href="142fec63-a847-4cde-9de9-c34805d2bb84.xhtml" target="_blank">Chapter 4</a>, <em>Representing Data and Engineering Features</em>.</p>
<p>Specifically, we want to make sure that all example images have the same mean grayscale level:</p>
<pre>In [5]: n_samples, n_features = X.shape[:2]...     X -= X.mean(axis=0)</pre>
<p>We repeat this procedure for every image to make sure the feature values of every data point (that is, a row in <kbd>X</kbd>) are centered around zero:</p>
<pre>In [6]: X -= X.mean(axis=1).reshape(n_samples, -1)</pre>
<p>The preprocessed data can be visualized using the following code:</p>
<pre>In [7]: for p, i in enumerate(idx_rand):...         plt.subplot(2, 4, p + 1)...         plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')... plt.axis('off') ...</pre></div>



  
<div><h1 class="header-title">Training and testing the random forest</h1>
                
            
            
                
<p>We continue to follow our best practice to split the data into training and test sets:</p>
<pre>In [8]: from sklearn.model_selection import train_test_split<br/>...     X_train, X_test, y_train, y_test = train_test_split(<br/>...         X, y, random_state=21<br/>...     )</pre>
<p>Then, we are ready to apply a random forest to the data:</p>
<pre>In [9]: import cv2<br/>...     rtree = cv2.ml.RTrees_create()</pre>
<p>Here, we want to create an ensemble with 50 decision trees:</p>
<pre>In [10]: n_trees = 50<br/>...      eps = 0.01<br/>...      criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,<br/>...                  n_trees, eps)<br/>...      rtree.setTermCriteria(criteria)</pre>
<p>Because we have a large number of categories (that is, 40), we want to make sure the random forest is set up to handle them accordingly:</p>
<pre>In [10]: rtree.setMaxCategories(len(np.unique(y)))</pre>
<p>We can play with other optional arguments, such as the number of data points required in a node before it can be split:</p>
<pre>In [11]: rtree.setMinSampleCount(2)</pre>
<p>However, we might not want to limit the depth of each tree. This is again a parameter we will have to experiment with in the end. But for now, let's set it to a large integer value, making the depth effectively unconstrained:</p>
<pre>In [12]: rtree.setMaxDepth(1000)</pre>
<p>Then, we can fit the classifier to the training data:</p>
<pre>In [13]: rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train);</pre>
<p>We can check the resulting depth of the tree using the following function:</p>
<pre>In [13]: rtree.getMaxDepth()<br/>Out[13]: 25</pre>
<p>This means that although we allowed the tree to go up to depth 1,000, in the end, only 25 layers were needed.</p>
<p>The evaluation of the classifier is done once again by predicting the labels first (<kbd>y_hat</kbd>) and then passing them to the <kbd>accuracy_score</kbd> function:</p>
<pre>In [14]: _, y_hat = tree.predict(X_test)<br/>In [15]: from sklearn.metrics import accuracy_score<br/>...      accuracy_score(y_test, y_hat)<br/>Out[15]: 0.87</pre>
<p>We find 87% accuracy, which turns out to be much better than with a single decision tree:</p>
<pre>In [16]: from sklearn.tree import DecisionTreeClassifier<br/>...      tree = DecisionTreeClassifier(random_state=21, max_depth=25)<br/>...      tree.fit(X_train, y_train)<br/>...      tree.score(X_test, y_test)<br/>Out[16]: 0.46999999999999997</pre>
<p>Not bad! We can play with the optional parameters to see whether we get better. The most important one seems to be the number of trees in the forest. We can repeat the experiment with a forest made from 1,000 trees instead of 50 trees:</p>
<pre>In [18]: num_trees = 1000<br/>... eps = 0.01<br/>... criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,<br/>... num_trees, eps)<br/>... rtree.setTermCriteria(criteria)<br/>... rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train);<br/>... _, y_hat = rtree.predict(X_test)<br/>... accuracy_score(y_test, y_hat)<br/>Out[18]: 0.94</pre>
<p>With this configuration, we get 94% accuracy!</p>
<p>Here, we tried to improve the performance of our model through creative trial and error: we varied some of the parameters we deemed important and observed the resulting change in performance until we found a configuration that satisfied our expectations. We will learn more sophisticated techniques for improving a model in <a href="904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml" target="_blank">Chapter 11</a>, <em>Selecting the Right Model with Hyperparameter Tuning</em>.</p>
<p>Another interesting use case of decision tree ensembles is AdaBoost.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing AdaBoost</h1>
                
            
            
                
<p>When the trees in the forest are trees of depth 1 (also known as <strong>decision stumps</strong>) and we perform boosting instead of bagging, the resulting algorithm is called <strong>AdaBoost</strong>.</p>
<p>AdaBoost adjusts the dataset at each iteration by performing the following actions:</p>
<ul>
<li>Selecting a decision stump</li>
<li>Increasing the weighting of cases that the decision stump labeled incorrectly while reducing the weighting of correctly labeled cases</li>
</ul>
<p>This iterative weight adjustment causes each new classifier in the ensemble to prioritize training the incorrectly labeled cases. As a result, the model adjusts by targeting highly weighted data points.</p>
<p>Eventually, the stumps are combined to form a final classifier.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing AdaBoost in OpenCV</h1>
                
            
            
                
<p>Although OpenCV provides a very efficient implementation of AdaBoost, it is hidden under the Haar cascade classifier. Haar cascade classifiers are a very popular tool for face detection, which we can illustrate through the example of the Lena image:</p>
<pre>In [1]: img_bgr = cv2.imread('data/lena.jpg', cv2.IMREAD_COLOR)<br/>...     img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)</pre>
<p>After loading the image in both color and grayscale, we load a pretrained Haar cascade:</p>
<pre>In [2]: import cv2<br/>...     filename = 'data/haarcascade_frontalface_default.xml'<br/>...     face_cascade = cv2.CascadeClassifier(filename)</pre>
<p>The classifier will then detect faces present in the image using the following function call:</p>
<pre>In [3]: faces = face_cascade.detectMultiScale(img_gray, 1.1, 5)</pre>
<p>Note that the algorithm operates only on grayscale images. That's why we saved two pictures of Lena, one to which we can apply the classifier (<kbd>img_gray</kbd>), and one on which we can draw the resulting bounding box (<kbd>img_bgr</kbd>):</p>
<pre class="mce-root">In [4]: color = (255, 0, 0)<br/>...     thickness = 2<br/>...     for (x, y, w, h) in faces:<br/>...         cv2.rectangle(img_bgr, (x, y), (x + w, y + h),<br/>...                       color, thickness)</pre>
<p>Then, we can plot the image using the following code:</p>
<pre>In [5]: import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>...     plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB));</pre>
<p>This results in the following output, with the location of the face indicated by a blue bounding box:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1001 image-border" src="img/1e4d1b54-f99e-423a-a8fd-e09aefbc989c.png" style="width:27.42em;height:26.92em;" width="535" height="524"/></p>
<p>Obviously, this screenshot contains only a single face. However, the preceding code will work even on images where multiple faces could be detected. Try it out!</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing AdaBoost in scikit-learn</h1>
                
            
            
                
<p>In scikit-learn, AdaBoost is just another ensemble estimator. We can create an ensemble from 50 decision stumps, as follows:</p>
<pre>In [6]: from sklearn.ensemble import AdaBoostClassifier...     ada = AdaBoostClassifier(n_estimators=50,...                              random_state=456)</pre>
<p>We can load the breast cancer set once more and split it 75-25:</p>
<pre>In [7]: from sklearn.datasets import load_breast_cancer...     cancer = load_breast_cancer()...     X = cancer.data...     y = cancer.targetIn [8]: from sklearn.model_selection import train_test_split...     X_train, X_test, y_train, y_test = train_test_split(...         X, y, random_state=456...     )</pre>
<p>Then, <kbd>fit</kbd> and <kbd>score</kbd> AdaBoost using the familiar procedure:</p>
<pre>In [9]: ada.fit(X_train, y_train)...     ada.score(X_test, y_test)</pre></div>



  
<div><h1 class="header-title">Combining different models into a voting classifier</h1>
                
            
            
                
<p>So far, we have seen how to combine different instances of the same classifier or regressor into an ensemble. In this chapter, we are going to take this idea a step further and combine conceptually different classifiers into what is known as a <strong>voting classifier</strong>.</p>
<p>The idea behind voting classifiers is that the individual learners in the ensemble don't necessarily need to be of the same type. After all, no matter how the individual classifiers arrived at their prediction, in the end, we are going to apply a decision rule that integrates all the votes of the individual classifiers. This is also known as a <strong>voting scheme</strong>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding different voting schemes</h1>
                
            
            
                
<p>Two different voting schemes are common among voting classifiers:</p>
<ul>
<li>In <strong>hard voting</strong> (also known as <strong>majority voting</strong>), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels.</li>
<li>In <strong>soft voting</strong>, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then, the target label with the greatest sum of weighted probabilities wins the vote.</li>
</ul>
<p>For example, let's assume we have three different classifiers in the ensemble that perform a ...</p></div>



  
<div><h1 class="header-title">Implementing a voting classifier</h1>
                
            
            
                
<p>Let's look at a simple example of a voting classifier that combines three different algorithms:</p>
<ul>
<li>A logistic regression classifier from <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em></li>
<li>A Gaussian Naive Bayes classifier from <a href="08148129-87ac-4042-944d-8e0a2bbbe0c5.xhtml" target="_blank">Chapter 7</a>, <em>Implementing a Spam Filter with Bayesian Learning</em></li>
<li>A random forest classifier from this chapter</li>
</ul>
<p>We can combine these three algorithms into a voting classifier and apply it to the breast cancer dataset with the following steps:</p>
<ol>
<li>Load the dataset, and split it into training and test sets:</li>
</ol>
<pre style="padding-left: 60px">In [1]: from sklearn.datasets import load_breast_cancer<br/>...     cancer = load_breast_cancer()<br/>...     X = cancer.data<br/>...     y = cancer.target<br/>In [2]: from sklearn.model_selection import train_test_split<br/>...     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)</pre>
<ol start="2">
<li>Instantiate the individual classifiers:</li>
</ol>
<pre style="padding-left: 60px"> In [3]: from sklearn.linear_model import LogisticRegression<br/>...     model1 = LogisticRegression(random_state=13)<br/> In [4]: from sklearn.naive_bayes import GaussianNB<br/>...     model2 = GaussianNB()<br/>In [5]: from sklearn.ensemble import RandomForestClassifier<br/>...     model3 = RandomForestClassifier(random_state=13)</pre>
<ol start="3">
<li>Assign the individual classifiers to the voting ensemble. Here, we need to pass a list of tuples (<kbd>estimators</kbd>), where every tuple consists of the name of the classifier (a string of letters depicting a short name of each classifier) and the model object. The voting scheme can be either <kbd>voting='hard'</kbd> or <kbd>voting='soft'</kbd>. For now, we will choose<strong> <kbd>voting='hard'</kbd>:</strong></li>
</ol>
<pre style="padding-left: 60px">In [6]: from sklearn.ensemble import VotingClassifier<br/>...     vote = VotingClassifier(estimators=[('lr', model1),<br/>...                                ('gnb', model2),('rfc', model3)],voting='hard')</pre>
<ol start="4">
<li>Fit the ensemble to the training data and score it on the test data:</li>
</ol>
<pre style="padding-left: 60px">In [7]: vote.fit(X_train, y_train)<br/>...     vote.score(X_test, y_test)<br/>Out[7]: 0.95104895104895104</pre>
<p>In order to convince us that 95.1% is a great accuracy score, we can compare the ensemble's performance to the theoretical performance of each individual classifier. We do this by fitting the individual classifiers to the data. Then, we will see that the logistic regression model achieves 94.4% accuracy on its own:</p>
<pre>In [8]: model1.fit(X_train, y_train)<br/>...     model1.score(X_test, y_test)<br/>Out[8]: 0.94405594405594406</pre>
<p>Similarly, the Naive Bayes classifier achieves 93.0% accuracy:</p>
<pre>In [9]:  model2.fit(X_train, y_train)<br/>...      model2.score(X_test, y_test)<br/>Out[9]:  0.93006993006993011</pre>
<p>Last but not least, the random forest classifier also achieved 94.4% accuracy:</p>
<pre>In [10]: model3.fit(X_train, y_train)<br/>... model3.score(X_test, y_test)<br/>Out[10]: 0.94405594405594406</pre>
<p>All in all, we were just able to gain a good percentage in performance by combining three unrelated classifiers into an ensemble. Each of these classifiers might have made different mistakes on the training set, but that's OK because, on average, we need just two out of three classifiers to be correct.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Plurality</h1>
                
            
            
                
<p>In the previous sections, we discussed ensemble methods. What we didn't mention earlier was how the results are aggregated across the individual models prepared by the ensemble techniques. The concept that is used for this is called <strong>plurality</strong>,<strong> </strong>which is nothing but voting. The higher the vote a class gets, the higher the chances of it being the final class. Imagine if we had three models prepared during ensemble techniques and 10 possible classes (think of them as digits from 0 to 9). Each model would choose one class based on the highest probability it obtained. Finally, the class with the maximum number of votes would be selected. This is the concept of plurality. In practice, plurality tries to bring benefit to both <em>k-</em>NN and Naive ...</p></div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we talked about how to improve various classifiers by combining them into an ensemble. We discussed how to average the predictions of different classifiers using bagging and how to have different classifiers correct each other's mistakes using boosting. A lot of time was spent discussing all possible ways to combine decision trees, be it decision stumps (AdaBoost), random forests, or extremely randomized trees. Finally, we learned how to combine even different types of classifiers in an ensemble by building a voting classifier.</p>
<p>In the next chapter, we will talk more about how to compare the results of different classifiers by diving into the world of model selection and hyperparameter tuning.</p>


            

            
        
    </div>



  </body></html>