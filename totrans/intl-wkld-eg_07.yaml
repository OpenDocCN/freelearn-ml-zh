- en: '*Chapter 5*: Ingesting and Streaming Data from the Edge'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Edge computing** can reduce the amount of data transferred to the cloud (or
    on-premises datacenter), thus saving on network bandwidth costs. Often, high-performance
    edge applications require local compute, storage, network, data analytics, and
    machine learning capabilities to process high-fidelity data in low latencies.
    AWS extends infrastructure to the edge, beyond **Regions** and **Availability
    Zones**, as close to the endpoint as required by your workload. As you will have
    learned in previous chapters, **AWS IoT Greengrass** allows you to run sophisticated
    edge applications on devices and gateways.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the different data design and transformation
    strategies applicable for edge workloads. We will explain how you can ingest data
    from different sensors through different workflows based on **data velocity**
    (such as hot, warm, and cold), **data variety** (such as structured and unstructured),
    and **data volume** (such as high frequency or low frequency) on the edge. Thereafter,
    you will learn the approaches of streaming the raw and transformed data from the
    edge to different cloud services. By the end of this chapter, you should be familiar
    with data processing using AWS IoT Greengrass.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Defining data models for IoT workloads
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing data patterns for the edge
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to know Stream Manager
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your first data orchestration workflow on the edge
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming from the edge to a data lake on the cloud
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technical requirements for this chapter are the same as those outlined in
    [*Chapter 2*](B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032)*, Foundations of
    Edge Workloads*. See the full requirements in that chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the GitHub code repository here: [https://github.com/PacktPublishing/Intelligent-Workloads-at-the-Edge/tree/main/chapter5](https://github.com/PacktPublishing/Intelligent-Workloads-at-the-Edge/tree/main/chapter5)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Defining data models for IoT workloads
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the IDC, the sum of the world's data will grow from 33 **zettabytes**
    (**ZB**) in 2018 to 175 ZB by 2025\. Additionally, the IDC estimates that there
    will be 41.6 billion connected IoT devices or *things*, generating 79.4 ZB of
    data in 2025 ([https://www.datanami.com/2018/11/27/global-datasphere-to-hit-175-zettabytes-by-2025-idc-says/](https://www.datanami.com/2018/11/27/global-datasphere-to-hit-175-zettabytes-by-2025-idc-says/)).
    Additionally, many other sources reiterate that data and information are the *currency*,
    the *lifeblood*, and even the *new oil* of the information industry.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the data-driven economy is here to stay and the **Internet of Things**
    (**IoT**) will act as the enabler to ingest data from a huge number of devices
    (or endpoints), such as sensors and actuators, and generate aggregated insights
    for achieving business outcomes. Thus, as an IoT practitioner, you should be comfortable
    with the basic concepts of **data modeling** and how that enables **data management**
    on the edge.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据驱动型经济将长期存在，**物联网**（**IoT**）将作为推动者，从大量设备（或端点）如传感器和执行器中获取数据，并生成汇总见解以实现业务成果。因此，作为一名物联网从业者，你应该熟悉**数据建模**的基本概念以及它是如何实现边缘**数据管理**的。
- en: All organizations across different verticals such as industrial, commercial,
    consumer, transportation, energy, healthcare, and others are exploring new use
    cases to improve their top line or bottom line and innovate on behalf of their
    customers. IoT devices such as a connected hub in a consumer home, a smart parking
    meter on a road, or a connected car will coexist with customers and will operate
    even when there is no connectivity to the internet.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有行业，如工业、商业、消费、交通、能源、医疗保健等，都在探索新的用例以提高其收入或利润，并为顾客创新。物联网设备，如消费者家庭中的连接中心、道路上的智能停车计费器或连接汽车，将与客户共存，即使在没有互联网连接的情况下也能运行。
- en: This is a paradigm shift from the centralized solutions that worked for enterprises
    in the past. For example, a banking employee might have hosted their workloads
    in datacenters, but now they can monitor customer activities (such as suspicious
    actions, footfalls, or availability of cash in an ATM) at their branch locations
    in near real time to serve customers better. Therefore, a new strategy is required
    to act on the data generated locally and be able to process and stream data from
    the edge to the cloud.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从过去为企业工作的集中式解决方案到的一种范式转变。例如，一位银行员工可能将他们的工作负载托管在数据中心，但现在他们可以在其分支机构实时监控客户活动（如可疑行为、人流量或ATM中的现金可用性），以更好地服务客户。因此，需要一种新的策略来处理本地生成数据，并能够从边缘处理和流式传输数据到云端。
- en: In this chapter, we are going to rethink and re-evaluate the applicability of
    different big data architectures in the context of IoT and edge computing. The
    three areas we will consider are data management, data architecture patterns,
    and anti-patterns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重新思考和评估不同大数据架构在物联网和边缘计算环境下的适用性。我们将考虑的三个领域是数据管理、数据架构模式和反模式。
- en: What is data management?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是数据管理？
- en: As per the **Data Management Body of Knowledge** (**DMBOK2**) from the **Data
    Management Association** (**DAMA**), data management is the development, execution,
    and supervision of plans, policies, programs, and practices that deliver, control,
    protect, and enhance the value of data and information assets throughout their
    life cycles (for more information, please refer to *DAMA-DMBOK2* at [https://technicspub.com/dmbok/](https://technicspub.com/dmbok/)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**数据管理协会**（**DAMA**）的**数据管理知识体系**（**DMBOK2**），数据管理是制定、执行和监督计划、政策、项目和做法，以在整个生命周期中交付、控制、保护和增强数据和信息资产的价值（更多信息，请参阅[DAMA-DMBOK2](https://technicspub.com/dmbok/)）。
- en: 'DAMA covers the data management framework in great detail, as shown in the
    following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DAMA详细介绍了数据管理框架，如下所示：
- en: '![Figure 5.1 – The data management life cycle'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 数据管理生命周期]'
- en: '](img/B17595_05_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B17595_05_001.jpg]'
- en: Figure 5.1 – The data management life cycle
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 数据管理生命周期]'
- en: Here, we recognized an opportunity to augment the framework from DAMA with concepts
    that are relevant to edge computing. Therefore, in this section, we will dive
    deeper into the principles related to data modeling, data architecture, and **Data
    Integration and Interoperability** (**DII**), which we think are relevant for
    edge computing and IoT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们认识到一个机会，即通过边缘计算的相关概念来增强DAMA框架。因此，在本节中，我们将更深入地探讨与数据建模、数据架构和**数据集成与互操作性**（**DII**）相关的原则，我们认为这些原则与边缘计算和物联网相关。
- en: 'Let''s define data in the context of IoT before we discuss how to model it.
    IoT data is generated from different sensors, actuators, and gateways. Therefore,
    they can come in different forms such as the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何建模数据之前，让我们先定义物联网环境下的数据。物联网数据来自不同的传感器、执行器和网关。因此，它们可以以不同的形式出现，如下所示：
- en: '**Structured data**: This refers to a predictable form of data; examples include
    device metadata and device relationships.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-structured data**: This is a form of data with a certain degree of variance
    and randomness; examples include sensor and actuator feeds.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unstructured data**: This is a form of data with a higher degree of variance
    and randomness; examples include raw images, audio, or videos.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's discuss how the different forms of data can be governed, organized,
    and stored using data modeling techniques.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: What is data modeling?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data modeling** is a common practice in software engineering, where data
    requirements are defined and analyzed to support the business processes of different
    information systems in the scope of the organization. There are three different
    types of data models:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Conceptual data models
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical data models
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical data models
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, the relationships between different modeling approaches
    are presented:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Data modeling approaches'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_002.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Data modeling approaches
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The Enigma machine was used by the German military as the primary mode of communication
    for all secure wireless communications during World War II. Alan Turing cracked
    the Enigma code roughly 80 years ago when he figured out the text that's placed
    at the end of every message. This helped to decipher key secret messages from
    the German military and helped end the world war. Additionally, this mechanism
    led to the era of unlocking insights by defining a language to decipher data,
    which was later formalized as data modeling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common data modeling technique for a database is an **Entity-Relationship**
    (**ER**) model. An ER model in software engineering is a common way to represent
    the relationship between different entities such as people, objects, places, events,
    and more in a graphical way in order to organize information better to drive the
    business processes of an organization. In other terms, an ER model is an abstract data
    model that defines a data structure for the required information and can be independent
    of any specific database technology. In this section, we will explain the different
    data models using the ER model approach. First, let''s define, with the help of
    a use case diagram, the relationship between a customer and their devices associated
    with a connected HBS hub:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – A use case diagram'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_003.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – A use case diagram
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s build the ER diagram through a series of conceptual, logical, and
    physical models:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The **conceptual data model** defines the entities and their relationships.
    This is the first step of the data modeling exercise and is used by personas such
    as data architects to gather the initial set of requirements from the business
    stakeholders. For example, *sensor*, *device*, and *customer* are three entities
    in a relationship, as shown in the following diagram:![Figure 5.4 – A conceptual
    data model
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17595_05_004.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.4 – A conceptual data model
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The conceptual model is then rendered into a **logical data model**. In this
    step of data modeling, the data structure along with additional properties are
    defined using a conceptual model as the foundation.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, you can define the properties of the different entities in the
    relationship such as the sensor type or device identifier (generally, a serial
    number or a MAC address), as shown in the following list. Additionally, there
    could be different forms of relationships, such as the following:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Association is the relationship between devices and sensors.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ownership is the relationship between customers and devices.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding points are illustrated in the following diagram:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – A logical data model'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17595_05_005.jpg)'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.5 – A logical data model
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final step in data modeling is to build a **physical data model** from
    the defined logical model. A physical model is often a technology or product-specific
    implementation of the data model. For example, you define the data types for the
    different properties of an entity, such as a number or a string, that will be
    deployed on a database solution from a specific vendor:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – A physical data model'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – A physical data model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Enterprises have used ER modeling for decades to design and govern complex
    distributed data solutions. All the preceding steps can be visualized as the following
    workflow, which is not limited to any specific technology, product, subject area,
    or operating environment (such as a data center, cloud, or edge):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The data modeling flow'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_007.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – The data modeling flow
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have understood the foundations of data modeling, in the next section,
    let's examine how this can be achieved for IoT workloads.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: How do you design data models for IoT?
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's take a look at some examples of how to apply the preceding data modeling
    concepts to the realm of structured, unstructured, and semi-structured data that
    are common with IoT workloads. Generally, when we refer to data modeling for structured
    data, a **relational database** (**RDBMS**) comes to mind first. However, for
    most IoT workloads, structured data generally includes hierarchical relationships
    between a device and other entities. And that is better illustrated using a graph
    or an ordered key-value database. Similarly, for semi-structured data, when it
    comes to IoT workloads, it's mostly illustrated as a key-value, time series, or
    document store.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will give you a glimpse of data modeling techniques using
    NoSQL data solutions to continue building additional functionalities for HBS.
    Modeling an RDBMS is outside the scope of this book. However, if you are interested
    in learning about them, there are tons of materials available on the internet
    that you can refer to.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'NoSQL databases are designed to offer freedom to developers to break away from
    a longer cycle of database schema designs. However, it''s a mistake to assume
    that NoSQL databases lack any sort of data model. Designing a NoSQL solution is
    quite different from an RDBMS design. For RDBMS, developers generally create a
    data model that adheres to normalization guidelines, without focusing on access
    patterns. The data model can be modified later when new business requirements
    arise, thus leading to a lengthy release cycle. The collected data is organized
    in different tables with rows, columns, and referential integrities. In contrast,
    for a NoSQL solution design, developers cannot begin designing the models until
    they know the questions that are required to be answered. Understanding the business
    queries working backward from the use case is quintessential. Therefore, the general
    rule of thumb to remember during data modeling through a relational or NoSQL database
    is the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Relational modeling primarily cares about the structure of data. The design
    principle is *What answers do I get?*
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL modeling primarily cares about application-specific access patterns. The
    design principle is *What questions do I ask?*
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The common translation of the NoSQL acronym is *Not only SQL*. This highlights
    the fact that NoSQL doesn't only support NoSQL, but it can handle relational,
    semi-structured, and unstructured data. Organizations such as Amazon, Facebook,
    Twitter, LinkedIn, and Google have designed different NoSQL technologies.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before I show you some examples of data modeling, let''s understand the five
    fundamental properties of our application''s (that is, the HBS hub) access patterns
    that need to be considered in order to come up with relevant questions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**Data type**: What''s the type of data in scope? For example, is the data
    related to telemetry, command-control, or critical events? Let''s quickly refresh
    the use of each of these data types:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a) **Telemetry**: This is a constant stream of data transmitted by sensors/actuators,
    such as temperature or humidity readings , which can be aggregated on the edge
    or published as it is to the cloud for further processing.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) **Command and Control**: These are actionable messages, such as turning
    on/off the lights, which can occur between two devices or between an end user
    and the device.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) **Events**: These are data patterns that identify more complex scenarios
    than regular telemetry data, such as network outages in a home or a fire alarm
    in a building.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data size**: What is the quantity of data in scope? Is it necessary to store
    and retrieve data locally (on the edge), or does the data require transmission
    to a different data persistence layer (such as a data lake on the cloud)?'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data shape**: What''s the form of data being generated from different edge
    devices such as text, blobs, and images? Note that different data forms such as
    images and videos might have different computational needs (think of GPUs).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data velocity**: What''s the speed of data to process queries based on the
    required latencies? Do you have a hot, warm, or cold path of data?'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: How much of this data needs to have strong versus eventual
    consistency?'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answering the preceding questions will help you to determine whether the solution
    should be based on one of the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**BASE methodology**: **Basically Available**, **Soft-state**, **Eventual consistency**,
    which are typical characteristics of NoSQL databases'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ACID methodology**: **Atomicity**, **Consistency**, **Isolation**,and **Durability**,
    which are typical characteristics of relational databases'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these concepts in more detail next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Selecting between ACID or BASE for IoT workloads
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table lists some of the key differences between the two methodologies.
    This should enable you to make an informed decision working backward from your
    use case:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – ACID versus BASE summary'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_008.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – ACID versus BASE summary
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'ACID and BASE represent opposing sides of the pH spectrum. Jim Grey conceived
    the idea in 1970 and subsequently published a paper, called *The Transaction Concept:
    Virtues and Limitations*, in June 1981\.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have understood the fundamentals of data modeling and design approaches.
    You must be curious about how to relate those concepts to the connected HBS product,
    which you have been developing in earlier chapters. Let's explore how the rubber
    meets the road.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Do you still remember the **first phase** of data modeling?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Bingo! Conceptual it is.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Conceptual modeling of the connected HBS hub
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram is a hypothetical conceptual model of the HBS hub:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – A conceptual data model for connected HBS'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_009.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – A conceptual data model for connected HBS
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you can observe how the different devices such as
    lights, HVAC, and washing machines are installed in different rooms of a house.
    Now the conceptual model is in place, let's take a look at the logical view.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The logical modeling of the connected HBS hub
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build the logical model, we need to ask ourselves the type of questions
    an end consumer might ask, such as the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Show the status of a device (such as is the washing complete?).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn a device on or off (such as turn off the lights).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show the readings of a device (such as what's the temperature now?).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a new reading (such as how much energy is being consumed by the refrigerator?).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show the aggregated connectivity status of a device (or devices) for a period.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address these questions, let''s determine the access patterns for our end
    application:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – A logical data model for connected HBS'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_Table_1.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – A logical data model for connected HBS
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have captured the summary of our data modeling requirements, you
    can observe that the solution needs to ingest data in both structured and semi-structured
    formats at high frequency. Additionally, it doesn't require strong consistency.
    Therefore, it makes sense to design the data layer using a NoSQL solution that
    leverages the BASE methodology.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经捕捉到了数据建模需求的总览，您可以看到解决方案需要以高频率摄取结构化和半结构化格式的数据。此外，它不需要强一致性。因此，使用利用 BASE
    方法的 NoSQL 解决方案来设计数据层是有意义的。
- en: The physical modeling of the connected HBS hub
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接的 HBS 中心物理建模
- en: As the final step, we need to define the physical data model from the gathered
    requirements. To do that, we will define a primary and a secondary key. You are
    not required to define all of the attributes if they're not known to you, which
    is a key advantage of a NoSQL solution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们需要根据收集到的需求定义物理数据模型。为此，我们将定义一个主键和一个二级键。如果您不知道所有属性，您不需要定义所有属性，这是 NoSQL
    解决方案的一个关键优势。
- en: Defining the primary key
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义主键
- en: As the name suggests, this is one of the required attributes in a table, which
    is often known as a **Partition key**. In a table, no two primary keys should
    have the same value. There is also a concept of a **Composite key**. It's composed
    of two attributes, a partition key and a **Sort key**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，这是表中的一个必需属性，通常被称为**分区键**。在一个表中，没有两个主键应该有相同的值。还有一个**复合键**的概念。它由两个属性组成，一个分区键和一个**排序键**。
- en: 'In our scenario, we will create a `Sensor` table with a composite key (as depicted
    in the following screenshot). The primary key is a **device identifier**, and
    the sort key is a timestamp that enables us to query data in a time range:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，我们将创建一个具有复合键（如图所示）的`Sensor`表。主键是一个**设备标识符**，排序键是一个时间戳，它使我们能够查询时间范围内的数据：
- en: '![Figure 5.11 – Composite keys in a sensor table'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11 – 传感器表中的复合键'
- en: '](img/B17595_05_011.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17595_05_011.jpg)'
- en: Figure 5.11 – Composite keys in a sensor table
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 传感器表中的复合键
- en: Defining the secondary indexes
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义二级索引
- en: A **secondary index** allows us to query the data in the table using a different
    key, in addition to queries against the primary or composite keys. This gives
    your applications more flexibility in querying the data for different use cases.
    Performing a query using a secondary index is pretty similar to querying from
    the table directly.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**二级索引**允许我们使用不同的键查询表中的数据，除了对主键或复合键的查询。这为您的应用程序在查询不同用例的数据时提供了更多的灵活性。使用二级索引进行查询与直接从表中查询非常相似。
- en: 'Therefore, for secondary indexes, as shown in the following chart, we have
    selected the primary key as a sensor identifier (`sensor_id`) along with timestamp
    as the sort key:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于二级索引，如以下图表所示，我们选择了主键作为传感器标识符（`sensor_id`）以及时间戳作为排序键：
- en: '![Figure 5.12 – Secondary indexes in a sensor table'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12 – 传感器表中的二级索引'
- en: '](img/B17595_05_012.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17595_05_012.jpg)'
- en: Figure 5.12 – Secondary indexes in a sensor table
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 传感器表中的二级索引
- en: Defining the additional attributes
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义额外的属性
- en: 'The key advantage of a NoSQL solution is that there is no enforced schema.
    Therefore, other attributes can be created on the fly as data comes in. That being
    said, if some of the attributes are already known to the developer, there is no
    restriction to include those in the data model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL 解决方案的关键优势是没有强制性的模式。因此，可以在数据到来时动态创建其他属性。话虽如此，如果开发人员已经知道一些属性，则没有限制将它们包含在数据模型中：
- en: '![Figure 5.13 – Other attributes in a sensor table'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13 – 传感器表中的其他属性'
- en: '](img/B17595_05_013.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17595_05_013.jpg)'
- en: Figure 5.13 – Other attributes in a sensor table
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 传感器表中的其他属性
- en: Now that the data layer exists, let's create the interfaces to access this data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据层已经存在，让我们创建访问这些数据的接口。
- en: Defining the interfaces
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义接口
- en: 'Now we will create two different facets for the sensor table. A **facet** is
    a virtual construct that enables different views of the data stored in a table.
    The facets can be mapped to a functional construct such as a method or an API
    for performing various **Create, Read, Update, Delete** (**CRUD**) operations
    on a table:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将为传感器表创建两个不同的方面。**方面**是一个虚拟结构，它使您能够以不同的视图查看存储在表中的数据。方面可以映射到功能结构，如方法或 API，以执行对表的各种**创建、读取、更新、删除**（**CRUD**）操作：
- en: '`putItems`: This facet allows write operations and requires the composite keys
    at the minimum in the payload.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`putItems`：此维度允许写入操作，并在有效载荷中至少需要复合键。'
- en: '`getItems`: This facet allows read operations that can query items with all
    or selective attributes.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getItems`：此维度允许查询具有所有或选择性属性的项的读取操作。'
- en: 'The following screenshot depicts the `getItems` facet definition:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了`getItems`维度的定义：
- en: '![Figure 5.14 – The getItems facet definition'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.14 – `getItems`维度的定义'
- en: '](img/B17595_05_014.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17595_05_014.jpg](img/B17595_05_014.jpg)'
- en: Figure 5.14 – The getItems facet definition
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – `getItems`维度的定义
- en: So, now you have created the data model along with its interfaces. This enables
    you to understand the data characteristics that are required to develop edge applications.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在你已经创建了数据模型及其接口。这使你能够理解开发边缘应用所需的数据特性。
- en: Designing data patterns on the edge
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在边缘设计数据模式
- en: As data flows securely from different sensors/actuators on the edge to the gateway
    or cloud over different protocols or channels, it is necessary for it to be safely
    stored, processed, and cataloged for further consumption. Therefore, any IoT data
    architecture needs to take into consideration the data models (as explained earlier),
    data storage, data flow patterns, and anti-patterns, which will be covered in
    this section. Let's start with data storage.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据从边缘的不同传感器/执行器通过不同的协议或通道安全地流向网关或云端时，它需要被安全存储、处理和编目以供进一步消费。因此，任何物联网数据架构都需要考虑数据模型（如前所述）、数据存储、数据流模式以及反模式，这些内容将在本节中介绍。让我们从数据存储开始。
- en: Data storage
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储
- en: Big data solutions on the cloud are designed to reliably store terabytes, petabytes,
    or exabytes of data and can scale across multiple geographic locations globally
    to provide high availability and redundancy for businesses to meet their **Recovery
    Time Objective** (**RTO**) and **Recovery Point Objective** (**RPO**). However,
    edge solutions, such as our very own connected HBS hub solution, are resource-constrained
    in terms of compute, storage, and network. Therefore, we need to design the edge
    solution to cater to different time-sensitive, low-latency use cases and hand
    off the heavy lifting to the cloud. A **data lake** is a well-known pattern on
    the cloud today, which allows a centralized repository to store data as it arrives,
    without having to first structure the data. Thereafter, different types of analytics,
    machine learning, and visualizations can be performed on that data for consumers
    to achieve better business outcomes. So, what is the equivalent of a data lake
    for the edge?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 云端的大数据解决方案旨在可靠地存储千兆字节、太字节或艾字节的数据，并能够跨全球多个地理位置进行扩展，以提供高可用性和冗余，以满足企业满足其**恢复时间目标**（**RTO**）和**恢复点目标**（**RPO**）。然而，边缘解决方案，如我们自己的连接HBS中心解决方案，在计算、存储和网络方面资源有限。因此，我们需要设计边缘解决方案以适应不同的时效性、低延迟用例，并将重负载交给云端。**数据湖**是云端的知名模式，它允许集中存储到达的数据，而无需首先对数据进行结构化。之后，可以对数据进行不同类型的分析、机器学习和可视化，以帮助消费者实现更好的业务成果。那么，边缘的数据湖是什么？
- en: 'Let''s introduce a new pattern, called a *data pond*, for the authoritative
    source of data (that is, the golden source) that is generated and temporarily
    persisted on the edge. Certain characteristics of a data pond are listed as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一种新的模式，称为*数据池塘*，用于在边缘生成并临时持久化的数据权威来源（即，金数据源）。以下列出了数据池塘的一些特性：
- en: A data pond enables the quick ingestion and consumption of data in a fast and
    flexible fashion. A data producer is only required to know where to push the data,
    that is, the local storage, local stream, or cloud. The choice of the storage
    layer, schema, ingestion frequency, and quality of the data is left to the data
    producer.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据池塘能够快速并以灵活的方式摄取和消费数据。数据生产者只需要知道将数据推送到哪里，即本地存储、本地流或云端。存储层、模式、摄取频率和数据质量的选择留给数据生产者。
- en: A data pond should work with low-cost storage. Generally, IoT devices are low
    in storage; therefore, only highly valuable data that's relevant for the edge
    operations can be persisted locally. The rest of the data is pushed to the cloud
    for additional processing or thrown away (if noisy).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据池塘应该与低成本存储一起工作。通常，物联网设备存储空间较小；因此，只有与边缘操作高度相关的数据才能在本地持久化。其余数据被推送到云端进行进一步处理或丢弃（如果数据噪声大）。
- en: A data pond supports schema on read. There can be multiple streams supporting
    multiple schemas in a data pond.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pond should support the data protection mechanisms at rest and in encryption.
    It's also useful to implement role-based access that allows auditing the data
    trail as it flows from the edge to the cloud.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows an edge architecture of how data collected from
    different sensors/actuators can be persisted and securely governed in a data pond:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – A data pond architecture at the edge'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_015.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – A data pond architecture at the edge
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The organizational entities involved in the preceding data flow include the
    following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '**Data producers**: These are entities that generate data. These include physical
    (such as sensors, actuators, or associated devices) or logical (such as applications)
    entities and are configured to store data in the data pond or publish data to
    the cloud.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data pond team**: Generally, the data operations team defines the data access
    mechanisms for the data pond (or lake) and the development team supports data
    management.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consumers**: Edge and cloud applications retrieve data from the data
    pond (or lake) using the mechanisms authorized to further iterate on the data
    and meet business needs.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the organizational entities for the data pond:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – The organizational entities for the data pond'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_016.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – The organizational entities for the data pond
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Now you have understood how data can be stored on a data pond and be managed
    or governed by different entities. Next, let's discuss the different flavors of
    data and how they can be integrated.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Data integration concepts
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DII occurs through different layers, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch**: This layer aggregates data that has been generated by data producers.
    The goal is to increase the accuracy of data insights through the consolidation
    of data from multiple sources or dimensions.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed**: This layer streams data generated by data producers. The goal is
    to allow a near real-time analysis of data with an acceptable level of accuracy.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving**: This layer merges the data from the batch layer and the speed
    layer to enable the downstream consumers or business users with holistic and incremental
    insights into the business.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of DII:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 -- Data Integration and Interoperability (DII)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_017.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 -- Data Integration and Interoperability (DII)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are multiple layers within the data flow that are commonly
    implemented using the **Extract, Transform, and Load** (**ETL**) methodology or
    the **Extract, Load, and Transform** (**ELT**) methodology in the big data world.
    The ETL methodology involves steps to extract data from different sources, implement
    data quality and consistency standards, transform (or aggregate) the data to conform
    to a standard format, and load (or deliver) data to downstream applications.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The ELT process is a variant of ETL with similar steps. The difference is that
    extracted data is loaded before the transformation. This is common for edge workloads
    as well, where the local gateway might not have enough resources to do the transformation
    locally; therefore, it publishes the data prior to additional processing.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: But how are these data integration patterns used in the edge? Let's explore
    this next.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Data flow patterns
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An ETL flow on the edge will include three distinct steps, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Data extraction from devices such as sensors/actuators
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data transformation to clean, filter, or restructure data into an optimized
    format for further consumption
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data loading to publish data to the persistence layer such as a data pond, a
    data lake, or a data warehouse
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For an ELT flow, *steps 2* and *3* will take place in the reverse order.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: An ETL Scenario for a Connected Home
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a connected home scenario, it's common to extract data from
    different sensors/actuators, followed by a data transformation that might include
    format changes, structural changes, semantic conversions, or deduplication. Additionally,
    data transformation allows you to filter out any noisy data from the home (think
    of a crying baby or a noisy pet), resulting in reduced network charges of publishing
    all the bits and bytes to the cloud. Based on a use case such as an intrusion
    alert or replenishing a printer toner, data transformation can be performed in
    batch or real time, by eitherphysically storing the result in a staging area or
    virtually storing the transferred data in memory until you are ready to move to
    the load step.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: These core patterns (ETL or ELT) have evolved, with time, into different data
    flow architectures, such as event-driven, batch, lambda, and**complex event processing**
    *(***CEP***)*. We will explain each of them in the next section.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven (or streaming)
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's very common for edge applications to generate and process data in smaller
    sets throughout the day when an event happens. Near real-time data processing
    has a lower latency and can be both synchronous and asynchronous.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: In an asynchronous data flow, the devices (such as sensors) do not wait for
    the receiving system to acknowledge updates before continuing processing. For
    example, in a connected home, a motion/occupancy sensor can trigger an intruder
    notification based on a detected event but continue to monitor without waiting
    for an acknowledgment.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, in a real-time synchronous data flow, no time delay or other
    differences between source and target are acceptable. For example, in a connected
    home, if there is a fire alarm, it should notify the emergency services in a deterministic
    way.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'With AWS Greengrass, you can design both **synchronous** and **asynchronous**
    data communications. In addition to this, as we build multi-faceted architectures
    on the edge, it''s quite normal to build multiprocessing or multithreaded polyglot
    solutions on the edge to support different low-latency use cases:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Event-driven architecture at the edge'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_018.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – Event-driven architecture at the edge
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Micro-batch (or aggregated processing)
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most enterprises perform frequent batch processing to enable end users with
    business insights. In this mode, data moving will represent either the full set
    of data at a given point in time, such as the energy meter reading of a connected
    home at the end of a period (such as the day, week, or month), or data that has
    changed values since the last time it was sent, such as a hvac reading or a triggered
    fire alarm. Generally, batch systems are designed to scale and grow proportionally
    along with the data. However, that's not feasible on the edge due to the lack
    of horsepower, as explained earlier.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, for IoT use cases, leveraging micro-batch processing is more common.
    Here, the data is stored locally and is processed on a much higher frequency,
    such as every few seconds, minutes, hours, or days (over weeks or months). This
    allows data consumers to gather insights from local data sources with reduced
    latency and cost, even when disconnected from the internet. The **Stream Manager**
    capability of AWS Greengrass allows you to perform aggregated processing on the
    edge. Stream Manager brings enhanced functionalities regarding how to process
    data on the edge such as defining a bandwidth and data prioritization for multiple
    channels, timeout behavior, and direct export mechanisms to different AWS services
    such as Amazon S3, AWS IoT Analytics, AWS IoT SiteWise, and Amazon Kinesis data
    streams:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Micro-batch architecture at the edge'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_019.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – Micro-batch architecture at the edge
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Lambda architecture
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lambda architecture is an approach that combines both micro-batch and stream
    (near real-time) data processing. It makes the consolidated data available for
    downstream consumption. For example, a refrigeration unit, a humidifier, or any
    critical piece of machinery on a manufacturing plant can be monitored and fixed
    before it becomes non-operational. So, for a connected HBS hub solution, micro-batch
    processing will allow you to detect long-term trends or failure patterns. This
    capability in turn, will help your fleet operators recommend preventive or predictive
    maintenance for the machines to end consumers. This workflow is often referred
    to as the warm or cold path of the data analytics flow.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, stream processing will allow the fleet operators to derive
    near real-time insights through telemetry data. This will enable consumers to
    take mission-critical actions such as locking the entire house and calling emergency
    services if any theft is detected. This is also referred to as the hot path in
    lambda architecture:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Lambda architecture at the edge'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_020.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.20 – Lambda architecture at the edge
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Lambda architecture has nothing to do with the AWS lambda service. The term was
    coined by Nathan Marz, who worked on big-data-related technologies at *BackType*
    and *Twitter*. This is a design pattern for describing data processing that is
    scalable and fault-tolerant.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Data flow anti-patterns for the edge
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned about the common data flow patterns on the edge. Let's
    also discuss some of the anti-patterns.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Complex Event Processing (CEP)
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Events are data patterns that identify complex circumstances from ingested data,
    such as network outages in a home or a fire alarm in a building. It might be easier
    to detect events from a few sensors or devices; however, getting visibility into
    complex events from disparate sources and being able to capture states or trigger
    conditional logic to identify and resolve issues quickly requires special treatment.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: That's where the CEP pattern comes into play. CEP can be resource-intensive
    and needs to scale to all sizes of data and grow proportionally. Therefore, it's
    still not a very common pattern on the edge. On the cloud, managed services such
    as AWS IoT events or AWS EventBridge can make it easier for you to perform CEP
    on the data generated from your IoT devices.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Batch
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, in batch processing, data moves in aggregates as blobs or files
    either on an ad hoc request from a consumer or automatically on a periodic schedule.
    Data will either be a full set (referred to as snapshot) or a changed set (delta)
    from a given point in time. Batch processing requires continuous scaling of the
    underlying infrastructure to facilitate the data growth and processing requirements.
    Therefore, it's a pattern that is better suited for big data or data warehousing
    solutions on the cloud. That being said, for an edge use case, you can still leverage
    the micro-batch pattern (as explained earlier) to aggregate data that's feasible
    in the context of a resource-constrained environment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Replication
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's a common practice on the cloud to maintain redundant copies of datasets
    across different locations to improve business continuity, improve the end user
    experience, or enhance data resiliency. However, in the context of the edge, **data
    replication** can be expensive, as you might require redundant deployments. For
    example, with a connected HBS hub solution, if the gateway needs to support redundant
    storage for replication, it will increase the **bill of materials** (**BOM**)
    cost of the hardware, and you can lose the competitive edge on the market.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Archiving
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data that is used infrequently or not actively used can be moved to an alternate
    data storage solution that is more cost-effective to the organization. Similar
    to replication, for archiving data locally on the edge, additional deployment
    of hardware resource is necessary. This increases the **bill of materials** (**BOM**)
    cost of the device and leads to additional operational overhead. Therefore, it's
    common to archive the transformed data from the data lake to a cost-effective
    storage service on the cloud such as **Amazon Glacier**. Thereafter, this data
    can be used for local operations, data recovery, or regulatory needs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: A hands-on approach with the lab
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to build a lambda architecture on the edge
    using different AWS services. The following diagram shows the lambda architecture:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – The lab architecture'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_021.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.21 – The lab architecture
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding workflow uses the following services. In this chapter, you will
    complete *steps 1–6* (as shown in *Figure 5.21*). This includes designing and
    deploying the edge components, processing, and transforming data locally, and
    pushing the data to different cloud services:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: \
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – The hands-on lab components'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_022.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.22 – The hands-on lab components
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'In this hands-on section, your objective will consist of the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Build the cloud resource (that is, Amazon Kinesis data streams, Amazon S3 bucket,
    and DynamoDB tables).
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build and deploy the edge components (that is, artifacts and recipes) locally
    on Raspberry Pi.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validate that the data is streamed from the edge to the cloud (AWS IoT Core).
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building cloud resources
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploy the CloudFormation template from the `chapter5/cfn` folder to create
    cloud resources such as Amazon S3 buckets, Kinesis data streams, and DynamoDB
    tables. You will need to substitute these respective names from the *Resources*
    section of the deployed stack, when requested, in the following section.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Building edge components
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s hop on to our device to build and deploy the required edge components:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the following working directory from the Terminal of your Raspberry
    Pi device:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Open the Python script using the editor of your choice (such as *nano*, *vi*,
    or *emac*):'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'def read_value(self):'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message = {}
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: device_list = ['hvac', 'refrigerator', 'washingmachine']
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: device_name = random.choice(device_list)
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if device_name == ''hvac'' :'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_id'] = "1"
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['timestamp'] = float("%.4f" % (time()))
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_name'] = device_name
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['temperature'] = round(random.uniform(10.0, 99.0), 2)
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['humidity'] = round(random.uniform(10.0, 99.0), 2)
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'elif device_name == ''washingmachine'' :'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_id'] = "2"
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['timestamp'] = float("%.4f" % (time()))
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_name'] = device_name
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['duty_cycles'] = round(random.uniform(10.0, 99.0), 2)
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'else :'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_id'] = "3"
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['timestamp'] = float("%.4f" % (time()))
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_name'] = device_name
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['vibration'] = round(random.uniform(100.0, 999.0), 2)
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return message
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, open the following `publisher` script and navigate through the code:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TIMEOUT = 10
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ipc_client = awsiot.greengrasscoreipc.connect()
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: sensor = DummySensor()
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'while True:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message = sensor.read_value()
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message_json = json.dumps(message).encode('utf-8')
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request = PublishToTopicRequest()
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.topic = args.pub_topic
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message = PublishMessage()
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message = JsonMessage()
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message.message = message
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.publish_message = publish_message
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation = ipc_client.new_publish_to_topic()
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation.activate(request)
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future = operation.get_response()
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future.result(TIMEOUT)
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print("publish")
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: time.sleep(5)
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now you have reviewed the code, check the following recipe file to review the
    access controls and dependencies that are required by the `Publisher` component:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we have the component and the recipe, let''s create a local deployment:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Verify that the component has successfully been deployed (and is running) using
    the following command:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the output:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that the `Publisher` component is up and running, let''s review the code
    in the `Subscriber` component as well:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'def setup_subscription():'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request = SubscribeToTopicRequest()
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.topic = args.sub_topic
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: handler = StreamHandler()
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation = ipc_client.new_subscribe_to_topic(handler)
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future = operation.activate(request)
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future.result(TIMEOUT)
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return operation
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'def send_cloud(message_json):'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message_json_string = json.dumps(message_json)
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request = PublishToIoTCoreRequest()
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.topic_name = args.pub_topic
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.qos = QOS.AT_LEAST_ONCE
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.payload = bytes(message_json_string,"utf-8")
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message = PublishMessage()
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message = JsonMessage()
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message.message = bytes(message_json_string, "utf-8")
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.publish_message = publish_message
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation = ipc_client.new_publish_to_iot_core()
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation.activate(request)
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: logger.debug(message_json)
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you have reviewed the code, let''s check the following recipe file to review
    the access controls and dependencies required by the `Subscriber` component:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we have the component and the recipe, let''s create a local deployment:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is the output:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Verify that the component has successfully been deployed (and is running) using
    the following command. Now you should see both the `Publisher` and `Subscriber`
    components running locally:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following is the output:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you have observed, in the preceding code, the `Subscriber` component will
    not only subscribe to the local `mqtt` topics on the Raspberry Pi, but it will
    also start publishing data to AWS IoT Core (on the cloud). Let''s verify that
    from the AWS IoT console:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please navigate to `hbs/cloudtopic`. | Click **Subscribe**.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you have changed the default topic names in the recipe file, please use that
    name when you subscribe; otherwise, you won't see the incoming messages.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have near real-time data flowing from the edge to the cloud, let''s
    work on the micro-batch flow by integrating with Stream Manager. This component
    will subscribe to the `hbslocal/topic` topic (same as the subscriber). However,
    it will append the data to a local data stream using the Stream Manager functionality
    rather than publishing it to the cloud. Stream Manager is a key functionality
    for you to build a lambda architecture on the edge. We will break down the code
    into different snippets for you to understand these concepts better. So, let''s
    navigate to the working directory:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'First, we create a local stream with the required properties such as stream
    name, data size, time to live, persistence, data flushing, data retention strategy,
    and more. Data within the stream can stay local for further processing or can
    be exported to the cloud using the export definition parameter. In our case, we
    are exporting the data to Kinesis, but you can use a similar approach to export
    the data to other supported services such as S3, IoT Analytics, and more:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now the stream is defined, the data is appended through `append_message api`:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Fact Check
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Stream Manager allows you to deploy a lambda architecture on the edge without
    having to deploy and manage a separate lightweight database or streaming solution.
    Therefore, you can reduce the operational overhead or BOM cost of this solution.
    In addition to this, with Stream Manager as a data pond, you can persist data
    on the edge using a schema-less approach dynamically (remember BASE?). And finally,
    you can publish data to the cloud using the native integrations between the Stream
    Manager and cloud data services, such as IoT Analytics, S3, and Kinesis, without
    having to write any additional code. Stream Manager can also be beneficial for
    use cases with larger payloads such as blobs, images, or videos that can be easily
    transmitted over HTTPS.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have reviewed the code, let''s add the required permission for
    the Stream Manager component to update the Kinesis stream:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please navigate to **AWS IoT console**. | Select **Secure** (on the left pane).
    | Choose **Role Aliases** and select the appropriate one. | Click on the **Edit
    IAM Role**. | Attach policies. | Choose **Amazon Kinesis Full Access**. | Attach
    policy.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Please note that it's not recommended to use a blanket policy similar to this
    for production workloads. This is used here in order to ease the reader into operating
    in a test environment.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s perform a quick check of the recipe file prior to deploying this component:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'ComponentConfiguration:'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'DefaultConfiguration:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'sub_topic: "hbs/localtopic"'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'kinesis_stream: "<replace-with-kinesis-stream-from cfn>"'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'accessControl:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'aws.greengrass.ipc.pubsub:'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'com.hbs.hub.Aggregator:pubsub:1:'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'policyDescription: "Allows access to subscribe to topics"'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'operations:'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- aws.greengrass#SubscribeToTopic'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- aws.greengrass#PublishToTopic'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'resources:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- "*"'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ComponentDependencies:'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'aws.greengrass.StreamManager:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'VersionRequirement: "^2.0.0"'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Manifests:'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Platform:'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'os: all'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lifecycle:'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Install:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pip3 install awsiotsdk numpy -t .
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run: |'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: export PYTHONPATH=$PYTHONPATH:{artifacts:path}/stream_manager
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: PYTHONPATH=$PWD python3 -u {artifacts:path}/hbs_aggregator.py --sub-topic="{configuration:/sub_topic}"
    --kinesis-stream="{configuration:/kinesis_stream}"
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, as we have the artifact and the recipe reviewed, let''s create a local
    deployment:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Verify that the component has been successfully deployed (and is running) using
    the following command. You should observe all the following components running
    locally:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `Aggregator` component will publish the data directly from the local stream
    to the Kinesis stream on the cloud. Let''s navigate to the AWS S3 console to check
    whether the incoming messages are appearing:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Amazon Kinesis console**. | Select **Data Streams**. | Choose the
    stream. | Go to the **Monitoring** tab. | Check the metrics such as **Incoming
    data** or **Get records**.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you see the metrics showing some data points on the chart, it means the data
    is successfully reaching the cloud.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: You can always find the specific resource names required for this lab (such
    as the preceding Kinesis stream) in the *Resources* or *Output* section of the
    CloudFormation stack deployed earlier.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Validating the data streamed from the edge to the cloud
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will perform some final validation to ensure the transactional
    and batch data streamed from the edge components is successfully persisted on
    the data lake:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 19 of the previous section*, you validated that the Kinesis stream
    is getting data through metrics. Now, let''s understand how that data is persisted
    to the data lake from the streaming layer:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Amazon Kinesis console****.** | Select **Delivery Streams**. | Choose
    the respective delivery stream. | Click on the **Configuration** tab. | Scroll
    down to **Destination Settings**. | Click on the S3 bucket under the Amazon S3
    destination.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the bucket and drill down to the child buckets that store the batch
    data in a zipped format to help optimize storage costs.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As the final step, navigate to `Tables`. Then, select the table for this lab.
    Click on **View Items**.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you view the time series data? Excellent work.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you are not able to complete any of the preceding steps, please refer to
    the *Troubleshooting* section in the GitHub repository or create an issue for
    additional instructions.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have come a long way to learn how to build a lambda architecture
    that spans from the edge to the cloud using different AWS edge and cloud services.
    Now, let's wrap up with some additional topics before we conclude this chapter.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Additional topics for reference
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from what we have read so far, there are a couple of topics that I wish
    to mention. Whenever you have the time, please check them out, as they do have
    lots of benefits and can be found online.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Time series databases
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we learned how to leverage a NoSQL (key-value) data store
    such as Amazon DynamoDB for persisting time series data. Another common way to
    persist IoT data is to use a **time series database** (**TSDB**) such as **Amazon
    Timestream** or **Apache Cassandra**. As you know by now, time series data consists
    of measurements or events collected from different sources such as sensors and
    actuators that are indexed over time. Therefore, the fundamentals of modeling
    a time series database are quite similar to what was explained earlier with NoSQL
    data solutions. So, the obvious question that remains is *How do you choose between
    NoSQL and TSDB?* Take a look at the following considerations:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '**Consider the data summarization and data precision requirements**:'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, show me the energy utilization on a monthly or yearly basis. This
    requires going over a series of data points indexed by a time range to calculate
    a percentile increase of energy over the same period in the last 12 weeks, summarized
    by weeks. This kind of querying could get expensive with a distributed key-value
    store.
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Consider purging the data after a period of time**:'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, do consumers really care about the high precision metrics from
    an hourly basis to calculate their overall energy utilization per month? Probably
    not. Therefore, it's more efficient to store high-precision data for a short period
    of time and, thereafter, aggregated and downsampled data for identifying long-term
    trends. This functionality can partially be achieved with some NoSQL databases
    as well (such as the DynamoDB item expiry functionality). However, TSDBs are better
    suited as they can also offer downsampling and aggregation capabilities using
    different means, such as materialized views.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unstructured data
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You must be curious that most of our discussion in this chapter was related
    to structured and semi-structured data. We did not touch upon unstructured data
    (such as images, audio, and videos) at all. You are spot on. Considering IoT is
    the bridge between the physical world and the cyber world, there will be a huge
    amount of unstructured data that will need to be processed for different analytics
    and machine learning use cases.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario where the security cameras installed in your
    customer's home detect any infiltration or unexpected movements through the motion
    sensors and start streaming a video feed of the surroundings. The feed will be
    available through your smart hub or mobile devices for consumption. Therefore,
    in this scenario, the security camera is streaming videos that are unstructured
    data, as a P2P feed that can also be stored (if the user allows) locally on the
    hub or to an object store on the cloud. In [*Chapter 7*](B17595_07_Final_SS_ePub.xhtml#_idTextAnchor138),
    *Machine Learning Workloads at the Edge*, you will learn the techniques to ingest,
    store, and infer unstructured data from the edge. However, we will not delve into
    the modeling techniques for unstructured data, as it primarily falls under data
    science and is not very relevant in the day-to-day life of IoT practitioners.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about different data modeling techniques, data
    storage, and data integration patterns that are common with IoT edge workloads.
    You learned how to build, test, and deploy edge components on Greengrass. Additionally,
    you implemented a lambda architecture to collect, process, and stream data from
    disparate sources on the edge. Finally, you validated the workflow by visualizing
    the incoming data on IoT Core.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how all this data can be served on the cloud
    to generate valuable insights for different end consumers.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge check
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before moving on to the next chapter, test your knowledge by answering these
    questions. The answers can be found at the end of the book:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: Data modeling is only applicable for relational databases.'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of performing a data modeling exercise?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Is there any relevance of ETL architectures for edge computing? (Hint: Think
    lambda.)'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: Lambda architecture is the same as AWS lambda service.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think of at least one benefit of data processing at the edge?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which component of Greengrass is required to be run at the bare minimum for
    the device to be functional?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: Managing streams for real-time processing is a cloud-only thing.'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What strategy could you implement to persist data on the edge locally for a
    longer time?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the following resources for additional information on the concepts
    discussed in this chapter:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '*Data Management Body of Knowledge*: [https://www.dama.org/cpages/body-of-knowledge](https://www.dama.org/cpages/body-of-knowledge)'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon''s Dynamo*: [https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html](https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NoSQL Design for DynamoDB*: [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html)'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lambda Architecture*: [http://lambda-architecture.net/](http://lambda-architecture.net/)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Managing data streams on the AWS IoT Greengrass Core*: [https://docs.aws.amazon.com/greengrass/v2/developerguide/manage-data-streams.html](https://docs.aws.amazon.com/greengrass/v2/developerguide/manage-data-streams.html)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Lake on AWS*: [https://aws.amazon.com/solutions/implementations/data-lake-solution/](https://aws.amazon.com/solutions/implementations/data-lake-solution/)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
