- en: '*Chapter 5*: Ingesting and Streaming Data from the Edge'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Edge computing** can reduce the amount of data transferred to the cloud (or
    on-premises datacenter), thus saving on network bandwidth costs. Often, high-performance
    edge applications require local compute, storage, network, data analytics, and
    machine learning capabilities to process high-fidelity data in low latencies.
    AWS extends infrastructure to the edge, beyond **Regions** and **Availability
    Zones**, as close to the endpoint as required by your workload. As you will have
    learned in previous chapters, **AWS IoT Greengrass** allows you to run sophisticated
    edge applications on devices and gateways.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the different data design and transformation
    strategies applicable for edge workloads. We will explain how you can ingest data
    from different sensors through different workflows based on **data velocity**
    (such as hot, warm, and cold), **data variety** (such as structured and unstructured),
    and **data volume** (such as high frequency or low frequency) on the edge. Thereafter,
    you will learn the approaches of streaming the raw and transformed data from the
    edge to different cloud services. By the end of this chapter, you should be familiar
    with data processing using AWS IoT Greengrass.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining data models for IoT workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing data patterns for the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to know Stream Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your first data orchestration workflow on the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming from the edge to a data lake on the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technical requirements for this chapter are the same as those outlined in
    [*Chapter 2*](B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032)*, Foundations of
    Edge Workloads*. See the full requirements in that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the GitHub code repository here: [https://github.com/PacktPublishing/Intelligent-Workloads-at-the-Edge/tree/main/chapter5](https://github.com/PacktPublishing/Intelligent-Workloads-at-the-Edge/tree/main/chapter5)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining data models for IoT workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the IDC, the sum of the world's data will grow from 33 **zettabytes**
    (**ZB**) in 2018 to 175 ZB by 2025\. Additionally, the IDC estimates that there
    will be 41.6 billion connected IoT devices or *things*, generating 79.4 ZB of
    data in 2025 ([https://www.datanami.com/2018/11/27/global-datasphere-to-hit-175-zettabytes-by-2025-idc-says/](https://www.datanami.com/2018/11/27/global-datasphere-to-hit-175-zettabytes-by-2025-idc-says/)).
    Additionally, many other sources reiterate that data and information are the *currency*,
    the *lifeblood*, and even the *new oil* of the information industry.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the data-driven economy is here to stay and the **Internet of Things**
    (**IoT**) will act as the enabler to ingest data from a huge number of devices
    (or endpoints), such as sensors and actuators, and generate aggregated insights
    for achieving business outcomes. Thus, as an IoT practitioner, you should be comfortable
    with the basic concepts of **data modeling** and how that enables **data management**
    on the edge.
  prefs: []
  type: TYPE_NORMAL
- en: All organizations across different verticals such as industrial, commercial,
    consumer, transportation, energy, healthcare, and others are exploring new use
    cases to improve their top line or bottom line and innovate on behalf of their
    customers. IoT devices such as a connected hub in a consumer home, a smart parking
    meter on a road, or a connected car will coexist with customers and will operate
    even when there is no connectivity to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: This is a paradigm shift from the centralized solutions that worked for enterprises
    in the past. For example, a banking employee might have hosted their workloads
    in datacenters, but now they can monitor customer activities (such as suspicious
    actions, footfalls, or availability of cash in an ATM) at their branch locations
    in near real time to serve customers better. Therefore, a new strategy is required
    to act on the data generated locally and be able to process and stream data from
    the edge to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to rethink and re-evaluate the applicability of
    different big data architectures in the context of IoT and edge computing. The
    three areas we will consider are data management, data architecture patterns,
    and anti-patterns.
  prefs: []
  type: TYPE_NORMAL
- en: What is data management?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As per the **Data Management Body of Knowledge** (**DMBOK2**) from the **Data
    Management Association** (**DAMA**), data management is the development, execution,
    and supervision of plans, policies, programs, and practices that deliver, control,
    protect, and enhance the value of data and information assets throughout their
    life cycles (for more information, please refer to *DAMA-DMBOK2* at [https://technicspub.com/dmbok/](https://technicspub.com/dmbok/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'DAMA covers the data management framework in great detail, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The data management life cycle'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – The data management life cycle
  prefs: []
  type: TYPE_NORMAL
- en: Here, we recognized an opportunity to augment the framework from DAMA with concepts
    that are relevant to edge computing. Therefore, in this section, we will dive
    deeper into the principles related to data modeling, data architecture, and **Data
    Integration and Interoperability** (**DII**), which we think are relevant for
    edge computing and IoT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define data in the context of IoT before we discuss how to model it.
    IoT data is generated from different sensors, actuators, and gateways. Therefore,
    they can come in different forms such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured data**: This refers to a predictable form of data; examples include
    device metadata and device relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-structured data**: This is a form of data with a certain degree of variance
    and randomness; examples include sensor and actuator feeds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unstructured data**: This is a form of data with a higher degree of variance
    and randomness; examples include raw images, audio, or videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's discuss how the different forms of data can be governed, organized,
    and stored using data modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: What is data modeling?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data modeling** is a common practice in software engineering, where data
    requirements are defined and analyzed to support the business processes of different
    information systems in the scope of the organization. There are three different
    types of data models:'
  prefs: []
  type: TYPE_NORMAL
- en: Conceptual data models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical data models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical data models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, the relationships between different modeling approaches
    are presented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Data modeling approaches'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Data modeling approaches
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: The Enigma machine was used by the German military as the primary mode of communication
    for all secure wireless communications during World War II. Alan Turing cracked
    the Enigma code roughly 80 years ago when he figured out the text that's placed
    at the end of every message. This helped to decipher key secret messages from
    the German military and helped end the world war. Additionally, this mechanism
    led to the era of unlocking insights by defining a language to decipher data,
    which was later formalized as data modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common data modeling technique for a database is an **Entity-Relationship**
    (**ER**) model. An ER model in software engineering is a common way to represent
    the relationship between different entities such as people, objects, places, events,
    and more in a graphical way in order to organize information better to drive the
    business processes of an organization. In other terms, an ER model is an abstract data
    model that defines a data structure for the required information and can be independent
    of any specific database technology. In this section, we will explain the different
    data models using the ER model approach. First, let''s define, with the help of
    a use case diagram, the relationship between a customer and their devices associated
    with a connected HBS hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – A use case diagram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – A use case diagram
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s build the ER diagram through a series of conceptual, logical, and
    physical models:'
  prefs: []
  type: TYPE_NORMAL
- en: The **conceptual data model** defines the entities and their relationships.
    This is the first step of the data modeling exercise and is used by personas such
    as data architects to gather the initial set of requirements from the business
    stakeholders. For example, *sensor*, *device*, and *customer* are three entities
    in a relationship, as shown in the following diagram:![Figure 5.4 – A conceptual
    data model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17595_05_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.4 – A conceptual data model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The conceptual model is then rendered into a **logical data model**. In this
    step of data modeling, the data structure along with additional properties are
    defined using a conceptual model as the foundation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, you can define the properties of the different entities in the
    relationship such as the sensor type or device identifier (generally, a serial
    number or a MAC address), as shown in the following list. Additionally, there
    could be different forms of relationships, such as the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Association is the relationship between devices and sensors.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ownership is the relationship between customers and devices.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding points are illustrated in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – A logical data model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17595_05_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.5 – A logical data model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final step in data modeling is to build a **physical data model** from
    the defined logical model. A physical model is often a technology or product-specific
    implementation of the data model. For example, you define the data types for the
    different properties of an entity, such as a number or a string, that will be
    deployed on a database solution from a specific vendor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – A physical data model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – A physical data model
  prefs: []
  type: TYPE_NORMAL
- en: 'Enterprises have used ER modeling for decades to design and govern complex
    distributed data solutions. All the preceding steps can be visualized as the following
    workflow, which is not limited to any specific technology, product, subject area,
    or operating environment (such as a data center, cloud, or edge):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The data modeling flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – The data modeling flow
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have understood the foundations of data modeling, in the next section,
    let's examine how this can be achieved for IoT workloads.
  prefs: []
  type: TYPE_NORMAL
- en: How do you design data models for IoT?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's take a look at some examples of how to apply the preceding data modeling
    concepts to the realm of structured, unstructured, and semi-structured data that
    are common with IoT workloads. Generally, when we refer to data modeling for structured
    data, a **relational database** (**RDBMS**) comes to mind first. However, for
    most IoT workloads, structured data generally includes hierarchical relationships
    between a device and other entities. And that is better illustrated using a graph
    or an ordered key-value database. Similarly, for semi-structured data, when it
    comes to IoT workloads, it's mostly illustrated as a key-value, time series, or
    document store.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will give you a glimpse of data modeling techniques using
    NoSQL data solutions to continue building additional functionalities for HBS.
    Modeling an RDBMS is outside the scope of this book. However, if you are interested
    in learning about them, there are tons of materials available on the internet
    that you can refer to.
  prefs: []
  type: TYPE_NORMAL
- en: 'NoSQL databases are designed to offer freedom to developers to break away from
    a longer cycle of database schema designs. However, it''s a mistake to assume
    that NoSQL databases lack any sort of data model. Designing a NoSQL solution is
    quite different from an RDBMS design. For RDBMS, developers generally create a
    data model that adheres to normalization guidelines, without focusing on access
    patterns. The data model can be modified later when new business requirements
    arise, thus leading to a lengthy release cycle. The collected data is organized
    in different tables with rows, columns, and referential integrities. In contrast,
    for a NoSQL solution design, developers cannot begin designing the models until
    they know the questions that are required to be answered. Understanding the business
    queries working backward from the use case is quintessential. Therefore, the general
    rule of thumb to remember during data modeling through a relational or NoSQL database
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Relational modeling primarily cares about the structure of data. The design
    principle is *What answers do I get?*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL modeling primarily cares about application-specific access patterns. The
    design principle is *What questions do I ask?*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The common translation of the NoSQL acronym is *Not only SQL*. This highlights
    the fact that NoSQL doesn't only support NoSQL, but it can handle relational,
    semi-structured, and unstructured data. Organizations such as Amazon, Facebook,
    Twitter, LinkedIn, and Google have designed different NoSQL technologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before I show you some examples of data modeling, let''s understand the five
    fundamental properties of our application''s (that is, the HBS hub) access patterns
    that need to be considered in order to come up with relevant questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data type**: What''s the type of data in scope? For example, is the data
    related to telemetry, command-control, or critical events? Let''s quickly refresh
    the use of each of these data types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a) **Telemetry**: This is a constant stream of data transmitted by sensors/actuators,
    such as temperature or humidity readings , which can be aggregated on the edge
    or published as it is to the cloud for further processing.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) **Command and Control**: These are actionable messages, such as turning
    on/off the lights, which can occur between two devices or between an end user
    and the device.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) **Events**: These are data patterns that identify more complex scenarios
    than regular telemetry data, such as network outages in a home or a fire alarm
    in a building.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data size**: What is the quantity of data in scope? Is it necessary to store
    and retrieve data locally (on the edge), or does the data require transmission
    to a different data persistence layer (such as a data lake on the cloud)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data shape**: What''s the form of data being generated from different edge
    devices such as text, blobs, and images? Note that different data forms such as
    images and videos might have different computational needs (think of GPUs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data velocity**: What''s the speed of data to process queries based on the
    required latencies? Do you have a hot, warm, or cold path of data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: How much of this data needs to have strong versus eventual
    consistency?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answering the preceding questions will help you to determine whether the solution
    should be based on one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BASE methodology**: **Basically Available**, **Soft-state**, **Eventual consistency**,
    which are typical characteristics of NoSQL databases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ACID methodology**: **Atomicity**, **Consistency**, **Isolation**,and **Durability**,
    which are typical characteristics of relational databases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these concepts in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting between ACID or BASE for IoT workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table lists some of the key differences between the two methodologies.
    This should enable you to make an informed decision working backward from your
    use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – ACID versus BASE summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – ACID versus BASE summary
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: 'ACID and BASE represent opposing sides of the pH spectrum. Jim Grey conceived
    the idea in 1970 and subsequently published a paper, called *The Transaction Concept:
    Virtues and Limitations*, in June 1981\.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have understood the fundamentals of data modeling and design approaches.
    You must be curious about how to relate those concepts to the connected HBS product,
    which you have been developing in earlier chapters. Let's explore how the rubber
    meets the road.
  prefs: []
  type: TYPE_NORMAL
- en: Do you still remember the **first phase** of data modeling?
  prefs: []
  type: TYPE_NORMAL
- en: Bingo! Conceptual it is.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptual modeling of the connected HBS hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram is a hypothetical conceptual model of the HBS hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – A conceptual data model for connected HBS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – A conceptual data model for connected HBS
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you can observe how the different devices such as
    lights, HVAC, and washing machines are installed in different rooms of a house.
    Now the conceptual model is in place, let's take a look at the logical view.
  prefs: []
  type: TYPE_NORMAL
- en: The logical modeling of the connected HBS hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build the logical model, we need to ask ourselves the type of questions
    an end consumer might ask, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Show the status of a device (such as is the washing complete?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn a device on or off (such as turn off the lights).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show the readings of a device (such as what's the temperature now?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a new reading (such as how much energy is being consumed by the refrigerator?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show the aggregated connectivity status of a device (or devices) for a period.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address these questions, let''s determine the access patterns for our end
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – A logical data model for connected HBS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_Table_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – A logical data model for connected HBS
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have captured the summary of our data modeling requirements, you
    can observe that the solution needs to ingest data in both structured and semi-structured
    formats at high frequency. Additionally, it doesn't require strong consistency.
    Therefore, it makes sense to design the data layer using a NoSQL solution that
    leverages the BASE methodology.
  prefs: []
  type: TYPE_NORMAL
- en: The physical modeling of the connected HBS hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the final step, we need to define the physical data model from the gathered
    requirements. To do that, we will define a primary and a secondary key. You are
    not required to define all of the attributes if they're not known to you, which
    is a key advantage of a NoSQL solution.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the primary key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, this is one of the required attributes in a table, which
    is often known as a **Partition key**. In a table, no two primary keys should
    have the same value. There is also a concept of a **Composite key**. It's composed
    of two attributes, a partition key and a **Sort key**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our scenario, we will create a `Sensor` table with a composite key (as depicted
    in the following screenshot). The primary key is a **device identifier**, and
    the sort key is a timestamp that enables us to query data in a time range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Composite keys in a sensor table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – Composite keys in a sensor table
  prefs: []
  type: TYPE_NORMAL
- en: Defining the secondary indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **secondary index** allows us to query the data in the table using a different
    key, in addition to queries against the primary or composite keys. This gives
    your applications more flexibility in querying the data for different use cases.
    Performing a query using a secondary index is pretty similar to querying from
    the table directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, for secondary indexes, as shown in the following chart, we have
    selected the primary key as a sensor identifier (`sensor_id`) along with timestamp
    as the sort key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Secondary indexes in a sensor table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – Secondary indexes in a sensor table
  prefs: []
  type: TYPE_NORMAL
- en: Defining the additional attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key advantage of a NoSQL solution is that there is no enforced schema.
    Therefore, other attributes can be created on the fly as data comes in. That being
    said, if some of the attributes are already known to the developer, there is no
    restriction to include those in the data model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Other attributes in a sensor table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – Other attributes in a sensor table
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data layer exists, let's create the interfaces to access this data.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we will create two different facets for the sensor table. A **facet** is
    a virtual construct that enables different views of the data stored in a table.
    The facets can be mapped to a functional construct such as a method or an API
    for performing various **Create, Read, Update, Delete** (**CRUD**) operations
    on a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '`putItems`: This facet allows write operations and requires the composite keys
    at the minimum in the payload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getItems`: This facet allows read operations that can query items with all
    or selective attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the `getItems` facet definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – The getItems facet definition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – The getItems facet definition
  prefs: []
  type: TYPE_NORMAL
- en: So, now you have created the data model along with its interfaces. This enables
    you to understand the data characteristics that are required to develop edge applications.
  prefs: []
  type: TYPE_NORMAL
- en: Designing data patterns on the edge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As data flows securely from different sensors/actuators on the edge to the gateway
    or cloud over different protocols or channels, it is necessary for it to be safely
    stored, processed, and cataloged for further consumption. Therefore, any IoT data
    architecture needs to take into consideration the data models (as explained earlier),
    data storage, data flow patterns, and anti-patterns, which will be covered in
    this section. Let's start with data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Big data solutions on the cloud are designed to reliably store terabytes, petabytes,
    or exabytes of data and can scale across multiple geographic locations globally
    to provide high availability and redundancy for businesses to meet their **Recovery
    Time Objective** (**RTO**) and **Recovery Point Objective** (**RPO**). However,
    edge solutions, such as our very own connected HBS hub solution, are resource-constrained
    in terms of compute, storage, and network. Therefore, we need to design the edge
    solution to cater to different time-sensitive, low-latency use cases and hand
    off the heavy lifting to the cloud. A **data lake** is a well-known pattern on
    the cloud today, which allows a centralized repository to store data as it arrives,
    without having to first structure the data. Thereafter, different types of analytics,
    machine learning, and visualizations can be performed on that data for consumers
    to achieve better business outcomes. So, what is the equivalent of a data lake
    for the edge?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce a new pattern, called a *data pond*, for the authoritative
    source of data (that is, the golden source) that is generated and temporarily
    persisted on the edge. Certain characteristics of a data pond are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A data pond enables the quick ingestion and consumption of data in a fast and
    flexible fashion. A data producer is only required to know where to push the data,
    that is, the local storage, local stream, or cloud. The choice of the storage
    layer, schema, ingestion frequency, and quality of the data is left to the data
    producer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pond should work with low-cost storage. Generally, IoT devices are low
    in storage; therefore, only highly valuable data that's relevant for the edge
    operations can be persisted locally. The rest of the data is pushed to the cloud
    for additional processing or thrown away (if noisy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pond supports schema on read. There can be multiple streams supporting
    multiple schemas in a data pond.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pond should support the data protection mechanisms at rest and in encryption.
    It's also useful to implement role-based access that allows auditing the data
    trail as it flows from the edge to the cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows an edge architecture of how data collected from
    different sensors/actuators can be persisted and securely governed in a data pond:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – A data pond architecture at the edge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – A data pond architecture at the edge
  prefs: []
  type: TYPE_NORMAL
- en: 'The organizational entities involved in the preceding data flow include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data producers**: These are entities that generate data. These include physical
    (such as sensors, actuators, or associated devices) or logical (such as applications)
    entities and are configured to store data in the data pond or publish data to
    the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data pond team**: Generally, the data operations team defines the data access
    mechanisms for the data pond (or lake) and the development team supports data
    management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consumers**: Edge and cloud applications retrieve data from the data
    pond (or lake) using the mechanisms authorized to further iterate on the data
    and meet business needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the organizational entities for the data pond:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – The organizational entities for the data pond'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – The organizational entities for the data pond
  prefs: []
  type: TYPE_NORMAL
- en: Now you have understood how data can be stored on a data pond and be managed
    or governed by different entities. Next, let's discuss the different flavors of
    data and how they can be integrated.
  prefs: []
  type: TYPE_NORMAL
- en: Data integration concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DII occurs through different layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch**: This layer aggregates data that has been generated by data producers.
    The goal is to increase the accuracy of data insights through the consolidation
    of data from multiple sources or dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed**: This layer streams data generated by data producers. The goal is
    to allow a near real-time analysis of data with an acceptable level of accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving**: This layer merges the data from the batch layer and the speed
    layer to enable the downstream consumers or business users with holistic and incremental
    insights into the business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of DII:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 -- Data Integration and Interoperability (DII)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 -- Data Integration and Interoperability (DII)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are multiple layers within the data flow that are commonly
    implemented using the **Extract, Transform, and Load** (**ETL**) methodology or
    the **Extract, Load, and Transform** (**ELT**) methodology in the big data world.
    The ETL methodology involves steps to extract data from different sources, implement
    data quality and consistency standards, transform (or aggregate) the data to conform
    to a standard format, and load (or deliver) data to downstream applications.
  prefs: []
  type: TYPE_NORMAL
- en: The ELT process is a variant of ETL with similar steps. The difference is that
    extracted data is loaded before the transformation. This is common for edge workloads
    as well, where the local gateway might not have enough resources to do the transformation
    locally; therefore, it publishes the data prior to additional processing.
  prefs: []
  type: TYPE_NORMAL
- en: But how are these data integration patterns used in the edge? Let's explore
    this next.
  prefs: []
  type: TYPE_NORMAL
- en: Data flow patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An ETL flow on the edge will include three distinct steps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data extraction from devices such as sensors/actuators
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data transformation to clean, filter, or restructure data into an optimized
    format for further consumption
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data loading to publish data to the persistence layer such as a data pond, a
    data lake, or a data warehouse
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For an ELT flow, *steps 2* and *3* will take place in the reverse order.
  prefs: []
  type: TYPE_NORMAL
- en: An ETL Scenario for a Connected Home
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a connected home scenario, it's common to extract data from
    different sensors/actuators, followed by a data transformation that might include
    format changes, structural changes, semantic conversions, or deduplication. Additionally,
    data transformation allows you to filter out any noisy data from the home (think
    of a crying baby or a noisy pet), resulting in reduced network charges of publishing
    all the bits and bytes to the cloud. Based on a use case such as an intrusion
    alert or replenishing a printer toner, data transformation can be performed in
    batch or real time, by eitherphysically storing the result in a staging area or
    virtually storing the transferred data in memory until you are ready to move to
    the load step.
  prefs: []
  type: TYPE_NORMAL
- en: These core patterns (ETL or ELT) have evolved, with time, into different data
    flow architectures, such as event-driven, batch, lambda, and**complex event processing**
    *(***CEP***)*. We will explain each of them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven (or streaming)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's very common for edge applications to generate and process data in smaller
    sets throughout the day when an event happens. Near real-time data processing
    has a lower latency and can be both synchronous and asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: In an asynchronous data flow, the devices (such as sensors) do not wait for
    the receiving system to acknowledge updates before continuing processing. For
    example, in a connected home, a motion/occupancy sensor can trigger an intruder
    notification based on a detected event but continue to monitor without waiting
    for an acknowledgment.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, in a real-time synchronous data flow, no time delay or other
    differences between source and target are acceptable. For example, in a connected
    home, if there is a fire alarm, it should notify the emergency services in a deterministic
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'With AWS Greengrass, you can design both **synchronous** and **asynchronous**
    data communications. In addition to this, as we build multi-faceted architectures
    on the edge, it''s quite normal to build multiprocessing or multithreaded polyglot
    solutions on the edge to support different low-latency use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Event-driven architecture at the edge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – Event-driven architecture at the edge
  prefs: []
  type: TYPE_NORMAL
- en: Micro-batch (or aggregated processing)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most enterprises perform frequent batch processing to enable end users with
    business insights. In this mode, data moving will represent either the full set
    of data at a given point in time, such as the energy meter reading of a connected
    home at the end of a period (such as the day, week, or month), or data that has
    changed values since the last time it was sent, such as a hvac reading or a triggered
    fire alarm. Generally, batch systems are designed to scale and grow proportionally
    along with the data. However, that's not feasible on the edge due to the lack
    of horsepower, as explained earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, for IoT use cases, leveraging micro-batch processing is more common.
    Here, the data is stored locally and is processed on a much higher frequency,
    such as every few seconds, minutes, hours, or days (over weeks or months). This
    allows data consumers to gather insights from local data sources with reduced
    latency and cost, even when disconnected from the internet. The **Stream Manager**
    capability of AWS Greengrass allows you to perform aggregated processing on the
    edge. Stream Manager brings enhanced functionalities regarding how to process
    data on the edge such as defining a bandwidth and data prioritization for multiple
    channels, timeout behavior, and direct export mechanisms to different AWS services
    such as Amazon S3, AWS IoT Analytics, AWS IoT SiteWise, and Amazon Kinesis data
    streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Micro-batch architecture at the edge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – Micro-batch architecture at the edge
  prefs: []
  type: TYPE_NORMAL
- en: Lambda architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lambda architecture is an approach that combines both micro-batch and stream
    (near real-time) data processing. It makes the consolidated data available for
    downstream consumption. For example, a refrigeration unit, a humidifier, or any
    critical piece of machinery on a manufacturing plant can be monitored and fixed
    before it becomes non-operational. So, for a connected HBS hub solution, micro-batch
    processing will allow you to detect long-term trends or failure patterns. This
    capability in turn, will help your fleet operators recommend preventive or predictive
    maintenance for the machines to end consumers. This workflow is often referred
    to as the warm or cold path of the data analytics flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, stream processing will allow the fleet operators to derive
    near real-time insights through telemetry data. This will enable consumers to
    take mission-critical actions such as locking the entire house and calling emergency
    services if any theft is detected. This is also referred to as the hot path in
    lambda architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Lambda architecture at the edge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.20 – Lambda architecture at the edge
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Lambda architecture has nothing to do with the AWS lambda service. The term was
    coined by Nathan Marz, who worked on big-data-related technologies at *BackType*
    and *Twitter*. This is a design pattern for describing data processing that is
    scalable and fault-tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: Data flow anti-patterns for the edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned about the common data flow patterns on the edge. Let's
    also discuss some of the anti-patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Complex Event Processing (CEP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Events are data patterns that identify complex circumstances from ingested data,
    such as network outages in a home or a fire alarm in a building. It might be easier
    to detect events from a few sensors or devices; however, getting visibility into
    complex events from disparate sources and being able to capture states or trigger
    conditional logic to identify and resolve issues quickly requires special treatment.
  prefs: []
  type: TYPE_NORMAL
- en: That's where the CEP pattern comes into play. CEP can be resource-intensive
    and needs to scale to all sizes of data and grow proportionally. Therefore, it's
    still not a very common pattern on the edge. On the cloud, managed services such
    as AWS IoT events or AWS EventBridge can make it easier for you to perform CEP
    on the data generated from your IoT devices.
  prefs: []
  type: TYPE_NORMAL
- en: Batch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, in batch processing, data moves in aggregates as blobs or files
    either on an ad hoc request from a consumer or automatically on a periodic schedule.
    Data will either be a full set (referred to as snapshot) or a changed set (delta)
    from a given point in time. Batch processing requires continuous scaling of the
    underlying infrastructure to facilitate the data growth and processing requirements.
    Therefore, it's a pattern that is better suited for big data or data warehousing
    solutions on the cloud. That being said, for an edge use case, you can still leverage
    the micro-batch pattern (as explained earlier) to aggregate data that's feasible
    in the context of a resource-constrained environment.
  prefs: []
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's a common practice on the cloud to maintain redundant copies of datasets
    across different locations to improve business continuity, improve the end user
    experience, or enhance data resiliency. However, in the context of the edge, **data
    replication** can be expensive, as you might require redundant deployments. For
    example, with a connected HBS hub solution, if the gateway needs to support redundant
    storage for replication, it will increase the **bill of materials** (**BOM**)
    cost of the hardware, and you can lose the competitive edge on the market.
  prefs: []
  type: TYPE_NORMAL
- en: Archiving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data that is used infrequently or not actively used can be moved to an alternate
    data storage solution that is more cost-effective to the organization. Similar
    to replication, for archiving data locally on the edge, additional deployment
    of hardware resource is necessary. This increases the **bill of materials** (**BOM**)
    cost of the device and leads to additional operational overhead. Therefore, it's
    common to archive the transformed data from the data lake to a cost-effective
    storage service on the cloud such as **Amazon Glacier**. Thereafter, this data
    can be used for local operations, data recovery, or regulatory needs.
  prefs: []
  type: TYPE_NORMAL
- en: A hands-on approach with the lab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to build a lambda architecture on the edge
    using different AWS services. The following diagram shows the lambda architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – The lab architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.21 – The lab architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding workflow uses the following services. In this chapter, you will
    complete *steps 1–6* (as shown in *Figure 5.21*). This includes designing and
    deploying the edge components, processing, and transforming data locally, and
    pushing the data to different cloud services:'
  prefs: []
  type: TYPE_NORMAL
- en: \
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – The hands-on lab components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17595_05_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.22 – The hands-on lab components
  prefs: []
  type: TYPE_NORMAL
- en: 'In this hands-on section, your objective will consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Build the cloud resource (that is, Amazon Kinesis data streams, Amazon S3 bucket,
    and DynamoDB tables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build and deploy the edge components (that is, artifacts and recipes) locally
    on Raspberry Pi.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validate that the data is streamed from the edge to the cloud (AWS IoT Core).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building cloud resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploy the CloudFormation template from the `chapter5/cfn` folder to create
    cloud resources such as Amazon S3 buckets, Kinesis data streams, and DynamoDB
    tables. You will need to substitute these respective names from the *Resources*
    section of the deployed stack, when requested, in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Building edge components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s hop on to our device to build and deploy the required edge components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the following working directory from the Terminal of your Raspberry
    Pi device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the Python script using the editor of your choice (such as *nano*, *vi*,
    or *emac*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def read_value(self):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message = {}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: device_list = ['hvac', 'refrigerator', 'washingmachine']
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: device_name = random.choice(device_list)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if device_name == ''hvac'' :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_id'] = "1"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['timestamp'] = float("%.4f" % (time()))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_name'] = device_name
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['temperature'] = round(random.uniform(10.0, 99.0), 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['humidity'] = round(random.uniform(10.0, 99.0), 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'elif device_name == ''washingmachine'' :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_id'] = "2"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['timestamp'] = float("%.4f" % (time()))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_name'] = device_name
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['duty_cycles'] = round(random.uniform(10.0, 99.0), 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'else :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_id'] = "3"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['timestamp'] = float("%.4f" % (time()))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['device_name'] = device_name
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message['vibration'] = round(random.uniform(100.0, 999.0), 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return message
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, open the following `publisher` script and navigate through the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: TIMEOUT = 10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ipc_client = awsiot.greengrasscoreipc.connect()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: sensor = DummySensor()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'while True:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message = sensor.read_value()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message_json = json.dumps(message).encode('utf-8')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request = PublishToTopicRequest()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.topic = args.pub_topic
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message = PublishMessage()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message = JsonMessage()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message.message = message
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.publish_message = publish_message
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation = ipc_client.new_publish_to_topic()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation.activate(request)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future = operation.get_response()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future.result(TIMEOUT)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print("publish")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: time.sleep(5)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now you have reviewed the code, check the following recipe file to review the
    access controls and dependencies that are required by the `Publisher` component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we have the component and the recipe, let''s create a local deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the component has successfully been deployed (and is running) using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the `Publisher` component is up and running, let''s review the code
    in the `Subscriber` component as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def setup_subscription():'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request = SubscribeToTopicRequest()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.topic = args.sub_topic
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: handler = StreamHandler()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation = ipc_client.new_subscribe_to_topic(handler)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future = operation.activate(request)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: future.result(TIMEOUT)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return operation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def send_cloud(message_json):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: message_json_string = json.dumps(message_json)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request = PublishToIoTCoreRequest()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.topic_name = args.pub_topic
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.qos = QOS.AT_LEAST_ONCE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.payload = bytes(message_json_string,"utf-8")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message = PublishMessage()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message = JsonMessage()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: publish_message.json_message.message = bytes(message_json_string, "utf-8")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: request.publish_message = publish_message
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation = ipc_client.new_publish_to_iot_core()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: operation.activate(request)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: logger.debug(message_json)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now you have reviewed the code, let''s check the following recipe file to review
    the access controls and dependencies required by the `Subscriber` component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we have the component and the recipe, let''s create a local deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the component has successfully been deployed (and is running) using
    the following command. Now you should see both the `Publisher` and `Subscriber`
    components running locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you have observed, in the preceding code, the `Subscriber` component will
    not only subscribe to the local `mqtt` topics on the Raspberry Pi, but it will
    also start publishing data to AWS IoT Core (on the cloud). Let''s verify that
    from the AWS IoT console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please navigate to `hbs/cloudtopic`. | Click **Subscribe**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you have changed the default topic names in the recipe file, please use that
    name when you subscribe; otherwise, you won't see the incoming messages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have near real-time data flowing from the edge to the cloud, let''s
    work on the micro-batch flow by integrating with Stream Manager. This component
    will subscribe to the `hbslocal/topic` topic (same as the subscriber). However,
    it will append the data to a local data stream using the Stream Manager functionality
    rather than publishing it to the cloud. Stream Manager is a key functionality
    for you to build a lambda architecture on the edge. We will break down the code
    into different snippets for you to understand these concepts better. So, let''s
    navigate to the working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we create a local stream with the required properties such as stream
    name, data size, time to live, persistence, data flushing, data retention strategy,
    and more. Data within the stream can stay local for further processing or can
    be exported to the cloud using the export definition parameter. In our case, we
    are exporting the data to Kinesis, but you can use a similar approach to export
    the data to other supported services such as S3, IoT Analytics, and more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now the stream is defined, the data is appended through `append_message api`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fact Check
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Stream Manager allows you to deploy a lambda architecture on the edge without
    having to deploy and manage a separate lightweight database or streaming solution.
    Therefore, you can reduce the operational overhead or BOM cost of this solution.
    In addition to this, with Stream Manager as a data pond, you can persist data
    on the edge using a schema-less approach dynamically (remember BASE?). And finally,
    you can publish data to the cloud using the native integrations between the Stream
    Manager and cloud data services, such as IoT Analytics, S3, and Kinesis, without
    having to write any additional code. Stream Manager can also be beneficial for
    use cases with larger payloads such as blobs, images, or videos that can be easily
    transmitted over HTTPS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have reviewed the code, let''s add the required permission for
    the Stream Manager component to update the Kinesis stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please navigate to **AWS IoT console**. | Select **Secure** (on the left pane).
    | Choose **Role Aliases** and select the appropriate one. | Click on the **Edit
    IAM Role**. | Attach policies. | Choose **Amazon Kinesis Full Access**. | Attach
    policy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Please note that it's not recommended to use a blanket policy similar to this
    for production workloads. This is used here in order to ease the reader into operating
    in a test environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s perform a quick check of the recipe file prior to deploying this component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'ComponentConfiguration:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'DefaultConfiguration:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'sub_topic: "hbs/localtopic"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'kinesis_stream: "<replace-with-kinesis-stream-from cfn>"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'accessControl:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'aws.greengrass.ipc.pubsub:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'com.hbs.hub.Aggregator:pubsub:1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'policyDescription: "Allows access to subscribe to topics"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'operations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- aws.greengrass#SubscribeToTopic'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- aws.greengrass#PublishToTopic'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'resources:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- "*"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ComponentDependencies:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'aws.greengrass.StreamManager:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'VersionRequirement: "^2.0.0"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Manifests:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Platform:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'os: all'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lifecycle:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Install:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pip3 install awsiotsdk numpy -t .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run: |'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: export PYTHONPATH=$PYTHONPATH:{artifacts:path}/stream_manager
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: PYTHONPATH=$PWD python3 -u {artifacts:path}/hbs_aggregator.py --sub-topic="{configuration:/sub_topic}"
    --kinesis-stream="{configuration:/kinesis_stream}"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, as we have the artifact and the recipe reviewed, let''s create a local
    deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the component has been successfully deployed (and is running) using
    the following command. You should observe all the following components running
    locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Aggregator` component will publish the data directly from the local stream
    to the Kinesis stream on the cloud. Let''s navigate to the AWS S3 console to check
    whether the incoming messages are appearing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Amazon Kinesis console**. | Select **Data Streams**. | Choose the
    stream. | Go to the **Monitoring** tab. | Check the metrics such as **Incoming
    data** or **Get records**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you see the metrics showing some data points on the chart, it means the data
    is successfully reaching the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can always find the specific resource names required for this lab (such
    as the preceding Kinesis stream) in the *Resources* or *Output* section of the
    CloudFormation stack deployed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the data streamed from the edge to the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will perform some final validation to ensure the transactional
    and batch data streamed from the edge components is successfully persisted on
    the data lake:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 19 of the previous section*, you validated that the Kinesis stream
    is getting data through metrics. Now, let''s understand how that data is persisted
    to the data lake from the streaming layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Amazon Kinesis console****.** | Select **Delivery Streams**. | Choose
    the respective delivery stream. | Click on the **Configuration** tab. | Scroll
    down to **Destination Settings**. | Click on the S3 bucket under the Amazon S3
    destination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the bucket and drill down to the child buckets that store the batch
    data in a zipped format to help optimize storage costs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As the final step, navigate to `Tables`. Then, select the table for this lab.
    Click on **View Items**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you view the time series data? Excellent work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you are not able to complete any of the preceding steps, please refer to
    the *Troubleshooting* section in the GitHub repository or create an issue for
    additional instructions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have come a long way to learn how to build a lambda architecture
    that spans from the edge to the cloud using different AWS edge and cloud services.
    Now, let's wrap up with some additional topics before we conclude this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Additional topics for reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from what we have read so far, there are a couple of topics that I wish
    to mention. Whenever you have the time, please check them out, as they do have
    lots of benefits and can be found online.
  prefs: []
  type: TYPE_NORMAL
- en: Time series databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we learned how to leverage a NoSQL (key-value) data store
    such as Amazon DynamoDB for persisting time series data. Another common way to
    persist IoT data is to use a **time series database** (**TSDB**) such as **Amazon
    Timestream** or **Apache Cassandra**. As you know by now, time series data consists
    of measurements or events collected from different sources such as sensors and
    actuators that are indexed over time. Therefore, the fundamentals of modeling
    a time series database are quite similar to what was explained earlier with NoSQL
    data solutions. So, the obvious question that remains is *How do you choose between
    NoSQL and TSDB?* Take a look at the following considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consider the data summarization and data precision requirements**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, show me the energy utilization on a monthly or yearly basis. This
    requires going over a series of data points indexed by a time range to calculate
    a percentile increase of energy over the same period in the last 12 weeks, summarized
    by weeks. This kind of querying could get expensive with a distributed key-value
    store.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Consider purging the data after a period of time**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, do consumers really care about the high precision metrics from
    an hourly basis to calculate their overall energy utilization per month? Probably
    not. Therefore, it's more efficient to store high-precision data for a short period
    of time and, thereafter, aggregated and downsampled data for identifying long-term
    trends. This functionality can partially be achieved with some NoSQL databases
    as well (such as the DynamoDB item expiry functionality). However, TSDBs are better
    suited as they can also offer downsampling and aggregation capabilities using
    different means, such as materialized views.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unstructured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You must be curious that most of our discussion in this chapter was related
    to structured and semi-structured data. We did not touch upon unstructured data
    (such as images, audio, and videos) at all. You are spot on. Considering IoT is
    the bridge between the physical world and the cyber world, there will be a huge
    amount of unstructured data that will need to be processed for different analytics
    and machine learning use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario where the security cameras installed in your
    customer's home detect any infiltration or unexpected movements through the motion
    sensors and start streaming a video feed of the surroundings. The feed will be
    available through your smart hub or mobile devices for consumption. Therefore,
    in this scenario, the security camera is streaming videos that are unstructured
    data, as a P2P feed that can also be stored (if the user allows) locally on the
    hub or to an object store on the cloud. In [*Chapter 7*](B17595_07_Final_SS_ePub.xhtml#_idTextAnchor138),
    *Machine Learning Workloads at the Edge*, you will learn the techniques to ingest,
    store, and infer unstructured data from the edge. However, we will not delve into
    the modeling techniques for unstructured data, as it primarily falls under data
    science and is not very relevant in the day-to-day life of IoT practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about different data modeling techniques, data
    storage, and data integration patterns that are common with IoT edge workloads.
    You learned how to build, test, and deploy edge components on Greengrass. Additionally,
    you implemented a lambda architecture to collect, process, and stream data from
    disparate sources on the edge. Finally, you validated the workflow by visualizing
    the incoming data on IoT Core.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how all this data can be served on the cloud
    to generate valuable insights for different end consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before moving on to the next chapter, test your knowledge by answering these
    questions. The answers can be found at the end of the book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: Data modeling is only applicable for relational databases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of performing a data modeling exercise?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Is there any relevance of ETL architectures for edge computing? (Hint: Think
    lambda.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: Lambda architecture is the same as AWS lambda service.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think of at least one benefit of data processing at the edge?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which component of Greengrass is required to be run at the bare minimum for
    the device to be functional?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: Managing streams for real-time processing is a cloud-only thing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What strategy could you implement to persist data on the edge locally for a
    longer time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the following resources for additional information on the concepts
    discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data Management Body of Knowledge*: [https://www.dama.org/cpages/body-of-knowledge](https://www.dama.org/cpages/body-of-knowledge)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon''s Dynamo*: [https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html](https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NoSQL Design for DynamoDB*: [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lambda Architecture*: [http://lambda-architecture.net/](http://lambda-architecture.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Managing data streams on the AWS IoT Greengrass Core*: [https://docs.aws.amazon.com/greengrass/v2/developerguide/manage-data-streams.html](https://docs.aws.amazon.com/greengrass/v2/developerguide/manage-data-streams.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Lake on AWS*: [https://aws.amazon.com/solutions/implementations/data-lake-solution/](https://aws.amazon.com/solutions/implementations/data-lake-solution/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
