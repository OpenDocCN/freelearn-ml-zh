<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Failures of Banks - Descriptive Analysis</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to understand and prepare our dataset of banks for model development. We will answer questions regarding the number of variables we have and their quality. <span>Descriptive analysis is crucial to understanding our data and for analyzing possible problems with the information </span>quality. We will see how to deal with missing values, convert variables into different formats, and how to split our data to train and validate our predictive model.</p>
<p>Specifically, we will cover the following topics:</p>
<ul>
<li>Data overview</li>
<li>Converting formats</li>
<li>Sampling</li>
<li>Dealing with missing and outliers values</li>
<li>Implementing descriptive analysis</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data overview</h1>
                </header>
            
            <article>
                
<p>First, we are going to analyze the types of variables that we have in the dataset. For that, we can use the <kbd>class</kbd> function, which tells us whether a variable is a number, a character, or a matrix. For example, the class of the identifying number of a bank <kbd>ID_RSSD</kbd> can be obtained as follows:</p>
<pre>class(Model_database$ID_RSSD)<br/><br/> ## [1] "integer"</pre>
<p>This function indicates that this variable is a number without decimals.</p>
<p class="mce-root"/>
<p>We can calculate the same information for all the variables and store it using the following code:</p>
<pre> classes&lt;-as.data.frame(sapply(Model_database, class))<br/> classes&lt;-cbind(colnames(Model_database),classes)<br/> colnames(classes)&lt;-c("variable","class")</pre>
<p>With <kbd>sapply</kbd>, calculate iteratively the <kbd>class</kbd> <span>function </span>on the dataset. Then, combine the name of variables with the class in only a data frame, and, finally, rename the resulting dataset:</p>
<pre>head(classes)<br/><br/> ##          variable   class<br/> ## ID_RSSD   ID_RSSD integer<br/> ## UBPR1795 UBPR1795 numeric<br/> ## UBPR4635 UBPR4635 numeric<br/> ## UBPRC233 UBPRC233 numeric<br/> ## UBPRD582 UBPRD582 numeric<br/> ## UBPRE386 UBPRE386 numeric</pre>
<p>This dataset contains four different types of variables:</p>
<pre>table(classes$class)<br/><br/> ## character      Date   integer   numeric<br/> ##       462         1         4      1027</pre>
<p>According to previous steps, we know that only variables with a <kbd>Date</kbd> format collect the date of financial statements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting acquainted with our variables</h1>
                </header>
            
            <article>
                
<p><span><span>T</span></span>he character type of some variables is not yet clear. Our dataset belongs to the financial domain and has only financial ratios as data, so we'd expect variable types to be integers or numerical. Let's find out whether we're right.</p>
<p>Filter the <kbd>character</kbd> variables:</p>
<pre>classes&lt;-classes[classes$class=="character",]<br/><br/>head(classes)<br/><br/> ##          variable     class<br/> ## UBPRE543 UBPRE543 character<br/> ## UBPRE586 UBPRE586 character<br/> ## UBPRE587 UBPRE587 character<br/> ## UBPRE594 UBPRE594 character<br/> ## UBPRFB64 UBPRFB64 character<br/> ## UBPRFB69 UBPRFB69 character</pre>
<p>The first variable, <kbd>UBPRE543</kbd>, measures the total losses of a bank providing construction loans divided by the total amount of granted construction loans. As we suspected, this variable should be numeric, a percentage, or a decimal number.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding missing values for a variable</h1>
                </header>
            
            <article>
                
<p>We are going to count the number of missing values of this variable, <kbd>UBPRE543</kbd><span>,</span> over time using this code, with the intention of understanding a little more about this variable:</p>
<pre>aggregate(UBPRE543 ~ Date, data=Model_database, function(x) {sum(is.na(x))}, na.action = NULL)<br/><br/> ##          Date UBPRE543<br/> ## 1  2002-12-31     1127<br/> ## 2  2003-12-31      954<br/> ## 3  2004-12-31      772<br/> ## 4  2005-12-31      732<br/> ## 5  2006-12-31      639<br/> ## 6  2007-12-31      309<br/> ## 7  2008-12-31      110<br/> ## 8  2009-12-31       98<br/> ## 9  2010-12-31       91<br/> ## 10 2011-12-31       76<br/> ## 11 2012-12-31      132<br/> ## 12 2013-12-31       98<br/> ## 13 2014-12-31       85<br/> ## 14 2015-12-31       89<br/> ## 15 2016-12-31       68</pre>
<p><span>As we can see, the ratio displays some missing values from </span>2002 to 2006<span>.</span></p>
<p>On the other hand, we can calculate the number of observations by year in the dataset using the <kbd>table</kbd> function:</p>
<pre>table(Model_database$Date)<br/><br/> ##<br/> ## 2002-12-31 2003-12-31 2004-12-31 2005-12-31 2006-12-31 2007-12-31<br/> ##       1127        954        772        732        639        652<br/> ## 2008-12-31 2009-12-31 2010-12-31 2011-12-31 2012-12-31 2013-12-31<br/> ##        686        671        587        533        664        615<br/> ## 2014-12-31 2015-12-31 2016-12-31<br/> ##        526        498        474</pre>
<p>Comparing the two preceding tables, we can see this variable is not informed during the first years.    </p>
<p>When we started to upload the <kbd>.txt</kbd> files into R at the beginning of the exercise, as this variable was not informed during the first years, R automatically assigns a character format to this variable.</p>
<p>For later years, when the variable was informed, the variable was read as numeric, but the format changed when all the years were merged together into a data frame, just when we executed this code:</p>
<pre>#database&lt;-rbind(year2002,year2003,year2004,year2005,year2006,year2007,year2008, year2009,year2010,year2011,year2012,year2013,year2014,year2015,year2016)</pre>
<p>The format of variables in the first table used in the <kbd>rbind</kbd> function, belonging to the year 2002, fixed and conditioned the format of the resulting merged table.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting the format of the variables</h1>
                </header>
            
            <article>
                
<p>We now need to convert all these variables into a numeric format. From the second variable (the first is the identifier) to the rest of variables in the data frame, variables will be explicitly converted to a numeric. The last two variables will be also be excluded from this process (<kbd>Date</kbd> and the target variable). Let’s convert variables to a numeric format with the following code:</p>
<pre>for (k in 2:(ncol(Model_database)-2)) <br/>   {<br/>    Model_database[,k]&lt;-as.numeric(Model_database[,k])<br/>   }</pre>
<p>Let’s see whether the changes have been applied:</p>
<pre>table(sapply(Model_database, class))<br/><br/> ##<br/> ##    Date integer numeric<br/> ##       1       1    1492</pre>
<p>Before continuing with the development, and once we have solved problems in the data formats, in the following section, we will specify which part of the sample will be used for the development and which part to validate the model.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling</h1>
                </header>
            
            <article>
                
<p>All the following steps and descriptive analysis will be done only considering the training or development sample. Therefore, our data will be divided into two samples:</p>
<ul>
<li><strong>Training set</strong>: It usually represents 70% of the data and it is used to train the model (select the parameters that better fit the model).</li>
<li><strong>Validation set</strong>: It usually represents 30% of the data and it is used to measure how well the model performs at making predictions.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partitioning samples</h1>
                </header>
            
            <article>
                
<p>Although there are numerous approaches to achieve data partitioning, the <kbd>caTools</kbd> package is one of the most useful. This package contains a function called <kbd>sample.split</kbd>, which generates random numbers to split a sample but keeps the proportion of <em>bads</em> and <em>goods</em> in the original dataset also in the separated samples.</p>
<p>As the <kbd>caTools</kbd> package uses random numbers, it is convenient to fix a <kbd>seed</kbd> to the replicability of the results:</p>
<pre>set.seed(1234)</pre>
<p>Then, use the <kbd>sample.split</kbd> function:</p>
<pre>library(caTools)<br/> index = sample.split(Model_database$Default, SplitRatio = .70)</pre>
<p>This function takes two arguments, the target variable and the partition size, in our case, the 70%.</p>
<p>It generates an <kbd>index</kbd> with two values, <kbd>TRUE</kbd> and <kbd>FALSE</kbd>, which can be used to split the dataset into the two desired samples:</p>
<pre>train&lt;-subset(Model_database, index == TRUE)<br/>test&lt;-subset(Model_database, index == FALSE)</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checking samples</h1>
                </header>
            
            <article>
                
<p>Let’s check the number of observations and the proportion of failed banks over the total banks in each sample:</p>
<pre>print("The development sample contains the following number of observations:")<br/><br/> ## [1] "The development sample contains the following number of observations:"<br/><br/>nrow(train)<br/><br/> ## [1] 7091<br/><br/>print("The average number of failed banks in the sample is:")<br/><br/> ## [1] "The average number of failed banks in the sample is:"<br/><br/>(sum(train$Default)/nrow(train))<br/><br/> ## [1] 0.04696094<br/><br/>print("The validation sample contains the following number of observations:")<br/><br/> ## [1] "The validation sample contains the following number of observations:"<br/><br/>nrow(test)<br/><br/> ## [1] 3039<br/><br/>print("The average number of failed banks in the sample is:")<br/><br/> ## [1] "The average number of failed banks in the sample is:"<br/><br/>(sum(test$Default)/nrow(test))<br/><br/> ## [1] 0.04705495</pre>
<p>As can be seen, train and test samples represent 70% and 30% of the total sample respectively. Both samples maintain approximately the same ratio of failed banks, that is, 4.7%.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing descriptive analysis</h1>
                </header>
            
            <article>
                
<p>Descriptive statistical analysis helps you to understand your data properly. Although R provides some functions by default to perform basic statistics, we will use two better alternatives, the <kbd>DataExplorer</kbd> and <kbd>fBasics</kbd> packages.</p>
<p>Follow these simple steps:</p>
<ol>
<li>As the number of variables in the dataset is high, we will create a list with the variable names to use in our descriptive functions:</li>
</ol>
<pre style="padding-left: 60px">Class&lt;-as.data.frame(sapply(train, class))<br/> colnames(Class)&lt;-"variable_class"<br/> Class$variable_name&lt;-colnames(train)<br/><br/>numeric_vars&lt;-Class[Class$variable_class=="numeric","variable_name"]</pre>
<ol start="2">
<li>A list of 1,492 variables is created. Pass this list to the <kbd>basicStats</kbd> function included in the <kbd>fBasics</kbd> package:</li>
</ol>
<pre style="padding-left: 60px">library(fBasics)<br/> descriptives_num&lt;-             as.data.frame(t(basicStats(train[,numeric_vars])))<br/> head(descriptives_num)</pre>
<p style="padding-left: 60px">We can calculate the following descriptives:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Number of observations (<kbd>nobs</kbd>)</li>
<li>Number of missing values (<kbd>NAs</kbd>)</li>
<li>Minimum value (<kbd>Minimum</kbd>)</li>
<li>Maximum value (<kbd>Maximum</kbd>)</li>
<li>First and third quartiles (<kbd>1. Quartile</kbd> and <kbd>3. Quartile</kbd>)</li>
<li>Median (<kbd>Median</kbd>)</li>
<li>Sum of the values in the variable (<kbd>Sum</kbd>)</li>
<li>Standard error of the mean (<kbd>SE Mean</kbd>)</li>
<li>Lower Confidence Limit (<kbd>LCL Mean</kbd>)</li>
<li>Upper Confidence Limit (<kbd>UCL Mean</kbd>)</li>
<li>Variance (<kbd>Variance</kbd>)</li>
<li>Standard deviation (<kbd>Stdev</kbd>)</li>
<li>Skewness (<kbd>Skewness</kbd>)</li>
<li>Kurtosis (<kbd>Kurtosis</kbd>)</li>
</ul>
</li>
</ul>
<ol start="3">
<li>In this step, we will detect variables with a high number of missing values, the range, and dispersion of variables, even if a variable has only a unique value.</li>
</ol>
<p>When the number of variables is high, as in our case, this task is not so easy and we need some time to analyze variables. A graphical analysis of variables is also important and complementary.</p>
<p>The <kbd>plot_histogram</kbd> function is very useful for visualizing variables. This function is available in the <kbd>DataExplorer</kbd> package:</p>
<pre>library(DataExplorer)<br/>plot_histogram(train[,1410:1441])</pre>
<p>The following diagrams display the output of the preceding code. These diagrams show a histogram for some of the variables in the data. Here's the first page of the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-684 image-border" src="assets/a7fd1cda-5444-4cdd-a4f9-6ed83a3b549a.png" style=""/></div>
<p>Here is the second page of the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-685 image-border" src="assets/a3c76d93-275a-4073-a805-f55e516dcd31.png" style=""/></div>
<p><span>This analysis of distribution of variables is needed to not only understand the distribution of variables but also to detect potential problems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dealing with outliers</h1>
                </header>
            
            <article>
                
<p>An important issue is the detection of outliers in the data. <strong>Outliers</strong> are the values that look different than a certain set of observations. Consider an example of a normal distribution, wherein the values at the tail of the distribution can be known as outliers. They are not so closely related to the nearest value of the sample.</p>
<p>There are some algorithms very sensitive to outliers, so its treatment is not a trivial issue. Detecting  outliers is easier if the number of variables is low.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The winsorization process</h1>
                </header>
            
            <article>
                
<p>When the number of outliers is high, we need to to use automatic procedures that help to automatically detect them. One of the most effective ways to avoid problems with outliers is the <strong>winsorization</strong> <strong>process</strong>.</p>
<p>According to this approach, outliers values will be replaced with fixed values. If a variable takes a value smaller than a specific threshold, this value will be replaced for this limit. The same situation occurs for high values in the variable.</p>
<p>Ideally, these limits or thresholds are based on percentiles. Some percentiles such as 1, 2.5, or 5 on the lower range, and 95, 97.5, and 99 on the upper one, can be selected for the winsorization technique, although other approaches can be chosen such as the use of the interquartile range.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing winsorization</h1>
                </header>
            
            <article>
                
<p>Let’s put the winsorization approach into practice. First, we need to know the position of ratios in the dataset:</p>
<pre>head(colnames(train))<br/><br/> ## [1] "ID_RSSD"  "UBPR1795" "UBPR4635" "UBPRC233" "UBPRD582" "UBPRE386"<br/><br/>tail(colnames(train))<br/><br/> ## [1] "UBPRE541" "UBPRE542" "UBPRJ248" "UBPRK447" "Date"     "Default"</pre>
<p>So, we need to apply the technique to all the variables, excluding the first and the last two variables.</p>
<p>All the transformations done in the training set should be applied later in the test dataset. Modifications in the test sample will be done using the limits of the training data. We will do winsorization for both datasets:</p>
<pre>for (k in 2:(ncol(train)-2))<br/>{<br/>   variable&lt;-as.character(colnames(train)[k])<br/>   limits &lt;- quantile(train[,k], probs=c(.01, .99), na.rm = TRUE)<br/>   train[complete.cases(train[,k]) &amp; train[,k] &lt;         as.numeric(limits[1]),k] &lt;-      as.numeric(limits[1])<br/>   train[complete.cases(train[,k]) &amp; train[,k] &gt; as.numeric(limits[2]),k] &lt;-      as.numeric(limits[2])<br/> test[complete.cases(test[,k]) &amp; test[,k] &lt; as.numeric(limits[1]),k]    &lt;- as.numeric(limits[1])<br/> test[complete.cases(test[,k]) &amp; test[,k] &gt; as.numeric(limits[2]),k] &lt;-as.numeric(limits[2])<br/> }</pre>
<p>For each variable, this will calculate the first and the ninety-ninth percentiles in the training set. Then, values of outliers exceeding the value of the <span>ninety-ninth</span> percentile, or a value smaller than the first percentile are replaced with the value of these corresponding percentiles. This means that it establishes a maximum and minimum value for each value fixed in the first and the ninety-ninth percentiles. This procedure is done for both train and test samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distinguishing single valued variables</h1>
                </header>
            
            <article>
                
<p>Now, we are going to calculate the number of unique values that a variable takes. Thus, if a variable only takes one single value, it can be directly removed from the dataset.</p>
<p>The <kbd>sapply</kbd> function allows the calculation of the <kbd>n_distinct</kbd> values of each variable. Create a new data frame with the name of the constant variables:</p>
<pre>library(dplyr)<br/><br/>unique_values&lt;-as.data.frame(sapply(train, n_distinct))</pre>
<p>Rename the name of the variable in this data frame:</p>
<pre>colnames(unique_values)&lt;-"Unique_values"</pre>
<p>Add a column in the data frame containing the name of the variables:</p>
<pre>unique_values$variable_name&lt;-colnames(train)</pre>
<p>Then create a list with the name of the constant variables:</p>
<pre>variables_to_remove&lt;-unique_values[unique_values$Unique_values==1,"variable_name"]<br/>length(variables_to_remove)<br/><br/> ## [1] 84</pre>
<p>Only <kbd>84</kbd> variables have a unique distinct value. These variables will be removed in the <kbd>train</kbd> and <kbd>test</kbd> samples:</p>
<pre> train&lt;-train[, !colnames(train) %in% variables_to_remove]<br/> test&lt;-test[, !colnames(test) %in% variables_to_remove]</pre>
<p>One of the problems of winsorization is that, if a variable displays a low number of different values, it can replace all the values with only one value . That's because a variable could take the same value during several percentile levels. It is important to be aware of the pros and cons of each procedure and its effect on the development.</p>
<p>Remember to save your workspace:</p>
<pre>save.image("Data6.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Treating missing information</h1>
                </header>
            
            <article>
                
<p>Most of the algorithms fail when the data includes missing values or takes a predetermined action about how to deal with them in an automatic way. It is important to take control when this happens.</p>
<p>Two actions are the most common to deal with missing information: to remove the observations with missing values or to replace them with a concrete value, usually the median or mean. When a value is imputed, you could be losing important information. For example, a missing value of the variable can be always observed in one of the classes of the target variable. A typical case is a model where we are trying to predict good and bad applicants for a bank loan.</p>
<p>It is common to have variables related to the number of days with some payment problems in the past. Sometimes, and depending on the dataset, these variables display a missing value simply because the applicant did not have previous problems. This is typically the case when the imputation of a value could lead us to lose relevant information.</p>
<p>Detailed analysis of missing values is most common when the number of variables is low. If the number of variables is high, some automatic alternative could be a more efficient approach.</p>
<p>Before taking an action on the missing values, let’s find the number of missing values by analyzing columns and rows. You can then remove the variables and rows (also known as observations) that have a high number of missing values.</p>
<p>In any case, thresholds to remove variables or observations are subjective, and it depends on the specific case of their application.</p>
<p>Using the <kbd>DataExplorer</kbd> package, it is possible to find the percentage of missing values in our data. Let’s try it with a small number of variables:</p>
<pre>plot_missing(train[,c(6:8,1000:1020)])</pre>
<p>The preceding line of code will print a graph similar to this:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-686 image-border" src="assets/ffd51532-474e-4ae0-916f-e9067258bb4e.png" style=""/></div>
<p>The preceding graph is only an example of the representation of missing values for some variables using the <kbd><span>DataExplorer</span></kbd> package. This package also provides recommendations about the usefulness of using a variable depending on its number of missing values.</p>
<p><span>We also have another way to determine variables with missing values. This is an alternative if you do not have access to the</span> <kbd>DataExplorer</kbd> <span>package, or simply don't wish to use it. This is more of a manual way. Let's write the</span> code:</p>
<pre>ncol=rep(nrow(train) ,each=ncol(train))<br/> missingdata=as.data.frame(cbind(colnames=names(train),ncol,nmsg=as.integer(as.character(as.vector(apply(train, 2, function(x) length(which(is.na(x)))))))))<br/>missingdata$nmsg=as.numeric(levels(missingdata$nmsg))[missingdata$nmsg]<br/>missingdata=cbind(missingdata,percmissing=(missingdata$nmsg/ncol*100))<br/><br/>head(missingdata)<br/><br/> ##   colnames ncol nmsg percmissing<br/> ## 1  ID_RSSD 7091    0           0<br/> ## 2 UBPR1795 7091    0           0<br/> ## 3 UBPR4635 7091    0           0<br/> ## 4 UBPRC233 7091    0           0<br/> ## 5 UBPRD582 7091    0           0<br/> ## 6 UBPRE386 7091    0           0</pre>
<p>For example, we can check variables where missing values represent more than 99% of the total observations (only a few lines are shown here):</p>
<pre>print(missingdata[missingdata$percmissing&gt;=99,])<br/><br/> ##      colnames ncol nmsg percmissing<br/> ## 19   UBPRE406 7091 7066    99.64744<br/> ## 26   UBPRE413 7091 7028    99.11155<br/> ## 35   UBPRFB69 7091 7038    99.25257<br/> ## 121  UBPRE137 7091 7048    99.39360<br/> ## 161  UBPRE184 7091 7046    99.36539<br/> ## 1347 UBPRE855 7091 7073    99.74616<br/> ## 1348 UBPRE856 7091 7047    99.37950<br/> ## 1356 UBPRE864 7091 7083    99.88718<br/> ## 1360 UBPRE868 7091 7056    99.50642</pre>
<p>In this case, I prefer not to remove any variables considering its number of missing values. There are no empty variables:</p>
<pre>print(missingdata[missingdata$percmissing==100,])<br/><br/> ## [1] colnames    ncol        nmsg        percmissing<br/> ## &lt;0 rows&gt; (or 0-length row.names)</pre>
<p>Let's count the number of missing values by analyzing the rows:</p>
<pre>train$missingvalues&lt;-rowSums(is.na(train[,2:1410]))/1409</pre>
<p>Now plot a histogram to graphically depict the distribution of the missing values:</p>
<pre>hist(train$missingvalues,main="Distribution of missing values",xlab="Percentage of missing values",border="blue", col="red",breaks=25)</pre>
<p class="CDPAlignLeft CDPAlign">The preceding code generates the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-687 image-border" src="assets/85f13606-976a-43ae-9209-0d0c24447437.png" style=""/></div>
<p>A summary of the percentage of missing values from banks can be obtained using the following code:</p>
<pre>summary(train$missingvalues)<br/><br/> ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.<br/> ## 0.06671 0.44003 0.47480 0.46779 0.50958 0.74663</pre>
<p>Although the number of missing values is high for some banks, it is recommended to not remove any observations for now, but to only remove the recently created missing variable:</p>
<pre>train$missingvalues&lt;-NULL</pre>
<p>An interesting package that we will use to visualize the number of missing values in the dataset is the <kbd>Amelia</kbd> package. It is a package for multiple imputations of missing data, which also includes a graphical interface.</p>
<p>Let’s see an example for some variables:</p>
<pre>library(Amelia)<br/><br/>missmap(train[,5:35], main = "Missing values vs observed",col=c("black", "grey"),,legend = FALSE)</pre>
<p>The preceding code generates the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-688 image-border" src="assets/a4025f79-7e3c-4cdc-8017-6a127d559e3e.png" style=""/></div>
<p>Although the representation is not very pretty, this graph displays some variables from the <em>x</em> axis and the observations in the dataset from the <em>y</em> axis. Black points indicate the presence of missing variables in the dataset.</p>
<p>As shown, some of variables display a high number of missing values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the missing value</h1>
                </header>
            
            <article>
                
<p>We've already mentioned that it is important to understand the origin of missing values and whether they can offer us some information. In the following example, we will analyze the missing values presented in the <kbd>UBPRE628</kbd> variable. This variable measures the amount of total long-term debt of a bank divided by the total bank equity capital. The c<span>apital of a bank is important because in the case of losses faced in the operations, this will be used by the bank to absorb them and to avoid a future insolvency. The higher the capital, the higher the buffer of a bank to face economic problems.</span></p>
<p>In general, the highest proportion of debt related to the bank capital, the more problems the bank could experience in the future if, for example, a new crisis occurs. In the event of a new financial crisis, a bank couldn’t repay its debt, even by selling its assets.</p>
<p>According to our analysis, this variable displays a high percentage of missing values, specifically this ratio is not informed for the 23.97% of banks in our data:</p>
<pre>missingdata[missingdata$colnames=="UBPRE628",]<br/><br/> ##     colnames ncol nmsg percmissing<br/> ## 281 UBPRE628 7091   17   0.2397405</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the results</h1>
                </header>
            
            <article>
                
<p>Now we will create an auxiliary data frame to count the number of failed banks and check whether this ratio is informed or not:</p>
<pre>missing_analysis&lt;-train[,c("UBPRE628","Default")]</pre>
<p>Now we will create a flag to check whether the variable is missing:</p>
<pre>missing_analysis$is_miss&lt;-ifelse(is.na(missing_analysis$UBPRE628),"missing_ratio","complete_ratio")</pre>
<p>Finally, let's sum up the number of existing defaults in the dataset for both the cases: the presence or lack of the missing values in this ratio:</p>
<pre>aggregate(missing_analysis$Default, by = list(missing_analysis$is_miss), sum)<br/><br/> ##          Group.1   x<br/> ## 1 complete_ratio 319<br/> ## 2  missing_ratio  14</pre>
<p>According to this table, only <kbd>14</kbd> failed banks displayed a missing value in this ratio. Apparently, we could conclude from this that a bank could intentionally not report a specific ratio because the calculated ratio could alert others about a bad economic situation of this bank. In this case, we don't observe a high proportion of bad banks if a missing value is observed.</p>
<p>Missing values will be estimated by calculating the mean of the ratio of the non-missing observations on the training dataset. This means that, if missing values are present in the validation dataset, they may also be present in the training dataset. Let's see an example:</p>
<pre>train_nomiss&lt;-train<br/>test_nomiss&lt;-test<br/> <br/> for(i in 2:(ncol(train_nomiss)-2))<br/>   {<br/>   train_nomiss[is.na(train_nomiss[,i]), i] &lt;- mean(train_nomiss[,i],      na.rm =          TRUE)<br/>   test_nomiss[is.na(test_nomiss[,i]), i] &lt;- mean(train_nomiss[,i],      na.rm = TRUE) <br/>   }</pre>
<p>We can check whether the process has worked using the <kbd>Amelia</kbd> package on both training and validation samples (it may take a few minutes). For example, you can check whether there are missing values in the training sample after the process has executed:</p>
<pre>missmap(train_nomiss[,2:(ncol(train_nomiss)-2)], main = "Missing values vs observed",col=c("black", "grey"),,legend = FALSE)</pre>
<p>Here is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-689 image-border" src="assets/0c603227-368f-48a2-a948-88201fd86bb3.png" style=""/></div>
<p>Carry out the same checks for the test sample:</p>
<pre>missmap(test_nomiss[,2:(ncol(train_nomiss)-2)], main = "Missing values vs observed",col=c("black", "grey"),,legend = FALSE)</pre>
<p>Again, a new output is displayed:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-690 image-border" src="assets/41df5e03-9ca2-4ad6-acb1-6683eef9f9b4.png" style=""/></div>
<p class="mce-root">Two maps are plotte<span>d in a gray color, indicating that there</span> are no missing values<span>.</span></p>
<p>Now we are going to make a new backup of our workspace and remove all the unnecessary tables:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","train_nomiss","test_nomiss")))<br/>save.image("Data7.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned some initial important steps to prepare and understand our data. How many variables are available in our dataset? What kind of information do we have? Are there some missing values in the data? How can I treat missing values and outliers? I hope you can now answer these questions.</p>
<p>Moreover, in this chapter, we also learned how to split our data to train and validate our forthcoming predictive model. In the next chapter, we will advance one step ahead, performing a univariate analysis on this data, which means analyzing whether variables are useful for predicting bank failures.</p>


            </article>

            
        </section>
    </body></html>