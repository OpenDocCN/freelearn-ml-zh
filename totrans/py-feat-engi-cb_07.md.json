["```py\n     import pandas as pd\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    X.drop(labels=[\"Latitude\", \"Longitude\"], axis=1,\n        inplace=True)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     scaler = StandardScaler().set_output(\n        transform=\"pandas\")\n    scaler.fit(X_train)\n    ```", "```py\n     X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    ```", "```py\n     scaler.mean_\n    ```", "```py\n    <st c=\"6133\">scaler</st>:\n\n    ```", "```py\n\n    <st c=\"6331\">Let’s compare the transformed data with the original data to understand</st> <st c=\"6404\">the changes.</st>\n    ```", "```py\n     X_test.describe()\n    ```", "```py\n     X_test_scaled.describe()\n    ```", "```py\n     import pandas as pd\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    X.drop(labels=[\"Latitude\", \"Longitude\"], axis=1,\n        inplace=True)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     scaler = MinMaxScaler().set_output(\n        transform=\"pandas\"\")\n    scaler.fit(X_train)\n    ```", "```py\n     X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    ```", "```py\n<st c=\"13747\">MedInc           0.000000</st>\n<st c=\"13763\">HouseAge        0.000000</st>\n<st c=\"13781\">AveRooms        0.004705</st>\n<st c=\"13799\">AveBedrms      0.004941</st>\n<st c=\"13818\">Population     0.000140</st>\n<st c=\"13838\">AveOccup      -0.000096</st>\n<st c=\"13886\">X_test_scaled.max()</st>, we see that the maximum values of the variables are around <st c=\"13966\">1</st>:\n\n```", "```py\n\n\t\t\t<st c=\"14091\">Note</st>\n\t\t\t<st c=\"14096\">If you check the maximum values of the variables in the train set after the transformation, you’ll see that they are exactly</st> `<st c=\"14222\">1</st>`<st c=\"14223\">. Yet, in the test set, we see values greater and smaller than</st> `<st c=\"14286\">1</st>`<st c=\"14287\">. This occurs because, in the test set, there are observations with larger or smaller magnitudes than those in the train set.</st> <st c=\"14413\">In fact, we see the greatest differences in the variables that deviate the most from the normal distribution (the last four variables in the dataset).</st> <st c=\"14564\">This behavior is expected because scaling to the minimum and maximum values is sensitive to outliers and very</st> <st c=\"14674\">skewed distributions.</st>\n\t\t\t<st c=\"14695\">Scaling to the</st> <st c=\"14710\">minimum and maximum value does not change the shape</st> <st c=\"14762\">of the variable’s distribution.</st> <st c=\"14795\">You can corroborate that by displaying the histograms before and after</st> <st c=\"14866\">the tr</st><st c=\"14872\">ansformation.</st>\n\t\t\t<st c=\"14886\">How it works...</st>\n\t\t\t<st c=\"14902\">In this rec</st><st c=\"14914\">ipe, we scaled the variables of the California housing dataset to values between</st> `<st c=\"14996\">0</st>` <st c=\"14997\">and</st> `<st c=\"15001\">1</st>`<st c=\"15002\">.</st>\n\t\t\t`<st c=\"15003\">MinMaxScaler()</st>` <st c=\"15018\">from scikit-learn learned the minimum and maximum values and the value range of each variable when we called the</st> `<st c=\"15132\">fit()</st>` <st c=\"15137\">method and stored these parameters in its</st> `<st c=\"15180\">data_max_</st>`<st c=\"15189\">,</st> `<st c=\"15191\">min_</st>`<st c=\"15195\">, and</st> `<st c=\"15201\">data_range_</st>` <st c=\"15212\">attributes.</st> <st c=\"15225\">By using</st> `<st c=\"15234\">transform()</st>`<st c=\"15245\">, we made the scaler remove the minimum value from each variable in the train and test sets and divide the result by the</st> <st c=\"15366\">value range.</st>\n\t\t\t<st c=\"15378\">Note</st>\n\t\t\t`<st c=\"15383\">MinMaxScaler()</st>` <st c=\"15398\">will scale all variables by default.</st> <st c=\"15436\">To scale only a subset of the variables in the dataset, you can use</st> `<st c=\"15504\">ColumnTransformer()</st>` <st c=\"15523\">from scikit-learn or</st> `<st c=\"15545\">SklearnTransformerWrapper()</st>` <st c=\"15572\">from</st> `<st c=\"15578\">Feature-engine</st>`<st c=\"15592\">.</st>\n\t\t\t`<st c=\"15593\">MinMaxScaler()</st>` <st c=\"15608\">will scale the variables between</st> `<st c=\"15642\">0</st>` <st c=\"15643\">and</st> `<st c=\"15648\">1</st>` <st c=\"15649\">by default.</st> <st c=\"15662\">However, we have the option to scale to a different range by adjusting the tuple passed to the</st> `<st c=\"15757\">feature_range</st>` <st c=\"15770\">parameter.</st>\n\t\t\t<st c=\"15781\">By default,</st> `<st c=\"15794\">MinMaxScaler()</st>` <st c=\"15808\">returns</st> <st c=\"15817\">NumPy arrays, but we can modify this</st> <st c=\"15854\">behavior to return</st> `<st c=\"15873\">pandas</st>` <st c=\"15879\">DataFrames with the</st> `<st c=\"15900\">set_output()</st>` <st c=\"15912\">method, as we</st> <st c=\"15926\">did in</st> *<st c=\"15934\">Step 4</st>*<st c=\"15940\">.</st>\n\t\t\t<st c=\"15941\">Scaling with the median and quantiles</st>\n\t\t\t<st c=\"15979\">When</st> <st c=\"15985\">scaling variab</st><st c=\"15999\">les to</st> <st c=\"16007\">the median and quantiles, the median value is removed from the observations, and the result is divided by</st> <st c=\"16112\">the</st> **<st c=\"16117\">Inter-Quarti</st><st c=\"16129\">le Range</st>** <st c=\"16138\">(</st>**<st c=\"16140\">IQR</st>**<st c=\"16143\">).</st> <st c=\"16147\">The IQR is the difference between the 3rd quartile and the 1st quartile, or, in other words, the difference between the 75th percentile and the</st> <st c=\"16291\">25th percent</st><st c=\"16303\">ile:</st>\n\t\t\t![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mi>x</mi><mo>_</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mn>3</mn><mi>r</mi><mi>d</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>l</mi><mi>e</mi><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>−</mo><mn>1</mn><mi>s</mi><mi>t</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/27.png)\n\t\t\t<st c=\"16310\">This method is known</st> <st c=\"16330\">as</st> **<st c=\"16334\">robust</st> <st c=\"16340\">scaling</st>** <st c=\"16348\">because it produces more robust estimates for the center and value range of the variable.</st> <st c=\"16439\">Robust scaling is a suitable alternative to standardization when models require the variables to be centered and the data contains outliers.</st> <st c=\"16580\">It is worth noting that robust scaling will not change the overall shape of the</st> <st c=\"16660\">vari</st><st c=\"16664\">able distribution.</st>\n\t\t\t<st c=\"16683\">How to do it...</st>\n\t\t\t<st c=\"16699\">In this recipe, we will implement scaling with the median and IQR by</st> <st c=\"16769\">utilizing scikit-learn:</st>\n\n\t\t\t\t1.  <st c=\"16792\">Let’s start</st> <st c=\"16805\">by importing</st> `<st c=\"16818\">pandas</st>` <st c=\"16824\">and the required scikit-learn classes</st> <st c=\"16863\">and functions:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"17051\">Let’s load the California housing dataset into a</st> `<st c=\"17101\">pandas</st>` <st c=\"17107\">DataFrame and drop the</st> `<st c=\"17131\">Latitude</st>` <st c=\"17139\">and</st> `<st c=\"17144\">Longitude</st>` <st c=\"17153\">variables:</st>\n\n    ```", "```py\n\n    \t\t\t\t3.  <st c=\"17294\">Let’s divide the data into train and</st> <st c=\"17332\">test sets:</st>\n\n    ```", "```py\n\n    \t\t\t\t4.  <st c=\"17432\">Let’s set up scikit-learn’s</st> `<st c=\"17461\">RobustScaler()</st>`<st c=\"17475\">and fit it to the train set so that it learns and stores the median</st> <st c=\"17544\">and IQR:</st>\n\n    ```", "```py\n\n    \t\t\t\t5.  <st c=\"17628\">Finally, let’s</st> <st c=\"17643\">scale the</st> <st c=\"17654\">variables in the train and test sets with the</st> <st c=\"17700\">trained scaler:</st>\n\n    ```", "```py\n\n    \t\t\t\t6.  <st c=\"17799\">Let’s print the variable median values learned</st> <st c=\"17847\">by</st> `<st c=\"17850\">RobustScaler()</st>`<st c=\"17864\">:</st>\n\n    ```", "```py\n\n    <st c=\"17881\">We see the parameters learned by</st> `<st c=\"17915\">RobustScaler()</st>` <st c=\"17929\">in the</st> <st c=\"17937\">following output:</st>\n\n    ```", "```py\n     scaler.scale_\n    <st c=\"18189\">array([2.16550000e+00, 1.90000000e+01, 1.59537022e+00,</st> <st c=\"18244\">9.41284380e-02, 9.40000000e</st><st c=\"18272\">+02, 8.53176853e-01])</st>\n    ```", "```py\n\n\t\t\t<st c=\"18473\">How it works...</st>\n\t\t\t<st c=\"18489\">To scale the</st> <st c=\"18502\">features using</st> <st c=\"18517\">the median and IQR, we created an instance of</st> `<st c=\"18564\">RobustScaler()</st>`<st c=\"18578\">. With</st> `<st c=\"18585\">fit()</st>`<st c=\"18590\">, the scaler learned the median and IQR for each variable from the train set.</st> <st c=\"18668\">With</st> `<st c=\"18673\">transform()</st>`<st c=\"18684\">, the scaler subtracted the median from each variable in the train and test sets and divided the result by</st> <st c=\"18791\">the IQR.</st>\n\t\t\t<st c=\"18799\">After the transformation, the median values of the variables were centered at</st> `<st c=\"18878\">0</st>`<st c=\"18879\">, but the overall shape of the distributions did not change.</st> <st c=\"18940\">You can corroborate the effect of the transformation by displaying the histograms of the variables before and after the transformation and by printing out the main statistical parameters through</st> `<st c=\"19135\">X_test.describe()</st>` <st c=\"19152\">and</st> `<st c=\"19157\">X_test_scaled.b()</st>`<st c=\"19174\">.</st>\n\t\t\t<st c=\"19175\">Performing mean normalization</st>\n\t\t\t<st c=\"19205\">I</st><st c=\"19207\">n mean normalization, we</st> <st c=\"19232\">center the variable at</st> `<st c=\"19255\">0</st>` <st c=\"19256\">and rescale the distribution to the value range, so that its values lie between</st> `<st c=\"19337\">-1</st>` <st c=\"19339\">and</st> `<st c=\"19344\">1</st>`<st c=\"19345\">. This procedure involves subtracting the mean from each observation and then dividing the result by the difference between the minimum and maximum values, as</st> <st c=\"19504\">sh</st><st c=\"19506\">own here:</st>\n\t\t\t![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>max</mi><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>−</mo><mi mathvariant=\"normal\">m</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/28.png)\n\t\t\t<st c=\"19558\">Note</st>\n\t\t\t<st c=\"19562\">Mean normalization is an alternative to standardization.</st> <st c=\"19620\">In both cases, the variables are centered at</st> `<st c=\"19665\">0</st>`<st c=\"19666\">. In mean normalization, the variance varies, while the values lie between</st> `<st c=\"19741\">-1</st>` <st c=\"19743\">and</st> `<st c=\"19748\">1</st>`<st c=\"19749\">. On the other hand, in standardization, the variance is set to</st> `<st c=\"19813\">1</st>` <st c=\"19814\">and the value</st> <st c=\"19829\">range varies.</st>\n\t\t\t<st c=\"19842\">Mean</st> <st c=\"19848\">normalization is a suitable alternative for models that need the variables to be centered at zero.</st> <st c=\"19947\">However, it is sensitive to outliers and not a suitable option for sparse data, as it will d</st><st c=\"20039\">estroy the</st> <st c=\"20051\">sparse nature.</st>\n\t\t\t<st c=\"20065\">How to do it...</st>\n\t\t\t<st c=\"20081\">In this recipe, we will implement mean normalization</st> <st c=\"20135\">with</st> `<st c=\"20140\">pandas</st>`<st c=\"20146\">:</st>\n\n\t\t\t\t1.  <st c=\"20148\">Let’s import</st> `<st c=\"20161\">pandas</st>` <st c=\"20167\">and the required scikit-learn class</st> <st c=\"20204\">and function:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"20344\">Let’s load the California housing dataset from scikit-learn into a</st> `<st c=\"20412\">pandas</st>` <st c=\"20418\">DataFrame, dropping the</st> `<st c=\"20443\">Latitude</st>` <st c=\"20451\">and</st> `<st c=\"20456\">Longitude</st>` <st c=\"20465\">variables:</st>\n\n    ```", "```py\n\n    \t\t\t\t3.  <st c=\"20606\">Let’s divide the data into train and</st> <st c=\"20644\">test sets:</st>\n\n    ```", "```py\n\n    \t\t\t\t4.  <st c=\"20744\">Let’s learn the mean values from the variables in the</st> <st c=\"20799\">train set:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"20839\">Note</st>\n\t\t\t<st c=\"20844\">We set</st> `<st c=\"20852\">axis=0</st>` <st c=\"20858\">to take the mean of the rows – that is, of the observations in each variable.</st> <st c=\"20937\">If we set</st> `<st c=\"20947\">axis=1</st>` <st c=\"20953\">instead,</st> `<st c=\"20963\">pandas</st>` <st c=\"20969\">will calculate the mean value per observation across all</st> <st c=\"21027\">the columns.</st>\n\t\t\t<st c=\"21039\">By</st> <st c=\"21043\">executing</st> `<st c=\"21053\">print(mean)</st>`<st c=\"21064\">, we display the mean values</st> <st c=\"21093\">per variable:</st>\n\n```", "```py\n\n\t\t\t\t1.  <st c=\"21235\">Now, let’s determine the difference between the maximum and minimum values</st> <st c=\"21311\">per variable:</st>\n\n    ```", "```py\n\n    <st c=\"21373\">By executing</st> `<st c=\"21387\">print(ranges)</st>`<st c=\"21400\">, we display the value ranges</st> <st c=\"21430\">per variable:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"21578\">Note</st>\n\t\t\t<st c=\"21583\">The</st> `<st c=\"21588\">pandas</st>` `<st c=\"21594\">mean()</st>`<st c=\"21601\">,</st> `<st c=\"21603\">max()</st>`<st c=\"21608\">, and</st> `<st c=\"21614\">min()</st>` <st c=\"21619\">methods return a</st> `<st c=\"21637\">pandas</st>` <st c=\"21643\">series.</st>\n\n\t\t\t\t1.  <st c=\"21651\">Now, we’ll</st> <st c=\"21662\">apply mean normalization to the train and test se</st><st c=\"21712\">ts by utilizing the</st> <st c=\"21733\">learned parameters:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"21838\">Note</st>\n\t\t\t<st c=\"21843\">In order to transform future data, you will need to store these parameters, for example, in a</st> `<st c=\"21938\">.txt</st>` <st c=\"21942\">or</st> `<st c=\"21946\">.</st>``<st c=\"21947\">csv</st>` <st c=\"21951\">file.</st>\n\t\t\t*<st c=\"21957\">Step 6</st>* <st c=\"21964\">returns</st> `<st c=\"21973\">pandas</st>` <st c=\"21979\">DataFrames with the transformed train and test sets.</st> <st c=\"22033\">Go ahead and compare the variables before and after the transformations.</st> <st c=\"22106\">You’ll see that the distributions did not change, but the variables are centered at</st> `<st c=\"22190\">0</st>`<st c=\"22191\">, and their v</st><st c=\"22204\">alues lie between</st> `<st c=\"22223\">-1</st>` <st c=\"22225\">and</st> `<st c=\"22230\">1</st>`<st c=\"22231\">.</st>\n\t\t\t<st c=\"22232\">How it works…</st>\n\t\t\t<st c=\"22246\">To implement mean normalization, we captured the mean values of the numerical variables in the train set using</st> `<st c=\"22358\">mean()</st>` <st c=\"22364\">from</st> `<st c=\"22370\">pandas</st>`<st c=\"22376\">. Next, we determined the difference between the maximum and minimum values of the numerical variables in the train set by utilizing</st> `<st c=\"22509\">max()</st>` <st c=\"22514\">and</st> `<st c=\"22519\">min()</st>` <st c=\"22524\">from</st> `<st c=\"22530\">pandas</st>`<st c=\"22536\">. Finally, we used the</st> `<st c=\"22559\">pandas</st>` <st c=\"22565\">series returned by these functions containing the mean values and the value ranges to normalize the train and test sets.</st> <st c=\"22687\">We subtracted the mean from each observation in our train and test sets</st> <st c=\"22758\">and divided the result by the value ranges.</st> <st c=\"22803\">This returned the normalized vari</st><st c=\"22836\">ables in a</st> `<st c=\"22848\">pandas</st>` <st c=\"22854\">DataFrame.</st>\n\t\t\t<st c=\"22865\">There’s more...</st>\n\t\t\t<st c=\"22881\">There is no dedicated scikit-learn transformer to implement mean normalization, but we can combine the use of two transformers to</st> <st c=\"23012\">do so.</st>\n\t\t\t<st c=\"23018\">To do this, we need to import</st> `<st c=\"23049\">pandas</st>` <st c=\"23055\">and load the data, just like we did in</st> *<st c=\"23095\">Steps 1</st>* <st c=\"23102\">to</st> *<st c=\"23105\">3</st>* <st c=\"23107\">in the</st> *<st c=\"23115\">How to do it...</st>* <st c=\"23130\">section of this recipe.</st> <st c=\"23155\">Then, follow</st> <st c=\"23168\">these steps:</st>\n\n\t\t\t\t1.  <st c=\"23180\">Import the</st> <st c=\"23192\">scikit-learn transformers:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"23285\">Let’s set up</st> `<st c=\"23299\">StandardScaler()</st>` <st c=\"23315\">to learn and subtract</st> <st c=\"23338\">the mean without dividing the result by the</st> <st c=\"23382\">standard deviation:</st>\n\n    ```", "```py\n\n    \t\t\t\t3.  <st c=\"23496\">Now, let’s set up</st> `<st c=\"23515\">RobustScaler()</st>` <st c=\"23529\">so that it does not remove the median from the values but divides them by the value range – that is, the difference between the maximum and</st> <st c=\"23670\">minimum values:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"23813\">Note</st>\n\t\t\t<st c=\"23818\">To divide by the difference between the minimum and maximum values, we need to specify</st> `<st c=\"23906\">(0, 100)</st>` <st c=\"23914\">in the</st> `<st c=\"23922\">quantile_range</st>` <st c=\"23936\">argument</st> <st c=\"23946\">of</st> `<st c=\"23949\">RobustScaler()</st>`<st c=\"23963\">.</st>\n\n\t\t\t\t1.  <st c=\"23964\">Let’s fit the scalers to the train set so that they learn and store the mean, maximum, and</st> <st c=\"24056\">minimum values:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"24123\">Finally, let’s apply mean normalization to the train and</st> <st c=\"24181\">test sets:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"24339\">We transformed the data with</st> `<st c=\"24369\">StandardScaler()</st>` <st c=\"24385\">to remove the mean and then transformed the resulting DataFrame with</st> `<st c=\"24455\">RobustScaler()</st>` <st c=\"24469\">to divide the result by the range between the minimum and maximum values.</st> <st c=\"24544\">We described the functionality of</st> `<st c=\"24578\">StandardScaler()</st>` <st c=\"24594\">in this chapter’s</st> *<st c=\"24613\">Standardizing the features</st>* <st c=\"24639\">recipe and</st> `<st c=\"24651\">RobustScaler()</st>` <st c=\"24665\">in the</st> *<st c=\"24673\">Scaling with the median and quant</st><st c=\"24706\">iles</st>* <st c=\"24711\">recipe of</st> <st c=\"24722\">this chapter.</st>\n\t\t\t<st c=\"24735\">Implementing maximum absol</st><st c=\"24762\">ute scaling</st>\n\t\t\t<st c=\"24774\">Maximum absolute scaling</st> <st c=\"24800\">scales the data to its maximum value – that is, it divides every observation by the maximum value of</st> <st c=\"24901\">the variable:</st>\n\t\t\t![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mi>x</mi><mrow><mi mathvariant=\"normal\">m</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">x</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/29.png)\n\t\t\t<st c=\"24936\">As a result, the maximum value of each feature will be</st> `<st c=\"24991\">1.0</st>`<st c=\"24994\">. Note that maximum absolute scaling does not center the data, and hence, it’s suitable for scaling sparse data.</st> <st c=\"25107\">In this recipe, we will implement maximum absolute scaling</st> <st c=\"25166\">with scikit-learn.</st>\n\t\t\t<st c=\"25184\">Note</st>\n\t\t\t<st c=\"25189\">Scikit-learn recommends using this transformer on data that is cen</st><st c=\"25256\">tered at</st> `<st c=\"25266\">0</st>` <st c=\"25267\">or on</st> <st c=\"25274\">sparse data.</st>\n\t\t\t<st c=\"25286\">Getting ready</st>\n\t\t\t<st c=\"25300\">Maximum absolute scaling</st> <st c=\"25325\">was specifically designed to scale sparse data.</st> <st c=\"25374\">Thus, we will use a bag-of-words dataset that contains sparse variables for the recipe.</st> <st c=\"25462\">In this dataset, the variables are words, the observations are documents, and the values are the number of times each word appears in the document.</st> <st c=\"25610\">Most entries</st> <st c=\"25623\">in the data</st> <st c=\"25635\">are</st> `<st c=\"25639\">0</st>`<st c=\"25640\">.</st>\n\t\t\t<st c=\"25641\">We will use a dataset consisting of a bag of words, which is available in the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Bag+of+Words), which is licensed under CC BY</st> <st c=\"25838\">4.0 (</st><st c=\"25843\">https://creativecommons.org/licenses/by/4.0/legalcode</st><st c=\"25897\">).</st>\n\t\t\t<st c=\"25900\">I downloaded and prepared a small bag of words representing a simplified version of one of those datasets.</st> <st c=\"26008\">You will find this dataset in the accompanying GitHub</st> <st c=\"26062\">repository:</st> [<st c=\"26074\">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch07-scaling</st>](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch07-scaling)<st c=\"26181\">.</st>\n\t\t\t<st c=\"26182\">How to do it...</st>\n\t\t\t<st c=\"26198\">Let’s begin by</st> <st c=\"26213\">importing the required packages and loading</st> <st c=\"26258\">the dataset:</st>\n\n\t\t\t\t1.  <st c=\"26270\">Let’s import the required libraries and</st> <st c=\"26311\">the scaler:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"26421\">Let’s load the</st> <st c=\"26437\">bag-of-words dataset:</st>\n\n    ```", "```py\n\n    <st c=\"26497\">If we execute</st> `<st c=\"26512\">data.head()</st>`<st c=\"26523\">, we will see the DataFrame consisting of the words as columns, the documents as rows, and the number of times each word appeared in a document</st> <st c=\"26667\">as values:</st>\n\n\t\t\t![Figure 7.5 – DataFrame with the bag of words](img/B22396_07_5.jpg)\n\n\t\t\t<st c=\"26944\">Figure 7.5 – DataFrame with the bag of words</st>\n\t\t\t<st c=\"26988\">Note</st>\n\t\t\t<st c=\"26993\">Although we omit this step in the recipe, remember that the maximum absolute values should be learned from a training dataset only.</st> <st c=\"27126\">Split the dataset into train and test sets when carrying out</st> <st c=\"27187\">your analysis.</st>\n\n\t\t\t\t1.  <st c=\"27201\">Let’s set</st> <st c=\"27211\">up</st> `<st c=\"27215\">MaxAbsScaler()</st>` <st c=\"27229\">and fit it to the data so that it learns the variables’</st> <st c=\"27286\">maximum values:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"27374\">Now, let’s scale the variables by utilizing the</st> <st c=\"27423\">trained scaler:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"27475\">Note</st>\n\t\t\t`<st c=\"27480\">MaxAbsScaler ()</st>` <st c=\"27496\">stores the maximum values in its</st> `<st c=\"27530\">max_ab</st><st c=\"27536\">s_</st>` <st c=\"27539\">attribute.</st>\n\n\t\t\t\t1.  <st c=\"27550\">Let’s display the maximum values stored by</st> <st c=\"27594\">the scaler:</st>\n\n    ```", "```py\n\n    <st c=\"27766\">To follow up, let’s plot the distributions of the original and</st> <st c=\"27830\">scaled variables.</st>\n\n    \t\t\t\t2.  <st c=\"27847\">Let’s make a histogram with the bag of words before</st> <st c=\"27900\">the scaling:</st>\n\n    ```", "```py\n\n    <st c=\"27960\">In the</st> <st c=\"27968\">following output, we see histograms with the number of times e</st><st c=\"28030\">ach word appears in</st> <st c=\"28051\">a document:</st>\n\n\t\t\t![Figure 7.6 – Histograms with differen﻿t word counts](img/B22396_07_6.jpg)\n\n\t\t\t<st c=\"28575\">Figure 7.6 – Histograms with differen</st><st c=\"28612\">t word counts</st>\n\n\t\t\t\t1.  <st c=\"28626\">Now, let’s make a histogram with the</st> <st c=\"28664\">scaled variables:</st>\n\n    ```", "```py\n\n    <st c=\"28736\">In the following output, we can corroborate the change of scale of the v</st><st c=\"28809\">ariables, but their</st> <st c=\"28830\">distr</st><st c=\"28835\">ibution shape remains</st> <st c=\"28858\">the same:</st>\n\n\t\t\t![Figure 7.7 – Histograms of the word counts after the scaling](img/B22396_07_7.jpg)\n\n\t\t\t<st c=\"29068\">Figure 7.7 – Histograms of the word counts after the scaling</st>\n\t\t\t<st c=\"29128\">With scaling to the maximum absolute value, we linearly scale down</st> <st c=\"29196\">the magnitude of</st> <st c=\"29213\">the features.</st>\n\t\t\t<st c=\"29226\">How</st> <st c=\"29231\">it works...</st>\n\t\t\t<st c=\"29242\">In this recipe, we</st> <st c=\"29261\">scaled the sparse variables of a bag of words to their absolute maximum values by using</st> `<st c=\"29350\">MaxAbsScaler()</st>`<st c=\"29364\">. With</st> `<st c=\"29371\">fit()</st>`<st c=\"29376\">, the scaler learned the maximum absolute values for each variable and stored them in its</st> `<st c=\"29466\">max_abs_</st>` <st c=\"29474\">attribute.</st> <st c=\"29486\">With</st> `<st c=\"29491\">transform()</st>`<st c=\"29502\">, the scaler divided the variables by their ab</st><st c=\"29548\">solute maximum values, returning a</st> `<st c=\"29584\">pandas</st>` <st c=\"29590\">DataFrame.</st>\n\t\t\t<st c=\"29601\">Note</st>\n\t\t\t<st c=\"29606\">Remember that you can change the output container to a NumPy array or a</st> `<st c=\"29679\">polars</st>` <st c=\"29685\">DataFrame through the</st> `<st c=\"29708\">set_output()</st>` <st c=\"29720\">method of the scik</st><st c=\"29739\">it-learn</st> <st c=\"29749\">library’s transformers.</st>\n\t\t\t<st c=\"29772\">There’s m</st><st c=\"29782\">ore...</st>\n\t\t\t<st c=\"29789\">If you want to center the variables’ distribution at</st> `<st c=\"29843\">0</st>` <st c=\"29844\">and then scale them to their absolute maximum, you can do so by combining the use of two scikit-learn transformers within</st> <st c=\"29967\">a pipeline:</st>\n\n\t\t\t\t1.  <st c=\"29978\">Let’s import the required libraries, transformers,</st> <st c=\"30030\">and functions:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"30275\">Let’s load the California housing dataset and split it into train and</st> <st c=\"30346\">test sets:</st>\n\n    ```", "```py\n\n    \t\t\t\t3.  <st c=\"30576\">Let’s set up</st> `<st c=\"30590\">StandardScaler()</st>` <st c=\"30606\">from scikit-learn so that it learns and subtracts the mean but does not divide the result by the</st> <st c=\"30704\">standard deviation:</st>\n\n    ```", "```py\n\n    \t\t\t\t4.  <st c=\"30785\">Now, let’s set up</st> `<st c=\"30804\">MaxAbsScaler()</st>` <st c=\"30818\">with its</st> <st c=\"30828\">default parameters:</st>\n\n    ```", "```py\n\n    \t\t\t\t5.  <st c=\"30878\">Let’s include both scalers within a pipeline that returns</st> <st c=\"30937\">pandas DataFrames:</st>\n\n    ```", "```py\n\n    \t\t\t\t6.  <st c=\"31070\">Let’s fit the scalers to the train set so that they learn the</st> <st c=\"31133\">required parameters:</st>\n\n    ```", "```py\n\n    \t\t\t\t7.  <st c=\"31173\">Finally, let’s transform the train and</st> <st c=\"31213\">test sets:</st>\n\n    ```", "```py\n\n    <st c=\"31307\">The pipeline applies</st> `<st c=\"31329\">StandardScaler()</st>` <st c=\"31345\">and</st> `<st c=\"31350\">MaxAbsScaler()</st>` <st c=\"31364\">in sequence to first remove the mean and then scale</st> <st c=\"31417\">the resulting</st> <st c=\"31431\">va</st><st c=\"31433\">riables to their</st> <st c=\"31451\">maximum values.</st>\n\n\t\t\t<st c=\"31466\">Scaling to vector unit length</st>\n\t\t\t<st c=\"31496\">Scaling to the vector unit length involves scaling individual observations (not features) to have a unit norm.</st> <st c=\"31608\">Each sample (that is, each row of the data) is rescaled independently of other samples so that its norm equals one.</st> <st c=\"31724\">Each</st> <st c=\"31729\">row constitutes a</st> **<st c=\"31747\">feature vector</st>** <st c=\"31761\">containing the values of every variable for that row.</st> <st c=\"31816\">Hence, with this scaling method, we rescale the</st> <st c=\"31864\">feature vector.</st>\n\t\t\t<st c=\"31879\">The norm of a vector is a measure of its magnitude or length in a given space and it can be determined by using the Manhattan (</st>*<st c=\"32007\">l1</st>*<st c=\"32010\">) or the Euclidean (</st>*<st c=\"32031\">l2</st>*<st c=\"32034\">) distance.</st> <st c=\"32047\">The Manhattan distance is given by the sum of the absolute components of</st> <st c=\"32120\">the vector:</st>\n\t\t\t![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mn>1</mn><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>=</mo><mo>|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>|</mo><mo>+</mo><mfenced open=\"|\" close=\"|\"><msub><mi>x</mi><mn>2</mn></msub></mfenced><mo>+</mo><mo>…</mo><mo>..</mo><mo>+</mo><mo>|</mo><msub><mi>x</mi><mi>n</mi></msub><mo>|</mo></mrow></mrow></mrow></math>](img/30.png)\n\t\t\t<st c=\"32163\">The Euclidean distance is given by the square root of the square sum of the component of</st> <st c=\"32252\">the vector:</st>\n\t\t\t![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mi>l</mi><mn>2</mn><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>=</mo><msqrt><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mo>…</mo><mo>+</mo><msubsup><mi>x</mi><mi>n</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mrow></math>](img/31.png)\n\t\t\t<st c=\"32284\">Here,</st> ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo></mrow></mrow></math>](img/32.png)<st c=\"32290\"><st c=\"32301\">and</st> ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>x</mi><mi>n</mi></msub></mrow></math>](img/33.png)<st c=\"32305\"><st c=\"32306\">are the values of variables</st> *<st c=\"32334\">1</st>*<st c=\"32335\">,</st> *<st c=\"32337\">2</st>*<st c=\"32338\">, and</st> *<st c=\"32344\">n</st>* <st c=\"32345\">for each observation.</st> <st c=\"32368\">Scaling to unit norm consists of dividing each feature vector’s value by either</st> *<st c=\"32448\">l1</st>* <st c=\"32450\">or</st> *<st c=\"32454\">l2</st>*<st c=\"32456\">, so that after the</st> <st c=\"32475\">scaling, the norm of the feature vector is</st> *<st c=\"32519\">1</st>*<st c=\"32520\">. To be clear, we divide each of</st> ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo></mrow></mrow></math>](img/34.png)<st c=\"32553\"><st c=\"32564\">and</st> ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo></mrow></mrow></math>](img/35.png)<st c=\"32568\"><st c=\"32569\">by</st> *<st c=\"32572\">l1</st>* <st c=\"32574\">or</st> *<st c=\"32578\">l2</st>*<st c=\"32580\">.</st></st></st></st></st>\n\t\t\t<st c=\"32581\">This scaling procedure changes the variables’ distribution, as illustrated in the</st> <st c=\"32664\">following figure:</st>\n\t\t\t![Figure 7.8 – Distribution of a normal and skewed variable before and after scaling each observation’s feature vector to its norm](img/B22396_07_8.jpg)\n\n\t\t\t<st c=\"33144\">Figure 7.8 – Distribution of a normal and skewed variable before and after scaling each observation’s feature vector to its norm</st>\n\t\t\t<st c=\"33272\">Note</st>\n\t\t\t<st c=\"33277\">This scaling technique scales each observation and not each variable.</st> <st c=\"33348\">The scaling methods that we discussed so far in this chapter aimed at shifting and resetting the scale of the variables’ distribution.</st> <st c=\"33483\">When we scale to the unit length, however, we normalize each observation individually, contemplating</st> <st c=\"33584\">their values across</st> <st c=\"33603\">all features.</st>\n\t\t\t<st c=\"33617\">Scaling</st> <st c=\"33626\">to the unit norm can be used when utilizing kernels to quantify similarity for text classification and clustering.</st> <st c=\"33741\">In this recipe, we will scale each observation’s feature vector to a un</st><st c=\"33812\">it length of</st> `<st c=\"33826\">1</st>` <st c=\"33827\">using scikit-learn.</st>\n\t\t\t<st c=\"33846\">How to do it...</st>\n\t\t\t<st c=\"33862\">To begin, we’ll</st> <st c=\"33879\">import the required packages, load the dataset, and prepare the train and</st> <st c=\"33953\">test sets:</st>\n\n\t\t\t\t1.  <st c=\"33963\">Let’s import the required Python packages, classes,</st> <st c=\"34016\">and functions:</st>\n\n    ```", "```py\n\n    \t\t\t\t2.  <st c=\"34221\">Let’s load the California housing dataset into a</st> `<st c=\"34271\">pandas</st>` <st c=\"34277\">DataFrame:</st>\n\n    ```", "```py\n\n    \t\t\t\t3.  <st c=\"34418\">Let’s divide the data into train and</st> <st c=\"34456\">test sets:</st>\n\n    ```", "```py\n\n    \t\t\t\t4.  <st c=\"34556\">Let’s set up the scikit-learn library’s</st> `<st c=\"34597\">Normalizer()</st>` <st c=\"34609\">transformer to scale each observation to the Manhattan distance</st> <st c=\"34674\">or</st> `<st c=\"34677\">l1</st>`<st c=\"34679\">:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"34712\">Note</st>\n\t\t\t<st c=\"34717\">To normalize to the Euclidean distance, you need to set the norm to</st> `<st c=\"34786\">l2</st>` <st c=\"34788\">using</st> `<st c=\"34795\">scaler =</st>` `<st c=\"34804\">Normalizer(no</st><st c=\"34817\">rm='l2')</st>`<st c=\"34826\">.</st>\n\n\t\t\t\t1.  <st c=\"34827\">Let’s</st> <st c=\"34834\">transform the train and test sets – that is, we’ll divide each observation’s feature vector by</st> <st c=\"34929\">its norm:</st>\n\n    ```", "```py\n\n    <st c=\"35026\">We can calculate the length (that is, the Manhattan distance of each observation’s feature vector) using</st> `<st c=\"35132\">linalg()</st>` <st c=\"35140\">from NumPy.</st>\n\n    \t\t\t\t2.  <st c=\"35152\">Let’s calculate the norm (Manhattan distance) before scaling</st> <st c=\"35214\">the variables:</st>\n\n    ```", "```py\n\n    <st c=\"35280\">As expected, the norm of each</st> <st c=\"35311\">observation varies:</st>\n\n    ```", "```py\n\n    \t\t\t\t3.  <st c=\"35389\">Let’s now calculate the norm after</st> <st c=\"35425\">the scaling:</st>\n\n    ```", "```py\n\n\t\t\t<st c=\"35497\">Note</st>\n\t\t\t<st c=\"35502\">You need to set</st> `<st c=\"35519\">ord=1</st>` <st c=\"35524\">for the Manhattan distance and</st> `<st c=\"35556\">ord=2</st>` <st c=\"35561\">for the Euclidean distance as arguments of NumPy’s</st> `<st c=\"35613\">linalg()</st>`<st c=\"35621\">function, depending on whether you scaled the features to the</st> `<st c=\"35684\">l1</st>` <st c=\"35686\">or</st> `<st c=\"35690\">l2</st>` <st c=\"35692\">norm.</st>\n\t\t\t<st c=\"35698\">We see that the Manhattan distance of each feature vector is</st> `<st c=\"35760\">1</st>` <st c=\"35761\">after scaling:</st>\n\n```", "```py\n\n\t\t\t<st c=\"35812\">Based on the scikit-learn library’s documentation, this scaling method can be useful when using a quadratic form such as the dot-product or any other kernel to quantify</st> <st c=\"35982\">the similarity of a pair</st> <st c=\"36007\">of samples.</st>\n\t\t\t<st c=\"36018\">How it wo</st><st c=\"36028\">rks...</st>\n\t\t\t<st c=\"36035\">In this recipe, we</st> <st c=\"36054\">scaled the observations from the California housing dataset to their feature vector unit norm by utilizing the Manhattan or Euclidean distance.</st> <st c=\"36199\">To scale the feature vectors, we created an instance of</st> `<st c=\"36255\">Normalizer()</st>` <st c=\"36267\">from scikit-learn and set the norm to</st> `<st c=\"36306\">l1</st>` <st c=\"36308\">for the Manhattan distance.</st> <st c=\"36337\">For the Euclidean distance, we set the norm to</st> `<st c=\"36384\">l2</st>`<st c=\"36386\">. Then, we applied the</st> `<st c=\"36409\">fit()</st>` <st c=\"36414\">method, although there were no parameters to be learned, as this normalization procedure depends exclusively on the values of the features for each observation.</st> <st c=\"36576\">Finally, with the</st> `<st c=\"36594\">transform()</st>` <st c=\"36605\">method, the scaler divided each observation’s feat</st><st c=\"36656\">ure vector by its norm.</st> <st c=\"36681\">This returned</st> <st c=\"36695\">a NumPy array with the scaled dataset.</st> <st c=\"36734\">After the scaling, we used NumPy’s</st> `<st c=\"36769\">linalg.norm</st>` <st c=\"36780\">function to calculate the norm (</st>`<st c=\"36813\">l1</st>` <st c=\"36816\">and</st> `<st c=\"36821\">l2</st>`<st c=\"36823\">) of each vector to confirm that after the transformation, it</st> <st c=\"36886\">was</st> `<st c=\"36890\">1</st>`<st c=\"36891\">.</st>\n\n```"]