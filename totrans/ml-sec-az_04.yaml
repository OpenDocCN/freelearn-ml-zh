- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Protection and Governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is a fundamental part of machine learning, so in this chapter, we will
    focus on all aspects of governing, storing, and securing data. We will start by
    explaining what data governance is and how vital it is to know what data we have.
    We need to know how to develop a management framework so that we can improve everything,
    from organizational workflows to business decisions. Data governance will also
    help us identify sensitive data and how we can better secure it, as part of our
    data governance program or our machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by learning the best practices to store and retrieve data in Azure
    machine learning. We will see many data services that we can use to save our datasets
    – for example, the Azure Blob Storage and the Azure SQL database and their basic
    encryption and security features. Azure Machine Learning already provides us with
    a lot of features, such as versioning and logging, but since data is usually connected
    to the workspace and not saved in the workspace, it is important to familiarize
    ourselves with different security features from different services. Finally, we
    will explore backup options and recovery options because even though we will do
    our best to protect our assets, that does not mean we should not be prepared for
    the worst-case scenario, which is the need to recover our data from any data loss
    following human error or security incidents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with data governance in Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing and retrieving data in Azure Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypting and securing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring backup and recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a better idea of how to set up a data
    protection and governance framework for your organization, using the proper tools
    and best practices to secure your data in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Working with data governance in Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data governance refers to the overall management and control of an organization’s
    data assets. It involves establishing processes, policies, and guidelines to ensure
    availability, integrity, security, privacy, and the effective and efficient use
    of information. This is always important, but it is especially crucial when we’re
    talking about ML, as ML models are based on data. Whether we’re talking about
    data used to train our models or data generated by our models, it does not change
    the fact that we need to be aware of every piece of information we process and
    what its life cycle is.
  prefs: []
  type: TYPE_NORMAL
- en: To implement data governance effectively, organizations typically need to establish
    a data governance framework or strategy, which outlines the structure, processes,
    and responsibilities for data management. This framework should include the formation
    of a data governance committee or council, data governance policies and procedures,
    data stewardship roles, and the use of technology solutions to support data governance
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: Data governance involves not only technical aspects but also business aspects.
    There are different types of information exchanged throughout different departments
    of an organization that all need to be a part of the company’s data governance
    policy, which can be daunting at first. Remember that data governance is a collaborative
    effort involving people, processes, and technology. It requires ongoing commitment,
    communication, and engagement from all stakeholders to drive successful implementation,
    ensuring the long-term effectiveness of the data governance strategy. However,
    with multiple global and regional data regulations about security and privacy,
    not having a data governance strategy in your organization might result in financial
    loss and damage your reputation.
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of creating those policies and processes presents us with many
    challenges; however, developing an efficient strategy also yields many benefits.
    Let us start with the challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Challenges can be identified in each step as either technical, operational,
    or even human-oriented. Not all of them would apply to all organizations, but
    having an idea of the possible issues will help us prepare better for them. We
    will explore them together here:'
  prefs: []
  type: TYPE_NORMAL
- en: As we develop a new strategy, regardless of what it is, the most difficult part
    is often **company-wide acceptance**. The implementation of data governance usually
    requires a shift within an organization. Employees will need to change their behaviors,
    adopt new processes, and embrace a different mindset – a data-driven one, and
    that will disrupt their everyday routine. From the outset, if there is a lack
    of awareness and understanding, accepting a new strategy will be a struggle. Unfortunately,
    as data governance requires collaboration and coordination across different departments
    and business units, any lack of participation, misalignment of goals, and inconsistencies
    might hinder development and lead to fragmented data management practices that
    open up the organization to a lot of risks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another challenge is **engaging and aligning the right stakeholders**. We need
    to involve relevant stakeholders from different departments and levels of the
    organization to ensure consistency and collaboration. This includes executives,
    IT professionals, data owners, data users, and legal and compliance teams. Understanding
    their perspectives, needs, and challenges is directly related to data management
    and governance and will lead to an effective data governance framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, there is consistency or **standardization**. We really need to find the
    right balance between governance standards and flexibility. Creating overly complicated
    processes that increase data security and privacy, but hinder everyday business,
    is also not a welcome result. Data governance as a strategy aims to protect an
    organization from potential data breaches and non-compliance. Some allowances
    or exceptions might need to be made in some cases to allow for smooth business
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have managed to more or less overcome the aforementioned challenges, there
    is then the question of the actual data. **Alignment of responsibilities** is
    something we really need to focus on even more if we are involved in ML development.
    Deciding who should and who should not have access to particular segments of data
    is crucial, but also only the first step. This can also be further segmented into
    what data should and should not be used to train an ML model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, enabling the right **data governance tools and technologies** is also
    vital. We need to identify and implement appropriate tools and technologies to
    support data governance activities. This may include data cataloging tools, metadata
    management systems, data quality tools, and data security solutions. These tools
    help automate and streamline data governance processes, provide visibility into
    data assets, and facilitate data discovery and collaboration. In this and the
    next chapter, we will see several practices, tools, and product features that
    will help us do exactly that, specifically for ML.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any of the preceding challenges are not addressed, it will lead to poor data
    management. That means unsecured and siloed information and incomplete and inconsistent
    processes. The result makes an organization vulnerable to data breaches and can
    even have adverse consequences in business operations, as decision-making could
    be impacted by a lack of information.
  prefs: []
  type: TYPE_NORMAL
- en: If we succeed in overcoming those challenges, we can reap multiple benefits.
    Let us review what they are in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data governance can not only protect an organization but also improve business
    operations. According to Microsoft, a robust data governance strategy helps ensure
    that your information is audited, assessed, verified, managed, properly secured,
    and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: The first and most obvious benefit is **improved data quality**. Data governance
    ensures that data is accurate, consistent, and reliable. By establishing data
    standards, implementing data quality controls, and assigning responsibilities,
    you can enhance the overall quality of your data. High-quality data enables more
    benefits, such as better decision-making, enhances operational efficiency, and
    improves business processes.
  prefs: []
  type: TYPE_NORMAL
- en: Data governance improves **decision-making**. It provides a solid foundation,
    and with reliable and trustworthy data available, decision-makers can make informed
    choices based on accurate insights, based in turn on current data. Data governance
    helps ensure that decision-makers have access to the right data, at the right
    time, in the right format, and with the appropriate context. This can also lead
    to reduced costs and **improved profitability**.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as data governance focuses on protecting sensitive data and ensuring
    compliance with data privacy regulations, we also need **increased data compliance,
    security, and privacy**. By implementing data security measures, establishing
    access controls, and defining data handling procedures, we can greatly mitigate
    the risk of data breaches and unauthorized access. In Azure Machine Learning with
    the proper controls, you can also create compliance reports with data laws and
    regulations.
  prefs: []
  type: TYPE_NORMAL
- en: By creating processes and best data practices, you end up with **improved data
    management**. Any need, issues, or problems that arise can be found and mitigated
    more effectively. Data sharing and collaboration are done more efficiently and
    securely across different teams and departments. Data management can promote data
    integration and ensure data interoperability across systems and applications.
    By defining data standards, data formats, and data integration guidelines, organizations
    can achieve better data consistency and enhance data interoperability.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, data management builds trust. Trust by your stakeholders and trust
    by your customers leads to a **stellar organizational reputation**. When it comes
    to trusting someone with data, an organization’s reputation can be the deciding
    factor in their choice, as machine learning is trained and learns from old and
    new data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how important data governance is, let us get started
    with learning best practices and exploring some tools and resources that can be
    applied with Azure Machine Learning and its related services.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started using cloud data best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure Machine Learning is a cloud data service, and as you will see in the following
    chapters, most of the services that work with Azure Machine Learning are also
    cloud services or are hosted in Azure. As a result, we will focus on best practices
    that relate to cloud data management, and what better way to get started than
    to follow the **Cloud Data Management Capabilities** (**CDMC**) framework developed
    by a global industry council – the EDM Council.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CDMC framework outlines the following 14 key controls to manage data risk:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data control compliance** means that everything that contains sensitive data
    must be monitored, and notifications should be implemented should any issues arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data ownership** must be clear on any piece of saved information, with the
    ability to be reported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authoritative data sources and provisioning points** should be available
    for all sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sovereignty and cross-border movement** outline that sensitive data
    must be controlled and audited based on policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cataloging** should be automated consistently across departments or
    environments and, ideally, when data is created or ingested.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data classification** should be completed and automated if possible for all
    sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data entitlements and access for sensitive data** should be managed by the
    owner and tracked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consumption purposes** should be provided for all data, including any
    agreements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security controls** must be implemented to enforce data compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data protection impact assessments** should be in place and automated when
    an issue arises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data retention** is the data that is to be retained considering the data
    life cycle and eventually this data is deleted. This should be clearly defined
    and possibly automated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality** ensures that data is fit and classified properly, depending
    on its purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost metrics** on how data is stored and moved must be available for reporting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data lineage** ensures that data origins and any usage can be identified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CDMC framework is available as a free license to all industries for internal
    use, including both EDM Council members and non-members. It can also be used to
    implement data governance, as well as an assessment tool. All you need to do is
    to accept their terms and conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Data Management Capabilities (CDMC)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to explore more about the CDMC framework and the EDM council, visit
    the [official page: https://edmcouncil.org/f](https://edmcouncil.org/frameworks/cdmc/)rameworks/cdmc/.'
  prefs: []
  type: TYPE_NORMAL
- en: However, no matter which framework or process you choose, completing everything
    at once might not be possible. Instead, you need to start small to guarantee a
    successful implementation. It might be good to appoint someone or a team to execute
    and monitor all the implementation across your organization. Then, make sure you
    have clear goals and a clear business case. Developing a proper metric to measure
    progress and keeping communication open throughout this entire process is also
    a vital part. The key is to realize that data governance goes beyond implementing
    a few IT solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Also, while there is not a one-fits-all strategy that will work for every organization
    or industry, Azure provides multiple tools to implement controls and apply governance
    in your data. Let us explore some of them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Azure tools and resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already seen some tools to help us enforce compliance in our resources,
    such as Azure Policy and Azure Blueprints in [*Chapter 3*](B21076_03.xhtml#_idTextAnchor077).
    Let us review a couple more that apply to data governance.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Cost Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with data governance, one of the goals could be to optimize your
    cloud usage and spend. To monitor and get recommendations, you can use Microsoft
    Cost Management. This is free for Azure subscriptions, but it can be used to monitor
    other clouds. In the latter case, you might need to check what the management
    costs are.
  prefs: []
  type: TYPE_NORMAL
- en: Azure cost management includes reporting on cost and usage, multiple categorization
    options, creating usage budgets, and the ability to set alerts on forecasted costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see some examples of how to use the service:'
  prefs: []
  type: TYPE_NORMAL
- en: In the next figure, you can see the **Cost analysis** blade of **Cost Management**.
    Some features are still in preview at the time of writing, but it gives us a good
    idea of what the subscriptions and each resource cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.1 – A Cost Management overview](img/B21076_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A Cost Management overview
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can create reports by resource, service, or even time period. These
    reports are customizable and provide a graphic representation of the actual and
    forecasted cost of each billing period, as shown in the following figure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Cost analysis](img/B21076_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Cost analysis
  prefs: []
  type: TYPE_NORMAL
- en: You can create alerts to detect anomalies or based on recommendations very easily
    in the **Cost alerts** menu. Once you click **Add** and fill in the fields, you
    have your alert ready. You will receive email alerts every time something happens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Create alert rule](img/B21076_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Create alert rule
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want an alert based on a budget, you will need to create it through
    the **Budgets** menu. Once you set a budget, you can create alerts based on usage
    or forecasted usage so that you are always up to date, ensuring that you will
    never go above your budget. The process has two steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, you create a **budget amount**. You get a graph with information about
    your past usage to help you decide what the approximate budget should be if you
    do not already have a predefined amount.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Create budget](img/B21076_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Create budget
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to set **Alert conditions** and **Alert recipients**. The
    alerts are either on the actual amount or the forecasted amount, which helps you
    get notified before you go over a certain threshold and plan accordingly. This
    is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Create budget alerts](img/B21076_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Create budget alerts
  prefs: []
  type: TYPE_NORMAL
- en: You can also work with the Azure Advisor recommendations to find usage and cost
    best practices for the services you use for your subscription. Generally, any
    information that can optimize cloud usage should be part of your organization’s
    governance.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Advisor cos[t recommendations](https://learn.microsoft.com/en-us/azure/advisor/advisor-reference-cost-recommendations)
  prefs: []
  type: TYPE_NORMAL
- en: '[At https://learn.microsoft.com/en-us/azure/advisor/advisor-reference](https://learn.microsoft.com/en-us/azure/advisor/advisor-reference-cost-recommendations)-cost-recommendations,
    you can find some cost recommendations for different services. However, they will
    not be shown in the **Advisor recommendation** blade if you have not deployed
    these services.'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Purview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Microsoft Purview** is a data governance, risk, and compliance solution designed
    to help organizations manage and govern their data assets across various sources
    and platforms. Its features apply both to Azure and **Microsoft 365**, and you
    can use it to protect sensitive data between different clouds, apps, and devices
    and identify and manage regulatory compliance.'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Purview offers several key features to support data governance. The
    basis of this support is the **Data Map**. Purview provides a data map feature
    that helps organizations visualize and understand the relationships and characteristics
    of their data assets. The data map in Purview represents a graphical representation
    of the data landscape within an organization, including data sources, connections,
    and metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of the Data Map, you will find many apps for different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Data Catalog** app finds data sources by searching your data and helping
    classify them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Data Estate Insights** app helps you find out what kinds of data you have
    and where it is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Data Sharing** app helps you secure your data between partners and customers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Data Policy** assists with access provisioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Purview integrates with various Azure services, such as Azure Data
    Factory and Azure SQL Database, to provide end-to-end data governance capabilities.
    It leverages Azure’s security and compliance features to ensure data protection
    and regulatory compliance, and any sensitive data discovered is shared with **Microsoft
    Defender for Cloud**. By using Purview, you can establish a comprehensive data
    governance framework to improve data visibility, control, and compliance. It helps
    to address challenges related to data sprawl, data privacy, and regulatory requirements
    by providing a centralized platform to manage and govern data assets effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Data governance is very important, so after we have identified where sensitive
    data is, we will see how to work with it securely with Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with the storage and retrieval of data.
  prefs: []
  type: TYPE_NORMAL
- en: Storing and retrieving data in Azure Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first task is storing and retrieving data in Azure Machine Learning. You
    can bring data into Machine Learning in a multitude of ways. That includes anything
    from your local machine, a source on the internet, or even cloud-based storage.
    In this section, we will explore all those concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see how to work with datastores.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting datastores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned in the Azure Machine Learning introduction in [*Chapter 1*](B21076_01.xhtml#_idTextAnchor015),
    **datastores** serve as a *reference* to an existing storage service, whether
    that is a storage account or a database. If you already have a reference or a
    connection to your data, this is not mandatory, as you can connect external sources
    as well, but connecting datastores has many benefits. Firstly, you have a common
    way to connect different data sources to your workspace without the need to add
    credential information anywhere in your scripts or your code, which is a best
    practice in terms of security. It is easier when you are working with a team to
    have the same reference available for your data sources that everyone can reuse.
    And, of course, this offers a common straightforward way to use APIs that can
    work with different storage types.
  prefs: []
  type: TYPE_NORMAL
- en: When you create a datastore, you can use different authentication methods. It
    can be either credential-based or identity-based. Credential-based means using
    a service principal or a **shared access signature** (**SAS**) token for authentication,
    while with identity-based authentication, you will use your Microsoft Entra ID
    identity or a managed identity. In any case, the connection and authentication
    information is stored in the workspace. We are not going to focus much on this
    now, as we will analyze all the different types of authentication options that
    you can use together with machine learning in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Every workspace has four built-in datastores, which are used by Azure Machine
    Learning as the system storage. There are two Azure Blob Storage datastores and
    two Azure Files datastores under the **Datastores** tab in the **Data** menu.
    You can see what the default is or set another one as the default datastore from
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list will look something similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Built-in datastores](img/B21076_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Built-in datastores
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to create a new datastore, the currently available options are
    the ones shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The Create datastore options](img/B21076_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The Create datastore options
  prefs: []
  type: TYPE_NORMAL
- en: You can create datastores from the Azure portal, the **command-line interface**
    (**CLI**), or the Python **software development kit** (**SDK**). Currently, it
    looks like you can only add datastores from a subscription you have access to.
    You can add or remove datastores at any time. Remember that they are only a connection
    to the service that has the data. So, creating or removing a datastore from the
    workspace does not actually delete or create any services and is not mandatory.
    Also, we still need to make sure all of the connected services are secured using
    their own features and best practices. What we will use in our experiments are
    data assets.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see how we can create data assets in the workspace, either from datastores
    we have added to our workspace or from external services.
  prefs: []
  type: TYPE_NORMAL
- en: Adding data assets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Azure Machine Learning, data assets refer to the several types of data that
    can be used and managed within the machine learning workflows. If datastores are
    a connection to the data source, then data assets are a reference to the actual
    data. You can create a data asset from datastores, local files, public URLs, or
    Azure Open Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Using data assets together with datastores, if possible, provides us with several
    security and data governance benefits. You can collaborate using the same data
    asset with your team and access it through your training code seamlessly, without
    looking for different credentials or paths. Data assets support versioning, so
    any updates or data processing can be saved as a new version and reused. Pipelines
    or jobs that use the data are recorded in the workspace, so any changes or updates
    can be audited.
  prefs: []
  type: TYPE_NORMAL
- en: Once you create a data asset version, it is **immutable**, which means it cannot
    be deleted or edited. So, if a new version of the data asset does not provide
    the expected results, you can always revert to a previous one to work with. You
    can use different asset types from older Azure Machine Learning APIs, but it is
    recommended to work with the latest version, which is the Azure ML v2 APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In v2, there are the following three main types of data assets you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**URI file**: The URI file points to a specific file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URI folder**: The URI folder points to a folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLTable**: An MLTable points to a folder or file and includes a schema to
    read as tabular data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice depends on the available data you have and the type of ML project
    and algorithm you choose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the data type, you have three available sources, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Creating a datastore options](img/B21076_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Creating a datastore options
  prefs: []
  type: TYPE_NORMAL
- en: If you are choosing **From a URI** as your source, ensure the endpoint is public
    and not protected by any credentials. If you want to choose an existing datastore,
    you can choose the **From Azure Storage** option. Then, if the datastore you want
    is not pre-created, you can create it right through the wizard and find the file
    path in the list. You can also upload your own files to the datastore of your
    choice by choosing the **From local** **files** option.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice
  prefs: []
  type: TYPE_NORMAL
- en: Use datastores in combination with data assets where supported. Datastores provide
    security, as credentials are stored in the workspace and not in your code. With
    data assets, you gain capabilities such as versioning, reproducibility, auditability,
    and lineage, which are all part of data governance best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to connect data to the workspace, let us ensure that it
    is as secure as possible, both when it is not used and in transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Encrypting and securing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous section, Azure Machine Learning relies on external
    services to pull in data as data assets. Depending on the service that hosts the
    data, there are different security and data protection features we can use, such
    as encryption, data classification, and data masking.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore encryption and classification features that
    relate to our data.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption at rest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encryption at rest refers to the practice of encrypting data while it is stored
    or *at rest* in a storage medium, such as cloud storage. The purpose of encryption
    at rest is to protect data from unauthorized access if the storage medium is compromised,
    lost, or stolen.
  prefs: []
  type: TYPE_NORMAL
- en: When data is encrypted at rest, it is transformed into an unreadable form using
    an encryption algorithm and a cryptographic key. Only authorized users or processes
    with the proper decryption key can access and decrypt the data to its original
    readable form. Without the decryption key, the encrypted data remains unintelligible
    and useless to unauthorized individuals.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of Azure services, Azure provides built-in encryption at rest
    for many of its storage services, such as Azure Blob Storage, Azure Data Lake
    Storage, and other services such as the relational databases used by Azure Machine
    Learning. These services automatically encrypt data at rest using industry-standard
    encryption algorithms. Azure manages the encryption keys and ensures the security
    of the encrypted data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing encryption at rest not only provides security but also forms a
    part of organizational compliance with data protection regulations, strengthens
    data security, and mitigates the risk of data breaches or unauthorized access
    to sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore those features in data services that work with Azure Machine
    Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we create an Azure Machine Learning workspace, you also create an Azure
    storage account. The service stores snapshots, outputs, and logs in a default
    storage account that is tied to the Azure Machine Learning workspace and your
    subscription, and it is used as the workspace filesystem. It also serves to host
    the training data that is used by training compute targets. It is mounted to those
    compute targets as a remote filesystem during the training process. Finally, if
    you choose to upload files, they are stored in your chosen datastore, which is
    usually an Azure Blob storage. So, knowing how to secure your Azure Blob storage
    account is necessary. The same applies to Azure Data Lake Storage, as it is essentially
    an upgraded Blob Storage account.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Azure storage account is encrypted at rest using Microsoft-managed keys.
    In this case, Microsoft online services automatically generate and securely store
    the encryption keys used. If you want to control and use your own encryption key,
    open the **Encryption** menu in the **Storage account** blade and switch to the
    encryption options with **Customer-managed keys**, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Creating datastore options](img/B21076_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Creating datastore options
  prefs: []
  type: TYPE_NORMAL
- en: Enter the key URI where your key is hosted and click **Save**.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice – customer-managed keys and Azure Key Vault
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the Azure Key Vault service to generate and store encryption keys. For
    more information about the service, follow this link: [https://learn.microsoft.com/en-us/azure/key-vault/general/overview](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data store connected to the Azure Blob storage uses an account access key
    for access. Each storage account has two access keys. Find yours on the **Access
    keys** menu of your **Storage** **account** blade:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Storage account access keys](img/B21076_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Storage account access keys
  prefs: []
  type: TYPE_NORMAL
- en: You can use either key for authentication in Azure Machine Learning. These keys
    must be rotated at frequent intervals for security. This is how the process might
    look like when rotating access keys. This process applies to any services that
    authenticate using **access keys**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – The key rotation process](img/B21076_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The key rotation process
  prefs: []
  type: TYPE_NORMAL
- en: You can also use an SAS token to authenticate to the storage account; however,
    be careful, as SAS tokens expire, and after the expiration date, you will not
    be able to have access to that storage account to work with Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an account-level SAS token, go to the **Shared access signature**
    menu in the **Storage account** blade and generate a token, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Generating an account SAS](img/B21076_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Generating an account SAS
  prefs: []
  type: TYPE_NORMAL
- en: Be mindful to include proper permissions and service visibility. Once you are
    happy with your choices, you can go back to the Azure Machine Learning Studio,
    open the datastore, and by clicking the **Update authentication** button, you
    can update the datastore with the new credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Updating the authentication credentials in the workspace](img/B21076_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Updating the authentication credentials in the workspace
  prefs: []
  type: TYPE_NORMAL
- en: However, the storage account is not our only datastore. Let us discuss more
    options and their features, starting with Azure databases.
  prefs: []
  type: TYPE_NORMAL
- en: Azure databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a lot of database options in Azure. Most of those services offer similar
    security options that can be leveraged to secure data adhering to encryption and
    other data best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Most services for relational databases use encryption similarly to an Azure
    storage account – for example, Azure Database for PostgreSQL and Azure Cosmos
    DB, the no-SQL multi-modal database. These services have encryption enabled by
    default, using Microsoft-managed keys, and you can always choose to use your own
    key by enabling the **Customer-managed keys** option and adding the key to the
    Azure Blob Storage account. Azure Database for MySQL also offers encryption, using
    the FIPS 140-2 validated cryptographic module for storage encryption of data at
    rest. Depending on the service, you might find more security features, such as
    the ones we have in the Azure SQL Database.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a closer look at the Azure SQL database security features.
  prefs: []
  type: TYPE_NORMAL
- en: Azure SQL Database
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Azure SQL Database offers several encryption features to protect data at rest
    and during transit. When it comes to encryption, Azure SQL Database offers multiple
    options.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transparent data encryption** (**TDE**): Transparent data encryption is a
    built-in feature in Azure SQL Database that automatically encrypts data at rest.
    TDE encrypts database files (e.g., data, backups, and log files) using a symmetric
    **database encryption key** (**DEK**). TDE helps protect data from unauthorized
    access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practice – TDE in Azure SQL Database
  prefs: []
  type: TYPE_NORMAL
- en: With the TDE and **Bring Your Own Key** (**BYOK**) option, you can take control
    of the TDE by using the Azure Key Vault service and the customer-managed keys
    for encryption.
  prefs: []
  type: TYPE_NORMAL
- en: '**Always Encrypted**: Always Encrypted is a feature that allows you to encrypt
    sensitive data such as credit card information within Azure SQL Database, both
    at rest and during transit. With Always Encrypted, the data remains encrypted
    within the database, and only authorized applications and users with the necessary
    encryption keys can access and decrypt the data. The encryption and decryption
    of data occur on the client side, ensuring that the sensitive data is never exposed
    in plain text to the database engine or administrators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azure SQL Database also provides data governance and protection features for
    data masking and classification to help protect sensitive data and manage data
    privacy. Let us review these features in Azure SQL Database:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Classification**: Data Classification is a feature in Azure SQL Database
    that helps you identify and label sensitive data within your database. It enables
    you to classify columns containing sensitive information, based on predefined
    or custom-defined classification labels. Azure SQL Database provides built-in
    classification labels for common types of sensitive data, such as credit card
    numbers, social security numbers, or email addresses. You can then use the Data
    Masking feature to protect this information from unauthorized access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Masking**: Data Masking is a feature that allows you to obfuscate sensitive
    data within Azure SQL Database. It helps protect sensitive information from unauthorized
    access by replacing the original values with masked values in query results. The
    masking rules include partial masking (for example, showing only the last four
    digits of a credit card number) or full masking (for example, replacing the original
    value with a constant value or random characters or stars). Data Masking operates
    at the database level and can be configured for specific columns containing sensitive
    data. This allows you to control the level of obfuscation applied to different
    types of sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Best practice: Azure SQL Database data classification and Azure Purview'
  prefs: []
  type: TYPE_NORMAL
- en: You can use the Data Classification feature in Azure SQL Database together with
    Azure Purview to discover, classify, and manage data across various data sources
    in your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Data integration Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Azure Machine Learning works with many data integration services, such as Azure
    Data Factory, Azure Databricks, and Azure Synapse Analytics. Azure Data Factory
    allows you to create data-driven workflows for data integration and data transformation
    at scale. It is a serverless and code-free platform for **extract-transform-load**
    (**ETL**), **extract-load-transform** (**ELT**), and data integration. Its pipelines
    can be used for data ingestion, so you can use them with Azure Machine Learning.
    Azure Databricks is a data analytics platform that runs data science workloads
    based on Apache Spark. Models created in Databricks can then be deployed in Azure
    Machine Learning. Both of these services offer built-in encryption with Microsoft-managed
    or customer-managed keys.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Synapse Analytics is a limitless analytics service that brings together
    big data and data warehousing. We can use it to ingest, prepare, manage, and serve
    data for business intelligence and support ways of implementing machine learning
    and integrating with Azure Machine Learning. Depending on the type of pool you
    use, there are different encryption options. For dedicated SQL pools, you can
    use TDE, whereas for the Azure Synapse SQL serverless pool or the Apache Spark
    pool, since they are based on **Azure Data Lake Gen2** (**ALDS Gen2**) or Azure
    Blob Storage, you can leverage the storage account security options.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how multiple services handle encryption at rest, but data is meant to
    be communicated between systems, so let us talk about encryption in transit.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption in transit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encryption in transit refers to the practice of encrypting data as it is being
    transmitted over a network or communication channel from one location to another.
    It ensures that the data remains secure and protected from unauthorized access.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to Azure Machine Learning, encryption in transit is a vital aspect
    of securing data and communication between different components of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning enforces the use of secure communication protocols, such
    as **HTTPS** (short for **Hypertext Transfer Protocol Secure**) or **TLS** (short
    for **Transport Layer Security**), for data transmission. These protocols establish
    an encrypted connection between the client applications, Azure Machine Learning
    services, and other components, ensuring that data transmitted over a network
    is encrypted and cannot be easily intercepted or tampered with.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing encryption at rest and in transit, along with other security
    features in Azure Machine Learning and all its related services, we can protect
    sensitive data and ensure its privacy and integrity during transmission. Then,
    we can prevent unauthorized access or interference, enhancing the overall security
    posture of your machine learning projects. This is only step one, however. As
    with any security matter, we can hope for the best, but we should always be prepared
    for the worst.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore in the next section how we can deploy guardrails to ensure we
    can recover data after an incident.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring backup and recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backup and recovery are closely related concepts and are both needed so that
    we can safeguard our data. In this section, we will explain backup and recovery
    options for our workspace and the data connected to them. We will also talk about
    how to approach situations where backup options are not available. However, before
    we get started, let us remember what backup and recovery are.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backup** refers to the process of creating copies or replicas of data and
    storing them in a separate location or medium. The purpose of backups is to provide
    a means of recovering data if there is data loss, accidental deletion, system
    failures, disasters, or other unforeseen events. Backups serve as a safety net,
    allowing you to restore data to its previous state or a specific point in time.
    Backups can be performed at different levels, such as full backups (copying all
    data), incremental backups (copying only the changes since the last backup), or
    differential backups (copying the changes since the last full backup). Different
    backup strategies and schedules can be implemented based on an organization’s
    requirements and data protection needs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recovery**, on the other hand, refers to the process of restoring data from
    backups after a data loss event or any other incident that results in the unavailability
    or corruption of data. The recovery process involves accessing the backup copies
    and returning the data to its original state, or a consistent state, before the
    data loss occurred. Recovery can take different forms, depending on the nature
    of the data loss and the backup strategy in place. It may involve restoring individual
    files or directories, recovering an entire system or database, or even restoring
    data to a different location or environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore our backup and recovery options in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing backup options for your datastores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since Azure Storage serves as a filesystem for a workspace, it is the most important
    thing that we need to secure and backup. We need to remember that the workspace
    does not extend any protection to its datastores, so we need to learn the basic
    features of each one.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the features of each service, we can take another step to
    protect our resources from accidental deletion by using resource locks.
  prefs: []
  type: TYPE_NORMAL
- en: Azure resource locks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Azure resource locks** allow users to apply restrictions on Azure resources
    to prevent accidental deletion or modification, providing an additional layer
    of protection and governance for critical resources. There are two types of locks
    to protect against delete and read-only instances. The process is extremely simple.
    You create a lock on the resource, the resource group, or the subscription, and
    nobody can make any changes unless they have the appropriate permissions to remove
    the lock temporarily in order to make changes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a resource lock, find the **Locks** menu on your desired resource
    and click the **Add** button, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Adding a resource lock](img/B21076_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Adding a resource lock
  prefs: []
  type: TYPE_NORMAL
- en: Add a lock name, lock type, and some notes if you want. As soon as you hit **OK**,
    the resources cannot be deleted or changed, depending on the Lock type. We have
    read-only and delete locks available.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Resource locks are not exactly a backup solution, and they only protect
    the resources, not the data, so let us review our options.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are multiple levels to protect data in the storage account that we use
    for ML. There are the blob containers, the blobs themselves, and the file shares.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the following features apply to one or more levels of the storage account.
    Let us explore each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Immutability policies**: At the container level and the blob level, you can
    enable immutability policies that help you enforce a time period where your data
    is protected from modifications or deletions. Immutability policies are not only
    for data protection but also provide auditing and monitoring. They should be part
    of your governance process, as they can ensure compliance with legal and compliance
    requirements and data preservation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To enable immutability policies at the container level, open the chosen container
    blade in the storage account, and click on **Access Policy**. In the **Immutable
    Blob Storage** section, click **+ Add policy**. Choose **Time-based retention**
    in the **Policy type** dropdown, add the *days* where blobs cannot be modified,
    and click **Save**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15 – A new immutable storage policy](img/B21076_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – A new immutable storage policy
  prefs: []
  type: TYPE_NORMAL
- en: '**Soft delete**: At both the container and the blob level you can also enable
    a soft delete. Soft delete provides an additional layer of data protection by
    allowing you to recover deleted blobs or containers within a specified time frame,
    reducing the risk of accidental data loss. It is particularly useful in scenarios
    where data needs to be retained and recoverable, such as compliance requirements,
    accidental deletions, or data recovery needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To enable soft delete, go to the **Data Protection** section in the **Storage
    account** blade, and enable soft delete for blobs and containers by checking the
    corresponding box, as demonstrated in the following screenshot. Fill in the retention
    in days for each option, and click **Save**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Enabling soft delete](img/B21076_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Enabling soft delete
  prefs: []
  type: TYPE_NORMAL
- en: To restore a blob, go to the container and enable the **Show deleted** blobs
    button. Then, choose the deleted blob, and select **Undelete** from the three-dot
    menu on the right.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Restoring a blob](img/B21076_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Restoring a blob
  prefs: []
  type: TYPE_NORMAL
- en: '**Point-in-time restore**: Containers and blobs also support point-in-time
    restore, meaning you can restore a blob or container to a previous state before
    it was deleted. It is useful in scenarios where an application or a system makes
    multiple deletes or unintended changes, and it is difficult to find each individual
    file or container to restore. Backup is continuous in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To enable point-in-time restore, go to the **Data protection** section in the
    **Storage account** blade, and enable point-in-time restore for containers by
    checking the corresponding box, as demonstrated in the next screenshot. Fill in
    the maximum restore point in days, and click **Save**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Enabling a point-in-time restore](img/B21076_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Enabling a point-in-time restore
  prefs: []
  type: TYPE_NORMAL
- en: To restore a container to a previous point in time, select the container and
    click on the **Restore containers** button. Then, choose a previous point in time.
    This action cannot be undone, so ensure you choose wisely.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Restoring to a previous point in time](img/B21076_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Restoring to a previous point in time
  prefs: []
  type: TYPE_NORMAL
- en: '**Vaulted backup (operational and vaulted)**: Vaulted backup uses object replication
    to replicate blobs to an external backup vault. This process happens asynchronously
    and requires an external Azure Backup vault resource. To restore the objects,
    we can start a restore operation. Backups are either operational (continuous)
    or periodic by creating a custom policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To enable vaulted backups, go to the **Data protection** section in the **Storage
    account** blade, and click the **Enable Azure Backup for blobs** checkbox. If
    you already have a backup vault, you can choose it in the dropdown; otherwise,
    choose to create and fill in the **Vault name**, **Resource group**, and **Backup
    storage redundancy** options, as shown in the following screenshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Enabling Azure Backup](img/B21076_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Enabling Azure Backup
  prefs: []
  type: TYPE_NORMAL
- en: Then, choose the backup policy. If you have one, you can select it; otherwise,
    click **Create new**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Creating or choosing a backup vault](img/B21076_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Creating or choosing a backup vault
  prefs: []
  type: TYPE_NORMAL
- en: Provide the policy with a name, and ensure that the **Datasource type** option
    is **Azure Blobs**. Then, click **Next**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Creating a policy](img/B21076_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Creating a policy
  prefs: []
  type: TYPE_NORMAL
- en: Set up the schedule and retention for both the operational and vaulted backups.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Configuring an operational backup](img/B21076_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Configuring an operational backup
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we are using a weekly backup schedule, as shown in the following
    screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Configuring a vaulted backup](img/B21076_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Configuring a vaulted backup
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can filter the containers included in this backup if necessary,
    by clicking the **Select containers** link and then clicking **Save**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Choosing a policy and filter containers](img/B21076_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Choosing a policy and filter containers
  prefs: []
  type: TYPE_NORMAL
- en: To restore, start a **Restore** operation from the **Overview** blade of the
    backup vault.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Starting a restore operation](img/B21076_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Starting a restore operation
  prefs: []
  type: TYPE_NORMAL
- en: Soft delete and cost management
  prefs: []
  type: TYPE_NORMAL
- en: Soft deleted data continues to occupy storage space and is billed accordingly
    during the retention period. However, storage costs are usually lower compared
    to active data storage. After the retention period is over, the soft deleted data
    is automatically purged, and you will no longer have any storage costs for it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Object replication**: Object replication essentially copies data from a container
    or the whole storage account to another storage account in the same or a different
    region. That way, you can keep two copies of your data if something happens. It
    is a great way to back up your data without downloading it into another medium
    and reuploading it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To set it up, click on the **Object replication** section of the **Storage account**
    blade, and then click on the **Create replication rules** button. Before you proceed,
    you need to have already created a second storage account to replicate the data
    and the necessary destination containers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Adding object replication](img/B21076_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Adding object replication
  prefs: []
  type: TYPE_NORMAL
- en: On the **Create replication** rules screen, select the destination of the storage
    account, create a source-to-destination container mapping so that objects are
    replicated to their corresponding containers, and then click **Save**, as shown
    in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – Choose a target storage account and container mapping](img/B21076_04_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – Choose a target storage account and container mapping
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs: []
  type: TYPE_NORMAL
- en: Out of the aforementioned options, the best combination for Azure Machine Learning
    would be object replication in a storage account, located in a different region
    or vaulted backups. These options protect us from regional disasters. Additionally,
    use soft delete enabled with delete resource locks to prevent resource and unintended
    data deletions. Immutability rules might interfere with the Azure Machine Learning
    workspace functions, so I would not recommend it, with the exception of legal
    holds.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding options are fine; however, you can also use `AzCopy`, a cross-platform
    command-line tool, or data integration services such as the Azure Data Factory
    to accomplish the same goal. Keep in mind that, in this case, you are essentially
    duplicating your data in another storage account, which is accompanied by extra
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: AzCopy
  prefs: []
  type: TYPE_NORMAL
- en: '`AzCopy` is a command-line tool provided by Microsoft that allows you to transfer
    data to and from Azure storage services. It is commonly used for large-scale data
    migration scenarios, as well as to transfer data between different Azure storage
    accounts or Azure regions. The tool is available for Windows, Linux, and macOS,
    and it can be executed from the command line or i[ntegrated into scripts or automation
    workflows. To start using this tool, vis](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)it
    this link: https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using relational databases or any other data services, they too have
    their individual backup and recovery options, so make sure you review everything.
    Most services do have defaults enabled, so you won’t be completely unprotected,
    but others require more configuration than others to be secured.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see how to recover the most important part of your ML project, the Azure
    Machine Learning workspace itself.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering your workspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: External services might not be the only thing that you need to be able to recover.
    Perhaps the most important thing is the Azure Machine Learning studio workspace.
    The workspace serves as a central point where you manage everything, from your
    data assets to your running jobs and publishing the models to be consumed by applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Azure Machine Learning workspace supports soft delete by default. This feature
    gives you the ability to recover your workspace data after accidental deletions.
    However, in this case, it does not work similarly to an Azure storage account.
    When you create and work in your workspace, there are a lot of components that
    connect to the workspace. Not everything can be soft-deleted; some of those items
    are permanently deleted, and there’s nothing you can do to recover them. Among
    the recoverable items are the run history, models, data, environments, components,
    notebooks, pipelines, data labeling, and datastores. Any queued or running jobs,
    role assignments, compute, inference endpoints, and any linked Databricks workspaces
    are hard-deleted. This means that these items cannot be recovered even if you
    try to recover the soft-deleted workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Soft delete has a default retention period of 14 days. After those 14 days,
    any remaining data is purged from the system, and the workspace can no longer
    be recovered. However, even between those 14 days, a full recovery isn’t guaranteed.
    You might be able to get a lot of your data and notebooks back, but you still
    might need to recreate any jobs or anything else that you had running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of recovering the workspace is simple. All you need to do is go
    to the Azure Machine Learning category, and from the top of the page, you will
    find the **Recently deleted** button to view workspaces that were soft-deleted
    and are within the retention period. From there, you can choose to either recover
    or permanently delete a workspace, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Recovering or deleting a workspace](img/B21076_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – Recovering or deleting a workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default behavior of soft-deleting the workspace
    and choose to permanently delete it when you first attempt to delete the service,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Adding a resource lock](img/B21076_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – Adding a resource lock
  prefs: []
  type: TYPE_NORMAL
- en: Best practice – Azure Machine Learning workspace recovery
  prefs: []
  type: TYPE_NORMAL
- en: Use resource locks to protect your resources from accidental deletions or modifications,
    instead of relying on soft delete for recovery. Even if the recovery is a success,
    you will still have lost a lot of data and configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Although data backup is very important, it is only one of the preventative steps
    we need to take to protect our assets. We also need to know how to recover data
    and its associated resources as part of our security baseline, a process that
    must be tested at specific intervals so that we are prepared if the need arises.
    Testing the recovery process will also help us measure how long it takes for a
    full recovery and what the impact would be on everyday operations. If any downtime,
    for example, is not acceptable, we might need to utilize other measures.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything is based on data, so having a clear map of what data you have, where
    it is stored, how sensitive it is, and how to protect it should be your number
    one priority if you work with ML. So, while working with models and algorithms
    might be the most exciting part of ML, having a data governance and protection
    plan will save you from data-related issues. The CDMC framework is a very comprehensive
    strategy that you can use especially with cloud data, but as always, it is not
    the only option. Building your own data strategy policy is ultimately your decision,
    and the result will always be beneficial, depending on the industry and location
    you belong to.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as you decide on a strategy, there are a lot of tools in Azure available
    for governance, such as Azure Policy, Azure Blueprints, Cost Management, and Microsoft
    Purview, each with its own benefits and limitations. As tools can come and go
    and data governance is not a one-off process, do not be afraid to start small
    and then add or improve over time as different needs arise.
  prefs: []
  type: TYPE_NORMAL
- en: Also, as exciting planning and strategizing sounds, starting a new project is
    the time to take action. Encrypting and securing data in Azure Machine Learning
    is a process affecting multiple services, and we need to leverage the benefits
    and features of each service to have a completely safe and performant system.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we only glimpsed at several connected data services and their
    security features. Backup and recovery options work in the same manner, and they
    need to be combined with the security and backup options of the Azure Machine
    Learning workspace. But it does not end here. We still have to cover security
    best practices in our network, infrastructure, DevOps, and so on. Before we can
    do that, let us go and see how we can extend our data protection to work with
    data privacy and responsible AI best practices in the next chapter.
  prefs: []
  type: TYPE_NORMAL
