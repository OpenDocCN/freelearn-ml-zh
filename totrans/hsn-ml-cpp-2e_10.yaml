- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Neural Networks for Image Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于图像分类的神经网络
- en: In recent years, we have seen huge interest in **neural networks**, which are
    successfully used in various areas, such as business, medicine, technology, geology,
    and physics. Neural networks have come into play wherever it is necessary to solve
    problems of forecasting, classification, or control. Neural networks are intuitive
    as they are based on a simplified biological model of the human nervous system.
    They arose from research in the field of artificial intelligence, namely, from
    attempts to reproduce the ability of biological nervous systems to learn and correct
    mistakes by modeling the low-level structure of the brain. Neural networks are
    compelling modeling methods that allow us to reproduce extremely complex dependencies
    because they are non-linear. Neural networks also cope better with the *curse
    of dimensionality* than other methods that don’t allow modeling dependencies for
    a large number of variables.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们对**神经网络**产生了巨大的兴趣，它们在各种领域得到成功应用，如商业、医学、技术、地质和物理。神经网络在需要解决预测、分类或控制问题时发挥了作用。神经网络直观，因为它们基于人类神经系统的简化生物模型。它们源于人工智能领域的研究，即尝试通过模拟大脑的低级结构来复制生物神经系统的学习和纠错能力。神经网络是非线性建模方法，使我们能够复制极其复杂的依赖关系，因为它们是非线性的。神经网络也比其他不允许对大量变量建模依赖关系的方法更好地处理*维度诅咒*。
- en: In this chapter, we’ll look at the basic concepts of artificial neural networks
    and show you how to implement neural networks with different C++ libraries. We’ll
    also go through the implementation of the multilayer perceptron and simple convolutional
    networks and find out what deep learning is and what its applications are.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨人工神经网络的基本概念，并展示如何使用不同的 C++ 库实现神经网络。我们还将介绍多层感知器和简单的卷积网络的实现，并了解深度学习及其应用。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络概述
- en: Delving into convolutional networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入卷积网络
- en: What is deep learning?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是什么？
- en: Examples of using C++ libraries to create neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 C++ 库创建神经网络的示例
- en: Understanding image classification using the LeNet architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LeNet 架构理解图像分类
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You will need the following to complete this chapter:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成本章内容你需要以下工具：
- en: The `Dlib` library
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: The `mlpack` library
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlpack` 库'
- en: The `Flashlight` library
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flashlight` 库'
- en: The PyTorch library
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PyTorch` 库'
- en: Modern C++ compiler with C++20 support
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++20 的现代 C++ 编译器
- en: CMake build system version >= 3.22
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统版本 >= 3.22
- en: 'The code files for this chapter can be found in the following GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10)。
- en: An overview of neural networks
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络概述
- en: In this section, we will discuss what artificial neural networks are and their
    building blocks. We will learn how artificial neurons work and how they relate
    to their biological analogs. We will also discuss how to train neural networks
    with the backpropagation method, as well as how to deal with the overfitting problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论人工神经网络及其构建块是什么。我们将学习人工神经元的工作原理以及它们与生物类似物的关系。我们还将讨论如何使用反向传播方法训练神经网络，以及如何处理过拟合问题。
- en: A neural network is a sequence of neurons interconnected by synapses. The structure
    of the neural network came into the world of programming directly from biology.
    Thanks to this structure, computers can analyze and even remember information.
    Neural networks are based on the human brain, which contains millions of neurons
    that transmit information in the form of electrical impulses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一系列通过突触相互连接的神经元。神经网络的结构直接来自生物学。得益于这种结构，计算机可以分析和甚至记住信息。神经网络基于人脑，人脑包含数百万个神经元，它们以电脉冲的形式传递信息。
- en: Artificial neural networks are inspired by biology because they are composed
    of elements with similar functionalities to those of biological neurons. These
    elements can be organized in a way that corresponds to the anatomy of the brain,
    and they demonstrate a large number of properties that are inherent to the brain.
    For example, they can learn from experience, generalize previous precedents to
    new cases, and identify significant features from input data that contains redundant
    information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络受到生物学的启发，因为它们由具有与生物神经元类似功能性的元素组成。这些元素可以组织成与大脑解剖相对应的方式，并且它们表现出许多大脑固有的特性。例如，它们可以从经验中学习，将先前的先例推广到新的案例，并从包含冗余信息的输入数据中识别出显著特征。
- en: Now, let’s understand the process of a single neuron.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解单个神经元的过程。
- en: Neurons
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元
- en: 'The **biological neuron** consists of a body and processes that connect it
    to the outside world. The processes along which a neuron receives excitation are
    called **dendrites**. The process through which a neuron transmits excitation
    is called an **axon**. Each neuron has only one axon. Dendrites and axons have
    a rather complex branching structure. The junction of the axon and a dendrite
    is called a **synapse**. The following figure shows the biological neuron scheme:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**生物神经元**由一个身体和连接它到外部世界的处理过程组成。神经元接收兴奋的处理过程称为**树突**。神经元传递兴奋的过程称为**轴突**。每个神经元只有一个轴突。树突和轴突具有相当复杂的分支结构。轴突和树突的连接点称为**突触**。以下图显示了生物神经元的结构：'
- en: '![Figure 10.1 – Biological neuron scheme](img/B19849_10_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 生物神经元结构](img/B19849_10_01.jpg)'
- en: Figure 10.1 – Biological neuron scheme
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 生物神经元结构
- en: The main functionality of a neuron is to transfer excitation from dendrites
    to an axon. However, signals that come from different dendrites can affect the
    signal in the axon. A neuron gives off a signal if the total excitation exceeds
    a certain limit value, which varies within certain limits. If the signal is not
    sent to the axon, the neuron does not respond to excitation. The intensity of
    the signal that the neuron receives (and therefore the activation possibility)
    strongly depends on synapse activity. A synapse is a contact for transmitting
    this information. Each synapse has a length, and special chemicals transmit a
    signal along it. This basic circuit has many simplifications and exceptions compared
    to a biological system, but most neural networks model themselves on these simple
    properties.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的主要功能是将来自树突的兴奋传递到轴突。然而，来自不同树突的信号可能会影响轴突中的信号。当总兴奋超过一定限值时，神经元会发出信号，这个限值在一定范围内变化。如果信号没有发送到轴突，神经元就不会对兴奋做出反应。神经元接收到的信号强度（因此是激活可能性）强烈依赖于突触活动。突触是传输这种信息的接触点。每个突触都有一个长度，并且特殊的化学物质沿着它传递信号。与生物系统相比，这个基本电路有很多简化和例外，但大多数神经网络都是基于这些简单特性来建模的。
- en: 'The artificial neuron receives a specific set of signals as input, each of
    which is the output of another neuron. Each input is multiplied by the corresponding
    weight, which is equivalent to its synaptic power. Then, all the products are
    summed up, and the result of this summation is used to determine the level of
    neuron activation. The following diagram shows a model that demonstrates this
    idea:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元接收一组特定的信号作为输入，每个输入都是另一个神经元的输出。每个输入乘以相应的权重，这相当于其突触功率。然后，将所有乘积相加，这个求和的结果用于确定神经元的激活水平。以下图示演示了这个概念：
- en: '![Figure 10.2 – Mathematical neuron scheme](img/B19849_10_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 数学神经元结构](img/B19849_10_02.jpg)'
- en: Figure 10.2 – Mathematical neuron scheme
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 数学神经元结构
- en: Here, a set of input signals, denoted by ![](img/B19849_Formula_0012.png), go
    to an artificial neuron. These input signals correspond to the signals that arrive
    at the synapses of a biological neuron. Each signal is multiplied by the corresponding
    weight, ![](img/B19849_Formula_0022.png), and passed to the summing block. Each
    weight corresponds to the strength of one biological synaptic connection. The
    summing block, which corresponds to the body of the biological neuron, algebraically
    combines the weighted inputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，一组输入信号，表示为![](img/B19849_Formula_0012.png)，进入一个人工神经元。这些输入信号对应于到达生物神经元突触的信号。每个信号乘以相应的权重，![](img/B19849_Formula_0022.png)，然后传递到求和块。每个权重对应于一个生物突触连接的强度。求和块对应于生物神经元的身体，代数上组合加权输入。
- en: The ![](img/B19849_Formula_0032.png) signal, which is called bias, displays
    the function of the limit value, known as the `sum` so that the values of `f (sum)`
    belong to a specific interval. That is, if we have a large input number, passing
    it through the activation function gets us output in the required range. There
    are many activation functions, and we’ll go through them later in this chapter.
    To learn more about neural networks, we’ll have a look at a few more of their
    components.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_0032.png)信号，称为偏置，显示了极限值函数的作用，即所谓的`和`，使得`f (和)`的值属于特定的区间。也就是说，如果我们有一个大的输入数，通过激活函数传递它，我们就能得到所需的输出。存在许多激活函数，我们将在本章后面讨论它们。要了解更多关于神经网络的信息，我们将查看它们的一些更多组件。'
- en: The perceptron and neural networks
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器和神经网络
- en: 'The first appearance of artificial neural networks can be traced to the article
    *A logical calculus of the ideas immanent in nervous activity*, which was published
    in 1943 where an early model of an artificial neuron was proposed. Later, American
    neurophysiologist Frank Rosenblatt invented the perceptron concept in 1957 as
    a mathematical model of the human brain’s information perception. Currently, terms
    such as **single-layer perceptron** (**SLP**), or just perceptron, and **multilayer
    perceptron** (**MLP**) are used. Usually, under the layers in the perceptron is
    a sequence of neurons, located at the same level and not connected. The following
    diagram shows this model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络首次出现可以追溯到1943年发表的文章《神经活动内在逻辑演算》，其中提出了人工神经元的早期模型。后来，美国神经生理学家弗兰克·罗森布拉特在1957年发明了感知器概念，作为人类大脑信息感知的数学模型。目前，使用诸如**单层感知器**（**SLP**）或简称为感知器，以及**多层感知器**（**MLP**）这样的术语。通常，在感知器的层下面是一系列位于同一水平且未连接的神经元。以下图表显示了此模型：
- en: '![Figure 10.3 – One layer of perceptrons](img/B19849_10_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 感知器的一层](img/B19849_10_03.jpg)'
- en: Figure 10.3 – One layer of perceptrons
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 感知器的一层
- en: 'Typically, we can distinguish between the following types of neural network
    layers:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以区分以下类型的神经网络层：
- en: '**Input**: This is just the source data or signals arriving as the input of
    the system (model). For example, these can be individual components of a specific
    vector from the training set, ![](img/B19849_Formula_0042.png).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：这仅仅是作为系统（模型）输入的数据或信号源。例如，这些可以是训练集中特定向量的单个分量，![](img/B19849_Formula_0042.png)。'
- en: '**Hidden**: This is a layer of neurons located between the input and output
    layers. There can be more than one hidden layer.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：这是位于输入层和输出层之间的一层神经元。可能存在多个隐藏层。'
- en: '**Output**: This is the last layer of neurons that aggregates the model’s work,
    and its outputs are used as the result of the model’s work.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：这是最后的一层神经元，它汇总了模型的工作，其输出被用作模型工作的结果。'
- en: 'The term **single-layer perceptron** is often understood to describe a model
    that consists of an input layer and an artificial neuron that aggregates the input
    data. This term is sometimes used in conjunction with the term **Rosenblatt’s
    perceptron**, but this is not entirely correct since Rosenblatt used a randomized
    procedure to set up connections between input data and neurons to transfer data
    to a different dimension, which made it possible to solve the problems that arose
    when classifying linearly non-separable data. In Rosenblatt’s work, a perceptron
    consists of *S* and *A* neuron types, and an *R* adder. *S* neurons are the input
    layers, *A* neurons are the hidden layers, and the *R* neuron generates the model’s
    result. The terminology’s ambiguity arose because the weights were used only for
    the *R* neuron, while constant weights were used between the *S* and *A* neuron
    types. However, note that connections between these types of neurons were established
    according to a particular randomized procedure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**单层感知器**这个术语通常被理解为描述一个由输入层和汇总输入数据的艺术神经元组成的模型。这个术语有时与**罗森布拉特感知器**这个术语一起使用，但这并不完全正确，因为罗森布拉特使用随机过程来设置输入数据和神经元之间的连接，以将数据传输到不同的维度，这使得解决分类线性不可分数据时出现的问题成为可能。在罗森布拉特的工作中，感知器由*S*和*A*神经元类型以及*R*加法器组成。*S*神经元是输入层，*A*神经元是隐藏层，*R*神经元生成模型的结果。术语的歧义产生是因为只使用了*R*神经元的权重，而在*S*和*A*神经元类型之间使用了常数权重。然而，请注意，这些类型神经元之间的连接是根据特定的随机过程建立的：'
- en: '![Figure 10.4 – Rosenblatt’s perceptron scheme](img/B19849_10_04.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 罗森布拉特感知器方案](img/B19849_10_04.jpg)'
- en: Figure 10.4 – Rosenblatt’s perceptron scheme
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 – 罗森布拉特感知器方案](img/B19849_10_04.jpg)'
- en: 'The term MLP refers to a model that consists of an input layer, a certain number
    of hidden layers, and an output layer. This can be seen in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 这个术语指的是一个由输入层、一定数量的隐藏层和输出层组成的模型。这可以在以下图中看到：
- en: '![Figure 10.5 – MLP](img/B19849_10_05.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – MLP](img/B19849_10_05.jpg)'
- en: Figure 10.5 – MLP
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 多层感知器（MLP）
- en: It should be noted that the architecture of a perceptron (or neural network)
    includes the direction of signal propagation. In the preceding examples, all communications
    are directed from the input neurons to the output ones—this is called a feedforward
    network. Other network architectures may also include feedback between neurons.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，感知器（或神经网络）的架构包括信号传播的方向。在先前的例子中，所有通信都是从输入神经元指向输出神经元——这被称为前馈网络。其他网络架构也可能包括神经元之间的反馈。
- en: The second point that we need to pay attention to in the architecture of a perceptron
    is the number of connections between neurons. In the preceding diagram, we can
    see that each neuron in one layer connects to all the neurons in the next layer—this
    is called a **fully connected layer**. This is not a requirement, but we can see
    an example of a layer with different types of connections in the *Rosenblatt’s
    perceptron* scheme in *Figure 10**.3*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器的架构中，我们需要注意的第二点是神经元之间的连接数量。在先前的图中，我们可以看到同一层的每个神经元都与下一层的所有神经元相连——这被称为 **全连接层**。这并不是一个要求，但我们可以在
    *图 10.3* 中的 *罗森布拉特感知器* 方案中看到一个具有不同类型连接的层的例子。
- en: Now, let’s learn about one of the ways in which artificial neural networks can
    be trained.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解人工神经网络可以训练的一种方法。
- en: Training with the backpropagation method
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反向传播法进行训练
- en: 'Let’s consider the most common method that’s used to train a feedforward neural
    network: the **error backpropagation method**. It is related to supervised methods.
    Therefore, it requires target values in the training examples.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑用于训练前馈神经网络的常见方法：**误差反向传播法**。它与监督方法相关，因此需要训练示例中的目标值。
- en: The algorithm uses the output error of a neural network. At each iteration of
    the algorithm, there are two network passes—forward and backward. On a forward
    pass, an input vector is propagated from the network inputs to its outputs and
    forms a specific output vector corresponding to the current (actual) state of
    the weights. Then, the neural network error is calculated. On the backward pass,
    this error propagates from the network output to its inputs, and the neuron weights
    are corrected.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用神经网络的输出误差。在算法的每次迭代中，有两个网络传递——正向和反向。在正向传递中，一个输入向量从网络输入传播到其输出，形成一个与当前（实际）权重状态相对应的特定输出向量。然后，计算神经网络误差。在反向传递中，这个错误从网络输出传播到其输入，并纠正神经元权重。
- en: 'The function that’s used to calculate the network error is called the **loss
    function**. An example of such a function is the square of the difference between
    the actual and target values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算网络误差的函数称为 **损失函数**。此类函数的一个例子是实际值和目标值之间差异的平方：
- en: '![](img/B19849_Formula_005.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![公式 005](img/B19849_Formula_005.jpg)'
- en: 'Here, *k* is the number of output neurons in the network, *y’* is the target
    value, and *y* is the actual output value. The algorithm is iterative and uses
    the principle of *step-by-step* training; the weights of the neurons of the network
    are adjusted after one training example is submitted as input. On the backward
    pass, this error propagates from the network output to its inputs, and the following
    rule corrects the neuron’s weights:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*k* 是网络中输出神经元的数量，*y’* 是目标值，而 *y* 是实际输出值。该算法是迭代的，并使用 *逐步* 训练的原则；在提交一个训练示例作为输入后，网络的神经元权重会进行调整。在反向传播过程中，这个错误会从网络输出传播到其输入，以下规则会纠正神经元的权重：
- en: '![](img/B19849_Formula_0061.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![公式 0061](img/B19849_Formula_0061.jpg)'
- en: Here, ![](img/B19849_Formula_0071.png) is the weight of the ![](img/B19849_Formula_0082.png)
    connection of the ![](img/B19849_Formula_0092.png) neuron, and ![](img/B19849_Formula_0101.png)
    is the learning rate parameter, which allows us to control the value of the correction
    step, ![](img/B19849_Formula_0112.png) . To accurately adjust to a minimum of
    errors, this is selected experimentally in the learning process (it varies in
    the range from 0 to 1).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![公式图片](img/B19849_Formula_0071.png)是第![公式图片](img/B19849_Formula_0082.png)个神经元与第![公式图片](img/B19849_Formula_0092.png)个神经元的连接权重，![公式图片](img/B19849_Formula_0101.png)是学习率参数，它允许我们控制校正步骤![公式图片](img/B19849_Formula_0112.png)的值。为了准确调整到误差的最小值，这个值在学习过程中是实验选择的（它在0到1的范围内变化）。
- en: 'Choosing an appropriate learning rate can significantly impact the performance
    and convergence of machine learning models. If the learning rate is too small,
    the model may converge slowly or not converge at all. On the other hand, if the
    learning rate is too large, the model can overshoot the optimal solution and diverge,
    resulting in poor accuracy and overfitting. To avoid such issues, it is important
    to carefully choose the learning rate based on the specific problem and dataset.
    Adaptive learning rates (such as Adam) can help automatically adjust the learning
    rate during training, making it easier to achieve good results. ![](img/B19849_Formula_0122.png)
    is the number of the hierarchy of the algorithm (that is, the step number). Let’s
    say that the output sum of the *i*th neuron is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的学习率可以显著影响机器学习模型的表现和收敛速度。如果学习率太小，模型可能收敛缓慢或根本不收敛。另一方面，如果学习率太大，模型可能会超过最优解并发散，导致精度差和过拟合。为了避免这些问题，根据具体问题和数据集仔细选择学习率非常重要。自适应学习率（如Adam）可以帮助在训练过程中自动调整学习率，从而更容易获得好的结果。![公式图片](img/B19849_Formula_0122.png)是算法的层次结构编号（即步骤编号）。比如说，第*i*个神经元的输出总和如下：
- en: '![](img/B19849_Formula_013.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_013.jpg)'
- en: 'From this, we can show the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们可以展示以下内容：
- en: '![](img/B19849_Formula_014.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_014.jpg)'
- en: Here, we can see that the differential, ![](img/B19849_Formula_0151.png), of
    the activation function of the neurons of the network, *f (s)*, must exist and
    not be equal to zero at any point; that is, the activation function must be differentiable
    on the entire numerical axis. Therefore, to apply the backpropagation method,
    sigmoidal activation functions, such as logistic or hyperbolic tangents, are often
    used.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到，网络中神经元的激活函数*f(s)*的微分![公式图片](img/B19849_Formula_0151.png)必须存在且在任何点上都不等于零；也就是说，激活函数必须在整个数值轴上可微分。因此，为了应用反向传播方法，通常使用sigmoid激活函数，如逻辑函数或双曲正切函数。
- en: In practice, training is continued not until the network is precisely tuned
    to the minimum of the error function, but until a sufficiently accurate approximation
    is achieved. This process allows us to reduce the number of learning iterations
    and prevent the network from overfitting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练不是一直持续到网络精确调整到误差函数的最小值，而是直到达到足够准确的近似值。这个过程使我们能够减少学习迭代次数，并防止网络过拟合。
- en: Currently, many modifications of the backpropagation algorithm have been developed.
    Let’s look at some of them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，已经开发了许多反向传播算法的改进版本。让我们看看其中的一些。
- en: Backpropagation method modes
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播方法模式
- en: 'There are three main modes of the backpropagation method:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播方法主要有三种模式：
- en: Stochastic
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机
- en: Batch
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量
- en: Mini-batch
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量
- en: Let’s see what these modes are and how they differ from each other.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些模式是什么，以及它们之间有何不同。
- en: Stochastic mode
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机模式
- en: In stochastic mode, the backpropagation method introduces corrections to the
    weight coefficients immediately after calculating the network output on one training
    sample.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机模式中，反向传播方法在计算一个训练样本的网络输出后立即对权重系数进行校正。
- en: The stochastic method is slower than the batch method. Given that it does not
    carry out an accurate gradient descent, instead introducing some *noise* using
    an undeveloped gradient, it can get out of local minima and produce better results.
    It is also easier to apply when working with large amounts of training data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随机方法比批量方法慢。鉴于它不执行精确的梯度下降，而是通过未发展的梯度引入一些*噪声*，它可以跳出局部最小值并产生更好的结果。当处理大量训练数据时，它也更容易应用。
- en: Batch mode
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量模式
- en: For the batch mode of gradient descent, the loss function is calculated immediately
    for all available training samples, and then corrections of the weight coefficients
    of the neuron are introduced by the error backpropagation method.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降的批量模式，损失函数会立即对所有可用的训练样本进行计算，然后通过误差反向传播方法引入神经元权重系数的校正。
- en: The batch method is faster and more stable than stochastic mode, but it tends
    to stop and get stuck at local minima. Also, when it needs to train large amounts
    of data, it requires substantial computational resources.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 批量方法比随机模式更快、更稳定，但它倾向于停止并陷入局部最小值。此外，当需要训练大量数据时，它需要大量的计算资源。
- en: Mini-batch mode
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小批量模式
- en: In practice, mini-batches are often used as a compromise. The weights are adjusted
    after processing several training samples (mini-batches). This is done less often
    than with stochastic descent, but more often than with batch mode.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，小批量通常被用作折衷方案。在处理几个训练样本（小批量）之后调整权重。这比随机下降做得少，但比批量模式做得多。
- en: Now that we’ve looked at the main backpropagation training modes, let’s discuss
    the problems of the backpropagation method.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了主要的反向传播训练模式，让我们来讨论反向传播方法的问题。
- en: Backpropagation method problems
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播方法问题
- en: Despite the mini-batch method not being universal, it is widespread at the moment
    because it provides a compromise between computational scalability and learning
    effectiveness. It also has individual flaws. Most of its problems come from the
    indefinitely long learning process. In complex tasks, it may take days or even
    weeks to train the network. Also, while training the network, the values of the
    weights can become enormous due to correction. This problem can lead to all or
    most of the neurons beginning to function at enormous values, in the region where
    the derivative of the loss function is very small. Since the error that’s sent
    back during the learning process is proportional to this derivative, the learning
    process can practically freeze.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管小批量方法不是万能的，但目前它很普遍，因为它在计算可扩展性和学习有效性之间提供了一个折衷方案。它也有个别缺陷。它的大部分问题都来自无限长的学习过程。在复杂任务中，训练网络可能需要几天甚至几周的时间。此外，在训练网络时，由于校正，权重的值可能会变得非常大。这个问题可能导致所有或大多数神经元开始以巨大的值工作，在损失函数导数非常小的区域。由于学习过程中发送回的误差与这个导数成比例，学习过程实际上可能会冻结。
- en: 'The gradient descent method can get stuck in a local minimum without hitting
    a global minimum. The error backpropagation method uses a kind of gradient descent;
    that is, it descends along the error surface, continuously adjusting the weights
    until they reach a minimum. The surface of the error of a complex network is rugged
    and consists of hills, valleys, folds, and ravines in a high-dimensional space.
    A network can fall into a local minimum when there is a much deeper minimum nearby.
    At the local minimum point, all directions lead upward, and the network is unable
    to get out of it. The main difficulty in training neural networks comes down to
    the methods that are used to exit the local minima: each time we leave a local
    minimum, the next local minimum is searched by the same method, thereby backpropagating
    the error until it is no longer possible to find a way out of it.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法可能会陷入局部最小值而无法达到全局最小值。误差反向传播方法使用了一种梯度下降；也就是说，它是沿着误差表面下降，不断调整权重直到它们达到最小值。复杂网络的误差表面在多维空间中崎岖不平，由山丘、山谷、褶皱和峡谷组成。当附近存在一个更深的局部最小值时，网络可能会陷入局部最小值。在局部最小值点，所有方向都向上，网络无法从中逃脱。训练神经网络的主要困难在于用于退出局部最小值的方法：每次我们离开一个局部最小值，下一次局部最小值都是通过相同的方法搜索的，从而通过误差反向传播直到无法找到出路。
- en: A careful analysis of the proof of convergence shows that weight corrections
    are assumed to be infinitesimal. This assumption is not feasible in practice since
    it leads to an infinite learning time. The step size should be taken as the final
    size. If the step size is fixed and very small, then the convergence will be too
    slow, while if it is fixed and too large, then paralysis or permanent instability
    can occur. Today, many optimization methods have been developed that use a variable
    correction step size. They adapt the step size depending on the learning process
    (examples of such algorithms include Adam, Adagrad, RMSProp, Adadelta, and Nesterov
    Accelerated Gradient).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对收敛证明的仔细分析表明，权重校正被假定为无穷小。在实际中，这个假设是不可行的，因为它会导致无限的学习时间。步长应该取最终大小。如果步长固定且非常小，则收敛会太慢；如果固定且太大，则可能导致瘫痪或永久不稳定。今天，已经开发了许多使用可变校正步长的优化方法。它们根据学习过程调整步长（这类算法的例子包括
    Adam、Adagrad、RMSProp、Adadelta 和 Nesterov 加速梯度）。
- en: Notice that there is the possibility of the network overfitting. With too many
    neurons, the ability of the network to generalize information can be lost. The
    network can learn an entire set of samples provided for training, but any other
    images, even very similar ones, may be classified incorrectly. To prevent this
    problem, we need to use regularization and pay attention to this when designing
    our network architecture.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，网络可能存在过拟合的可能性。随着神经元数量的增加，网络泛化信息的能力可能会丧失。网络可以学习提供的整个训练样本集，但对于任何其他图像，即使是非常相似的，也可能被错误分类。为了防止这个问题，我们需要使用正则化，并在设计我们的网络架构时注意这一点。
- en: The backpropagation method – an example
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播方法 – 一个例子
- en: To understand how the backpropagation method works, let’s look at an example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解反向传播方法是如何工作的，让我们来看一个例子。
- en: 'We’ll introduce the following indexing for all expression elements: ![](img/B19849_Formula_0161.png)
    is the index of the layer, ![](img/B19849_Formula_194.png) is the index of the
    neuron in the layer, and ![](img/B19849_Formula_195.png) is the index of the current
    element or connection (for example, weight). We use these indexes as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为所有表达式元素引入以下索引：![](img/B19849_Formula_0161.png) 是层的索引，![](img/B19849_Formula_194.png)
    是层中神经元的索引，![](img/B19849_Formula_195.png) 是当前元素或连接（例如，权重）的索引。我们使用以下索引：
- en: '![](img/B19849_Formula_0191.png)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0191.png)'
- en: This expression should be read as the ![](img/B19849_Formula_0201.png)element
    of the ![](img/B19849_Formula_0213.png) neuron in the ![](img/B19849_Formula_0222.png)
    layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式应该被读作 ![](img/B19849_Formula_0201.png) 层中 ![](img/B19849_Formula_0213.png)
    神经元的元素。
- en: 'Let’s say we have a network that consists of three layers, each of which contains
    two neurons:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由三层组成的网络，每层包含两个神经元：
- en: '![Figure 10.6 – Three-layer neural network](img/B19849_10_06.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 三层神经网络](img/B19849_10_06.jpg)'
- en: Figure 10.6 – Three-layer neural network
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 三层神经网络
- en: 'As the loss function, we choose the square of the difference between the actual
    and target values:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为损失函数，我们选择实际值和目标值之间差异的平方：
- en: '![](img/B19849_Formula_023.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_023.jpg)'
- en: Here, ![](img/B19849_Formula_0242.png) is the target value of the network output,
    ![](img/B19849_Formula_0251.png) is the actual result of the output layer of the
    network, and ![](img/B19849_Formula_0262.png) is the number of neurons in the
    output layer.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_0242.png) 是网络输出的目标值，![](img/B19849_Formula_0251.png)
    是网络输出层的实际结果，![](img/B19849_Formula_0262.png) 是输出层中的神经元数量。
- en: 'This formula calculates the output sum of the neuron, ![](img/B19849_Formula_0721.png),
    in the layer, ![](img/B19849_Formula_0281.png):'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个公式计算了层中的神经元输出总和 ![](img/B19849_Formula_0721.png)，在 ![](img/B19849_Formula_0281.png)
    层：
- en: '![](img/B19849_Formula_029.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_029.jpg)'
- en: Here, ![](img/B19849_Formula_0302.png) is the number of inputs of a specific
    neuron and ![](img/B19849_Formula_0312.png) is the bias value for a specific neuron.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_0302.png) 是特定神经元的输入数量，![](img/B19849_Formula_0312.png)
    是特定神经元的偏置值。
- en: 'For example, for the first neuron from the second layer, it is equal to the
    following:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，对于第二层的第一个神经元，它等于以下内容：
- en: '![](img/B19849_Formula_0321.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0321.jpg)'
- en: Don’t forget that no weights for the first layer exist because this layer only
    represents the input values.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要忘记，第一层没有权重，因为这一层只代表输入值。
- en: The activation function that determines the output of a neuron should be a sigmoid,
    as follows:![](img/B19849_Formula_0331.jpg)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定神经元输出的激活函数应该是Sigmoid，如下所示：![](img/B19849_Formula_0331.jpg)
- en: 'Its properties, as well as other activation functions, will be discussed later
    in this chapter. Accordingly, the output of the *i*th neuron in the *l*th layer
    (![](img/B19849_Formula_0342.png)) is equal to the following:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的性质以及其他激活函数将在本章后面讨论。因此，第 *l* 层的第 *i* 个神经元的输出(![](img/B19849_Formula_0342.png))等于以下表达式：
- en: '![](img/B19849_Formula_035.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_035.jpg)'
- en: 'Now, we implement stochastic gradient descent; that is, we correct the weights
    after each training example and move in a multidimensional space of weights. To
    get to the minimum of the error, we need to move in the direction opposite to
    the gradient. We have to add error correction to each weight, ![](img/B19849_Formula_0362.png),
    based on the corresponding output. The following formula shows how we calculate
    the error correction value, ![](img/B19849_Formula_037.png), with respect to the
    ![](img/B19849_Formula_0381.png) output:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们实现随机梯度下降；也就是说，我们在每个训练示例之后校正权重，并在权重多维空间中移动。为了达到误差的最小值，我们需要朝着梯度相反的方向移动。我们必须根据相应的输出为每个权重，![](img/B19849_Formula_0362.png)，添加误差校正。以下公式显示了我们是如何根据![](img/B19849_Formula_0381.png)输出计算误差校正值，![](img/B19849_Formula_037.png)的：
- en: '![](img/B19849_Formula_039.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_039.jpg)'
- en: 'Now that we have the formula for the error correction value, we can write a
    formula for the weight update:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们有了误差校正值的公式，我们可以写出权重更新的公式：
- en: '![](img/B19849_Formula_040.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_040.jpg)'
- en: Here,- ![](img/B19849_Formula_0412.png) is a learning rate value.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，-![](img/B19849_Formula_0412.png)是一个学习率值。
- en: 'The partial derivative of the error with respect to the weights, ![](img/B19849_Formula_0422.png),
    is calculated using the chain rule, which is applied twice. Note that ![](img/B19849_Formula_0432.png)
    affects the error only in the sum, ![](img/B19849_Formula_0441.png):'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误差相对于权重的偏导数，![](img/B19849_Formula_0422.png)，使用链式法则计算，该法则应用了两次。注意![](img/B19849_Formula_0432.png)只影响求和，![](img/B19849_Formula_0441.png)：
- en: '![](img/B19849_Formula_045.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_045.jpg)'
- en: 'We start with the output layer and derive an expression that’s used to calculate
    the correction for the weight, ![](img/B19849_Formula_0461.png). To do this, we
    must sequentially calculate the components. Consider how the error is calculated
    for our network:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从输出层开始，推导出一个用于计算权重校正的表达式，![](img/B19849_Formula_0461.png)。为此，我们必须依次计算各个组成部分。考虑我们网络中误差的计算方式：
- en: '![](img/B19849_Formula_047.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_047.jpg)'
- en: Here, we can see that ![](img/B19849_Formula_0482.png) does not depend on the
    weight of ![](img/B19849_Formula_0491.png). Its partial derivative with respect
    to this variable is equal to ![](img/B19849_Formula_0501.png):![](img/B19849_Formula_0511.jpg)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，我们可以看到![](img/B19849_Formula_0482.png)不依赖于![](img/B19849_Formula_0491.png)的权重。这个变量相对于它的偏导数等于![](img/B19849_Formula_0501.png)：![](img/B19849_Formula_0511.jpg)
- en: 'Then, the general expression changes to follow the next formula:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，一般表达式变为遵循下一个公式：
- en: '![](img/B19849_Formula_0521.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0521.jpg)'
- en: 'The first part of the expression is calculated as follows:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表达式的第一部分计算如下：
- en: '![](img/B19849_Formula_053.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_053.jpg)'
- en: 'The sigmoid derivative is ![](img/B19849_Formula_0541.png), respectively. For
    the second part of the expression, we get the following:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid导数是![](img/B19849_Formula_0541.png)，分别。对于表达式的第二部分，我们得到以下结果：
- en: '![](img/B19849_Formula_055.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_055.jpg)'
- en: 'The third part is the partial derivative of the sum, which is calculated as
    follows:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三部分是求和的偏导数，其计算如下：
- en: '![](img/B19849_Formula_0561.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0561.jpg)'
- en: 'Now, we can combine everything into one formula:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们可以将所有这些组合成一个公式：
- en: '![](img/B19849_Formula_0571.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0571.jpg)'
- en: 'We can also derive a general formula in order to calculate the error correction
    for all the weights of the output layer:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以推导出一个通用公式，用于计算输出层所有权重的误差校正：
- en: '![](img/B19849_Formula_0581.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0581.jpg)'
- en: Here, ![](img/B19849_Formula_059.png) is the index of the output layer of the
    network.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_059.png)是网络输出层的索引。
- en: 'Now, we can consider how the corresponding calculations are carried out for
    the inner (hidden) layers of the network. Let’s take, for example, the weight,
    ![](img/B19849_Formula_0601.png). Here, the approach is the same, but with one
    significant difference—the output of the neuron of the hidden layer is passed
    to the input of all (or several) of the neurons of the output layer, and this
    must be taken into account:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0611.jpg)![](img/B19849_Formula_0621.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see that ![](img/B19849_Formula_0631.png) and ![](img/B19849_Formula_0642.png)
    have already been calculated in the previous step and that we can use their values
    to perform calculations:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_065.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'By combining the obtained results, we receive the following output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_066.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can calculate the second component of the sum using the values
    that were calculated in the previous steps—![](img/B19849_Formula_0672.png) and
    ![](img/B19849_Formula_0681.png):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0691.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'The remaining parts of the expression for weight correction, ![](img/B19849_Formula_0702.png),
    are obtained as follows, similar to how the expressions were obtained for the
    weights of the output layer:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0711.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'By combining the obtained results, we obtain a general formula that we can
    use to calculate the magnitude of the adjustment of the weights of the hidden
    layers:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0721.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0732.png) is the index of the hidden layer and
    ![](img/B19849_Formula_0742.png) is the number of neurons in
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: the layer, ![](img/B19849_Formula_0752.png).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have all the necessary formulas to describe the main steps of the error
    backpropagation algorithm:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Initialize all weights, ![](img/B19849_Formula_0762.png), with small random
    values (the initialization process will be discussed later).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this several times, sequentially, for all the training samples, or a
    mini-batch of samples.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass a training sample (or a mini-batch of samples) to the network input and
    calculate and remember all the outputs of the neurons. These calculate all the
    sums and values of our activation functions.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the errors for all the neurons of the output layer:![](img/B19849_Formula_077.jpg)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each neuron on all *l* layers, starting from the penultimate one, calculate
    the error:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_078.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Here, *Lnext* is the number of neurons in the *l +* *1* layer.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the network weights:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_079.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0801.png) is the learning rate value.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many versions of the backpropagation algorithm that improve the stability
    and convergence rate of the algorithm. One of the very first proposed improvements
    was the use of momentum. At each step, the value ![](img/B19849_Formula_0811.png)
    is memorized, and at the next step, we use a linear combination of the current
    gradient value and the previous one:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_082.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: '![](img/B19849_Formula_0832.png) is the hyperparameter that’s used for additional
    algorithm tuning. This algorithm is more common now than the original version
    because it allows us to achieve better results during training.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_0832.png)是用于额外算法调整的超参数。这个算法现在比原始版本更常见，因为它允许我们在训练过程中取得更好的结果。'
- en: The next important element that’s used to train the neural network is the loss
    function.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练神经网络的下一个重要元素是损失函数。
- en: Loss functions
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'With the loss function, neural network training is reduced to the process of
    optimally selecting the coefficients of the matrix of weights in order to minimize
    the error. This function should correspond to the task, for example, categorical
    cross-entropy for the classification problem or the square of the difference for
    regression. Differentiability is also an essential property of the loss function
    if the backpropagation method is used to train the network. Let’s look at some
    of the popular loss functions that are used in neural networks:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用损失函数，神经网络训练被简化为选择权重矩阵系数的过程，以最小化误差。这个函数应该对应于任务，例如，对于分类问题使用类别交叉熵，对于回归使用差异的平方。如果使用反向传播方法来训练网络，可微性也是损失函数的一个基本属性。让我们看看神经网络中使用的流行损失函数：
- en: The **mean squared error** (**MSE**) loss function is widely used for regression
    and classification tasks. Classifiers can predict continuous scores, which are
    intermediate results that are only converted into class labels (usually by a threshold)
    as the very last step of the classification process. The MSE can be calculated
    using these continuous scores rather than the class labels. The advantage of this
    is that we avoid losing information due to dichotomization. The standard form
    of the MSE loss function is defined as follows:![](img/B19849_Formula_0841.jpg)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）损失函数在回归和分类任务中得到了广泛的应用。分类器可以预测连续分数，这些是中间结果，只有在分类过程的最后一步才被转换为类别标签（通常通过阈值）。MSE可以使用这些连续分数而不是类别标签来计算。这种做法的优点是避免了由于二分化而丢失信息。MSE损失函数的标准形式定义如下：![](img/B19849_Formula_0841.jpg)'
- en: 'The **mean squared logarithmic error** (**MSLE**) loss function is a variant
    of the MSE and is defined as follows:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方对数误差**（**MSLE**）损失函数是MSE的一种变体，其定义如下：'
- en: '![](img/B19849_Formula_085.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_085.jpg)'
- en: By taking the log of the predictions and target values, the variance that we
    are measuring has changed. It is often used when we do not want to penalize considerable
    differences in the predicted and target values when both the predicted and actual
    values are big numbers. Also, the MSLE penalizes underestimates more than overestimates.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对预测值和目标值取对数，我们测量的方差已经改变。当预测值和实际值都是大数时，我们通常不希望对预测值和目标值之间的较大差异进行惩罚。此外，MSLE对低估的惩罚比对高估的惩罚更大。
- en: 'The **L2** loss function is the square of the L2 norm of the difference between
    the actual value and the target value. It is defined as follows:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2**损失函数是实际值与目标值之间差异的L2范数的平方。其定义如下：'
- en: '![](img/B19849_Formula_0861.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0861.jpg)'
- en: 'The **mean absolute error** (**MAE**) loss function is used to measure how
    close forecasts or predictions are to the eventual outcomes:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）损失函数用于衡量预测或预测值与最终结果之间的接近程度：'
- en: '![](img/B19849_Formula_087.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_087.jpg)'
- en: The MAE requires complicated tools such as linear programming to compute the
    gradient. The MAE is more robust to outliers than the MSE since it does not make
    use of the square.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: MAE需要复杂的工具，如线性规划来计算梯度。由于它没有使用平方，MAE比MSE对异常值更鲁棒。
- en: 'The **L1** loss function is the sum of absolute errors of the difference between
    the actual value and target value. Similar to the relationship between the MSE
    and L2, L1 is mathematically similar to the MAE except it does not have division
    by **n**. It is defined as follows:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1**损失函数是实际值与目标值之间差异的绝对误差之和。与MSE和L2的关系类似，L1在数学上与MAE相似，但它没有除以**n**。其定义如下：'
- en: '![](img/B19849_Formula_088.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_088.jpg)'
- en: 'The **cross-entropy** loss function is commonly used for binary classification
    tasks where labels are assumed to take values of 0 or 1\. It is defined as follows:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉熵**损失函数通常用于二元分类任务，其中标签被假定为取0或1的值。它被定义为如下：'
- en: '![](img/B19849_Formula_0891.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0891.jpg)'
- en: Cross-entropy measures the divergence between two probability distributions.
    If the cross-entropy is large, this means that the difference between the two
    distributions is significant, while if the cross-entropy is small, this means
    that the two distributions are similar to each other. The cross-entropy loss function
    has the advantage of faster convergence, and it is more likely to reach global
    optimization than the quadratic loss function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵衡量两个概率分布之间的差异。如果交叉熵很大，这意味着两个分布之间的差异显著；如果交叉熵很小，这意味着两个分布彼此相似。交叉熵损失函数的优点是收敛速度更快，并且比二次损失函数更有可能达到全局优化。
- en: 'The **negative log-likelihood** loss function is used in neural networks for
    classification tasks. It is used when the model outputs a probability for each
    class rather than the class label. It is defined as follows:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负对数似然**损失函数用于神经网络中的分类任务。当模型为每个类别输出一个概率而不是类别标签时使用。它定义如下：'
- en: '![](img/B19849_Formula_0901.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/B19849_Formula_0901.jpg)'
- en: 'The **cosine proximity** loss function computes the cosine proximity between
    the predicted value and the target value. It is defined as follows:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦邻近**损失函数计算预测值和目标值之间的余弦邻近度。它定义如下：'
- en: '![](img/B19849_Formula_0912.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/B19849_Formula_0912.jpg)'
- en: This function is the same as the cosine similarity, which is a measure of similarity
    between two non-zero vectors. This is expressed as the cosine of the angle between
    them. Unit vectors are maximally similar if they are parallel and maximally dissimilar
    if they are orthogonal.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数与余弦相似度相同，它是两个非零向量之间相似度的度量。这表示为它们之间角度的余弦值。如果单位向量平行，则它们最大程度相似；如果它们正交，则最大程度不相似。
- en: 'The **hinge loss** function is used for training classifiers. The hinge loss
    is also known as the max-margin objective and is used for *maximum-margin* classification.
    It uses the raw output of the classifier’s decision function, not the predicted
    class label. It is defined as follows:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hinge损失**函数用于训练分类器。Hinge损失也称为最大间隔目标，用于**最大间隔**分类。它使用分类器决策函数的原始输出，而不是预测的类别标签。它定义如下：'
- en: '![](img/B19849_Formula_0921.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/B19849_Formula_0921.jpg)'
- en: 'There are many other loss functions. Complex network architectures often use
    several loss functions to train different parts of a network. For example, the
    *Mask RCNN* architecture, which is used for predicting object classes and boundaries
    on images, uses different loss functions: one for regression and another for classifiers.
    In the next section, we will discuss the neuron’s activation functions.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他的损失函数。复杂的网络架构通常使用多个损失函数来训练网络的各个部分。例如，用于在图像上预测对象类别和边界的*Mask RCNN*架构，使用不同的损失函数：一个用于回归，另一个用于分类器。在下一节中，我们将讨论神经元的激活函数。
- en: Activation functions
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: What does an artificial neuron do? Simply put, it calculates the weighted sum
    of inputs, adds the bias, and decides whether to exclude this value or use it
    further. The artificial neuron doesn’t know of a threshold that can be used to
    figure out whether the output value switches neurons to the activated state. For
    this purpose, we add an activation function. It checks the value that’s produced
    by the neuron for whether external connections should recognize that this neuron
    is activated or whether it can be ignored. It determines the output value of a
    neuron, depending on the result of a weighted sum of inputs and a threshold value.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元的作用是什么？简单来说，它计算输入的加权和，加上偏置，并决定是否排除这个值或进一步使用它。人工神经元不知道一个阈值，可以用来判断输出值是否将神经元切换到激活状态。为此，我们添加一个激活函数。它检查神经元产生的值，以确定外部连接是否应该识别这个神经元是激活的，还是可以忽略。它根据输入加权和与阈值值的计算结果来确定神经元的输出值。
- en: Let’s consider some examples of activation functions and their properties.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些激活函数及其特性的例子。
- en: The stepwise activation function
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步进激活函数
- en: The stepwise activation function works like this—if the sum value is higher
    than a particular threshold value, we consider the neuron activated. Otherwise,
    we say that the neuron is inactive.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 步进激活函数的工作方式是这样的——如果总和值高于特定的阈值值，我们认为神经元被激活。否则，我们说神经元是不活跃的。
- en: 'A graph of this function can be seen in the following figure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的图形可以在以下图中看到：
- en: '![](img/B19849_10_07.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_10_07.jpg)'
- en: Figure 10.7 – Stepwise activation function
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 步进激活函数
- en: 'The function returns *1* (the neuron has been activated) when the argument
    is *> 0* (the zero value is a threshold), and the function returns *0* (the neuron
    hasn’t been activated) otherwise. This approach is easy, but it has flaws. Imagine
    that we are creating a binary classifier—a model that should say *yes* or *no*
    (activated or not). A stepwise function can do this for us—it prints *1* or *0*.
    Now, imagine a case when more neurons are required to classify many classes: *class1*,
    *class2*, *class3*, or even more. What happens if more than one neuron is activated?
    All the neurons from the activation function derive *1*.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当参数大于*0*时（零值是一个阈值），函数返回*1*（神经元已被激活），否则函数返回*0*（神经元未被激活）。这种方法很简单，但它有缺陷。想象一下我们正在创建一个二元分类器——一个应该说出*是*或*否*（激活或不）的模型。阶梯函数可以为我们做到这一点——它打印*1*或*0*。现在，想象一个需要更多神经元来分类许多类别的情形：*class1*、*class2*、*class3*，甚至更多。如果有多个神经元被激活会发生什么？所有激活函数的神经元都会得出*1*。
- en: In this case, questions arise about what class should ultimately be obtained
    for a given object. We only want one neuron to be activated, and the activation
    functions of other neurons should be zero (except in this case, we can be sure
    that the network correctly defines the class). Such a network is more challenging
    to train and achieve convergence. If the activation function is not binary, then
    the possible values are activated at 50%, activated at 20%, and so on. If several
    neurons are activated, we can find the neuron with the highest value of the activation
    function. Since there are intermediate values at the output of the neuron, the
    learning process runs smoother and faster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，关于给定对象最终应该获得哪个类别的疑问产生了。我们只想激活一个神经元，其他神经元的激活函数应该是零（除了在这种情况下，我们可以确信网络正确地定义了类别）。这样的网络更难以训练并达到收敛。如果激活函数不是二元的，那么可能的值是激活50%，激活20%，等等。如果有几个神经元被激活，我们可以找到激活函数值最高的神经元。由于神经元输出存在中间值，学习过程运行得更平滑、更快。
- en: In the stepwise activation function, the likelihood of several fully activated
    neurons appearing during training decreases (although this depends on what we
    are training and on what data). Also, the stepwise activation function is not
    differentiable at point 0 and its derivative is equal to 0 at all other points.
    This leads to difficulties when we use gradient descent methods for training.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在阶梯激活函数中，在训练期间出现几个完全激活的神经元的可能性降低（尽管这取决于我们正在训练的内容以及数据）。此外，阶梯激活函数在点0处不可导，其导数在所有其他点都等于0。这导致我们在使用梯度下降方法进行训练时遇到困难。
- en: The linear activation function
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: The linear activation function, *y = c x*, is a straight line and is proportional
    to the input (that is, the weighted sum on this neuron). Such a choice of activation
    function allows us to get a range of values, not just a binary answer. We can
    connect several neurons and if more than one neuron is activated, the decision
    is made based on the choice of, for example, the maximum value.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数，*y = c x*，是一条直线，并且与输入（即这个神经元的加权求和）成正比。这种激活函数的选择使我们能够得到一系列值，而不仅仅是二元答案。我们可以连接几个神经元，如果多个神经元被激活，决策将基于例如最大值的选取。
- en: 'The following diagram shows what the linear activation function looks like:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了线性激活函数的形状：
- en: '![Figure 10.8 – Linear activation function](img/B19849_10_08.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8 – 线性激活函数](img/B19849_10_08.jpg)'
- en: Figure 10.8 – Linear activation function
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 线性激活函数
- en: The derivative of *y = c x* with respect to *x* is *c*. This conclusion means
    that the gradient has nothing to do with the argument of the function. The gradient
    is a constant vector, while the descent is made according to a constant gradient.
    If an erroneous prediction is made, then the backpropagation error’s update changes
    are also constant and do not depend on the change that’s made regarding the input.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*y = c x*相对于*x*的导数是*c*。这个结论意味着梯度与函数的参数无关。梯度是一个常数向量，而下降是根据一个常数梯度进行的。如果做出了错误的预测，那么反向传播误差的更新变化也是常数，并且不依赖于对输入所做的任何变化。
- en: 'There is another problem: related layers. A linear function activates each
    layer. The value from this function goes to the next layer as input while the
    second layer considers the weighted sum at its inputs and, in turn, includes neurons,
    depending on another linear activation function. It doesn’t matter how many layers
    we have. If they are all linear, then the final activation function in the last
    layer is just a linear function of the inputs on the first layer. This means that
    two layers (or *N* layers) can be replaced with one layer. Due to this, we lose
    the ability to make sets of layers. The entire neural network is still similar
    to the one layer with a linear activation function because it''s the linear combination
    of linear functions.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题：相关层。线性函数激活每一层。这个函数的值作为输入传递到下一层，而第二层考虑其输入的加权总和，并反过来包含神经元，这取决于另一个线性激活函数。我们有多少层无关紧要。如果它们都是线性的，那么最后一层的最终激活函数只是第一层输入的线性函数。这意味着两层（或
    *N* 层）可以被一层替换。因此，我们失去了创建层集的能力。整个神经网络仍然类似于具有线性激活函数的一层，因为它是由线性函数的线性组合。
- en: The sigmoid activation function
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数
- en: 'The sigmoid activation function, ![](img/B19849_Formula_0931.png), is a smooth
    function, similar to a stepwise function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数，![](img/B19849_Formula_0931.png)，是一个平滑函数，类似于阶梯函数：
- en: '![Figure 10.9 – Sigmoid activation function](img/B19849_10_09.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – Sigmoid 激活函数](img/B19849_10_09.jpg)'
- en: Figure 10.9 – Sigmoid activation function
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – Sigmoid 激活函数
- en: A sigmoid is a non-linear function, and a combination of sigmoids also produces
    a non-linear function. This allows us to combine neuron layers. A sigmoid activation
    function is not binary, which makes an activation with a set of values from the
    range [0,1], in contrast to a stepwise function. A smooth gradient also characterizes
    a sigmoid. In the range of values of ![](img/B19849_Formula_0942.png) from -2
    to 2, the values, ![](img/B19849_Formula_0421.png), change very quickly. This
    gradient property means that any small change in the value of ![](img/B19849_Formula_0911.png)
    in this area entails a significant change in the value of ![](img/B19849_Formula_0972.png).
    This behavior of the function indicates that ![](img/B19849_Formula_0421.png)
    tends to cling to one of the edges of the curve.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是一个非线性函数，sigmoid 的组合也产生一个非线性函数。这使我们能够组合神经元层。Sigmoid 激活函数不是二元的，它具有从范围
    [0,1] 的值集合的激活，与阶梯函数相反。Sigmoid 也以平滑梯度为特征。在 ![](img/B19849_Formula_0942.png) 的值从
    -2 到 2 的范围内，值 ![](img/B19849_Formula_0421.png) 变化非常快。这个梯度属性意味着在这个区域任何 ![](img/B19849_Formula_0911.png)
    值的微小变化都会导致 ![](img/B19849_Formula_0972.png) 值的显著变化。这种函数的行为表明 ![](img/B19849_Formula_0421.png)
    倾向于附着在曲线的一侧。
- en: The sigmoid looks like a suitable function for classification tasks. It tries
    to bring the values to one of the sides of the curve (for example, to the upper
    edge at ![](img/B19849_Formula_0992.png) and the lower edge at ![](img/B19849_Formula_1001.png)).
    This behavior allows us to find clear boundaries in the prediction.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 看起来是一个适合分类任务的函数。它试图将值带到曲线的一侧（例如，在 ![](img/B19849_Formula_0992.png) 的上边缘和
    ![](img/B19849_Formula_1001.png) 的下边缘）。这种行为使我们能够在预测中找到清晰的边界。
- en: 'Another advantage of a sigmoid over a linear function is as follows: in the
    first case, we have a fixed range of function values, [0, 1], while a linear function
    varies within ![](img/B19849_Formula_1013.png). This is advantageous because it
    does not lead to errors in numerical calculations when dealing with large values
    on the activation function.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 相对于线性函数的另一个优点如下：在第一种情况下，我们有一个固定的函数值范围，[0, 1]，而线性函数在 ![](img/B19849_Formula_1013.png)
    内变化。这有利于处理激活函数上的大值时，不会导致数值计算错误。
- en: Today, the sigmoid is one of the most popular activation functions in neural
    networks. But it also has flaws that we have to take into account. When the sigmoid
    function approaches its maximum or minimum, the output value of ![](img/B19849_Formula_0421.png)
    tends to weakly reflect changes in ![](img/B19849_Formula_1032.png). This means
    that the gradient in such areas takes small values, and the small values cause
    the gradient to vanish. The **vanishing gradient** problem is a situation where
    a gradient value becomes too small or disappears and the neural network refuses
    to learn further or learns very slowly.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Sigmoid函数是神经网络中最受欢迎的激活函数之一。但它也存在我们必须考虑的缺陷。当Sigmoid函数接近其最大值或最小值时，![img/B19849_Formula_0421.png](img/B19849_Formula_0421.png)的输出值往往弱地反映![img/B19849_Formula_1032.png](img/B19849_Formula_1032.png)的变化。这意味着这些区域的梯度取值很小，而小的值会导致梯度消失。**梯度消失**问题是一种梯度值变得太小或消失，神经网络拒绝进一步学习或学习速度非常慢的情况。
- en: The hyperbolic tangent
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双曲正切
- en: 'The hyperbolic tangent is another commonly used activation function. It can
    be represented graphically as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切是另一种常用的激活函数。它可以如下图形表示：
- en: '![Figure 10.10 – Hyperbolic tangent activation function](img/B19849_10_10.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 10.10 – 双曲正切激活函数](img/B19849_10_10.jpg)'
- en: Figure 10.10 – Hyperbolic tangent activation function
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 10.10 – 双曲正切激活函数
- en: The hyperbolic tangent is very similar to the sigmoid. This is the correct sigmoid
    function, ![](img/B19849_Formula_1042.png). Therefore, such a function has the
    same characteristics as the sigmoid we looked at earlier. Its nature is non-linear,
    it is well suited for a combination of layers, and the range of values of the
    function is ![A black background with a black square
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数与Sigmoid函数非常相似。这是正确的Sigmoid函数，![img/B19849_Formula_1042.png](img/B19849_Formula_1042.png)。因此，这样的函数具有与我们之前看到的Sigmoid函数相同的特性。它的本质是非线性的，非常适合层与层的组合，函数值的范围是![A
    black background with a black square
- en: Description automatically generated with medium confidence](img/B19849_Formula_105.png).
    Therefore, it makes no sense to worry about the values of the activation function
    leading to computational problems. However, it is worth noting that the gradient
    of the tangential function has higher values than that of the sigmoid (the derivative
    is steeper than it is for the sigmoid). Whether we choose a sigmoid or a tangent
    function depends on the requirements of the gradient’s amplitude. As well as the
    sigmoid, the hyperbolic tangent has the inherent vanishing gradient problem.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述，置信度中等](img/B19849_Formula_105.png)。因此，没有必要担心激活函数的值会导致计算问题。然而，值得注意的是，切向函数的梯度值比Sigmoid函数的梯度值高（导数比Sigmoid函数的导数更陡峭）。我们选择Sigmoid函数还是双曲正切函数取决于梯度幅度的要求。与Sigmoid函数一样，双曲正切函数也有固有的梯度消失问题。
- en: 'The **rectified linear unit** (**ReLU**), ![](img/B19849_Formula_106.png),
    returns ![](img/B19849_Formula_0431.png) if ![](img/B19849_Formula_1082.png) is
    positive, and ![](img/B19849_Formula_1091.png) otherwise:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**），![img/B19849_Formula_106.png](img/B19849_Formula_106.png)，如果![img/B19849_Formula_1082.png](img/B19849_Formula_1082.png)为正，则返回![img/B19849_Formula_0431.png](img/B19849_Formula_0431.png)，否则返回![img/B19849_Formula_1091.png](img/B19849_Formula_1091.png)：'
- en: '![Figure 10.11 – ReLU activation function](img/B19849_10_11.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 10.11 – ReLU激活函数](img/B19849_10_11.jpg)'
- en: Figure 10.11 – ReLU activation function
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 10.11 – ReLU激活函数
- en: At first glance, it seems that ReLU has all the same problems as a linear function
    since ReLU is linear in the first quadrant. But in fact, ReLU is non-linear, and
    a combination of ReLU is also non-linear. A combination of ReLU can approximate
    any function. This property means that we can use layers and they won’t degenerate
    into a linear combination. The range of permissible values of ReLU is ![](img/B19849_Formula_1101.png),
    which means that its values can be quite high, thus leading to computational problems.
    However, this same property removes the problem of a vanishing gradient. It is
    recommended to use regularization and normalize the input data to solve the problem
    with large function values (for example, to the range of values [0,1] ).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，ReLU似乎与线性函数有相同的问题，因为ReLU在第一象限是线性的。但实际上，ReLU是非线性的，ReLU的组合也是非线性的。ReLU的组合可以逼近任何函数。这一特性意味着我们可以使用层，它们不会退化成线性组合。ReLU允许的值范围是![img/B19849_Formula_1101.png](img/B19849_Formula_1101.png)，这意味着它的值可以相当高，从而导致计算问题。然而，这一特性也消除了梯度消失的问题。建议使用正则化和归一化输入数据来解决大函数值问题（例如，到值范围[0,1]）。
- en: Let’s look at activation sparseness as a property of neural networks. Imagine
    a large neural network with many neurons. The use of a sigmoid or hyperbolic tangent
    entails the activation of all neurons. This action means that almost all activations
    must be processed to calculate the network output. In other words, activation
    is dense and costly.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看激活稀疏性作为神经网络的一个特性。想象一个拥有许多神经元的庞大神经网络。使用sigmoid或双曲正切函数意味着所有神经元都会被激活。这一动作意味着几乎所有的激活都必须被处理以计算网络输出。换句话说，激活是密集且昂贵的。
- en: Ideally, we want some neurons not to be activated, and this would make activations
    sparse and efficient. ReLU allows us to do this. Imagine a network with randomly
    initialized weights (or normalized) in which approximately 50% of activations
    are 0 because of the ReLU property, returning 0 for negative values of ![](img/B19849_Formula_1112.png).
    In such a network, fewer neurons are included (sparse activation), and the network
    itself becomes lightweight.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望某些神经元不活跃，这将使激活变得稀疏且高效。ReLU允许我们做到这一点。想象一个具有随机初始化权重（或归一化）的网络，由于ReLU的特性，大约50%的激活为0，因为![img/B19849_Formula_1112.png]的负值返回0。在这样的网络中，包含的神经元较少（稀疏激活），网络本身也变得轻量。
- en: Since part of the ReLU is a horizontal line (for negative values of ![](img/B19849_Formula_1123.png)),
    the gradient on this part is 0\. This property leads to the fact that weights
    cannot be adjusted during training. This phenomenon is called the **dying ReLU
    problem**. Because of this problem, some neurons are turned off and do not respond,
    making a significant part of the neural network passive. However, there are variations
    of ReLU that help solve this problem. For example, it makes sense to replace the
    horizontal part of the function (the region where ![](img/B19849_Formula_1133.png))
    with the linear one using the expression ![](img/B19849_Formula_1142.png). There
    are other ways to avoid a zero gradient, but the main idea is to make the gradient
    non-zero and gradually restore it during training.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ReLU的一部分是水平线（对于负值，见![img/B19849_Formula_1123.png]），这部分上的梯度为0。这一特性导致在训练过程中无法调整权重。这种现象被称为**渐逝ReLU问题**。由于这个问题，一些神经元被关闭并且不响应，使得神经网络的大部分变得被动。然而，ReLU的一些变体有助于解决这个问题。例如，用表达式![img/B19849_Formula_1142.png]将函数的水平部分（![img/B19849_Formula_1133.png]的区域）替换为线性部分是有意义的。还有其他避免零梯度的方法，但主要思想是使梯度非零，并在训练过程中逐渐恢复它。
- en: Also, ReLU is significantly less demanding on computational resources than hyperbolic
    tangent or sigmoid because it performs simpler mathematical operations than the
    aforementioned functions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ReLU在计算资源上的要求远低于双曲正切或sigmoid，因为它执行比上述函数更简单的数学运算。
- en: The critical properties of ReLU are its small computational complexity, nonlinearity,
    and unsusceptibility to the vanishing gradient problem. This makes it one of the
    most frequently used activation functions for creating deep neural networks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU的关键特性是其小的计算复杂度、非线性和对梯度消失问题的不敏感性。这使得它成为创建深度神经网络中最常用的激活函数之一。
- en: Now that we’ve looked at a number of activation functions, we can highlight
    their main properties.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了多种激活函数，我们可以突出它们的主要特性。
- en: Activation function properties
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数特性
- en: 'The following is a list of activation function properties that are worth considering
    when deciding which activation function to choose:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个激活函数特性的列表，在决定选择哪种激活函数时值得考虑：
- en: '**Nonlinearity**: If the activation function is non-linear, it can be proved
    that even a two-level neural network can be a universal approximator of the function.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性**：如果激活函数是非线性的，可以证明即使是双层神经网络也能成为函数的通用逼近器。'
- en: '**Continuous differentiability**: This property is desirable for providing
    gradient descent optimization methods.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续可微性**：这一特性对于提供梯度下降优化方法是有益的。'
- en: '**Value range**: If the set of values for the activation function is limited,
    gradient-based learning methods are more stable and less prone to calculation
    errors since there are no large values. If the range of values is infinite, training
    is usually more effective, but care must be taken to avoid exploding the gradient
    (it means that gradient values can get extremal values and the learning ability
    will be lost).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值范围**：如果激活函数的值集合有限，基于梯度的学习方法更稳定，更不容易出现计算错误，因为没有大值。如果值的范围是无限的，训练通常更有效，但必须小心避免梯度爆炸（这意味着梯度值可以达到极值，学习能力将丧失）。'
- en: '**Monotonicity**: If the activation function is monotonic, the error surface
    associated with the single-level model is guaranteed to be convex. This allows
    us to learn more effectively.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单调性**：如果激活函数是单调的，单层模型相关的误差表面将保证是凸的。这使我们能够更有效地学习。'
- en: '**Smooth functions with monotone derivatives**: In some cases, these provide
    a higher degree of generality.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单调导数的平滑函数**：在某些情况下，这些提供了更高程度的一般性。'
- en: Now that we’ve discussed the main components used to train neural networks,
    it’s time to learn how to deal with the overfitting problem, which regularly appears
    during the training process.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了用于训练神经网络的主体组件，是时候学习如何处理在训练过程中经常出现的过度拟合问题。
- en: Regularization in neural networks
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的正则化
- en: Overfitting is one of the problems of machine learning models and neural networks
    in particular. The problem is that the model only explains the samples from the
    training set, thus adapting to the training samples instead of learning to classify
    samples that were not involved in the training process (losing the ability to
    generalize). Usually, the primary cause of overfitting is the model’s complexity
    (in terms of the number of parameters it has). The complexity can be too high
    for the training set available and, ultimately, for the problem to be solved.
    The task of the regularizer is to reduce the model’s complexity, preserving the
    number of parameters. Let’s consider the most common regularization methods that
    are used in neural networks.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 过度拟合是机器学习模型，尤其是神经网络模型的问题之一。问题在于模型只解释了训练集中的样本，因此适应了训练样本而不是学习分类未参与训练过程的样本（失去了泛化的能力）。通常，过度拟合的主要原因在于模型的复杂性（从参数数量来看）。这种复杂性可能对于可用的训练集来说太高，最终对于要解决的问题来说也是如此。正则化器的作用是降低模型的复杂性，同时保留参数数量。让我们考虑在神经网络中常用的最常见正则化方法。
- en: The most popular regularization methods are L2 regularization, dropout, and
    batch normalization. Let’s take a look at each.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的正则化方法是L2正则化、dropout和批量归一化。让我们逐一看看。
- en: L2 regularization
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L2正则化
- en: '**L2 regularization** (weight decay) is performed by penalizing the weights
    with the highest values. Penalizing is performed by minimizing their ![](img/B19849_Formula_1151.png)-norm
    using the ![](img/B19849_Formula_1161.png) parameter—a regularization coefficient
    that expresses the preference for minimizing the norm when we need to minimize
    losses on the training set. That is, for each weight, ![](img/B19849_Formula_1172.png),
    we add the term, ![](img/B19849_Formula_1182.png), to the loss function, ![](img/B19849_Formula_1192.png)
    (the ![](img/B19849_Formula_1201.png) factor is used so that the gradient of this
    term with respect to the ![](img/B19849_Formula_1212.png) parameter is equal to
    ![](img/B19849_Formula_1223.png) and not ![](img/B19849_Formula_1233.png) for
    the convenience of applying the error backpropagation method). We must select
    ![](img/B19849_Formula_1243.png) correctly. If the coefficient is too small, then
    the effect of regularization is negligible. If it is too large, the model can
    reset all the weights.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2正则化**（权重衰减）通过惩罚具有最高值的权重来实现。惩罚是通过最小化它们的![公式](img/B19849_Formula_1151.png)-范数来实现的，使用![公式](img/B19849_Formula_1161.png)参数——一个正则化系数，表示在需要最小化训练集上的损失时，我们倾向于最小化范数。也就是说，对于每个权重![公式](img/B19849_Formula_1172.png)，我们在损失函数![公式](img/B19849_Formula_1192.png)中添加项![公式](img/B19849_Formula_1182.png)（使用![公式](img/B19849_Formula_1201.png)因子是为了使这个项相对于![公式](img/B19849_Formula_1212.png)参数的梯度等于![公式](img/B19849_Formula_1223.png)而不是![公式](img/B19849_Formula_1233.png)，以便于应用误差反向传播方法）。我们必须正确选择![公式](img/B19849_Formula_1243.png)。如果系数太小，那么正则化的效果可以忽略不计。如果太大，模型可以重置所有权重。'
- en: Dropout regularization
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout正则化
- en: '**Dropout regularization** consists of changing the structure of the network.
    Each neuron can be excluded from a network structure with some probability, ![](img/B19849_Formula_119.png).
    The exclusion of a neuron means that with any input data or parameters, it returns
    0.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout正则化**包括改变网络结构。每个神经元有概率被排除在网络结构之外，概率为![](img/B19849_Formula_119.png)。排除一个神经元意味着无论输入数据或参数如何，它都会返回0。'
- en: Excluded neurons do not contribute to the learning process at any stage of the
    backpropagation algorithm. Therefore, the exclusion of at least one of the neurons
    is equal to learning a new neural network. This *thinning* network is used to
    train the remaining weights. A gradient step is taken, after which all ejected
    neurons are returned to the neural network. Thus, at each step of the training,
    we set up one of the possible 2*N* network architectures. By architecture, we
    mean the structure of connections between neurons, and by *N*, we’re denoting
    the total number of neurons. When we are evaluating a neural network, neurons
    are no longer thrown out. Each neuron output is multiplied by (*1 - p*). This
    means that in the neuron’s output, we receive its response expectation for all
    2*N* architectures. Thus, a neural network trained using dropout regularization
    can be considered a result of averaging responses from an ensemble of 2*N* networks.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 被排除的神经元在任何阶段的反向传播算法中都不会对学习过程做出贡献。因此，至少排除一个神经元相当于学习一个新的神经网络。这种*稀疏*网络用于训练剩余的权重。在执行梯度步骤之后，所有被驱逐的神经元都会返回到神经网络中。因此，在训练的每一步，我们设置可能的2*N个网络架构之一。当我们评估神经网络时，神经元不再被排除。每个神经元的输出乘以(*1
    - p*)。这意味着在神经元的输出中，我们接收所有2*N个架构的响应期望。因此，使用Dropout正则化训练的神经网络可以被视为从2*N个网络集合中平均响应的结果。
- en: Batch normalization
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '**Batch normalization** makes sure that the effective learning process of neural
    networks isn’t impeded. The input signal can be significantly distorted by the
    mean and variance as the signal propagates through the inner layers of a network,
    even if we initially normalized the signal at the network input. This phenomenon
    is called the internal covariance shift and is fraught with severe discrepancies
    between the gradients at different levels or layers. Therefore, we have to use
    stronger regularizers, which slows down the pace of learning.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量归一化**确保神经网络的有效学习过程不受阻碍。当信号通过网络内部层的传播时，均值和方差可能会显著扭曲输入信号，即使我们在网络输入处最初已经对信号进行了归一化。这种现象称为内部协方差偏移，并在不同层或级别的梯度之间产生严重差异。因此，我们必须使用更强的正则化器，这会减慢学习速度。'
- en: 'Batch normalization offers a straightforward solution to this problem: normalize
    the input data in such a way as to obtain zero mean and unit variance. Normalization
    is performed before entering each layer. During the training process, we normalize
    the batch samples, and during use, we normalize the statistics obtained based
    on the entire training set since we cannot see the test data in advance. We calculate
    the mean and variance for a specific batch, ![](img/B19849_Formula_1261.png),
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化为此问题提供了一个简单直接的解决方案：以获得零均值和单位方差的方式对输入数据进行归一化。归一化在每个层进入之前执行。在训练过程中，我们对批次样本进行归一化，在使用过程中，我们根据整个训练集获得的统计数据归一化，因为我们无法提前看到测试数据。我们按照以下方式计算特定批次![](img/B19849_Formula_1261.png)的均值和方差：
- en: '![](img/B19849_Formula_1271.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_1271.jpg)'
- en: 'Using these statistical characteristics, we transform the activation function
    in such a way that it has zero mean and unit variance throughout the whole batch:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些统计特性，我们将激活函数转换为在整个批次中具有零均值和单位方差的形式：
- en: '![](img/B19849_Formula_1281.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_1281.jpg)'
- en: 'Here, ![](img/B19849_Formula_1291.png) is a parameter that protects us from
    dividing by 0 in cases where the standard deviation of the batch is very small
    or even equal to zero. Finally, to get the final activation function, ![](img/B19849_Formula_0421.png),
    we need to make sure that, during normalization, we don’t lose the ability to
    generalize. Since we applied scaling and shifting operations to the original data,
    we can allow arbitrary scaling and shifting of normalized values, thereby obtaining
    the final activation function:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_1291.png) 是一个参数，它保护我们在批次的方差非常小或甚至等于零的情况下避免除以0。最后，为了得到最终的激活函数
    ![](img/B19849_Formula_0421.png)，我们需要确保在归一化过程中，我们不会失去泛化的能力。由于我们对原始数据应用了缩放和移位操作，我们可以允许对归一化值进行任意的缩放和移位，从而获得最终的激活函数：
- en: '![](img/B19849_Formula_131.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_131.jpg)'
- en: Here, ![](img/B19849_Formula_1323.png) and ![](img/B19849_Formula_1331.png)
    are the parameters of batch normalization that the system can be trained with
    (they can be optimized by the gradient descent method on the training data). This
    generalization also means that batch normalization can be useful when applying
    the input of a neural network directly.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_1323.png) 和 ![](img/B19849_Formula_1331.png) 是系统可以训练的批归一化的参数（它们可以通过在训练数据上的梯度下降法进行优化）。这种泛化还意味着，当直接应用神经网络的输入时，批归一化可能是有用的。
- en: This method, when applied to multilayer networks, almost always successfully
    reaches its goal—it accelerates learning. Moreover, it’s an excellent regularizer,
    allowing us to choose the learning rate, the power of the ![](img/B19849_Formula_1341.png)
    regularizer, and the dropout. The regularization here is a consequence of the
    fact that the result of the network for a specific sample is no longer deterministic
    (it depends on the whole batch that this result was obtained from), which simplifies
    the generalization process.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种方法应用于多层网络时，几乎总是成功地达到其目标——加速学习。此外，它是一个出色的正则化器，允许我们选择学习率、![](img/B19849_Formula_1341.png)
    正则化器的幂和dropout。这里的正则化是这样一个事实的结果，即网络对特定样本的结果不再是确定的（它依赖于得到这个结果的整体批次），这简化了泛化过程。
- en: The next important topic we’ll look at is neural network initialization. This
    affects the convergence of the training process, training speed, and overall network
    performance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要探讨的下一个重要主题是神经网络初始化。它影响训练过程的收敛性、训练速度和整体网络性能。
- en: Neural network initialization
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络初始化
- en: The principle of choosing the initial values of weights for the layers that
    make up the model is very important. Setting all the weights to 0 is a severe
    obstacle to learning because none of the weights can be active initially. Assigning
    weights to the random values from the interval, [0, 1], is also usually not the
    best option. Actually, model performance and learning process convergence can
    strongly rely on correct weight initialization; however, the initial task and
    model complexity can also play an important role. Even if the task’s solution
    does not assume a strong dependency on the values of the initial weights, a well-chosen
    method of initializing weights can significantly affect the model’s ability to
    learn. This is because it presets the model parameters while taking the loss function
    into account. Let’s look at two popular methods that are used to initialize weights.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 选择构成模型层的权重初始值的原则非常重要。将所有权重设置为0是一个严重的阻碍学习的障碍，因为没有任何权重可以最初是活跃的。从区间[0, 1]的随机值分配权重通常也不是最佳选择。实际上，模型性能和学习过程收敛可以很大程度上依赖于正确的权重初始化；然而，初始任务和模型复杂性也可能扮演着重要的角色。即使任务的解决方案不假设对初始权重值有强烈的依赖性，一个精心选择的重置权重的方法也可以显著影响模型学习的能力。这是因为它在考虑损失函数的同时预设了模型参数。让我们看看两种常用的初始化权重的流行方法。
- en: Xavier initialization method
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Xavier初始化方法
- en: The **Xavier initialization method** is used to simplify the signal flow through
    the layer during both the forward pass and the backward pass of the error for
    the linear activation function. This method also works well for the sigmoid function,
    since the region where it is unsaturated also has a linear character. When calculating
    weights, this method relies on probability distribution (such as the uniform or
    the normal ones) with a variance of ![](img/B19849_Formula_1351.png), where ![](img/B19849_Formula_1361.png)
    and ![](img/B19849_Formula_1371.png) are the number of neurons in the previous
    and subsequent layers, respectively.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xavier初始化方法**用于简化在正向传递和反向传递误差时通过层的信号流。这种方法对sigmoid函数也适用，因为其未饱和区域也具有线性特征。在计算权重时，此方法依赖于具有方差![](img/B19849_Formula_1351.png)的概率分布（如均匀分布或正态分布），其中![](img/B19849_Formula_1361.png)和![](img/B19849_Formula_1371.png)分别是前一层和后一层中的神经元数量。'
- en: He initialization method
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 他初始化方法
- en: 'The **He initialization method** is a variation of the Xavier method that’s
    more suitable for ReLU activation functions because it compensates for the fact
    that this function returns zero for half of the definition domain. This method
    of weight calculation relies on a probability distribution with the following
    variance:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**He初始化方法**是Xavier方法的变体，更适合ReLU激活函数，因为它补偿了该函数在定义域的一半返回零的事实。这种权重计算方法依赖于以下方差的概率分布：'
- en: '![](img/B19849_Formula_138.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_138.jpg)'
- en: There are also other methods of weight initialization. Which one you choose
    is usually determined by the problem being solved, the network topology, the activation
    functions being used, and the loss function. For example, for recursive networks,
    the orthogonal initialization method can be used. We’ll provide a concrete programming
    example of neural network initialization in [*Chapter 12*](B19849_12.xhtml#_idTextAnchor660)*,
    Exporting and* *Importing Models*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他权重初始化方法。你选择哪种方法通常取决于要解决的问题、网络拓扑、使用的激活函数和损失函数。例如，对于递归网络，可以使用正交初始化方法。我们将在[*第12章*](B19849_12.xhtml#_idTextAnchor660)*，导出和导入模型*中提供一个具体的神经网络初始化编程示例。
- en: In the previous sections, we looked at the basic components of artificial neural
    networks, which are common to almost all types of networks. In the next section,
    we will discuss the features of convolutional neural networks that are often used
    for image processing.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了人工神经网络的基本组件，这些组件几乎适用于所有类型的网络。在下一节中，我们将讨论常用于图像处理的卷积神经网络的特点。
- en: Delving into convolutional networks
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入卷积网络
- en: The MLP is the most powerful feedforward neural network. It consists of several
    layers, where each neuron receives its copy of all the output from the previous
    layer of neurons. This model is ideal for certain types of tasks, for example,
    training on a limited number of more or less unstructured parameters.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: MLP（多层感知器）是最强大的前馈神经网络。它由多个层组成，其中每个神经元接收来自前一层神经元的所有输出副本。这种模型适用于某些类型的任务，例如，在有限数量的更多或更少的非结构化参数上进行训练。
- en: Nevertheless, let’s see what happens to the number of parameters (weights) in
    such a model when raw data is used as input. For example, the CIFAR-10 dataset
    contains 32 x 32 x 32 color images, and if we consider each channel of each pixel
    as an independent input parameter for the MLP, each neuron in the first hidden
    layer adds about 3,000 new parameters to the model! With the increase in image
    size, the situation quickly gets out of hand, producing images that users can’t
    use for real applications.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们看看当使用原始数据作为输入时，这种模型中的参数（权重）数量会发生什么变化。例如，CIFAR-10数据集包含32 x 32 x 32彩色图像，如果我们考虑每个像素的每个通道作为MLP的独立输入参数，第一隐藏层中的每个神经元都会为模型增加大约3,000个新参数！随着图像尺寸的增加，情况迅速失控，产生用户无法用于实际应用的图像。
- en: One popular solution is to lower the resolution of the images so that an MLP
    becomes applicable. Nevertheless, when we lower the resolution, we risk losing
    a large amount of information. It would be great if it were possible to process
    the information before applying a decrease in quality so that we don’t cause an
    explosive increase in the number of model parameters. There is a very effective
    way to solve this problem, which is based on the convolution operation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的解决方案是降低图像的分辨率，以便MLP（多层感知器）变得适用。然而，当我们降低分辨率时，我们可能会丢失大量信息。如果在应用降低质量之前处理信息，那么我们不会导致模型参数数量的爆炸性增加，这将是极好的。有一种非常有效的方法可以解决这个问题，那就是基于卷积操作。
- en: Convolution operator
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积算子
- en: This approach was first used for neural networks that worked with images, but
    it has been successfully used to solve problems from other subject areas. Let’s
    consider using this method for image classification.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最初用于处理图像的神经网络，但它已经成功地用于解决其他学科领域的问题。让我们考虑使用这种方法进行图像分类。
- en: Let’s assume that the image pixels that are close to each other interact more
    closely when forming a feature of interest for us (the feature of an object in
    the image) than pixels located at a considerable distance. Also, if a small trait
    is considered very important in the process of image classification, it does not
    matter in which part of the image this trait is found.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 假设当形成我们感兴趣的特征（图像中的物体特征）时，相邻的图像像素比位于相当距离的像素相互作用更紧密。此外，如果一个小特征在图像分类过程中被认为非常重要，那么这个特征在图像的哪个部分被发现并不重要。
- en: 'Let’s have a look at the concept of a convolution operator. We have a two-dimensional
    image of *I* and a small *K* matrix that has dimensions of *h x w* (the so-called
    convolution kernel) constructed in such a way that it graphically encodes a feature.
    We compute a minimized image of *I * K*, superimposing the core to the image in
    all possible ways and recording the sum of the elements of the original image
    and the kernel:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看卷积算子的概念。我们有一个二维图像 *I* 和一个小的 *K* 矩阵，其尺寸为 *h x w*（所谓的卷积核），它以图形方式编码了一个特征。我们计算
    *I * K* 的最小化图像，将核心以所有可能的方式叠加到图像上，并记录原始图像和核的元素之和：
- en: '![](img/B19849_Formula_139.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_139.jpg)'
- en: 'An exact definition assumes that the kernel matrix is transposed, but for machine
    learning tasks, it doesn’t matter whether this operation was performed or not.
    The convolution operator is the basis of the convolutional layer in a CNN. The
    layer consists of a certain number of kernels, ![](img/B19849_Formula_1401.png)
    (with additive displacement components, ![](img/B19849_Formula_1412.png), for
    each kernel), and calculates the convolution of the output image of the previous
    layer using each of the kernels, each time adding a displacement component. In
    the end, the activation function, ![](img/B19849_Formula_1422.png), can be applied
    to the entire output image. Usually, the input stream for a convolutional layer
    consists of *d* channels—for example, red/green/blue for the input layer, in which
    case the kernels are also expanded so that they also consist of *d* channels.
    The following formula is obtained for one channel of the output image of the convolutional
    layer, where *K* is the kernel and *b* is the stride (shift) component:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的定义假设核矩阵是转置的，但对于机器学习任务来说，这个操作是否执行并不重要。卷积算子是CNN（卷积神经网络）中卷积层的基础。该层由一定数量的核组成，![](img/B19849_Formula_1401.png)（每个核具有加性位移组件，![](img/B19849_Formula_1412.png)），并使用每个核计算前一层输出图像的卷积，每次添加一个位移组件。最后，可以将激活函数，![](img/B19849_Formula_1422.png)，应用于整个输出图像。通常，卷积层的输入流由
    *d* 个通道组成——例如，对于输入层，红色/绿色/蓝色，在这种情况下，核也扩展，以便它们也由 *d* 个通道组成。以下公式是卷积层输出图像的一个通道，其中
    *K* 是核，*b* 是步长（位移）组件：
- en: '![](img/B19849_Formula_143.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_143.jpg)'
- en: 'The following diagram schematically depicts the preceding formula:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示以示意图形式展示了前面的公式：
- en: '![Figure 10.12 – Convolution operation scheme](img/B19849_10_12.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图10.12 – 卷积操作方案](img/B19849_10_12.jpg)'
- en: Figure 10.12 – Convolution operation scheme
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 – 卷积操作方案
- en: 'If the additive (stride) component is not equal to 1, then this can be schematically
    depicted as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果加法（步长）组件不等于1，则可以如下示意图表示：
- en: '![Figure 10.13 – Convolution with the stride equals one](img/B19849_10_13.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图10.13 – 步长等于一的卷积](img/B19849_10_13.jpg)'
- en: Figure 10.13 – Convolution with the stride equals one
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13 – 步长等于一的卷积
- en: Please note that since all we are doing here is adding and scaling the input
    pixels, the kernels can be obtained from the existing training sample using the
    gradient descent method, similar to calculating weights in an MLP. An MLP could
    perfectly cope with the functions of the convolutional layer, but it requires
    a much longer training time, as well as a more significant amount of training
    data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们在这里所做的只是添加和缩放输入像素，因此可以使用梯度下降法从现有的训练样本中获得内核，类似于在MLP中计算权重。MLP可以完美地处理卷积层的功能，但它需要更长的训练时间，以及更多的训练数据。
- en: 'Notice that the convolution operator is not limited to two-dimensional data:
    most deep learning frameworks provide layers for one-dimensional or *N*-dimensional
    convolutions directly out of the box. It is also worth noting that although the
    convolutional layer reduces the number of parameters compared to a fully connected
    layer, it uses more hyperparameters—parameters that are selected before training.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到卷积算子不仅限于二维数据：大多数深度学习框架直接提供了一维或*N*维卷积的层。还值得注意的是，尽管与全连接层相比，卷积层减少了参数的数量，但它使用了更多的超参数——在训练之前选择的参数。
- en: 'In particular, the following hyperparameters are selected:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，以下超参数被选中：
- en: '**Depth**: How many kernels and bias coefficients will be involved in one layer.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度**：一个层中涉及多少个内核和偏置系数。'
- en: The **height** and **width** of each kernel.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个内核的**高度**和**宽度**。
- en: '**Step (stride)**: How much the kernel is shifted at each step when calculating
    the next pixel of the resulting image. Usually, the step value that’s taken is
    equal to 1, and the larger the value is, the smaller the size of the output image
    that’s produced.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步长（步距）**：在计算结果图像的下一个像素时，内核在每一步中移动的距离。通常，所采用的步长值等于1，值越大，产生的输出图像的大小就越小。'
- en: '**Padding**: Note that convoluting any kernel of a dimension greater than 1
    x 1 reduces the size of the output image. Since it is generally desirable to keep
    the size of the original image, the pattern is supplemented with zeros along the
    edges.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充**：请注意，对大于1 x 1维度的任何内核进行卷积都会减小输出图像的大小。由于通常希望保持原始图像的大小，因此沿边缘补充了模式。'
- en: One pass of the convolutional layer affects the image by reducing the length
    and width of a particular channel but increasing its value (depth).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的一次遍历通过减少特定通道的长度和宽度来影响图像，但增加其值（深度）。
- en: Another way to reduce the image dimension and save its general properties is
    to downsample the image. Network layers that perform such operations are called
    **pooling layers**.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少图像维度并保留其一般属性的方法是对图像进行下采样。执行此类操作的神经网络层被称为**池化层**。
- en: Pooling operation
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化操作
- en: 'A pooling layer receives small, separate fragments of the image and combines
    each fragment into one value. There are several possible methods of aggregation.
    The most straightforward one is to take the maximum from a set of pixels. This
    method is shown schematically in the following diagram:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层接收图像的小片段，并将每个片段组合成一个值。有几种可能的聚合方法。最直接的方法是从像素集中取最大值。这种方法在以下示意图中显示：
- en: '![Figure 10.14 – Pooling operation](img/B19849_10_14.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图10.14 – 池化操作](img/B19849_10_14.jpg)'
- en: Figure 10.14 – Pooling operation
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 – 池化操作
- en: Let’s consider how maximum pooling works. In the preceding diagram, we have
    a matrix of numbers that’s 6 x 6 in size. The pooling window’s size equals 3,
    so we can divide this matrix into the four smaller submatrices of size 3 x 3\.
    Then, we can choose the maximum number from each submatrix and make a smaller
    matrix of size 2 x 2 from these numbers.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑最大池化是如何工作的。在先前的图中，我们有一个6 x 6大小的数字矩阵。池化窗口的大小为3，因此我们可以将这个矩阵分成四个3 x 3大小的更小的子矩阵。然后，我们可以从每个子矩阵中选择最大值，并从这些数字中构建一个2
    x 2大小的更小的矩阵。
- en: The most important characteristic of a convolutional or pooling layer is its
    receptive field value, which allows us to understand how much information is used
    for processing. Let’s discuss it in detail.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层或池化层最重要的特性是其受感野值，这使我们能够了解用于处理的信息量。让我们详细讨论一下。
- en: Receptive field
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受感野
- en: 'An essential component of the convolutional neural network architecture is
    a reduction in the amount of data from the input to the output of the model while
    still increasing the channel depth. As mentioned earlier, this is usually done
    by choosing a convolution step (stride) or pooling layers. The receptive field
    determines how much of the original input from the source is processed at the
    output. The expansion of the receptive field allows convolutional layers to combine
    low-level features (lines, edges) to create higher-level features (curves, textures):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Receptive field concept](img/B19849_10_15.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Receptive field concept
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'The receptive field, ![](img/B19849_Formula_1441.png), of layer *k* can be
    given by the following formula:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_145.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_1461.png) is the receptive field of the layer,
    *k - 1*, ![](img/B19849_Formula_1471.png) is the filter size, and ![](img/B19849_Formula_1481.png)
    is the stride of layer *i*. So, for the preceding example, the input layer has
    *RF = 1*, the hidden layer has *RF = 3*, and the last layer has *RF =* *5*.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re acquainted with the basic concepts of CNNs, let’s look at how
    we can combine them to create a concrete network architecture for image classification.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Convolution network architecture
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network is developed from a small number of low-level filters in the initial
    stages to a vast number of filters, each of which finds a specific high-level
    attribute. The transition from level to level provides a hierarchy of pattern
    recognition.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first convolutional network architectures that was successfully
    applied to the pattern recognition task was the LeNet-5, which was developed by
    Yann LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner. It was used to recognize
    handwritten and printed numbers in the 1990s. The following diagram shows this
    architecture:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – LeNet-5 network architecture](img/B19849_10_16.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – LeNet-5 network architecture
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'The network layers of this architecture are explained in the following table:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number** | **Layer** | **Feature** **map (depth)** | **Size** | **Kernel
    size** | **Stride** | **Activation** |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| Input | Image | 1 | 32 x 32 | - | - | - |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| 1 | Convolution | 6 | 28 x 28 | 5 x 5 | 1 | tanh |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| 2 | Average pool | 6 | 14 x 14 | 2 x 2 | 2 | tanh |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| 3 | Convolution | 16 | 10 x 10 | 5 x 5 | 1 | tanh |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| 4 | Average pool | 16 | 5 x 5 | 2 x 2 | 2 | tanh |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| 5 | Convolution | 120 | 1 x 1 | 5 x 5 | 1 | tanh |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| 6 | FC |  | 84 | - | - | tanh |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Output | FC |  | 10 | - | - | softmax |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – LeNet-5 layer properties
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the depth and size of the layer are changing toward the final layer.
    We can see that the depth was increasing and that the size became smaller. This
    means that toward the final layer, the number of features the network can learn
    increased, but their size became smaller. Such behavior is very common among different
    convolutional network architectures.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss deep learning, which is a subset of machine
    learning that uses artificial neural networks to learn and make decisions. It’s
    called *deep* learning because the neural networks used have multiple layers,
    allowing them to model complex relationships and patterns in data.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most often, the term deep learning is used to describe artificial neural networks
    that are designed to work with large amounts of data and use complex algorithms
    to train the model. Algorithms for deep learning can use both supervised and unsupervised
    algorithms (reinforcement learning). The learning process is *deep* because, over
    time, the neural network covers an increasing number of levels. The deeper the
    network is (that is, the more hidden layers, filters, and levels of feature abstraction
    it has), the higher the network’s performance. On large datasets, deep learning
    shows better accuracy than traditional machine learning algorithms.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'The real breakthrough that led to the current resurgence of interest in deep
    neural networks occurred in 2012, after the publication of the article *ImageNet
    classification with deep convolutional neural networks*, by *Alex Krizhevsky*,
    *Ilya Sutskever*, and *Geoff Hinton* in the *Communications of the ACM* magazine.
    The authors have put together many different learning acceleration techniques.
    These techniques include convolutional neural networks, the intelligent use of
    GPUs, and some innovative math tricks: optimized linear neurons (ReLU) and dropout,
    showing that in a few weeks, they could train a complex neural network to a level
    that would surpass the result of traditional approaches used in computer vision.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, systems based on deep learning are applied in various fields and have
    successfully replaced the traditional approaches to machine learning. Some examples
    of areas where deep learning is used are as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech recognition**: All major commercial speech recognition systems (such
    as Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple
    Siri, Baidu, and iFlytek) are based on deep learning.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computer vision**: Today, deep learning image recognition systems are already
    able to give more accurate results than the human eye, for example, when analyzing
    medical research images (MRI, X-ray, and so on).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discovery of new drugs**: For example, the AtomNet neural network was used
    to predict new biomolecules and was put forward for the treatment of diseases
    such as the Ebola virus and multiple sclerosis.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender systems**: Today, deep learning is used to study user preferences.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bioinformatics**: It is also used to study the prediction of genetic ontologies.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we delve deeper into the realm of neural network development, we will explore
    how C++ libraries can be used for creating and training artificial neural network
    models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using C++ libraries to create neural networks
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning libraries have an API for creating and working with neural
    networks. All the libraries we used in the previous chapters—`mlpack`, `Dlib`,
    and `Flashlight`—are supported by neural networks. But there are also specialized
    frameworks for neural networks; for example, one popular one is the PyTorch framework.
    The difference between a specialized library and a general-purpose library is
    that a specialized library supports more configurable options and different network
    types, layers, and loss functions. Also, specialized libraries usually have more
    modern instruments, and these instruments are introduced to their APIs more quickly.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll create a simple MLP for a regression task with the `mlpack`,
    `Dlib`, and `Flashlight` libraries. We’ll also use the PyTorch C++ API to create
    a more advanced network—a convolutional deep neural network with the LeNet5 architecture,
    which we discussed earlier in the *Convolution network architecture* section.
    We’ll use this network for image classification.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how to use the `mlpack`, `Dlib`, and `Flashlight` libraries to create
    a simple MLP for a regression task. The task is the same for all series samples—MLP
    should learn cosine functions at limited intervals. In this book’s code samples,
    we can find the full program for data generation and MLP training. Here, we’ll
    discuss the essential parts of the programs that are used for the neural network’s
    API view. Note that the activation functions we’ll be using for these samples
    are the Tanh and ReLU functions. We’ve chosen them in order to achieve better
    convergence for this particular task.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Dlib
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Dlib` library has an API for working with neural networks. It can also
    be built with Nvidia CUDA support for performance optimization. Using CUDA or
    OpenCL for GPUs is important if we are planning to work with a large amount of
    data and deep neural networks.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The approach used in the `Dlib` library for neural networks is the same as for
    other machine learning algorithms in this library. We should instantiate and configure
    an object of the required algorithm class and then use a particular trainer to
    train it on a dataset.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'There is the `dnn_trainer` class for training neural networks in the `Dlib`
    library. Objects of this class should be initialized with an object of the concrete
    network and the object of the optimization algorithm. The most popular optimization
    algorithm is the stochastic gradient descent algorithm with momentum, which we
    discussed in the *Backpropagation method modes* section. This algorithm is implemented
    in the `sgd` class. Objects of the `sgd` class should be configured with the weight
    decay regularization and momentum parameter values. The `dnn_trainer` class has
    the following essential configuration methods: `set_learning_rate`, `set_mini_batch_size`,
    and `set_max_num_epochs`. These set the learning rate parameter value, the mini-batch
    size, and the maximum number of training epochs, respectively. Also, this trainer
    class supports dynamic learning rate change so that we can, for example, make
    a lower learning rate for later epochs. The learning rate shrink parameter can
    be configured with the `set_learning_rate_shrink_factor` method. But for the following
    example, we’ll use the constant learning rate because, for this particular data,
    it gives better training results.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The next essential item for instantiating the trainer object is the neural network
    type object. The `Dlib` library uses a declarative style to define the network
    architecture, and for this purpose, it uses C++ templates. So, to define the neural
    network architecture, we should start with the network’s input. In our case, this
    is of the `matrix<double>` type. We need to pass this as the template argument
    to the next layer type; in our case, this is the fully connected layer of the
    `fc` type. The fully connected layer type also takes the number of neurons as
    the template argument. To define the whole network, we should create the nested
    type definitions, until we reach the last layer and the loss function. In our
    case, this is the `loss_mean_squared` type, which implements the mean squared
    loss function, which is usually used for regression tasks.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the network definition with the `Dlib` library
    API:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This definition can be read in the following order:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'We started with the input layer:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we added the first hidden layer with 32 neurons:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After, we added the hyperbolic tangent activation function to the first hidden
    layer:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we added the second hidden layer with 16 neurons and an activation function:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we added the third hidden layer with 8 neurons and an activation function:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we added the last output layer with 1 neuron and without an activation
    function:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we finished with the loss function:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following snippet shows the complete source code example with a network
    definition:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we’ve configured the trainer object, we can use the `train` method
    to start the actual training process. This method takes two C++ vectors as input
    parameters. The first one should contain training objects of the `matrix<double>`
    type and the second one should contain the target regression values that are `float`
    types. We can also call the `be_verbose` method to see the output log of the training
    process. After the network has been trained, we call the `clean` method to allow
    the network object to clear the memory from the intermediate training values and
    therefore reduce memory usage.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: mlpack
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create the neural network with the `mlpack` library, we have to start by
    defining the architecture of the network. We use the `FFN` class in the `mlpack`
    library to do so, which is used for aggregating the network layers. **FFN** stands
    for **feedforward network**. The library has classes for creating layers:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '`Linear`: The fully connected layer with a value for the output size'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sigmoid`: The sigmoid activation function layer'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Convolution`: The 2D convolutional layer'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReLU`: The ReLU activation function layer'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxPooling`: The maximum pooling layer'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Softmax`: The layer with the softmax activation function'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other types of layers in the library. All of them can be added to
    the FFN type object to create a neural network. The first step of neural network
    creation is `FFN` object instantiation, and it can be done as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see that `FFN` class constructor takes two arguments. The first one
    is a loss function object, which in our case is the `MeanSeqaredError` type object.
    The second one is an initialization object; we used the `ConstantInitialization`
    type with `0` value. There are other initialization types in the `mlpack` library;
    for example, you can use the `HeInitialization` or `GlorotInitialization` types.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to add a new layer to the network, we can use the following code:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The new object layers were added with the `Add` method, and the template parameter
    was used to specialize the layer type. This method takes as a variable the number
    of parameters, which depends on the layer type. In this example, we passed a single
    parameter—the output dimensions of the fully connected linear layer. The `FNN`
    object automatically configures the input dimension.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we will be able to train the network, we have to create an optimization
    method object. We can do so as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We created an object that implements stochastic gradient descent optimization
    with momentum. The optimizer types in the `mlpack` library take not only the optimization
    parameters, such as the learning rate value, but also parameters to configure
    the whole training cycle. We passed the learning rate, the batch size, the number
    of iterations, the loss value tolerance for early stopping, and the flag to shuffle
    the dataset as arguments. Notice that we didn’t pass the training epochs number
    directly; instead, we calculated the `maxIteration` parameter value as a product
    of the epoch number and the training element number. The `MomentumSGD` class is
    just the template specialization of the SGD class with the `MomentumUpdate` policy
    class. So, to update the default momentum value, we have to access the particular
    field, as follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are various other optimizers in the `mlpack` library that follow the same
    initialization scheme.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the network and the optimizer objects, we can train a model as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We passed the `x` and `y` matrices with training samples and the optimizer
    objects as arguments into the `Train` method of the `FFN` type object. There is
    no special type for a dataset in the `mlpack` library, so the raw `arma::mat`
    objects are used for this purpose. In the general case, the training is done silently,
    which is not useful during experiments. So, there are additional parameters in
    the `Train` method to add verbosity. Following the optimizer parameter, the `Train`
    method accepts a number of callbacks. For example, if we want to see the training
    process log with loss values in the console, we can add the `ProgressBar` object
    callback as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Also, we can add another type callback. In the following example, we add the
    early stopping callback and the callback to record the best parameter values:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We configured the training to stop if the loss value doesn’t change for 20 batches,
    and to save parameters for the best loss value into the `best_params` object.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete source code for this example is as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After we have a trained model, we can use it for prediction as follows:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here we created an output variable named `predictions` and passed it with the
    input variable, `x`, to the `Predict` method. The `model` object has all the latest
    trained weights, but we can replace them with the best weights that we saved in
    the `best_weights` callback as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We just replaced the current weights by using the `Parameters` method of the
    `model` object.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: In the next sub-section, we will implement the same neural network but with
    the Flashlight framework.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Flashlight
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a neural network with the Flashlight library, you have to follow
    the same steps as we did with `mlpack` library. The main difference is that you
    will need to implement the training loop yourself. This gives you more flexibility
    when you deal with complex architectures and training approaches. Let’s start
    with a network definition. We create a feedforward model with fully connected
    linear layers as follows:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `Sequential` class was used to create the network object, and then the `add`
    method was used to populate it with layers. We used the `Linear` and `ReLU` layers
    as we did in the previous example. The main difference is that the first layer
    we added was the `View` type object. It was needed to make the model correctly
    process a batch of input data. The Flashlight tensor data layout is `{1,1,1,-1}`
    means that our input data is single-channel, one-dimensional data, and the batch
    size should be detected automatically because we used `-1` for the last dimension.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have to define a loss function object as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We used the MSE loss again because we are solving the same regression task.
    Creating an optimizer object also looks the same as for other frameworks:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We also used stochastic gradient descent with a momentum algorithm. Notice that
    the optimizer object constructor takes the model parameters as the first argument.
    It’s a different approach from the `Dlib` and `mlpack` libraries, where the training
    process is mostly hidden in the top-level training API.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: The approach where you pass model parameters to an optimizer is more common
    for frameworks where you configure the training process more precisely; you will
    see it in PyTorch.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Having all base blocks initialized, we can implement a training loop. Such
    a loop will contain the following important steps:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Prediction step—the forward propagation.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss value calculation.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradients calculation—the backpropagation.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimization step where the gradient values will be used.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearing gradients.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can implement these steps as follows:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Notice that we made two loops, one over epochs and an internal one over batches,
    in our training dataset. In the inner loop, we used the `batch_dataset` variable;
    we assume that it has the `fl::BatchDataset` dataset type, so the `batch` loop
    variable is the `std::vector` of tensors. Usually, it will have only two tensors,
    for the input data and the target batch data.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: We used the `fl::input` function to wrap our input batch tensor, `batch[0]`,
    into the Flashlight `Variable` type with disabled gradient calculations. The `Variable`
    type is used for the Flashlight auto-gradient mechanism. For the target batch
    data, `batch[1]`, we used the `fl::noGrad` function to disable gradient calculation.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Our `model` object returns a prediction tensor with a 4D shape in the WHCN format.
    If didn’t reshape your dataset for a convenient shape, you will have to use the
    `fl::reshape` function for every batch as we did in this example; otherwise, you
    will get shape inconsistency errors in the loss value calculation.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: After we calculated the loss value with the `loss` object using the predicted
    and target values, we calculated the gradient values. This was done with the `backward`
    method of the `loss_value` object, which has the `fl::Variable` type.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Having the gradient values calculated, we used the `step` method of the `sgd`
    object to apply the optimization step for the network parameters. Remember that
    we initialized the optimization `sgd` object with the model parameters (weights).
    For the final step, we called the `zeroGrad` method for the optimizer to clear
    the network parameter gradients.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'When the network (model) is trained, it’s easy to use it for prediction, as
    follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`x` should be your input data. Disabling the gradient calculation for the model
    evaluation (prediction) stage is very important because it can save a lot of computational
    resources and increase overall model throughput.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will implement a more complex neural network to solve
    an image classification task using the `PyTorch` library.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Understanding image classification using the LeNet architecture
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll implement a CNN for image classification. We are going
    to use the famous dataset of handwritten digits called the **Modified National
    Institute of Standards and Technology** (**MNIST**) dataset, which can be found
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/). The
    dataset is a standard that was proposed by the *US National Institute of Standards
    and Technology* to calibrate and compare image recognition methods using machine
    learning, primarily based on neural networks.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'The creators of the dataset used a set of samples from the US Census Bureau,
    with some samples written by students of American universities added later. All
    the samples are normalized, anti-aliased grayscale images of 28 x 28 pixels. The
    MNIST dataset contains 60,000 images for training and 10,000 images for testing.
    There are four files:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '`train-images-idx3-ubyte`: Training set images'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train-labels-idx1-ubyte`: Training set labels'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t10k-images-idx3-ubyte`: Test set images'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t10k-labels-idx1-ubyte`: Test set labels'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The files that contain labels are in the following format:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '| **Offset** | **Type** | **Value** | **Description** |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 0x00000801(2049) | Magic number (MSB first) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| 4 | 32-bit integer | 60,000 or 10,000 | Number of items |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| 8 | Unsigned char | ?? | Label |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| 9 | Unsigned char | ?? | Label |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – MNIST labels file format
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'The label values are from `0` to `9`. The files that contain images are in
    the following format:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '| **Offset** | **Type** | **Value** | **Description** |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 0x00000803(2051) | Magic number (MSB first) |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 60,000 or 10,000 | Number of images |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 28 | Number of rows |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 28 | Number of columns |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| 0 | Unsigned byte | ?? | Pixel |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| 0 | Unsigned byte | ?? | Pixel |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: Table 10.3 – MNIST image file format
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Pixels are stored in a row-wise manner, with values in the range of [0, 255].
    `0` means background (white), while `255` means foreground (black).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using the PyTorch deep learning framework. This framework
    is primarily used with the Python language. However, its core part is written
    in C++, and it has a well-documented and actively developed C++ client API called
    **LibPyTorch**. This framework is based on the linear algebra library called **ATen**,
    which heavily uses the Nvidia CUDA technology for performance improvement. The
    Python and C++ APIs are pretty much the same but have different language notations,
    so we can use the official Python documentation to learn how to use the framework.
    This documentation also contains a section stating the differences between C++
    and Python APIs and specific articles about the usage of the C++ API.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch framework is widely used for research in deep learning. As we discussed
    previously, the framework provides functionality for managing big datasets. It
    can automatically parallelize loading the data from a disk, manage pre-loaded
    buffers for the data to reduce memory usage, and limit expensive performance disk
    operations. It provides the `torch::data::Dataset` base class for the implementation
    of the user custom dataset. We only need to override two methods here: `get` and
    `size`. These methods are not virtual because we have to use the C++ template’s
    polymorphism to inherit from this class.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Reading the training dataset
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the `MNISTDataset` class, which provides access to the MNIST dataset.
    The constructor of this class takes two parameters: one is the name of the file
    that contains the images, and the other is the name of the file that contains
    the labels. It loads whole files into its memory, which is not a best practice,
    but for this dataset, this approach works well because the dataset is small. For
    bigger datasets, we have to implement another scheme of reading data from the
    disk because usually, for real tasks, we are unable to load all the data into
    the computer’s memory.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the OpenCV library to deal with images, so we store all the loaded images
    in the C++ `vector` of the `cv::Mat` type. Labels are stored in a vector of the
    `unsigned char` type. We write two additional helper functions to read images
    and labels from the disk: `ReadImages` and `ReadLabels`. The following snippet
    shows the header file for this class:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following snippet shows the implementation of the public interface of the
    class:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can see that the constructor passed the filenames to the corresponding loader
    functions. The `size` method returns the number of items that were loaded from
    the disk into the `labels` container:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following snippet shows the `get` method’s implementation:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `get` method returns an object of the `torch::data::Example<>` class. In
    general, this type holds two values: the training sample represented with the
    `torch::Tensor` type and the target value, which is also represented with the
    `torch::Tensor` type. This method retrieves an image from the corresponding container
    using a given subscript, converts the image into the `torch::Tensor` type with
    the `CvImageToTensor` function, and uses the label value converted into the `torch::Tensor`
    type as a target value.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: There is a set of `torch::tensor` functions that are used to convert a C++ variable
    into the `torch::Tensor` type. They automatically deduce the variable type and
    create a tensor with corresponding values. In our case, we explicitly convert
    the label into the `int64_t` type because the loss function we’ll be using later
    assumes that the target values have a `torch::Long` type. Also, notice that we
    passed `torch::TensorOptions` as a second argument to the `torch::tensor` function.
    We specified the `torch` type of the tensor values and told the system to place
    this tensor in the GPU memory by setting the `device` argument to be equal to
    the `torch::DeviceType::CUDA value` and by using the `torch::TensorOptions` object.
    When we manually create the PyTorch tensors, we have to explicitly configure where
    to place them—in the CPU or in the GPU. Tensors that are placed in different types
    of memory can’t be used together.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert the OpenCV image into a tensor, write the following function:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The most important part of this function is the call to the `torch::from_blob`
    function. This function constructs the tensor from values located in memory that
    are referenced by the pointer that’s passed as a first argument. A second argument
    should be a C++ vector with tensor dimension values; in our case, we specified
    a three-dimensional tensor with one channel and two image dimensions. The third
    argument is the `torch::TensorOptions` object. We specified that the data should
    be of the floating-point type and that it doesn’t require a gradient calculation.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch uses the auto-gradient approach for model training, and it means that
    it doesn’t construct a static network graph with pre-calculated gradient dependencies.
    Instead, it uses a dynamic network graph, which means that gradient flow paths
    for modules are connected and calculated dynamically during the backward pass
    of the training process. Such an architecture allows us to dynamically change
    the network’s topology and characteristics while running the program. All the
    libraries we covered previously use a static network graph.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: The third interesting PyTorch function that’s used here is the `torch::Tensor::to`
    function, which allows us to move tensors from CPU memory to GPU memory and back.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to read dataset files.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Reading dataset files
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We read the labels file with the `ReadLabels` function:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This function opens the file in binary mode and reads the header records, the
    magic number, and the number of items in the file. It also reads all the items
    directly to the C++ vector. The most important part is to correctly read the header
    records. To do this, we can use the `read_header` function:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This function reads the value from the input stream—in our case, the file stream—and
    flips the endianness. This function also assumes that header records are 32-bit
    integer values. In a different scenario, we would have to think of other ways
    to flip the endianness.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Reading the image file
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reading the images file is also pretty straightforward; we read the header records
    and sequentially read the images. From the header records, we get the total number
    of images in the file and the image size. Then, we define the OpenCV matrix object
    that has a corresponding size and type—the one-channel image with the underlying
    byte `CV_8UC1` type. We read images from the disk in a loop directly to the OpenCV
    matrix object by passing a pointer, which is returned by the `data` object variable,
    to the stream read function. The size of the data we need to read is determined
    by calling the `cv::Mat::size()` function, followed by the call to the `area`
    function. Then, we use the `convertTo` OpenCV function to convert an image from
    the `unsigned byte` type to the 32-bit floating-point type. This is important
    so that we have enough precision while performing math operations in the network
    layers. We also normalize all the data so that it’s in the range [0, 1] by dividing
    it by 255.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'We resize all the images so that they’re 32 x 32 in size because the LeNet5
    network architecture requires us to hold the original dimensions of the convolution
    filters:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now that we’ve loaded the training data, we have to define our neural network.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Neural network definition
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we chose the LeNet5 architecture, which was developed by Yann
    LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner ([http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)).
    The architecture’s details were discussed earlier in the *Convolution network
    architecture* section. Here, we’ll show you how to implement it with the PyTorch
    framework.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'All the structural parts of the neural networks in the PyTorch framework should
    be derived from the `torch::nn::Module` class. The following snippet shows the
    header file of the `LeNet5` class:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Notice that we defined the intermediate implementation class, which is called
    `LeNet5Impl`. This is because PyTorch uses a memory management model based on
    smart pointers, and all the modules should be wrapped in a special type. There
    is a special class called `torch::nn::ModuleHolder`, which is a wrapper around
    `std::shared_ptr` but also defines some additional methods for managing modules.
    So, if we want to follow all PyTorch conventions and use our module (network)
    with all PyTorch’s functions without any problems, our module class definition
    should be as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Impl` is the implementation of our module, which is derived from the `torch::nn::Module`
    class. There is a special macro that can do this definition for us automatically;
    it is called `TORCH_MODULE`. We need to specify the name of our module in order
    to use it.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: The most important function in this definition is the `forward` function. This
    function, in our case, takes the network’s input and passes it through all the
    network layers until an output value is returned from this function. If we don’t
    implement a whole network but rather *some* custom layers or *some* structural
    parts of a network, this function should assume we take the values from the previous
    layers or other parts of the network as input. Also, if we are implementing a
    custom module that isn’t from the PyTorch standard modules, we should define the
    `backward` function, which should calculate gradients for our custom operations.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: The next essential thing in our module definition is the usage of the `torch::nn::Sequential`
    class. This class is used to group sequential layers in the network and automate
    the process of forwarding values between them. We broke our network into two parts,
    one containing convolutional layers and another containing the final fully connected
    layers.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch framework contains many functions for creating layers. For example,
    the `torch::nn::Conv2d` function created the two-dimensional convolutional layer.
    Another way to create a layer in PyTorch is to use the `torch::nn::Functional`
    function to wrap some simple function into the layer, which can then be connected
    with all the outputs of the previous layer. Notice that activation functions are
    not part of the neurons in PyTorch and should be connected as a separate layer.
    The following code snippet shows the definition of our network components:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, we initialized two `torch::nn::Sequential` modules. They take a variable
    number of other modules as arguments for constructors. Notice that for the initialization
    of the `torch::nn::Conv2d` module, we have to pass the instance of the `torch::nn::Conv2dOptions`
    class, which can be initialized with the number of input channels, the number
    of output channels, and the kernel size. We used `torch::tanh` as an activation
    function; notice that it is wrapped in the `torch::nn::Functional` class instance.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: The average pooling function is also wrapped in the `torch::nn::Functional`
    class instance because it is not a layer in the PyTorch C++ API; it’s a function.
    Also, the pooling function takes several arguments, so we bound their fixed values.
    When a function in PyTorch requires the values of the dimensions, it assumes that
    we provide an instance of the `torch::IntArrayRef` type. An object of this type
    behaves as a wrapper for an array with dimension values. We should be careful
    here because such an array should exist at the same time as the wrapper lifetime;
    notice that `torch::nn::Functional` stores `torch::IntArrayRef` objects internally.
    That is why we defined `k_size` and `p_size` as static global variables.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Also, pay attention to the `register_module` function. It associates the string
    name with the module and registers it in the internals of the parent module. If
    the module is registered in a certain way, we can use a string-based parameter
    search later (often used when we need to manually manage weight updates during
    training) and automatic module serialization.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: The `torch::nn::Linear` module defines the fully connected layer and should
    be initialized with an instance of the `torch::nn::LinearOptions` type, which
    defines the number of inputs and the number of outputs, that is, a count of the
    layer’s neurons. Notice that the last layer returns 10 values, not one label,
    despite us only having a single target label. This is the standard approach in
    classification tasks.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the `forward` function’s implementation, which performs
    model inference:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This function is implemented as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: We passed the input tensor (image) to the `forward` function of the sequential
    convolutional group.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we flattened its output with the `view` tensor method because fully connected
    layers assume that the input is flat. The `view` method takes the new dimensions
    for the tensor and returns a tensor view without exactly copying the data; *-1*
    means that we don’t care about the dimension’s value and that it can be flattened.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the flattened output from the convolutional group is passed to the fully
    connected group.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we applied the softmax function to the final output. We’re unable to
    wrap `torch::log_softmax` in the `torch::nn::Functional` class instance because
    of multiple overrides.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The softmax function converts a vector, ![](img/B19849_Formula_093.png), of
    dimension ![](img/B19849_Formula_1512.png) into a vector, ![](img/B19849_Formula_291.png),
    of the same dimension, where each coordinate, ![](img/B19849_Formula_1531.png),
    of the resulting vector is represented by a real number in the range ![](img/B19849_Formula_154.png)
    and the sum of the coordinates is 1.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: 'The coordinates are calculated as follows:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_155.jpg)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
- en: The softmax function is used in machine learning for classification problems
    when the number of possible classes is more than two (for two classes, a logistic
    function is used). The coordinates, ![](img/B19849_Formula_1531.png), of the resulting
    vector can be interpreted as the probabilities that the object belongs to the
    class, ![](img/B19849_Formula_1571.png). We chose this function because its results
    can be directly used for the cross-entropy loss function, which measures the difference
    between two probability distributions. The target distribution can be directly
    calculated from the target label value—we create the 10 value’s vector of zeros
    and put one in the place indexed by the label value. Now, we have all the required
    components to train the neural network.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Network training
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we should create PyTorch data loader objects for the train and test
    datasets. The data loader object is responsible for sampling objects from the
    dataset and making mini-batches from them. This object can be configured as follows:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the `MNISTDataset` type objects representing our datasets.
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we use the `torch::data::make_data_loader` function to create a data loader
    object. This function takes the `torch::data::DataLoaderOptions` type object with
    configuration settings for the data loader. We set the mini-batch size equal to
    256 items and set 8 parallel data loading threads. We should also configure the
    sampler type, but in this case, we’ll leave the default one—the random sampler.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following snippet shows how to initialize the train and test data loaders:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Notice that we didn’t pass our dataset objects directly to the `torch::data::make_data_loader`
    function, but we applied the stacking transformation mapping to it. This transformation
    allows us to sample mini-batches in the form of the `torch::Tensor` object. If
    we skip this transformation, the mini-batches will be sampled as the C++ vector
    of tensors. Usually, this isn’t very useful because we can’t apply linear algebra
    operations to the whole batch in a vectorized manner.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to initialize the neural network object of the `LeNet5` type,
    which we defined previously. We’ll move it to the GPU to improve training and
    evaluation performance:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: When the model of our neural network has been initialized, we can initialize
    an optimizer. We chose stochastic gradient descent with momentum optimization
    for this. It is implemented in the `torch::optim::SGD` class. The object of this
    class should be initialized with model (network) parameters and the `torch::optim::SGDOptions`
    type object. All `torch::nn::Module` type objects have the `parameters()` method,
    which returns the `std::vector<Tensor>` object containing all the parameters (weights)
    of the network. There is also the `named_parameters` method, which returns the
    dictionary of named parameters. Parameter names are created with the names we
    used in the `register_module` function call. This method is handy if we want to
    filter parameters and exclude some of them from the training process.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torch::optim::SGDOptions` object can be configured with the values of
    the learning rate, the weight decay regularization factor, and the momentum value
    factor:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that we have our initialized data loaders, the `network` object, and the
    `optimizer` object, we are ready to start the training cycle. The following snippet
    shows the training cycle’s implementation:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We’ve made a loop that repeats the training cycle for 100 epochs. At the beginning
    of the training cycle, we switched our network object to training mode with `model->train()`.
    For one epoch, we iterate over all the mini-batches provided by the data loader
    object:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'For every mini-batch, we did the next training steps, cleared the previous
    gradient values by calling the `zero_grad` method for the optimizer object, made
    a forward step over the network object, `model->forward(batch.data)`, and computed
    the loss value with the `nll_loss` function. This function computes the *negative
    log-likelihood* loss. It takes two parameters: the vector containing the probability
    that a training sample belongs to a class identified by position in the vector
    and the numeric class label (number). Then, we called the `backward` method of
    the loss tensor. It recursively computes the gradients for the overall network.
    Finally, we called the `step` method for the optimizer object, which updated all
    the parameters (weights) and their corresponding gradient values. The `step` method
    only updated the parameters that were used for initialization.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s common practice to use test or validation data to check the training process
    after each epoch. We can do this in the following way:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: First, we switched the model to evaluation mode by calling the `eval` method.
    Then we iterated over all the batches from the test data loader. For each of these
    batches, we performed a forward pass over the network, calculating the loss value
    in the same way that we did for our training process. To estimate the total loss
    (error) value for the model, we averaged the loss values for all the batches.
    To get the total loss for the batch, we used `loss.sum().item<float>()`. Here,
    we summarized the losses for each training sample in the batch and moved it to
    the CPU floating-point variable with the `item<float>()` method.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we calculate the accuracy value. This is the ratio between correct answers
    and misclassified ones. Let’s go through this calculation with the following approach.
    First, we determine the predicted class labels by using the `max` method of the
    tensor object:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `max` method returns a tuple, where the values are the maximum value of
    each row of the input tensor in the given dimension and the location indices of
    each maximum value the method found. Then, we compare the predicted labels with
    the target ones and calculate the number of correct answers:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We used the `eq` tensor’s method for our comparison. This method returns a boolean
    vector whose size is equal to the input vector, with values equal to `1` where
    the vector element components are equal and with values equal to `0` where they’re
    not. To perform the comparison operation, we made a view for the target labels
    tensor with the same dimensions as the predictions tensor. The `view_as` method
    is used for this comparison. Then, we calculated the sum of `1` values and moved
    the value to the CPU variable with the `item<long>()` method.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, we can see that the specialized framework has more options we
    can configure and is more flexible for neural network development. It has more
    layer types and supports dynamic network graphs. It also has a powerful specialized
    linear algebra library that can be used to create new layers, as well as new loss
    and activation functions. It has powerful abstractions that enable us to work
    with big training data. One more important thing to note is that it has a C++
    API very similar to the Python API, so we can easily port Python programs to C++
    and vice versa.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-527
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at what artificial neural networks are, looked at
    their history, and examined the reasons for their appearance, rise, and fall and
    why they have become one of the most actively developed machine learning approaches
    today. We looked at the difference between biological and artificial neurons before
    learning the basics of the perceptron concept, which was created by Frank Rosenblatt.
    Then, we discussed the internal features of artificial neurons and networks, such
    as activation functions and their characteristics, network topology, and convolution
    layer concepts. We also learned how to train artificial neural networks with the
    error backpropagation method. We saw how to choose the right loss function for
    different types of tasks. Then, we discussed the regularization methods that are
    used to combat overfitting during training.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented a simple MLP for a regression task with the mlpack,
    Dlib, and Flashlight C++ machine learning libraries. Then, we implemented a more
    advanced convolution network for an image classification task with PyTorch, a
    specialized neural network framework. This showed us the benefits of specialized
    frameworks over general-purpose libraries.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to use pre-trained **large language
    models** (**LLMs**) and adapt them to our particular tasks. We will see how to
    use the transfer learning technique and the BERT network to perform sentiment
    analysis.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-531
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Loss Functions for Deep Neural Networks in* *Classification*: [https://arxiv.org/pdf/1702.05659.pdf](https://arxiv.org/pdf/1702.05659.pdf)'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Networks and Deep Learning*, by Michael Nielsen: [http://neuralnetwork
    sanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Principles of Neurodynamics*, Rosenblatt, Frank (1962), Washington, DC: Spartan
    Books'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Perceptrons*, Minsky M. L. and Papert S. A. 1969\. Cambridge, MA: MIT Press'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Networks and Learning Machines*, Simon O. Haykin 2008'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning*, Ian Goodfellow, Yoshua Bengio, Aaron Courville 2016'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PyTorch GitHub page: [https://github.com/pytorch/](https://github.com/pytorch/)'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PyTorch documentation site: [https://pytorch.org/docs/](https://pytorch.org/docs/)'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The LibPyTorch (C++) documentation site: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
