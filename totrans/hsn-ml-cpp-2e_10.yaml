- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Networks for Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, we have seen huge interest in **neural networks**, which are
    successfully used in various areas, such as business, medicine, technology, geology,
    and physics. Neural networks have come into play wherever it is necessary to solve
    problems of forecasting, classification, or control. Neural networks are intuitive
    as they are based on a simplified biological model of the human nervous system.
    They arose from research in the field of artificial intelligence, namely, from
    attempts to reproduce the ability of biological nervous systems to learn and correct
    mistakes by modeling the low-level structure of the brain. Neural networks are
    compelling modeling methods that allow us to reproduce extremely complex dependencies
    because they are non-linear. Neural networks also cope better with the *curse
    of dimensionality* than other methods that don’t allow modeling dependencies for
    a large number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at the basic concepts of artificial neural networks
    and show you how to implement neural networks with different C++ libraries. We’ll
    also go through the implementation of the multilayer perceptron and simple convolutional
    networks and find out what deep learning is and what its applications are.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delving into convolutional networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of using C++ libraries to create neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding image classification using the LeNet architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following to complete this chapter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Dlib` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mlpack` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Flashlight` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found in the following GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10).'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss what artificial neural networks are and their
    building blocks. We will learn how artificial neurons work and how they relate
    to their biological analogs. We will also discuss how to train neural networks
    with the backpropagation method, as well as how to deal with the overfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is a sequence of neurons interconnected by synapses. The structure
    of the neural network came into the world of programming directly from biology.
    Thanks to this structure, computers can analyze and even remember information.
    Neural networks are based on the human brain, which contains millions of neurons
    that transmit information in the form of electrical impulses.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks are inspired by biology because they are composed
    of elements with similar functionalities to those of biological neurons. These
    elements can be organized in a way that corresponds to the anatomy of the brain,
    and they demonstrate a large number of properties that are inherent to the brain.
    For example, they can learn from experience, generalize previous precedents to
    new cases, and identify significant features from input data that contains redundant
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s understand the process of a single neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **biological neuron** consists of a body and processes that connect it
    to the outside world. The processes along which a neuron receives excitation are
    called **dendrites**. The process through which a neuron transmits excitation
    is called an **axon**. Each neuron has only one axon. Dendrites and axons have
    a rather complex branching structure. The junction of the axon and a dendrite
    is called a **synapse**. The following figure shows the biological neuron scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Biological neuron scheme](img/B19849_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Biological neuron scheme
  prefs: []
  type: TYPE_NORMAL
- en: The main functionality of a neuron is to transfer excitation from dendrites
    to an axon. However, signals that come from different dendrites can affect the
    signal in the axon. A neuron gives off a signal if the total excitation exceeds
    a certain limit value, which varies within certain limits. If the signal is not
    sent to the axon, the neuron does not respond to excitation. The intensity of
    the signal that the neuron receives (and therefore the activation possibility)
    strongly depends on synapse activity. A synapse is a contact for transmitting
    this information. Each synapse has a length, and special chemicals transmit a
    signal along it. This basic circuit has many simplifications and exceptions compared
    to a biological system, but most neural networks model themselves on these simple
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'The artificial neuron receives a specific set of signals as input, each of
    which is the output of another neuron. Each input is multiplied by the corresponding
    weight, which is equivalent to its synaptic power. Then, all the products are
    summed up, and the result of this summation is used to determine the level of
    neuron activation. The following diagram shows a model that demonstrates this
    idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Mathematical neuron scheme](img/B19849_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Mathematical neuron scheme
  prefs: []
  type: TYPE_NORMAL
- en: Here, a set of input signals, denoted by ![](img/B19849_Formula_0012.png), go
    to an artificial neuron. These input signals correspond to the signals that arrive
    at the synapses of a biological neuron. Each signal is multiplied by the corresponding
    weight, ![](img/B19849_Formula_0022.png), and passed to the summing block. Each
    weight corresponds to the strength of one biological synaptic connection. The
    summing block, which corresponds to the body of the biological neuron, algebraically
    combines the weighted inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The ![](img/B19849_Formula_0032.png) signal, which is called bias, displays
    the function of the limit value, known as the `sum` so that the values of `f (sum)`
    belong to a specific interval. That is, if we have a large input number, passing
    it through the activation function gets us output in the required range. There
    are many activation functions, and we’ll go through them later in this chapter.
    To learn more about neural networks, we’ll have a look at a few more of their
    components.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron and neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first appearance of artificial neural networks can be traced to the article
    *A logical calculus of the ideas immanent in nervous activity*, which was published
    in 1943 where an early model of an artificial neuron was proposed. Later, American
    neurophysiologist Frank Rosenblatt invented the perceptron concept in 1957 as
    a mathematical model of the human brain’s information perception. Currently, terms
    such as **single-layer perceptron** (**SLP**), or just perceptron, and **multilayer
    perceptron** (**MLP**) are used. Usually, under the layers in the perceptron is
    a sequence of neurons, located at the same level and not connected. The following
    diagram shows this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – One layer of perceptrons](img/B19849_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – One layer of perceptrons
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, we can distinguish between the following types of neural network
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: This is just the source data or signals arriving as the input of
    the system (model). For example, these can be individual components of a specific
    vector from the training set, ![](img/B19849_Formula_0042.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden**: This is a layer of neurons located between the input and output
    layers. There can be more than one hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: This is the last layer of neurons that aggregates the model’s work,
    and its outputs are used as the result of the model’s work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The term **single-layer perceptron** is often understood to describe a model
    that consists of an input layer and an artificial neuron that aggregates the input
    data. This term is sometimes used in conjunction with the term **Rosenblatt’s
    perceptron**, but this is not entirely correct since Rosenblatt used a randomized
    procedure to set up connections between input data and neurons to transfer data
    to a different dimension, which made it possible to solve the problems that arose
    when classifying linearly non-separable data. In Rosenblatt’s work, a perceptron
    consists of *S* and *A* neuron types, and an *R* adder. *S* neurons are the input
    layers, *A* neurons are the hidden layers, and the *R* neuron generates the model’s
    result. The terminology’s ambiguity arose because the weights were used only for
    the *R* neuron, while constant weights were used between the *S* and *A* neuron
    types. However, note that connections between these types of neurons were established
    according to a particular randomized procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Rosenblatt’s perceptron scheme](img/B19849_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Rosenblatt’s perceptron scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'The term MLP refers to a model that consists of an input layer, a certain number
    of hidden layers, and an output layer. This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – MLP](img/B19849_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – MLP
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the architecture of a perceptron (or neural network)
    includes the direction of signal propagation. In the preceding examples, all communications
    are directed from the input neurons to the output ones—this is called a feedforward
    network. Other network architectures may also include feedback between neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The second point that we need to pay attention to in the architecture of a perceptron
    is the number of connections between neurons. In the preceding diagram, we can
    see that each neuron in one layer connects to all the neurons in the next layer—this
    is called a **fully connected layer**. This is not a requirement, but we can see
    an example of a layer with different types of connections in the *Rosenblatt’s
    perceptron* scheme in *Figure 10**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn about one of the ways in which artificial neural networks can
    be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Training with the backpropagation method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider the most common method that’s used to train a feedforward neural
    network: the **error backpropagation method**. It is related to supervised methods.
    Therefore, it requires target values in the training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm uses the output error of a neural network. At each iteration of
    the algorithm, there are two network passes—forward and backward. On a forward
    pass, an input vector is propagated from the network inputs to its outputs and
    forms a specific output vector corresponding to the current (actual) state of
    the weights. Then, the neural network error is calculated. On the backward pass,
    this error propagates from the network output to its inputs, and the neuron weights
    are corrected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function that’s used to calculate the network error is called the **loss
    function**. An example of such a function is the square of the difference between
    the actual and target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *k* is the number of output neurons in the network, *y’* is the target
    value, and *y* is the actual output value. The algorithm is iterative and uses
    the principle of *step-by-step* training; the weights of the neurons of the network
    are adjusted after one training example is submitted as input. On the backward
    pass, this error propagates from the network output to its inputs, and the following
    rule corrects the neuron’s weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0071.png) is the weight of the ![](img/B19849_Formula_0082.png)
    connection of the ![](img/B19849_Formula_0092.png) neuron, and ![](img/B19849_Formula_0101.png)
    is the learning rate parameter, which allows us to control the value of the correction
    step, ![](img/B19849_Formula_0112.png) . To accurately adjust to a minimum of
    errors, this is selected experimentally in the learning process (it varies in
    the range from 0 to 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing an appropriate learning rate can significantly impact the performance
    and convergence of machine learning models. If the learning rate is too small,
    the model may converge slowly or not converge at all. On the other hand, if the
    learning rate is too large, the model can overshoot the optimal solution and diverge,
    resulting in poor accuracy and overfitting. To avoid such issues, it is important
    to carefully choose the learning rate based on the specific problem and dataset.
    Adaptive learning rates (such as Adam) can help automatically adjust the learning
    rate during training, making it easier to achieve good results. ![](img/B19849_Formula_0122.png)
    is the number of the hierarchy of the algorithm (that is, the step number). Let’s
    say that the output sum of the *i*th neuron is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From this, we can show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the differential, ![](img/B19849_Formula_0151.png), of
    the activation function of the neurons of the network, *f (s)*, must exist and
    not be equal to zero at any point; that is, the activation function must be differentiable
    on the entire numerical axis. Therefore, to apply the backpropagation method,
    sigmoidal activation functions, such as logistic or hyperbolic tangents, are often
    used.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, training is continued not until the network is precisely tuned
    to the minimum of the error function, but until a sufficiently accurate approximation
    is achieved. This process allows us to reduce the number of learning iterations
    and prevent the network from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, many modifications of the backpropagation algorithm have been developed.
    Let’s look at some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation method modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three main modes of the backpropagation method:'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mini-batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see what these modes are and how they differ from each other.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In stochastic mode, the backpropagation method introduces corrections to the
    weight coefficients immediately after calculating the network output on one training
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: The stochastic method is slower than the batch method. Given that it does not
    carry out an accurate gradient descent, instead introducing some *noise* using
    an undeveloped gradient, it can get out of local minima and produce better results.
    It is also easier to apply when working with large amounts of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Batch mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the batch mode of gradient descent, the loss function is calculated immediately
    for all available training samples, and then corrections of the weight coefficients
    of the neuron are introduced by the error backpropagation method.
  prefs: []
  type: TYPE_NORMAL
- en: The batch method is faster and more stable than stochastic mode, but it tends
    to stop and get stuck at local minima. Also, when it needs to train large amounts
    of data, it requires substantial computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-batch mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In practice, mini-batches are often used as a compromise. The weights are adjusted
    after processing several training samples (mini-batches). This is done less often
    than with stochastic descent, but more often than with batch mode.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at the main backpropagation training modes, let’s discuss
    the problems of the backpropagation method.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation method problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the mini-batch method not being universal, it is widespread at the moment
    because it provides a compromise between computational scalability and learning
    effectiveness. It also has individual flaws. Most of its problems come from the
    indefinitely long learning process. In complex tasks, it may take days or even
    weeks to train the network. Also, while training the network, the values of the
    weights can become enormous due to correction. This problem can lead to all or
    most of the neurons beginning to function at enormous values, in the region where
    the derivative of the loss function is very small. Since the error that’s sent
    back during the learning process is proportional to this derivative, the learning
    process can practically freeze.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent method can get stuck in a local minimum without hitting
    a global minimum. The error backpropagation method uses a kind of gradient descent;
    that is, it descends along the error surface, continuously adjusting the weights
    until they reach a minimum. The surface of the error of a complex network is rugged
    and consists of hills, valleys, folds, and ravines in a high-dimensional space.
    A network can fall into a local minimum when there is a much deeper minimum nearby.
    At the local minimum point, all directions lead upward, and the network is unable
    to get out of it. The main difficulty in training neural networks comes down to
    the methods that are used to exit the local minima: each time we leave a local
    minimum, the next local minimum is searched by the same method, thereby backpropagating
    the error until it is no longer possible to find a way out of it.'
  prefs: []
  type: TYPE_NORMAL
- en: A careful analysis of the proof of convergence shows that weight corrections
    are assumed to be infinitesimal. This assumption is not feasible in practice since
    it leads to an infinite learning time. The step size should be taken as the final
    size. If the step size is fixed and very small, then the convergence will be too
    slow, while if it is fixed and too large, then paralysis or permanent instability
    can occur. Today, many optimization methods have been developed that use a variable
    correction step size. They adapt the step size depending on the learning process
    (examples of such algorithms include Adam, Adagrad, RMSProp, Adadelta, and Nesterov
    Accelerated Gradient).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that there is the possibility of the network overfitting. With too many
    neurons, the ability of the network to generalize information can be lost. The
    network can learn an entire set of samples provided for training, but any other
    images, even very similar ones, may be classified incorrectly. To prevent this
    problem, we need to use regularization and pay attention to this when designing
    our network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation method – an example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how the backpropagation method works, let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll introduce the following indexing for all expression elements: ![](img/B19849_Formula_0161.png)
    is the index of the layer, ![](img/B19849_Formula_194.png) is the index of the
    neuron in the layer, and ![](img/B19849_Formula_195.png) is the index of the current
    element or connection (for example, weight). We use these indexes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0191.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: This expression should be read as the ![](img/B19849_Formula_0201.png)element
    of the ![](img/B19849_Formula_0213.png) neuron in the ![](img/B19849_Formula_0222.png)
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we have a network that consists of three layers, each of which contains
    two neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Three-layer neural network](img/B19849_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Three-layer neural network
  prefs: []
  type: TYPE_NORMAL
- en: 'As the loss function, we choose the square of the difference between the actual
    and target values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0242.png) is the target value of the network output,
    ![](img/B19849_Formula_0251.png) is the actual result of the output layer of the
    network, and ![](img/B19849_Formula_0262.png) is the number of neurons in the
    output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This formula calculates the output sum of the neuron, ![](img/B19849_Formula_0721.png),
    in the layer, ![](img/B19849_Formula_0281.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0302.png) is the number of inputs of a specific
    neuron and ![](img/B19849_Formula_0312.png) is the bias value for a specific neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, for the first neuron from the second layer, it is equal to the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0321.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Don’t forget that no weights for the first layer exist because this layer only
    represents the input values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function that determines the output of a neuron should be a sigmoid,
    as follows:![](img/B19849_Formula_0331.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Its properties, as well as other activation functions, will be discussed later
    in this chapter. Accordingly, the output of the *i*th neuron in the *l*th layer
    (![](img/B19849_Formula_0342.png)) is equal to the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we implement stochastic gradient descent; that is, we correct the weights
    after each training example and move in a multidimensional space of weights. To
    get to the minimum of the error, we need to move in the direction opposite to
    the gradient. We have to add error correction to each weight, ![](img/B19849_Formula_0362.png),
    based on the corresponding output. The following formula shows how we calculate
    the error correction value, ![](img/B19849_Formula_037.png), with respect to the
    ![](img/B19849_Formula_0381.png) output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have the formula for the error correction value, we can write a
    formula for the weight update:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here,- ![](img/B19849_Formula_0412.png) is a learning rate value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The partial derivative of the error with respect to the weights, ![](img/B19849_Formula_0422.png),
    is calculated using the chain rule, which is applied twice. Note that ![](img/B19849_Formula_0432.png)
    affects the error only in the sum, ![](img/B19849_Formula_0441.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We start with the output layer and derive an expression that’s used to calculate
    the correction for the weight, ![](img/B19849_Formula_0461.png). To do this, we
    must sequentially calculate the components. Consider how the error is calculated
    for our network:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_047.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that ![](img/B19849_Formula_0482.png) does not depend on the
    weight of ![](img/B19849_Formula_0491.png). Its partial derivative with respect
    to this variable is equal to ![](img/B19849_Formula_0501.png):![](img/B19849_Formula_0511.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, the general expression changes to follow the next formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0521.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first part of the expression is calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_053.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid derivative is ![](img/B19849_Formula_0541.png), respectively. For
    the second part of the expression, we get the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_055.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The third part is the partial derivative of the sum, which is calculated as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0561.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can combine everything into one formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0571.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also derive a general formula in order to calculate the error correction
    for all the weights of the output layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0581.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_059.png) is the index of the output layer of the
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can consider how the corresponding calculations are carried out for
    the inner (hidden) layers of the network. Let’s take, for example, the weight,
    ![](img/B19849_Formula_0601.png). Here, the approach is the same, but with one
    significant difference—the output of the neuron of the hidden layer is passed
    to the input of all (or several) of the neurons of the output layer, and this
    must be taken into account:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0611.jpg)![](img/B19849_Formula_0621.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see that ![](img/B19849_Formula_0631.png) and ![](img/B19849_Formula_0642.png)
    have already been calculated in the previous step and that we can use their values
    to perform calculations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_065.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By combining the obtained results, we receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_066.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can calculate the second component of the sum using the values
    that were calculated in the previous steps—![](img/B19849_Formula_0672.png) and
    ![](img/B19849_Formula_0681.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0691.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The remaining parts of the expression for weight correction, ![](img/B19849_Formula_0702.png),
    are obtained as follows, similar to how the expressions were obtained for the
    weights of the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0711.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By combining the obtained results, we obtain a general formula that we can
    use to calculate the magnitude of the adjustment of the weights of the hidden
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0721.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0732.png) is the index of the hidden layer and
    ![](img/B19849_Formula_0742.png) is the number of neurons in
  prefs: []
  type: TYPE_NORMAL
- en: the layer, ![](img/B19849_Formula_0752.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have all the necessary formulas to describe the main steps of the error
    backpropagation algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize all weights, ![](img/B19849_Formula_0762.png), with small random
    values (the initialization process will be discussed later).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this several times, sequentially, for all the training samples, or a
    mini-batch of samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass a training sample (or a mini-batch of samples) to the network input and
    calculate and remember all the outputs of the neurons. These calculate all the
    sums and values of our activation functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the errors for all the neurons of the output layer:![](img/B19849_Formula_077.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each neuron on all *l* layers, starting from the penultimate one, calculate
    the error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Lnext* is the number of neurons in the *l +* *1* layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the network weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_079.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_0801.png) is the learning rate value.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many versions of the backpropagation algorithm that improve the stability
    and convergence rate of the algorithm. One of the very first proposed improvements
    was the use of momentum. At each step, the value ![](img/B19849_Formula_0811.png)
    is memorized, and at the next step, we use a linear combination of the current
    gradient value and the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_082.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B19849_Formula_0832.png) is the hyperparameter that’s used for additional
    algorithm tuning. This algorithm is more common now than the original version
    because it allows us to achieve better results during training.'
  prefs: []
  type: TYPE_NORMAL
- en: The next important element that’s used to train the neural network is the loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the loss function, neural network training is reduced to the process of
    optimally selecting the coefficients of the matrix of weights in order to minimize
    the error. This function should correspond to the task, for example, categorical
    cross-entropy for the classification problem or the square of the difference for
    regression. Differentiability is also an essential property of the loss function
    if the backpropagation method is used to train the network. Let’s look at some
    of the popular loss functions that are used in neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: The **mean squared error** (**MSE**) loss function is widely used for regression
    and classification tasks. Classifiers can predict continuous scores, which are
    intermediate results that are only converted into class labels (usually by a threshold)
    as the very last step of the classification process. The MSE can be calculated
    using these continuous scores rather than the class labels. The advantage of this
    is that we avoid losing information due to dichotomization. The standard form
    of the MSE loss function is defined as follows:![](img/B19849_Formula_0841.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **mean squared logarithmic error** (**MSLE**) loss function is a variant
    of the MSE and is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_085.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By taking the log of the predictions and target values, the variance that we
    are measuring has changed. It is often used when we do not want to penalize considerable
    differences in the predicted and target values when both the predicted and actual
    values are big numbers. Also, the MSLE penalizes underestimates more than overestimates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **L2** loss function is the square of the L2 norm of the difference between
    the actual value and the target value. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0861.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **mean absolute error** (**MAE**) loss function is used to measure how
    close forecasts or predictions are to the eventual outcomes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_087.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The MAE requires complicated tools such as linear programming to compute the
    gradient. The MAE is more robust to outliers than the MSE since it does not make
    use of the square.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **L1** loss function is the sum of absolute errors of the difference between
    the actual value and target value. Similar to the relationship between the MSE
    and L2, L1 is mathematically similar to the MAE except it does not have division
    by **n**. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_088.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **cross-entropy** loss function is commonly used for binary classification
    tasks where labels are assumed to take values of 0 or 1\. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0891.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cross-entropy measures the divergence between two probability distributions.
    If the cross-entropy is large, this means that the difference between the two
    distributions is significant, while if the cross-entropy is small, this means
    that the two distributions are similar to each other. The cross-entropy loss function
    has the advantage of faster convergence, and it is more likely to reach global
    optimization than the quadratic loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **negative log-likelihood** loss function is used in neural networks for
    classification tasks. It is used when the model outputs a probability for each
    class rather than the class label. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0901.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **cosine proximity** loss function computes the cosine proximity between
    the predicted value and the target value. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0912.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This function is the same as the cosine similarity, which is a measure of similarity
    between two non-zero vectors. This is expressed as the cosine of the angle between
    them. Unit vectors are maximally similar if they are parallel and maximally dissimilar
    if they are orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **hinge loss** function is used for training classifiers. The hinge loss
    is also known as the max-margin objective and is used for *maximum-margin* classification.
    It uses the raw output of the classifier’s decision function, not the predicted
    class label. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0921.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many other loss functions. Complex network architectures often use
    several loss functions to train different parts of a network. For example, the
    *Mask RCNN* architecture, which is used for predicting object classes and boundaries
    on images, uses different loss functions: one for regression and another for classifiers.
    In the next section, we will discuss the neuron’s activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What does an artificial neuron do? Simply put, it calculates the weighted sum
    of inputs, adds the bias, and decides whether to exclude this value or use it
    further. The artificial neuron doesn’t know of a threshold that can be used to
    figure out whether the output value switches neurons to the activated state. For
    this purpose, we add an activation function. It checks the value that’s produced
    by the neuron for whether external connections should recognize that this neuron
    is activated or whether it can be ignored. It determines the output value of a
    neuron, depending on the result of a weighted sum of inputs and a threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider some examples of activation functions and their properties.
  prefs: []
  type: TYPE_NORMAL
- en: The stepwise activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The stepwise activation function works like this—if the sum value is higher
    than a particular threshold value, we consider the neuron activated. Otherwise,
    we say that the neuron is inactive.
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph of this function can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Stepwise activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'The function returns *1* (the neuron has been activated) when the argument
    is *> 0* (the zero value is a threshold), and the function returns *0* (the neuron
    hasn’t been activated) otherwise. This approach is easy, but it has flaws. Imagine
    that we are creating a binary classifier—a model that should say *yes* or *no*
    (activated or not). A stepwise function can do this for us—it prints *1* or *0*.
    Now, imagine a case when more neurons are required to classify many classes: *class1*,
    *class2*, *class3*, or even more. What happens if more than one neuron is activated?
    All the neurons from the activation function derive *1*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, questions arise about what class should ultimately be obtained
    for a given object. We only want one neuron to be activated, and the activation
    functions of other neurons should be zero (except in this case, we can be sure
    that the network correctly defines the class). Such a network is more challenging
    to train and achieve convergence. If the activation function is not binary, then
    the possible values are activated at 50%, activated at 20%, and so on. If several
    neurons are activated, we can find the neuron with the highest value of the activation
    function. Since there are intermediate values at the output of the neuron, the
    learning process runs smoother and faster.
  prefs: []
  type: TYPE_NORMAL
- en: In the stepwise activation function, the likelihood of several fully activated
    neurons appearing during training decreases (although this depends on what we
    are training and on what data). Also, the stepwise activation function is not
    differentiable at point 0 and its derivative is equal to 0 at all other points.
    This leads to difficulties when we use gradient descent methods for training.
  prefs: []
  type: TYPE_NORMAL
- en: The linear activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear activation function, *y = c x*, is a straight line and is proportional
    to the input (that is, the weighted sum on this neuron). Such a choice of activation
    function allows us to get a range of values, not just a binary answer. We can
    connect several neurons and if more than one neuron is activated, the decision
    is made based on the choice of, for example, the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows what the linear activation function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Linear activation function](img/B19849_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Linear activation function
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of *y = c x* with respect to *x* is *c*. This conclusion means
    that the gradient has nothing to do with the argument of the function. The gradient
    is a constant vector, while the descent is made according to a constant gradient.
    If an erroneous prediction is made, then the backpropagation error’s update changes
    are also constant and do not depend on the change that’s made regarding the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another problem: related layers. A linear function activates each
    layer. The value from this function goes to the next layer as input while the
    second layer considers the weighted sum at its inputs and, in turn, includes neurons,
    depending on another linear activation function. It doesn’t matter how many layers
    we have. If they are all linear, then the final activation function in the last
    layer is just a linear function of the inputs on the first layer. This means that
    two layers (or *N* layers) can be replaced with one layer. Due to this, we lose
    the ability to make sets of layers. The entire neural network is still similar
    to the one layer with a linear activation function because it''s the linear combination
    of linear functions.'
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The sigmoid activation function, ![](img/B19849_Formula_0931.png), is a smooth
    function, similar to a stepwise function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Sigmoid activation function](img/B19849_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Sigmoid activation function
  prefs: []
  type: TYPE_NORMAL
- en: A sigmoid is a non-linear function, and a combination of sigmoids also produces
    a non-linear function. This allows us to combine neuron layers. A sigmoid activation
    function is not binary, which makes an activation with a set of values from the
    range [0,1], in contrast to a stepwise function. A smooth gradient also characterizes
    a sigmoid. In the range of values of ![](img/B19849_Formula_0942.png) from -2
    to 2, the values, ![](img/B19849_Formula_0421.png), change very quickly. This
    gradient property means that any small change in the value of ![](img/B19849_Formula_0911.png)
    in this area entails a significant change in the value of ![](img/B19849_Formula_0972.png).
    This behavior of the function indicates that ![](img/B19849_Formula_0421.png)
    tends to cling to one of the edges of the curve.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid looks like a suitable function for classification tasks. It tries
    to bring the values to one of the sides of the curve (for example, to the upper
    edge at ![](img/B19849_Formula_0992.png) and the lower edge at ![](img/B19849_Formula_1001.png)).
    This behavior allows us to find clear boundaries in the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another advantage of a sigmoid over a linear function is as follows: in the
    first case, we have a fixed range of function values, [0, 1], while a linear function
    varies within ![](img/B19849_Formula_1013.png). This is advantageous because it
    does not lead to errors in numerical calculations when dealing with large values
    on the activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, the sigmoid is one of the most popular activation functions in neural
    networks. But it also has flaws that we have to take into account. When the sigmoid
    function approaches its maximum or minimum, the output value of ![](img/B19849_Formula_0421.png)
    tends to weakly reflect changes in ![](img/B19849_Formula_1032.png). This means
    that the gradient in such areas takes small values, and the small values cause
    the gradient to vanish. The **vanishing gradient** problem is a situation where
    a gradient value becomes too small or disappears and the neural network refuses
    to learn further or learns very slowly.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperbolic tangent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperbolic tangent is another commonly used activation function. It can
    be represented graphically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Hyperbolic tangent activation function](img/B19849_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Hyperbolic tangent activation function
  prefs: []
  type: TYPE_NORMAL
- en: The hyperbolic tangent is very similar to the sigmoid. This is the correct sigmoid
    function, ![](img/B19849_Formula_1042.png). Therefore, such a function has the
    same characteristics as the sigmoid we looked at earlier. Its nature is non-linear,
    it is well suited for a combination of layers, and the range of values of the
    function is ![A black background with a black square
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated with medium confidence](img/B19849_Formula_105.png).
    Therefore, it makes no sense to worry about the values of the activation function
    leading to computational problems. However, it is worth noting that the gradient
    of the tangential function has higher values than that of the sigmoid (the derivative
    is steeper than it is for the sigmoid). Whether we choose a sigmoid or a tangent
    function depends on the requirements of the gradient’s amplitude. As well as the
    sigmoid, the hyperbolic tangent has the inherent vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **rectified linear unit** (**ReLU**), ![](img/B19849_Formula_106.png),
    returns ![](img/B19849_Formula_0431.png) if ![](img/B19849_Formula_1082.png) is
    positive, and ![](img/B19849_Formula_1091.png) otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – ReLU activation function](img/B19849_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – ReLU activation function
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it seems that ReLU has all the same problems as a linear function
    since ReLU is linear in the first quadrant. But in fact, ReLU is non-linear, and
    a combination of ReLU is also non-linear. A combination of ReLU can approximate
    any function. This property means that we can use layers and they won’t degenerate
    into a linear combination. The range of permissible values of ReLU is ![](img/B19849_Formula_1101.png),
    which means that its values can be quite high, thus leading to computational problems.
    However, this same property removes the problem of a vanishing gradient. It is
    recommended to use regularization and normalize the input data to solve the problem
    with large function values (for example, to the range of values [0,1] ).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at activation sparseness as a property of neural networks. Imagine
    a large neural network with many neurons. The use of a sigmoid or hyperbolic tangent
    entails the activation of all neurons. This action means that almost all activations
    must be processed to calculate the network output. In other words, activation
    is dense and costly.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we want some neurons not to be activated, and this would make activations
    sparse and efficient. ReLU allows us to do this. Imagine a network with randomly
    initialized weights (or normalized) in which approximately 50% of activations
    are 0 because of the ReLU property, returning 0 for negative values of ![](img/B19849_Formula_1112.png).
    In such a network, fewer neurons are included (sparse activation), and the network
    itself becomes lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: Since part of the ReLU is a horizontal line (for negative values of ![](img/B19849_Formula_1123.png)),
    the gradient on this part is 0\. This property leads to the fact that weights
    cannot be adjusted during training. This phenomenon is called the **dying ReLU
    problem**. Because of this problem, some neurons are turned off and do not respond,
    making a significant part of the neural network passive. However, there are variations
    of ReLU that help solve this problem. For example, it makes sense to replace the
    horizontal part of the function (the region where ![](img/B19849_Formula_1133.png))
    with the linear one using the expression ![](img/B19849_Formula_1142.png). There
    are other ways to avoid a zero gradient, but the main idea is to make the gradient
    non-zero and gradually restore it during training.
  prefs: []
  type: TYPE_NORMAL
- en: Also, ReLU is significantly less demanding on computational resources than hyperbolic
    tangent or sigmoid because it performs simpler mathematical operations than the
    aforementioned functions.
  prefs: []
  type: TYPE_NORMAL
- en: The critical properties of ReLU are its small computational complexity, nonlinearity,
    and unsusceptibility to the vanishing gradient problem. This makes it one of the
    most frequently used activation functions for creating deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at a number of activation functions, we can highlight
    their main properties.
  prefs: []
  type: TYPE_NORMAL
- en: Activation function properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a list of activation function properties that are worth considering
    when deciding which activation function to choose:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nonlinearity**: If the activation function is non-linear, it can be proved
    that even a two-level neural network can be a universal approximator of the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous differentiability**: This property is desirable for providing
    gradient descent optimization methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value range**: If the set of values for the activation function is limited,
    gradient-based learning methods are more stable and less prone to calculation
    errors since there are no large values. If the range of values is infinite, training
    is usually more effective, but care must be taken to avoid exploding the gradient
    (it means that gradient values can get extremal values and the learning ability
    will be lost).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monotonicity**: If the activation function is monotonic, the error surface
    associated with the single-level model is guaranteed to be convex. This allows
    us to learn more effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smooth functions with monotone derivatives**: In some cases, these provide
    a higher degree of generality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve discussed the main components used to train neural networks,
    it’s time to learn how to deal with the overfitting problem, which regularly appears
    during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting is one of the problems of machine learning models and neural networks
    in particular. The problem is that the model only explains the samples from the
    training set, thus adapting to the training samples instead of learning to classify
    samples that were not involved in the training process (losing the ability to
    generalize). Usually, the primary cause of overfitting is the model’s complexity
    (in terms of the number of parameters it has). The complexity can be too high
    for the training set available and, ultimately, for the problem to be solved.
    The task of the regularizer is to reduce the model’s complexity, preserving the
    number of parameters. Let’s consider the most common regularization methods that
    are used in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular regularization methods are L2 regularization, dropout, and
    batch normalization. Let’s take a look at each.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**L2 regularization** (weight decay) is performed by penalizing the weights
    with the highest values. Penalizing is performed by minimizing their ![](img/B19849_Formula_1151.png)-norm
    using the ![](img/B19849_Formula_1161.png) parameter—a regularization coefficient
    that expresses the preference for minimizing the norm when we need to minimize
    losses on the training set. That is, for each weight, ![](img/B19849_Formula_1172.png),
    we add the term, ![](img/B19849_Formula_1182.png), to the loss function, ![](img/B19849_Formula_1192.png)
    (the ![](img/B19849_Formula_1201.png) factor is used so that the gradient of this
    term with respect to the ![](img/B19849_Formula_1212.png) parameter is equal to
    ![](img/B19849_Formula_1223.png) and not ![](img/B19849_Formula_1233.png) for
    the convenience of applying the error backpropagation method). We must select
    ![](img/B19849_Formula_1243.png) correctly. If the coefficient is too small, then
    the effect of regularization is negligible. If it is too large, the model can
    reset all the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Dropout regularization** consists of changing the structure of the network.
    Each neuron can be excluded from a network structure with some probability, ![](img/B19849_Formula_119.png).
    The exclusion of a neuron means that with any input data or parameters, it returns
    0.'
  prefs: []
  type: TYPE_NORMAL
- en: Excluded neurons do not contribute to the learning process at any stage of the
    backpropagation algorithm. Therefore, the exclusion of at least one of the neurons
    is equal to learning a new neural network. This *thinning* network is used to
    train the remaining weights. A gradient step is taken, after which all ejected
    neurons are returned to the neural network. Thus, at each step of the training,
    we set up one of the possible 2*N* network architectures. By architecture, we
    mean the structure of connections between neurons, and by *N*, we’re denoting
    the total number of neurons. When we are evaluating a neural network, neurons
    are no longer thrown out. Each neuron output is multiplied by (*1 - p*). This
    means that in the neuron’s output, we receive its response expectation for all
    2*N* architectures. Thus, a neural network trained using dropout regularization
    can be considered a result of averaging responses from an ensemble of 2*N* networks.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Batch normalization** makes sure that the effective learning process of neural
    networks isn’t impeded. The input signal can be significantly distorted by the
    mean and variance as the signal propagates through the inner layers of a network,
    even if we initially normalized the signal at the network input. This phenomenon
    is called the internal covariance shift and is fraught with severe discrepancies
    between the gradients at different levels or layers. Therefore, we have to use
    stronger regularizers, which slows down the pace of learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization offers a straightforward solution to this problem: normalize
    the input data in such a way as to obtain zero mean and unit variance. Normalization
    is performed before entering each layer. During the training process, we normalize
    the batch samples, and during use, we normalize the statistics obtained based
    on the entire training set since we cannot see the test data in advance. We calculate
    the mean and variance for a specific batch, ![](img/B19849_Formula_1261.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1271.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using these statistical characteristics, we transform the activation function
    in such a way that it has zero mean and unit variance throughout the whole batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_1281.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B19849_Formula_1291.png) is a parameter that protects us from
    dividing by 0 in cases where the standard deviation of the batch is very small
    or even equal to zero. Finally, to get the final activation function, ![](img/B19849_Formula_0421.png),
    we need to make sure that, during normalization, we don’t lose the ability to
    generalize. Since we applied scaling and shifting operations to the original data,
    we can allow arbitrary scaling and shifting of normalized values, thereby obtaining
    the final activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_131.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_1323.png) and ![](img/B19849_Formula_1331.png)
    are the parameters of batch normalization that the system can be trained with
    (they can be optimized by the gradient descent method on the training data). This
    generalization also means that batch normalization can be useful when applying
    the input of a neural network directly.
  prefs: []
  type: TYPE_NORMAL
- en: This method, when applied to multilayer networks, almost always successfully
    reaches its goal—it accelerates learning. Moreover, it’s an excellent regularizer,
    allowing us to choose the learning rate, the power of the ![](img/B19849_Formula_1341.png)
    regularizer, and the dropout. The regularization here is a consequence of the
    fact that the result of the network for a specific sample is no longer deterministic
    (it depends on the whole batch that this result was obtained from), which simplifies
    the generalization process.
  prefs: []
  type: TYPE_NORMAL
- en: The next important topic we’ll look at is neural network initialization. This
    affects the convergence of the training process, training speed, and overall network
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The principle of choosing the initial values of weights for the layers that
    make up the model is very important. Setting all the weights to 0 is a severe
    obstacle to learning because none of the weights can be active initially. Assigning
    weights to the random values from the interval, [0, 1], is also usually not the
    best option. Actually, model performance and learning process convergence can
    strongly rely on correct weight initialization; however, the initial task and
    model complexity can also play an important role. Even if the task’s solution
    does not assume a strong dependency on the values of the initial weights, a well-chosen
    method of initializing weights can significantly affect the model’s ability to
    learn. This is because it presets the model parameters while taking the loss function
    into account. Let’s look at two popular methods that are used to initialize weights.
  prefs: []
  type: TYPE_NORMAL
- en: Xavier initialization method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Xavier initialization method** is used to simplify the signal flow through
    the layer during both the forward pass and the backward pass of the error for
    the linear activation function. This method also works well for the sigmoid function,
    since the region where it is unsaturated also has a linear character. When calculating
    weights, this method relies on probability distribution (such as the uniform or
    the normal ones) with a variance of ![](img/B19849_Formula_1351.png), where ![](img/B19849_Formula_1361.png)
    and ![](img/B19849_Formula_1371.png) are the number of neurons in the previous
    and subsequent layers, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: He initialization method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **He initialization method** is a variation of the Xavier method that’s
    more suitable for ReLU activation functions because it compensates for the fact
    that this function returns zero for half of the definition domain. This method
    of weight calculation relies on a probability distribution with the following
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_138.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are also other methods of weight initialization. Which one you choose
    is usually determined by the problem being solved, the network topology, the activation
    functions being used, and the loss function. For example, for recursive networks,
    the orthogonal initialization method can be used. We’ll provide a concrete programming
    example of neural network initialization in [*Chapter 12*](B19849_12.xhtml#_idTextAnchor660)*,
    Exporting and* *Importing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we looked at the basic components of artificial neural
    networks, which are common to almost all types of networks. In the next section,
    we will discuss the features of convolutional neural networks that are often used
    for image processing.
  prefs: []
  type: TYPE_NORMAL
- en: Delving into convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MLP is the most powerful feedforward neural network. It consists of several
    layers, where each neuron receives its copy of all the output from the previous
    layer of neurons. This model is ideal for certain types of tasks, for example,
    training on a limited number of more or less unstructured parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, let’s see what happens to the number of parameters (weights) in
    such a model when raw data is used as input. For example, the CIFAR-10 dataset
    contains 32 x 32 x 32 color images, and if we consider each channel of each pixel
    as an independent input parameter for the MLP, each neuron in the first hidden
    layer adds about 3,000 new parameters to the model! With the increase in image
    size, the situation quickly gets out of hand, producing images that users can’t
    use for real applications.
  prefs: []
  type: TYPE_NORMAL
- en: One popular solution is to lower the resolution of the images so that an MLP
    becomes applicable. Nevertheless, when we lower the resolution, we risk losing
    a large amount of information. It would be great if it were possible to process
    the information before applying a decrease in quality so that we don’t cause an
    explosive increase in the number of model parameters. There is a very effective
    way to solve this problem, which is based on the convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach was first used for neural networks that worked with images, but
    it has been successfully used to solve problems from other subject areas. Let’s
    consider using this method for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that the image pixels that are close to each other interact more
    closely when forming a feature of interest for us (the feature of an object in
    the image) than pixels located at a considerable distance. Also, if a small trait
    is considered very important in the process of image classification, it does not
    matter in which part of the image this trait is found.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the concept of a convolution operator. We have a two-dimensional
    image of *I* and a small *K* matrix that has dimensions of *h x w* (the so-called
    convolution kernel) constructed in such a way that it graphically encodes a feature.
    We compute a minimized image of *I * K*, superimposing the core to the image in
    all possible ways and recording the sum of the elements of the original image
    and the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_139.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An exact definition assumes that the kernel matrix is transposed, but for machine
    learning tasks, it doesn’t matter whether this operation was performed or not.
    The convolution operator is the basis of the convolutional layer in a CNN. The
    layer consists of a certain number of kernels, ![](img/B19849_Formula_1401.png)
    (with additive displacement components, ![](img/B19849_Formula_1412.png), for
    each kernel), and calculates the convolution of the output image of the previous
    layer using each of the kernels, each time adding a displacement component. In
    the end, the activation function, ![](img/B19849_Formula_1422.png), can be applied
    to the entire output image. Usually, the input stream for a convolutional layer
    consists of *d* channels—for example, red/green/blue for the input layer, in which
    case the kernels are also expanded so that they also consist of *d* channels.
    The following formula is obtained for one channel of the output image of the convolutional
    layer, where *K* is the kernel and *b* is the stride (shift) component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_143.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram schematically depicts the preceding formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Convolution operation scheme](img/B19849_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Convolution operation scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'If the additive (stride) component is not equal to 1, then this can be schematically
    depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Convolution with the stride equals one](img/B19849_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – Convolution with the stride equals one
  prefs: []
  type: TYPE_NORMAL
- en: Please note that since all we are doing here is adding and scaling the input
    pixels, the kernels can be obtained from the existing training sample using the
    gradient descent method, similar to calculating weights in an MLP. An MLP could
    perfectly cope with the functions of the convolutional layer, but it requires
    a much longer training time, as well as a more significant amount of training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the convolution operator is not limited to two-dimensional data:
    most deep learning frameworks provide layers for one-dimensional or *N*-dimensional
    convolutions directly out of the box. It is also worth noting that although the
    convolutional layer reduces the number of parameters compared to a fully connected
    layer, it uses more hyperparameters—parameters that are selected before training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the following hyperparameters are selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Depth**: How many kernels and bias coefficients will be involved in one layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **height** and **width** of each kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step (stride)**: How much the kernel is shifted at each step when calculating
    the next pixel of the resulting image. Usually, the step value that’s taken is
    equal to 1, and the larger the value is, the smaller the size of the output image
    that’s produced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Padding**: Note that convoluting any kernel of a dimension greater than 1
    x 1 reduces the size of the output image. Since it is generally desirable to keep
    the size of the original image, the pattern is supplemented with zeros along the
    edges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One pass of the convolutional layer affects the image by reducing the length
    and width of a particular channel but increasing its value (depth).
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce the image dimension and save its general properties is
    to downsample the image. Network layers that perform such operations are called
    **pooling layers**.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A pooling layer receives small, separate fragments of the image and combines
    each fragment into one value. There are several possible methods of aggregation.
    The most straightforward one is to take the maximum from a set of pixels. This
    method is shown schematically in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Pooling operation](img/B19849_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Pooling operation
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider how maximum pooling works. In the preceding diagram, we have
    a matrix of numbers that’s 6 x 6 in size. The pooling window’s size equals 3,
    so we can divide this matrix into the four smaller submatrices of size 3 x 3\.
    Then, we can choose the maximum number from each submatrix and make a smaller
    matrix of size 2 x 2 from these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The most important characteristic of a convolutional or pooling layer is its
    receptive field value, which allows us to understand how much information is used
    for processing. Let’s discuss it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Receptive field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An essential component of the convolutional neural network architecture is
    a reduction in the amount of data from the input to the output of the model while
    still increasing the channel depth. As mentioned earlier, this is usually done
    by choosing a convolution step (stride) or pooling layers. The receptive field
    determines how much of the original input from the source is processed at the
    output. The expansion of the receptive field allows convolutional layers to combine
    low-level features (lines, edges) to create higher-level features (curves, textures):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Receptive field concept](img/B19849_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Receptive field concept
  prefs: []
  type: TYPE_NORMAL
- en: 'The receptive field, ![](img/B19849_Formula_1441.png), of layer *k* can be
    given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_145.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_1461.png) is the receptive field of the layer,
    *k - 1*, ![](img/B19849_Formula_1471.png) is the filter size, and ![](img/B19849_Formula_1481.png)
    is the stride of layer *i*. So, for the preceding example, the input layer has
    *RF = 1*, the hidden layer has *RF = 3*, and the last layer has *RF =* *5*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re acquainted with the basic concepts of CNNs, let’s look at how
    we can combine them to create a concrete network architecture for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution network architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network is developed from a small number of low-level filters in the initial
    stages to a vast number of filters, each of which finds a specific high-level
    attribute. The transition from level to level provides a hierarchy of pattern
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first convolutional network architectures that was successfully
    applied to the pattern recognition task was the LeNet-5, which was developed by
    Yann LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner. It was used to recognize
    handwritten and printed numbers in the 1990s. The following diagram shows this
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – LeNet-5 network architecture](img/B19849_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – LeNet-5 network architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The network layers of this architecture are explained in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number** | **Layer** | **Feature** **map (depth)** | **Size** | **Kernel
    size** | **Stride** | **Activation** |'
  prefs: []
  type: TYPE_TB
- en: '| Input | Image | 1 | 32 x 32 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Convolution | 6 | 28 x 28 | 5 x 5 | 1 | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Average pool | 6 | 14 x 14 | 2 x 2 | 2 | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Convolution | 16 | 10 x 10 | 5 x 5 | 1 | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Average pool | 16 | 5 x 5 | 2 x 2 | 2 | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Convolution | 120 | 1 x 1 | 5 x 5 | 1 | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | FC |  | 84 | - | - | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| Output | FC |  | 10 | - | - | softmax |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – LeNet-5 layer properties
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the depth and size of the layer are changing toward the final layer.
    We can see that the depth was increasing and that the size became smaller. This
    means that toward the final layer, the number of features the network can learn
    increased, but their size became smaller. Such behavior is very common among different
    convolutional network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss deep learning, which is a subset of machine
    learning that uses artificial neural networks to learn and make decisions. It’s
    called *deep* learning because the neural networks used have multiple layers,
    allowing them to model complex relationships and patterns in data.
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most often, the term deep learning is used to describe artificial neural networks
    that are designed to work with large amounts of data and use complex algorithms
    to train the model. Algorithms for deep learning can use both supervised and unsupervised
    algorithms (reinforcement learning). The learning process is *deep* because, over
    time, the neural network covers an increasing number of levels. The deeper the
    network is (that is, the more hidden layers, filters, and levels of feature abstraction
    it has), the higher the network’s performance. On large datasets, deep learning
    shows better accuracy than traditional machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real breakthrough that led to the current resurgence of interest in deep
    neural networks occurred in 2012, after the publication of the article *ImageNet
    classification with deep convolutional neural networks*, by *Alex Krizhevsky*,
    *Ilya Sutskever*, and *Geoff Hinton* in the *Communications of the ACM* magazine.
    The authors have put together many different learning acceleration techniques.
    These techniques include convolutional neural networks, the intelligent use of
    GPUs, and some innovative math tricks: optimized linear neurons (ReLU) and dropout,
    showing that in a few weeks, they could train a complex neural network to a level
    that would surpass the result of traditional approaches used in computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, systems based on deep learning are applied in various fields and have
    successfully replaced the traditional approaches to machine learning. Some examples
    of areas where deep learning is used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech recognition**: All major commercial speech recognition systems (such
    as Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple
    Siri, Baidu, and iFlytek) are based on deep learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computer vision**: Today, deep learning image recognition systems are already
    able to give more accurate results than the human eye, for example, when analyzing
    medical research images (MRI, X-ray, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discovery of new drugs**: For example, the AtomNet neural network was used
    to predict new biomolecules and was put forward for the treatment of diseases
    such as the Ebola virus and multiple sclerosis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender systems**: Today, deep learning is used to study user preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bioinformatics**: It is also used to study the prediction of genetic ontologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we delve deeper into the realm of neural network development, we will explore
    how C++ libraries can be used for creating and training artificial neural network
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using C++ libraries to create neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning libraries have an API for creating and working with neural
    networks. All the libraries we used in the previous chapters—`mlpack`, `Dlib`,
    and `Flashlight`—are supported by neural networks. But there are also specialized
    frameworks for neural networks; for example, one popular one is the PyTorch framework.
    The difference between a specialized library and a general-purpose library is
    that a specialized library supports more configurable options and different network
    types, layers, and loss functions. Also, specialized libraries usually have more
    modern instruments, and these instruments are introduced to their APIs more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll create a simple MLP for a regression task with the `mlpack`,
    `Dlib`, and `Flashlight` libraries. We’ll also use the PyTorch C++ API to create
    a more advanced network—a convolutional deep neural network with the LeNet5 architecture,
    which we discussed earlier in the *Convolution network architecture* section.
    We’ll use this network for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how to use the `mlpack`, `Dlib`, and `Flashlight` libraries to create
    a simple MLP for a regression task. The task is the same for all series samples—MLP
    should learn cosine functions at limited intervals. In this book’s code samples,
    we can find the full program for data generation and MLP training. Here, we’ll
    discuss the essential parts of the programs that are used for the neural network’s
    API view. Note that the activation functions we’ll be using for these samples
    are the Tanh and ReLU functions. We’ve chosen them in order to achieve better
    convergence for this particular task.
  prefs: []
  type: TYPE_NORMAL
- en: Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Dlib` library has an API for working with neural networks. It can also
    be built with Nvidia CUDA support for performance optimization. Using CUDA or
    OpenCL for GPUs is important if we are planning to work with a large amount of
    data and deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The approach used in the `Dlib` library for neural networks is the same as for
    other machine learning algorithms in this library. We should instantiate and configure
    an object of the required algorithm class and then use a particular trainer to
    train it on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is the `dnn_trainer` class for training neural networks in the `Dlib`
    library. Objects of this class should be initialized with an object of the concrete
    network and the object of the optimization algorithm. The most popular optimization
    algorithm is the stochastic gradient descent algorithm with momentum, which we
    discussed in the *Backpropagation method modes* section. This algorithm is implemented
    in the `sgd` class. Objects of the `sgd` class should be configured with the weight
    decay regularization and momentum parameter values. The `dnn_trainer` class has
    the following essential configuration methods: `set_learning_rate`, `set_mini_batch_size`,
    and `set_max_num_epochs`. These set the learning rate parameter value, the mini-batch
    size, and the maximum number of training epochs, respectively. Also, this trainer
    class supports dynamic learning rate change so that we can, for example, make
    a lower learning rate for later epochs. The learning rate shrink parameter can
    be configured with the `set_learning_rate_shrink_factor` method. But for the following
    example, we’ll use the constant learning rate because, for this particular data,
    it gives better training results.'
  prefs: []
  type: TYPE_NORMAL
- en: The next essential item for instantiating the trainer object is the neural network
    type object. The `Dlib` library uses a declarative style to define the network
    architecture, and for this purpose, it uses C++ templates. So, to define the neural
    network architecture, we should start with the network’s input. In our case, this
    is of the `matrix<double>` type. We need to pass this as the template argument
    to the next layer type; in our case, this is the fully connected layer of the
    `fc` type. The fully connected layer type also takes the number of neurons as
    the template argument. To define the whole network, we should create the nested
    type definitions, until we reach the last layer and the loss function. In our
    case, this is the `loss_mean_squared` type, which implements the mean squared
    loss function, which is usually used for regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the network definition with the `Dlib` library
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This definition can be read in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We started with the input layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we added the first hidden layer with 32 neurons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After, we added the hyperbolic tangent activation function to the first hidden
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we added the second hidden layer with 16 neurons and an activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we added the third hidden layer with 8 neurons and an activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we added the last output layer with 1 neuron and without an activation
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we finished with the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following snippet shows the complete source code example with a network
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve configured the trainer object, we can use the `train` method
    to start the actual training process. This method takes two C++ vectors as input
    parameters. The first one should contain training objects of the `matrix<double>`
    type and the second one should contain the target regression values that are `float`
    types. We can also call the `be_verbose` method to see the output log of the training
    process. After the network has been trained, we call the `clean` method to allow
    the network object to clear the memory from the intermediate training values and
    therefore reduce memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create the neural network with the `mlpack` library, we have to start by
    defining the architecture of the network. We use the `FFN` class in the `mlpack`
    library to do so, which is used for aggregating the network layers. **FFN** stands
    for **feedforward network**. The library has classes for creating layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Linear`: The fully connected layer with a value for the output size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sigmoid`: The sigmoid activation function layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Convolution`: The 2D convolutional layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReLU`: The ReLU activation function layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxPooling`: The maximum pooling layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Softmax`: The layer with the softmax activation function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other types of layers in the library. All of them can be added to
    the FFN type object to create a neural network. The first step of neural network
    creation is `FFN` object instantiation, and it can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can see that `FFN` class constructor takes two arguments. The first one
    is a loss function object, which in our case is the `MeanSeqaredError` type object.
    The second one is an initialization object; we used the `ConstantInitialization`
    type with `0` value. There are other initialization types in the `mlpack` library;
    for example, you can use the `HeInitialization` or `GlorotInitialization` types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to add a new layer to the network, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The new object layers were added with the `Add` method, and the template parameter
    was used to specialize the layer type. This method takes as a variable the number
    of parameters, which depends on the layer type. In this example, we passed a single
    parameter—the output dimensions of the fully connected linear layer. The `FNN`
    object automatically configures the input dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we will be able to train the network, we have to create an optimization
    method object. We can do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We created an object that implements stochastic gradient descent optimization
    with momentum. The optimizer types in the `mlpack` library take not only the optimization
    parameters, such as the learning rate value, but also parameters to configure
    the whole training cycle. We passed the learning rate, the batch size, the number
    of iterations, the loss value tolerance for early stopping, and the flag to shuffle
    the dataset as arguments. Notice that we didn’t pass the training epochs number
    directly; instead, we calculated the `maxIteration` parameter value as a product
    of the epoch number and the training element number. The `MomentumSGD` class is
    just the template specialization of the SGD class with the `MomentumUpdate` policy
    class. So, to update the default momentum value, we have to access the particular
    field, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are various other optimizers in the `mlpack` library that follow the same
    initialization scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the network and the optimizer objects, we can train a model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We passed the `x` and `y` matrices with training samples and the optimizer
    objects as arguments into the `Train` method of the `FFN` type object. There is
    no special type for a dataset in the `mlpack` library, so the raw `arma::mat`
    objects are used for this purpose. In the general case, the training is done silently,
    which is not useful during experiments. So, there are additional parameters in
    the `Train` method to add verbosity. Following the optimizer parameter, the `Train`
    method accepts a number of callbacks. For example, if we want to see the training
    process log with loss values in the console, we can add the `ProgressBar` object
    callback as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can add another type callback. In the following example, we add the
    early stopping callback and the callback to record the best parameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We configured the training to stop if the loss value doesn’t change for 20 batches,
    and to save parameters for the best loss value into the `best_params` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete source code for this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have a trained model, we can use it for prediction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we created an output variable named `predictions` and passed it with the
    input variable, `x`, to the `Predict` method. The `model` object has all the latest
    trained weights, but we can replace them with the best weights that we saved in
    the `best_weights` callback as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We just replaced the current weights by using the `Parameters` method of the
    `model` object.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sub-section, we will implement the same neural network but with
    the Flashlight framework.
  prefs: []
  type: TYPE_NORMAL
- en: Flashlight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a neural network with the Flashlight library, you have to follow
    the same steps as we did with `mlpack` library. The main difference is that you
    will need to implement the training loop yourself. This gives you more flexibility
    when you deal with complex architectures and training approaches. Let’s start
    with a network definition. We create a feedforward model with fully connected
    linear layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `Sequential` class was used to create the network object, and then the `add`
    method was used to populate it with layers. We used the `Linear` and `ReLU` layers
    as we did in the previous example. The main difference is that the first layer
    we added was the `View` type object. It was needed to make the model correctly
    process a batch of input data. The Flashlight tensor data layout is `{1,1,1,-1}`
    means that our input data is single-channel, one-dimensional data, and the batch
    size should be detected automatically because we used `-1` for the last dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have to define a loss function object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the MSE loss again because we are solving the same regression task.
    Creating an optimizer object also looks the same as for other frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We also used stochastic gradient descent with a momentum algorithm. Notice that
    the optimizer object constructor takes the model parameters as the first argument.
    It’s a different approach from the `Dlib` and `mlpack` libraries, where the training
    process is mostly hidden in the top-level training API.
  prefs: []
  type: TYPE_NORMAL
- en: The approach where you pass model parameters to an optimizer is more common
    for frameworks where you configure the training process more precisely; you will
    see it in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having all base blocks initialized, we can implement a training loop. Such
    a loop will contain the following important steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction step—the forward propagation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss value calculation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradients calculation—the backpropagation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimization step where the gradient values will be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearing gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can implement these steps as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we made two loops, one over epochs and an internal one over batches,
    in our training dataset. In the inner loop, we used the `batch_dataset` variable;
    we assume that it has the `fl::BatchDataset` dataset type, so the `batch` loop
    variable is the `std::vector` of tensors. Usually, it will have only two tensors,
    for the input data and the target batch data.
  prefs: []
  type: TYPE_NORMAL
- en: We used the `fl::input` function to wrap our input batch tensor, `batch[0]`,
    into the Flashlight `Variable` type with disabled gradient calculations. The `Variable`
    type is used for the Flashlight auto-gradient mechanism. For the target batch
    data, `batch[1]`, we used the `fl::noGrad` function to disable gradient calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Our `model` object returns a prediction tensor with a 4D shape in the WHCN format.
    If didn’t reshape your dataset for a convenient shape, you will have to use the
    `fl::reshape` function for every batch as we did in this example; otherwise, you
    will get shape inconsistency errors in the loss value calculation.
  prefs: []
  type: TYPE_NORMAL
- en: After we calculated the loss value with the `loss` object using the predicted
    and target values, we calculated the gradient values. This was done with the `backward`
    method of the `loss_value` object, which has the `fl::Variable` type.
  prefs: []
  type: TYPE_NORMAL
- en: Having the gradient values calculated, we used the `step` method of the `sgd`
    object to apply the optimization step for the network parameters. Remember that
    we initialized the optimization `sgd` object with the model parameters (weights).
    For the final step, we called the `zeroGrad` method for the optimizer to clear
    the network parameter gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the network (model) is trained, it’s easy to use it for prediction, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`x` should be your input data. Disabling the gradient calculation for the model
    evaluation (prediction) stage is very important because it can save a lot of computational
    resources and increase overall model throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will implement a more complex neural network to solve
    an image classification task using the `PyTorch` library.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding image classification using the LeNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll implement a CNN for image classification. We are going
    to use the famous dataset of handwritten digits called the **Modified National
    Institute of Standards and Technology** (**MNIST**) dataset, which can be found
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/). The
    dataset is a standard that was proposed by the *US National Institute of Standards
    and Technology* to calibrate and compare image recognition methods using machine
    learning, primarily based on neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creators of the dataset used a set of samples from the US Census Bureau,
    with some samples written by students of American universities added later. All
    the samples are normalized, anti-aliased grayscale images of 28 x 28 pixels. The
    MNIST dataset contains 60,000 images for training and 10,000 images for testing.
    There are four files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train-images-idx3-ubyte`: Training set images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train-labels-idx1-ubyte`: Training set labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t10k-images-idx3-ubyte`: Test set images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t10k-labels-idx1-ubyte`: Test set labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The files that contain labels are in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Offset** | **Type** | **Value** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 0x00000801(2049) | Magic number (MSB first) |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 32-bit integer | 60,000 or 10,000 | Number of items |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Unsigned char | ?? | Label |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Unsigned char | ?? | Label |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – MNIST labels file format
  prefs: []
  type: TYPE_NORMAL
- en: 'The label values are from `0` to `9`. The files that contain images are in
    the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Offset** | **Type** | **Value** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 0x00000803(2051) | Magic number (MSB first) |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 60,000 or 10,000 | Number of images |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 28 | Number of rows |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 32-bit integer | 28 | Number of columns |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Unsigned byte | ?? | Pixel |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Unsigned byte | ?? | Pixel |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: Table 10.3 – MNIST image file format
  prefs: []
  type: TYPE_NORMAL
- en: Pixels are stored in a row-wise manner, with values in the range of [0, 255].
    `0` means background (white), while `255` means foreground (black).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using the PyTorch deep learning framework. This framework
    is primarily used with the Python language. However, its core part is written
    in C++, and it has a well-documented and actively developed C++ client API called
    **LibPyTorch**. This framework is based on the linear algebra library called **ATen**,
    which heavily uses the Nvidia CUDA technology for performance improvement. The
    Python and C++ APIs are pretty much the same but have different language notations,
    so we can use the official Python documentation to learn how to use the framework.
    This documentation also contains a section stating the differences between C++
    and Python APIs and specific articles about the usage of the C++ API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch framework is widely used for research in deep learning. As we discussed
    previously, the framework provides functionality for managing big datasets. It
    can automatically parallelize loading the data from a disk, manage pre-loaded
    buffers for the data to reduce memory usage, and limit expensive performance disk
    operations. It provides the `torch::data::Dataset` base class for the implementation
    of the user custom dataset. We only need to override two methods here: `get` and
    `size`. These methods are not virtual because we have to use the C++ template’s
    polymorphism to inherit from this class.'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the `MNISTDataset` class, which provides access to the MNIST dataset.
    The constructor of this class takes two parameters: one is the name of the file
    that contains the images, and the other is the name of the file that contains
    the labels. It loads whole files into its memory, which is not a best practice,
    but for this dataset, this approach works well because the dataset is small. For
    bigger datasets, we have to implement another scheme of reading data from the
    disk because usually, for real tasks, we are unable to load all the data into
    the computer’s memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the OpenCV library to deal with images, so we store all the loaded images
    in the C++ `vector` of the `cv::Mat` type. Labels are stored in a vector of the
    `unsigned char` type. We write two additional helper functions to read images
    and labels from the disk: `ReadImages` and `ReadLabels`. The following snippet
    shows the header file for this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following snippet shows the implementation of the public interface of the
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the constructor passed the filenames to the corresponding loader
    functions. The `size` method returns the number of items that were loaded from
    the disk into the `labels` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following snippet shows the `get` method’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get` method returns an object of the `torch::data::Example<>` class. In
    general, this type holds two values: the training sample represented with the
    `torch::Tensor` type and the target value, which is also represented with the
    `torch::Tensor` type. This method retrieves an image from the corresponding container
    using a given subscript, converts the image into the `torch::Tensor` type with
    the `CvImageToTensor` function, and uses the label value converted into the `torch::Tensor`
    type as a target value.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a set of `torch::tensor` functions that are used to convert a C++ variable
    into the `torch::Tensor` type. They automatically deduce the variable type and
    create a tensor with corresponding values. In our case, we explicitly convert
    the label into the `int64_t` type because the loss function we’ll be using later
    assumes that the target values have a `torch::Long` type. Also, notice that we
    passed `torch::TensorOptions` as a second argument to the `torch::tensor` function.
    We specified the `torch` type of the tensor values and told the system to place
    this tensor in the GPU memory by setting the `device` argument to be equal to
    the `torch::DeviceType::CUDA value` and by using the `torch::TensorOptions` object.
    When we manually create the PyTorch tensors, we have to explicitly configure where
    to place them—in the CPU or in the GPU. Tensors that are placed in different types
    of memory can’t be used together.
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert the OpenCV image into a tensor, write the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The most important part of this function is the call to the `torch::from_blob`
    function. This function constructs the tensor from values located in memory that
    are referenced by the pointer that’s passed as a first argument. A second argument
    should be a C++ vector with tensor dimension values; in our case, we specified
    a three-dimensional tensor with one channel and two image dimensions. The third
    argument is the `torch::TensorOptions` object. We specified that the data should
    be of the floating-point type and that it doesn’t require a gradient calculation.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch uses the auto-gradient approach for model training, and it means that
    it doesn’t construct a static network graph with pre-calculated gradient dependencies.
    Instead, it uses a dynamic network graph, which means that gradient flow paths
    for modules are connected and calculated dynamically during the backward pass
    of the training process. Such an architecture allows us to dynamically change
    the network’s topology and characteristics while running the program. All the
    libraries we covered previously use a static network graph.
  prefs: []
  type: TYPE_NORMAL
- en: The third interesting PyTorch function that’s used here is the `torch::Tensor::to`
    function, which allows us to move tensors from CPU memory to GPU memory and back.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to read dataset files.
  prefs: []
  type: TYPE_NORMAL
- en: Reading dataset files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We read the labels file with the `ReadLabels` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This function opens the file in binary mode and reads the header records, the
    magic number, and the number of items in the file. It also reads all the items
    directly to the C++ vector. The most important part is to correctly read the header
    records. To do this, we can use the `read_header` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This function reads the value from the input stream—in our case, the file stream—and
    flips the endianness. This function also assumes that header records are 32-bit
    integer values. In a different scenario, we would have to think of other ways
    to flip the endianness.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the image file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reading the images file is also pretty straightforward; we read the header records
    and sequentially read the images. From the header records, we get the total number
    of images in the file and the image size. Then, we define the OpenCV matrix object
    that has a corresponding size and type—the one-channel image with the underlying
    byte `CV_8UC1` type. We read images from the disk in a loop directly to the OpenCV
    matrix object by passing a pointer, which is returned by the `data` object variable,
    to the stream read function. The size of the data we need to read is determined
    by calling the `cv::Mat::size()` function, followed by the call to the `area`
    function. Then, we use the `convertTo` OpenCV function to convert an image from
    the `unsigned byte` type to the 32-bit floating-point type. This is important
    so that we have enough precision while performing math operations in the network
    layers. We also normalize all the data so that it’s in the range [0, 1] by dividing
    it by 255.
  prefs: []
  type: TYPE_NORMAL
- en: 'We resize all the images so that they’re 32 x 32 in size because the LeNet5
    network architecture requires us to hold the original dimensions of the convolution
    filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve loaded the training data, we have to define our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we chose the LeNet5 architecture, which was developed by Yann
    LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner ([http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)).
    The architecture’s details were discussed earlier in the *Convolution network
    architecture* section. Here, we’ll show you how to implement it with the PyTorch
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the structural parts of the neural networks in the PyTorch framework should
    be derived from the `torch::nn::Module` class. The following snippet shows the
    header file of the `LeNet5` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we defined the intermediate implementation class, which is called
    `LeNet5Impl`. This is because PyTorch uses a memory management model based on
    smart pointers, and all the modules should be wrapped in a special type. There
    is a special class called `torch::nn::ModuleHolder`, which is a wrapper around
    `std::shared_ptr` but also defines some additional methods for managing modules.
    So, if we want to follow all PyTorch conventions and use our module (network)
    with all PyTorch’s functions without any problems, our module class definition
    should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`Impl` is the implementation of our module, which is derived from the `torch::nn::Module`
    class. There is a special macro that can do this definition for us automatically;
    it is called `TORCH_MODULE`. We need to specify the name of our module in order
    to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: The most important function in this definition is the `forward` function. This
    function, in our case, takes the network’s input and passes it through all the
    network layers until an output value is returned from this function. If we don’t
    implement a whole network but rather *some* custom layers or *some* structural
    parts of a network, this function should assume we take the values from the previous
    layers or other parts of the network as input. Also, if we are implementing a
    custom module that isn’t from the PyTorch standard modules, we should define the
    `backward` function, which should calculate gradients for our custom operations.
  prefs: []
  type: TYPE_NORMAL
- en: The next essential thing in our module definition is the usage of the `torch::nn::Sequential`
    class. This class is used to group sequential layers in the network and automate
    the process of forwarding values between them. We broke our network into two parts,
    one containing convolutional layers and another containing the final fully connected
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch framework contains many functions for creating layers. For example,
    the `torch::nn::Conv2d` function created the two-dimensional convolutional layer.
    Another way to create a layer in PyTorch is to use the `torch::nn::Functional`
    function to wrap some simple function into the layer, which can then be connected
    with all the outputs of the previous layer. Notice that activation functions are
    not part of the neurons in PyTorch and should be connected as a separate layer.
    The following code snippet shows the definition of our network components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, we initialized two `torch::nn::Sequential` modules. They take a variable
    number of other modules as arguments for constructors. Notice that for the initialization
    of the `torch::nn::Conv2d` module, we have to pass the instance of the `torch::nn::Conv2dOptions`
    class, which can be initialized with the number of input channels, the number
    of output channels, and the kernel size. We used `torch::tanh` as an activation
    function; notice that it is wrapped in the `torch::nn::Functional` class instance.
  prefs: []
  type: TYPE_NORMAL
- en: The average pooling function is also wrapped in the `torch::nn::Functional`
    class instance because it is not a layer in the PyTorch C++ API; it’s a function.
    Also, the pooling function takes several arguments, so we bound their fixed values.
    When a function in PyTorch requires the values of the dimensions, it assumes that
    we provide an instance of the `torch::IntArrayRef` type. An object of this type
    behaves as a wrapper for an array with dimension values. We should be careful
    here because such an array should exist at the same time as the wrapper lifetime;
    notice that `torch::nn::Functional` stores `torch::IntArrayRef` objects internally.
    That is why we defined `k_size` and `p_size` as static global variables.
  prefs: []
  type: TYPE_NORMAL
- en: Also, pay attention to the `register_module` function. It associates the string
    name with the module and registers it in the internals of the parent module. If
    the module is registered in a certain way, we can use a string-based parameter
    search later (often used when we need to manually manage weight updates during
    training) and automatic module serialization.
  prefs: []
  type: TYPE_NORMAL
- en: The `torch::nn::Linear` module defines the fully connected layer and should
    be initialized with an instance of the `torch::nn::LinearOptions` type, which
    defines the number of inputs and the number of outputs, that is, a count of the
    layer’s neurons. Notice that the last layer returns 10 values, not one label,
    despite us only having a single target label. This is the standard approach in
    classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the `forward` function’s implementation, which performs
    model inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We passed the input tensor (image) to the `forward` function of the sequential
    convolutional group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we flattened its output with the `view` tensor method because fully connected
    layers assume that the input is flat. The `view` method takes the new dimensions
    for the tensor and returns a tensor view without exactly copying the data; *-1*
    means that we don’t care about the dimension’s value and that it can be flattened.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the flattened output from the convolutional group is passed to the fully
    connected group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we applied the softmax function to the final output. We’re unable to
    wrap `torch::log_softmax` in the `torch::nn::Functional` class instance because
    of multiple overrides.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The softmax function converts a vector, ![](img/B19849_Formula_093.png), of
    dimension ![](img/B19849_Formula_1512.png) into a vector, ![](img/B19849_Formula_291.png),
    of the same dimension, where each coordinate, ![](img/B19849_Formula_1531.png),
    of the resulting vector is represented by a real number in the range ![](img/B19849_Formula_154.png)
    and the sum of the coordinates is 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The coordinates are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_155.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The softmax function is used in machine learning for classification problems
    when the number of possible classes is more than two (for two classes, a logistic
    function is used). The coordinates, ![](img/B19849_Formula_1531.png), of the resulting
    vector can be interpreted as the probabilities that the object belongs to the
    class, ![](img/B19849_Formula_1571.png). We chose this function because its results
    can be directly used for the cross-entropy loss function, which measures the difference
    between two probability distributions. The target distribution can be directly
    calculated from the target label value—we create the 10 value’s vector of zeros
    and put one in the place indexed by the label value. Now, we have all the required
    components to train the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Network training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we should create PyTorch data loader objects for the train and test
    datasets. The data loader object is responsible for sampling objects from the
    dataset and making mini-batches from them. This object can be configured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the `MNISTDataset` type objects representing our datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we use the `torch::data::make_data_loader` function to create a data loader
    object. This function takes the `torch::data::DataLoaderOptions` type object with
    configuration settings for the data loader. We set the mini-batch size equal to
    256 items and set 8 parallel data loading threads. We should also configure the
    sampler type, but in this case, we’ll leave the default one—the random sampler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following snippet shows how to initialize the train and test data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we didn’t pass our dataset objects directly to the `torch::data::make_data_loader`
    function, but we applied the stacking transformation mapping to it. This transformation
    allows us to sample mini-batches in the form of the `torch::Tensor` object. If
    we skip this transformation, the mini-batches will be sampled as the C++ vector
    of tensors. Usually, this isn’t very useful because we can’t apply linear algebra
    operations to the whole batch in a vectorized manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to initialize the neural network object of the `LeNet5` type,
    which we defined previously. We’ll move it to the GPU to improve training and
    evaluation performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: When the model of our neural network has been initialized, we can initialize
    an optimizer. We chose stochastic gradient descent with momentum optimization
    for this. It is implemented in the `torch::optim::SGD` class. The object of this
    class should be initialized with model (network) parameters and the `torch::optim::SGDOptions`
    type object. All `torch::nn::Module` type objects have the `parameters()` method,
    which returns the `std::vector<Tensor>` object containing all the parameters (weights)
    of the network. There is also the `named_parameters` method, which returns the
    dictionary of named parameters. Parameter names are created with the names we
    used in the `register_module` function call. This method is handy if we want to
    filter parameters and exclude some of them from the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torch::optim::SGDOptions` object can be configured with the values of
    the learning rate, the weight decay regularization factor, and the momentum value
    factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our initialized data loaders, the `network` object, and the
    `optimizer` object, we are ready to start the training cycle. The following snippet
    shows the training cycle’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve made a loop that repeats the training cycle for 100 epochs. At the beginning
    of the training cycle, we switched our network object to training mode with `model->train()`.
    For one epoch, we iterate over all the mini-batches provided by the data loader
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'For every mini-batch, we did the next training steps, cleared the previous
    gradient values by calling the `zero_grad` method for the optimizer object, made
    a forward step over the network object, `model->forward(batch.data)`, and computed
    the loss value with the `nll_loss` function. This function computes the *negative
    log-likelihood* loss. It takes two parameters: the vector containing the probability
    that a training sample belongs to a class identified by position in the vector
    and the numeric class label (number). Then, we called the `backward` method of
    the loss tensor. It recursively computes the gradients for the overall network.
    Finally, we called the `step` method for the optimizer object, which updated all
    the parameters (weights) and their corresponding gradient values. The `step` method
    only updated the parameters that were used for initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s common practice to use test or validation data to check the training process
    after each epoch. We can do this in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: First, we switched the model to evaluation mode by calling the `eval` method.
    Then we iterated over all the batches from the test data loader. For each of these
    batches, we performed a forward pass over the network, calculating the loss value
    in the same way that we did for our training process. To estimate the total loss
    (error) value for the model, we averaged the loss values for all the batches.
    To get the total loss for the batch, we used `loss.sum().item<float>()`. Here,
    we summarized the losses for each training sample in the batch and moved it to
    the CPU floating-point variable with the `item<float>()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we calculate the accuracy value. This is the ratio between correct answers
    and misclassified ones. Let’s go through this calculation with the following approach.
    First, we determine the predicted class labels by using the `max` method of the
    tensor object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `max` method returns a tuple, where the values are the maximum value of
    each row of the input tensor in the given dimension and the location indices of
    each maximum value the method found. Then, we compare the predicted labels with
    the target ones and calculate the number of correct answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We used the `eq` tensor’s method for our comparison. This method returns a boolean
    vector whose size is equal to the input vector, with values equal to `1` where
    the vector element components are equal and with values equal to `0` where they’re
    not. To perform the comparison operation, we made a view for the target labels
    tensor with the same dimensions as the predictions tensor. The `view_as` method
    is used for this comparison. Then, we calculated the sum of `1` values and moved
    the value to the CPU variable with the `item<long>()` method.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, we can see that the specialized framework has more options we
    can configure and is more flexible for neural network development. It has more
    layer types and supports dynamic network graphs. It also has a powerful specialized
    linear algebra library that can be used to create new layers, as well as new loss
    and activation functions. It has powerful abstractions that enable us to work
    with big training data. One more important thing to note is that it has a C++
    API very similar to the Python API, so we can easily port Python programs to C++
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at what artificial neural networks are, looked at
    their history, and examined the reasons for their appearance, rise, and fall and
    why they have become one of the most actively developed machine learning approaches
    today. We looked at the difference between biological and artificial neurons before
    learning the basics of the perceptron concept, which was created by Frank Rosenblatt.
    Then, we discussed the internal features of artificial neurons and networks, such
    as activation functions and their characteristics, network topology, and convolution
    layer concepts. We also learned how to train artificial neural networks with the
    error backpropagation method. We saw how to choose the right loss function for
    different types of tasks. Then, we discussed the regularization methods that are
    used to combat overfitting during training.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented a simple MLP for a regression task with the mlpack,
    Dlib, and Flashlight C++ machine learning libraries. Then, we implemented a more
    advanced convolution network for an image classification task with PyTorch, a
    specialized neural network framework. This showed us the benefits of specialized
    frameworks over general-purpose libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to use pre-trained **large language
    models** (**LLMs**) and adapt them to our particular tasks. We will see how to
    use the transfer learning technique and the BERT network to perform sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Loss Functions for Deep Neural Networks in* *Classification*: [https://arxiv.org/pdf/1702.05659.pdf](https://arxiv.org/pdf/1702.05659.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Networks and Deep Learning*, by Michael Nielsen: [http://neuralnetwork
    sanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Principles of Neurodynamics*, Rosenblatt, Frank (1962), Washington, DC: Spartan
    Books'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Perceptrons*, Minsky M. L. and Papert S. A. 1969\. Cambridge, MA: MIT Press'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Networks and Learning Machines*, Simon O. Haykin 2008'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning*, Ian Goodfellow, Yoshua Bengio, Aaron Courville 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PyTorch GitHub page: [https://github.com/pytorch/](https://github.com/pytorch/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PyTorch documentation site: [https://pytorch.org/docs/](https://pytorch.org/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The LibPyTorch (C++) documentation site: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
