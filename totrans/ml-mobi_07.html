<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Spam Message Detection</h1>
                </header>
            
            <article>
                
<p>This chapter will provide you with an overview of <span><strong>natural language processing</strong> (</span><strong>NLP</strong>) and discuss how NLP can be combined with machine learning to provide solutions to problems. Then, the chapter will take a real-<span><span>world</span></span><span> use case of doing</span> <span>s</span>pam <span>message detection by utilizing NLP, combined with the linear SVM classification model. The program will be implemented as a mobile application using Core ML for iOS.</span></p>
<p><span>To handle text in machine learning algorithms, we will go through the various NLP techniques that will be used on the text data to make it ready for learning algorithms. Once the text is prepared, we will see how we can classify it using the linear SVM model. </span><br/></p>
<p><strong>Problem definition</strong>: The bulk SMS message data is provided, and these messages need to be classified as spam or non-spam messages.</p>
<p>We will be covering<span> </span>the following topics in this chapter:</p>
<ul>
<li>Understanding NLP</li>
<li>Understanding the linear SVM algorithm</li>
<li>Solving the problem using linear SVM in Core ML:
<ul>
<li>Technical requirements</li>
<li>How to create the model file using scikit-learn </li>
<li>Testing the model</li>
<li>Importing the scikit-learn model into the Core ML project</li>
<li>Writing an iOS mobile application, using the scikit-learn model in it, and doing spam message detection</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding NLP</h1>
                </header>
            
            <article>
                
<p><span>NLP </span>is a huge topic, and it is beyond the scope of this book to go into detail on the subject. However, in this section, we will go through the high-level details of NLP and try to understand the key concepts required to prepare and process the textual data using NLP, in order to make it ready for consumption by machine learning algorithms for prediction. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing NLP</h1>
                </header>
            
            <article>
                
<p>Huge, unstructured textual data is getting generated on a daily basis. Social media, websites such as Twitter and Facebook, and communication apps, such as WhatsApp,<span> </span><span>generate an enormous volume of this unstructured data daily—not to mention the volume created by </span>blogs, news articles, product reviews, service reviews, advertisements, emails, and SMS. So, to summarize, there is<span> </span><strong>huge data </strong>(in TBS).</p>
<p>However, it is not possible for a computer to get any insight from this data and to carry out specific actions based on the insights, directly from this huge data, because of the following reasons:</p>
<ul>
<li>The data is unstructured</li>
<li>The data cannot be understood directly without preprocessing</li>
<li>This data cannot be directly fed in an unprocessed form into any ML algorithms</li>
</ul>
<p>To make this data more meaningful and to derive information from it, we use NLP. The field of study that focuses on the interactions between human language and computers is called <strong>NLP</strong>. NLP is a branch of data science that is closely related to computational linguistics. It deals with the science of the computer <span>–</span> analyzing, understanding, and deriving information from human natural language-based data, which is usually unstructured like text, speech, and so on.</p>
<p><span>Through NLP, computers can analyze and derive meaning from human language and do many useful things. By utilizing NLP, many complex tasks, such as an automatic summary of huge documents, translations, relationship extraction between a different mass of unstructured data, sentiment analysis, and speech recognition, can be accomplished.</span></p>
<p>For computers to understand and analyze human language, we need to analyze the sentence in a more structured manner and understand the core of it. In any sentence, we need to understand three core things:</p>
<ul>
<li><strong>Semantic information</strong>: This relates to the meaning of the sentence. This is the specific meaning of the words in the sentence, for example, <em>The k</em><em>ite flies</em>. Here, we don't know whether the kite is man-made or a bird.</li>
<li><strong>Syntactic information</strong>: This relates to the structure of the sentence. This is the specific syntactic meaning of the words in a sentence. <em>Sreeja saw Geetha with candy</em>. Here, we are not sure who has the candy: Sreeja or Geetha?</li>
<li><strong>Pragmatic information (context)</strong>: This relates to the context (linguistic or non-linguistic) of the sentence. This is the specific context in which the words in the sentence are used. For example, <em>He is out</em> in the context of baseball and healthcare is different.</li>
</ul>
<p>However, computers cannot analyze and recognize sentences as humans do. Therefore, there is a well-defined way to enable computers to perform text processing. Here are the main steps involved in that exercise:</p>
<ol>
<li><strong>Preprocessing</strong>: This step deals with removing all the noise from the sentence, so the only information critical in the context of the sentence is retained for the next step. For example, language stop words ("noise"), such as <em>is</em>, <em>the</em>, or <em>an</em>, can be removed from the sentence for further processing. When processing the sentence, the human brain doesn't take into consideration the noise that's present in the language. Similarly, the computer can be fed with noiseless text for further processing. </li>
<li><strong>Feature engineering</strong>: For the computer to process the preprocessed text, it needs to know the key features of the sentence. This is what is accomplished through the feature engineering step.</li>
<li><strong>NLP processing</strong>: With the human language converted into a feature matrix, the computer can perform NLP processing, which could either be classification, sentiment analysis, or text matching. </li>
</ol>
<p>Now, let's try to understand the high-level activities that would be performed in each of these steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text-preprocessing techniques</h1>
                </header>
            
            <article>
                
<p>Before we can process text, it needs to be preprocessed. Preprocessing would deal with the following:</p>
<ul>
<li>Removing noise from the text under consideration</li>
<li>Normalizing the sentence</li>
<li>Standardizing the sentence</li>
</ul>
<p>There can be additional steps, such as a grammar check or spellcheck, based on the requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing noise</h1>
                </header>
            
            <article>
                
<p>Any text present in the sentence that may not be relevant to the context of the data can be termed noise.</p>
<p>For example, this can include language stop words (commonly used words in a language <span>–</span> <em>is</em>, <em>am</em>, <em>the</em>, <em>of</em>, and <em>in</em>), URLs or links, social media entities (mentions, hashtags), and punctuation.</p>
<p>To remove the noise from the sentence, the general approach is to maintain a dictionary of noise words and then iterate through the tokens of the sentence under consideration against this dictionary and remove matching stop words. The dictionary of noise words is updated frequently to cover all possible noise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalization</h1>
                </header>
            
            <article>
                
<p>The disparities of words in sentences are converted into a normalized form. The words in a sentence may vary, such as <em>sing</em>, <em>singer</em>, <em>sang</em>, or <em>singing</em>, but they all would more or less fit into the same context and could be standardized.</p>
<p>There are different ways to normalize sentences:</p>
<ul>
<li><strong>Stemming: </strong>A basic rule-based process of stripping the suffixes (<em>-ing</em>, <em>-ly</em>, <em>-es</em>, <em>-s</em>) from a word. </li>
<li><strong>Lemmatization: </strong>The more sophisticated procedure to identify the root form of a word. It involves a more complex process of verifying the semantics and syntax.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standardization</h1>
                </header>
            
            <article>
                
<p>This step involves standardizing the sentence to make sure it contains tokens that are from the standard language dictionary only and not anything else, such as hashtags, colloquial words, and so on. All these are removed in this step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering</h1>
                </header>
            
            <article>
                
<p>Now that the text has been processed, the next step to arrange the features from the text so that they can be fed into any machine learning algorithm to carry out classification, clustering, and so on. There are various methods to convert the text into a feature matrix, and we will go through some of them in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Entity extraction</h1>
                </header>
            
            <article>
                
<p>Here, the key entities from the sentence that would be used for NLP processing are extracted. <strong>Named entity recognition</strong> (<strong>NER</strong>)<span> </span>is one such method, where the entities could be named entities, such as that of a place, person, or monument.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Topic modeling</h1>
                </header>
            
            <article>
                
<p>This is another method, where the topics are identified from the corpus of text. The topics can be single words, patterns of words, or sequences of co-occurring words. Based on a number of words in the topic, these could be called<span> <strong>N-Gram.</strong> So, based on context and repeatability, bigrams and trigrams could be used as features.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bag-of-words model</h1>
                </header>
            
            <article>
                
<p>A bag-of-words model is a representation of text that describes the occurrence of words within a document. It involves the representation of known words and a measure of the presence of known words in the document. The model is more centered around the occurrence of known words in the document, and not about the order of words or the structure of words in the document.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical Engineering</h1>
                </header>
            
            <article>
                
<p>Text data can also be represented as numerical values using various techniques. <strong>Term Frequency-Inverse<span> </span>Document Frequency</strong><span> (<strong>TF-IDF</strong>) </span>for a huge corpus of text documents is an important technique in this class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TF–IDF</h1>
                </header>
            
            <article>
                
<p>TF-IDF is a weighted model  that's used to convert the text documents into vector models on the basis of the occurrence of words in the documents without considering the exact ordering of text in the document.</p>
<p>Let's consider a set of N text documents and any one document to be D. Then, we define the following.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TF</h1>
                </header>
            
            <article>
                
<p><span>This measures how frequently a term occurs in a document. Since every document is a different length, it is possible that a term would appear more in long documents than shorter ones. Thus, the TF is often divided by the document length to normalize it:</span><br/>
<span><em>TF(t) = (Number of times term t appears in a document(D))/(Total number of terms in the document(N))</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inverse Document Frequency (IDF)</h1>
                </header>
            
            <article>
                
<p><span>This measures how important a term is for the corpus. While computing TF, all terms are considered equally important. However, it is common thinking that stop words occur more often, but they are less important as far as NLP is concerned. Thus, there is a need to bring down the importance of common terms and bring up the importance of rare terms, hence the IDF, which is calculated as follows:</span></p>
<p><em><span>IDF(t) = log_e(Total number of documents/Number of documents with term t in it)</span></em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TF-IDF</h1>
                </header>
            
            <article>
                
<p>The TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2f33724e-a938-4bcc-a947-1a91644caded.png" style="width:14.83em;height:3.75em;"/></p>
<p>Where:</p>
<ul>
<li><em>tf</em><sub><em>i,j</em></sub> = number of occurence of <em>i</em> in <em>j</em></li>
<li><em>df<sub>i</sub></em> = number of documents containing <em>i</em></li>
<li><em>N</em> = total number of document</li>
</ul>
<div class="packt_tip"><span>Consider a document that contains 1,000 words, wherein the word </span>rat <span>appears 3 times. The <strong>term frequency</strong> (<strong>TF</strong>) for </span>rat<span> is then (3/1000=) 0.003. Now, in 10,000 documents, the word </span>cat<span> appears in 1,000 of them. Therefore, the <strong>inverse document frequency</strong> (<strong>IDF</strong>) is calculated as log(10000/1000) = </span><span>1. Thus, the TF-IDF weight is the product of these quantities is 0.003 * 1 = 0.12.</span></div>
<p>The words or features in the text corpus could also be organized as feature vectors for easy feeding into the next step of NLP processing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying/clustering the text</h1>
                </header>
            
            <article>
                
<p>The last step is to actually carry out classification or clustering using the feature engineered matrix or word vectors. We could use any classification algorithm and feed the feature vector to carry out classification or clustering.</p>
<p>Similar to carrying out the clustering, different similarity measures could be used, such as Cosine Distance or Levenshtein distance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding linear SVM algorithm</h1>
                </header>
            
            <article>
                
<p><span>In</span> <a href="1b52495b-c6cb-4197-8fcd-a1e764c1f1c2.xhtml" target="_blank">Chapter 2</a><span>, <em>Supervised and Unsupervised Learning Algorithms</em>, we covered the SVM algorithm and now have an idea of how the SVM model works. A </span>linear support vector machine<span> or </span>linear SVM<span> is a </span>linear classifier<span> that tries to </span><span>find a hyperplane with the largest margin that splits the input space into two regions. </span></p>
<div class="packt_tip">A hyperplane is a generalization of a plane. <span>In one dimension, a hyperplane is called a <strong>point</strong>. I</span><span>n two dimensions, it is a line. I</span><span>n three dimensions, it is a plane. I</span><span>n more dimensions, you can call it a hyperplane.</span></div>
<p>As we saw, the goal of SVM is to identify the hyperplane that tries to find the largest margin that splits the input space into two regions. If the input space is linearly separable, it is easy to separate them. However, in real life, we find that the input space is very non-linear:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-949 image-border" src="assets/53a98c4a-8f3d-4517-9e7e-c004a1a91794.png" style="width:12.50em;height:11.17em;"/></p>
<p>In the preceding scenario, the SVM can help us separate the red and blue balls by using what is called a<span> </span><strong>Kernel Trick</strong>, which is the <span>method of using a linear classifier to solve a non-linear problem.</span></p>
<p><span>The kernel function is applied to each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable.</span></p>
<p>The most popular kernel functions available are as follows:</p>
<ul>
<li>The linear kernel</li>
<li>The polynomial kernel</li>
<li>The RBF (Gaussian) kernel</li>
<li>The string kernel</li>
</ul>
<p>The linear kernel is often recommended for text classification, as most text classification problems need to be categorized into two classes. In our example, we also want to classify the SMS messages into spam and non-spam.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Solving the problem using linear SVM in Core ML</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to look at how we can solve the spam message detection problem using all the concepts we have gone through in this chapter.</p>
<p>We are going to take a bunch of SMS messages and attempt to classify them as spam or non-spam. This is a classification problem and we will use the linear SVM algorithm to perform this, considering the advantages of using this algorithm for text classification.</p>
<p>We are going to use NLP techniques to convert the data-SMS messages into a feature vector to feed into the linear SVM algorithm. We are going to use the scikit-learn vectorizer methods to transform the SMS messages into the TF-IDF vector, which could be fed into the linear SVM model to perform SMS spam detection (classification into spam and non-spam).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">About the data</h1>
                </header>
            
            <article>
                
<p>The data that we are using to create the model that detects the spam messages is taken from<span> </span><a href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/">http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/</a>,<span> </span>which contains 747 spam message samples, along with 4,827 non-spam messages.</p>
<p>These messages are taken from different sources and labeled with the category of spam and non-spam. If you open the downloaded file in Notepad or any text editor, it will be in the following format:</p>
<pre><span>ham   What you doing?how are you?<br/></span>ham   Ok lar... Joking wif u oni...<br/> ham   dun say so early hor... U c already then say...<br/> ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*<br/> ham   Siva is in hostel aha:-.<br/> ham   Cos i was out shopping with darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.<br/> spam  FreeMsg: Txt: CALL to No: 86888 &amp; claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop<br/> spam  Sunshine Quiz! Win a super Sony DVD recorder if you can name the capital of Australia? Text MQUIZ to 82277. B<br/> spam  URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU</pre>
<p>In the preceding sample, we can see that every line starts with the category name and is followed by the actual message.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To create a model to classify a message as spam or non-spam, we need a library that is capable of doing so. Here, we've selected scikit-Learn.</p>
<p>To write this application, you need to have the Python3+ version installed on your desktop, and Xcode 9+ must be installed on your Mac machine. If you don't have either of these, please check the appendix of this book to learn how to get them. Once you have installed Python in your machine, execute the following commands to get the required packages:</p>
<pre><strong>pip install scikit-learn <br/></strong><strong>pip install numpy<br/></strong><strong>pip install coremltools<br/></strong><strong>pip install pandas</strong></pre>
<p>Using the preceding code, we installed scikit-learn to get access to the algorithms and NumPy as the scikit-learn requires it, and pandas (<em>pandas</em><span> is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming) </span>to read the model from the file and core-ML tools to generate a Core ML model file.</p>
<p>Now, download <kbd>SMSSpamCollection.txt</kbd>, a plain text file from the model link stated in the <span><span>preceding </span></span>section, onto your disk and put it in your <kbd>project</kbd> folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the Model file using Scikit Learn </h1>
                </header>
            
            <article>
                
<p>In your project folder, create a python file with the following code to create a model file:</p>
<pre># importing required packages<br/>import numpy as np<br/>import pandas as pd</pre>
<pre># Reading in and parsing data<br/>raw_data = open('<strong>SMSSpamCollection.txt</strong>', 'r')<br/>sms_data = []<br/>for line in raw_data:<br/>    split_line = line.split("\t")<br/>    sms_data.append(split_line)<br/><br/>#Splitting data into messages and labels and training and test in y we are having labels and x with the message text<br/><br/>sms_data = np.array(sms_data)<br/>X = sms_data[:, 1]<br/>y = sms_data[:, 0]<br/><br/>#Build a LinearSVC model<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.svm import LinearSVC<br/><br/>#Build tf-idf vector representation of data<br/>vectorizer = TfidfVectorizer()<br/><br/># converting the message text as vector<br/>vectorized_text = vectorizer.fit_transform(X)<br/><br/>text_clf = LinearSVC()<br/># fitting the model<br/>text_clf = text_clf.fit(vectorized_text, y)</pre>
<p>Test the fitted model, we can append the following code:</p>
<pre>print text_clf.predict(vectorizer.transform(["""XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"""]))</pre>
<div class="packt_quote packt_infobox">Upon executing the preceding program, it will show you whether the given message is spam or non-spam.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting the scikit-learn model into the Core ML model</h1>
                </header>
            
            <article>
                
<p>In the preceding section, we created our model to classify the messages as <span>spam </span>and non-<span>spam</span>. Now, let's convert that into the Core ML model so that we can use that in an IOS app.</p>
<p>To create a core-ML model, append the following lines to the preceding code and run them. This will create a <kbd>.mlmodel</kbd> file:</p>
<pre># importing the library<br/>import coremltools<br/><br/># convert to fitted model in to coreml model<br/>coreml_model = coremltools.converters.sklearn.convert(text_clf, "message", "spam_or_not")<br/><br/>#set parameters of the model<br/>coreml_model.short_description = "Classify whether message is spam or not"<br/>coreml_model.input_description["message"] = "TFIDF of message to be classified"<br/>coreml_model.output_description["spam_or_not"] = "Whether message is spam or not"<br/><br/>#save the model<br/>coreml_model.save("SpamMessageClassifier.mlmodel")</pre>
<p>Now, you can take the generated <kbd>SpamMessageClassifier.mlmodel</kbd> file and use this in your Xcode.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the iOS application</h1>
                </header>
            
            <article>
                
<p>You can get the code for the iOS project in our GitHub repository (<a href="https://github.com/PacktPublishing/Machine-Learning-for-Mobile" target="_blank">https://github.com/PacktPublishing/Machine-Learning-for-Mobile</a>). Once you download the project and open the project in Xcode, you will find the directory structure:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/130e69e7-19e1-4d52-baf7-dd837444d45e.png" style="width:19.58em;height:19.33em;"/></p>
<p>In this, I want to explain the important files to you. Main. Storyboard is having the UI design for the app:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-965 image-border" src="assets/d7fdf840-e9a4-4f6d-84c3-ac32757e3fc9.png" style="width:50.42em;height:43.58em;"/></p>
<p>Here, we have two labels, one button, and one text box. The two labels are a heading label and on result label. Button to submit the input and get the result. And we have a textbox to give a message as input. Here, the main processing is written in the <kbd>controller.swift</kbd> view:</p>
<pre>//common imports<br/>import UIKit<br/>import CoreML<br/><br/>class ViewController: UIViewController {<br/>    //binding to the UI elements<br/>    @IBOutlet weak var messageTextField: UITextField!<br/>    @IBOutlet weak var messageLabel: UILabel!<br/>    @IBOutlet weak var spamLabel: UILabel!<br/><br/>// This function will take the text from the user input and convert it in to a vector format which our model requires using the wordslist.txt file and the SMSSpamCollection.txt file that we have downloaded.<br/>    func tfidf(sms: String) -&gt; MLMultiArray{<br/>        //get path for files<br/>        let wordsFile = Bundle.main.path(forResource: "wordlist", ofType: "txt")<br/>        let smsFile = Bundle.main.path(forResource: "SMSSpamCollection", ofType: "txt")<br/>        do {<br/>            //read words file<br/>            let wordsFileText = try String(contentsOfFile: wordsFile!, encoding: String.Encoding.utf8)<br/>            var wordsData = wordsFileText.components(separatedBy: .newlines)<br/>            wordsData.removeLast() // Trailing newline.<br/>            //read spam collection file<br/>            let smsFileText = try String(contentsOfFile: smsFile!, encoding: String.Encoding.utf8)<br/>            var smsData = smsFileText.components(separatedBy: .newlines)<br/>            smsData.removeLast() // Trailing newline.<br/>            let wordsInMessage = sms.split(separator: " ")<br/>            //create a multi-dimensional array<br/>            let vectorized = try MLMultiArray(shape: [NSNumber(integerLiteral: wordsData.count)], dataType: MLMultiArrayDataType.double)<br/>            for i in 0..&lt;wordsData.count{<br/>                let word = wordsData[i]<br/>                if sms.contains(word){<br/>                    var wordCount = 0<br/>                    for substr in wordsInMessage{<br/>                        if substr.elementsEqual(word){<br/>                            wordCount += 1<br/>                        }<br/>                    }<br/>                    let tf = Double(wordCount) / Double(wordsInMessage.count)<br/>                    var docCount = 0<br/>                    for sms in smsData{<br/>                        if sms.contains(word) {<br/>                            docCount += 1<br/>                        }<br/>                    }<br/>                    let idf = log(Double(smsData.count) / Double(docCount))<br/>                    vectorized[i] = NSNumber(value: tf * idf)<br/>                } else {<br/>                    vectorized[i] = 0.0<br/>                }<br/>            }<br/>            return vectorized<br/>        } catch {<br/>            return MLMultiArray()<br/>        }<br/>    }<br/>    override func viewDidLoad() {<br/>        super.viewDidLoad()<br/>        // Do any additional setup after loading the view, typically from a nib.<br/>    }<br/>//This function will call when you click the predict button<br/>    @IBAction func predictSpam(_ sender: UIButton) {<br/>        let enteredMessage =  messageTextField.text!<br/>// checking and handling empty message.<br/>        if (enteredMessage != ""){<br/>            spamLabel.text = ""<br/>        }<br/>// Calling the preceding function to convert the text to vector<br/>        let vec = tfidf(sms: enteredMessage)<br/>        do {<br/>// Passing input to the our model to get the prediction results.<br/>            let prediction = try SpamMessageClassifier().prediction(message: vec).spam_or_not<br/>            print (prediction)<br/>            if (prediction == "spam"){<br/>                spamLabel.text = "SPAM!"<br/>            }<br/><br/>// Our model is having ham as label for not spam messages so our model will send the label as ham. Hence we are converting to Not Spam for displaying purpose<br/>           else if(prediction == "ham"){<br/>                spamLabel.text = "NOT SPAM"<br/>            }<br/>        }<br/>        catch{<br/>                // catching the exception<br/>            spamLabel.text = "No Prediction"<br/>        }<br/>    }<br/>}</pre>
<p>When you run the app in the simulator of Xcode, it will generate the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-966 image-border" src="assets/0d8d52c5-36b8-456b-8ee5-9dab19a0155f.png" style="width:21.50em;height:40.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went through many things, such as, understanding NLP at a high level. There are<span> various steps involved in NLP, such as<span> t</span></span><span>ext preprocessing, as well as techniques to carry this out, such as f</span><span>eature engineering and methods to perform feature engineering and c</span><span>lassification or clustering of the feature vectors. We also looked into</span><span> the linear SVM algorithm in which we</span><span> went through the details of the SVM algorithm, the kernel function, and how</span> it <span>is more applicable to text classification.</span></p>
<p>We solved our problem using linear SVM in Core ML and<span> we also saw a practical example of performing spam message detection using the linear SVM algorithm model that we developed in scikit learn</span> and converted into a Core ML model<span>. We wrote an iOS application using the converted Core ML model.</span></p>
<p>In the next chapter, we will be introduced to another ML framework, Fritz, which tries to solve the common problems that we see in model deployment and upgrades, and the unification of handling ML models across mobile OS platforms.</p>


            </article>

            
        </section>
    </body></html>