["```py\nMode                 LastWriteTime                     Name\n------        --------------------        -----------------\nd----l        2023-07-01     10:25        checkpoint-340000\nd----l        2023-07-01     10:25        checkpoint-350000\n-a---l        2023-06-27     21:30        config.json\n-a---l        2023-06-27     17:55        merges.txt\n-a---l        2023-06-27     21:30        pytorch_model.bin\n-a---l        2023-06-27     21:30        training_args.bin\n-a---l        2023-06-27     17:55        vocab.json\n```", "```py\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='mstaron/wolfBERTa')\nunmasker(\"Hello I'm a <mask> model.\")\n```", "```py\nfrom transformers import pipeline\n# Load the text classification pipeline\nclassifier = pipeline(\"text-classification\")\n# Classify a sample text\nresult = classifier(\"This movie is amazing and highly recommended!\")\nprint(result)\n```", "```py\nfrom transformers import pipeline\n# Load the text generation pipeline\ngenerator = pipeline(\"text-generation\")\n# Generate text based on a prompt\nprompt = \"In a galaxy far, far away… \"\nresult = generator(prompt, max_length=50, num_return_sequences=3)\nfor output in result:\n    print(output['generated_text'])\n```", "```py\nfrom transformers import pipeline\n# Load the summarization pipeline\nsummarizer = pipeline(\"summarization\")\n# Summarize a long article\narticle = \"\"\"\nIn a groundbreaking discovery, scientists have found a new species of dinosaur in South America. The dinosaur, named \"Titanus maximus,\" is estimated to have been the largest terrestrial creature to ever walk the Earth. It belonged to the sauropod group of dinosaurs, known for their long necks and tails. The discovery sheds new light on the diversity of dinosaurs that once inhabited our planet.\n\"\"\"\nresult = summarizer(article, max_length=100, min_length=30, do_sample=False)\nprint(result[0]['summary_text'])\n```", "```py\nfrom transformers import pipeline\n# first, create an instance of the image classification pipeline for the selected model\nclassifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n# now, use the pipeline to classify an image\nclassifier(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n```", "```py\nfrom transformers import pipeline\nsegmenter = pipeline(model=\"facebook/detr-resnet-50-panoptic\")\nsegments = segmenter(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\nsegments[0][\"label\"]\n```", "```py\nfrom transformers import pipeline\ndetector = pipeline(model=\"facebook/detr-resnet-50\")\ndetector(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n```", "```py\n>> pip install pytest\n```", "```py\n# import json to be able to read the embedding vector for the test\nimport json\n# import the model via the huggingface library\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n# load the tokenizer and the model for the pretrained SingBERTa\ntokenizer = AutoTokenizer.from_pretrained('mstaron/SingBERTa')\n# load the model\nmodel = AutoModelForMaskedLM.from_pretrained(\"mstaron/SingBERTa\")\n# import the feature extraction pipeline\nfrom transformers import pipeline\n# create the pipeline, which will extract the embedding vectors\n# the models are already pre-defined, so we do not need to train anything here\nfeatures = pipeline(\n    \"feature-extraction\",\n    model=model,\n    tokenizer=tokenizer,\n    return_tensor = False\n)\n```", "```py\ndef test_features():\n    # get the embeddings of the word \"Test\"\n    lstFeatures = features(\"Test\")\n    # read the oracle from the json file\n    with open('test.json', 'r') as f:\n        lstEmbeddings = json.load(f)\n    # assert the embeddings and the oracle are the same\n    assert lstFeatures[0][0] == lstEmbeddings\n```", "```py\n>> pytest\n```", "```py\n=================== test session starts ===================\nplatform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.2.0\nrootdir: C:\\machine_learning_best_practices\\chapter_12\nplugins: anyio-3.7.0\ncollected 1 item\nchapter_12_download_model_test.py .                 [100%]\n====================== 1 passed in 4.17s ==================\n```", "```py\n# import the libraries pandas and joblib\nimport pandas as pd\nimport joblib\n# load the model\nmodel = joblib.load('./chapter_12_decision_tree_model.joblib')\n# load the data that we used for training\ndfDataAnt13 = pd.read_excel('./chapter_12.xlsx',\n                            sheet_name='ant_1_3',\n                            index_col=0)\n```", "```py\n# test that the model is not null\n# which means that it actually exists\ndef test_model_not_null():\n    assert model is not None\n# test that the model predicts class 1 correctly\n# here correctly means that it predicts the same way as when it was trained\ndef test_model_predicts_class_correctly():\n    X = dfDataAnt13.drop(['Defect'], axis=1)\n    assert model.predict(X)[0] == 1\n# test that the model predicts class 0 correctly\n# here correctly means that it predicts the same way as when it was trained\ndef test_model_predicts_class_0_correctly():\n    X = dfDataAnt13.drop(['Defect'], axis=1)\n    assert model.predict(X)[1] == 0\n```", "```py\n=============== test session starts =======================\nplatform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.2.0\nrootdir: C:\\machine_learning_best_practices\\chapter_12\nplugins: anyio-3.7.0\ncollected 4 items\nchapter_12_classical_ml_test.py                      [ 75%]\nchapter_12_download_model_test.py                    [100%]\n================ 4 passed in 12.76s =======================\n```"]