<html><head></head><body>
<div id="_idContainer103">
<h1 class="chapter-number" id="_idParaDest-177"><a id="_idTextAnchor286"/><span class="koboSpan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-178"><a id="_idTextAnchor287"/><span class="koboSpan" id="kobo.2.1">Versioning and Reproducible Machine Learning Modeling</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Reproducibility is an important topic to help machine learning developers go back to different stages of the machine learning life cycle and identify opportunities for model improvement. </span><span class="koboSpan" id="kobo.3.2">Having access to different versions of the data and models generated through the machine learning life cycles could help us in improving the reproducibility of </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">our projects.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, you will learn about the meaning and importance of reproducibility in machine learning modeling. </span><span class="koboSpan" id="kobo.5.2">You will learn about tools for incorporating data versioning in machine learning pipelines to help you attain more effective collaboration in your projects and achieve reproducibility in your models. </span><span class="koboSpan" id="kobo.5.3">You will also learn about different aspects of model versioning and tools for incorporating it into </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">your pipelines.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Reproducibility in </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">machine learning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.11.1">Data versioning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.12.1">Model versioning</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.13.1">By the end of this chapter, you’ll have learned how to use data and model versioning for your modeling projects in Python to </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">achieve reproducibility.</span></span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor288"/><span class="koboSpan" id="kobo.15.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.16.1">The following are the requirements for this chapter and will help you better understand the concepts, use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.18.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.20.1">pandas</span></strong><span class="koboSpan" id="kobo.21.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">1.4.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.23.1">sklearn</span></strong><span class="koboSpan" id="kobo.24.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">1.2.2</span></span></li></ul></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">DVC</span></strong><span class="koboSpan" id="kobo.27.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">1.10.0</span></span></li>
<li><span class="koboSpan" id="kobo.29.1"> You should also have basic knowledge of the machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">life cycle</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.31.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter10"><span class="No-Break"><span class="koboSpan" id="kobo.33.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter10</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.34.1">.</span></span></p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor289"/><span class="koboSpan" id="kobo.35.1">Reproducibility in machine learning</span></h1>
<p><span class="koboSpan" id="kobo.36.1">Lack of </span><em class="italic"><span class="koboSpan" id="kobo.37.1">reproducibility</span></em><span class="koboSpan" id="kobo.38.1"> in </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.39.1">your machine learning projects could be a waste of resources and decrease the credibility of your models and findings in your research projects. </span><em class="italic"><span class="koboSpan" id="kobo.40.1">Reproducibility</span></em><span class="koboSpan" id="kobo.41.1"> is not the only term used in this context; there are also two other key terms: </span><em class="italic"><span class="koboSpan" id="kobo.42.1">repeatability</span></em><span class="koboSpan" id="kobo.43.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.44.1">replicability</span></em><span class="koboSpan" id="kobo.45.1">. </span><span class="koboSpan" id="kobo.45.2">We don’t want to get into the details of these</span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.46.1"> differences. </span><span class="koboSpan" id="kobo.46.2">Instead, we want to have a definition of reproducibility to use in this book. </span><span class="koboSpan" id="kobo.46.3">We</span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.47.1"> define reproducibility in machine learning as the ability of different individuals or teams of scientists and developers to achieve the same results using the same dataset, methodology, and development environment as reported in an original report or study. </span><span class="koboSpan" id="kobo.47.2">We can ensure reproducibility through the proper sharing of code, data, model parameters and hyperparameters, and other relevant information, which allows others to validate and build upon our findings. </span><span class="koboSpan" id="kobo.47.3">Let’s better understand the importance of reproducibility by going through </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">two examples.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">Scientists from a biotechnology company tried to reproduce the findings of 53 cancer studies (Begley et al., 2012). </span><span class="koboSpan" id="kobo.49.2">But they were only able to reproduce the results of 6 out of the 53 studies. </span><span class="koboSpan" id="kobo.49.3">These were not necessarily in the context of reproducibility in machine learning, but it highlights the importance of reproducibility in scientific research and the potential consequences of basing decisions or further research and development on </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">irreproducible findings.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Another example of highlighting the importance of reproducibility in the context of data analysis and data-driven discovery is what is known as the </span><em class="italic"><span class="koboSpan" id="kobo.52.1">Reinhart-Rogoff Excel Error</span></em><span class="koboSpan" id="kobo.53.1"> (Reinhart, C., and Rogoff, K., 2010). </span><span class="koboSpan" id="kobo.53.2">In 2010, the economists Carmen Reinhart and Kenneth Rogoff published a paper suggesting a negative correlation between high public debt and economic growth. </span><span class="koboSpan" id="kobo.53.3">This paper influenced economic policies worldwide. </span><span class="koboSpan" id="kobo.53.4">However, in 2013, other researchers discovered an error in their Excel calculations, which significantly impacted the results. </span><span class="koboSpan" id="kobo.53.5">But later, it was argued that the error was not the driver behind the conclusions (Maziarz, 2017). </span><span class="koboSpan" id="kobo.53.6">Here, we don’t want to focus on their findings but want to emphasize that the reproducibility of the analysis could eliminate any further argument regardless of whether or not </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.54.1">there was an error or not in the </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">original analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">The following three concepts can help you achieve reproducibility in your machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">modeling projects:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.58.1">Code versioning</span></strong><span class="koboSpan" id="kobo.59.1">: Having </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.60.1">access to the version of the code used in any given stage of a machine learning life cycle is fundamentally important to repeat an analysis or training and </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">evaluation processes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Data versioning</span></strong><span class="koboSpan" id="kobo.63.1">: To </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.64.1">achieve reproducibility, we need to have access to the version of the data that’s used in any given stage of the machine learning life cycle, such as training </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">and testing</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Model versioning</span></strong><span class="koboSpan" id="kobo.67.1">: Having a</span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.68.1"> version of your model with frozen parameters and no randomness in initializing, evaluating, or other processes in modeling, helps you eliminate risks </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">of irreproducibility</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.70.1">We briefly talked about code versioning in </span><a href="B16369_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.71.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.72.1">, </span><em class="italic"><span class="koboSpan" id="kobo.73.1">Beyond Code Debugging</span></em><span class="koboSpan" id="kobo.74.1">. </span><span class="koboSpan" id="kobo.74.2">Here, we will focus on data and model versioning to help you in designing reproducible machine </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">learning m</span><a id="_idTextAnchor290"/><span class="koboSpan" id="kobo.76.1">odels.</span></span></p>
<h1 id="_idParaDest-181"><a id="_idTextAnchor291"/><span class="koboSpan" id="kobo.77.1">Data versioning</span></h1>
<p><span class="koboSpan" id="kobo.78.1">We have different stages in the </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.79.1">machine learning life cycle, from data collection and selection to data wrangling and transformation, in which the data gets prepared step by step for model training and evaluation. </span><span class="koboSpan" id="kobo.79.2">Data versioning helps us maintain data integrity and reproducibility throughout these processes. </span><span class="koboSpan" id="kobo.79.3">Data versioning is the process of tracking and managing changes in datasets. </span><span class="koboSpan" id="kobo.79.4">It involves keeping a record of different versions or iterations of the data, allowing us to access and compare previous states or recover earlier versions when needed. </span><span class="koboSpan" id="kobo.79.5">We can reduce the risk of data loss or inconsistencies by ensuring that changes are properly documented </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">and versioned.</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">There are </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.82.1">data versioning tools that can help us in managing and tracking changes in the data we want to use for machine learning modeling or processes to assess the reliability and fairness of our models. </span><span class="koboSpan" id="kobo.82.2">Here are some popular </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">data-versioning tools:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">MLflow</span></strong><span class="koboSpan" id="kobo.85.1">: We introduced MLflow</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.86.1"> for experiment tracking and </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.87.1">model monitoring in previous chapters, but you can also use it for</span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.88.1"> data </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">versioning (</span></span><a href="https://mlflow.org/"><span class="No-Break"><span class="koboSpan" id="kobo.90.1">https://mlflow.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.91.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.92.1">Data Version Control</span></strong><span class="koboSpan" id="kobo.93.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.94.1">DVC</span></strong><span class="koboSpan" id="kobo.95.1">): This is</span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.96.1"> an open source version</span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.97.1"> control system for managing data, code, and </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.98.1">ML models. </span><span class="koboSpan" id="kobo.98.2">It is designed to handle large datasets and integrates with </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">Git (</span></span><a href="https://dvc.org/"><span class="No-Break"><span class="koboSpan" id="kobo.100.1">https://dvc.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.101.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.102.1">Pachyderm</span></strong><span class="koboSpan" id="kobo.103.1">: This is a</span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.104.1"> data-versioning platform that provides reproducibility, provenance, and </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.105.1">scalability in machine </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.106.1">learning </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">workflows (</span></span><a href="https://www.pachyderm.com/"><span class="No-Break"><span class="koboSpan" id="kobo.108.1">https://www.pachyderm.com/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.109.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.110.1">Delta Lake</span></strong><span class="koboSpan" id="kobo.111.1">: This is </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.112.1">an open source storage layer for Apache </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.113.1">Spark and big data workloads that provides data</span><a id="_idIndexMarker602"/> <span class="No-Break"><span class="koboSpan" id="kobo.114.1">versioning (</span></span><a href="https://delta.io/"><span class="No-Break"><span class="koboSpan" id="kobo.115.1">https://delta.io/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.116.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.117.1">Git Large File Storage</span></strong><span class="koboSpan" id="kobo.118.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.119.1">Git-LFS</span></strong><span class="koboSpan" id="kobo.120.1">): This is an extension of Git that allows the versioning of large files, such as data</span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.121.1"> files or </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.122.1">models, alongside </span><a id="_idIndexMarker605"/><span class="No-Break"><span class="koboSpan" id="kobo.123.1">code (</span></span><a href="https://git-lfs.github.com/"><span class="No-Break"><span class="koboSpan" id="kobo.124.1">https://git-lfs.github.com/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.125.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.126.1">Each of these tools provides you with different data-versioning capabilities. </span><span class="koboSpan" id="kobo.126.2">You can choose the one that meets your needs considering the size of the data, the nature of the project, and the desired level of integration with </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">other tools.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">Here is an example of</span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.129.1"> using DVC with Python for data versioning. </span><span class="koboSpan" id="kobo.129.2">After installing DVC, you can initialize it by writing the following command in </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">the Terminal:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.131.1">
dvc init</span></pre> <p><span class="koboSpan" id="kobo.132.1">This will create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">.dvc</span></strong><span class="koboSpan" id="kobo.134.1"> directory and set up the necessary configuration. </span><span class="koboSpan" id="kobo.134.2">Now, let’s create a small DataFrame and save it as a CSV file </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">in Python:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.136.1">
import pandas as pd# create a sample dataset
data_df = pd.DataFrame({'feature 1': [0.5, 1.2, 0.4, 0.8],
    'feature 2': [1.1, 1.3, 0.6, 0.1]})
# save the dataset to a CSV file
data_df.to_csv('dataset.csv', index=False)</span></pre>
<p><span class="koboSpan" id="kobo.137.1">Now, we can add the </span><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">dataset.csv</span></strong><span class="koboSpan" id="kobo.139.1"> file to DVC and commit the changes, similar to committing code changes </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">using Git:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.141.1">
dvc add dataset.csvgit add dataset.csv.dvc .gitignore
git commit -m "add initial dataset"</span></pre>
<p><span class="koboSpan" id="kobo.142.1">This creates a </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">data.csv.dvc</span></strong><span class="koboSpan" id="kobo.144.1"> file that tracks the dataset’s version, and it adds </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">data.csv</span></strong><span class="koboSpan" id="kobo.146.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.147.1">.gitignore</span></strong><span class="koboSpan" id="kobo.148.1"> so that Git doesn’t track the actual data file. </span><span class="koboSpan" id="kobo.148.2">Now, we can modify the dataset as follows and save it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">same name:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.150.1">
# Add a new column to the datasetdata_df['feature 3'] = [0.05, 0.6, 0.4, 0.9]
# Save the modified dataset to the same CSV file
data_df.to_csv('dataset.csv', index=False)</span></pre>
<p><span class="koboSpan" id="kobo.151.1">We can also commit the changes and save it as a </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">different version:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.153.1">
dvc add dataset.csvgit add dataset.csv.dvc
git commit -m "update dataset with new feature column"</span></pre>
<p><span class="koboSpan" id="kobo.154.1">Now that we have two versions of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">dataset.csv</span></strong><span class="koboSpan" id="kobo.156.1"> file, we can switch to the previous version or the latest version of the datasets when needed by using the following commands in</span><a id="_idIndexMarker607"/> <span class="No-Break"><span class="koboSpan" id="kobo.157.1">the Terminal:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.158.1">
# go back to the previous version of the datasetgit checkout HEAD^
dvc checkout
# return to the latest version of the dataset
git checkout master
dvc checkout</span></pre>
<p><span class="koboSpan" id="kobo.159.1">But if you have many versions of the same file or data, you can use other simple commands available as part </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">of DVC.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">In addition</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.162.1"> to versioning our data, we need to track and manage different versions of our models throughout the development life cycle</span><a id="_idTextAnchor292"/><span class="koboSpan" id="kobo.163.1">. </span><span class="koboSpan" id="kobo.163.2">We will cover </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">this next.</span></span></p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor293"/><span class="koboSpan" id="kobo.165.1">Model versioning</span></h1>
<p><span class="koboSpan" id="kobo.166.1">A model that goes to </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.167.1">production is the eventual result of a series of experimentation and model modifications with different versions of training and test data, and different machine learning methods and their corresponding hyperparameters. </span><span class="koboSpan" id="kobo.167.2">Model versioning helps us ensure that changes that are made to models are traceable, helping to establish reproducibility in our machine learning projects. </span><span class="koboSpan" id="kobo.167.3">It ensures that every version of a model can be easily reproduced by providing a complete snapshot of the model’s parameters, hyperparameters, and training data at a given point in time. </span><span class="koboSpan" id="kobo.167.4">It allows us to easily roll back to a previous version in case of issues with a newly deployed model or to recover an older version that may have been unintentionally modified </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">or deleted.</span></span></p>
<p><span class="koboSpan" id="kobo.169.1">Let’s go</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.170.1"> through a very simple example to better understand the need for model versioning. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.171.1">Figure 10</span></em></span><em class="italic"><span class="koboSpan" id="kobo.172.1">.1</span></em><span class="koboSpan" id="kobo.173.1"> shows the performance of a random forest model with five estimators, or decision trees, and the different maximum depths allowed for these decision trees. </span><span class="koboSpan" id="kobo.173.2">If we simply change the random states that are used to split the data into train and test sets, using </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.175.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.177.1">, and perform model initialization for a </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">RandomForestClassifier()</span></strong><span class="koboSpan" id="kobo.179.1"> model, we get different log-loss values and dependencies on the maximum depth of the trees in the random </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">forest model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<span class="koboSpan" id="kobo.181.1"><img alt="Figure 10.1 – Log-loss in training and validation sets separated from the breast cancer dataset using different random states for modeling and data split" src="image/B16369_10_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.182.1">Figure 10.1 – Log-loss in training and validation sets separated from the breast cancer dataset using different random states for modeling and data split</span></p>
<p><span class="koboSpan" id="kobo.183.1">This was a small example to show how such simple changes, which can happen if our models are not versioned, can have drastic effects on our machine learning modeling. </span><span class="koboSpan" id="kobo.183.2">When we use experiment tracking tools such as MLflow, we have access to all the tracked information for a </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">selected model.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">To version our model, we need to make </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.186.1">sure of </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.188.1">We have access to a saved version of the parameters of the </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">corresponding model</span></span></li>
<li><span class="koboSpan" id="kobo.190.1">Other necessary information such as model hyperparameters are documented or saved for </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">model retraining</span></span></li>
<li><span class="koboSpan" id="kobo.192.1">The code that needs to be used with the model parameters for inference or even retraining and testing </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">is versioned</span></span></li>
<li><span class="koboSpan" id="kobo.194.1">Processes with </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.195.1">randomization, such as model initialization and data split for training and testing, have specified random states, </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">or seeds</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.197.1">There are different ways of storing your models and their related documentation. </span><span class="koboSpan" id="kobo.197.2">For example, you can store your model using serialization libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">pickle</span></strong><span class="koboSpan" id="kobo.199.1"> alone or in combination with DVC (</span><a href="https://dvc.org/doc/api-reference/open"><span class="koboSpan" id="kobo.200.1">https://dvc.org/doc/api-reference/open</span></a><span class="koboSpan" id="kobo.201.1">), </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.203.1">
with dvc.api.open(model_path, mode='w', remote=remote_url) as f:     pickle.dump(model, f)</span></pre>
<p><span class="koboSpan" id="kobo.204.1">For this, you need to specify a local path on which to save the model using </span><strong class="source-inline"><span class="koboSpan" id="kobo.205.1">pickle.dump</span></strong><span class="koboSpan" id="kobo.206.1"> and a remote path fo</span><a id="_idTextAnchor294"/><span class="koboSpan" id="kobo.207.1">r model versioning </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">using DVC.</span></span></p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor295"/><span class="koboSpan" id="kobo.209.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.210.1">In this chapter, you learned about the meaning and importance of reproducibility in machine learning modeling. </span><span class="koboSpan" id="kobo.210.2">You also learned about data and model versioning, which help us to develop more reliable and reproducible models and data analysis results. </span><span class="koboSpan" id="kobo.210.3">Next, you learned about the different tools and Python libraries you can use to version your data and models. </span><span class="koboSpan" id="kobo.210.4">With the concepts and practices introduced in this chapter, you are ready to ensure reproducibility in your machine </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">learning projects.</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">In the next chapter, you will learn about techniques you can use to avoid and eliminate data drift and concept drift, which constitute two differences between the behavior of models</span><a id="_idTextAnchor296"/><span class="koboSpan" id="kobo.213.1"> before and </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">after deployment.</span></span></p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor297"/><span class="koboSpan" id="kobo.215.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.216.1">What are three examples of tools that you can use for </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">data versioning?</span></span></li>
<li><span class="koboSpan" id="kobo.218.1">When you generate different versions of the same data, such as by using DVC, do you need to save it with </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">different names?</span></span></li>
<li><span class="koboSpan" id="kobo.220.1">Can you provide an example where you would use the same method and training and evaluation data but get different trainin</span><a id="_idTextAnchor298"/><span class="koboSpan" id="kobo.221.1">g and </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">evaluation performance?</span></span></li>
</ol>
<h1 id="_idParaDest-185"><a id="_idTextAnchor299"/><span class="koboSpan" id="kobo.223.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.224.1">Reinhart, C., &amp; Rogoff, K. </span><span class="koboSpan" id="kobo.224.2">(2010b). </span><em class="italic"><span class="koboSpan" id="kobo.225.1">Debt and growth revisited</span></em><span class="koboSpan" id="kobo.226.1">. </span><span class="koboSpan" id="kobo.226.2">VOX. </span><span class="koboSpan" id="kobo.226.3">CEPRs Policy Portal. </span><span class="koboSpan" id="kobo.226.4">Retrieved September </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">18, 2015.</span></span></li>
<li><span class="koboSpan" id="kobo.228.1">Reinhart, C., &amp; Rogoff, K. </span><span class="koboSpan" id="kobo.228.2">(2010a). </span><em class="italic"><span class="koboSpan" id="kobo.229.1">Growth in a time of debt</span></em><span class="koboSpan" id="kobo.230.1">. </span><span class="koboSpan" id="kobo.230.2">American Economic Review, </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">100, 573–578.10.1257/aer.100.2.573.</span></span></li>
<li><span class="koboSpan" id="kobo.232.1">Maziarz, Mariusz. </span><em class="italic"><span class="koboSpan" id="kobo.233.1">The Reinhart-Rogoff controversy as an instance of the ‘emerging contrary result’ phenomenon</span></em><span class="koboSpan" id="kobo.234.1">. </span><span class="koboSpan" id="kobo.234.2">Journal of Economic Methodology 24.3 (</span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">2017): 213-225.</span></span></li>
<li><span class="koboSpan" id="kobo.236.1">Begley, C. </span><span class="koboSpan" id="kobo.236.2">G., &amp; Ellis, L. </span><span class="koboSpan" id="kobo.236.3">M. </span><span class="koboSpan" id="kobo.236.4">(2012). </span><em class="italic"><span class="koboSpan" id="kobo.237.1">Drug development: Raise standards for preclinical cancer research</span></em><span class="koboSpan" id="kobo.238.1">. </span><span class="koboSpan" id="kobo.238.2">Nature, </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">483(7391), 531-533.</span></span></li>
<li><span class="koboSpan" id="kobo.240.1">Association for Computing Machinery (2016). </span><em class="italic"><span class="koboSpan" id="kobo.241.1">Artifact Review and Badging</span></em><span class="koboSpan" id="kobo.242.1">. </span><span class="koboSpan" id="kobo.242.2">Available online at </span><a href="https://www.acm.org/publications/policies/artifact-review-badging"><span class="koboSpan" id="kobo.243.1">https://www.acm.org/publications/policies/artifact-review-badging</span></a><span class="koboSpan" id="kobo.244.1"> (Accessed November </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">24, 2017).</span></span></li>
<li><span class="koboSpan" id="kobo.246.1">Plesser, Hans E. </span><em class="italic"><span class="koboSpan" id="kobo.247.1">Reproducibility vs. </span><span class="koboSpan" id="kobo.247.2">replicability: a brief history of a confused terminology</span></em><span class="koboSpan" id="kobo.248.1">. </span><span class="koboSpan" id="kobo.248.2">Frontiers in neuroinformatics 11 (</span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">2018): 76.</span></span></li>
<li><span class="koboSpan" id="kobo.250.1">Pineau, J., Vincent, M., Larochelle, H., &amp; Bengio, Y. </span><span class="koboSpan" id="kobo.250.2">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.251.1">Improving reproducibility in machine learning research (A report from the NeurIPS 2019 reproducibility program)</span></em><span class="koboSpan" id="kobo.252.1">. </span><span class="koboSpan" id="kobo.252.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">preprint arXiv:2003.12206.</span></span></li>
<li><span class="koboSpan" id="kobo.254.1">Raff, E., Lemire, D., &amp; Nicholas, C. </span><span class="koboSpan" id="kobo.254.2">(2019). </span><em class="italic"><span class="koboSpan" id="kobo.255.1">A new measure of algorithmic stability for machine learning</span></em><span class="koboSpan" id="kobo.256.1">. </span><span class="koboSpan" id="kobo.256.2">Journal of Machine Learning Research, </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">20(168), 1-32.</span></span></li>
<li><span class="koboSpan" id="kobo.258.1">Gundersen, O. </span><span class="koboSpan" id="kobo.258.2">E., &amp; Kjensmo, S. </span><span class="koboSpan" id="kobo.258.3">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.259.1">State of the art: Reproducibility in artificial intelligence</span></em><span class="koboSpan" id="kobo.260.1">. </span><span class="koboSpan" id="kobo.260.2">In Thirty-Second AAAI Conference on </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">Artificial Intelligence.</span></span></li>
<li><span class="koboSpan" id="kobo.262.1">Jo, T., &amp; Bengio, Y. </span><span class="koboSpan" id="kobo.262.2">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.263.1">Measuring the tendency of CNNs to Learn Surface Statistical Regularities</span></em><span class="koboSpan" id="kobo.264.1">. </span><span class="koboSpan" id="kobo.264.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">preprint arXiv:1711.11561.</span></span></li>
<li><span class="koboSpan" id="kobo.266.1">Haibe-Kains, B., Adam, G. </span><span class="koboSpan" id="kobo.266.2">A., Hosny, A., Khodakarami, F., &amp; Waldron, L. </span><span class="koboSpan" id="kobo.266.3">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.267.1">Transparency and reproducibility in artificial intelligence</span></em><span class="koboSpan" id="kobo.268.1">. </span><span class="koboSpan" id="kobo.268.2">Nature, </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">586(7829), E14-E16.</span></span></li>
</ul>
</div>
</body></html>