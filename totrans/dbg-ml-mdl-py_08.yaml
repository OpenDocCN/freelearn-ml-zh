- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Controlling Risks Using Test-Driven Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are risks, such as selecting unreliable models, associated with creating
    models and technologies built on top of our models. The question is, could we
    avoid them and better manage the risks associated with machine learning modeling?
    In this chapter, we will talk about programming strategies such as unit testing,
    which could help us not only in developing and selecting better models but also
    in reducing risks associated with modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning differential testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking machine learning experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned how to reduce the risk of
    unreliable modeling and software development using unit and differential testing
    and how to reliably build upon previous attempts in model training and evaluation
    using machine learning experiment tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytest` >= 7.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipytest` >= 0.13.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlflow` >= 2.1.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aif360` >= 0.5.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shap` >= 0.41.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` >= 1.4.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also require basic knowledge of model bias and the definition of bias
    measures such as the **disparate impact** **ratio** (**DIR**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter08](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development for machine learning modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One approach to reducing the risks of developing unreliable models and pushing
    them to production is test-driven development. We aim to design unit tests (that
    is, tests designed to test individual components of software) that reduce the
    risks of code revision either within the same or in different life cycles. To
    better understand this concept, we need to understand what unit tests are and
    how we can design and use them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unit tests are designed to test the smallest components, or units, in the code
    and software we design. In machine learning modeling, we might have many modules
    taking care of different steps of a machine learning life cycle, such as data
    curation and wrangling or model evaluation. Unit tests help us avoid errors and
    mistakes, and design our code without the need to worry about whether we made
    a mistake that will not be detected early on. Detecting issues in our code early
    has lower costs and helps us to avoid error pile-ups, which makes the debugging
    process easier. Pytest is a Python testing framework that helps us in designing
    different tests, including unit tests, in machine learning programming.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pytest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pytest is a simple-to-use Python library that we can use to design unit tests
    by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the component we want to design the unit test for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a small operation to be used for testing that component. For example,
    if the module is part of data processing, the test can be designed using a very
    small toy dataset, either real or synthetic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design a function starting with `"test_"` for the corresponding component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 1* to *3* for all the components of the code for which we want
    to design unit tests. It is better to cover as many components as possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The designed tests can then be used to test changes in your code. We will practice
    unit test design here, using Pytest, for a function that calculates the DIR and
    returns “unbiased data” or “biased data” using the input thresholds for DIR for
    bias detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve defined an example usage of this function to use for our unit
    test design, we can select the first 100 rows of the dataset and calculate the
    DIR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the calculated DIR, this subset of the data is biased concerning
    the `''Sex''` attribute. To design the unit tests, we need to import the `pytest`
    library. But if you are using Jupyter or Colab notebooks for prototyping, you
    can use `ipytest` to test your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We must add `%%ipytest -qq` if we’re using `pytest` in a Jupyter or Colab notebook
    and want to run the tests using `ipytest`. Then, we can define our unit test function,
    `test_dir_grouping()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `assert` command checks whether the result of the `dir_grouping()` function
    is “biased data,” as it is supposed to be according to our previous analysis,
    for the first 100 rows of the dataset. If the result is different, then the test
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have all the unit tests ready for the components of your software,
    you can run `pytest` in the `test_dir_grouping`, as shown in the preceding code,
    within a Python script called `test_script.py`, you can only test that script
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can run `pytest` in a specific directory. If you have a
    code base that contains many different modules, you can organize your tests according
    to the grouping of your main functions and classes and then test each directory,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, if you simply run `pytest`, it will execute all the tests in all the
    files named `test_*.py` or `\*_test.py` in the current directory and its subdirectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use a Python interpreter to execute the tests using `pytest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using Jupyter or Colab Notebook and used `ipytest`, you can run
    Pytest as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, imagine we execute the designed `test_dir_grouping()` function in one
    of these ways. When the test is passed, we will see a message like the following,
    which tells us 100% of the tests passed. This is because we are only testing one
    test and the test passed (*Figure 8**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – The output of Pytest when the designed test passed](img/B16369_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – The output of Pytest when the designed test passed
  prefs: []
  type: TYPE_NORMAL
- en: 'If we mistakenly change `assessment = "biased data"` to `assessment = "unbiased
    data"` in the `dir_grouping()` function, we get the following result instead,
    which tells us 100% of the tests failed. This is because we only have one test,
    which failed in this case (*Figure 8**.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Failure message after running Pytest](img/B16369_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Failure message after running Pytest
  prefs: []
  type: TYPE_NORMAL
- en: The failure message in `pytest` contains some complementary information that
    we can use to debug our code. In this case, it is telling us that in `test_dir_grouping()`,
    it tried to assert the output of `test_dir_grouping()`, which was “unbiased data,”
    with “biased data.”
  prefs: []
  type: TYPE_NORMAL
- en: Pytest fixtures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When programming for data analysis and machine learning modeling, we need to
    use data that is in different variables or data objects, comes from a file in
    our local machine or the cloud, is queried from a database, or comes from a URL
    in our tests. Fixtures help us in these processes by removing the need to repeat
    the same code across our tests. Attaching a fixture function to a test will run
    it and return data to the test before each test runs. Here, we have used examples
    provided on the Pytest documentation page for fixtures (source: [https://docs.pytest.org/en/7.1.x/how-to/fixtures.html](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html)).
    First, let’s define two very simple classes called `Fruit` and `FruitSalad`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we use `pytest`, it looks at the parameters in the test function signature
    and looks for fixtures with the same names as those parameters. Pytest then runs
    those fixtures, captures what they return, and passes those objects as arguments
    to the test function. We inform Pytest that a function is a fixture by decorating
    it with `@pytest.fixture`. In the following example, when we run the tests, `test_fruit_salad`
    requests `fruit_bowl`, and Pytest executes `fruit_bowl` and passes the returned
    object into `test_fruit_salad`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some of the features of fixtures that can help us in designing our
    tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures can request other fixtures. This helps us in designing smaller fixtures
    that can be even used as part of other fixtures to make more complex tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixtures can be reused in different tests. They work like functions to be used
    in different tests with their own returned results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A test or fixture can request more than one fixture at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixtures can be requested more than once per test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In test-driven development, we aim to write production-ready code that passes
    the designed unit tests. Higher coverage of the modules and components in your
    code by the designed unit test could help you in revising your code that’s related
    to any component of a machine learning life cycle with peace of mind.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned about unit testing, but other techniques can help
    us in reliable programming and machine learning model development, such as differential
    testing. We will introduce this next.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning differential testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Differential testing attempts to check two versions of a piece of software,
    considered as base and test versions, on the same input and then compare the outputs.
    This process helps us identify whether the outputs are the same and identify unexpected
    differences (Gulzar et al., 2019; *Figure 8**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Simplified flowchart of differential testing as a process to
    test the outputs of two implementations of the same process on the same data](img/B16369_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Simplified flowchart of differential testing as a process to test
    the outputs of two implementations of the same process on the same data
  prefs: []
  type: TYPE_NORMAL
- en: In differential testing, the base version is already verified and considered
    the approved version, while the test version needs to be checked in comparison
    with the base version in producing the correct output. In differential testing,
    we can also aim to assess whether the observed differences between the outputs
    of the base and test versions are expected or can be explained.
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning modeling, we can also benefit from differential testing
    when comparing two different implementations of the same algorithms on the same
    data. For example, we can use it to compare models built using `scikit-learn`
    and Spark `MLlib` as two different libraries for machine learning modeling. If
    we need to recreate a model using `scikit-learn` and add it to our pipeline while
    the original model is built in Spark `MLlib`, we can use differential testing
    to assess the outputs and make sure either there is no difference or the differences
    are expected (Herbold and Tunkel, 2023). *Table 8.1* provides some examples of
    algorithms with available classes in both `scikit-learn` and Spark `MLlib`. This
    approach has been used more extensively to compare models between different deep
    learning frameworks, such as TensorFlow and PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **scikit-learn** | **Spark MLlib** |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | `LogisticRegression` | `LogisticRegression` |'
  prefs: []
  type: TYPE_TB
- en: '| Naive Bayes | `GaussianNB, MultinomialNB` | `NaiveBayes` |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | `DecisionTree Classifier` | `DecisionTreeClassifier` |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | `RandomForest Classifier` | `RandomForestClassifier` |'
  prefs: []
  type: TYPE_TB
- en: '| Support vector machine | `LinearSVC` | `LinearSVC` |'
  prefs: []
  type: TYPE_TB
- en: '| Multilayer perceptron | `MLPClassifier` | `MultilayerPerceptron Classifier`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient boosting | `GradientBoosting Classifier` | `GBTClassifier` |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Some of the overlapping algorithms and their class names in scikit-learn
    and Spark MLlib
  prefs: []
  type: TYPE_NORMAL
- en: Experiment tracking is another technique that we can benefit from besides unit
    and differential testing in our machine learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking machine learning experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keeping track of our machine learning experiments will help us reduce the risks
    of invalid conclusions and selecting unreliable models. Experiment tracking in
    machine learning is about saving the information about the experiments – for instance,
    the data that has been used – the testing performance and the metric used for
    performance assessment, and the algorithms and the hyperparameters used for modeling.
    Here are some of the important considerations for using a machine learning experiment
    tracking tool:'
  prefs: []
  type: TYPE_NORMAL
- en: Can you integrate the tool with your **continuous integration/continuous development**
    (**CI/CD**) pipeline and machine learning modeling frameworks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you reproduce your experiments?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you easily search through the experiments to find the best models or models
    with bad or unexpected behaviors?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it cause any security or privacy issues?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the tool help you better collaborate in your machine learning projects?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the tool let you track hardware (for example, memory) consumption?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the commonly used machine learning experiment tracking tools and their
    URLs are provided in *Table 8.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **URL** |'
  prefs: []
  type: TYPE_TB
- en: '| MLflow Tracking | [https://mlflow.org/docs/latest/tracking.html](https://mlflow.org/docs/latest/tracking.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DVC | [https://dvc.org/doc/use-cases/experiment-tracking](https://dvc.org/doc/use-cases/experiment-tracking)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Weights & Biases | [https://wandb.ai/site/experiment-tracking](https://wandb.ai/site/experiment-tracking)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Comet ML | [https://www.comet.com/site/products/ml-experiment-tracking/](https://www.comet.com/site/products/ml-experiment-tracking/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ClearML | [https://clear.ml/clearml-experiment/](https://clear.ml/clearml-experiment/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Polyaxon | [https://polyaxon.com/product/#tracking](https://polyaxon.com/product/#tracking)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TensorBoard | [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Neptune AI | [https://neptune.ai/product/experiment-tracking](https://neptune.ai/product/experiment-tracking)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SageMaker | [https://aws.amazon.com/sagemaker/experiments/](https://aws.amazon.com/sagemaker/experiments/)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 8.2 – Examples of tools for teaching machine learning experiments
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we want to practice **MLflow Tracking** in Python. First, we need to
    import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must define a function for evaluating the results of the models we
    would like to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must load the breast cancer dataset from `scikit-learn` for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to define an experiment using `mlflow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we must go over three different numbers of decision trees, or three different
    numbers of estimators, to build, train, and test three different random forest
    models on the loaded breast cancer dataset. All the information from these three
    runs will be stored within the specified experiment but as different runs. As
    you can see in the following code, we use different functionalities in `mlflow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mlflow.start_run`: To start a run as part of an experiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlflow.log_param`: To log the number of estimators as a hyperparameter of
    the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlflow.log_metric`: To log the calculated metric for the performance of the
    model on the defined test set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlflow.sklearn.log_model`: To log the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also retrieve an already stored experiment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can get information on different runs in that experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also identify the best runs according to a metric used for model testing,
    which is ROC-AUC in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also delete runs of a run or an experiment altogether if needed, as
    follows. But you need to make sure you wish to delete such information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this section, you learned about experiment tracking in a machine learning
    setting. You will learn more about the techniques you can use for risk control
    in your machine learning projects in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about test-driven development using unit testing
    to control risks in your machine learning development projects. You learned about
    unit testing in Python using the `pytest` library. We also briefly reviewed the
    concept of differential testing, which helps you in comparing different versions
    of your machine learning modules and software. Later, you learned about model
    experiment tracking as an important tool that not only facilitates your model
    experimentations and selection but also helps you in risk control in your machine
    learning projects. You practiced using `mlflow` in Python as one of the widely
    used machine learning experiment tracking tools. Now, you know how to develop
    reliable models and programming modules through test-driven development and experiment
    tracking.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about strategies to test models, assess
    their qualities, and monitor their performance in production. You will learn about
    practical methods for model monitoring, integration testing, and model pipeline
    and infrastructure testing.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does `pytest` help you in developing code modules in your machine learning
    projects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do `pytest` fixtures help you in using `pytest`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is differential testing and when do you need it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `mlflow` and how does it help you in your machine learning modeling
    projects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Herbold, Steffen, and Steffen Tunkel. *Differential testing for machine learning:
    an analysis for classification algorithms beyond deep learning*. Empirical Software
    Engineering 28.2 (2023): 34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lichman, M. (2013). *UCI Machine Learning Repository* [[https://archive.ics.uci.edu/ml](https://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulzar, Muhammad Ali, Yongkang Zhu, and Xiaofeng Han. *Perception and practices
    of differential testing*. 2019 IEEE/ACM 41st International Conference on Software
    Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
