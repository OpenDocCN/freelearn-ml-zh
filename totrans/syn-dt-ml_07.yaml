- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce **Generative Adversarial Networks** (**GANs**)
    and discuss the evolution of this data generation method. You will learn about
    the typical architecture of a GAN. After this, we will explain its training process
    and discuss the main challenges. Then, we will highlight various applications
    of GANs, including generating images and text-to-image translation. Additionally,
    we will study a practical coding example demonstrating how to use GANs to generate
    photorealistic images. Finally, we will also discuss variations of GANs, such
    as conditional GANs, CycleGANs, CTGANs, WGANs, WGAN-GPs, and f-GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a GAN?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing GANs to generate synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on GANs in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variations of GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code used in this chapter will be available in the corresponding chapter
    folder in the book’s GitHub repository: [https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning](https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning).'
  prefs: []
  type: TYPE_NORMAL
- en: What is a GAN?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce GANs and briefly discuss the evolution and
    progression of this particular data generation method. Then, we will explain the
    standard architecture of a typical GAN and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of GANs was introduced in the 2014 paper *Generative Adversarial
    Networks* ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)),
    by Ian J. Goodfellow and his research team. In the same year, **conditional GANs**
    were introduced, allowing us to generate more customizable synthetic data. Then,
    **Deep Convolutional GANs** (**DCGANs**) were suggested in 2015, which facilitated
    the generation of high-resolution images. After that, **CycleGANs** were proposed
    in 2017 for unsupervised image-to-image translation tasks. This opened the door
    for enormous applications such as domain adaptation. **StyleGAN** was introduced
    in 2019, bringing GANs to new fields such as art and fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have also been showing impressive progress in the field of video synthesis.
    In fact, the recent work by NVIDIA is a testament to their tremendous potential
    (please check this paper for more details: *One-Shot Free-View Neural Talking-Head
    Synthesis for Video Conferencing* at [https://arxiv.org/pdf/2011.15126.pdf](https://arxiv.org/pdf/2011.15126.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This work shows that GANs can now recreate a talking-head video using only
    a single source image. For the code, dataset, and online demo, refer to the project’s
    page: [https://nvlabs.github.io/face-vid2vid](https://nvlabs.github.io/face-vid2vid).
    Next, we delve into the architecture of GANs.'
  prefs: []
  type: TYPE_NORMAL
- en: Most **deep learning** (**DL**) methods and architectures are designed to predict
    something. It could be weather conditions, stock prices, object classes, or something
    else. However, GANs were proposed to *generate* something. It could be images,
    videos, texts, music, or point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of this capability lies the essential problem of learning how to
    generate training samples from a given domain or dataset. GANs are DL methods
    that can learn complex data distributions and can be leveraged to generate an
    unlimited number of samples that belong to a specific distribution. These generated
    synthetic samples have many applications for data augmentation, style transfer,
    and data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A typical architecture and training process of GANs](img/Figure_07_01_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – A typical architecture and training process of GANs
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we will learn how to train GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Training a GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to train a typical GAN. Then, we will discuss
    the main challenges and difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: 'A GAN is trained using **unsupervised learning** techniques where both submodels
    are trained simultaneously using a process called **adversarial training**. A
    typical GAN consists of two neural networks (usually convolutional neural networks):
    the **generator** and the **discriminator**. The generator takes in a random noise
    vector as input and generates a synthetic (fake) sample. The goal of the generator
    is to produce synthetic data that is realistic and indistinguishable from real
    data. The discriminator, on the other hand, is trained to distinguish between
    real and fake samples. It receives a sample and predicts its data source domain:
    real or fake. If the discriminator correctly identifies a real data sample, no
    error is backpropagated. However, if the discriminator fails to identify a synthetic
    sample, it is penalized, and the generator is rewarded. The generator is penalized
    if the discriminator is able to correctly identify generated, synthetic data.
    In this way, both the generator and discriminator are constantly trying to improve
    their performance, resulting in the generation of increasingly realistic synthetic
    data. Refer to *Figure 7**.1* for a visualization of the training process. Let’s
    explore more and learn about the training process in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer on hands-on training of GANs
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we do not provide hands-on elements on how to train GANs because
    the chapter is committed to the theoretical, conceptual, and design aspects of
    GANs for synthetic data generation. Thus, hands-on examples are out of the scope
    of this chapter. However, if you are keen to train your GAN, please refer to the
    *Deep Convolutional Generative Adversarial Network* *Tutorial* ([https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan)).
  prefs: []
  type: TYPE_NORMAL
- en: GAN training algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training algorithm is a crucial aspect of enabling GANs to generate useful
    synthetic data. The following is a step-by-step procedure that can be utilized
    to train GANs:'
  prefs: []
  type: TYPE_NORMAL
- en: Create z by sampling a random noise following a suitable noise distribution
    such as uniform, Gaussian, Binomial, Poisson, Exponential, Gamma, and Weibull
    distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed z to the generator to produce a synthetic or fake sample, x fake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass both x fake and x real to a `switch` block, which randomly selects one
    of its inputs and passes it to the discriminator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The discriminator classifies the given sample as real or fake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the error to both the generator and discriminator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights of the generator and discriminator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we’ll discuss the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Training loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loss shown next is one of many losses that can be used to train a GAN.
    This particular loss is derived from the **cross-entropy loss**:'
  prefs: []
  type: TYPE_NORMAL
- en: L = E x[log(D(x))] + E z[log(1 − D(G(z)))]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break this formula down:'
  prefs: []
  type: TYPE_NORMAL
- en: D(x) is the discriminator’s estimate that x is drawn from the real dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E x and E z are the expected values over real and generated synthetic (fake)
    samples, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: G(z) is the output of the generator for a noise vector, z
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D(G(z)) is the discriminator’s estimate that a synthetic sample is real
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As expected, the training process of GANs is complex, but it is a powerful technique
    for generating realistic data, which motivated researchers to examine new ways
    to enhance and speed its training and convergence. Next, let us discuss some of
    these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: GANs in action
  prefs: []
  type: TYPE_NORMAL
- en: For an interactive demonstration of how a GAN is trained, please refer to *Play
    with Generative Adversarial Networks (GANs)* in your browser ([https://poloclub.github.io/ganlab](https://poloclub.github.io/ganlab)).
    For more details, check out the corresponding paper *GAN lab:* *Understanding
    Complex Deep Generative Models using Interactive Visual* *Experimentation* ([https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf](https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will cover some common issues and challenges encountered when training
    a GAN. Let’s explore insights and explanations about the cause of such issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mode collapse**: In this scenario, the generator overfits to a limited number
    of samples and patterns producing the same or similar synthetic samples for different
    z values. For example, a GAN being trained to generate cat images may keep generating
    the same cat image again and again with just minor modifications. This is something
    that we do not want to happen. The point of using GANs is to generate diverse
    synthetic examples. This problem occurs when the generator learns to produce one
    or a few excellent synthetic samples that fool the discriminator. Thus, the generator
    avoids generating other samples and prefers to repeat these excellent synthetic
    samples. There are various solutions to this problem, such as *unrolled GANs*
    ([https://arxiv.org/pdf/1611.02163.pdf](https://arxiv.org/pdf/1611.02163.pdf))
    and *Wasserstein* *loss* ([https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discriminator saturations (diminished gradients)**: As we discussed earlier,
    the training of the generator and discriminator is done in an adversarial manner.
    When the discriminator becomes too successful at classifying real from synthetic
    samples, the error becomes minimal. Thus, the generator can no longer learn useful
    things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter sensitivity and tuning**: Similar to other DL architectures,
    GANs have many hyperparameters, such as learning rate, batch size, number of layers,
    activation functions, and others. Finding the optimal hyperparameters is problem-
    and task-dependent and usually a train-error process. Thus, it is challenging
    to find the right architecture and hyperparameters to successfully train your
    GAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instability and non-convergence**: It is not easy to stabilize the training
    process of the generator and discriminator. In fact, it is common to observe that
    one submodel is learning better than another, which causes the GAN to oscillate,
    giving us unpredictable behavior, and the models may never converge. For more
    details, please refer to *On Convergence and Stability of* *GANs* ([https://arxiv.org/pdf/1705.07215.pdf](https://arxiv.org/pdf/1705.07215.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computation complexity**: GANs have a complex structure, being composed of
    two DL models. This makes the training process computationally expensive and time-consuming.
    However, there are some techniques proposed to speed up the training process,
    such as *Small-GAN: Speeding up GAN Training using Core-Sets* ([http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf](http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf))
    and *Projected GANs Converge* *Faster* ([https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we delve into deploying GANs to generate synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing GANs to generate synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will highlight some interesting applications of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: GANs have enormous applications because they can be used for data augmentation,
    style transfer, privacy protection, and generating photo-realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss some of these applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating images**: GANs can be utilized to generate photorealistic images.
    For instance, GANs were utilized to generate handwritten digits, human faces,
    animals, objects, and scenes. Please check this paper for more details: *Progressive
    Growing of GANs for Improved Quality, Stability, and* *Variation* ([https://arxiv.org/pdf/1710.10196.pdf](https://arxiv.org/pdf/1710.10196.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generating cartoon and anime characters**: GANs can be trained to generate
    appealing and diverse characters. This can be utilized to assess artists, game
    developers, and designers with anime characters. For more details, please check
    the paper *Towards the Automatic Anime Characters Creation with Generative Adversarial
    Networks* ([https://arxiv.org/pdf/1708.05509.pdf](https://arxiv.org/pdf/1708.05509.pdf))
    and the website ([https://make.girls.moe](https://make.girls.moe)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image-to-image translation**: GANs can be utilized to transform images from
    one domain to another domain. For example, **machine learning** (**ML**)-based
    colorizers usually utilize GANs for turning grayscale images into colored ones.
    *Image-to-Image Translation with Conditional Adversarial Networks* ([https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))
    and *StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image
    Translation* ([https://arxiv.org/pdf/1711.09020.pdf](https://arxiv.org/pdf/1711.09020.pdf))
    are well-known examples of image-to-image GAN-based translators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-to-image translation**: Another interesting application of GANs is to
    generate appealing images from a given short textual description of scenes and
    objects. As examples, check *StackGAN: Text to Photo-realistic Image Synthesis
    with Stacked Generative Adversarial Networks* ([https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))
    and DALL-E ([https://openai.com/research/dall-e](https://openai.com/research/dall-e)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to the applications we have discussed, GANs can be used for the
    following non-exhaustive list of interesting tasks and applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic image-to-photo translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate photographs of human faces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face aging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pose guided person image generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Photos to emojis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Photograph editing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image blending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image inpainting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Super-resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D object generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Texture synthesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will delve into a hands-on example that demonstrates the practical
    application of GANs for generating photorealistic synthetic images.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on GANs in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s examine how we can utilize a GAN to generate some synthetic images in
    practice. We will examine *Closed-Form Factorization of Latent Semantics in GANs*
    ([https://arxiv.org/abs/2007.06600](https://arxiv.org/abs/2007.06600)) to learn
    how we can simply generate synthetic images for our ML problem. The code for this
    example was adapted from the paper’s original GitHub ([https://github.com/genforce/sefa](https://github.com/genforce/sefa)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the essential libraries as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we select the parameters of the generation process such as the number
    of images to generate, and the noise seed. Please note that the `seed` parameter
    will help us to get diverse images in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have the latent semantics parameters of the GAN as proposed by **SeFa**.
    Simply, we can change some semantics of the synthesized image by changing these
    parameters. For example, we can change the painting style, gender, posture, and
    other semantics of the generated image. For more details about **SeFa**, please
    refer to *Closed-Form Factorization of Latent Semantics in* *GANs* ([https://arxiv.org/abs/2007.06600](https://arxiv.org/abs/2007.06600)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stylegan_animeface512`: This can be used to generate anime faces with diverse
    expressions. For more details, please refer to *A Style-Based Generator* *Architecture
    for Generative Adversarial* *Networks* ([https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stylegan_car512`: This can be utilized to generate interesting car models.
    We will use this model in our example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stylegan_cat256`: We can leverage this model to generate photorealistic cat
    images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pggan_celebahq1024`: This is a **Progressive Growing GAN** (**PGGAN**) that
    was trained to generate photorealistic celebrity images. For more details, please
    refer to *Progressive Growing of GANs for Improved Quality, Stability, and* *Variation*
    ([https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stylegan_bedroom256`: This can be deployed to generate bedroom layout images.
    For more details, please refer to *Analyzing and Improving the Image Quality of*
    *StyleGAN* ([https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We select the model name that we want to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to load the generator of GAN. Please remember that we do not
    need the discriminator to generate the images. It is only used to help the generator
    to train on generating the images that we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we send the code to the generator to sample from the latent space. The
    code is simply random noise. It is the random noise vector, z, which we saw in
    *Figure 7**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we synthetize the image by sending the noise vector (code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us visualize the output of the GAN in *Figure 7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Images generated using StyleGAN](img/Figure_07_02_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Images generated using StyleGAN
  prefs: []
  type: TYPE_NORMAL
- en: 'After changing the latent semantic parameters as described by **SeFa**, we
    get the outputs shown in *Figure 7**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – SeFa approach for controlling generation process by changing
    latent semantic parameters](img/Figure_07_03_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – SeFa approach for controlling generation process by changing latent
    semantic parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, we can generate images of anime facial expressions, celebrities’
    faces, and bedroom layouts using the aforementioned models, as shown in *Figure
    7**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – A sample of images generated using different GAN models](img/Figure_07_04_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – A sample of images generated using different GAN models
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this example, we can effortlessly utilize GANs to generate
    diverse and photorealistic data for training and testing our own ML models. Next,
    we will explore the variations of GANs that facilitate many amazing applications.
  prefs: []
  type: TYPE_NORMAL
- en: Variations of GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore the main variation of GANs. For an interesting
    practical application of GANs, please refer to [*Chapter 12*](B18494_12.xhtml#_idTextAnchor203)
    and *Case Study 3 – Predictive Analytics* to see how Amazon utilized GANs for
    fraud transaction prediction. For more applications, please refer to *Generative
    Adversarial Networks in the built environment: A comprehensive review of the application
    of GANs across data types and* *scales* ([https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089](https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089)).'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional GAN (cGAN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A typical GAN generates images given a random noise vector. However, in many
    scenarios, we really want to control the attributes and properties of the generated
    synthetic samples. For example, suppose you are deploying a GAN to generate human
    faces. The standard GAN architecture has no way to let you specify some attributes
    of the generated faces such as gender, age, eye color, and hair length. Using
    cGAN, we can condition the GAN on these attributes in the training process. Thus,
    we are able to generate synthetic samples with certain attributes. For more details,
    refer to *Conditional Generative Adversarial Nets* at [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  prefs: []
  type: TYPE_NORMAL
- en: CycleGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an image-to-image translation task that aims to transform an image from one
    domain to another, DL models usually require matching pairs or pairwise correspondences
    between images from the two domains. This is extremely difficult to achieve. For
    instance, imagine preparing such a dataset for mapping images from one season
    (winter) to another (summer). An elegant solution to the problem is using CycleGANs,
    which can be trained to perform unpaired image-to-image translation between domains
    given only two sets of images from both domains without the need for any matching
    pairs. Thus, you only need to provide images taken in winter and summer and there
    is no need to capture the same scenes in winter and summer to provide matching
    pairs. For more details, please check *Unpaired Image-to-Image Translation using
    Cycle-Consistent Adversarial* *Networks* ([https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)).
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Tabular GAN (CTGAN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CTGANs are a specific variant of GANs that can generate tabular synthetic data.
    It is very challenging for other GANs to capture the dependencies between columns
    or attributes of a given tabular dataset. A CTGAN is a cGAN that can be utilized
    to model these joint probability distributions between these columns. CTGANs have
    enormous applications in data augmentation, imputation, and anomaly detection.
    For more details, please refer to *Modeling Tabular Data using Conditional* *GAN*
    ([https://arxiv.org/abs/1907.00503](https://arxiv.org/abs/1907.00503)).
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein GAN (WGAN) and Wasserstein GAN with Gradient Penalty (WGAN-GP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WGAN and WGAN-GP are variants of the original GANs. Unlike GANs, which use a
    binary cross-entropy loss to classify real and fake samples, this variation utilizes
    Wasserstein distance to measure the distance between the real and fake data probability
    distributions. Furthermore, WGAN-GP implements a gradient penalty term to enforce
    the Lipschitz constraint on the discriminator. These two variants were shown to
    produce better results and to be more stable. For more details, check *Wasserstein
    GAN* ([https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)) and
    *Improved Training of Wasserstein* *GANs* ([https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)).
  prefs: []
  type: TYPE_NORMAL
- en: f-GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'f-GANs are another family of GANs that utilize *f*-divergences to measure and
    minimize the divergence between real and fake samples’ probability distributions.
    This variant of GANs has been widely utilized in image and text generation. For
    more details, please check *f-GAN: Training Generative Neural Samplers using Variational
    Divergence* *Minimization* ([https://arxiv.org/abs/1606.00709](https://arxiv.org/abs/1606.00709)).'
  prefs: []
  type: TYPE_NORMAL
- en: DragGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DragGANs are another recent promising variation of GANs that open the door
    for many amazing applications, such as point-based image editing. DragGANs allow
    users to generate photorealistic synthetic images in an interactive and intuitive
    manner. DragGANs stand out due to their distinctive approach to optimizing the
    latent space and their unique method of point tracking. For more information,
    please refer to *Drag Your GAN: Interactive Point-based Manipulation on the Generative
    Image* *Manifold* ([https://arxiv.org/abs/2305.10973](https://arxiv.org/abs/2305.10973)).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s wrap things up before we move on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed what GANs are, their architecture, and the
    training process. At the same time, we explored how GANs were utilized for various
    applications such as image-to-image translation. Additionally, we covered a coding
    example demonstrating how to use GANs to generate photorealistic images. In this
    chapter, we also learned about the main variations of GANs. In the next chapter,
    we will continue our learning journey by exploring another exciting approach for
    generating synthetic data by utilizing video games.
  prefs: []
  type: TYPE_NORMAL
