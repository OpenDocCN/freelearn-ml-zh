- en: Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have talked a lot about training, or teaching, machines to
    make predictions. To this end, we have employed a variety of useful and interesting
    algorithms including various types of regression, decisions trees, and nearest
    neighbors. However, let's take a step back and think about what entity we might
    want to mimic if we are trying to make accurate predictions and learn about data.
  prefs: []
  type: TYPE_NORMAL
- en: Well, the most obvious answer to this question is that we should mimic our own
    brains. We as humans have a natural ability to recognize objects, predict quantities,
    recognize frauds, and more, and these are all things that we would like machines
    to do artificially. Granted, we are not perfect at these activities, but we are
    pretty good!
  prefs: []
  type: TYPE_NORMAL
- en: This type of thinking was what lead to the development of **artificial neural
    networks** (also known as **neural networks** or just **neural nets**). These
    models attempt to roughly mimic certain structures in our brains, such as **neurons**.
    They have been widely successful across industries and are currently being applied
    to solve a variety of interesting problems.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, more specialized and complicated types of neural networks have been
    attracting a lot of interest and attention. These neural networks fall under the
    category of **deep learning** and generally have a **deeper** structure than what
    would be considered regular neural networks. That is, they have many hidden layers
    of structure and could be parameterized with tens of millions of parameters or
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: We will attempt to introduce both Go-based neural networks and deep learning
    models in this chapter. These topics are incredibly broad and there are entire
    books on deep learning alone. Thus, we will only be touching the surface here.
    That being said, this following content should provide you with a solid starting
    point to build neural networks in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural net jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a huge variety of neural network flavors, and each of these flavors
    has its own set of jargon. However, there is some common jargon that we should
    know regardless of the type of neural network that we are utilizing. This jargon
    is presented in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes**, **perceptrons**, or **neurons**: These interchangeable terms refer
    to the basic building blocks of a neural network. Each node or neuron takes in
    input data and performs an operation on this data. After performing the operation,
    the node/neuron may or may not pass the results of the operation on to other nodes/neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation**: The output or values associated with the operation of a node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: The definition of the function that transforms the
    inputs to a node into the output, or activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights** or **biases**: These values define the relationships between input
    and output data in the activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input layer**: The input layer of a neural network includes a series of nodes
    that take the initial input into the neural network model (a series of features
    or attributes, for example).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The output layer of a neural network includes a series of
    nodes that take information passed inside the neural network and transform it
    into a final output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layers**: These layers of a neural network exist between the input
    and output layers and are thus **hidden** from outside inputs or outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedforward** or **feed forward**: This refers to a scenario in which data
    is fed into the input layer of a neural network and transferred in a forward direction
    to the output layer (without any cycles).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backpropagation:** This is a method for training neural network models that
    involves feeding forward values through the network, calculating generated errors,
    and then transferring changes based on these errors back into the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architecture**: The overall structure of how neurons are connected together
    in the neural network is called an **architecture**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To solidify these concepts, consider the following schematic of a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2dedab7-b035-4fcf-b7c9-fdbec00e31de.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a basic feedforward (that is, acyclic or recursive) neural network.
    It has two hidden layers, accepts two inputs, and outputs two class values (or
    results).
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry if all this jargon seems a little bit overwhelming right now. We
    will be looking at a concrete example next that should solidify all of these pieces.
    Also, if you get down in the weeds with the various examples in this chapter and
    get confused with terminology, circle back here as a reminder.
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many neural network packages and applications of neural networks treat the models
    as **black boxes**. That is, people tend to utilize some framework that allows
    them to quickly build a neural network using a bunch of default values and automation.
    They are often able to produce some results, but this sort of convenience usually
    does not build much intuition about how the models actually work. As a result,
    when the models do not behave as expected, it's very hard to understand why they
    might be making weird predictions or having trouble converging.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into more complicated neural networks, let's build up some basic
    intuition about neural networks such that we do not fall into this pattern. We
    are going to build a simple neural network from scratch to learn about the basic
    components of neural networks and how they operate together.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: that even though we are going to build our neural net from scratch
    here (which you may also want to do in certain scenarios), there are a variety
    of Go packages that help you build, train, and make predictions with neural networks.
    These include `github.com/tleyden/neurgo`, `github.com/fxsjy/gonn`, `github.com/NOX73/go-neural`,
    `github.com/milosgajdos83/gosom`, `github.com/made2591/go-perceptron-go`, and
    `github.com/chewxy/gorgonia`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to utilize neural networks for classification in this chapter.
    However, neural nets can also be utilized to perform regression. You can learn
    more about that topic here: [https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/](https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/).'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes in the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The nodes, or neurons, of our neural network have a relatively simple functionality
    in and of themselves. Each neuron will take in one or more values (*x[1]*, *x[2]*,
    and so on), combine these values according to an activation function, and produce
    a single output. The following is the output pictured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfb6068c-26a3-4800-8b3a-7bdbf9dd764d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How exactly should we combine the inputs to get the output? Well, we need a
    method to combine the inputs that is adjustable (such that we can train the model),
    and we have already seen that combining variables using coefficients and an intercept
    is one trainable way to combine inputs. Just think back to [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*. In this spirit, we are going to combine the inputs linearly with
    some coefficients (**weights**) and an intercept (the **bias**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3feefece-dc75-4015-a1f8-e222859fdbac.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w[1]*, *w[2]*, and so on are our weights and *b* is the bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'This combination of inputs is a good start, but it is linear at the end of
    the day, and thus not able to model non-linear relationships between the input
    and output. To introduce some non-linearity, we are going to apply an activation
    function to this linear combination of inputs. The activation function that we
    will use here is similar to the logistic function that was introduced in [Chapter
    5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*. In the context
    of neural networks and in the following form, the logistic function is referred
    to as the **sigmoid** **function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad4415d6-4781-4d70-b883-b83fecd57f96.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `sigmoid` function is a good choice for use in our node because it introduces
    non-linearity, but it also has a limited range (between *0* and *1*), has a simply
    defined derivative (which we will use during training of the network), and it
    can have a probabilistic interpretation (as discussed further in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*).
  prefs: []
  type: TYPE_NORMAL
- en: The `sigmoid` function is by no means the only choice for an activation function
    in a neural network. Other popular choices include the hyperbolic tangent function,
    softmax, and rectified linear units. Choices of activation functions along with
    their advantages and disadvantages are further discussed at [https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f](https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and define our activation function in Go along with its derivative.
    These definitions are shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simple neural network that we are going to build will contain an input
    and output layer (as does any neural network). The network will include a single
    hidden layer between the input and output layer. This architecture is depicted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0b2fc3d-f452-4196-83b0-a162870f3293.png)'
  prefs: []
  type: TYPE_IMG
- en: In particular, we are going to include four nodes in the input layer, three
    nodes in the hidden layer, and three nodes in the output layer. The four nodes
    in the input layer correspond to the number of attributes that we are going to
    feed into the network. Think about these four inputs as something like the four
    measurements we used to classify iris flowers in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*. The output layer will have three nodes because we will be setting
    up our network to make classifications for the iris flowers, which could be in
    one of three classes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, regarding the hidden layer--why are we using one hidden layer with three
    nodes? Well, one hidden layer is sufficient for a very large majority of simple
    tasks. If you have a large amount of non-linearity in your data, many inputs,
    and/or large amounts of training data, you may need to introduce more hidden layers
    (as further discussed later in this chapter in relation to deep learning). The
    choice of three nodes in the hidden layer can be tuned based on an evaluation
    metric and a little trial and error. You can also search the numbers of nodes
    in the hidden layer to automate your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the more nodes you introduce into the hidden layer, the more
    perfectly your network can learn your training set. In other words, you are putting
    yourself in danger of overfitting your model such that it does not generalize.
    When adding this complication, it is important to validate your model very carefully
    to ensure that it generalizes.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we expect this architecture to work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's pause for a moment and think about why setting up a series of nodes in
    the preceding arrangement would allow us to predict something. As you can see,
    all we are doing is combining inputs over and over to produce some result. How
    can we expect this result to be useful in making binary classifications?
  prefs: []
  type: TYPE_NORMAL
- en: Well, what exactly do we mean when we define a binary classification problem,
    or any classification or regression problem, for that matter? We are basically
    saying that we expect there to be some rules or relationships that exist between
    a series of inputs (our attributes) and an output (our labels or response). In
    essence, we are saying that there exists some simple or complicated function that
    is able to transform our attributes into the response or labels that we are after.
  prefs: []
  type: TYPE_NORMAL
- en: We could make a choice for a type of function that might model this transformation
    of attributes to output, as we do in linear or logistic regression. However, we
    could also set up a series of chained and adjustable functions that we could algorithmically
    train to detect the relationships and rules between our inputs and output. This
    is what we are doing with the neural network!
  prefs: []
  type: TYPE_NORMAL
- en: The nodes of the neural network are like sub-functions that are adjusted until
    they mimic the rules and relationships between our supplied inputs and the output,
    regardless of what those rules and relationships actually are (linear, non-linear,
    dynamic, and so on). If underlying rules exist between the inputs and outputs,
    it is likely that there exists some neural network that can mimic the rules.
  prefs: []
  type: TYPE_NORMAL
- en: Training our neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, so now we have some good motivation for why this combination of nodes
    might help us make predictions. How are we actually going to adjust all of the
    sub-functions of our neural network nodes based on some input data? The answer
    is called **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation is a method for training our neural network that involves doing
    the following iteratively over a series of **epochs** (or exposure to our training
    dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: Feeding our training data forward through the neural network to calculate an
    output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating errors in the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using gradient descent (or other relevant method) to determine how we should
    change our weights and biases based on the errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagating these weight/bias changes into the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will implement this process later and go through some of the details. However,
    certain things like gradient descent are covered in more detail in the [Appendix](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml),
    *Algorithms/Techniques Related to Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: This backpropagation method of training a neural network is ubiquitous in the
    modeling world, but there are tons of unique network architectures and methods
    that we do not have space to cover here. Neural nets are an active area of research
    in academia and industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a struct `neuralNetConfig` that will contain all of the
    parameters that define our network architecture and how we will go about our backpropagation
    iterations. Let''s also define a struct `neuralNet` that will contain all of the
    information that defines a trained neural network. Later on, we will utilize trained
    `neuralNet` values to make predictions. These definitions are shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `wHidden` and `bHidden` are the weights and biases for the hidden layer
    of the network and `wOut` and `bOut` are the weights and biases for the output
    layer of the network, respectively. Note that we are using `gonum.org/v1/gonum/mat`
    matrices for all the weights and biases, and we will use similar matrices to represent
    our inputs and outputs. This will let us easily perform the operations related
    to the backpropagation and generalize our training to any number of nodes in the
    input, hidden, and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s define a function that initializes a new neural network based
    on a `neuralNetConfig` value and a method that trains a `neuralNet` value based
    on a matrix of inputs (*x*) and a matrix of labels (*y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `train()` method, we need to complete our backpropagation method and
    place the resulting trained weights and biases into the receiver. First, let''s
    initialize the weights and biases randomly in the `train()` method, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to loop over each of our epochs completing the backpropagation
    of the network. This again involves a feed forward stage in which outputs are
    calculated and a backpropagating stage in which changes to the weights and biases
    are applied. The backpropagation process will be discussed in more detail in the
    [Appendix](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml), *Algorithms/Techniques
    Related to Machine Learning*, if you are interested in diving a little deeper.
    For now, let''s focus on the implementation. The loop over epochs look like the
    following, with the various pieces of the backpropagation process indicated by
    comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the feed forward section of this loop, the inputs are propagated forward
    through our network of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, once we have an output from the feed forward process, we go backward
    through the network calculating deltas (or changes) for the output and hidden
    layers, as can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The deltas are then utilized to update the weights and biases of our network.
    A **learning rate** is also used to scale these changes, which can help the algorithm
    converge. This final piece of the backprogation loop is implemented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, here we have utilized another helper function that allows us to sum matrices
    along one dimension while keeping the other dimension intact. This `sumAlongAxis()`
    function is shown in the following code for completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing that we will do in the `train()` method is add the trained weights
    and biases to the receiver value and return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Awesome! That wasn''t so bad. We have a method to train our neural network
    in about 100 lines of Go code. To check whether this code runs and to get a sense
    about what our weights and biases might look like, let''s train a neural network
    on some simple dummy data. We will perform a more realistic example with the network
    in the next section, but for now, let''s just sanity check ourselves with some
    dummy data, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this neural network training code results in the following
    output weights and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These weights and biases fully define our trained neural network. That is, `wHidden`
    and `bHidden` allow us to translate our inputs into values that are input to the
    output layer of our network, and `wOut` and `bOut` allow us to translate those
    values into the final output of our neural net.
  prefs: []
  type: TYPE_NORMAL
- en: Because of our use of random numbers, your program may return slightly different
    results from those which are described earlier. However, you should see a similar
    range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the simple neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have some neural network training functionality that appears to
    be working, let's try to utilize this functionality in a more realistic modeling
    scenario. In particular, let's bring back our favorite classification dataset,
    the iris flower dataset (utilized in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*).
  prefs: []
  type: TYPE_NORMAL
- en: If you remember, when trying to classify iris flowers using this dataset, we
    are trying to classify them into one of three species (setosa, virginica, or versicolor).
    As our neural net is expecting matrices of float values, we need to encode the
    three species into numerical columns. One way to do this is to create a column
    in our dataset for each species. We will then set that column's values to either
    *1.0* or *0.0* depending on whether the corresponding row's measurements correspond
    to that species (*1.0*) or to another species (*0.0*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also going to standardize our data for reasons similar to those that
    were discussed in reference to logistic regression in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*. This makes our dataset look slightly different than it did for
    previous examples, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use an 80/20 training and test split of our data to evaluate the model.
    The training data will be stored in `train.csv` and the testing data will be stored
    in `test.csv`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Training the neural network on real data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train our neural network on this iris data, we need to read in the training
    data and create two matrices. The first matrix will hold all of the attributes
    (matrix `inputs`), and the second matrix will hold all the labels (matrix `labels`).
    We are going to construct these matrices as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then initialize and train our neural network, similar to how we did
    with the dummy data. The only difference is that we are going to use three output
    neurons corresponding to our three output classes. The training process is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make predictions with the trained neural network, we will create a `predict()`
    method for the `neuralNet` type. This `predict` method will take in new inputs
    and complete and feed the new inputs forward through the network to produce predicted
    outputs, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can then use this `predict()` method on the testing data to evaluate our
    trained model. Specifically, we will read in the data in `test.csv` into two new
    matrices `testInputs` and `testLabels`, similar to how we read in `train.csv`
    (as such, we will not include the details here). `testInputs` can be supplied
    to the `predict()` method to get our predictions and then we can compare our predictions
    and `testLabels` to calculate an evaluation metric. In this case, we will calculate
    the accuracy of our model.
  prefs: []
  type: TYPE_NORMAL
- en: One detail that we will take care of as we calculate the accuracy is determining
    a single output prediction from our model. The network actually produces one output
    between 0.0 and 1.0 for each of the iris species. We will take the highest of
    these as the predicted species.
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculation of accuracy for our model is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the evaluation yields something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Yeah! That's pretty good. 97% accuracy in our predictions for the iris flower
    species with our from-scratch neural network. As mentioned earlier, these single
    hidden layer neural networks are pretty powerful when it comes to a majority of
    classification tasks, and you can see that the underlying principles are not really
    that complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although simple neural networks, like the one utilized in the preceding section,
    are extremely powerful for many scenarios, deep neural network architectures have
    been applied across industries in recent years on various types of data. These
    more complicated architectures have been used to beat champions at board/video
    games, drive cars, generate art, transform images, and much more. It almost seems
    like you can throw anything at these models and they will do something interesting,
    but they seem to be particularly well-suited for computer vision, speech recognition,
    textual inference, and other very complicated and hard-to-define tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to introduce deep learning here and run a deep learning model in
    Go. However, the application and diversity of deep learning models is huge and
    growing every day.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many books and tutorials on the subject, so if this brief introduction
    sparks your interest, we recommend that you look into one of the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-On Deep Learning with TensorFlow: [https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Learning by Example: [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Learning for Computer Vision: [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These resources might not all mention or acknowledge Go but, as you will see
    in the following example, Go is perfectly capable of applying these techniques
    and interfacing with things like TensorFlow or H2O.
  prefs: []
  type: TYPE_NORMAL
- en: What is a deep learning model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep in deep learning refers to the successive combination and utilization
    of various neural network structures to form an architecture that is large and
    complicated. These large and complicated architectures generally require large
    amounts of data to train, and the resulting structure is very hard to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give an example of the scale and complexity of modern deep learning models,
    take Google''s LeNet ([http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf))
    as an example. This model, which can be utilized for object recognition, is depicted
    here (image courtesy of [https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b4148b1-6241-4279-8d3e-9190f72f1e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: This structure is hard to digest in and of itself. However, it is made even
    more impressive when you realize that each of the blocks in this diagram can be
    a complicated, neural network primitive in and of itself (as shown in the legend).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, deep learning models are constructed by chaining together and, in
    some cases, rechaining together, one or more of the following structures:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A fully connected neural network architecture: This building block might be
    something similar to what we built previously in this chapter for iris flower
    detection. It includes a number of neural network nodes arranged in fully connected
    layers. That is, the layers flow from one to the other and all the nodes are connected
    from one layer to the next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **convolutional neural network** (**ConvNet** or **CNN**): This is a neural
    network that implements at least one **convolutional layer**. A convolutional
    layer contains neurons parameterized by a set of weights (also called a convolutional
    kernel or filter) that is only partially connected to the input data. Most ConvNets
    include a hierarchical combination of these convolutional layers allows a ConvNet
    to respond to low level features in the input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **recurrent neural network** and/or **Long Short-Term Memory** (**LSTM**)
    **cells**: These components attempt to factor in something like memory into the
    model. Recurrent neural nets make their output dependent on a sequence of inputs
    rather than assuming that every input is independent. LSTM cells are related to
    recurrent neural networks but also try to factor in a longer term sort of memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although we will utilize image data to illustrate deep learning in this chapter,
    deep learning models using recurrent neural nets and LSTMs are particularly well-adapted
    at solving text-based and other NLP problems. The memory that is built into these
    networks allows them to understand what words might come next in a sentence, for
    example. If you are interested in running a deep learning model that is based
    on text data, we recommend looking at this example that uses the `github.com/chewxy/gorgonia`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/chewxy/gorgonia/tree/master/examples/charRNN.](https://github.com/chewxy/gorgonia/tree/master/examples/charRNN)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning models are powerful! Especially for tasks like computer vision.
    However, you should also keep in mind that complicated combinations of these neural
    net components are also extremely hard to interpret. That is, determining why
    the model made a certain prediction can be near impossible. This can be a problem
    when you need to maintain compliance in certain industries and jurisdictions,
    and it also might inhibit debugging or maintenance of your applications. That
    being said, there are some major efforts to improve the interpretability of deep
    learning models. Notable among these efforts is the LIME project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/marcotcr/lime.](https://github.com/marcotcr/lime)'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a variety of options when you are looking to build or utilize deep
    learning models from Go. This, as with deep learning itself, is an ever changing
    landscape. However, the options for building, training and utilizing deep learning
    models in Go are generally as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use a Go package**: There are Go packages that allow you to use Go as your
    main interface to build and train deep learning models. The most features and
    developed of these packages is `github.com/chewxy/gorgonia`. `github.com/chewxy/gorgonia`
    treats Go as a first-class citizen and is written in Go, even if it does make
    significant usage of `cgo` to interface with numerical libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use an API or Go client for a non-Go DL framework**: You can interface with
    popular deep learning services and frameworks from Go including TensorFlow, MachineBox,
    H2O, and the various cloud providers or third-party API offerings (such as IBM
    Watson). TensorFlow and Machine Box actually have Go bindings or SDKs, which are
    continually improving. For the other services, you may need to interact via REST
    or even call binaries using `exec`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use cgo**: Of course, Go can talk to and integrate with C/C++ libraries for
    deep learning, including the TensorFlow libraries and various libraries from Intel.
    However, this is a difficult road, and it is only recommended when absolutely
    necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As TensorFlow is by far the most popular framework for deep learning (at the
    moment), we will briefly explore the second category listed here. However, the
    Tensorflow Go bindings are under active development and some functionality is
    quite crude at the moment. The TensorFlow team recommends that if you are going
    to use a TensorFlow model in Go, you first train and export this model using Python.
    That pre-trained model can then be utilized from Go, as we will demonstrate in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of members of the community working very hard to make Go
    more of a first-class citizen for TensorFlow. As such, it is likely that the rough
    edges of the TensorFlow bindings will be smoothed over the coming year. To keep
    updated on this front, make sure and join the #data-science channel on Gophers
    Slack ([https://invite.slack.golangbridge.org/](https://invite.slack.golangbridge.org/)).
    This is a frequent topic of conversation there and would be a great place to ask
    questions and get involved.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though we will explore a quick TensorFlow example here, we highly recommend
    that you should look into `github.com/chewxy/gorgonia`, especially if you are
    considering doing more custom or extensive modeling in Go. This package is super
    powerful and is used in production.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up TensorFlow for use with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow team has provided some good docs to install TensorFlow and get
    it ready for usage with Go. These docs can be found at [https://www.tensorflow.org/install/install_go](https://www.tensorflow.org/install/install_go).
    There are a couple of preliminary steps, but once you have the TensorFlow C libraries
    installed, you can get the following Go package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything should be good to go if you were able to get `github.com/tensorflow/tensorflow/tensorflow/go`
    without error, but you can make sure that you are ready to use TensorFlow by executing
    the following tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Retrieving and calling a pretrained TensorFlow model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The model that we are going to use is a Google model for object recognition
    in images called Inception. The model can be retrieved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: After unzipping the compressed model, you should see a `*.pb` file. This is
    a `protobuf` file that represents a frozen state of the model. Think back to our
    simple neural network. The network was fully defined by a series of weights and
    biases. Although more complicated, this model can be defined in a similar way
    and these definitions are stored in this `protobuf` file.
  prefs: []
  type: TYPE_NORMAL
- en: To call this model, we will use some example code from the TensorFlow Go bindings
    docs--[https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go).
    This code loads the model and uses the model to detect and label the contents
    of a `*.jpg` image.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the code is included in the TensorFlow docs, I will spare the details and
    just highlight a couple of snippets. To load the model, we perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the graph definition of the deep learning model and create a new
    TensorFlow session with the graph, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can make an inference using the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Object detection using TensorFlow from Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Go program for object detection, as specified in the TensorFlow GoDocs,
    can be called as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When the program is called, it will utilize the pretrained and loaded model
    to infer the contents of the specified image. It will then output the most likely
    contents of that image along with its calculated probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, let''s try performing the object detection on the following
    image of an airplane, saved as `airplane.jpg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9a16e04-62b2-4c65-b75e-ce06878aa534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Running the TensorFlow model from Go gives the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After some suggestions about speeding up CPU computations, we get a result:
    `airliner`. Wow! That''s pretty cool. We just performed object recognition with
    TensorFlow right from our Go program!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let try another one. This time, we will use `pug.jpg`, which looks like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3701af5-dc11-4561-9bad-135d546807e6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Running our program again with this image gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Success! Not only did the model detect that there was a dog in the picture,
    it correctly identified that there was a pug dog in the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let try just one more. As this is a Go book, we cannot resist trying `gopher.jpg`,
    which looks like the following (huge thanks to Renee French, the artist behind
    the Go gopher):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb3144d9-4aef-4af3-b3f4-cd22b41df16f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Running the model gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Well, I guess we can't win them all. Looks like we need to refactor our model
    to be able to recognize Go gophers. More specifically, we should probably add
    a bunch of Go gophers to our training dataset, because a Go gopher is definitely
    not a safety pin!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Basic neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick introduction to neural networks: [https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A friendly introduction to deep learning and neural networks: [https://youtu.be/BR9h47Jtqyw](https://youtu.be/BR9h47Jtqyw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/tleyden/neurgo` docs: [https://godoc.org/github.com/tleyden/neurgo](https://godoc.org/github.com/tleyden/neurgo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/fxsjy/gonn/gonn` docs: [https://godoc.org/github.com/fxsjy/gonn/gonn](https://godoc.org/github.com/fxsjy/gonn/gonn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More detailed deep learning resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On Deep Learning with TensorFlow - [https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Learning by Example - [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Learning for Computer Vision - [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep learning with Go:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow Go bindings docs: [https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/chewxy/gorgonia` docs: [https://godoc.org/github.com/chewxy/gorgonia](https://godoc.org/github.com/chewxy/gorgonia)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MachineBox Go SDK docs: [https://godoc.org/github.com/machinebox/sdk-go](https://godoc.org/github.com/machinebox/sdk-go)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calling pretrained models using `cgo` example: [https://github.com/gopherdata/dlinfer](https://github.com/gopherdata/dlinfer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! We have gone from parsing data with Go to calling deep learning
    models from Go. You now know the basics of neural networks and can implement them
    and utilize them in your Go programs. In the next chapter, we will discuss how
    to get these models and applications off of your laptops and run them at production
    scale in data pipelines.
  prefs: []
  type: TYPE_NORMAL
