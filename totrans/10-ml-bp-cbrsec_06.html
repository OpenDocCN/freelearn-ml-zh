<html><head></head><body>
		<div id="_idContainer064">
			<h1 id="_idParaDest-80" class="chapter-number"><a id="_idTextAnchor017"/>5</h1>
			<h1 id="_idParaDest-81">Detecting Deepfakes</h1>
			<p>In recent times, the problem of deepfakes has become prevalent on the internet. Easily accessible technology allows attackers to create images of people who have never existed, through the magic of deep neural networks! These images can be used to enhance fraudulent or bot accounts to provide an illusion of being a real person. As if deepfake images were not enough, deepfake videos are just as easy to create. These videos allow attackers to either morph someone’s face onto a different person in an existing video, or craft a video clip in which a person says something. Deepfakes are a hot research topic and have far-reaching impacts. Abuse of deepfake technology can result in misinformation, identity theft, sexual harassment, and even <span class="No-Break">political crises.</span></p>
			<p>This chapter will focus on machine learning methods to detect deepfakes. First, we will understand the theory behind deepfakes, how they are created, and what their impact can be. We will then cover two approaches to detecting deepfake images – vanilla approaches using standard machine learning models, followed by some advanced methods. Finally, as deepfake videos are major drivers of misinformation and have the most scope for exploitation, the last part of the chapter will focus on detecting <span class="No-Break">deepfake videos.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>All <span class="No-Break">about deepfakes</span></li>
				<li>Detecting <span class="No-Break">fake images</span></li>
				<li>Detecting <span class="No-Break">fake videos</span></li>
			</ul>
			<p>By the end of this chapter, you will have an in-depth understanding of deepfakes, the technology behind them, and how they can <span class="No-Break">be detected.</span></p>
			<h1 id="_idParaDest-82">Technical requirements</h1>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205"><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-83">All about deepfakes</h1>
			<p>The word <em class="italic">deepfake</em> is a combination of two words – <em class="italic">deep learning</em> and <em class="italic">fake</em>. Put simply, deepfakes are fake <a id="_idIndexMarker386"/>media created using deep learning technology. In the past decade, there have been significant advances in machine learning and generative models – models that create content instead of<a id="_idIndexMarker387"/> merely classifying it. These models (such as <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>)) can synthesize images that look real – even of <span class="No-Break">human faces!</span></p>
			<p>Deepfake technology is readily accessible to attackers and malicious actors today. It requires no sophistication or technical skills. As an experiment, head over to the website <a href="http://thispersondoesnotexist.com">thispersondoesnotexist.com</a>. This website allows you to generate images of people – people who have <span class="No-Break">never existed!</span></p>
			<p>For example, the people in the following figure are not real. They are deepfakes that have been generated by <a href="http://thispersondoesnotexist.com">thispersondoesnotexist.com</a>, and it only took a <span class="No-Break">few seconds!</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B19327_05_01.jpg" alt="Figure 5.1 – Deepfakes generated from a website"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Deepfakes generated from a website</p>
			<p>Amazing, isn’t it? Let us <a id="_idIndexMarker388"/>now understand what makes generating these images possible, and the role machine learning has to play <span class="No-Break">in it.</span></p>
			<h2 id="_idParaDest-84">A foray into GANs</h2>
			<p>Let us now look at GANs, the technology that makes <span class="No-Break">deepfakes possible.</span></p>
			<p>GANs are deep learning models that use neural networks to <a id="_idIndexMarker389"/>synthesize data rather than merely classify it. The name <em class="italic">adversarial</em> comes from the architectural design of these models; a GAN architecture consists of two neural networks, and we can force them into a cat-and-mouse game to generate synthetic media that can be passed off <span class="No-Break">as real.</span></p>
			<h3>GAN architecture</h3>
			<p>A GAN consists of two main parts – the generator and the discriminator. Both models are deep neural <a id="_idIndexMarker390"/>networks. Synthetic images are generated by plotting these networks against <span class="No-Break">each other:</span></p>
			<ul>
				<li><strong class="bold">Generator</strong>: The generator is the model that learns to generate real-looking data. It takes a fixed-length random vector as input and generates an output in the target domain, such as an image. The random vector is sampled randomly from a Gaussian distribution (that is, a standard normal distribution, where most observations cluster around the mean, and the further away an observation is from the mean, the lower the probability of it <span class="No-Break">occurring is).</span></li>
				<li><strong class="bold">Discriminator</strong>: The discriminator is a traditional machine learning model. It takes in data samples and classifies them as real or fake. Positive examples (those labeled as real) come from a training dataset, and negative examples (those labeled as fake) come from <span class="No-Break">the generator.</span></li>
			</ul>
			<h3>GAN working</h3>
			<p>Generating images is an <a id="_idIndexMarker391"/>unsupervised machine learning task and classifying the images is a supervised one. By training both the generator and discriminator jointly, we refine both and are able to obtain a generator that can generate samples good enough to fool <span class="No-Break">the discriminator.</span></p>
			<p>In joint training, the generator first generates a batch of sample data. These are treated as negative samples and augmented with images from a training dataset as positive samples. Together, these are used to fine-tune the discriminator. The discriminator model is updated to change parameters based on this data. Additionally, the discriminator loss (i.e., how well the generated images fooled the discriminator) is fed back to the generator. This loss is used to update the generator to generate <span class="No-Break">better data.</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B19327_05_02.jpg" alt="Figure 5.2 – How a GAN model works"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – How a GAN model works</p>
			<p>At first, the generator <a id="_idIndexMarker392"/>produces random data that is clearly noisy and of poor quality. The discriminator learns to easily discern between real and fake examples. As the training progresses, the generator starts to get better as it leverages the signal from the discriminator, changing its generation parameters accordingly. In the end, in an ideal situation, the generator will have been so well trained that the generator loss decreases and the discriminator loss begins increasing, indicating that the discriminator can no longer effectively distinguish between actual data and <span class="No-Break">generated data.</span></p>
			<h2 id="_idParaDest-85">How are deepfakes created?</h2>
			<p>Along with <a id="_idIndexMarker393"/>being a portmanteau of the words <em class="italic">deep learning</em> and <em class="italic">fake</em>, the word <em class="italic">deepfake</em> has another origin. The very first deepfake video was created by a Reddit user by the name of <strong class="source-inline">r/deepfakes</strong>. This user used the open source implementation provided by Google to swap the face of several actresses into pornographic videos. Much of the amateur deepfakes in the wild today build upon this code to <span class="No-Break">generate deepfakes.</span></p>
			<p>In general, any deepfake creation entails the<a id="_idIndexMarker394"/> following <span class="No-Break">four steps:</span></p>
			<ol>
				<li>Analyzing the source image, identifying the area where the face is located, and cropping the image to <span class="No-Break">that area.</span></li>
				<li>Computing features that are typically representations of the cropped image in a latent low-dimensional space, thus encoding the image into a <span class="No-Break">feature vector.</span></li>
				<li>Modifying the generated feature vector based on certain signals, such as the <span class="No-Break">destination image.</span></li>
				<li>Decoding the modified vector and blending the image into the <span class="No-Break">destination frame.</span></li>
			</ol>
			<p>Most deepfake generation methods rely on neural networks and, in particular, the encoder-decoder architecture. The encoder transforms an image into a lower dimensional subspace and maps it to a latent vector (similar to the context vectors we described when discussing transformers). This latent vector captures features about the person in the picture, such as color, expression, facial structure, and body posture. The decoder does the reverse mapping and converts the latent representation into the target image. Adding a GAN into the mix leads to a much more robust and powerful <span class="No-Break">deepfake generator.</span></p>
			<p>The first commercial application of deepfakes started with the development of FakeApp in January 2018. This is a desktop application that allows users to create videos with faces swapped for other faces. It is based on autoencoder architecture and consists of two encoder-decoder pairs. One encoder-decoder pair is trained on the images of the source image, and the other is trained on the images of the target. However, both encoders have shared common weights; in simpler terms, the same encoder is used in both autoencoders. This means that the latent vector representation for both images is in the same context, hence representing similar features. At the end of the training period, the common encoder will have learned to identify common features in <span class="No-Break">both faces.</span></p>
			<p>Let’s say A is the source image (the original image of our victim) and B is the target image (the one where we want to<a id="_idIndexMarker395"/> insert A). The high-level process to generate deepfakes using this methodology is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Obtain multiple images of A and B using data augmentation techniques and image transformations so that the same picture from multiple viewpoints <span class="No-Break">is considered.</span></li>
				<li>Train the first autoencoder model on images <span class="No-Break">of A.</span></li>
				<li>Extract the encoder from the first autoencoder. Use this to train a second autoencoder model on images <span class="No-Break">of B.</span></li>
				<li>At the end of training, we have two autoencoders with a shared encoder that can identify common features and characteristics in the images of A <span class="No-Break">and B.</span></li>
				<li>To produce the deepfake, pass image A through the common encoder and obtain the <span class="No-Break">latent representation.</span></li>
				<li>Use the decoder from the second autoencoder (i.e., the decoder for images of B) to decode this back into the deepfake image. The following shows a diagram of <span class="No-Break">this process:</span></li>
			</ol>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B19327_05_03.jpg" alt="Figure 5.3 – Deepfake creation methodology"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Deepfake creation methodology</p>
			<p>FakeApp has been<a id="_idIndexMarker396"/> widely used and inspired many other open source implementations, such as DeepFaceLab, FaceSwap, <span class="No-Break">and Dfaker.</span></p>
			<h2 id="_idParaDest-86">The social impact of deepfakes</h2>
			<p>While deepfake<a id="_idIndexMarker397"/> technology is certainly revolutionary in the field of machine learning and deep learning in particular, it has a far-reaching societal impact. Since their inception, deepfakes have been used for both benign and malicious purposes. We will now discuss both briefly. Understanding the full impact of deepfakes is essential to their study, especially for machine learning practitioners in the <span class="No-Break">cybersecurity industry.</span></p>
			<h3>Benign</h3>
			<p>Not all deepfakes are bad. There are some very good reasons why the use of deepfakes may be warranted and, at times, beneficial. Deepfakes have shown to be of great utility in <span class="No-Break">some fields:</span></p>
			<ul>
				<li><strong class="bold">Entertainment</strong>: Deepfake technology is now accessible to everyone. Powerful pre-trained models allow the<a id="_idIndexMarker398"/> exposure of endpoints through apps on smartphones, where they have been widely used to create entertaining videos. Such deepfakes include comedy videos where cartoons say certain dialogue, images where the faces of two people are swapped, or filters that generate human faces morphed <span class="No-Break">into animals.</span></li>
				<li><strong class="bold">Resurrection</strong>: An innovative use case of deepfakes has been reviving the deceased. Deepfakes can portray deceased people saying certain things, in their own voice! This is like magic, especially for someone who has no videographic memories of themselves left in this world. Deepfakes have also been used to create images that are able to portray how someone would have looked in a few years' or <span class="No-Break">decades' time.</span></li>
			</ul>
			<h3>Malicious</h3>
			<p>Unfortunately, however, every coin has two sides. The same deepfake technology that can power entertainment and <a id="_idIndexMarker399"/>enable resurrected digital personas can be used maliciously by attackers. Here are a few attack vectors that practitioners in this space should be <span class="No-Break">aware of:</span></p>
			<ul>
				<li><strong class="bold">Misinformation</strong>: This has probably been the biggest use of deepfakes and the most challenging one to solve. Because deepfakes allow us to create videos of someone saying things they did not, this has been used to create videos that spread fake news. For example, a video featuring a surgeon general saying that vaccines are harmful and cause autism would certainly provoke widespread fear and lead people to believe that it is true. Malicious entities can create and disseminate such deepfakes to further their own causes. This can also lead to political crises and loss of life. In 2022, a deepfake video featuring the Ukrainian president Volodymyr Zelenskyy was created by Russian groups, where the former was shown to accept defeat and ask soldiers to stand down – this was not the case, but the video spread like wildfire on social media before it <span class="No-Break">was removed.</span></li>
				<li><strong class="bold">Fraud</strong>: Traditionally, many<a id="_idIndexMarker400"/> applications depended on visual identification verification. After the COVID-19 pandemic, most of these transitioned to verifying identities through documents and video online. Deepfakes have tremendous potential to be used for identity theft here; by crafting a deepfake video, you can pretend to be someone you are not and bypass automatic identity verification systems. In June 2022, the <strong class="bold">Federal Bureau of Investigation</strong> (<strong class="bold">FBI</strong>) published a public service announcement warning companies of deepfakes being used in online interviews and during remote work. Deepfakes can also be used to create sock puppet accounts on social media websites, GANs can be used to generate images, and deepfake videos can be used to enrich profiles with media (videos showing the person talking) so that they <span class="No-Break">appear real.</span></li>
				<li><strong class="bold">Pornography</strong>: The very first deepfake that was created was a pornographic movie. Revenge porn is an alarmingly growing application of deepfakes in today’s world. By using images of a person and any pornographic clip as a base, it is possible to depict the said person in the pornographic clip. The naked eye may not be able to ascertain that the video is <span class="No-Break">a spoof.</span></li>
			</ul>
			<h1 id="_idParaDest-87">Detecting fake images</h1>
			<p>In the previous <a id="_idIndexMarker401"/>section, we looked at how deepfake images and videos can be generated. As the technology to do so is accessible to everyone, we also discussed the impact that this can have at multiple levels. Now, we will look at how fake images can be detected. This is an important problem to solve and has far-reaching impacts on social media and the internet <span class="No-Break">in general.</span></p>
			<h2 id="_idParaDest-88">A naive model to detect fake images</h2>
			<p>We know that machine<a id="_idIndexMarker402"/> learning has driven significant progress in the domain of image processing. Convolutional neural networks (CNNs) have surpassed prior image detectors and achieved accuracy even greater than that of humans. As a first step toward detecting deepfake images, we will treat the task as a simple binary classification and use standard deep learning image <span class="No-Break">classification approaches.</span></p>
			<h3>The dataset</h3>
			<p>There are several publicly <a id="_idIndexMarker403"/>available datasets for deepfake detection. We will use the 140k Real and Fake Faces Dataset. This dataset is freely available to download from <span class="No-Break">Kaggle (</span><a href="https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces"><span class="No-Break">https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces</span></a><span class="No-Break">).</span></p>
			<p>As the name suggests, the dataset consists of 140,000 images. Half of these are real faces from Flickr, and the remaining half are fake ones generated from a GAN. The real faces have been collected by researchers from NVIDIA, and there is significant coverage of multiple ethnicities, age groups, and accessories in <span class="No-Break">the images.</span></p>
			<p>The dataset is fairly large (~4 GB). You will need to download the compressed dataset from Kaggle, unzip it, and store it locally. The root directory consists of three folders – one each for training, testing, and <span class="No-Break">validation data.</span></p>
			<h3>The model</h3>
			<p>We are going to use a CNN model for classification. These<a id="_idIndexMarker404"/> neural networks specialize in understanding and classifying data in which the spatial <a id="_idIndexMarker405"/>representation of data matters. This makes it ideal for processing images, as images are fundamentally matrices of pixels arranged in a grid-like topology. A CNN typically has three main layers – convolution, pooling, <span class="No-Break">and classification:</span></p>
			<ul>
				<li><strong class="bold">Convolution layer</strong>: This is the <a id="_idIndexMarker406"/>fundamental building block of a CNN. This layer traverses through an image and generates a<a id="_idIndexMarker407"/> matrix known as an <strong class="bold">activation map</strong>. Each convolution multiplies some part of the image with a kernel matrix (the weights in the matrix are parameters that can be learned using gradient descent and backpropagation). The convolution matrix slides across the image row by row and performs the dot product. The convolution layer <a id="_idIndexMarker408"/>aggregates multiple neighborhoods of the image and produces a condensed output, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B19327_05_04.jpg" alt="Figure 5.4 – The convolution layer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – The convolution layer</p>
			<ul>
				<li><strong class="bold">Pooling layer</strong>: The <a id="_idIndexMarker409"/>pooling layer performs aggregations over the convolution output. It calculates a summary statistic over a<a id="_idIndexMarker410"/> neighborhood and replaces the neighborhood cells with the summary. The statistic can be the mean, max, median, or any other standard metric. Pooling reduces redundancy in the convolutional representation and reduces dimensions by summarizing over multiple elements, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B19327_05_05.jpg" alt="Figure 5.5 – The pooling layer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – The pooling layer</p>
			<ul>
				<li><strong class="bold">Fully connected layer</strong>: The output from the pooling layer is flattened and converted into a one-dimensional <a id="_idIndexMarker411"/>vector. This is done simply by appending the rows to each other. This vector is now passed to a fully connected neural network, at the end of <a id="_idIndexMarker412"/>which is a <strong class="bold">softmax layer</strong>. This layer operates as a standard neural <a id="_idIndexMarker413"/>network, with weights being updated with gradient descent and backpropagation through multiple epochs. The softmax layer will output a probability distribution of the predicted class of the image, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B19327_05_06.jpg" alt="Figure 5.6 – Flattening and a fully connected layer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Flattening and a fully connected layer</p>
			<p>While training, data flows into the network and undergoes convolution and pooling. We can stack multiple convolution-pooling layers one after the other; the hope is that each layer will learn something new from the image and obtain more and more specific representations. The pooling output after the last convolution layer flows into the fully connected neural network (which can also consist of multiple layers). After the softmax output, a loss is calculated, which then flows back through the whole network. All layers, including the convolution layers, update their weights, using gradient descent to minimize <span class="No-Break">this loss.</span></p>
			<h3>Putting it all together</h3>
			<p>We will now use a<a id="_idIndexMarker414"/> CNN model to detect deepfakes and run this experiment on the <span class="No-Break">140k dataset.</span></p>
			<p>First, we import the <span class="No-Break">required libraries:</span></p>
			<pre class="source-code">
# Data Processing
import numpy as np
import pandas as pd
import scipy.misc
from sklearn.datasets import load_files
import matplotlib.pyplot as plt
%matplotlib inline
# Deep Learning Libraries
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape
from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D
from keras_vggface.vggface import VGGFace
from keras_vggface import utils
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils</pre>
			<p>Recall that the first <a id="_idIndexMarker415"/>step in the data science pipeline is data preprocessing. We need to parse the images we downloaded and convert them into a form suitable for consumption by the CNN model. Fortunately, the <strong class="source-inline">ImageDataGenerator</strong> class in the <strong class="source-inline">keras</strong> library helps us do just that. We will use this library and define our training and <span class="No-Break">test datasets:</span></p>
			<pre class="source-code">
training_data_path = ""
test_data_path = ""
batch_size = 64
print("Loading Train…")
training_data = ImageDataGenerator(rescale = 1./255.) .flow_from_directory(
    training_data_path,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='binary'
)
print("Loading Test…")
test_data = ImageDataGenerator(rescale = 1./255.)
.flow_from_directory(
    test_data_path,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='binary'
)</pre>
			<p>After you run this, you <a id="_idIndexMarker416"/>should see an output somewhat <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Loading Train…
Found 100000 images belonging to 2 classes.
Loading Test…
Found 20000 images belonging to 2 classes.</pre>
			<p>Now that the data is preprocessed, we can define the actual model and specify the architecture of the CNN. Our model will consist of convolution, pooling, and the fully connected layer, as <span class="No-Break">described earlier:</span></p>
			<pre class="source-code">
input_shape = (224,224,3)
epsilon=0.001
dropout = 0.1
model = Sequential()
# Convolution and Pooling -- 1
model.add(BatchNormalization(input_shape=input_shape))
model.add(Conv2D(filters=16, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization(epsilon=epsilon))
# Convolution and Pooling -- 2
model.add(Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization(epsilon=epsilon))
model.add(Dropout(dropout))
#Convolution and Pooling -- 3
model.add(Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization(epsilon=epsilon))
model.add(Dropout(dropout))
# Aggregation
model.add(GlobalAveragePooling2D())
# Fully Connected Layer
model.add(Dense(1, activation='sigmoid'))</pre>
			<p>Let us take a closer look at what we did here. First, we set some basic model parameters and defined an empty model using the <strong class="source-inline">Sequential</strong> class. This model will be our base, where we pile on different layers to build the <span class="No-Break">whole architecture.</span></p>
			<p>Then, we defined our <strong class="bold">convolution layers</strong>. Each <a id="_idIndexMarker417"/>convolution layer has a <strong class="bold">max pooling</strong> <strong class="bold">layer</strong>, followed by a <strong class="bold">normalization layer</strong>. The normalization layer <a id="_idIndexMarker418"/>simply <a id="_idIndexMarker419"/>normalizes and rescales the <a id="_idIndexMarker420"/>pooled output. This results in stable gradients and avoids <span class="No-Break">exploding loss.</span></p>
			<p>Then, we added an aggregation layer using the <strong class="source-inline">GlobalAveragePooling2D</strong> class. This will concatenate the previous output into a one-dimensional vector. Finally, we have a fully connected layer at the end with a softmax activation; this layer is responsible for the <span class="No-Break">actual classification.</span></p>
			<p>You can take a look at your model architecture by printing <span class="No-Break">the summary:</span></p>
			<pre class="source-code">
model.summary()</pre>
			<p>Finally, we can train the model we defined on the data <span class="No-Break">we preprocessed:</span></p>
			<pre class="source-code">
model.compile(loss='binary_crossentropy',optimizer=Adam(0.0001), metrics=['accuracy'])
training_steps = 40000//batch_size
num_epochs = 10
history = model.fit_generator(
    training_data,
    epochs=num_epochs,
    steps_per_epoch = training_steps
)</pre>
			<p>This will take about 30 minutes to an hour to complete. Of course, the exact time will depend on your processor, scheduling, and <span class="No-Break">system usage.</span></p>
			<p>Once the model is trained, we can use it to make predictions for our test data and compare the predictions with the <span class="No-Break">ground truth:</span></p>
			<pre class="source-code">
y_pred = model.predict(test_data)
y_actual = test_data.classes</pre>
			<p>Now, you can plot<a id="_idIndexMarker421"/> the confusion matrix, just like we did for the other models in the <span class="No-Break">previous chapters.</span></p>
			<h3>Playing around with the model</h3>
			<p>In the previous section, we looked at how an end-to-end CNN model can be defined and how to train it. While <a id="_idIndexMarker422"/>doing so, we made several design choices that can potentially affect the performance of our model. As an experiment, you should explore these design choices and rerun the experiment with different parameters. We will not go through the analysis and hyperparameter tuning in detail, but we will discuss some of the parameters that can be tweaked. We will leave it up to you to test it out, as it is a good learning exercise. Here are some things that you could try out to play around with <span class="No-Break">the model:</span></p>
			<ul>
				<li><strong class="bold">Convolutional layers</strong>: In our<a id="_idIndexMarker423"/> model, we have three layers of convolution and max pooling. However, you can extend this to as many (or as few) layers as you want. What happens if you have 10 layers? What happens if you have only one? How does the resulting confusion <span class="No-Break">matrix change?</span></li>
				<li><strong class="bold">Pooling layers</strong>: We used <a id="_idIndexMarker424"/>max pooling in our model. However, as discussed in the CNN architecture, pooling can leverage any statistical aggregation. What happens if we use mean pooling or min pooling instead of max pooling? How does the resulting confusion <span class="No-Break">matrix change?</span></li>
				<li><strong class="bold">Fully connected layers</strong>: Our model has just one fully connected layer at the end. However, this does not have to<a id="_idIndexMarker425"/> be the case; you can have a full-fledged neural network with multiple layers. You should examine what happens if the number of layers <span class="No-Break">is increased.</span></li>
				<li><strong class="bold">Dropout</strong>: Note that each layer is followed by a dropout layer. Dropout is a technique used in neural networks to avoid overfitting. A certain fraction of the weights is randomly <a id="_idIndexMarker426"/>set to 0; these are considered to be “dropped out.” Here, we have a dropout fraction of 0.1. What happens if it is increased? What happens if it is set to 0 (i.e., with no dropout <span class="No-Break">at all)?</span></li>
			</ul>
			<p>This completes our<a id="_idIndexMarker427"/> discussion of deepfake image detection. Next, we will see how deepfakes transitioned from images to videos, and we will look at methods for <span class="No-Break">detecting them.</span></p>
			<h1 id="_idParaDest-89">Detecting deepfake videos</h1>
			<p>As if deepfake images<a id="_idIndexMarker428"/> were not enough, deepfake videos are now revolutionizing the internet. From benign uses such as comedy and entertainment to malicious uses such as pornography and political unrest, deepfake videos are taking social media by storm. Because deepfakes appear so realistic, simply looking at a video with the naked eye does not provide any clues as to whether it is real or fake. As a machine learning practitioner working in the security field, it is essential to know how to develop models and techniques to identify <span class="No-Break">deepfake videos.</span></p>
			<p>A video can be thought of <a id="_idIndexMarker429"/>as an extension of an image. A video is multiple images arranged one after the other and viewed in quick succession. Each such image is known as a frame. By viewing the frames at a high speed (multiple frames per second), we see <span class="No-Break">images moving.</span></p>
			<p>Neural networks cannot directly process videos – there does not exist an appropriate method to encode images and convert them into a form suitable for consumption by machine learning models. Therefore, deepfake video detection involves deepfake image detection on each frame of the video. We will look at the succession of frames, examine how they evolve, and determine whether the transformations and movements are normal or similar to those expected in <span class="No-Break">real videos.</span></p>
			<p>In general, detecting video deepfakes follows the <span class="No-Break">following pattern:</span></p>
			<ol>
				<li>Parsing the video file and decomposing it into <span class="No-Break">multiple frames.</span></li>
				<li>Reading frames up to a maximum number of frames. If the number of frames is less than the maximum set, pad them with <span class="No-Break">empty frames.</span></li>
				<li>Detecting the faces in <span class="No-Break">each frame.</span></li>
				<li>Cropping the faces in <span class="No-Break">each frame.</span></li>
				<li>Training a model with a sequence of cropped faces (one face obtained per frame) as input and real/fake labels <span class="No-Break">as output.</span></li>
			</ol>
			<p>We will leverage these <a id="_idIndexMarker430"/>steps to detect fake videos in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-90">Building deepfake detectors</h2>
			<p>In this section, we will look<a id="_idIndexMarker431"/> at how to build models to detect <span class="No-Break">deepfake videos.</span></p>
			<h3>The dataset</h3>
			<p>We will use the dataset from the Kaggle Deepfake Detection Challenge (<a href="https://www.kaggle.com/competitions/deepfake-detection-challenge/overview">https://www.kaggle.com/competitions/deepfake-detection-challenge/overview</a>). As part of the challenge, three<a id="_idIndexMarker432"/> datasets were provided – training, testing, and validation. With prizes worth $1 million, participants were required to submit code and output files, which were evaluated on a private <span class="No-Break">test set.</span></p>
			<p>The actual dataset is too large (around 0.5 TB) for us to feasibly download and process, given that most of you will have access to only limited compute power. We will use a subset of the data available in the <strong class="source-inline">train_sample_videos.zip</strong> and <strong class="source-inline">test_videos.zip</strong> files, which are around 4 GB altogether. You will need to download this data and save it locally so that you can run <span class="No-Break">the experiments.</span></p>
			<h3>The model</h3>
			<p>In the previous section, we<a id="_idIndexMarker433"/> developed our own CNN model and trained it end to end for classification. In this section, we will explore a different technique based on the concept of <em class="italic">transfer learning</em>. In machine learning, transfer learning is a paradigm where parameters learned from one task can be applied to another task and improve the performance of <span class="No-Break">the latter.</span></p>
			<p>In transfer learning, we train a classification model on a base task. Once that model is trained, we can use it for another task in one of <span class="No-Break">two ways:</span></p>
			<ul>
				<li><strong class="bold">Training the model again on new data from the second task</strong>: By doing so, we will initialize the model weights as the ones produced after training on the first task. The hope is that the model (which will effectively be trained on both sets of data) will show a good performance on <span class="No-Break">both tasks.</span></li>
				<li><strong class="bold">Using the model as a feature extractor</strong>: The final layer of the model is typically a softmax layer. We can extract the pre-final layer and use the model without softmax to generate features for our new data. These features can be used to train a downstream <span class="No-Break">classification model.</span></li>
			</ul>
			<p>Note that the second task is<a id="_idIndexMarker434"/> generally a refined and specific version of the first task. For example, the first task can be sentiment classification, and the second task can be movie review classification. The hope is that training on the first task will help the model learn high-level signals for sentiment classifications (keywords indicating certain sentiments, such as <strong class="source-inline">excellent</strong>, <strong class="source-inline">amazing</strong>, <strong class="source-inline">good</strong>, and <strong class="source-inline">terrible</strong>). Fine-tuning in the second task will help it learn task-specific knowledge in addition to the high-level knowledge (movie-specific keywords such as <strong class="source-inline">blockbuster</strong> and <strong class="source-inline">flop-show</strong>). Another example is that the base task is image classification on animals, and the fine-tuning task is a classifier for cat and <span class="No-Break">dog images.</span></p>
			<p>We will use the second approach listed here. Instead of developing custom features, we will let a pre-trained model do all the work. The <a id="_idIndexMarker435"/>model we will use is the <strong class="bold">InceptionV3 model</strong>, which will extract features for every frame in the video. We will use a <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>) to classify the<a id="_idIndexMarker436"/> video, based on the sequence of frames. This technique has been adapted from a submission to the <span class="No-Break">challenge (</span><a href="https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook"><span class="No-Break">https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook</span></a><span class="No-Break">).</span></p>
			<h3>Putting it all together</h3>
			<p>First, we will import the <span class="No-Break">necessary libraries:</span></p>
			<pre class="source-code">
# Libraries for Machine Learning
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
# Helper Libraries
import imageio
import cv2
import os</pre>
			<p>We will then read the video file and label for each video into a DataFrame. The dataset contains a metadata file for <a id="_idIndexMarker437"/>each set that provides us with this information. Note that you may have to adjust the paths depending on where and how you stored the <span class="No-Break">data locally:</span></p>
			<pre class="source-code">
train_meta_file = '../deepfake-detection-challenge/train_sample_videos/metadata.json'
train_sample_metadata = pd.read_json(train_meta_file).T</pre>
			<p>Let’s take a look at one of <span class="No-Break">the DataFrames:</span></p>
			<pre class="source-code">
train_sample_metadata.head()</pre>
			<p>It should show you the list of video files along with their associated label (real or fake). We will now define a few helper functions. The first one is a cropping function that will take in a frame and crop it into <span class="No-Break">a square:</span></p>
			<pre class="source-code">
def crop_image(frame):
  # Read dimensions
    y = frame.shape[0]
    x = frame.shape[1]
  # Calculate dimensions of cropped square
    min_dimension = min(x, y)
    start_x = (x/2) - (min_dimension/2)
    start_y = (y/2) - (min_dimension/2)
  # Pick only the part of the image to be retained
    cropped = frame[start_y : start_y + min_dim,
                    start_x : start_x + min_dim]
    return cropped</pre>
			<p>The second helper function will parse the video file and extract frames from it up to a specified maximum number of frames. We will resize each frame as (<strong class="source-inline">224,224</strong>) for standardization and then<a id="_idIndexMarker438"/> crop it into a square. The output will be a sequence of uniformly sized and <span class="No-Break">cropped frames:</span></p>
			<pre class="source-code">
def parse_video_into_frames(video_file_path):
    new_shape = (224, 224)
    capture = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
      # Read from the video stream
            ret, frame = capture.read()
            if not ret: # Have reached the end of frames
                break
      # Crop the frame and resize it
            frame = crop_image(frame)
            frame = cv2.resize(frame, new_shape)
            frame = frame[:, :, [2, 1, 0]]
       # Append it to a sequence
            frames.append(frame)
    finally:
        capture.release()
    return np.array(frames)</pre>
			<p>We will now define our feature extractor, which uses transfer learning and leverages the <strong class="source-inline">InceptionV3</strong> model to <a id="_idIndexMarker439"/>extract features from a frame. Fortunately, the <strong class="source-inline">keras</strong> library provides a convenient interface for us to load <span class="No-Break">pre-trained models:</span></p>
			<pre class="source-code">
inception_model = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(224,224, 3),
    )
preprocess_input = keras.applications.inception_v3.preprocess_input
inputs = keras.Input((224,224, 3))
preprocessed = preprocess_input(inputs)
outputs = inception_model(preprocessed)
feature_extractor = keras.Model(inputs, outputs, name="feature_extractor")</pre>
			<p>Finally, we will write another function that parses the entire dataset, generates a sequence of frames, extracts features from each frame, and generates a tuple of the feature sequence and the label (real or fake) for <span class="No-Break">the video:</span></p>
			<pre class="source-code">
def prepare_data(df, data_dir):
    MAX_SEQ_LENGTH = 20
    NUM_FEATURES = 2048
    num_samples = len(df)
    video_paths = list(df.index)
  # Binarize labels as 0/1
    labels = df["label"].values
    labels = np.array(labels=='FAKE').astype(np.int)
   # Placeholder arrays for frame masks and features
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool")
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")
  # Parse through each video file
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = parse_video_into_frames(os.path.join(data_dir, path))
        frames = frames[None, ...]
        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")
        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked
        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()
    return (frame_features, frame_masks), labels</pre>
			<p>To train the model, we need to split the available data into training and test sets (note that the test set provided does <a id="_idIndexMarker440"/>not come with labels, so we will not be able to evaluate our model on it). We will use our preprocessing function to prepare the data and convert it into the <span class="No-Break">required form:</span></p>
			<pre class="source-code">
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(train_sample_metadata,test_size=0.1,random_state=42,stratify=train_sample_metadata['label'])
train_data, train_labels = prepare_data(train_set, "train")
test_data, test_labels = prepare_data(test_set, "test")</pre>
			<p>Now, we can define a model and use this data to train it. Here, we will use the pre-trained model only as a feature extractor. The extracted features will be passed to a <span class="No-Break">downstream model:</span></p>
			<pre class="source-code">
MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
DROPOUT = 0.2
frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")
x = keras.layers.GRU(32, return_sequences=True)(frame_features_input, mask=mask_input)
x = keras.layers.GRU(16)(x)
x = keras.layers.GRU(8)(x)
x = keras.layers.Dropout(DROPOUT)(x)
x = keras.layers.Dense(8, activation="relu")(x)
x = keras.layers.Dense(8, activation="relu")(x)
output = keras.layers.Dense(1, activation="sigmoid")(x)
model = keras.Model([frame_features_input, mask_input], output)
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])</pre>
			<p>Here, we have used <strong class="bold">Gated Recurrent Unit</strong> (<strong class="bold">GRU</strong>) layers<a id="_idIndexMarker441"/> followed by two fully connected layers, each with <strong class="source-inline">8</strong> neurons. The last layer contains a single neuron with a sigmoid activation. You<a id="_idIndexMarker442"/> can examine the architecture by printing <span class="No-Break">it out:</span></p>
			<pre class="source-code">
model.summary()</pre>
			<p>Note that, here, we don’t use softmax but sigmoid instead. Why do we do this? What we have here is a binary classification problem; the sigmoid will give us the probability that the sequence of frames is a deepfake. If we had a multi-class classification, we would have used softmax instead. Remember that softmax is just sigmoid generalized to <span class="No-Break">multiple classes.</span></p>
			<p>Finally, we train the model. As we use Keras, training is as simple as calling the <strong class="source-inline">fit()</strong> function on the model and passing it the <span class="No-Break">required data:</span></p>
			<pre class="source-code">
EPOCHS = 10
model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_data=([test_data[0], test_data[1]],test_labels),
        epochs=EPOCHS,
        batch_size=8
    )</pre>
			<p>You should be able to observe the training of the model through each epoch. At every epoch, you can see the<a id="_idIndexMarker443"/> loss of the model both over the training data and <span class="No-Break">validation data.</span></p>
			<p>After we have the trained model, we can run the test examples through it and obtain the model prediction. We will obtain the output probability, and if it is greater than our threshold (generally, <strong class="source-inline">0.5</strong>), we classify it as fake (label = <span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
predicted_labels = []
THRESHOLD = 0.5
for idx in range(len(test_data[0])):
  frame_features = test_data[0][idx]
  frame_mask = test_data[1][idx]
  output_prob = model.predict([frame_features, frame_mask])[0]
  if (output_prob &gt;= THRESHOLD):
    predicted_labels.append(1)
  else:
    predicted_labels.append(0)</pre>
			<p>To examine the performance of our model, we can compare the actual and predicted labels by generating a<a id="_idIndexMarker444"/> <span class="No-Break">confusion matrix.</span></p>
			<h3>Playing with the model</h3>
			<p>Recall that in the last section on detecting deepfake images, we included pointers to playing around with the model and tracking performance. We will do the same here for deepfake video<a id="_idIndexMarker445"/> classification as well. While such a task may seem redundant, given that we already have a model, in the real world and industry, model tuning is an essential task that data scientists have <span class="No-Break">to handle.</span></p>
			<p>As a learning exercise, you should experiment with the model and determine what the best set of parameters you can have is. Here are some of the things you can <span class="No-Break">experiment with:</span></p>
			<ul>
				<li><strong class="bold">Image processing</strong>: Note that we parsed videos into frames and resized each image to a 224 x 224 shape. What happens if this shape is changed? How do extremely large sizes (1,048 x 1,048) or extremely small sizes (10 x 10) affect the model? Is the <span class="No-Break">performance affected?</span></li>
				<li><strong class="bold">Data parameters</strong>: In the preceding walkthrough, while preprocessing the data, we set two important parameters. The first is the <em class="italic">maximum sequence length</em> that determines how many frames we consider. The second is the <em class="italic">number of features</em> that controls the length of the feature vector extracted from the pre-trained model. How is the model affected if we change these parameters – both individually and in conjunction with <span class="No-Break">one another?</span></li>
				<li><strong class="bold">Feature extraction</strong>: We used the paradigms of transfer learning and a pre-trained model to process our input and give us our features. InceptionV3 is one model; however, there are several others that can be used. Do any other models result in better features (as evidenced by better <span class="No-Break">model performance)?</span></li>
				<li><strong class="bold">Model architecture</strong>: The number of GRU layers, fully connected layers, and the neurons in each layer were chosen arbitrarily. What happens if, instead of 32-16-8 GRU layers, we choose 16-8-8? How about 8-8-8? How about 5 layers, each with 16 neurons (i.e., 16-16-16-16-16)? Similar experiments can be done with the fully connected layers as well. The possibilities here <span class="No-Break">are endless.</span></li>
				<li><strong class="bold">Training</strong>: We trained our model over 10 epochs. What happens if we train for only 1 epoch? How about 50? 100? Intuitively, you would expect that more epochs leads to <a id="_idIndexMarker446"/>better performance, but is that really the case? Additionally, our prediction threshold is set to 0.5. How are the precision and recall affected if we vary <span class="No-Break">this threshold?</span></li>
			</ul>
			<p>This brings us to the end of our discussion on <span class="No-Break">deepfake videos.</span></p>
			<h1 id="_idParaDest-91">Summary</h1>
			<p>In this chapter, we studied deepfakes, which are synthetic media (images and videos) that are created using deep neural networks. These media often show people in positions that they have not been in and can be used for several nefarious purposes, including misinformation, fraud, and pornography. The impact can be catastrophic; deepfakes can cause political crises and wars, cause widespread panic among the public, facilitate identity theft, and cause defamation and loss of life. After understanding how deepfakes are created, we focused on detecting them. First, we used CNNs to detect deepfake images. Then, we developed a model that parsed deepfake videos into frames and used transfer learning to convert them into vectors, the sequence of which was used for fake or <span class="No-Break">real classification.</span></p>
			<p>Deepfakes are a growing challenge and have tremendous potential for cybercrime. There is a strong demand in the industry for professionals who understand deepfakes, their generation, the social impact they can have, and most importantly, methods to counter them. This chapter provided you with a deep understanding of deepfakes and equips you with the tools and technology needed to <span class="No-Break">detect them.</span></p>
			<p>In the next chapter, we will look at the text counterpart of deepfake images – <span class="No-Break">machine-generated text.</span></p>
		</div>
	</body></html>