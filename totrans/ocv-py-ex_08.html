<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Detecting Shapes and Segmenting an Image</h1></div></div></div><p>In this chapter, we are going to learn about shape analysis and image segmentation. We will learn how to recognize shapes and estimate the exact boundaries. We will discuss how to segment an image into its constituent parts using various methods. We will learn how to separate the foreground from the background as well.</p><p>By the end of this chapter, you will know:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is contour analysis and shape matching</li><li class="listitem" style="list-style-type: disc">How to match shapes</li><li class="listitem" style="list-style-type: disc">What is image segmentation</li><li class="listitem" style="list-style-type: disc">How to segment an image into its constituent parts</li><li class="listitem" style="list-style-type: disc">How to separate the foreground from the background</li><li class="listitem" style="list-style-type: disc">How to use various techniques to segment an image</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec67"/>Contour analysis and shape matching</h1></div></div></div><p>Contour analysis<a id="id247" class="indexterm"/> is a very useful tool in the field of computer vision. We deal with <a id="id248" class="indexterm"/>a lot of shapes in the real world and contour analysis helps in analyzing those shapes using various algorithms. When we convert an image to grayscale and threshold it, we are left with a bunch of lines and contours. Once we understand the properties of different shapes, we will be able to extract detailed information from an image.</p><p>Let's say we want to identify the boomerang shape<a id="id249" class="indexterm"/> in the following image:</p><div><img src="img/B04554_08_01.jpg" alt="Contour analysis and shape matching"/></div><p>In order <a id="id250" class="indexterm"/>to do that, we <a id="id251" class="indexterm"/>first need to know what a <a id="id252" class="indexterm"/>regular boomerang looks like:</p><div><img src="img/B04554_08_02.jpg" alt="Contour analysis and shape matching"/></div><p>Now using the above image as a reference, can we identify what shape in our original image corresponds to a boomerang? If you notice, we cannot use a simple correlation based approach because the shapes are all distorted. This means that an approach where we look for an exact match won't work! We need to understand the properties of this shape and match the<a id="id253" class="indexterm"/> corresponding properties to identify the<a id="id254" class="indexterm"/> boomerang shape. OpenCV provides a nice shape matcher function that we can use to achieve this. The matching is based on the concept of Hu <a id="id255" class="indexterm"/>moment, which <a id="id256" class="indexterm"/>in turn is related to image moments. You can refer to the following paper to learn more about <a id="id257" class="indexterm"/>moments: <a class="ulink" href="http://zoi.utia.cas.cz/files/chapter_moments_color1.pdf">http://zoi.utia.cas.cz/files/chapter_moments_color1.pdf</a>. The concept of "image moments" basically refers to the weighted and power-raised summation of the pixels within a shape.</p><div><img src="img/B04554_08_20.jpg" alt="Contour analysis and shape matching"/></div><p>In the above equation, <strong>p</strong> refers to the pixels inside the contour, <strong>w</strong> refers to the weights, <strong>N</strong> refers to the number of points inside the contour, <strong>k</strong> refers to the power, and <strong>I</strong> refers to the moment. Depending on the values we choose for w and k, we can extract different characteristics from that contour.</p><p>Perhaps the simplest example is to compute the area of the contour. To do this, we need to count the number of pixels within that region. So mathematically speaking, in the weighted and power raised summation form, we just need to set w to 1 and k to 0. This will give us the area of the contour. Depending on how we compute these moments, they will help us in understanding these different shapes. This also gives rise to some interesting properties that help us in determining the shape similarity metric.</p><p>If we match the shapes, you will see something like this:</p><div><img src="img/B04554_08_03.jpg" alt="Contour analysis and shape matching"/></div><p>Let's take <a id="id258" class="indexterm"/>a look<a id="id259" class="indexterm"/> at the code to <a id="id260" class="indexterm"/>do this:</p><div><pre class="programlisting">import sys

import cv2
import numpy as np

# Extract reference contour from the image
def get_ref_contour(img):
    ref_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(ref_gray, 127, 255, 0)

    # Find all the contours in the thresholded image. The values
    # for the second and third parameters are restricted to a # certain number of possible values. You can learn more # 'findContours' function here: http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html
    contours, hierarchy = cv2.findContours(thresh, 1, 2)

    # Extract the relevant contour based on area ratio. We use the # area ratio because the main image boundary contour is # extracted as well and we don't want that. This area ratio # threshold will ensure that we only take the contour inside # the image.
    for contour in contours:
        area = cv2.contourArea(contour)
        img_area = img.shape[0] * img.shape[1]
        if 0.05 &lt; area/float(img_area) &lt; 0.8:
            return contour

# Extract all the contours from the image
def get_all_contours(img):
    ref_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(ref_gray, 127, 255, 0)
    contours, hierarchy = cv2.findContours(thresh, 1, 2)
    return contours

if __name__=='__main__':
    # Boomerang reference image
    img1 = cv2.imread(sys.argv[1])

    # Input image containing all the different shapes
    img2 = cv2.imread(sys.argv[2])

    # Extract the reference contour
    ref_contour = get_ref_contour(img1)

    # Extract all the contours from the input image
    input_contours = get_all_contours(img2)

    closest_contour = input_contours[0]
    min_dist = sys.maxint
    # Finding the closest contour
    for contour in input_contours:
        # Matching the shapes and taking the closest one
        ret = cv2.matchShapes(ref_contour, contour, 1, 0.0)
        if ret &lt; min_dist:
            min_dist = ret
            closest_contour = contour

    cv2.drawContours(img2, [closest_contour], -1, (0,0,0), 3)
    cv2.imshow('Output', img2)
    cv2.waitKey()</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec68"/>Approximating a contour</h1></div></div></div><p>A lot of contours that <a id="id261" class="indexterm"/>we encounter in real life are noisy. This means that the contours don't look smooth, and hence our analysis takes a hit. So how do we deal with this? One way to go about this would be to get all the points on the contour and then approximate it with a smooth polygon.</p><p>Let's consider the boomerang image again. If you approximate the contours using various thresholds, you will see the contours changing their shapes. Let's start with a factor of 0.05:</p><div><img src="img/B04554_08_04.jpg" alt="Approximating a contour"/></div><p>If you reduce this factor, the contours will get smoother. Let's make it 0.01:</p><div><img src="img/B04554_08_05.jpg" alt="Approximating a contour"/></div><p>If you make it really <a id="id262" class="indexterm"/>small, say 0.00001, then it will look like the original image:</p><div><img src="img/B04554_08_06.jpg" alt="Approximating a contour"/></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec69"/>Identifying the pizza with the slice taken out</h1></div></div></div><p>The title might be <a id="id263" class="indexterm"/>slightly misleading, because we will not be talking about pizza slices. But let's say you are in a situation where you have an image containing different types of pizzas with different shapes. Now, somebody has taken a slice out of one of those pizzas. How would we automatically identify this?</p><p>We cannot take the approach we took earlier because we don't know what the shape looks like. So we don't have any template. We are not even sure what shape we are looking for, so we cannot build a template based on any prior information. All we know is the fact that a slice has been taken from one of the pizzas. Let's consider the following image:</p><div><img src="img/B04554_08_07.jpg" alt="Identifying the pizza with the slice taken out"/></div><p>It's not exactly a real image, but you get the idea. You know what shape we are talking about. Since we <a id="id264" class="indexterm"/>don't know what we are looking for, we need to use some of the properties of these shapes to identify the sliced pizza. If you notice, all the other shapes are nicely closed. As in, you can take any two points within those shapes and draw a line between them, and that line will always lie within that shape. These kinds of shapes are called <a id="id265" class="indexterm"/><strong>convex shapes</strong>.</p><p>If you look at the sliced pizza shape, we can choose two points such that the line between them goes outside the shape as shown in the figure that follows:</p><div><img src="img/B04554_08_08.jpg" alt="Identifying the pizza with the slice taken out"/></div><p>So, all we need to do is <a id="id266" class="indexterm"/>detect the non-convex shape in the image and we'll be done. Let's go ahead and do that:</p><div><pre class="programlisting">import sys

import cv2
import numpy as np

# Input is a color image
def get_contours(img):
    # Convert the image to grayscale
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Threshold the input image
    ret, thresh = cv2.threshold(img_gray, 127, 255, 0)

    # Find the contours in the above image
    contours, hierarchy = cv2.findContours(thresh, 2, 1)

    return contours

if __name__=='__main__':
    img = cv2.imread(sys.argv[1])

    # Iterate over the extracted contours
    for contour in get_contours(img):
        # Extract convex hull from the contour
        hull = cv2.convexHull(contour, returnPoints=False)

        # Extract convexity defects from the above hull
        defects = cv2.convexityDefects(contour, hull)

        if defects is None:
            continue

        # Draw lines and circles to show the defects
        for i in range(defects.shape[0]):
            start_defect, end_defect, far_defect, _ = defects[i,0]
            start = tuple(contour[start_defect][0])
            end = tuple(contour[end_defect][0])
            far = tuple(contour[far_defect][0])
            cv2.circle(img, far, 5, [128,0,0], -1)
            cv2.drawContours(img, [contour], -1, (0,0,0), 3)

    cv2.imshow('Convexity defects',img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()</pre></div><p>If you run the above code, you will see something like this:</p><div><img src="img/B04554_08_09.jpg" alt="Identifying the pizza with the slice taken out"/></div><p>Wait a minute, what<a id="id267" class="indexterm"/> happened here? It looks so cluttered. Did we do something wrong? As it turns out, the curves are not really smooth. If you observe closely, there are tiny ridges everywhere along the curves. So, if you just run your convexity detector, it's not going to work. This is where contour approximation comes in really handy. Once we've detected the contours, we need to smoothen them so that the ridges do not affect them. Let's go ahead and do that:</p><div><pre class="programlisting">import sys

import cv2
import numpy as np

# Input is a color image
def get_contours(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(img_gray, 127, 255, 0)
    contours, hierarchy = cv2.findContours(thresh, 2, 1)
    return contours

if __name__=='__main__':
    img = cv2.imread(sys.argv[1])

    # Iterate over the extracted contours
    for contour in get_contours(img):
        orig_contour = contour
        epsilon = 0.01 * cv2.arcLength(contour, True)
        contour = cv2.approxPolyDP(contour, epsilon, True)

        # Extract convex hull and the convexity defects
        hull = cv2.convexHull(contour, returnPoints=False)
        defects = cv2.convexityDefects(contour,hull)

        if defects is None:
            continue

        # Draw lines and circles to show the defects
        for i in range(defects.shape[0]):
            start_defect, end_defect, far_defect, _ = defects[i,0]
            start = tuple(contour[start_defect][0])
            end = tuple(contour[end_defect][0])
            far = tuple(contour[far_defect][0])
            cv2.circle(img, far, 7, [255,0,0], -1)
            cv2.drawContours(img, [orig_contour], -1, (0,0,0), 3)

    cv2.imshow('Convexity defects',img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()</pre></div><p>If you run the<a id="id268" class="indexterm"/> preceding code, the output will look like the following:</p><div><img src="img/B04554_08_10.jpg" alt="Identifying the pizza with the slice taken out"/></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec70"/>How to censor a shape?</h1></div></div></div><p>Let's say you are<a id="id269" class="indexterm"/> dealing with images and you want to block out a particular shape. Now, you might say that you will use shape matching to identify the shape and then just block it out, right? But the problem here is that we don't have any template available. So, how do we go about doing this? Shape analysis comes in various forms, and we need to build our algorithm depending on the situation. Let's consider the following figure:</p><div><img src="img/B04554_08_11.jpg" alt="How to censor a shape?"/></div><p>Let's say we want to identify all the boomerang shapes and then block them out without using any template images. As you can see, there are various other weird shapes in that image and the boomerang shapes are not really smooth. We need to identify the property that's going to differentiate the boomerang shape from the other shapes present. Let's consider the convex hull. If you take the ratio of the area of each shape to the area of the convex hull, we can see that this can be a distinguishing metric. This metric is called <strong>solidity factor</strong><a id="id270" class="indexterm"/> in shape analysis. This metric will have a lower value for the boomerang shapes because of the empty area that will be left out, as shown in the following figure:</p><div><img src="img/B04554_08_12.jpg" alt="How to censor a shape?"/></div><p>The black <a id="id271" class="indexterm"/>boundaries represent the convex hulls. Once we compute these values for all the shapes, how do separate them out? Can we just use a fixed threshold to detect the boomerang shapes? Not really! We cannot have a fixed threshold value because you never know what kind of shape you might encounter later. So, a better approach would be to use <a id="id272" class="indexterm"/><strong>K-Means clustering</strong>. K-Means is an unsupervised learning technique that can be used to separate out the input data into K classes. You can quickly brush up on K-Means before proceeding <a id="id273" class="indexterm"/>further at <a class="ulink" href="http://docs.opencv.org/master/de/d4d/tutorial_py_kmeans_understanding.html">http://docs.opencv.org/master/de/d4d/tutorial_py_kmeans_understanding.html</a>.</p><p>We know that we want to separate the shapes into two groups, that is, boomerang shapes and other shapes. So, we know what our <em>K</em> will be in K-Means. Once we use that and cluster the values, we pick the cluster with the lowest solidity factor and that will give us our boomerang shapes. Bear in mind that this approach works only in this particular case. If you are dealing with other kinds of shapes, then you will have to use some other metrics to make sure that the shape detection works. As we discussed earlier, it depends heavily on the situation. If you detect the shapes and block them out, it will look like this:</p><div><img src="img/B04554_08_13.jpg" alt="How to censor a shape?"/></div><p>Following is the <a id="id274" class="indexterm"/>code to do it:</p><div><pre class="programlisting">import sys

import cv2
import numpy as np

def get_all_contours(img):
    ref_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(ref_gray, 127, 255, 0)
    contours, hierarchy = cv2.findContours(thresh, 1, 2)
    return contours

if __name__=='__main__':
    # Input image containing all the shapes
    img = cv2.imread(sys.argv[1])

    img_orig = np.copy(img)
    input_contours = get_all_contours(img)
    solidity_values = []

    # Compute solidity factors of all the contours
    for contour in input_contours:
        area_contour = cv2.contourArea(contour)
        convex_hull = cv2.convexHull(contour)
        area_hull = cv2.contourArea(convex_hull)
        solidity = float(area_contour)/area_hull
        solidity_values.append(solidity)

    # Clustering using KMeans
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    flags = cv2.KMEANS_RANDOM_CENTERS
    solidity_values = np.array(solidity_values).reshape((len(solidity_values),1)).astype('float32')
    compactness, labels, centers = cv2.kmeans(solidity_values, 2, criteria, 10, flags)

    closest_class = np.argmin(centers)
    output_contours = []
    for i in solidity_values[labels==closest_class]:
        index = np.where(solidity_values==i)[0][0]
        output_contours.append(input_contours[index])

    cv2.drawContours(img, output_contours, -1, (0,0,0), 3)
    cv2.imshow('Output', img)

    # Censoring
    for contour in output_contours:
        rect = cv2.minAreaRect(contour)
        box = cv2.cv.BoxPoints(rect)
        box = np.int0(box)
        cv2.drawContours(img_orig,[box],0,(0,0,0),-1)

    cv2.imshow('Censored', img_orig)
    cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec71"/>What is image segmentation?</h1></div></div></div><p>Image segmentation<a id="id275" class="indexterm"/> is the process of separating an image into its constituent parts. It is an important step in many computer vision applications in the real world. There are many different ways of segmenting an image. When we segment an image, we separate the regions based on various metrics such as color, texture, location, and so on. All the pixels within each region have something in common, depending on the metric we are using. Let's take a look at some of the popular approaches here.</p><p>To start with, we will be looking at a technique called <a id="id276" class="indexterm"/><strong>GrabCut</strong>. It is an image segmentation method based on a more generic approach called<a id="id277" class="indexterm"/> <strong>graph-cuts</strong>. In the graph-cuts method, we consider the entire image to be a graph, and then we segment the graph based on the strength of the edges in that graph. We construct the graph by considering each pixel to be a node and edges are constructed between the nodes, where edge weight is a function of the pixel values of those two nodes. Whenever there is a boundary, the pixel values are higher. Hence, the edge weights will also be higher. This graph is then segmented by minimizing the Gibss energy of the graph. This is analogous to finding the maximum entropy segmentation. You can refer to the original <a id="id278" class="indexterm"/>paper to learn more about it at <a class="ulink" href="http://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf">http://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf</a>. Let's consider the following image:</p><div><img src="img/B04554_08_14.jpg" alt="What is image segmentation?"/></div><p>Let's select the <a id="id279" class="indexterm"/>region of interest:</p><div><img src="img/B04554_08_15.jpg" alt="What is image segmentation?"/></div><p>Once the image has been segmented, it will look something like this:</p><div><img src="img/B04554_08_16.jpg" alt="What is image segmentation?"/></div><p>Following is the <a id="id280" class="indexterm"/>code to do this:</p><div><pre class="programlisting">import cv2
import numpy as np

# Draw rectangle based on the input selection
def draw_rectangle(event, x, y, flags, params):
    global x_init, y_init, drawing, top_left_pt, bottom_right_pt, img_orig

    # Detecting mouse button down event
    if event == cv2.EVENT_LBUTTONDOWN:
        drawing = True
        x_init, y_init = x, y

    # Detecting mouse movement
    elif event == cv2.EVENT_MOUSEMOVE:
        if drawing:
            top_left_pt, bottom_right_pt = (x_init,y_init), (x,y)
            img[y_init:y, x_init:x] = 255 - img_orig[y_init:y, x_init:x]
            cv2.rectangle(img, top_left_pt, bottom_right_pt, (0,255,0), 2)

    # Detecting mouse button up event
    elif event == cv2.EVENT_LBUTTONUP:
        drawing = False
        top_left_pt, bottom_right_pt = (x_init,y_init), (x,y)
        img[y_init:y, x_init:x] = 255 - img[y_init:y, x_init:x]
        cv2.rectangle(img, top_left_pt, bottom_right_pt, (0,255,0), 2)
        rect_final = (x_init, y_init, x-x_init, y-y_init)

        # Run Grabcut on the region of interest
        run_grabcut(img_orig, rect_final)

# Grabcut algorithm
def run_grabcut(img_orig, rect_final):
    # Initialize the mask
    mask = np.zeros(img_orig.shape[:2],np.uint8)

    # Extract the rectangle and set the region of
    # interest in the above mask
    x,y,w,h = rect_final
    mask[y:y+h, x:x+w] = 1

    # Initialize background and foreground models
    bgdModel = np.zeros((1,65), np.float64)
    fgdModel = np.zeros((1,65), np.float64)

    # Run Grabcut algorithm
    cv2.grabCut(img_orig, mask, rect_final, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)

    # Extract new mask
    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')

    # Apply the above mask to the image
    img_orig = img_orig*mask2[:,:,np.newaxis]

    # Display the image
    cv2.imshow('Output', img_orig)

if __name__=='__main__':
    drawing = False
    top_left_pt, bottom_right_pt = (-1,-1), (-1,-1)

    # Read the input image
    img_orig = cv2.imread(sys.argv[1])
    img = img_orig.copy()

    cv2.namedWindow('Input')
    cv2.setMouseCallback('Input', draw_rectangle)

    while True:
        cv2.imshow('Input', img)
        c = cv2.waitKey(1)
        if c == 27:
            break

    cv2.destroyAllWindows()</pre></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec44"/>How does it work?</h2></div></div></div><p>We start with the seed <a id="id281" class="indexterm"/>points specified by the user. This is the bounding box within which we have the object of interest. Underneath the surface, the algorithm estimates the color distribution of the object and the background. The algorithm represents the color distribution of the image as a <a id="id282" class="indexterm"/><strong>Gaussian Mixture Markov Random Field</strong> (<strong>GMMRF</strong>). You can refer to the <a id="id283" class="indexterm"/>detailed paper to learn more about GMMRF at <a class="ulink" href="http://research.microsoft.com/pubs/67898/eccv04-GMMRF.pdf">http://research.microsoft.com/pubs/67898/eccv04-GMMRF.pdf</a>. We need the color distribution of both, the object and the background, because we will be using this knowledge to separate the object. This information is used to find the maximum entropy segmentation by applying the min-cut algorithm to the Markov Random Field. Once we have this, we use the graph cuts optimization method to infer the labels.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec72"/>Watershed algorithm</h1></div></div></div><p>OpenCV comes with a default implementation of the watershed algorithm<a id="id284" class="indexterm"/>. It's pretty famous and there are a lot of implementations available out there. You can read<a id="id285" class="indexterm"/> more about it at <a class="ulink" href="http://docs.opencv.org/master/d3/db4/tutorial_py_watershed.html">http://docs.opencv.org/master/d3/db4/tutorial_py_watershed.html</a>. Since you already have access to the OpenCV source code, we will not be looking at the code here.</p><p>We will just see what the output looks like. Consider the following image:</p><div><img src="img/B04554_08_17.jpg" alt="Watershed algorithm"/></div><p>Let's select the regions:</p><div><img src="img/B04554_08_18.jpg" alt="Watershed algorithm"/></div><p>If you run the<a id="id286" class="indexterm"/> watershed algorithm on this, the output will look something like the following:</p><div><img src="img/B04554_08_19.jpg" alt="Watershed algorithm"/></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec73"/>Summary</h1></div></div></div><p>In this chapter, we learned about contour analysis and image segmentation. We learned how to match shapes based on a template. We learned about the various different properties of shapes and how we can use them to identify different kinds of shapes. We discussed image segmentation and how we can use graph-based methods to segment regions in an image. We briefly discussed watershed transformation as well.</p><p>In the next chapter, we are going to discuss how to track an object in a live video.</p></div></div>
</body></html>