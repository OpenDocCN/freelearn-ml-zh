- en: '*Chapter 4*: Exploring Bayesian Optimization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：探索贝叶斯优化'
- en: '**Bayesian optimization** (**BO**) is the second out of four groups of hyperparameter
    tuning methods. Unlike grid search and random search, which are categorized as
    uninformed search methods, all of the methods that belong to the BO group are
    categorized as **informed search** methods, meaning they are learning from previous
    iterations to (hopefully) provide a better search space in the future.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化**（**BO**）是四种超参数调整方法组中的第二种。与被归类为无信息搜索方法的网格搜索和随机搜索不同，属于BO组的所有方法都被归类为**信息搜索**方法，这意味着它们是从之前的迭代中学习的，以便（希望）在将来提供一个更好的搜索空间。'
- en: In this chapter, we will discuss several methods that belong to the BO group,
    including **Gaussian process** (**GP**), **sequential model-based algorithm configuration**
    (**SMAC**), **Tree-structured Parzen Estimators** (**TPE**), and Metis. Similar
    to [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031), *Exploring Exhaustive
    Search*, we will discuss the definition of each method, the differences between
    them, how they work, and the pros and cons of each method.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论属于BO组的几种方法，包括**高斯过程**（**GP**）、**基于模型的算法配置序列**（**SMAC**）、**树结构帕尔森估计器**（**TPE**）和Metis。与[第3章](B18753_03_ePub.xhtml#_idTextAnchor031)
    *探索穷举搜索*类似，我们将讨论每种方法的定义、它们之间的区别、它们的工作原理以及每种方法的优缺点。
- en: By the end of this chapter, you will be able to explain BO and its variations
    when someone asks you. You will not only be able to explain what they are, but
    also how they work, in a high-level and technical way. You will also be able to
    tell the differences between them, along with the pros and cons of each of the
    methods. Furthermore, you will experience a crucial benefit once you understand
    the ins and outs of each method; that is, you will be able to understand what’s
    happening if there are errors or unexpected results and understand how to set
    up the method configuration to match your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够解释BO及其变体，当有人问你时。你不仅能够解释它们是什么，而且能够以高级和技术的角度解释它们是如何工作的。你还将能够说出它们之间的区别，以及每种方法的优缺点。此外，一旦你理解了每种方法的来龙去脉，你将体验到一项关键的好处；那就是，你将能够理解如果出现错误或意外结果时发生了什么，并理解如何设置方法配置以匹配你的特定问题。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Introducing BO
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍BO
- en: Understanding BO GP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解BO GP
- en: Understanding SMAC
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SMAC
- en: Understanding TPE
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解TPE
- en: Understanding Metis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Metis
- en: Introducing BO
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍BO
- en: BO is categorized as an informed search hyperparameter tuning method, meaning
    the search is learning from previous iterations to have a (hopefully) better subspace
    in the next iterations. It is also categorized as the **sequential model-based
    optimization** (**SMBO**) group. All SMBO methods work by sequentially updating
    probability models to estimate the effect of a set of hyperparameters on their
    performance based on historical observed data, as well as suggesting new hyperparameters
    to be tested in the following trials.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: BO被归类为一种基于信息的搜索超参数调整方法，这意味着搜索是从之前的迭代中学习的，以便在下一个迭代中有一个（希望）更好的子空间。它也被归类为**基于模型的序列优化**（**SMBO**）组。所有SMBO方法都是通过顺序更新概率模型来估计一组超参数对其性能的影响，基于历史观察数据，并在接下来的试验中建议要测试的新超参数。
- en: BO is a popular hyperparameter tuning method due to its *data-efficient* property,
    meaning it needs a relatively small number of samples to get to the optimal solution.
    You may be wondering, how exactly does BO get this ground-breaking data-efficient
    property? This property exists thanks to BO’s ability to learn from previous iterations.
    BO can learn and predict which subspace is worth visiting in the future by utilizing
    a **probabilistic regression model**, which acts as the *cheap cloned version
    of the expensive objective function*, and an **acquisition function**, which governs
    which *set of hyperparameters should be tested* in the next iteration.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其*数据高效*的特性，BO是一种流行的超参数调整方法，这意味着它需要相对较少的样本就能达到最优解。你可能想知道，BO是如何获得这种开创性的数据高效特性的？这种特性得益于BO从之前的迭代中学习的能力。BO可以通过利用**概率回归模型**（作为昂贵的目标函数的*廉价克隆版本*）和**获取函数**（它控制下一个迭代中应该测试哪组超参数）来学习和预测未来值得访问的子空间。
- en: 'The objective function is just a function that takes hyperparameter values
    as input and returns the cross-validation score (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014),
    *Evaluating Machine Learning Models*). We do not know what the output of the objective
    function for all possible hyperparameter values is. If we did, there would be
    no need to perform hyperparameter tuning. We could just use that function to get
    the hyperparameter values, which results in the highest cross-validation score.
    That’s why we need a probabilistic regression model, to approximate the objective
    function by fitting a set of known hyperparameter and cross-validation score value
    pairs (see *Figure 4.1*). The approximation concept is *similar to the concept
    of ML-based regressor* models, such as random forest, linear regression, and many
    more. First, we fit the regressor to the samples of independent and dependent
    variables; then, the model will try to *learn* from the data, which in the end
    can be used to predict new given data. The probabilistic regression model is also
    often called the **surrogate model**:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数只是一个函数，它接受超参数值作为输入并返回交叉验证分数（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)，*评估机器学习模型*）。我们不知道目标函数对所有可能超参数值的输出是什么。如果我们知道，就没有必要进行超参数调整。我们可以直接使用该函数来获取超参数值，这将导致获得最高的交叉验证分数。这就是为什么我们需要一个概率回归模型，通过拟合一组已知的超参数和交叉验证分数值对来近似目标函数（参见*图4.1*）。近似的概念与基于机器学习的回归器模型的概念类似，例如随机森林、线性回归等。首先，我们将回归器拟合到独立变量和依赖变量的样本；然后，模型将尝试*学习*数据，最终可以用来预测新的给定数据。概率回归模型也常被称为**代理模型**：
- en: '![Figure 4.1 – Illustration of the probabilistic regression model, M'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 4.1 – Illustration of the probabilistic regression model, M'
- en: '](img/B18753_04_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_04_001.jpg](img/B18753_04_001.jpg)'
- en: Figure 4.1 – Illustration of the probabilistic regression model, M
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 概率回归模型，M的示意图
- en: The acquisition function *governs which subspace we should search in the next
    iteration*. Thanks to this function, BO enables us to learn from past experiences
    and have fewer hyperparameter tuning iterations compared to random search, in
    general.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 获取函数*控制我们在下一次迭代中应该搜索哪个子空间*。多亏了这个函数，贝叶斯优化（BO）使我们能够从过去的经验中学习，并且与随机搜索相比，通常需要更少的超参数调整迭代。
- en: Important Note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember that, to get the cross-validation score, we need to perform multiple
    training and evaluation processes (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,*
    *Evaluating Machine Learning Models*). This is an *expensive process* when you
    have a big, complex model with a large amount of training data. That’s why the
    acquisition function plays a big role here.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，为了获得交叉验证分数，我们需要执行多次训练和评估过程（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，*评估机器学习模型*）。当你有一个大型的、复杂的模型以及大量的训练数据时，这是一个*昂贵的流程*。这就是为什么获取函数在这里扮演着重要角色。
- en: 'In general, BO works as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，贝叶斯优化（BO）的工作原理如下：
- en: Split the original full data into train and test sets. (See [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,*
    *Evaluating Machine Learning Models*).
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据集分为训练集和测试集。（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，*评估机器学习模型*）
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间，*H*及其伴随的分布。
- en: Define the objective function, *f*, based on the train set.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集定义目标函数，*f*。
- en: Define the stopping criterion. Usually, the number of trials is used. However,
    it is also possible to use the time taken or convergence as the stopping criterion.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义停止标准。通常，使用试验次数。然而，也可以使用时间或收敛性作为停止标准。
- en: Initializes the empty set, *D*, which will be used to store the initial pairs
    of hyperparameter values and cross-validation scores, as well as the resulting
    pairs suggested by the acquisition function, *A*.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空集合，*D*，该集合将用于存储初始的超参数值对和交叉验证分数，以及由获取函数，*A*建议的结果值对。
- en: Initialize several pairs of hyperparameter values and cross-validation scores
    and store them in *D*.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化几个超参数值对和交叉验证分数，并将它们存储在*D*中。
- en: Fit the probabilistic regression model/surrogate model, *M*, using the value
    pairs in *D*.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*D*中的值对拟合概率回归模型/代理模型，*M*。
- en: 'Sample the next set of hyperparameters by utilizing the acquisition function,
    *A*:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过利用获取函数，*A*，采样下一组超参数：
- en: Perform optimization on the acquisition function, *A*, with the help of the
    surrogate model, *M*, to sample which hyperparameters are to be passed to the
    acquisition function.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理模型*M*的帮助下，对获取函数*A*进行优化，以采样哪些超参数应该传递给获取函数。
- en: Get the expected optimal set of hyperparameters based on the acquisition function,
    *A*.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据获取函数*A*，获取预期的最佳超参数集。
- en: Compute the cross-validation score using the objective function, *f*, based
    on the output from *Step 8*.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基于*步骤 8*输出的目标函数*f*计算交叉验证得分。
- en: Add the hyperparameters and cross-validation score pair from *Step 8* and *Step
    9* to set *D*.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*步骤 8*和*步骤 9*中的超参数和交叉验证得分对添加到设置*D*中。
- en: Repeat *Steps 7* to *10* until the stopping criterion is met.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 7*到*步骤 10*，直到满足停止标准。
- en: Trains on the full training set using the final hyperparameter values.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最终的超参数值在完整训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: You can initialize the hyperparameter values and cross-validation scores, as
    shown in *Step 6*, using several sampling strategies. The most straightforward
    and go-to way, in practice, is to just perform **random sampling**. However, there
    are also other methods that you may consider during your experiments, such as
    the **quasi-random** or **Latin hypercube** sampling methods.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用几种采样策略来初始化超参数值和交叉验证得分，如*步骤 6*所示。在实践中，最直接和常用的方法是进行**随机采样**。然而，在实验过程中，你也可以考虑其他方法，例如**准随机**或**拉丁超立方体**采样方法。
- en: Similar to random search, we also need to define the distribution of each hyperparameter
    in BO. You may wonder if BO can also work on a non-numerical type of hyperparameter.
    The answer is *based on the probabilistic regression model* you are using. There
    are several surrogate models you can choose from. Those options will be discussed
    in the next three sections of this chapter, and they include **GP**, **Tree-structured
    Parzen Estimator** (**TPE**), random forest, extra trees, or other ML-based regressors.
    In this book, we will discuss the random forest regressor that’s implemented in
    the SMAC model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机搜索类似，在贝叶斯优化（BO）中，我们还需要定义每个超参数的分布。你可能想知道BO是否也可以用于非数值类型的超参数。答案是*基于你使用的概率回归模型*。你可以选择几种代理模型。这些选项将在本章接下来的三个部分中讨论，包括**高斯过程**（**GP**）、**树结构帕累托估计器**（**TPE**）、随机森林、额外树或其他基于机器学习的回归器。在这本书中，我们将讨论SMAC模型中实现的随机森林回归器。
- en: It is also worth noting that the optimization process in *Step 8* can be *replaced
    with a random search*. So, instead of performing some kind of second-order optimization
    method, we can randomly sample sets of hyperparameters from the search space and
    pass them onto the acquisition function. Then, we can get the optimal set of hyperparameters
    based on the output from the acquisition function. When using random search in
    this step, we still utilize the acquisition function to govern which subspace
    we should search for in the next iteration, but we add some random behavior to
    it, with the hope that we can escape the local optimum and converge toward the
    global optimum.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，*步骤 8*中的优化过程可以被*随机搜索*所替代。因此，我们不必执行某种二阶优化方法，而是可以从搜索空间中随机采样一组超参数，并将它们传递给获取函数。然后，我们可以根据获取函数的输出获取最佳超参数集。当在此步骤中使用随机搜索时，我们仍然利用获取函数来指导我们在下一次迭代中应该搜索哪个子空间，但我们添加了一些随机行为，希望可以逃离局部最优并收敛到全局最优。
- en: 'The first and the most popular acquisition function is **expected improvement**
    (**EI**), which is defined as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最先且最受欢迎的获取函数是**期望改进**（**EI**），其定义如下：
- en: '![](img/Formula_B18753_04_001.png) when ![](img/Formula_B18753_04_002.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: 当![](img/Formula_B18753_04_002.png)时，![](img/Formula_B18753_04_001.png)。
- en: '![](img/Formula_B18753_04_003.png) when ![](img/Formula_B18753_04_004.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: 当![](img/Formula_B18753_04_004.png)时，![](img/Formula_B18753_04_003.png)。
- en: Here, ![](img/Formula_B18753_04_005.png), ![](img/Formula_B18753_04_006.png)
    and ![](img/Formula_B18753_04_007.png) are the cumulative distribution and probability
    density functions of the standard normal distribution, respectively. ![](img/Formula_B18753_04_008.png)
    and ![](img/Formula_B18753_04_009.png) represent the expected performance and
    the uncertainty, respectively, that are captured by the surrogate model. Finally,
    ![](img/Formula_B18753_04_010.png) represents the current best value of the objective
    function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_04_005.png)、![](img/Formula_B18753_04_006.png)和![](img/Formula_B18753_04_007.png)分别表示标准正态分布的累积分布函数和概率密度函数。![](img/Formula_B18753_04_008.png)和![](img/Formula_B18753_04_009.png)分别代表由代理模型捕捉到的预期性能和不确定性。最后，![](img/Formula_B18753_04_010.png)代表目标函数的当前最佳值。
- en: Implicitly, the EI acquisition function enables BO methods to have the *exploration
    versus exploitation trade-off property*. This property can be achieved by two
    terms competing within the formula. When the value of the first term is high,
    meaning the expected performance, ![](img/Formula_B18753_04_011.png), is higher
    than the current best value, ![](img/Formula_B18753_04_012.png), EI will favor
    the exploitation process. On the other hand, when the uncertainty is very high,
    meaning we have a high value of ![](img/Formula_B18753_04_013.png), EI will favor
    the exploration process. By exploitation, this means that the acquisition function
    will recommend the set of hyperparameters that possibly get a higher value of
    the objective function, *f*. In terms of exploration, this means that the acquisition
    function will recommend the set of hyperparameters from the subspace that we haven’t
    explored yet.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 暗示地，EI获取函数使得BO方法具有*探索与利用的权衡特性*。这种特性可以通过公式内的两个术语之间的竞争来实现。当第一个术语的值很高时，意味着预期性能![](img/Formula_B18753_04_011.png)高于当前最佳值![](img/Formula_B18753_04_012.png)，EI将倾向于利用过程。另一方面，当不确定性非常高时，意味着我们有一个高值![](img/Formula_B18753_04_013.png)，EI将倾向于探索过程。通过利用，这意味着获取函数将推荐可能获得目标函数*f*更高值的超参数集。至于探索，这意味着获取函数将推荐来自我们尚未探索的子空间的超参数集。
- en: 'You can imagine this exploration and exploitation trade-off as when you are
    craving some food. Let’s say you want to have lunch with your brother today. Imagine
    the following two scenarios:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这种探索与利用的权衡想象成当你渴望食物的时候。比如说，你今天想和你的兄弟一起吃午饭。想象以下两种情况：
- en: “Hey bro, let’s have lunch at our favorite restaurant today!”
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “嘿，兄弟，我们今天就去我们最喜欢的餐厅吃午饭吧！”
- en: “Hey bro, have you heard of the new restaurant up there? Why don’t we try it
    for lunch?”
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “嘿，兄弟，你听说过那里的新餐厅吗？我们为什么不去那里吃午饭呢？”
- en: In the first scenario, you choose to eat at your favorite restaurant since you
    are confident that there is nothing wrong with the food and, more importantly,
    you are *confident about the taste of the food and the overall experience* of
    eating at that restaurant. This first scenario best explains what we call the
    exploitation process. In the second scenario, you *don’t have any idea what the
    overall experience* of eating at that new restaurant is. It may be worse than
    your favorite restaurant, but it may also potentially be your new favorite restaurant!
    This is what we call the exploration process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，你选择在你最喜欢的餐厅用餐，因为你确信食物没有问题，更重要的是，你对这家餐厅的食物和整体用餐体验*非常有信心*。这个第一种情况最好地解释了我们所说的利用过程。在第二种情况下，你对那家新餐厅的整体用餐体验*一无所知*。它可能比你最喜欢的餐厅差，但也可能成为你新的最爱！这就是我们所说的探索过程。
- en: Important Note
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In some implementations, such as in the **Scikit-optimize** package, there is
    a hyperparameter that enables us to *control how much we are leaning toward exploitation*
    compared to exploration. In Scikit-optimize, the sign of the EI function is negative.
    This is because the package *treats the optimization problem as the minimization
    problem* by default.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些实现中，例如在**Scikit-optimize**包中，有一个超参数可以让我们控制相对于探索，我们有多大的倾向于利用。在Scikit-optimize中，EI函数的符号是负的。这是因为该包默认将优化问题视为最小化问题。
- en: In our previous explanation, we treated the optimization problem as the maximization
    problem since we wanted to get the highest cross-validation score possible. Don’t
    confuse this with the minimization versus maximization problem – just choose what
    best describes the problem you will be facing in practice!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的解释中，我们将优化问题视为最大化问题，因为我们想要得到尽可能高的交叉验证分数。不要将这个问题与最小化与最大化问题混淆——只需选择最能描述您在实践中将面临的问题即可！
- en: 'The following is the EI acquisition function that’s implemented in the Scikit-optimize
    package:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在Scikit-optimize包中实现的EI获取函数：
- en: '![](img/Formula_B18753_04_014.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_014.png)'
- en: As you can see in the first term, the value of ![](img/Formula_B18753_04_015.png)
    will control how big our tendency is toward exploitation compared to exploration.
    The smaller the ![](img/Formula_B18753_04_016.png) value is, the more we lean
    toward exploitation. We will learn more about the implementation part of BO using
    Scikit or other packages from [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit* to [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在第一项中看到的，![](img/Formula_B18753_04_015.png)的值将控制我们向利用倾斜的程度与探索相比。![](img/Formula_B18753_04_016.png)的值越小，我们越倾向于利用。我们将从[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)*，通过Scikit进行超参数调整*到[*第10章*](B18753_10_ePub.xhtml#_idTextAnchor092)*，使用DEAP和Microsoft
    NNI进行高级超参数调整*中学习更多关于使用Scikit或其他包实现BO的细节。
- en: 'To get a better understanding of how the exploration and exploitation trade-off
    happens during the hyperparameter tuning phase, let’s look at an example. Let’s
    say, for instance, we are using the GP surrogate model to estimate the following
    objective function. There’s no need to worry about what and how GP works for now;
    we will discuss it in more detail in the next section:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解在超参数调整阶段探索与利用权衡是如何发生的，让我们看一个例子。比如说，我们正在使用GP代理模型来估计以下目标函数。现在不需要担心GP是如何工作的，我们将在下一节中详细讨论：
- en: '![](img/Formula_B18753_04_017.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_017.png)'
- en: 'Here, ![](img/Formula_B18753_04_018.png) is a noise that follows the standard
    normal distribution. The following is a plot of this function within the range
    of ![](img/Formula_B18753_04_019.png). Note that, in this example, we are assuming
    that we know what the true objective function is. However, in practice, this function
    is unknown:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_04_018.png) 是遵循标准正态分布的噪声。以下是在![](img/Formula_B18753_04_019.png)范围内的该函数的绘图。请注意，在这个例子中，我们假设我们知道真正的目标函数是什么。然而，在实践中，这个函数是未知的：
- en: '![Figure 4.2 – Plot of the objective function, f(x)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 目标函数f(x)的绘图](img/B18753_04_002.jpg)'
- en: '](img/B18753_04_002.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18753_04_002.jpg](img/B18753_04_002.jpg)'
- en: Figure 4.2 – Plot of the objective function, f(x)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 目标函数f(x)的绘图
- en: 'Let’s say we are using the EI as the acquisition function, setting the number
    of trials as `15`, setting the initial number of points as `5`, and setting the
    ![](img/Formula_B18753_04_020.png) value to `0.01`. You can see how the fitting
    process works for the first five trials in the following figure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在使用EI作为获取函数，将试验次数设置为`15`，将初始点数设置为`5`，将![](img/Formula_B18753_04_020.png)的值设置为`0.01`。您可以在以下图中看到前五次试验的拟合过程：
- en: '![Figure 4.3 – GP and EI illustration, δ = 0.01'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – GP和EI说明，δ = 0.01](img/B18753_04_003.jpg)'
- en: '](img/B18753_04_003.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18753_04_003.jpg](img/B18753_04_003.jpg)'
- en: Figure 4.3 – GP and EI illustration, δ = 0.01
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – GP和EI说明，δ = 0.01
- en: Each row in the preceding figure corresponds to the first until the fifth trial.
    The left column contains information on the objective function (*red dashed line*),
    the GP surrogate model approximation of the objective function (*green dashed
    line*), how sure the approximation is (*green transparent area*), and the observed
    points up to each trial (*red dots*). The right column contains information on
    the EI acquisition function values (*blue line*) and the next point (*blue dot*)
    to be included in the next trials.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前图中每一行对应第一次到第五次试验。左侧列包含关于目标函数（**红色虚线**）、目标函数的GP代理模型近似（**绿色虚线**）、近似的确信度（**绿色透明区域**）以及到每次试验为止的观测点（**红色点**）。右侧列包含关于EI获取函数值（**蓝色线**）和下一个要包含在下次试验中的点（**蓝色点**）的信息。
- en: Let’s run through each of the rows in *Figure 4.3* so that you understand how
    it works. In the first trial (*see the first row from the top in the left column*),
    we initialize five random sample points – or hyperparameter values, in the context
    of hyperparameter tuning – and fit the GP model based on those five points. Remember
    that the GP model doesn’t know the actual objective function; the only information
    it has is just those five random points. Then (*see the first row from the top
    in the right column*), based on the fitted GP model, we get the value of the EI
    acquisition function across the space. In this case, the space is just a range
    – that is, ![](img/Formula_B18753_04_022.png). We also get the point to be included
    in the next trials, which in this case is around point `0.5`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析*图4.3*中的每一行，以便您了解它是如何工作的。在第一次试验中（*参见左列顶部的第一行*），我们初始化五个随机样本点——或者说是超参数调整中的超参数值，然后基于这五个点拟合GP模型。请记住，GP模型不知道实际的目标函数；它所拥有的唯一信息就是那五个随机点。然后（*参见右列顶部的第一行*），基于拟合的GP模型，我们在整个空间中获取EI获取函数的值。在这种情况下，空间只是一个范围——也就是说，![公式](img/Formula_B18753_04_022.png)。我们还得到了下一个试验要包含的点，在这个例子中大约是点`0.5`。
- en: In the second trial, we utilize the point suggested by the EI acquisition function
    and fit the GP model again based on the six sample points we have (*see the second
    row from the top in the left column*). If you compare the GP approximation of
    the second trial with the first trial, you will see that it is closer to the true
    objective function. Next (*see the second row from the top in the right column*),
    we repeat the same process, which is to generate the EI function value across
    the space and the point to be included in the next trial. The suggested point
    in this step is around `0.7`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次试验中，我们利用EI获取函数建议的点，并基于我们已有的六个样本点再次拟合GP模型（*参见左列顶部的第二行*）。如果您比较第二次试验的GP近似和第一次试验，您会看到它更接近真实的目标函数。接下来（*参见右列顶部的第二行*），我们重复同样的过程，即在整个空间中生成EI函数值和下一个试验要包含的点。在这个步骤中建议的点大约是`0.7`。
- en: 'We keep repeating the same process until the stopping criteria are met, which
    in this case is 15 trials. The following plot shows the result after 15 trials.
    It is much better than the approximation in the first trial (*see the green dashed
    line*)! You can also see that there are some ranges of ![](img/Formula_B18753_04_023.png)
    where the confidence of the GP approximation is high, such as around points `–1.5`
    and `1.6`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会重复进行同样的过程，直到满足停止标准，在这个例子中是15次试验。下面的图表显示了15次试验后的结果。这比第一次试验的近似要好得多（*参见绿色虚线*）！您还可以看到，在![公式](img/Formula_B18753_04_023.png)的一些范围内，高斯过程近似的置信度很高，例如在点`-1.5`和`1.6`附近：
- en: '![Figure 4.4 – Result after 15 trials, δ = 0.01'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 – 15次试验后的结果，δ = 0.01](img/Formula_B18753_04_023.png)'
- en: '](img/B18753_04_004.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_04_004.jpg)'
- en: Figure 4.4 – Result after 15 trials, δ = 0.01
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 15次试验后的结果，δ = 0.01
- en: 'Based on the preceding plot, the final suggested point, or the hyperparameter
    value, is `–1.5218`, which results in the value of the objective function being
    equal to `–1.9765`. Let’s also look at the convergence plot from the first until
    the last trial. From the following convergence plot, we can see how our surrogate
    model and acquisition function help us get the minimum value of the objective
    function based on all the trials:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的图表，最终建议的点，或者说超参数值，是`-1.5218`，这导致目标函数的值为`-1.9765`。让我们也看看从第一次试验到最后一次试验的收敛图。从下面的收敛图中，我们可以看到我们的代理模型和获取函数是如何帮助我们根据所有试验得到目标函数的最小值的：
- en: '![Figure 4.5 – Convergence plot'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5 – 收敛图](img/Formula_B18753_04_023.png)'
- en: '](img/B18753_04_005.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_04_005.jpg)'
- en: Figure 4.5 – Convergence plot
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 收敛图
- en: 'Now, let’s try to change the value of ![](img/Formula_B18753_04_025.png) to
    a lower value than what we had previously to see how the EI acquisition function
    will favor exploitation more than exploration. Let’s set the ![](img/Formula_B18753_04_026.png)
    value to be 1,000 times lower than the previous value. Note that we only change
    the ![](img/Formula_B18753_04_027.png)value and leave the other setups as-is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将![公式](img/Formula_B18753_04_025.png)的值改为比我们之前更低的值，以看看EI获取函数将如何更多地偏向利用而不是探索。让我们将![公式](img/Formula_B18753_04_026.png)的值设置为比之前低1000倍。请注意，我们只改变了![公式](img/Formula_B18753_04_027.png)的值，其他设置保持不变：
- en: '![Figure 4.6 – GP and EI illustration, δ = 0.00001'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.6 – GP和EI说明，δ = 0.00001](img/Formula_B18753_04_023.png)'
- en: '](img/B18753_04_006.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_04_006.jpg)'
- en: Figure 4.6 – GP and EI illustration, δ = 0.00001
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – GP和EI说明，δ = 0.00001
- en: 'As you can see, the EI acquisition function suggested most of the points in
    a range between **0.5** and **1.4**. The acquisition function doesn’t suggest
    exploring the ![](img/Formula_B18753_04_029.png) range, although we can get a
    much lower objective function value in that range. This happens because there
    are no initial random points in that range, and we favor exploitation a lot in
    this example. The following plot shows the final results after 15 trials. In this
    case, we get a worse result when we favor more exploitation over exploration.
    However, this is not always the case. *You have to experiment* since different
    data, different objective functions, a different hyperparameter space, and different
    implementations may result in different conclusions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，EI获取函数建议了介于 **0.5** 和 **1.4** 之间的大多数点。获取函数不建议探索 ![](img/Formula_B18753_04_029.png)
    范围，尽管我们可以在该范围内获得更低的目标函数值。这是因为该范围内没有初始随机点，在这个例子中，我们非常倾向于利用。以下图表显示了15次试验后的最终结果。在这种情况下，当我们更倾向于利用而不是探索时，我们得到了更差的结果。然而，这并不总是如此。*您必须进行实验*，因为不同的数据、不同的目标函数、不同的超参数空间和不同的实现可能会导致不同的结论：
- en: '![Figure 4.7 – Result after 15 trials, δ = 0.00001'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7 – 15次试验后的结果，δ = 0.00001'
- en: '](img/B18753_04_007.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18753_04_007.jpg)'
- en: Figure 4.7 – Result after 15 trials, δ = 0.00001
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 15次试验后的结果，δ = 0.00001
- en: 'Now, let’s see what the impact is if we set the ![](img/Formula_B18753_04_031.png)
    value to `100`, which in this case means that we favor exploration more than exploitation.
    Similar to the previous trial, after running 15 trials, we got the following results:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如果我们将 ![](img/Formula_B18753_04_031.png) 值设置为 `100`，这在这种情况下意味着我们更倾向于探索而不是利用，会产生什么影响。与之前的试验相似，运行15次试验后，我们得到了以下结果：
- en: '![Figure 4.8 – Result after 15 trials, δ = 100'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 – 15次试验后的结果，δ = 100'
- en: '](img/B18753_04_008.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18753_04_008.jpg)'
- en: Figure 4.8 – Result after 15 trials, δ = 100
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 15次试验后的结果，δ = 100
- en: As you can see, the points that are suggested by the acquisition function (*the
    red dots*) are all over the place. This is because we set such a high ![](img/Formula_B18753_04_033.png)
    value. This means that the acquisition function’s outputs will suggest points
    in the space that haven’t been observed yet. We will learn how to produce the
    plots shown here in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062), *Hyperparameter
    Tuning via Scikit*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，由获取函数（*红色点*）建议的点遍布各处。这是因为我们设置了如此高的 ![](img/Formula_B18753_04_033.png) 值。这意味着获取函数的输出将建议空间中尚未观察到的点。我们将在
    [*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)，*通过Scikit进行超参数调整* 中学习如何生成这里显示的图表。
- en: Besides the EI acquisition function, there are also other popular acquisition
    functions that you may consider using, including **Probability of Improvement**
    (**PI**) and **Upper Confidence Bound** (**UCB**).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 EI 获取函数之外，还有其他一些流行的获取函数可以考虑使用，包括 **改进概率**（**PI**）和 **上置信界**（**UCB**）。
- en: 'PI is the acquisition function that existed before EI. It is simpler than EI
    – in fact, the formula of ![](img/Formula_B18753_04_034.png) is derived based
    on the following simple definition of *improvement*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: PI 是在 EI 之前存在的获取函数。它比 EI 简单 – 事实上，![](img/Formula_B18753_04_034.png) 的公式是根据以下简单的
    *改进* 定义推导出来的：
- en: '![](img/Formula_B18753_04_035.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_035.png)'
- en: 'The idea of ![](img/Formula_B18753_04_036.png) is to return the size of improvement,
    if there is improvement between the expected performance and the current best
    performance, or just return zero if there is no improvement. Based on ![](img/Formula_B18753_04_037.png),
    we can define PI as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B18753_04_036.png) 的想法是返回改进的大小，如果预期性能和当前最佳性能之间存在改进，则返回改进的大小；如果没有改进，则返回零。基于
    ![](img/Formula_B18753_04_037.png)，我们可以定义 PI 如下：'
- en: '![](img/Formula_B18753_04_038.png) when ![](img/Formula_B18753_04_039.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_038.png) 当 ![](img/Formula_B18753_04_039.png)'
- en: '![](img/Formula_B18753_04_040.png) when ![](img/Formula_B18753_04_041.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_040.png) 当 ![](img/Formula_B18753_04_041.png)'
- en: 'The problem with PI is that it will *give the same reward for all sets of hyperparameters*,
    so long as there’s an improvement compared to the current best value, ![](img/Formula_B18753_04_042.png),
    no matter how big the improvement is. This behavior is not very preferable in
    practice since it can *guide us to the local minima and get us stuck in there*.
    If you are familiar with calculus and statistics, you will realize that EI is
    just the expectation over ![](img/Formula_B18753_04_037.png), as shown here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PI 的问题在于，只要与当前最佳值 ![](img/Formula_B18753_04_042.png) 相比有所改进，它就会为所有超参数集提供相同的奖励，无论改进有多大。在实践中，这种行为并不太可取，因为它可能会
    *引导我们走向局部最小值并使我们陷入其中*。如果你熟悉微积分和统计学，你会意识到 EI 只是 ![](img/Formula_B18753_04_037.png)
    的期望，如这里所示：
- en: '![](img/Formula_B18753_04_044.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_044.png)'
- en: Here, ![](img/Formula_B18753_04_045.png) is the probability density function
    of the standard normal distribution. Unlike PI, the *EI acquisition function will
    take the size of improvement into account*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_04_045.png) 是标准正态分布的概率密度函数。与 PI 不同，*EI 收敛函数会考虑改进的大小*。
- en: 'As for the UCB, it is very straightforward compared to others. We have the
    power to control the trade-off between exploration and exploitation by ourselves
    via the ![](img/Formula_B18753_04_046.png)parameter. This acquisition function
    can be defined as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 UCB 来说，与其他方法相比，它非常直接。我们通过 ![](img/Formula_B18753_04_046.png) 参数有权力自己控制探索和利用之间的权衡。这个收敛函数可以定义为以下：
- en: '![](img/Formula_B18753_04_047.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_047.png)'
- en: As you can see, UCB *doesn’t take into account the current best value* of the
    objective function. It only considers the expected performance and the uncertainty
    captured by the surrogate model. You can control the exploration and exploitation
    trade-off by changing the ![](img/Formula_B18753_04_048.png)value. If you want
    to lean toward exploring the search space, then you can increase the value of
    ![](img/Formula_B18753_04_049.png). However, if you want to focus more on the
    set of hyperparameters that are expected to perform well, then you can decrease
    the value of ![](img/Formula_B18753_04_050.png).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，UCB 并没有考虑目标函数的当前最佳值。它只考虑了代理模型的预期性能和不确定性。你可以通过改变 ![](img/Formula_B18753_04_048.png)
    的值来控制探索和利用之间的权衡。如果你想偏向于探索搜索空间，那么你可以增加 ![](img/Formula_B18753_04_049.png) 的值。然而，如果你想更多地关注那些预期表现良好的超参数集，那么你可以减少
    ![](img/Formula_B18753_04_050.png) 的值。
- en: Apart from the variations of surrogate model and acquisition functions, there
    are also other variations of BO methods based on modifying the algorithm itself,
    including Metis and **Bayesian optimization and HyperBand** (**BOHB**). We will
    discuss Metis in the *Understanding Metis* section and BOHB in [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054),
    *Exploring* *Multi-Fidelity Optimization*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了代理模型和收敛函数的变体之外，还有基于修改算法本身的 BO 方法变体，包括 Metis 和 **贝叶斯优化和 HyperBand** (**BOHB**)。我们将在
    *理解 Metis* 部分讨论 Metis，并在 [*第 6 章*](B18753_06_ePub.xhtml#_idTextAnchor054) *探索多保真优化*
    中讨论 BOHB。
- en: 'The following are the pros and cons of BO hyperparameter tuning, in general,
    compared to other hyperparameter tuning methods:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与其他超参数调整方法相比，BO 超参数调整的优缺点：
- en: '![Figure 4.9 – Pros and cons of BO'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9 – BO 的优缺点](#)'
- en: '](img/B18753_04_009.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_009.jpg)'
- en: Figure 4.9 – Pros and cons of BO
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – BO 的优缺点
- en: BO can handle expensive objective functions and is more data-efficient and arguably
    better than random search when it has good initial points. You can utilize the
    set of hyperparameters we used for the initial points up to *Step 6* from the
    procedure mentioned at the beginning of this section. However, if you don’t have
    that privileged access, BO still can outperform random search if you give the
    method some more time since it has to build a good surrogate model first from
    scratch, especially if you have a huge hyperparameter space. Once BO has built
    a good surrogate model, it tends to work faster than random search to find the
    optimal set of hyperparameters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: BO 可以处理昂贵的目标函数，并且当有良好的初始点时，比随机搜索更数据高效，可以说是更好的。你可以利用我们在本节开头提到的流程中从 *步骤 6* 开始使用的超参数集。然而，如果你没有这种特权访问，BO
    仍然可以通过给你更多时间来超越随机搜索，因为它必须首先从头开始构建一个好的代理模型，特别是如果你有一个巨大的超参数空间。一旦 BO 构建了一个好的代理模型，它往往比随机搜索更快地找到最优的超参数集。
- en: There is also another way to speed up the relatively slow warm-up process of
    BO. The idea is to adopt a **meta-learning** procedure to initialize the initial
    set of hyperparameters by learning from meta-features in other, similar datasets.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种方法可以加速BO相对较慢的预热过程。这个想法是采用**元学习**程序，通过从其他类似数据集的元特征中学习来初始化初始超参数集。
- en: Speeding Up BO’s Warm-Up
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 加速BO的预热过程
- en: 'See the following paper for more information: *Efficient and Robust Automated
    Machine Learning*, by Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost
    Springenberg, Manuel Blum, Frank Hutter ([https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html](https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html)).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅以下论文：*高效且鲁棒的自动机器学习*，作者Matthias Feurer，Aaron Klein，Katharina Eggensperger，Jost
    Springenberg，Manuel Blum，Frank Hutter ([https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html](https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html))。
- en: BO also has a nice feature that random search doesn’t have – the ability to
    control the exploration and exploitation trade-off, as explained previously in
    this section. This feature enables BO to do more than just constantly explore,
    as random search does.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: BO还有一个随机搜索没有的不错特性——能够控制探索和利用之间的权衡，如本节之前所述。这个特性使得BO不仅仅像随机搜索那样不断探索。
- en: Now that you are aware of what BO is, how it works, what its important components
    are, and the pros and cons of this method, we will dive deeper into the variations
    of BO in the following sections.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了什么是BO（贝叶斯优化），它是如何工作的，它的重要组件是什么，以及这种方法的优势和劣势，我们将在接下来的章节中深入探讨BO的变体。
- en: Understanding BO GP
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解BO GP
- en: '**Bayesian optimization Gaussian process** (**BOGP**) is one of the variants
    of the BO hyperparameter tuning method. It is well-known for its good capability
    in describing the objective function. This variant is very popular due to the
    unique *analytically tractable* nature of the surrogate model and its ability
    to produce relatively accurate approximation, even with only a few observed points.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化高斯过程**（**BOGP**）是BO超参数调整方法的一种变体。它因其良好的描述目标函数的能力而闻名。这种变体因其代理模型的独特**可分析性**和即使只有少量观测点也能产生相对准确的近似而非常受欢迎。'
- en: However, BOGP has limitations. It *only works on continuous hyperparameters*,
    not on the discrete or categorical types of hyperparameters. It is not recommended
    to use BOGP when you need a lot of iterations to get the optimal set of hyperparameters,
    especially when you have a large number of samples. This is BOGP has a ![](img/Formula_B18753_04_051.png)
    runtime, where ![](img/Formula_B18753_04_052.png) is the number of samples. If
    you have *more than 10 hyperparameters* to be optimized, the common belief is
    that BOGP is not the right hyperparameter tuning method for you.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BOGP（贝叶斯优化高斯过程）有其局限性。它**仅适用于连续超参数**，不适用于离散或分类类型的超参数。当你需要大量迭代来获得最佳超参数集时，不建议使用BOGP，尤其是当你有大量样本时。这是因为BOGP的运行时间，其中![](img/Formula_B18753_04_051.png)是样本数量。如果你有**超过10个需要优化的超参数**，普遍认为BOGP不是适合你的超参数调整方法。
- en: Having GP as the surrogate model means that we utilize GP as the *prior* for
    our objective function. Then, we can utilize the prior along with a *likelihood
    model* to compute the *posterior* that we care about. All of these nerdy terms
    can easily be understood if we are familiar with the famous **Bayes Theorem**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以GP（高斯过程）作为代理模型意味着我们利用GP作为目标函数的**先验**。然后，我们可以利用先验和**似然模型**来计算我们关心的**后验**。如果我们熟悉著名的**贝叶斯定理**，所有这些复杂的术语都可以很容易地理解。
- en: 'Bayes Theorem allows us to calculate the probability of an event, given a specific
    condition, by utilizing our previous knowledge or common belief that we have.
    Formally, Bayes Theorem is defined as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理允许我们通过利用我们已有的知识或普遍信念来计算在特定条件下事件发生的概率。形式上，贝叶斯定理定义为如下：
- en: '![](img/Formula_B18753_04_053.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_053.png)'
- en: Here, ![](img/Formula_B18753_04_054.png)is the event we want to know the probability
    of, and ![](img/Formula_B18753_04_116.png) refers to the specific condition we
    mentioned previously. The left-hand side of the equation, ![](img/Formula_B18753_04_055.png),
    is what we called as the posterior. ![](img/Formula_B18753_04_056.png) is the
    prior and ![](img/Formula_B18753_04_057.png) is what we call the likelihood model.
    Finally, ![](img/Formula_B18753_04_058.png) is just a constant to ensure that
    the resulting value of this formula is bound in the range of ![](img/Formula_B18753_04_059.png).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![公式 B18753_04_054](img/Formula_B18753_04_054.png)是我们想要知道概率的事件，而![公式 B18753_04_116](img/Formula_B18753_04_116.png)指的是我们之前提到的特定条件。方程的左边，![公式
    B18753_04_055](img/Formula_B18753_04_055.png)，就是我们所说的后验。![公式 B18753_04_056](img/Formula_B18753_04_056.png)是先验，![公式
    B18753_04_057](img/Formula_B18753_04_057.png)是我们所说的似然模型。最后，![公式 B18753_04_058](img/Formula_B18753_04_058.png)只是一个常数，以确保这个公式的结果被限制在![公式
    B18753_04_059](img/Formula_B18753_04_059.png)的范围内。
- en: To understand Bayes Theorem, let’s walk through an example. Let’s say we want
    to know the probability of you eating at your favorite restaurant, given that
    today’s weather is sunny. In this example, you eating at your favorite restaurant
    is the event we are interested in. This is ![](img/Formula_B18753_04_060.png)
    in the equation. The information that today is sunny refers to ![](img/Formula_B18753_04_117.png)
    in the equation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解贝叶斯定理，让我们通过一个例子来解释。假设我们想知道在今天是晴天的情况下，你吃你最喜欢的餐厅的概率。在这个例子中，你吃你最喜欢的餐厅是我们感兴趣的事件。这在方程中是![公式
    B18753_04_060](img/Formula_B18753_04_060.png)。今天是晴天这一信息指的是方程中的![公式 B18753_04_117](img/Formula_B18753_04_117.png)。
- en: Let’s say you are eating at your favorite restaurant for 40 out of 100 days.
    This means that before knowing what today’s weather is, your ![](img/Formula_B18753_04_061.png)
    is equal to ![](img/Formula_B18753_04_062.png). Let’s also assume that out of
    100 days, there are 30 sunny days. Then, the ![](img/Formula_B18753_04_063.png)
    value is equal to ![](img/Formula_B18753_04_064.png). Based on your experience
    of eating at your favorite restaurant, you have realized that you ate in the sunny
    weather condition 20 out of 40 times. Thus, the likelihood, ![](img/Formula_B18753_04_065.png),
    is equal to ![](img/Formula_B18753_04_066.png). Using all of this information,
    we can calculate the probability of you eating at your restaurant, given that
    today’s weather is sunny, as ![](img/Formula_B18753_04_067.png)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你中有40天在你的最喜欢的餐厅吃饭，这意味着在知道今天的天气之前，你的![公式 B18753_04_061](img/Formula_B18753_04_061.png)等于![公式
    B18753_04_062](img/Formula_B18753_04_062.png)。让我们再假设在100天中，有30天是晴天。那么，![公式 B18753_04_063](img/Formula_B18753_04_063.png)的值等于![公式
    B18753_04_064](img/Formula_B18753_04_064.png)。根据你在你最喜欢的餐厅吃饭的经验，你已经意识到在晴天条件下你吃了40次中的20次。因此，似然，![公式
    B18753_04_065](img/Formula_B18753_04_065.png)，等于![公式 B18753_04_066](img/Formula_B18753_04_066.png)。使用所有这些信息，我们可以计算出在今天是晴天的情况下，你吃你餐厅的概率，如![公式
    B18753_04_067](img/Formula_B18753_04_067.png)所示。
- en: 'Now, we are ready to revisit the GP. BOGP utilizes GP as the surrogate model.
    GP as the surrogate model means that we utilize it as the prior of our objective
    function, which implies that the *posterior distribution is also a GP*. You can
    think of GP as a generalization of a Gaussian distribution that you are familiar
    with. Unlike Gaussian distribution, which describes the distribution of a random
    variable, *GP describes the distribution over functions*. Similar to the Gaussian
    distribution that is accompanied by the mean and variance of the random variable,
    GP is also accompanied by the *mean and covariance* of the function. As for the
    *likelihood*, we assume that the objective function, *f*, follows a normal likelihood
    with noise:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备重新审视GP。BOGP利用GP作为代理模型。GP作为代理模型意味着我们将其作为目标函数的先验，这意味着*后验分布也是一个GP*。你可以把GP看作是你熟悉的高斯分布的推广。与描述随机变量分布的高斯分布不同，*GP描述的是函数的分布*。与伴随随机变量均值和方差的Gaussian分布类似，GP也伴随函数的*均值和协方差*。至于*似然*，我们假设目标函数*f*遵循带有噪声的正态似然：
- en: '![](img/Formula_B18753_04_068.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B18753_04_068](img/Formula_B18753_04_068.png)'
- en: '![](img/Formula_B18753_04_069.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B18753_04_069](img/Formula_B18753_04_069.png)'
- en: 'Then, we can describe ![](img/Formula_B18753_04_070.png), or the values of
    our objective function for all *n* samples. as a GP with a mean function of ![](img/Formula_B18753_04_071.png)
    and a covariance kernel, ![](img/Formula_B18753_04_072.png), sized *n x n*, which
    is defined as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以描述![公式 B18753_04_070](img/Formula_B18753_04_070.png)，即所有*n*个样本的目标函数的值，作为一个具有均值函数![公式
    B18753_04_071](img/Formula_B18753_04_071.png)和协方差核![公式 B18753_04_072](img/Formula_B18753_04_072.png)的GP，大小为*n
    x n*，其定义如下：
- en: '![](img/Formula_B18753_04_073.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B18753_04_073](img/Formula_B18753_04_073.png)'
- en: 'The distribution of prediction from GP also follows the Gaussian distribution,
    which can be defined as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GP 的预测分布也遵循高斯分布，可以定义为以下：
- en: '![](img/Formula_B18753_04_074.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_074.png)'
- en: Here, the value of ![](img/Formula_B18753_04_075.png)and ![](img/Formula_B18753_04_076.png)
    can be *analytically derived from the kernel*, ![](img/Formula_B18753_04_077.png).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_04_075.png)和![](img/Formula_B18753_04_076.png)的值可以从核，![](img/Formula_B18753_04_077.png)，中解析地推导出来。
- en: To summarize, *GP approximates the objective function by following a normal
    distribution assumption*. In practice, GP can also be utilized when we don’t have
    zero mean processes, as per our previous assumption. However, we need to do some
    preprocessing on the values of the objective function to center them to zero.
    Choosing the *right covariance kernel*, ![](img/Formula_B18753_04_078.png), is
    also crucial. It highly impacts the performance of our hyperparameter tuning process.
    The most popular kernel that’s used in practice is the *Matern kernel*. However,
    we must choose the right kernel for our case, since each kernel has a characteristic
    that may or may not be suitable for our objective function. We will discuss the
    kernels that are available in the Scikit package in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062),
    *Hyperparameter Tuning via Scikit*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，*GP 通过遵循正态分布假设来近似目标函数*。在实践中，即使我们没有零均值过程，也可以使用 GP，正如我们之前的假设。然而，我们需要对目标函数的值进行一些预处理，以便将其中心化到零。选择
    *合适的协方差核*，![](img/Formula_B18753_04_078.png)，也是至关重要的。它极大地影响了我们的超参数调整过程。在实践中最常用的核是
    *Matern 核*。然而，我们必须为我们的案例选择合适的核，因为每个核都有可能或可能不适合我们的目标函数。我们将在 [*第 7 章*](B18753_07_ePub.xhtml#_idTextAnchor062)，*通过
    Scikit 进行超参数调整*中讨论 Scikit 包中可用的核。
- en: 'The following table shows the list of pros and cons of BOGP compared to other
    variants of the BO hyperparameter tuning method:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了 BOGP 与其他 BO 超参数调整方法变体相比的优缺点列表：
- en: '![Figure 4.10 – Pros and cons of BOGP'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10 – BOGP 的优缺点'
- en: '](img/B18753_04_010.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_010.jpg)'
- en: Figure 4.10 – Pros and cons of BOGP
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – BOGP 的优缺点
- en: In the previous section, we saw how GP works in practice, where we discussed
    the exploration and exploitation trade-off. You can revisit that example to get
    a better understanding of how GP works in practice through the help of visualizations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了 GP 在实际中的应用，其中我们讨论了探索与利用的权衡。你可以回顾那个例子，通过可视化更好地理解 GP 在实际中的应用。
- en: In this section, we learned about utilizing GP as the surrogate model in BO,
    along with the pros and cons compared to other variants of BO. In the next section,
    we will learn about another variant of BO that utilizes random forest as the surrogate
    model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了在 BO 中使用 GP 作为代理模型，以及与其他 BO 变体相比的优缺点。在下一节中，我们将学习另一种使用随机森林作为代理模型的 BO
    变体。
- en: Understanding SMAC
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 SMAC
- en: '**SMAC** is part of the BO hyperparameter tuning method group and utilizes
    random forest as the surrogate model. This method is optimized to handle discrete
    or categorical hyperparameters. If your hyperparameter space is huge and is dominated
    by discrete hyperparameters, then SMAC is a good choice for you.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**SMAC** 是 BO 超参数调整方法组的一部分，并使用随机森林作为代理模型。这种方法经过优化以处理离散或分类超参数。如果你的超参数空间很大，且主要由离散超参数主导，那么
    SMAC 是你的一个不错的选择。'
- en: Similar to BOGP, SMAC also works by modeling the objective function. Specifically,
    it utilizes random forest as the surrogate model to create an estimation of the
    real objective function, which can then be passed to the acquisition function
    (see the *Introducing BO* section for more details).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BOGP 类似，SMAC 也通过建模目标函数来工作。具体来说，它使用随机森林作为代理模型来创建对真实目标函数的估计，然后可以将该估计传递给获取函数（有关更多详细信息，请参阅
    *介绍 BO* 部分）。
- en: Random forest is a **machine learning** (**ML**) algorithm that can be utilized
    in classification or regression tasks. It is built upon a collection of decision
    trees, which is known to perform well with categorical types of features. The
    name random forest comes from the fact that it is built from several decision
    trees. We will discuss random forest, along with its hyperparameters, in more
    detail in [*Chapter 11*](B18753_11_ePub.xhtml#_idTextAnchor110), *Understanding
    Hyperparameters of Popular Algorithms*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种**机器学习**（**ML**）算法，可用于分类或回归任务。它基于一系列决策树，已知在处理分类类型特征时表现良好。随机森林这个名字来源于它是由多个决策树构建而成的。我们将在[*第11章*](B18753_11_ePub.xhtml#_idTextAnchor110)《理解流行算法的超参数》中更详细地讨论随机森林及其超参数。
- en: 'The main difference between SMAC and BOGP lies in the type of surrogate model
    that’s used in each method. While BOGP utilizes GP as the surrogate model, SMAC
    utilizes random forest as the surrogate model. The acquisition function that was
    used in the original paper on SMAC is the *EI function with some modifications*
    on how the optimization process in *Step 8* in the *Introducing BO* section is
    done, which also can be seen in the following screenshot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: SMAC与BOGP之间的主要区别在于每种方法中使用的代理模型类型。虽然BOGP使用高斯过程（GP）作为代理模型，但SMAC使用随机森林作为代理模型。在SMAC的原论文中使用的获取函数是经过一些修改的*EI函数*，这些修改涉及在*介绍BO*部分的*步骤8*中的优化过程，这也可以在下面的截图中看到：
- en: '![Figure 4.11 – Optimization process of the acquisition function'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11 – 获取函数的优化过程](img/Formula_B18753_04_080.png)'
- en: '](img/B18753_04_011.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_04_011.jpg)'
- en: Figure 4.11 – Optimization process of the acquisition function
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 获取函数的优化过程
- en: 'In SMAC, similar to BOGP, we are also assuming that the distribution of our
    surrogate model’s *prediction follows the Gaussian distribution*, as shown here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在SMAC中，类似于BOGP，我们也假设我们的代理模型的*预测分布遵循高斯分布*，如下所示：
- en: '![](img/Formula_B18753_04_079.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![公式图](img/Formula_B18753_04_079.png)'
- en: Here, the ![](img/Formula_B18753_04_080.png)and ![](img/Formula_B18753_04_081.png)
    values are derived from the *random forest prediction’s mean and variance*, respectively.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_04_080.png)和![](img/Formula_B18753_04_081.png)的值分别来自随机森林预测的均值和方差。
- en: We can also *utilize random forest to perform hyperparameter tuning on a random
    forest model*! How is this possible? How can a model be used to improve the performance
    of another model of the same type?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以*利用随机森林对一个随机森林模型进行超参数调整*！这是如何实现的？一个模型如何被用来提高同类型另一个模型的表现？
- en: It is possible because we are treating one model as the surrogate model while
    the other one is the actual model that is fitted to the independent variables
    to predict the dependent variable. As the surrogate model, random forest will
    act as the regressor, which has the goal of learning the relationship between
    the hyperparameter space and the corresponding objective function. So, when we
    said that we are utilizing random forest to perform hyperparameter tuning on a
    random forest model, there are two random forest models with different goals and
    different input-output pairs!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们将一个模型作为代理模型处理，而另一个模型是实际模型，它被拟合到独立变量以预测因变量。作为代理模型，随机森林将充当回归器，其目标是学习超参数空间与相应目标函数之间的关系。因此，当我们说我们在随机森林模型上使用随机森林进行超参数调整时，存在两个具有不同目标和不同输入输出对的随机森林模型！
- en: 'Take a look at the following steps to get a better understanding of this concept.
    Note that the following procedure replaces *Steps 7* to *11* in the *Introducing
    BO* section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下步骤以更好地理解这个概念。请注意，以下程序替换了*介绍BO*部分的*步骤7*到*11*：
- en: 6\. (The first few steps are the same as we saw earlier).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 6. (最初几步与之前看到的一样)。
- en: 7\. Fit the *first random forest model*, which acts as a surrogate model, *M*,
    using the value pairs in *D*. Remember that *D* consists of pairs of hyperparameter
    values and the cross-validation score.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 使用*D*中的值对拟合第一个随机森林模型，该模型作为代理模型*M*。记住，*D*由超参数值和交叉验证分数的成对值组成。
- en: '8\. Sample the next set of hyperparameters by utilizing the acquisition function,
    *A*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 通过使用获取函数*A*采样下一组超参数：
- en: Perform optimization on the acquisition function with the help of the surrogate
    model, *M*, to sample which hyperparameters are to be passed to the acquisition
    function.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理模型*M*的帮助下对获取函数进行优化，以采样要传递给获取函数的超参数。
- en: Get the optimal set of hyperparameters based on the acquisition function.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据获取函数获取最佳超参数集。
- en: 9\. Compute the cross-validation score using the objective function, *f*, based
    on the output from *Step 8*. Note that the cross-validation score is computed
    based on the *second random forest model*, whose goal is to learn the relationship
    between the dependent and independent variables from our original problem.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. 使用 *步骤 8* 的输出，根据目标函数 *f* 计算交叉验证分数。请注意，交叉验证分数是基于 *第二个随机森林模型* 计算的，其目标是学习原始问题中自变量和因变量之间的关系。
- en: 10\. Add the hyperparameters and cross-validation score pair from *Step 8* and
    *Step 9* to set *D*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 10\. 将 *步骤 8* 和 *步骤 9* 中的超参数和交叉验证分数对添加到设置 *D*。
- en: 11\. Repeat *Steps 7* to *10* until the stopping criteria are met.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 11\. 重复 *步骤 7* 到 *10*，直到满足停止标准。
- en: 12\. (The last few steps are the same as we saw earlier).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 12\. （最后几步与之前看到的是相同的）。
- en: You may be wondering, why bother utilizing the same ML algorithm as the surrogate
    model? Why don’t we just perform a grid search or random search instead? Remember
    that the surrogate model is just one piece of the full BO algorithm. There is
    also the acquisition function and other optimization steps that can help us get
    the optimal set of hyperparameters faster. It is worth noting that *we can utilize
    any ML model* other than random forest. When it comes to tree-based ML models,
    XGBoost, CatBoost, and LightGBM are also popular among data scientists since they
    work well in practice.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道，为什么还要使用与代理模型相同的机器学习算法？为什么我们不直接进行网格搜索或随机搜索呢？记住，代理模型只是全局优化算法的一部分。还有获取函数和其他优化步骤可以帮助我们更快地获得最佳超参数集。值得注意的是，*我们可以利用除了随机森林之外的任何机器学习模型*。当涉及到基于树的机器学习模型时，XGBoost、CatBoost
    和 LightGBM 在数据科学家中也同样受欢迎，因为它们在实际应用中表现良好。
- en: 'In the *Introducing BO* section, we saw how GP works with the EI acquisition
    function to estimate a dummy objective function. Let’s use the same dummy objective
    function, as defined here, and see the result of utilizing random forest (not
    necessarily the SMAC algorithm) as the surrogate model instead of GP. We will
    still use EI as the acquisition function in this example and the Scikit-optimize
    package as the implementation:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *介绍全局优化（BO）* 部分，我们看到了 GP 如何与 EI 获取函数一起估计虚拟目标函数。让我们使用这里定义的相同虚拟目标函数，并查看使用随机森林（不一定是
    SMAC 算法）作为代理模型而不是 GP 的结果。在这个例子中，我们仍然将 EI 作为获取函数，并使用 Scikit-optimize 包作为实现：
- en: '![](img/Formula_B18753_04_082.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_082.png)'
- en: Here, ![](img/Formula_B18753_04_083.png) is a noise that follows the standard
    normal distribution. Please see *Figure 4.2* for a visualization of this dummy
    objective function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_04_083.png) 是一个遵循标准正态分布的噪声。请参见 *图 4.2* 以查看此虚拟目标函数的可视化。
- en: 'Let’s set the number of trials and the exploitation versus exploration trade-off
    controller, ![](img/Formula_B18753_04_084.png), using the default values given
    by the Scikit-optimize package for the random forest surrogate model, which are
    `100` and `0.01`, respectively. You can see how the random forest surrogate model
    fitting process works for the first five trials in the following figure:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Scikit-optimize 包为随机森林代理模型提供的默认值来设置试验次数和探索与利用权衡控制器，![](img/Formula_B18753_04_084.png)，分别是
    `100` 和 `0.01`。您可以在以下图中看到随机森林代理模型拟合过程在前五次试验中的工作情况：
- en: '![Figure 4.12 – Random forest and EI illustration; δ = 0.01; trials 1 – 5'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12 – 随机森林和 EI 插图；δ = 0.01；试验 1 – 5'
- en: '](img/B18753_04_012.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_012.jpg)'
- en: Figure 4.12 – Random forest and EI illustration; δ = 0.01; trials 1 – 5
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 随机森林和 EI 插图；δ = 0.01；试验 1 – 5
- en: 'As you can see, not many things happened in the first five trials. Even the
    approximation of the objective function that’s given by the random forest (*see
    the green-dashed line*) is still very bad since it is just a straight line! Let’s
    see what the condition is during trials 71 until 75:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在前五次试验中并没有发生太多事情。即使是随机森林给出的目标函数近似（见绿色虚线）仍然非常糟糕，因为它仅仅是一条直线！让我们看看在试验 71 到
    75 期间的条件是什么：
- en: '![Figure 4.13 – Random forest and EI illustration; δ = 0.01; trials 71- 75'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13 – 随机森林和 EI 插图；δ = 0.01；试验 71- 75'
- en: '](img/B18753_04_013.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_013.jpg)'
- en: Figure 4.13 – Random forest and EI illustration; δ = 0.01; trials 71- 75
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 随机森林和 EI 插图；δ = 0.01；试验 71- 75
- en: 'Here, we can see that our random forest surrogate model has improved a lot
    in estimating the true objective function. One interesting point is that the acquisition
    function curve looks very different from the one we saw when utilizing GP as the
    surrogate model. Here, the acquisition function looks edgier, just like the one
    we usually see from visualizing random forest. Finally, let’s see what the final
    form of the approximated function is:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的随机森林代理模型在估计真实目标函数方面有了很大的改进。一个有趣的观点是，获取函数曲线看起来与我们使用 GP 作为代理模型时看到的非常不同。在这里，获取函数看起来更锋利，就像我们通常从可视化随机森林中看到的那样。最后，让我们看看近似函数的最终形式：
- en: '![Figure 4.14 – Result after 100 trials; δ = 0.01'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14 – 100 次试验后的结果；δ = 0.01'
- en: '](img/B18753_04_014.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_014.jpg)'
- en: Figure 4.14 – Result after 100 trials; δ = 0.01
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 100 次试验后的结果；δ = 0.01
- en: 'Here, we can see that random forest fails to fit the true objective function
    in general, but it succeeds to focus on the local minima of the objective function.
    This happens because *random forest needs a lot of data*, or in this case, the
    observed points (*see red dots*), to have a good approximation of the objective
    function. You can also see the convergence plot of the fitting process, starting
    from the first until the last trial, in the following plot. If we compare *Figure
    4.15* to *Figure 4.5*, we can easily see that, in this example, random forest,
    when supported by the EI acquisition function, learns much slower than GP supported
    by EI:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到随机森林通常无法拟合真实的目标函数，但它成功地关注了目标函数的局部最小值。这是因为 *随机森林需要大量的数据*，或者在这种情况下，观察到的点（*见红色圆点*），以对目标函数进行良好的近似。您还可以在以下图中看到拟合过程的收敛图，从第一次到最后的试验。如果我们比较
    *图 4.15* 和 *图 4.5*，我们可以很容易地看出，在这个例子中，当有 EI 获取函数的支持时，随机森林的学习速度比 EI 支持的 GP 慢得多：
- en: '![Figure 4.15 – Convergence plot'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.15 – 收敛图'
- en: '](img/B18753_04_015.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_015.jpg)'
- en: Figure 4.15 – Convergence plot
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 收敛图
- en: 'From *Figure 4.14*, we can also see that, currently, we are only focusing on
    several ranges and missing the global minima of the dummy objective function,
    which is located around the ![](img/Formula_B18753_04_088.png) range. Let’s see
    if changing the value of ![](img/Formula_B18753_04_089.png) to `100` can solve
    this issue. The expectation is that the EI acquisition function can help the random
    forest surrogate model *explore more* in other ranges of values as well. You can
    see the result of the first five trials in the following figure:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 4.14* 我们还可以看到，目前我们只关注几个范围，而忽略了虚拟目标函数的全局最小值，它位于 ![](img/Formula_B18753_04_088.png)
    范围附近。让我们看看将 ![](img/Formula_B18753_04_089.png) 的值更改为 `100` 是否可以解决这个问题。预期 EI 获取函数可以帮助随机森林代理模型在其他值范围内
    *探索更多*。您可以在以下图中看到前五次试验的结果：
- en: '![Figure 4.16 – Random forest and EI illustration; δ = 100; trials 1 – 5'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.16 – 随机森林和 EI 插图；δ = 100；试验 1 – 5'
- en: '](img/B18753_04_016.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_016.jpg)'
- en: Figure 4.16 – Random forest and EI illustration; δ = 100; trials 1 – 5
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 随机森林和 EI 插图；δ = 100；试验 1 – 5
- en: 'Similar to the first five trials of the default ![](img/Formula_B18753_04_091.png)
    value, we still can’t see much of the learning process. Let’s see what the condition
    is during trials 71 until 75:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 与默认值的前五次试验类似，我们仍然看不到太多学习过程。让我们看看试验 71 到 75 期间的条件：
- en: '![Figure 4.17 – Random forest and EI illustration; δ = 100; trials 71 – 75'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.17 – 随机森林和 EI 插图；δ = 100；试验 71 – 75'
- en: '](img/B18753_04_017.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_017.jpg)'
- en: Figure 4.17 – Random forest and EI illustration; δ = 100; trials 71 – 75
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 – 随机森林和 EI 插图；δ = 100；试验 71 – 75
- en: 'Here, we can see a very big difference between *Figure 4.17* and *Figure 4.13*.
    Finally, let’s see what the final form of the approximated function is:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 *图 4.17* 和 *图 4.13* 之间有很大的差异。最后，让我们看看近似函数的最终形式：
- en: '![Figure 4.18 – Result after 100 trials; δ = 100'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.18 – 100 次试验后的结果；δ = 100'
- en: '](img/B18753_04_018.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_018.jpg)'
- en: Figure 4.18 – Result after 100 trials; δ = 100
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 – 100 次试验后的结果；δ = 100
- en: By changing the value of ![](img/Formula_B18753_04_094.png) to `100`, it seems
    that our expectation has been achieved. The approximation from the random forest
    surrogate model (*see the green-dashed line*) is now focusing on more than specific
    ranges. Moreover, we even get a better result compared to GP (see *Figure 4.4*).
    Again, it is worth noting that this is not always the case – you must experiment
    a lot on your own since different data, different objective functions, a different
    hyperparameter space, and different implementations may result in different conclusions.
    We will learn how to implement random forest as the surrogate model and how to
    produce these figures in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062),
    *Hyperparameter Tuning via Scikit*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将![](img/Formula_B18753_04_094.png)的值改为`100`，似乎我们的期望已经实现。随机森林代理模型的近似（*见绿色虚线*）现在更多地关注特定范围之外。此外，我们甚至比GP（*见*图4.4*）得到了更好的结果。再次强调，这并不总是如此——你必须自己大量实验，因为不同的数据、不同的目标函数、不同的超参数空间和不同的实现可能会导致不同的结论。我们将在[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)中学习如何实现随机森林作为代理模型以及如何生成这些图表，*通过Scikit进行超参数调整*。
- en: There is another method, called **Bayesian optimization inside a Grove** (**BOinG**),
    whose goal is to get the best of both worlds by utilizing random forest and GP
    as surrogate models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一种方法，称为**Grove内的贝叶斯优化**（**BOinG**），其目标是利用随机森林和GP作为代理模型，从两个世界中获取最佳效果。
- en: Bayesian Optimization Inside a Grove
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Grove内的贝叶斯优化
- en: 'See the following paper for more information: *Searching in the Forest for
    Local Bayesian Optimization*, by Difan Deng and Marius Lindauer ([https://arxiv.org/abs/2111.05834](https://arxiv.org/abs/2111.05834)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅以下论文：*在森林中搜索局部贝叶斯优化*，作者为Difan Deng和Marius Lindauer ([https://arxiv.org/abs/2111.05834](https://arxiv.org/abs/2111.05834))。
- en: BOinG works by using *two-stage optimization* by using global and local models
    to cut down the computational cost and focus more on the promising subspace, respectively.
    In BOinG, random forest is utilized as the global model and GP as the local model.
    The global model is responsible for searching the promising subspace of the local
    model. Thus, a global model should be flexible enough to handle complex problems
    with different types of hyperparameters. Since the local model only searches in
    a promising subspace, it is possible to utilize a more accurate but expensive
    model, such as GP.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: BOinG通过使用*两阶段优化*来工作，利用全局和局部模型来降低计算成本并更多地关注有希望的子空间。在BOinG中，随机森林被用作全局模型，GP作为局部模型。全局模型负责搜索局部模型的有希望子空间。因此，全局模型应该足够灵活，能够处理具有不同类型超参数的复杂问题。由于局部模型只在有希望的子空间中搜索，因此可以使用更准确但更昂贵的模型，例如GP。
- en: 'The following table lists the pros and cons of utilizing random forest as a
    surrogate model compared to other variants of the BO hyperparameter tuning method:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了利用随机森林作为代理模型与其他BO超参数调整方法变体的优缺点：
- en: '![Figure 4.19 – Pros and cons of utilizing random forest as a surrogate model'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.19 – 利用随机森林作为代理模型的优缺点'
- en: '](img/B18753_04_019.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_019.jpg)'
- en: Figure 4.19 – Pros and cons of utilizing random forest as a surrogate model
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 – 利用随机森林作为代理模型的优缺点
- en: A conditional hyperparameter is a hyperparameter that will only be utilized
    when a certain condition is met. The tree structure of random forest is very suitable
    for this kind of situation since it can just add another branch of the tree to
    check whether the condition is met or not. The condition is usually just a specific
    value or range of other hyperparameters in the space.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 条件超参数是指在满足特定条件时才会被利用的超参数。随机森林的树结构非常适合这种情况，因为它可以添加另一个树枝来检查条件是否满足。条件通常只是空间中其他超参数的特定值或范围。
- en: Now that you are aware of SMAC and utilizing random forest as a surrogate model
    in general, in the next section, we will discuss another variant of BO that has
    a different approach in terms of approximating the objective function.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了SMAC以及一般性地利用随机森林作为代理模型，在下一节中，我们将讨论BO的另一种变体，它在近似目标函数方面有不同的方法。
- en: Understanding TPE
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TPE
- en: '**TPE** is another variant of BO that performs well in general and can be utilized
    for both categorical and continuous types of hyperparameters. Unlike BOGP, which
    has cubical time complexity, TPE runs in linear time. TPE is suggested if you
    have a huge hyperparameter space and have a very tight budget for evaluating the
    cross-validation score.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**TPE** 是 BO 的另一种变体，在一般情况下表现良好，可以用于分类和连续类型的超参数。与具有立方体时间复杂度的 BOGP 不同，TPE 以线性时间运行。如果你有一个非常大的超参数空间，并且对评估交叉验证分数的预算非常紧张，建议使用
    TPE。'
- en: The main difference between TPE and BOGP or SMAC is in the way that it models
    the relationship between hyperparameters and the cross-validation score. Unlike
    BOGP or SMAC, which approximate the value of the objective function, or the posterior
    probability, ![](img/Formula_B18753_04_095.png), *TPE works the other way around*.
    It tries to get the optimal hyperparameters based on the condition of the objective
    function, or the likelihood probability, ![](img/Formula_B18753_04_096.png) (see
    the explanation of Bayes Theorem in the *Understanding BO GP* section).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: TPE 与 BOGP 或 SMAC 之间的主要区别在于它建模超参数与交叉验证分数之间的关系。与 BOGP 或 SMAC 不同，它们近似目标函数的值或后验概率
    ![](img/Formula_B18753_04_095.png)，*TPE 采用相反的方式*。它试图根据目标函数的条件或似然概率 ![](img/Formula_B18753_04_096.png)（参见
    *理解 BO GP* 部分的贝叶斯定理解释）来获取最佳超参数。
- en: 'In other words, unlike BOGP or SMAC, which construct a predictive distribution
    over the objective function, TPE tries to utilize the information of the objective
    function to *model the hyperparameter distributions*. To be more precise, when
    the optimization problem is in the form of a *minimization problem*, ![](img/Formula_B18753_04_097.png)
    is defined as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，与构建在目标函数上的预测分布的 BOGP 或 SMAC 不同，TPE 尝试利用目标函数的信息来 *建模超参数分布*。更准确地说，当优化问题以
    *最小化问题* 的形式出现时，![](img/Formula_B18753_04_097.png) 定义如下：
- en: '![](img/Formula_B18753_04_098.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_098.png)'
- en: Here, ![](img/Formula_B18753_04_099.png) and ![](img/Formula_B18753_04_100.png)
    are utilized when the value of the objective function is lower or higher than
    the threshold, ![](img/Formula_B18753_04_101.png), respectively. There is no specific
    rule on how to choose the threshold, ![](img/Formula_B18753_04_102.png). However,
    in the **Hyperopt** and **Microsoft NNI** implementations, this threshold is chosen
    based on the TPE’s hyperparameter, ![](img/Formula_B18753_04_103.png), and the
    number of observed points in *D* up to the current trial. The definition of ![](img/Formula_B18753_04_104.png)
    tells us that TPE has two models that act as the learning algorithm based on the
    value of the objective function, which is ruled by the threshold, ![](img/Formula_B18753_04_105.png).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，当目标函数的值低于或高于阈值时，分别使用 ![](img/Formula_B18753_04_099.png) 和 ![](img/Formula_B18753_04_100.png)。如何选择阈值
    ![](img/Formula_B18753_04_101.png) 没有具体的规则。然而，在 **Hyperopt** 和 **Microsoft NNI**
    的实现中，这个阈值是基于 TPE 的超参数 ![](img/Formula_B18753_04_103.png) 和当前试验中观察到的 *D* 中的点的数量来选择的。![](img/Formula_B18753_04_104.png)
    的定义告诉我们，TPE 有两个模型，根据目标函数的值作为学习算法，由阈值 ![](img/Formula_B18753_04_105.png) 控制。
- en: When the *distribution of hyperparameters is continuous*, TPE will utilize **Gaussian
    mixture models** (**GMMs**), along with the EI acquisition function, to suggest
    the next set of hyperparameters to be tested. If the continuous distribution is
    not a Gaussian distribution, then TPE will convert it to mimic the Gaussian distribution.
    For example, if the specified hyperparameter distribution is the uniform distribution,
    then it will be converted into a truncated Gaussian distribution.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当超参数的 *分布是连续的* 时，TPE 将利用高斯混合模型（**GMMs**）以及 EI 收集函数，来建议下一组要测试的超参数。如果连续分布不是高斯分布，那么
    TPE 将将其转换为模拟高斯分布。例如，如果指定的超参数分布是均匀分布，那么它将被转换为截断高斯分布。
- en: The probabilities of the different possible outcomes for the multinomial distribution
    within the GMM, and the mean and variance values for the normal distribution within
    the GMM, are generated by the **adaptive Parzen estimator**. This estimator is
    responsible for constructing the two probability distributions, ![](img/Formula_B18753_04_106.png)
    and ![](img/Formula_B18753_04_107.png), based on the mean and variance of the
    normal hyperparameter distribution, as well as the hyperparameter value of all
    observed points in *D* up to the current trial.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 内部多项式分布的不同可能结果的概率，以及 GMM 内部正态分布的均值和方差，是由 **自适应 Parzen 估计器** 生成的。此估计器负责根据正态超参数分布的均值和方差，以及
    *D* 中所有观察到的点的超参数值，构建两个概率分布 ![](img/Formula_B18753_04_106.png) 和 ![](img/Formula_B18753_04_107.png)。
- en: When the *distribution is categorical or discrete*, TPE will convert the categorical
    distribution into a re-weighted categorical and use *weighted random sampling*,
    along with the EI acquisition function, to suggest the expected best set of hyperparameters.
    The weights in the random sampling procedure are generated based on the historical
    counts of the hyperparameter value.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *分布是分类或离散的* 时，TPE 将将分类分布转换为重新加权的分类分布，并使用 *加权随机采样* 以及 EI 收集函数来建议预期的最佳超参数集。随机采样过程中的权重是基于历史超参数值计数生成的。
- en: 'The EI acquisition function definition in TPE is a bit different from the definition
    we learned about in the *Introducing BO* section. In TPE, we are using Bayes Theorem
    when deriving the EI formula. The simple formulation of the EI acquisition function
    in TPE is defined as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: TPE 中的 EI 收集函数定义与我们在 *介绍 BO* 部分中学到的定义略有不同。在 TPE 中，我们在推导 EI 公式时使用贝叶斯定理。TPE 中
    EI 收集函数的简单公式定义如下：
- en: '![](img/Formula_B18753_04_108.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_04_108.png)'
- en: The proportionality defined here tells us that to get a high value of EI, we
    need to get a high ![](img/Formula_B18753_04_109.png) ratio. In other words, when
    the optimization problem is in the form of a *minimization problem*, the EI acquisition
    function must suggest more hyperparameters from ![](img/Formula_B18753_04_110.png)
    over ![](img/Formula_B18753_04_111.png). It is the other way around when the optimization
    problem is in the form of a *maximization problem*. For example, when we use accuracy
    to measure the performance of our classification model, then we should sample
    more hyperparameters from ![](img/Formula_B18753_04_112.png) over ![](img/Formula_B18753_04_113.png).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里定义的成比例关系告诉我们，为了获得高 EI 值，我们需要获得高的 ![](img/Formula_B18753_04_109.png) 比率。换句话说，当优化问题是
    *最小化问题* 形式时，EI 收集函数必须从 ![](img/Formula_B18753_04_110.png) 中建议比从 ![](img/Formula_B18753_04_111.png)
    中更多的超参数。当优化问题是 *最大化问题* 形式时，情况则相反。例如，当我们使用准确率来衡量我们分类模型的性能时，那么我们应该从 ![](img/Formula_B18753_04_112.png)
    中采样更多的超参数，而不是从 ![](img/Formula_B18753_04_113.png) 中采样。
- en: 'To summarize, TPE works as follows. Note that the following procedure describes
    how TPE works for the *minimization problem*. This procedure replaces *Steps 7*
    to *11* in the *Introducing BO* section:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，TPE 的工作原理如下。请注意，以下过程描述了 TPE 在 *最小化问题* 中的工作方式。这个过程替换了 *介绍 BO* 部分中的 *步骤 7*
    到 *11*：
- en: 6\. (The first few steps are the same as we saw earlier).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 6. （前几个步骤与之前看到的一样）。
- en: 7\. Divide pairs of hyperparameter values and cross-validation scores in *D*
    into two groups based on the threshold, ![](img/Formula_B18753_04_114.png), namely
    *below* and *above* groups (see *Figure 4.19*).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 将超参数值和交叉验证分数的成对数据在 *D* 中根据阈值 ![](img/Formula_B18753_04_114.png) 分为两组，即 *低于*
    和 *高于* 组（参见 *图4.19*）。
- en: '8\. Sample the next set of hyperparameters by utilizing the EI acquisition
    function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 通过使用 EI 收集函数来采样下一组超参数：
- en: For each group, calculate the probabilities, means, and variances for the GMM
    using the adaptive Parzen estimator (if it’s a continuous type) or weights for
    random sampling (if it’s a categorical type).
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个组，使用自适应 Parzen 估计器（如果它是连续类型）或随机采样的权重（如果它是分类类型）来计算 GMM 的概率、均值和方差。
- en: For each group, fit the GMM (if it’s a continuous type), or perform random sampling
    (if it’s a categorical type), to sample which hyperparameters will be passed to
    the EI acquisition function.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个组，如果它是连续类型，则拟合 GMM（高斯混合模型），如果它是分类类型，则进行随机采样，以采样哪些超参数将传递给 EI 收集函数。
- en: For each group, calculate the probability of those samples being good samples
    (for the below group), or the probability of those samples being bad samples (for
    the above group).
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个组，计算这些样本成为好样本（对于下面的组）或坏样本（对于上面的组）的概率。
- en: Get the expected optimal set of hyperparameters based on the EI acquisition
    function.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据EI获取函数，获取预期的最佳超参数集。
- en: 9\. Compute the cross-validation score using the objective function, *f*, based
    on the output from *Step 8*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 9. 使用第8步的输出，根据目标函数*f*计算交叉验证分数。
- en: 10\. Add the hyperparameters and cross-validation score pair from *Step 8* and
    *Step 9* to set *D*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 10. 将第8步和第9步中的超参数和交叉验证分数对添加到设置*D*。
- en: 11\. Repeat *Steps 7* to *10* until the stopping criteria have been met.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 11. 重复*步骤7*到*10*，直到满足停止标准。
- en: '12\. (The last few steps are the same as we saw earlier):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 12. （最后几步与我们之前看到的是一样的）：
- en: '![Figure 4.20 – Illustration of groups division in TPE'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.20 – TPE中分组划分的示意图'
- en: '](img/B18753_04_020.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_020.jpg)'
- en: Figure 4.20 – Illustration of groups division in TPE
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 – TPE中分组划分的示意图
- en: Based on the stated procedure and the preceding plot, we can see that, unlike
    BOGP or SMAC, which constructs a predictive distribution over the objective function,
    TPE tries to utilize the information of the objective function to model the hyperparameter
    distributions. This way, we are not only focusing on the best-observed points
    during the trials – we are focusing on the *distribution of the best-observed
    points* instead.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所述程序和前面的图，我们可以看到，与构建在目标函数上的预测分布的BOGP或SMAC不同，TPE试图利用目标函数的信息来建模超参数分布。这样，我们不仅关注试验期间观察到的最佳点——我们关注的是*观察到的最佳点的分布*。
- en: You may be wondering why the *Tree-structured* term is within the TPE method’s
    name. This term refers to the conditional hyperparameters that we discussed in
    the previous section. This means that there are hyperparameters in the space that
    will only be utilized when a certain condition is met. We will see what a tree-structured
    or conditional hyperparameter space looks like in [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074),
    *Hyperparameter Tuning via Hyperopt*, and [*Chapter 9*](B18753_09_ePub.xhtml#_idTextAnchor082),
    *Hyperparameter Tuning via Optuna*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么“树结构”这个词在TPE方法的名字中。这个词指的是我们在上一节中讨论的条件超参数。这意味着在空间中有一些超参数只有在满足一定条件时才会被使用。我们将在[*第8章*](B18753_08_ePub.xhtml#_idTextAnchor074)“通过Hyperopt进行超参数调整”和[*第9章*](B18753_09_ePub.xhtml#_idTextAnchor082)“通过Optuna进行超参数调整”中看到树结构或条件超参数空间的样子。
- en: One of the drawbacks that TPE has is that it may *overlook the interdependencies
    among hyperparameters* in a certain space since the Parzen estimators work univariately.
    However, this is not the case for BOGP or SMAC, since the surrogate model is constructed
    based on the configurations in the hyperparameter space. Thus, they can take into
    account the interdependencies among hyperparameters. Fortunately, there is an
    implementation of TPE that overcomes this drawback. The **Optuna** package provides
    the **multivariate TPE** implementation, which can take into account the interdependencies
    among hyperparameters.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: TPE的一个缺点是，由于Parzen估计器是单变量工作的，它可能在某个空间中忽略超参数之间的相互依赖性。然而，对于BOGP或SMAC来说并非如此，因为代理模型是基于超参数空间中的配置构建的。因此，它们可以考虑到超参数之间的相互依赖性。幸运的是，有一种TPE的实现可以克服这个缺点。**Optuna**包提供了**多元TPE**实现，它可以考虑到超参数之间的相互依赖性。
- en: 'The following table lists of pros and cons of utilizing TPE compared to other
    variants of the BO hyperparameter tuning method:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了与其他BO超参数调整方法的变体相比，利用TPE的优缺点：
- en: '![Figure 4.21 – Pros and cons of TPE'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.21 – TPE的优缺点'
- en: '](img/B18753_04_021.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_04_021.jpg)'
- en: Figure 4.21 – Pros and cons of TPE
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.21 – TPE的优缺点
- en: Important Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Some implementations support parallel tuning, but with a trade-off between the
    suggested hyperparameter quality and the wall time. The Microsoft NNI package
    supports this feature via the `constant_liar_type` argument, which will be discussed
    in more detail in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092), *Advanced
    Hyperparameter Tuning with DEAP and Microsoft NNI*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实现支持并行调整，但需要在建议的超参数质量和墙时间之间做出权衡。Microsoft NNI包通过`constant_liar_type`参数支持此功能，这将在[*第10章*](B18753_10_ePub.xhtml#_idTextAnchor092)“使用DEAP和Microsoft
    NNI进行高级超参数调整”中更详细地讨论。
- en: In this section, we learned about TPE, along with its pros and cons compared
    to other variants of BO. In the next section, we will learn about another variant
    of BO that has a slightly modified algorithm compared to the BO method in general.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了 TPE，以及与其他 BO 变体的优缺点。在下一节中，我们将了解另一种 BO 变体，其算法与一般的 BO 方法略有不同。
- en: Understanding Metis
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Metis
- en: Metis is one of the variants of BO that has several algorithm modifications
    compared to the BO method in general. Metis utilizes GP and GMM in its algorithm.
    GP is used as the surrogate model and outliers detector, while GMM is used as
    part of the acquisition function, similar to TPE.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与一般的 BO 方法相比，Metis 是具有几个算法修改的 BO 变体之一。Metis 在其算法中利用 GP 和 GMM。GP 用作代理模型和异常值检测器，而
    GMM 用作获取函数的一部分，类似于 TPE。
- en: What makes Metis different from other BO methods, in general, is that it can
    *balance exploration and exploitation more data-efficiently* than the EI acquisition
    function. It can also *handle noise in the data that doesn’t follow the Gaussian*
    distribution, and this is the case most of the time. Unlike most of the methods
    that perform random sampling to initialize the set of hyperparameters and cross-validation
    score, *D*, Metis utilizes **Latin Hypercube Sampling** (**LHS**), which is a
    stratified sampling procedure based on the equal interval of each hyperparameter.
    This sampling method is believed to be more data-efficient compared to random
    sampling to achieve the same exploration coverage.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他一般的 BO 方法相比，Metis 的不同之处在于它可以比 EI 获取函数更 *数据高效地平衡探索和利用*。它还可以 *处理不遵循高斯分布的数据噪声*，这种情况在大多数情况下都是如此。与大多数执行随机采样以初始化超参数集和交叉验证分数的方法不同，*D*，Metis
    利用 **拉丁超立方抽样**（**LHS**），这是一种基于每个超参数等间隔的分层抽样过程。这种方法被认为比随机采样更数据高效，可以达到相同的探索覆盖率。
- en: 'So, how can Metis balance exploration and exploitation more efficiently than
    the EI acquisition function, in terms of the needs of the observed points? This
    is achieved through the *custom acquisition function* that Metis has, which consists
    of three sub-acquisition functions, as shown here:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Metis 如何在观察点的需求方面比 EI 获取函数更有效地平衡探索和利用？这是通过 Metis 拥有的 *自定义获取函数* 实现的，该函数由三个子获取函数组成，如下所示：
- en: '**Lowest confidence** (**LC**): This sub-acquisition function’s goal is to
    sample hyperparameters with the highest uncertainty. In other words, the goal
    of this sub-acquisition function is to *maximize exploration*. This function is
    defined as follows:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最低置信度**（**LC**）：此子获取函数的目标是采样具有最高不确定性的超参数。换句话说，此子获取函数的目标是 *最大化探索*。此函数定义如下：'
- en: '![](img/Formula_B18753_04_115.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B18753_04_115](img/Formula_B18753_04_115.png)'
- en: '**Parzen estimator**: This sub-acquisition function is inspired by the TPE
    method, which utilizes GMM to estimate how likely the sampled hyperparameter is
    part of the *below* or *above* group (see the *Understanding TPE* section for
    more details). The goal of this sub-acquisition function is to sample hyperparameters
    with the highest probability to be the optimum hyperparameters. In other words,
    it is *optimized for exploitation*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parzen 估计器**：此子获取函数受到 TPE 方法的启发，该方法利用 GMM 估计采样超参数是 *低于* 或 *高于* 组的可能性（有关更多详细信息，请参阅
    *理解 TPE* 部分）。此子获取函数的目标是采样具有最高概率成为最佳超参数的超参数。换句话说，它是 *针对利用进行优化的*。'
- en: '`2.326`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2.326`。'
- en: Based on candidates suggested by these three sub-acquisition functions, Metis
    will then compute their *information gain* to select the final candidate to be
    included in the next trial. This selection process is done by utilizing the lower
    bound of the GP estimation confidence interval. Metis will measure the difference
    between the lower bound of the interval and the expected mean from GP. The candidate
    that has the highest improvement will be selected as the final candidate.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这三个子获取函数建议的候选人，Metis 将计算他们的 *信息增益* 以选择最终候选者，并将其包含在下一个试验中。此选择过程是通过利用 GP 估计置信区间的下限来完成的。Metis
    将测量区间下限与 GP 预期均值之间的差异。具有最高改进的候选者将被选为最终候选者。
- en: It is worth noting that Metis can handle non-Gaussian noise in the data because
    of the diagnostic model. The detected outliers made it possible for Metis to resample
    the previously tested hyperparameters so that it is robust to non-Gaussian noise
    as well. This way, Metis can *balance exploration, exploitation, and re-sampling*
    during the hyperparameter tuning process.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: To have a better understanding of how Metis works, take a look at the following
    procedure. Note that the following procedure replaces *Steps 6* to *11* in the
    *Introducing BO* section.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 5\. (The first few steps are the same as we saw earlier).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Initialize several pairs of hyperparameter values and cross-validations
    scores using the LHS method, and store them in *D*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Fit a GP that acts as a surrogate model, *M*, using the value pairs in *D*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '8\. Sample the next set of hyperparameters by utilizing the *custom acquisition
    function*, which consists of three sub-acquisition functions:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Get the current best optimum set of hyperparameters
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the suggested hyperparameters for *exploration* via the LC sub-acquisition
    function
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the suggested hyperparameters for *exploitation* via the Parzen estimator
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the suggested hyperparameters to be resampled based on the *detected outliers*
    by the diagnostic model.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the *information gain* from each suggested candidate.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the candidate that has the highest information gain.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If no candidate is suggested, then pick one random candidate.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 9\. Compute the cross-validation score using the objective function, *f*, based
    on the output from *Step 8*. Note that the cross-validation score is computed
    based on the *second random forest model*, whose goal is to learn the relationship
    between the dependent and independent variables from our original problem.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Add the hyperparameters and cross-validation score pair from *Step 8* and
    *Step 9* to set *D*.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 11\. Repeat *Steps 7* to *10* until the stopping criteria are met.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 12\. (*The last few steps are the same as we saw earlier*).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the pros and cons of utilizing Metis compared to
    other variants of the BO hyperparameter tuning method:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Pros and cons of Metis'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_04_022.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.22 – Pros and cons of Metis
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that, unlike other BO variants, there is only one package
    that implements Metis for the hyperparameter tuning method, which is **Microsoft
    NNI**. As you may have noticed, all the variants of BO that were discussed in
    this chapter have the drawback of not being able to exploit parallel computing
    resources. So, why didn’t we put that drawback in the first section instead? Because
    there is a variant of BO, namely BOHB, that can exploit the parallel computing
    resources. We will discuss BOHB in more detail in [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054),
    *Exploring Multi-Fidelity Optimization*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered Metis in detail, including, what it is, how it works,
    what makes it different from other BO variants, and its pros and cons.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了Metis，包括它是什么，它是如何工作的，它与其他BO变体有什么不同，以及它的优缺点。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the second out of four groups of hyperparameter
    tuning methods, called the BO group. We not only discussed BO in general but also
    several of its variants, including BOGP, SMAC, TPE, and Metis. We saw what makes
    each of the variants differ from each other, along with the pros and cons of each.
    At this point, you should be able to explain BO with confidence when someone asks
    you and apply hyperparameter tuning methods in this group with ease.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了四组超参数调整方法中的第二组，称为BO组。我们不仅讨论了BO的一般情况，还讨论了其几个变体，包括BOGP、SMAC、TPE和Metis。我们看到了每个变体之间有什么不同，以及每个变体的优缺点。到这一点，当有人问你时，你应该能够自信地解释BO，并且能够轻松地应用这一组中的超参数调整方法。
- en: 'In the next chapter, we will start discussing heuristic search, the third group
    of hyperparameter tuning methods. The goal of the next chapter is similar to this
    chapter: to provide a better understanding of the methods that belong to the heuristic
    search group.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论启发式搜索，这是超参数调整方法的第三组。下一章的目标与本章类似：提供对属于启发式搜索组的各种方法的更好理解。
