["```py\n> # ADAPTIVE BOOSTING with a Toy Dataset\n> # https://www.youtube.com/watch?v=gmok1h8wG-Q \n> # Jessica Noss\n> # The Toy Data\n> x1 <- c(1,5,3,1,5)\n> x2 <- c(5,5,3,1,1)\n> y <- c(1,1,-1,1,1)\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"The TOY Data Depiction\")\n> text(x1,x2,labels=names(y),pos=1)\n```", "```py\n> # Visualizing the stump models\n> windows(height=200,width=300)\n> par(mfrow=c(2,3))\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"Classification with Stump X1<2\")\n> text(x1,x2,labels=names(y),pos=1)\n> plim <- par(\"usr\")\n> rect(xleft=2,ybottom = plim[3],xright = plim[2],ytop = plim[4],\n+      border = \"red\",col=\"red\",density=20 )\n> rect(xleft=plim[1],ybottom = plim[3],xright = 2,ytop = plim[4],\n+      border = \"green\",col=\"green\",density=20 )\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"Classification with Stump X1<4\")\n> text(x1,x2,labels=names(y),pos=1)\n> rect(xleft=4,ybottom = plim[3],xright = plim[2],ytop = plim[4],\n+      border = \"red\",col=\"red\",density=20 )\n> rect(xleft=plim[1],ybottom = plim[3],xright = 4,ytop = plim[4],\n+      border = \"green\",col=\"green\",density=20 )\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"Classification with Stump X1<6\")\n> text(x1,x2,labels=names(y),pos=1)\n> rect(xleft=6,ybottom = plim[3],xright = plim[2],ytop = plim[4],\n+      border = \"red\",col=\"red\",density=20 )\n> rect(xleft=plim[1],ybottom = plim[3],xright = 6,ytop = plim[4],\n+      border = \"green\",col=\"green\",density=20 )\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"Classification with Stump X1>2\")\n> text(x1,x2,labels=names(y),pos=1)\n> rect(xleft=2,ybottom = plim[3],xright = plim[2],ytop = plim[4],\n+      border = \"green\",col=\"green\",density=20 )\n> rect(xleft=plim[1],ybottom = plim[3],xright = 2,ytop = plim[4],\n+      border = \"red\",col=\"red\",density=20 )\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"Classification with Stump X1>4\")\n> text(x1,x2,labels=names(y),pos=1)\n> rect(xleft=4,ybottom = plim[3],xright = plim[2],ytop = plim[4],\n+      border = \"green\",col=\"green\",density=20 )\n> rect(xleft=plim[1],ybottom = plim[3],xright = 4,ytop = plim[4],\n+      border = \"red\",col=\"red\",density=20 )\n> plot(x1,x2,pch=c(\"+\",\"+\",\"-\",\"+\",\"+\"),cex=2,\n+      xlim=c(0,6),ylim=c(0,6),\n+      xlab=expression(x[1]),ylab=expression(x[2]),\n+      main=\"Classification with Stump X1>6\")\n> text(x1,x2,labels=names(y),pos=1)\n> rect(xleft=6,ybottom = plim[3],xright = plim[2],ytop = plim[4],\n+      border = \"green\",col=\"green\",density=20 )\n> rect(xleft=plim[1],ybottom = plim[3],xright = 6,ytop = plim[4],\n+      border = \"red\",col=\"red\",density=20 )\n```", "```py\n> # The Simple Stump Models\n> M1 <- c(1,-1,-1,1,-1)   # M1 = X1<2 predicts 1, else -1\n> M2 <- c(1,-1,1,1,-1)    # M2 = X1<4 predicts 1, else -1\n> M3 <- c(1,1,1,1,1)      # M3 = X1<6 predicts 1, else -1\n> M4 <- c(-1,1,1,-1,1)    # M4 = X1>2 predicts 1, else -1;M4=-1*M1\n> M5 <- c(-1,1,-1,-1,1)   # M5 = X1>4 predicts 1, else -1;M5=-1*M2\n> M6 <- c(-1,-1,-1,-1,-1) # M6 = X1>6 predicts 1, else -1;M6=-1*M3\n```", "```py\n> # Stem Model Errors\n> Err_M1 <- M1!=y\n> Err_M2 <- M2!=y\n> Err_M3 <- M3!=y\n> Err_M4 <- M4!=y\n> Err_M5 <- M5!=y\n> Err_M6 <- M6!=y\n> # Their Misclassifications\n> rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)\n          P1    P2    P3    P4    P5\nErr_M1 FALSE  TRUE FALSE FALSE  TRUE\nErr_M2 FALSE  TRUE  TRUE FALSE  TRUE\nErr_M3 FALSE FALSE  TRUE FALSE FALSE\nErr_M4  TRUE FALSE  TRUE  TRUE FALSE\nErr_M5  TRUE FALSE FALSE  TRUE FALSE\nErr_M6  TRUE  TRUE FALSE  TRUE  TRUE\n```", "```py\n> # ROUND 1\n> # Weighted Error Computation\n> weights_R1 <- rep(1/length(y),length(y)) #Initializaing the weights\n> Err_R1 <- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%\n+   weights_R1\n> Err_R1 # Error rate\n       [,1]\nErr_M1  0.4\nErr_M2  0.6\nErr_M3  0.2/\nErr_M4  0.6\nErr_M5  0.4\nErr_M6  0.8\n```", "```py\n> # The best classifier error rate\n> err_rate_r1 <- min(Err_R1)\n> alpha_3 <- 0.5*log((1-err_rate_r1)/err_rate_r1)\n> alpha_3\n[1] 0.6931472\n```", "```py\n> alpha_3*M3\n[1] 0.6931472 0.6931472 0.6931472 0.6931472 0.6931472\n> sign(alpha_3*M3)\n[1] 1 1 1 1 1\n```", "```py\n\n> # Weights Update Formula and Function\n> Weights_update <- function(weights,error,error_rate){\n+   weights_new <- NULL\n+   for(i in 1:length(weights)){\n+     if(error[i]==FALSE) weights_new[i] <- 0.5*weights[i]/(1-error_rate)\n+     if(error[i]==TRUE) weights_new[i] <- 0.5*weights[i]/error_rate\n+   }\n+   return(weights_new)\n+ }\n```", "```py\n> # ROUND 2\n> # Update the weights and redo the analyses\n> weights_R2 <- Weights_update(weights=weights_R1,error=Err_M3,\n+                              error_rate=err_rate_r1)\n> Err_R2 <- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%\n+   weights_R2\n> Err_R2 # Error rates\n       [,1]\nErr_M1 0.25\nErr_M2 0.75\nErr_M3 0.50\nErr_M4 0.75\nErr_M5 0.25\nErr_M6 0.50\n```", "```py\n> err_rate_r2 <- min(Err_R2)\n> alpha_1 <- 0.5*log((1-err_rate_r2)/err_rate_r2)\n> alpha_1\n[1] 0.5493061\n> alpha_3*M3+alpha_1*M1\n[1] 1.242453 0.143841 0.143841 1.242453 0.143841\n> sign(alpha_3*M3+alpha_1*M1)\n[1] 1 1 1 1 1\n```", "```py\n> # ROUND 3\n> # Update the weights and redo the analyses\n> weights_R3 <- Weights_update(weights=weights_R2,error=Err_M1,\n+                              error_rate=err_rate_r2)\n> Err_R3 <- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%\n+   weights_R3\n> Err_R3 # Error rates\n            [,1]\nErr_M1 0.5000000\nErr_M2 0.8333333\nErr_M3 0.3333333\nErr_M4 0.5000000\nErr_M5 0.1666667\nErr_M6 0.6666667\n> err_rate_r3 <- min(Err_R3)\n> alpha_5 <- 0.5*log((1-err_rate_r3)/err_rate_r3)\n> alpha_5\n[1] 0.804719\n> alpha_3*M3+alpha_1*M1+alpha_5*M5\n[1]  0.4377344  0.9485600 -0.6608779  0.4377344  0.9485600\n> sign(alpha_3*M3+alpha_1*M1+alpha_5*M5)\n[1]  1  1 -1  1  1\n```", "```py\n> getNode <- function(x,y)\t{\n+   xu <- sort(unique(x),decreasing=TRUE)\n+   ss <- numeric(length(xu)-1)\n+   for(i in 1:length(ss))\t{\n+     partR <- y[x>xu[i]]\n+     partL <- y[x<=xu[i]]\n+     partRSS <- sum((partR-mean(partR))^2)\n+     partLSS <- sum((partL-mean(partL))^2)\n+     ss[i] <- partRSS + partLSS\n+   }\n+   xnode <- xu[which.min(ss)]\n+   minss <- min(ss)\n+   pR <- mean(y[x>xnode])\n+   pL <- mean(y[x<=xnode])\n+   return(list(xnode=xnode,yR=pR,yL=pL))\n+ }\n```", "```py\n> # Can Boosting Learn the Sine Wave!\n> x <- seq(0,2*pi,pi/20)\n> y <- sin(x)\n> windows(height=300,width=100)\n> par(mfrow=c(3,1))\n> plot(x,y,\"l\",col=\"red\",main=\"Oh My Waves!\")\n```", "```py\n> first_split <- getNode(x,y)\n> first_split\n$xnode\n[1] 3.141593\n$yR\n[1] -0.6353102\n$yL\n[1] 0.6050574\n```", "```py\n> segments(x0=min(x),y0=first_split$yL,\n+          x1=first_split$xnode,y1=first_split$yL)\n> segments(x0=first_split$xnode,y0=first_split$yR,\n+          x1=max(x),y1=first_split$yR)\n```", "```py\n> yfit1 <- ifelse(x<first_split$xnode,first_split$yL,first_split$yR)\n> GBFit <- yfit1\n> segments(x0=x,x1=x,y0=y,y1=yfit1)\n> first_residuals <- y-yfit1\n> summary(first_residuals)\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.60506 -0.25570  0.04752  0.03025  0.32629  0.63531 \n```", "```py\n> second_split <- getNode(x,first_residuals)\n> plot(x,first_residuals,\"l\",col=\"red\",main=\"The Second Wave!\")\n> segments(x0=min(x),y0=second_split$yL,\n+          x1=second_split$xnode,y1=second_split$yL)\n> segments(x0=second_split$xnode,y0=second_split$yR,\n+          x1=max(x),y1=second_split$yR)\n> yfit2 <- ifelse(x<second_split$xnode,second_split$yL,second_split$yR)\n> GBFit <- GBFit+yfit2\n> segments(x0=x,x1=x,y0=first_residuals,y1=yfit2)\n> second_residuals <- first_residuals-yfit2\n> summary(second_residuals)\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.51678 -0.24187 -0.02064 -0.01264  0.25813  0.56715 \n```", "```py\n> third_split <- getNode(x,second_residuals)\n> plot(x,second_residuals,\"l\",col=\"red\",main=\"The Third Wave!\")\n> segments(x0=min(x),y0=third_split$yL,\n+          x1=third_split$xnode,y1=third_split$yL)\n> segments(x0=third_split$xnode,y0=third_split$yR,\n+          x1=max(x),y1=third_split$yR)\n> yfit3 <- ifelse(x<third_split$xnode,third_split$yL,third_split$yR)\n> GBFit <- GBFit+yfit3\n> segments(x0=x,x1=x,y0=second_residuals,y1=yfit3)\n> third_residuals <- second_residuals-yfit3\n> summary(third_residuals)\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.47062 -0.27770 -0.03927 -0.01117  0.18196  0.61331 \n```", "```py\n> pdf(\"Sine_Wave_25_Iterations.pdf\")\n> curr_residuals <- third_residuals\n> for(j in 4:25){\n+   jth_split <- getNode(x,curr_residuals)\n+   plot(x,curr_residuals,\"l\",col=\"red\",main=paste0(c(\"The \", j, \"th Wave!\")))\n+   segments(x0=min(x),y0=jth_split$yL,\n+            x1=jth_split$xnode,y1=jth_split$yL)\n+   segments(x0=jth_split$xnode,y0=jth_split$yR,\n+            x1=max(x),y1=jth_split$yR)\n+   yfit_next <- ifelse(x<jth_split$xnode,jth_split$yL,jth_split$yR)\n+   GBFit <- GBFit+yfit_next\n+   segments(x0=x,x1=x,y0=curr_residuals,y1=yfit_next)\n+   curr_residuals <- curr_residuals-yfit_next\n+ }\n> dev.off()\n> summary(curr_residuals)\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.733811 -0.093432  0.008481 -0.001632  0.085192  0.350122 \n```", "```py\n> plot(y,GBFit,xlab=\"True Y\",ylab=\"Gradient Boosting Fit\")\n```", "```py\n> # Gradiend Boosting Using the Squared-error Loss Function\n> GB_SqEL <- function(y,X,depth,iter,shrinkage){\n+   curr_res <- y\n+   GB_Hat <- data.frame(matrix(0,nrow=length(y),ncol=iter))\n+   fit <- y*0\n+   for(i in 1:iter){\n+     tdf <- cbind(curr_res,X)\n+     tpart <- rpart(curr_res~.,data=tdf,maxdepth=depth)\n+     gb_tilda <- predict(tpart)\n+     gb_hat <- shrinkage*gb_tilda\n+     fit <- fit+gb_hat\n+     curr_res <- curr_res-gb_hat\n+     GB_Hat[,i] <- fit\n+   }\n+   return(list(GB_Hat = GB_Hat))\n+ }\n```", "```py\n> als <- read.table(\"../Data/ALS.txt\",header=TRUE)\n> alst <- als[als$testset==FALSE,-1]\n> temp <- GB_SqEL(y=alst$dFRS,X=alst[,-1],depth=4,\n+                 iter=500,shrinkage = 0.02)\n> MSE_Train <- 0\n> for(i in 1:500){\n+   MSE_Train[i] <- mean(temp$GB_Hat[,i]-alst$dFRS)^2\n+ }\n> windows(height=100,width=100)\n> plot.ts(MSE_Train)\n```", "```py\n> # The adabag and gbm Packages\n> x1 <- c(1,5,3,1,5)\n> x1 <- rep(x1,times=10)\n> x2 <- c(5,5,3,1,1)\n> x2 <- rep(x2,times=10)\n> y <- c(1,1,0,1,1)\n> y <- rep(y,times=10)\n> toy <- data.frame(x1=x1,x2=x2,y=y)\n> toy$y <- as.factor(toy$y)\n> AB1 <- boosting(y~.,data=toy,boos=TRUE,mfinal = 10,\n+                 maxdepth=1,minsplit=1,minbucket=1)\n> predict.boosting(AB1,newdata=toy[,1:2])$class\n [1] \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\"\n[19] \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\"\n[37] \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\"\n```", "```py\n> x <- seq(0,2*pi,pi/200)\n> y <- sin(x)\n> sindata <- data.frame(cbind(x,y))\n> sin_gbm <- gbm(y~x,distribution=\"gaussian\",data=sindata,\n+                n.trees=250,bag.fraction = 0.8,shrinkage = 0.1)\n> par(mfrow=c(1,2))\n> plot.ts(sin_gbm$fit, main=\"The gbm Sine Predictions\")\n> plot(y,sin_gbm$fit,main=\"Actual vs gbm Predict\")\n```", "```py\n> AB1$importance\n x1  x2 \n100   0 \n```", "```py\n> summary(sin_gbm)\n  var rel.inf\nx   x     100\n```", "```py\n> data(\"spam\")\n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(spam),replace = TRUE,\n+ prob = c(0.7,0.3))\n> head(Train_Test)\n[1] \"Test\"  \"Test\"  \"Test\"  \"Test\"  \"Train\" \"Train\"\n> spam_Train <- spam[Train_Test==\"Train\",]\n> spam_TestX <- within(spam[Train_Test==\"Test\",],\n+                      rm(type))\n> spam_TestY <- spam[Train_Test==\"Test\",\"type\"]\n> spam_Formula <- as.formula(\"type~.\")\n> spam_rf <- randomForest(spam_Formula,data=spam_Train,coob=TRUE,\n+                         ntree=500,keepX=TRUE,mtry=5)\n> spam_rf_predict <- predict(spam_rf,newdata=spam_TestX,type=\"class\")\n> rf_accuracy <- sum(spam_rf_predict==spam_TestY)/nrow(spam_TestX)\n> rf_accuracy\n[1] 0.9436117\n> spam_bag <- randomForest(spam_Formula,data=spam_Train,coob=TRUE,\n+                          ntree=500,keepX=TRUE,mtry=ncol(spam_TestX))\n> spam_bag_predict <- predict(spam_bag,newdata=spam_TestX,type=\"class\")\n> bag_accuracy <- sum(spam_bag_predict==spam_TestY)/nrow(spam_TestX)\n> bag_accuracy\n[1] 0.9350464\n> spam_Train2 <- spam_Train\n> spam_Train2$type <- ifelse(spam_Train2$type==\"spam\",1,0)\n> spam_gbm <- gbm(spam_Formula,distribution=\"bernoulli\",data=spam_Train2,\n+                 n.trees=500,bag.fraction = 0.8,shrinkage = 0.1)\n> spam_gbm_predict <- predict(spam_gbm,newdata=spam_TestX,\n+                             n.trees=500,type=\"response\")\n> spam_gbm_predict_class <- ifelse(spam_gbm_predict>0.5,\"spam\",\"nonspam\")\n> gbm_accuracy <- sum(spam_gbm_predict_class==spam_TestY)/nrow(spam_TestX)\n> gbm_accuracy\n[1] 0.945753\n> summary(spam_gbm)\n                                var      rel.inf\ncharExclamation     charExclamation 21.985502703\ncharDollar               charDollar 18.665385239\nremove                       remove 11.990552362\nfree                           free  8.191491706\nhp                               hp  7.304531600\n\nnum415                       num415  0.000000000\ndirect                       direct  0.000000000\ncs                               cs  0.000000000\noriginal                   original  0.000000000\ntable                         table  0.000000000\ncharHash                   charHash  0.000000000\n```"]