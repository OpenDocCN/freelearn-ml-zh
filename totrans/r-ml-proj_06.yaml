- en: Image Recognition Using Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1966, Professor Seymour Papert at MIT conceptualized an ambitious summer
    project titled *The Summer Vision Project*. The task for the graduate student
    was to *plug a camera into a computer and enable it to understand what it sees*! I
    am sure it would have been super-difficult for the graduate student to have finished
    this project, as even today the task remains half complete.
  prefs: []
  type: TYPE_NORMAL
- en: A human being, when they look outside, is able to recognize the objects that
    they see. Without thinking, they are able to classify a cat as a cat, a dog as
    a dog, a plant as a plant, an animal as an animal—this is happening because the
    human brain draws knowledge from its extensive prelearned database. After all,
    as human beings, we have millions of years' worth of evolutionary context that
    enables us draw inferences from the thing that we see. Computer vision deals with
    replicating the human vision processes so as to pass them on to machines and automate
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is all about learning the theory and implementation of computer
    vision through **machine learning** (**ML**). We will build a feedforward deep
    learning network and **LeNet** to enable handwritten digit recognition. We will
    also build a project that uses a pretrained Inception-BatchNorm network to identify
    objects in an image. We will cover the following topics as we progress in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving computer vision with deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a deep learning network for handwritten digit recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing computer vision with pretrained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the projects covered in this chapter, we'll make use of a very popular open
    dataset called MNIST. We'll use **Apache MXNet**, a modern open source deep learning
    software framework to train and deploy the required deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In today''s world, we have advanced cameras that are very successful at mimicking
    how a human eye captures light and color; but image-capturing in the right way
    is just stage one in the whole image-comprehension aspect. Post image-capturing,
    we will need to enable technology that interprets what has been captured and build
    context around it. This is what the human brain does when the eyes see something.
    Here comes the huge challenge: we all know that computers see images as huge piles
    of integer values that represent intensities across a spectrum of colors, and
    of course, computer have no context associated with the image itself. This is
    where ML comes into play. ML allows us to train a context for a dataset such that
    it enables computers to understand what objects certain sequences of numbers actually
    represent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision is one of the emerging areas where ML is applied. It can be
    used for several purposes in various domains, including healthcare, agriculture,
    insurance, and the automotive industry. The following are some of its most popular
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting diseases from medical images, such as CT scan/MRI scan images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying crop diseases and soil quality to support a better crop yield
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying oil reserves from satellite images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-driving cars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and managing skin condition for psoriasis patients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying and distinguishing weeds from crops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting information from personal documents, such as passports and ID cards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting terrain for drones and airplanes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biometrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Public surveillance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing personal photos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering visual questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is just the tip of the iceberg. It's not an overstatement to say that there
    is no domain where we cannot find an application for computer vision. Therefore,
    computer vision is a key area for ML practitioners to focus on.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving computer vision with deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with, let''s understand the term **deep learning**. It simply means
    **multilayered neural networks**. The multiple layers enable deep learning to
    be an enhanced and powerful form of a neural network. **Artificial neural networks**
    (**ANNs**) have been in existence since the 1950s. They have always been designed
    with two layers; however, deep learning models are built with multiple hidden
    layers. The following diagram shows a hypothetical deep learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d09fdae-a6ec-4587-9d21-fe9efc0062dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning model—High level architecture
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are heavy on computation, therefore the **central processing
    unit** (**CPU**) that can be enabled with a maximum of 22 cores is generally thought
    of as an infrastructure blocker until recently. This infrastructure limitation
    also limited the usage of neural networks to solve real-world problems. However,
    recently, the availability of a **graphical processing unit** (**GPU**) with thousands
    of cores enabled has exponentially powerful computation possibilities when compared
    to CPUs. This gave a huge push to the usage of deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Data comes in many forms, such as tables, sounds, HTML files, TXT files, and
    images. Linear models do not generally learn from non-linear data. Non-linear
    algorithms, such as decision trees and gradient-boosting machines, also do not
    learn well from this kind of data. One the other hand, deep learning models that
    create non-linear interactions among the features give better solutions with non-linear
    data, so they have become the preferred models in the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: A deep learning model consists of a chain of interconnected neurons that creates
    the neural architecture. Any deep learning model will have an input layer, two
    or more hidden layers (middle layers), and an output layer. The input layer consists
    of neurons equal to the number of input variables in the data. Users can decide
    on the number of neurons and the number of hidden layers that a deep learning
    network should have. Generally, it is something that is optimized by the user
    building the network through a cross-validation strategy. The choice of the number
    of neurons and the number of hidden layers represents the challenge of the researcher.
    The number of neurons in the output layer is decided based on the outcome of the
    problem. For example, one output neuron in case it is regression, for a classification
    problem the output neurons is equal to the number of classes involved in the problem
    on-hand.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple types of deep learning algorithms, the one we generally use
    in computer vision is called a **Convolutional Neural Network **(**CNN**). CNNs
    break down images into small groups of pixels and then run calculations on them
    by applying filters. The result is then compared against pixel matrices they already
    know about. This helps CNNs to come up with a probability for the image belonging
    to one of the known classes.
  prefs: []
  type: TYPE_NORMAL
- en: In the first few layers, the CNN identifies shapes, such as curves and rough
    edges, but after several convolutions, they are able to recognize objects such
    as animals, cars, and humans.
  prefs: []
  type: TYPE_NORMAL
- en: When the CNN is first built for the available data, the filter values of the
    network are randomly initialized and so the predictions it produce are mostly
    false. But then it keeps comparing its own predictions on labeled datasets to
    the actual ones, updating the filter values and improving performance of the CNN
    with each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Layers of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A CNN consists of an input and an output layer; it also has various hidden
    layers. The following are the various hidden layers in a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolution**: Assume that we have an image represented as pixels, a convolution
    is something where we have a little matrix nearly always 3 x 3 in deep learning
    and multiply every element of the matrix by every element of 3 x 3 section of
    the image and then add them all together to get the result of that convolution
    at one point. The following diagram illustrates the process of convolution on
    a pixel:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5357ec97-6f89-4348-92a2-26e7a6cea9b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution application on an image
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLU**): A non-linear activation that throws away
    the negatives in an input matrix. For example, let''s assume we have a 3 x 3 matrix
    with negative numbers, zeros, and positive numbers as values in the cells of the
    matrix. Given this matrix as input to ReLU, it transforms all negative numbers
    in the matrix to zeros and returns the 3 x 3 matrix. ReLU is an activation function
    that can be defined as part of the CNN architecture. The following diagram demonstrates
    the function of ReLU in CNNs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ad096701-f380-414d-8028-f55c77a9f905.png)'
  prefs: []
  type: TYPE_IMG
- en: Rectified Linear Unit (ReLU) in CNNs
  prefs: []
  type: TYPE_NORMAL
- en: '**Max pooling**: Max pooling is something that can be set as a layer in the CNN
    architecture. It allows to identify if the specific characteristic is present
    in the previous level. It replaces the highest value in an input matrix with the
    maximum and gives the output. Let''s consider an example, with a 2 x 2 max pooling
    layer, given a 4 x 4 matrix as input, the max pooling layer replaces each 2 x
    2 in the input matrix with the highest value among the four cells. The output
    matrix thus obtained is non-overlapping and it''s an image representation with
    a reduced resolution. The following diagram illustrates the functionality of max
    pooling in a CNN:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ce2cc9f2-2ea0-4c38-9c48-f99f26af8ebc.png)'
  prefs: []
  type: TYPE_IMG
- en: Functionality of max pooling layer in CNNs
  prefs: []
  type: TYPE_NORMAL
- en: There are various reasons to apply max pooling, such as to reduce the amount
    of parameters and computation load, to eliminate overfitting, and, most importantly,
    to force the neural network to see the larger picture, as in previous layers it
    was focused on seeing bits and pieces of the image.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully-connected layer**: Also known as a **dense layer**, this involves a
    linear operation on the layer''s input vector. The layer ensures every input is
    connected to every output by a weight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Softmax**: An activation function that is generally applied at the last layer
    of the deep neural network. In a multiclass classification problem, we require
    the fully-connected output of a deep learning network to be interpreted as a probability.
    The total probability of a particular observation in data (for all classes) should
    add up to 1, and the probability of the observation belonging to each class should
    range between 0 and 1\. Therefore, we transform each output of the fully-connected
    layer as a portion of a total sum. However, instead of simply doing the standard
    proportion, we apply this non-linear exponential function for a very specific
    reason: we would like to make our highest output as close to 1 as possible and
    our lower output as close to 0\. Softmax does this by pushing the true linear
    proportions closer to either 1 or 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the softmax activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/532ddd0f-24a2-41f4-bdac-293eecfdaf2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax activation function
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid**: This is similar to softmax, except that it is applied to a binary
    classification, such as cats versus dogs. With this activation function, the class
    to which the observation belongs is assigned a higher probability compared to
    the other class. Unlike softmax, the probabilities do not have to add up to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the MXNet framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MXNet is a super-powerful open source deep learning framework that is built
    to ease the development of deep learning algorithms. It is used to define, train,
    and deploy deep neural networks. MXNet is lean, flexible, and ultra-scalable,
    that is, it allows fast model training and supports a flexible programming model
    with multiple languages. The problem with existing deep learning frameworks, such
    as Torch7, Theano, and Caffe, is that users need to learn another system or a
    different programming flavor.
  prefs: []
  type: TYPE_NORMAL
- en: However, MXNet resolves this issue by supporting multiple languages, such as
    C++, Python, R, Julia, and Perl. This eliminates the need for users to learn a
    new language; therefore, they can use the framework and simplify network definitions.
    MXNet models are able to fit in small amounts of memory and they can be trained
    on CPUs, GPUs, and on multiple machines with ease. The `mxnet` package is readily
    available for the R language and the details of the install can be looked up in
    **Apache Incubator** at [https://mxnet.incubator.apache.org](https://mxnet.incubator.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) is
    a dataset that contains images of handwritten digits. This dataset is pretty popular
    in the ML community for implementing and testing computer vision algorithms. The
    MNIST dataset is an open dataset made available by Professor Yann LeCun at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/),
    where separate files that represent the training dataset and test dataset are
    available. The labels corresponding to the test and training datasets are also
    available as separate files. The training dataset has 60,000 samples and the test
    dataset has 10,000 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows some sample images from the MNIST dataset. Each
    of the images also comes with a label indicating the digit shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f8f082b-0bbb-4e47-ab65-6cd430a0b1d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample images from MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The labels for the images shown in the preceding diagram are **5**, **0**,
    **4**, and **1**. Each image in the dataset is a grayscale image and is represented
    in 28 x 28 pixels. A sample image represented with pixels is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/853fd20e-35be-49ed-949c-57b05ea199fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample image from MNIST dataset represented with 28 * 28 pixels
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to flatten the 28 x 28 pixel matrix and represent it as a vector
    of 784 pixel values. Essentially, the training dataset is a 60,000 x 784 matrix
    that could be used with ML algorithms. The test dataset is a 10,000 x 784 matrix.
    The training and test datasets may be downloaded from the source with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is downloaded and unzipped, we will see the files in our working
    directory. However, these files are in binary format and they cannot be directly
    loaded through the regular `read.csv` command. The following custom function code
    helps to read the training and test data from the binary files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The functions may be called with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In RStudio, when we execute the code, we see `train` , `test`,  `train.y`,
    and `test.y` displayed under the Environment tab. This confirms that the datasets
    are successfully loaded and the respective dataframes are created, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b85709ce-90db-428b-a38a-aa7c6f57cb78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the image data is loaded into the dataframe, it is in the form of a series
    of numbers that represent the pixel values. The following is a helper function
    that visualizes the pixel data as an image in RStudio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `show_digit()` function may be called like any other R function with the
    dataframe record number as a parameter. For example, the function in the following
    code block helps to visualize the `3` record in the training dataset as an image
    in RStudio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88617ad9-3c55-44fb-9a2e-8957dae8647c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dr. David Robinson, in his blog on *Exploring handwritten digit classification:
    a tidy analysis of the MNIST dataset* ([http://varianceexplained.org/r/digit-eda/](http://varianceexplained.org/r/digit-eda/)),
    performed a beautiful exploratory data analysis of the MNIST dataset, which will
    help you better understand the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep learning network for handwritten digit recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mxnet` library offers several functions that enable us to define the layers
    and activations that comprise the deep learning network. The definition of layers,
    the usage of activation functions, and the number of neurons to be used in each
    of the hidden layers is generally termed the **network architecture**. Deciding
    on the network architecture is more of an art than a science. Often, several iterations
    of experiments may be needed to decide on the right architecture for the problem.
    We call it an art as there are no exact rules for finding the ideal architecture.
    The number of layers, neurons in these layers, and the type of layers are pretty
    much decided through trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll build a simple deep learning network with three hidden
    layers. Here is the general architecture of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is defined as the initial layer in the network. The `mx.symbol.Variable` MXNet
    function defines the input layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A fully-connected layer is defined, also called a dense layer, with 128 neurons
    as the first hidden layer in the network. This can be done using the `mx.symbol.FullyConnected` MXNet function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A ReLU activation function is defined as part of the network. The `mx.symbol.Activation`
    function helps us to define the ReLU activation function as part of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the second hidden layer; it is another dense layer with 64 neurons. This
    can be accomplished through the `mx.symbol.FullyConnected` function, similar to
    the first hidden layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a ReLU activation function on the second hidden layer's output. This can
    be done through the `mx.symbol.Activation` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final hidden layer in our network is another fully-connected layer, but
    with just ten outputs (equal to the number of classes). This can be done through
    the `mx.symbol.FullyConnected` function as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output layer needs to be defined and this should be probabilities of prediction
    for each class; therefore, we apply softmax at the output layer. The `mx.symbol.SoftmaxOutput`
    function enables us to configure the softmax in the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are not saying that this is the best network architecture possible for the
    problem, but this is the network we are going to build to demonstrate the implementation
    of a deep learning network with MXNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a blueprint in place, let''s delve into coding the network
    using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the three layers and start training the network to obtain class
    probabilities and ensure the results are reproducible using the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To make predictions on the test dataset and get the label for each observation
    in the test dataset, use the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the performance of the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the network architecture, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b6945d7-f014-474a-b699-64cef7c3374d.png)'
  prefs: []
  type: TYPE_IMG
- en: With the simple architecture running for a few minutes on a CPU-based laptop
    and with minimal effort, we were able to achieve an accuracy of `97.7%` on the
    test dataset. The deep learning network was able to learn to interpret the digits
    by seeing the images it was given as input. The accuracy of the system can be
    further improved by altering the architecture or by increasing the number of iterations.
    It may be noted that, in the earlier experiment, we ran it for 10 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of iterations can simply be amended when model-building through
    the `num.round` parameter. There is no hard-and-fast rule in terms of the optimal
    number of rounds, so this is something to be determined by trial and error. Let''s
    build the model with 50 iterations and observe its impact on performance. The
    code will remain the same as the earlier project, except with the following amendment
    to the model-building code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Observe that the `num.round` parameter is now set to `50`, instead of the earlier
    value of `10`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can observe from the output that 100% accuracy was obtained with the training
    dataset. However, with the test dataset, we observe the accuracy as 98%. Essentially,
    our model is expected to perform the same with both the training and test dataset
    for it to be called a good model. Unfortunately, in this case, we have encountered
    a situation known as **overfitting,** which means that the model we created did
    not generalize well. In other words, the model has trained itself with too many
    parameters or it got trained for too long and has become super-specialized with
    data in the training dataset alone; as an effect, it is not doing a good job with
    new data. Model generalization is something we should specifically aim for. There
    is a technique, known as **dropout**, that can help us to overcome the overfitting
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing dropout to avoid overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout is defined in the network architecture after the activation layers,
    and it randomly sets activations to zero. In other words, dropout randomly deletes
    parts of the neural network, which allows us to prevent overfitting. We can't
    overfit exactly to our training data when we're consistently throwing away information
    learned along the way. This allows our neural network to learn to generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MXNet, dropout can be easily defined as part of network architecture using
    the `mx.symbol.Dropout` function. For example, the following code defines dropouts
    post the first ReLU activation (`act1`) and second ReLU activation (`act2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `data` parameter specifies the input that the dropout takes and the value
    of `p` specifies the amount of dropout to be done. In case of `dropout1`, we are
    specifying that 50% of weights are to be dropped. Again, there is no hard-and-fast
    rule in terms of how much dropout should be included and at what layers. This
    is something to be determined through trial and error. The code with dropouts
    almost remains identical to the earlier project except that it now includes the
    dropouts after the activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output and the visual network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e7e2381-a190-44e2-a9cd-cb85033c0b96.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the output that dropout is now included as part of the network
    architecture. We also observe that this network architecture yields a lower accuracy
    on the test dataset when compared with our initial project. One reason could be
    that the dropout percentages (50% and 30%) we included are too high. We could
    play with these percentages and rebuild the model to determine whether the accuracy
    gets better. The idea, however, is to demonstrate the use of dropout as a regularization
    technique so as to avoid overfitting in deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from dropout, there are other techniques you could employ to avoid an
    overfitting situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Addition of data**: Adding more training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation**: Creating additional data synthetically by applying techniques
    such as flipping, distorting, adding random noise, and rotation. The following
    screenshot shows sample images created after applying data augmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cdd28deb-0725-4120-aeec-075ba07f4e98.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample images from applying data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: '**Reducing complexity of the network architecture**: Fewer layers, fewer epochs,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch normalization**: A process of ensuring that the weights generated in
    the network do not push very high or very low. This is generally achieved by subtracting
    the mean and dividing by the standard deviation of all weights at a layer from
    each weight in a layer. It shields against overfitting, performs regularization,
    and significantly improves the training speed.  The `mx.sym.batchnorm()` function
    enables us to define batch normalization after the activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will not focus on developing another project with batch normalization as
    using this function in the project is very similar to the other functions we used
    in our earlier projects. So far, we have focused on increasing the epochs to improve
    the performance of the model, another option is to try a different architecture
    and evaluate whether that improves the accuracy on the test dataset. On that note,
    let's explore LeNet, which is specifically designed for optical character recognition
    in documents.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the LeNet architecture with the MXNet library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In their 1998 paper, *Gradient-Based Learning Applied to Document Recognition*,
    LeCun et al. introduced the LeNet architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LeNet architecture consists of two sets of convolutional, activation, and
    pooling layers, followed by a fully-connected layer, activation, another fully-connected
    layer, and finally a softmax classifier. The following diagram illustrates the
    LeNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fe3623a-6aaf-4b55-b59a-1de8f19a48ea.png)'
  prefs: []
  type: TYPE_IMG
- en: LeNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement the LeNet architecture with the `mxnet` library in our
    project using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output and the visual network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75a79bd5-f0a2-4b1a-a66e-25c17044489d.png)'
  prefs: []
  type: TYPE_IMG
- en: The code ran for less than 5 minutes on my 4-core CPU box, but still got us
    a 98% accuracy on the test dataset with just three epochs. We can also see that
    we obtained 98% accuracy with both the training and test datasets, confirming
    that there is no overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We see `tanh` is used as the activation function; let''s experiment and see
    whether it has any impact if we change it to ReLU. The code for the project will
    be identical except that we need to find and replace `tanh` with ReLU. We will
    not repeat the code as the only lines that have changed from the earlier project
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output on running the code with ReLU as the activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With ReLU being used as the activation function, we do not see a significant
    improvement in the accuracy. It stayed at 98%, which is the same as obtained with
    the `tanh` activation function.
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, we could try to rebuild the model with additional epochs to
    see whether the accuracy improves. Alternatively, we could try tweaking the number
    of filters and filter sizes per convolutional layer to see what happens! Further
    experiments could also include adding more layers of several kinds. We don't know
    what the result is going to be unless we experiment!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing computer vision with pretrained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](d8e2df34-05df-451e-88ce-62fdf17184d4.xhtml), *Exploring the Machine
    Learning Landscape*, we touched upon a concept called **transfer learning**. The
    idea is to take the knowledge learned in a model and apply it to another related
    task. Transfer learning is used on almost all computer vision tasks nowadays.
    It's rare to train models from scratch unless there is a huge labeled dataset
    available for training.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, in computer vision, CNNs try to detect edges in the earlier layers,
    shapes in the middle layer, and some task-specific features in the later layers.
    Irrespective of the image to be detected by the CNNs, the function of the earlier
    and middle layers remains the same, which makes it possible to exploit the knowledge
    gained by a pretrained model. With transfer learning, we can reuse the early and
    middle layers and only retrain the later layers. It helps us to leverage the labeled
    data of the task it was initially trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning offers two main advantages: it saves us training time and
    ensures that we have a good model even if we have a lot less labelled training
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Xception`, `VGG16`, `VGG19`, `ResNet50`, `InceptionV3`, `InceptionResNetV2`,
    `MobileNet`, `DenseNet`, `NASNet`, `MobileNetV2`, `QuocNet`, `AlexNet`, `Inception`
    (GoogLeNet), and `BN-Inception-v2` are some widely-used pretrained models. While
    we won''t delve into the details of each of these pretrained models, the idea
    of this section is to implement a project to detect the contents of images (input)
    by making use of a pretrained model through MXNet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code presented in this section, we make use of the pretrained Inception-BatchNorm
    network to predict the class of an image. The pretrained model needs to be downloaded
    to the working directory prior to running the code. The model can be downloaded
    from [http://data.mxnet.io/mxnet/data/Inception.zip](http://data.mxnet.io/mxnet/data/Inception.zip).
    Let''s explore the following code to label a few test images using the `inception_bn`
    pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15bfe7cc-5c35-4403-b29f-14806e65f863.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To process the images and predict the image IDs that have the highest probability
    of using the pretrained model, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output with the IDs of the highest probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the labels that correspond to the top-three predicted IDs with
    the highest probabilities using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output, we see that it has correctly labelled the image that is passed
    as input. We can test a few more images with the following code to confirm that
    the classification is done correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8595173-55f3-466a-a41e-f56c844220ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we can try for a third image using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0549ed49-82c2-4ead-9f2d-8c43781f2dc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about computer vision and its association with deep
    learning. We explored a specific type of deep learning algorithm, CNNs, that is
    widely used in computer vision. We studied an open source deep learning framework
    called MXNet. After a detailed discussion of the MNIST dataset, we built models
    using various network architectures and successfully classified the handwritten
    digits in the MNIST dataset. At the end of the chapter, we delved into the concept
    of transfer learning and explored its association with computer vision. The last
    project we built in this chapter classified images using an Inception-BatchNorm
    pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore an unsupervised learning algorithm called
    the autoencoder neural network. I am really excited to implement a project to
    capture credit card fraud using autoencoders. Are you game? Let's go!
  prefs: []
  type: TYPE_NORMAL
