- en: Image Recognition Using Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1966, Professor Seymour Papert at MIT conceptualized an ambitious summer
    project titled *The Summer Vision Project*. The task for the graduate student
    was to *plug a camera into a computer and enable it to understand what it sees*! I
    am sure it would have been super-difficult for the graduate student to have finished
    this project, as even today the task remains half complete.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: A human being, when they look outside, is able to recognize the objects that
    they see. Without thinking, they are able to classify a cat as a cat, a dog as
    a dog, a plant as a plant, an animal as an animal—this is happening because the
    human brain draws knowledge from its extensive prelearned database. After all,
    as human beings, we have millions of years' worth of evolutionary context that
    enables us draw inferences from the thing that we see. Computer vision deals with
    replicating the human vision processes so as to pass them on to machines and automate
    them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is all about learning the theory and implementation of computer
    vision through **machine learning** (**ML**). We will build a feedforward deep
    learning network and **LeNet** to enable handwritten digit recognition. We will
    also build a project that uses a pretrained Inception-BatchNorm network to identify
    objects in an image. We will cover the following topics as we progress in this
    chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computer vision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving computer vision with deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the MNIST dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a deep learning network for handwritten digit recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing computer vision with pretrained models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the projects covered in this chapter, we'll make use of a very popular open
    dataset called MNIST. We'll use **Apache MXNet**, a modern open source deep learning
    software framework to train and deploy the required deep neural networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computer vision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In today''s world, we have advanced cameras that are very successful at mimicking
    how a human eye captures light and color; but image-capturing in the right way
    is just stage one in the whole image-comprehension aspect. Post image-capturing,
    we will need to enable technology that interprets what has been captured and build
    context around it. This is what the human brain does when the eyes see something.
    Here comes the huge challenge: we all know that computers see images as huge piles
    of integer values that represent intensities across a spectrum of colors, and
    of course, computer have no context associated with the image itself. This is
    where ML comes into play. ML allows us to train a context for a dataset such that
    it enables computers to understand what objects certain sequences of numbers actually
    represent.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision is one of the emerging areas where ML is applied. It can be
    used for several purposes in various domains, including healthcare, agriculture,
    insurance, and the automotive industry. The following are some of its most popular
    applications:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Detecting diseases from medical images, such as CT scan/MRI scan images
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying crop diseases and soil quality to support a better crop yield
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying oil reserves from satellite images
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-driving cars
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and managing skin condition for psoriasis patients
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying and distinguishing weeds from crops
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial recognition
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting information from personal documents, such as passports and ID cards
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting terrain for drones and airplanes
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biometrics
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Public surveillance
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing personal photos
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering visual questions
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is just the tip of the iceberg. It's not an overstatement to say that there
    is no domain where we cannot find an application for computer vision. Therefore,
    computer vision is a key area for ML practitioners to focus on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Achieving computer vision with deep learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with, let''s understand the term **deep learning**. It simply means
    **multilayered neural networks**. The multiple layers enable deep learning to
    be an enhanced and powerful form of a neural network. **Artificial neural networks**
    (**ANNs**) have been in existence since the 1950s. They have always been designed
    with two layers; however, deep learning models are built with multiple hidden
    layers. The following diagram shows a hypothetical deep learning model:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d09fdae-a6ec-4587-9d21-fe9efc0062dc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Deep learning model—High level architecture
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are heavy on computation, therefore the **central processing
    unit** (**CPU**) that can be enabled with a maximum of 22 cores is generally thought
    of as an infrastructure blocker until recently. This infrastructure limitation
    also limited the usage of neural networks to solve real-world problems. However,
    recently, the availability of a **graphical processing unit** (**GPU**) with thousands
    of cores enabled has exponentially powerful computation possibilities when compared
    to CPUs. This gave a huge push to the usage of deep learning models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Data comes in many forms, such as tables, sounds, HTML files, TXT files, and
    images. Linear models do not generally learn from non-linear data. Non-linear
    algorithms, such as decision trees and gradient-boosting machines, also do not
    learn well from this kind of data. One the other hand, deep learning models that
    create non-linear interactions among the features give better solutions with non-linear
    data, so they have become the preferred models in the ML community.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: A deep learning model consists of a chain of interconnected neurons that creates
    the neural architecture. Any deep learning model will have an input layer, two
    or more hidden layers (middle layers), and an output layer. The input layer consists
    of neurons equal to the number of input variables in the data. Users can decide
    on the number of neurons and the number of hidden layers that a deep learning
    network should have. Generally, it is something that is optimized by the user
    building the network through a cross-validation strategy. The choice of the number
    of neurons and the number of hidden layers represents the challenge of the researcher.
    The number of neurons in the output layer is decided based on the outcome of the
    problem. For example, one output neuron in case it is regression, for a classification
    problem the output neurons is equal to the number of classes involved in the problem
    on-hand.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple types of deep learning algorithms, the one we generally use
    in computer vision is called a **Convolutional Neural Network **(**CNN**). CNNs
    break down images into small groups of pixels and then run calculations on them
    by applying filters. The result is then compared against pixel matrices they already
    know about. This helps CNNs to come up with a probability for the image belonging
    to one of the known classes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: In the first few layers, the CNN identifies shapes, such as curves and rough
    edges, but after several convolutions, they are able to recognize objects such
    as animals, cars, and humans.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: When the CNN is first built for the available data, the filter values of the
    network are randomly initialized and so the predictions it produce are mostly
    false. But then it keeps comparing its own predictions on labeled datasets to
    the actual ones, updating the filter values and improving performance of the CNN
    with each iteration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Layers of CNNs
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A CNN consists of an input and an output layer; it also has various hidden
    layers. The following are the various hidden layers in a CNN:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolution**: Assume that we have an image represented as pixels, a convolution
    is something where we have a little matrix nearly always 3 x 3 in deep learning
    and multiply every element of the matrix by every element of 3 x 3 section of
    the image and then add them all together to get the result of that convolution
    at one point. The following diagram illustrates the process of convolution on
    a pixel:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5357ec97-6f89-4348-92a2-26e7a6cea9b4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Convolution application on an image
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLU**): A non-linear activation that throws away
    the negatives in an input matrix. For example, let''s assume we have a 3 x 3 matrix
    with negative numbers, zeros, and positive numbers as values in the cells of the
    matrix. Given this matrix as input to ReLU, it transforms all negative numbers
    in the matrix to zeros and returns the 3 x 3 matrix. ReLU is an activation function
    that can be defined as part of the CNN architecture. The following diagram demonstrates
    the function of ReLU in CNNs:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU（修正线性单元）**：一个非线性激活，它丢弃输入矩阵中的负数。例如，假设我们有一个3 x 3的矩阵，其值在矩阵的单元格中为负数、零和正数。将此矩阵作为ReLU的输入，它将矩阵中的所有负数转换为零并返回3
    x 3矩阵。ReLU是一个可以作为CNN架构一部分定义的激活函数。以下图表展示了ReLU在CNN中的功能：'
- en: '![](img/ad096701-f380-414d-8028-f55c77a9f905.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad096701-f380-414d-8028-f55c77a9f905.png)'
- en: Rectified Linear Unit (ReLU) in CNNs
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的ReLU
- en: '**Max pooling**: Max pooling is something that can be set as a layer in the CNN
    architecture. It allows to identify if the specific characteristic is present
    in the previous level. It replaces the highest value in an input matrix with the
    maximum and gives the output. Let''s consider an example, with a 2 x 2 max pooling
    layer, given a 4 x 4 matrix as input, the max pooling layer replaces each 2 x
    2 in the input matrix with the highest value among the four cells. The output
    matrix thus obtained is non-overlapping and it''s an image representation with
    a reduced resolution. The following diagram illustrates the functionality of max
    pooling in a CNN:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**：最大池化可以在CNN架构中设置为一个层。它允许识别特定特征是否存在于前一层。它将输入矩阵中的最高值替换为最大值并给出输出。让我们考虑一个例子，给定一个2
    x 2的最大池化层，输入一个4 x 4的矩阵，最大池化层将输入矩阵中的每个2 x 2替换为四个单元格中的最高值。因此获得的输出矩阵是非重叠的，它是一个具有降低分辨率的图像表示。以下图表说明了CNN中最大池化的功能：'
- en: '![](img/ce2cc9f2-2ea0-4c38-9c48-f99f26af8ebc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce2cc9f2-2ea0-4c38-9c48-f99f26af8ebc.png)'
- en: Functionality of max pooling layer in CNNs
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中最大池化层的功能
- en: There are various reasons to apply max pooling, such as to reduce the amount
    of parameters and computation load, to eliminate overfitting, and, most importantly,
    to force the neural network to see the larger picture, as in previous layers it
    was focused on seeing bits and pieces of the image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 应用最大池化的原因有很多，例如减少参数数量和计算负载，消除过拟合，最重要的是，迫使神经网络看到更大的图景，因为在之前的层中，它专注于看到图像的片段。
- en: '**Fully-connected layer**: Also known as a **dense layer**, this involves a
    linear operation on the layer''s input vector. The layer ensures every input is
    connected to every output by a weight.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**：也称为**密集层**，它涉及对层的输入向量进行线性操作。该层确保每个输入都通过权重与每个输出相连接。'
- en: '**Softmax**: An activation function that is generally applied at the last layer
    of the deep neural network. In a multiclass classification problem, we require
    the fully-connected output of a deep learning network to be interpreted as a probability.
    The total probability of a particular observation in data (for all classes) should
    add up to 1, and the probability of the observation belonging to each class should
    range between 0 and 1\. Therefore, we transform each output of the fully-connected
    layer as a portion of a total sum. However, instead of simply doing the standard
    proportion, we apply this non-linear exponential function for a very specific
    reason: we would like to make our highest output as close to 1 as possible and
    our lower output as close to 0\. Softmax does this by pushing the true linear
    proportions closer to either 1 or 0.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：通常应用于深度神经网络最后一层的激活函数。在多类分类问题中，我们需要深度学习网络的全连接输出被解释为概率。数据中特定观察的总概率（对于所有类别）应加起来为1，并且观察属于每个类别的概率应在0到1之间。因此，我们将全连接层的每个输出转换为总和中的一部分。然而，我们不是简单地做标准比例，而是出于一个非常具体的原因应用这个非线性指数函数：我们希望将最高输出尽可能接近1，将较低输出尽可能接近0。Softmax通过将真实线性比例推向1或0来实现这一点。'
- en: 'The following diagram illustrates the softmax activation function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了softmax激活函数：
- en: '![](img/532ddd0f-24a2-41f4-bdac-293eecfdaf2e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/532ddd0f-24a2-41f4-bdac-293eecfdaf2e.png)'
- en: Softmax activation function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax激活函数
- en: '**Sigmoid**: This is similar to softmax, except that it is applied to a binary
    classification, such as cats versus dogs. With this activation function, the class
    to which the observation belongs is assigned a higher probability compared to
    the other class. Unlike softmax, the probabilities do not have to add up to 1.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the MXNet framework
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MXNet is a super-powerful open source deep learning framework that is built
    to ease the development of deep learning algorithms. It is used to define, train,
    and deploy deep neural networks. MXNet is lean, flexible, and ultra-scalable,
    that is, it allows fast model training and supports a flexible programming model
    with multiple languages. The problem with existing deep learning frameworks, such
    as Torch7, Theano, and Caffe, is that users need to learn another system or a
    different programming flavor.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: However, MXNet resolves this issue by supporting multiple languages, such as
    C++, Python, R, Julia, and Perl. This eliminates the need for users to learn a
    new language; therefore, they can use the framework and simplify network definitions.
    MXNet models are able to fit in small amounts of memory and they can be trained
    on CPUs, GPUs, and on multiple machines with ease. The `mxnet` package is readily
    available for the R language and the details of the install can be looked up in
    **Apache Incubator** at [https://mxnet.incubator.apache.org](https://mxnet.incubator.apache.org).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the MNIST dataset
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) is
    a dataset that contains images of handwritten digits. This dataset is pretty popular
    in the ML community for implementing and testing computer vision algorithms. The
    MNIST dataset is an open dataset made available by Professor Yann LeCun at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/),
    where separate files that represent the training dataset and test dataset are
    available. The labels corresponding to the test and training datasets are also
    available as separate files. The training dataset has 60,000 samples and the test
    dataset has 10,000 samples.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows some sample images from the MNIST dataset. Each
    of the images also comes with a label indicating the digit shown in the following
    screenshot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f8f082b-0bbb-4e47-ab65-6cd430a0b1d5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Sample images from MNIST dataset
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The labels for the images shown in the preceding diagram are **5**, **0**,
    **4**, and **1**. Each image in the dataset is a grayscale image and is represented
    in 28 x 28 pixels. A sample image represented with pixels is shown in the following
    screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/853fd20e-35be-49ed-949c-57b05ea199fd.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Sample image from MNIST dataset represented with 28 * 28 pixels
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to flatten the 28 x 28 pixel matrix and represent it as a vector
    of 784 pixel values. Essentially, the training dataset is a 60,000 x 784 matrix
    that could be used with ML algorithms. The test dataset is a 10,000 x 784 matrix.
    The training and test datasets may be downloaded from the source with the following
    code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the data is downloaded and unzipped, we will see the files in our working
    directory. However, these files are in binary format and they cannot be directly
    loaded through the regular `read.csv` command. The following custom function code
    helps to read the training and test data from the binary files:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The functions may be called with the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In RStudio, when we execute the code, we see `train` , `test`,  `train.y`,
    and `test.y` displayed under the Environment tab. This confirms that the datasets
    are successfully loaded and the respective dataframes are created, as shown in
    the following screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b85709ce-90db-428b-a38a-aa7c6f57cb78.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'Once the image data is loaded into the dataframe, it is in the form of a series
    of numbers that represent the pixel values. The following is a helper function
    that visualizes the pixel data as an image in RStudio:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `show_digit()` function may be called like any other R function with the
    dataframe record number as a parameter. For example, the function in the following
    code block helps to visualize the `3` record in the training dataset as an image
    in RStudio:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will give the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88617ad9-3c55-44fb-9a2e-8957dae8647c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: 'Dr. David Robinson, in his blog on *Exploring handwritten digit classification:
    a tidy analysis of the MNIST dataset* ([http://varianceexplained.org/r/digit-eda/](http://varianceexplained.org/r/digit-eda/)),
    performed a beautiful exploratory data analysis of the MNIST dataset, which will
    help you better understand the dataset.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep learning network for handwritten digit recognition
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mxnet` library offers several functions that enable us to define the layers
    and activations that comprise the deep learning network. The definition of layers,
    the usage of activation functions, and the number of neurons to be used in each
    of the hidden layers is generally termed the **network architecture**. Deciding
    on the network architecture is more of an art than a science. Often, several iterations
    of experiments may be needed to decide on the right architecture for the problem.
    We call it an art as there are no exact rules for finding the ideal architecture.
    The number of layers, neurons in these layers, and the type of layers are pretty
    much decided through trial and error.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll build a simple deep learning network with three hidden
    layers. Here is the general architecture of our network:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is defined as the initial layer in the network. The `mx.symbol.Variable` MXNet
    function defines the input layer.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A fully-connected layer is defined, also called a dense layer, with 128 neurons
    as the first hidden layer in the network. This can be done using the `mx.symbol.FullyConnected` MXNet function.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A ReLU activation function is defined as part of the network. The `mx.symbol.Activation`
    function helps us to define the ReLU activation function as part of the network.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the second hidden layer; it is another dense layer with 64 neurons. This
    can be accomplished through the `mx.symbol.FullyConnected` function, similar to
    the first hidden layer.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a ReLU activation function on the second hidden layer's output. This can
    be done through the `mx.symbol.Activation` function.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final hidden layer in our network is another fully-connected layer, but
    with just ten outputs (equal to the number of classes). This can be done through
    the `mx.symbol.FullyConnected` function as well.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output layer needs to be defined and this should be probabilities of prediction
    for each class; therefore, we apply softmax at the output layer. The `mx.symbol.SoftmaxOutput`
    function enables us to configure the softmax in the output.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are not saying that this is the best network architecture possible for the
    problem, but this is the network we are going to build to demonstrate the implementation
    of a deep learning network with MXNet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a blueprint in place, let''s delve into coding the network
    using the following code block:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will give the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, define the three layers and start training the network to obtain class
    probabilities and ensure the results are reproducible using the following code
    block:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will give the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To make predictions on the test dataset and get the label for each observation
    in the test dataset, use the following code block:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will give the following output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s check the performance of the model using the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will give the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To visualize the network architecture, use the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will give the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b6945d7-f014-474a-b699-64cef7c3374d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: With the simple architecture running for a few minutes on a CPU-based laptop
    and with minimal effort, we were able to achieve an accuracy of `97.7%` on the
    test dataset. The deep learning network was able to learn to interpret the digits
    by seeing the images it was given as input. The accuracy of the system can be
    further improved by altering the architecture or by increasing the number of iterations.
    It may be noted that, in the earlier experiment, we ran it for 10 iterations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of iterations can simply be amended when model-building through
    the `num.round` parameter. There is no hard-and-fast rule in terms of the optimal
    number of rounds, so this is something to be determined by trial and error. Let''s
    build the model with 50 iterations and observe its impact on performance. The
    code will remain the same as the earlier project, except with the following amendment
    to the model-building code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Observe that the `num.round` parameter is now set to `50`, instead of the earlier
    value of `10`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'This will give the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can observe from the output that 100% accuracy was obtained with the training
    dataset. However, with the test dataset, we observe the accuracy as 98%. Essentially,
    our model is expected to perform the same with both the training and test dataset
    for it to be called a good model. Unfortunately, in this case, we have encountered
    a situation known as **overfitting,** which means that the model we created did
    not generalize well. In other words, the model has trained itself with too many
    parameters or it got trained for too long and has become super-specialized with
    data in the training dataset alone; as an effect, it is not doing a good job with
    new data. Model generalization is something we should specifically aim for. There
    is a technique, known as **dropout**, that can help us to overcome the overfitting
    issue.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Implementing dropout to avoid overfitting
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout is defined in the network architecture after the activation layers,
    and it randomly sets activations to zero. In other words, dropout randomly deletes
    parts of the neural network, which allows us to prevent overfitting. We can't
    overfit exactly to our training data when we're consistently throwing away information
    learned along the way. This allows our neural network to learn to generalize better.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'In MXNet, dropout can be easily defined as part of network architecture using
    the `mx.symbol.Dropout` function. For example, the following code defines dropouts
    post the first ReLU activation (`act1`) and second ReLU activation (`act2`):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `data` parameter specifies the input that the dropout takes and the value
    of `p` specifies the amount of dropout to be done. In case of `dropout1`, we are
    specifying that 50% of weights are to be dropped. Again, there is no hard-and-fast
    rule in terms of how much dropout should be included and at what layers. This
    is something to be determined through trial and error. The code with dropouts
    almost remains identical to the earlier project except that it now includes the
    dropouts after the activations:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will give the following output and the visual network architecture:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Take a look at the following diagram:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e7e2381-a190-44e2-a9cd-cb85033c0b96.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: We can see from the output that dropout is now included as part of the network
    architecture. We also observe that this network architecture yields a lower accuracy
    on the test dataset when compared with our initial project. One reason could be
    that the dropout percentages (50% and 30%) we included are too high. We could
    play with these percentages and rebuild the model to determine whether the accuracy
    gets better. The idea, however, is to demonstrate the use of dropout as a regularization
    technique so as to avoid overfitting in deep neural networks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from dropout, there are other techniques you could employ to avoid an
    overfitting situation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**Addition of data**: Adding more training data.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation**: Creating additional data synthetically by applying techniques
    such as flipping, distorting, adding random noise, and rotation. The following
    screenshot shows sample images created after applying data augmentation:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cdd28deb-0725-4120-aeec-075ba07f4e98.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Sample images from applying data augmentation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Reducing complexity of the network architecture**: Fewer layers, fewer epochs,
    and so on.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch normalization**: A process of ensuring that the weights generated in
    the network do not push very high or very low. This is generally achieved by subtracting
    the mean and dividing by the standard deviation of all weights at a layer from
    each weight in a layer. It shields against overfitting, performs regularization,
    and significantly improves the training speed.  The `mx.sym.batchnorm()` function
    enables us to define batch normalization after the activation.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will not focus on developing another project with batch normalization as
    using this function in the project is very similar to the other functions we used
    in our earlier projects. So far, we have focused on increasing the epochs to improve
    the performance of the model, another option is to try a different architecture
    and evaluate whether that improves the accuracy on the test dataset. On that note,
    let's explore LeNet, which is specifically designed for optical character recognition
    in documents.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the LeNet architecture with the MXNet library
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In their 1998 paper, *Gradient-Based Learning Applied to Document Recognition*,
    LeCun et al. introduced the LeNet architecture.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The LeNet architecture consists of two sets of convolutional, activation, and
    pooling layers, followed by a fully-connected layer, activation, another fully-connected
    layer, and finally a softmax classifier. The following diagram illustrates the
    LeNet architecture:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fe3623a-6aaf-4b55-b59a-1de8f19a48ea.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: LeNet architecture
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement the LeNet architecture with the `mxnet` library in our
    project using the following code block:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will give the following output and the visual network architecture:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Take a look at the following diagram:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75a79bd5-f0a2-4b1a-a66e-25c17044489d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: The code ran for less than 5 minutes on my 4-core CPU box, but still got us
    a 98% accuracy on the test dataset with just three epochs. We can also see that
    we obtained 98% accuracy with both the training and test datasets, confirming
    that there is no overfitting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'We see `tanh` is used as the activation function; let''s experiment and see
    whether it has any impact if we change it to ReLU. The code for the project will
    be identical except that we need to find and replace `tanh` with ReLU. We will
    not repeat the code as the only lines that have changed from the earlier project
    are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will get the following output on running the code with ReLU as the activation
    function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With ReLU being used as the activation function, we do not see a significant
    improvement in the accuracy. It stayed at 98%, which is the same as obtained with
    the `tanh` activation function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, we could try to rebuild the model with additional epochs to
    see whether the accuracy improves. Alternatively, we could try tweaking the number
    of filters and filter sizes per convolutional layer to see what happens! Further
    experiments could also include adding more layers of several kinds. We don't know
    what the result is going to be unless we experiment!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Implementing computer vision with pretrained models
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](d8e2df34-05df-451e-88ce-62fdf17184d4.xhtml), *Exploring the Machine
    Learning Landscape*, we touched upon a concept called **transfer learning**. The
    idea is to take the knowledge learned in a model and apply it to another related
    task. Transfer learning is used on almost all computer vision tasks nowadays.
    It's rare to train models from scratch unless there is a huge labeled dataset
    available for training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Generally, in computer vision, CNNs try to detect edges in the earlier layers,
    shapes in the middle layer, and some task-specific features in the later layers.
    Irrespective of the image to be detected by the CNNs, the function of the earlier
    and middle layers remains the same, which makes it possible to exploit the knowledge
    gained by a pretrained model. With transfer learning, we can reuse the early and
    middle layers and only retrain the later layers. It helps us to leverage the labeled
    data of the task it was initially trained on.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning offers two main advantages: it saves us training time and
    ensures that we have a good model even if we have a lot less labelled training
    data.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '`Xception`, `VGG16`, `VGG19`, `ResNet50`, `InceptionV3`, `InceptionResNetV2`,
    `MobileNet`, `DenseNet`, `NASNet`, `MobileNetV2`, `QuocNet`, `AlexNet`, `Inception`
    (GoogLeNet), and `BN-Inception-v2` are some widely-used pretrained models. While
    we won''t delve into the details of each of these pretrained models, the idea
    of this section is to implement a project to detect the contents of images (input)
    by making use of a pretrained model through MXNet.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code presented in this section, we make use of the pretrained Inception-BatchNorm
    network to predict the class of an image. The pretrained model needs to be downloaded
    to the working directory prior to running the code. The model can be downloaded
    from [http://data.mxnet.io/mxnet/data/Inception.zip](http://data.mxnet.io/mxnet/data/Inception.zip).
    Let''s explore the following code to label a few test images using the `inception_bn`
    pretrained model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will result in the following output:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15bfe7cc-5c35-4403-b29f-14806e65f863.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'To process the images and predict the image IDs that have the highest probability
    of using the pretrained model, we use the following code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will result in the following output with the IDs of the highest probabilities:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s print the labels that correspond to the top-three predicted IDs with
    the highest probabilities using the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will give the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'From the output, we see that it has correctly labelled the image that is passed
    as input. We can test a few more images with the following code to confirm that
    the classification is done correctly:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will give the following output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8595173-55f3-466a-a41e-f56c844220ad.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Likewise, we can try for a third image using the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will give the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0549ed49-82c2-4ead-9f2d-8c43781f2dc1.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about computer vision and its association with deep
    learning. We explored a specific type of deep learning algorithm, CNNs, that is
    widely used in computer vision. We studied an open source deep learning framework
    called MXNet. After a detailed discussion of the MNIST dataset, we built models
    using various network architectures and successfully classified the handwritten
    digits in the MNIST dataset. At the end of the chapter, we delved into the concept
    of transfer learning and explored its association with computer vision. The last
    project we built in this chapter classified images using an Inception-BatchNorm
    pretrained model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore an unsupervised learning algorithm called
    the autoencoder neural network. I am really excited to implement a project to
    capture credit card fraud using autoencoders. Are you game? Let's go!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
