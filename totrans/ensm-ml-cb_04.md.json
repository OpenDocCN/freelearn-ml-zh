["```py\n# import os for operating system dependent functionalities\nimport os\n\n# import other required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n```", "```py\n# Set your working directory according to your requirement\nos.chdir(\".../Chapter 4/Linear Regression\")\nos.getcwd()\n```", "```py\ndf_housingdata = pd.read_csv(\"Final_HousePrices.csv\")\n```", "```py\n# See the variables and their data types\ndf_housingdata.dtypes\n```", "```py\n# We pass 'pearson' as the method for calculating our correlation\ndf_housingdata.corr(method='pearson')\n```", "```py\n# we store the correlation matrix output in a variable\npearson = df_housingdata.corr(method='pearson')\n\n# assume target attr is the last, then remove corr with itself\ncorr_with_target = pearson.iloc[-1][:-1]\n\n# attributes sorted from the most predictive\ncorr_with_target.sort_values(ascending=False)\n```", "```py\nf, ax = plt.subplots(figsize=(11, 11))\n\n# Generate a mask for the upper triangle\n# np.zeros_like - Return an array of zeros with the same shape and type as a given array\n# In this case we pass the correlation matrix\n# we create a variable “mask” which is a 14 X 14 numpy array\n\nmask = np.zeros_like(pearson, dtype=np.bool)\ntt = np.triu_indices_from(mask)\n\n# We create a tuple with triu_indices_from() by passing the “mask” array\n# k is used to offset diagonal\n# with k=0, we offset all diagnoals\n# If we put k=13, means we offset 14-13=1 diagonal\n\n# triu_indices_from() Return the indices for the upper-triangle of arr.\nmask[np.triu_indices_from(mask, k=0)] = True\n\n# First 2 param - anchor hues for negative and positive extents of the map.\n# 3rd param - Anchor saturation for both extents of the map\n# If true, return a matplotlib colormap object rather than a list of colors.\n\ncmap = sns.diverging_palette(10, 129, s=50, as_cmap=True)\n\n# Adjust size of the legend bar with cbar_kws={“shrink”: 0.5}\n# cmap=“YlGnBu” gives the color from Yellow-Green-Blue palette\n\nsns.heatmap(pearson, mask=mask, cmap=\"YlGnBu\", vmax=.3, center=0,\n           square=True, linewidths=.1, cbar_kws={\"shrink\": 0.5})\n```", "```py\n# Setting the plot size\nplt.figure(figsize=(8, 8))\n\nsns.distplot(df_housingdata['SalePrice'], bins=50, kde=True)\n```", "```py\nfrom scipy import stats\ng = sns.JointGrid(df_housingdata['YearBuilt'], df_housingdata['SalePrice'])\ng = g.plot(sns.regplot, sns.distplot)\ng = g.annotate(stats.pearsonr)\n```", "```py\n# create a variable to hold the names of the data types viz int16, in32 and so on\nnum_cols = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\n# Filter out variables with numeric data types\ndf_numcols_only = df_housingdata.select_dtypes(include=num_cols)\n```", "```py\n# Importing MinMaxScaler and initializing it\nfrom sklearn.preprocessing import MinMaxScaler\nmin_max=MinMaxScaler()\n\n# Scaling down the numeric variables\n# We exclude SalePrice using iloc() on df_numcols_only DataFrame\ndf_housingdata_numcols=pd.DataFrame(min_max.fit_transform(df_numcols_only.iloc[:,0:36]), columns=df_numcols_only.iloc[:,0:36].columns.tolist())\n```", "```py\n# We exclude all numeric columns\ndf_housingdata_catcol = df_housingdata.select_dtypes(exclude=num_cols)\n\n# Steps to one-hot encoding:\n# We iterate through each categorical column name\n# Create encoded variables for each categorical columns\n# Concatenate the encoded variables to the DataFrame\n# Remove the original categorical variable\nfor col in df_housingdata_catcol.columns.values:\n   one_hot_encoded_variables = pd.get_dummies(df_housingdata_catcol[col],prefix=col)\n   df_housingdata_catcol = pd.concat([df_housingdata_catcol,one_hot_encoded_variables],axis=1)\n   df_housingdata_catcol.drop([col],axis=1, inplace=True)\n```", "```py\ndf_housedata = pd.concat([df_housingdata_numcols, df_housingdata_catcol], axis=1)\n```", "```py\n# Concatenate SalePrice to the final DataFrame\ndf_housedata_final = pd.concat([df_housedata, df_numcols_only.iloc[:,36]], axis=1)\n```", "```py\n# Create feature and response variable set\n# We create train & test sample from our dataset\nfrom sklearn.model_selection import train_test_split\n\n# create feature & response variables\nX = df_housedata_final.iloc[:,0:302]\nY = df_housedata_final['SalePrice']\n\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = \\\ntrain_test_split(X, Y, test_size=0.30, random_state=1)\n```", "```py\nimport numpy as np\nfrom sklearn.linear_model import SGDRegressor\n\nlin_model = SGDRegressor()\n\n# We fit our model with train data\nlin_model.fit(X_train, Y_train)\n\n# We use predict() to predict our values\nlin_model_predictions = lin_model.predict(X_test)\n\n# We check the coefficient of determination with score()\nprint(lin_model.score(X_test, Y_test))\n\n# We can also check the coefficient of determination with r2_score() from sklearn.metrics\nfrom sklearn.metrics import r2_score\nprint(r2_score(Y_test, lin_model_predictions))\n```", "```py\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(Y_test, lin_model_predictions)\nrmse = np.sqrt(mse)\nprint(rmse)\n```", "```py\nplt.figure(figsize=(8, 8))\nplt.scatter(Y_test, lin_model_predictions)\nplt.xlabel('Actual Median value of house prices ($1000s)')\nplt.ylabel('Predicted Median value of house prices ($1000s)')\nplt.tight_layout()\n```", "```py\nlin_model = SGDRegressor(alpha=0.0000001, max_iter=2000)\n```", "```py\n# import os for operating system dependent functionalities\nimport os\n\n# import other required libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nimport matplotlib.pyplot as plt\n```", "```py\n# Set your working directory according to your requirement\nos.chdir(\".../Chapter 4/Logistic Regression\")\nos.getcwd()\n```", "```py\ndf_creditdata = pd.read_csv(\"UCI_Credit_Card.csv\")\n```", "```py\nprint(df_creditdata.shape)\nprint(df_creditdata.head())\n```", "```py\ndf_creditdata.dtypes\n```", "```py\ndf_creditdata.drop([\"ID\"],axis=1,inplace=True)\n```", "```py\ndf_creditdata.isnull().sum()\n```", "```py\n# split features & response variable\nX = df_creditdata.iloc[:,0:23]\nY = df_creditdata['default.payment.next.month']\n\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)\n```", "```py\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n```", "```py\n# We create an instance of SGDClassifier()\nlogistic_model = SGDClassifier(alpha=0.000001, loss=‘log’, max_iter=100000, penalty=‘l2’)\n\n# We fit our model to the data\nfitted_model = logistic_model.fit(X_train, Y_train)\n\n# We use predict_proba() to predict the probabilities\npredictedvalues = fitted_model.predict_proba(X_test)\n\n# We print the probabilities to take a glance\npredictedvalues\n```", "```py\n# We take the predicted values of class 1\nY_predicted = predictedvalues[:, 1]\n\n# We check to see if the right values have been considered from the predicted values\nprint(Y_predicted)\n```", "```py\n# Check for accuracy\nlogistic_model.score(X_test,Y_test)\n```", "```py\n# We use roc_curve() to generate fpr & tpr values\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predicted)\n\n# we pass the fpr & tpr values to auc() to calculate the area under curve\nroc_auc = auc(fpr, tpr)\nprint(roc_auc)\n```", "```py\nplt.figure()\nplt.plot(fpr,tpr, color=‘orange’, lw=2, label=‘ROC curve (area under curve = %0.2f)’ % roc_auc)\nplt.plot([0, 1], [0, 1], color=‘darkgrey’, lw=2, linestyle=‘--’)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel(‘False Positive Rate (1-Specificity)’)\nplt.ylabel(‘True Positive Rate (Sensitivity)’)\nplt.title(‘ROC Curve’)\nplt.legend(loc=“upper left”)\nplt.show()\n```", "```py\n# import os for operating system dependent functionalities\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n```", "```py\nos.chdir(\".../Chapter 4/Naive Bayes\")\nos.getcwd()\n```", "```py\ndf_messages = pd.read_csv('spam.csv', encoding='latin-1', \\\n                          sep=',', names=['labels','message'])\n```", "```py\n df_messages.head(3)\n```", "```py\ndf_messages.describe()\n```", "```py\ndf_messages.groupby('labels').describe()\n```", "```py\ndf_messages['word_count'] = df_messages['message'].apply(lambda x: len(str(x).split(\" \")))\ndf_messages['character_count'] = df_messages['message'].str.len() \n\ndf_messages[['message','word_count', 'character_count']].head()\n```", "```py\nlabels_count = pd.DataFrame(df_messages.groupby('labels')['message'].count())\nlabels_count.reset_index(inplace = True)\nplt.figure(figsize=(4,4))\nsns.barplot(labels_count['labels'], labels_count['message'])\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Labels', fontsize=12)\nplt.show()\n```", "```py\n# create a variable that holds a key-value pair for ham and spam\nclass_labels = {\"ham\":0,\"spam\":1}\n\n# use the class_labels variable with map()\ndf_messages['labels']=df_messages['labels'].map(class_labels)\ndf_messages.head()\n```", "```py\n# Split your data into train & test set\nX_train, X_test, Y_train, Y_test = train_test_split(df_messages[‘message’],\\\n                                 df_messages[‘labels’],test_s=0.2,random_state=1)\n```", "```py\n# Creating an instance of the CountVectorizer class\n# If ‘english’, a built-in stop word list for English is used.\n# There are known issues with ‘english’ and you should consider an alternative\nvectorizer = CountVectorizer(lowercase=True, stop_words=‘english’, analyzer=‘word’)\n\n# Learn a vocabulary from one or more message using the fit_transform() function\nvect_train = vectorizer.fit_transform(X_train)\n```", "```py\n# Create an instance of MultinomialNB()\nmodel_nb = MultinomialNB()\n\n# Fit your data to the model\nmodel_nb.fit(vect_train,Y_train)\n\n# Use predict() to predict target class\npredict_train = model_nb.predict(vect_train)\n```", "```py\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n```", "```py\n# Calculate Train Accuracy\nprint(‘Accuracy score: {}’.format(accuracy_score(Y_train, predict_train)))\n\n# Calculate other metrics on your train results\nprint(‘Precision score: {}’.format(precision_score(Y_train, predict_train)))\nprint(‘Recall score: {}’.format(recall_score(Y_train, predict_train)))\nprint(‘F1 score: {}’.format(f1_score(Y_train, predict_train)))\n```", "```py\n# We apply the model into our test data\nvect_test = vectorizer.transform(X_test)\nprediction = model_nb.predict(vect_test)\n\n# Calculate Test Accuracy\nprint(‘Accuracy score: {}’.format(accuracy_score(Y_test, prediction)))\n\n# Calculate other metrics on your test data\nprint(‘Precision score: {}’.format(precision_score(Y_test, prediction)))\nprint(‘Recall score: {}’.format(recall_score(Y_test, prediction)))\nprint(‘F1 score: {}’.format(f1_score(Y_test, prediction)))\n```", "```py\n# import os for operating system dependent functionalities\nimport os\n\n# import other required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nimport itertools\nfrom sklearn import tree\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```", "```py\n# Set your working directory according to your requirement\nos.chdir(\".../Chapter 4/Decision Tree\")\n\n# Check Working Directory \nos.getcwd()\n```", "```py\ndf_backorder = pd.read_csv(\"BackOrders.csv\")\n```", "```py\ndf_backorder.shape\ndf_backorder.head()\ndf_backorder.describe()\n```", "```py\ndf_backorder.dtypes\n```", "```py\ndf_backorder.drop('sku', axis=1, inplace=True)\n```", "```py\ndf_backorder.isnull().sum()\n```", "```py\ndf_backorder = df_backorder.dropna(axis=0)\n```", "```py\nnon_numeric_attributes = df_backorder.select_dtypes(include=['object']).columns\ndf_backorder = pd.get_dummies(columns=non_numeric_attributes, data=df_backorder, prefix=non_numeric_attributes, prefix_sep=\"_\",drop_first=True)\ndf_backorder.dtypes\n```", "```py\n# Target variable distribution\npd.value_counts(df_backorder['went_on_backorder_Yes'].values)\n```", "```py\n#Performing train test split on the data\nX, Y = df_backorder.loc[:,df_backorder.columns!=‘went_on_backorder_Yes’].values, df_backorder.loc[:,‘went_on_backorder_Yes’].values\n\n# Split our dataset into train & test set\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n```", "```py\n# Create an instance of DecisionTreeClassifier()\nclassifier = tree.DecisionTreeClassifier(random_state=1)\n\n# Fit our model to the data\nmodel_DT_Gini = classifier.fit(X_train, Y_train)\nmodel_DT_Gini\n```", "```py\n# Predict with our test data\ntest_predictedvalues = model_DT_Gini.predict(X_test)\n\n# Check accuracy\nacc = accuracy_score(Y_test, test_predictedvalues)\nprint(\"Accuracy is\", acc)\n\n# Check TP, TN, FP, FN\ntn, fp, fn, tp = confusion_matrix(Y_test, test_predictedvalues).ravel()\nprint(\"TN:\",tn, \" FP:\",fp, \" FN:\",fn, \" TP:\",tp)\n```", "```py\ntarget_names = [ ‘No’, ‘Yes’]\n\n#Pass Actual & Predicted values to confusion_matrix()\ncm = confusion_matrix(Y_test, test_predictedvalues)\n\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names, normalize=False)\nplt.show()\n```", "```py\n# set the parameters for grid search\ngrid_search_parameters = {“criterion”: [“gini”, “entropy”],\n             “min_samples_split”: [2],\n             “max_depth”: [None, 2, 3],\n             “min_samples_leaf”: [1, 5],\n             “max_leaf_nodes”: [None],\n             }\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\n# Create an instance of DecisionTreeClassifier()\nclassifier = tree.DecisionTreeClassifier()\n\n# Use GridSearchCV(), pass the values you have set for grid search\nmodel_DT_Grid = GridSearchCV(classifier, grid_search_parameters, cv=10)\nmodel_DT_Grid.fit(X_train, Y_train)\n```", "```py\nmodel_DT_Grid.best_params_ \n```", "```py\ntest_predictedvalues = model_DT_Grid.predict(X_test)\n\ncc = accuracy_score(Y_test, test_predictedvalues)\nprint(\"Accuracy is\", acc)\n\ntn, fp, fn, tp = confusion_matrix(Y_test, test_predictedvalues).ravel()\nprint(\"TN:\",tn, \" FP:\",fp, \" FN:\",fn, \" TP:\",tp)\n\ncm = confusion_matrix(Y_test, test_predictedvalues)\n\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names, normalize=False)\nplt.show()\n```", "```py\nfrom sklearn.metrics import classification_report\n\ntarget_names = [ 'No', 'Yes']\nprint(classification_report(Y_test, test_predictedvalues, target_names=target_names))\n```", "```py\n# import os for operating system dependent functionalities\nimport os\n\n# import other required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\n```", "```py\n# Set your working directory according to your requirement\nos.chdir(\".../Chapter 4/Support Vector Machine\")\nos.getcwd()\n```", "```py\ndf_bankdata = pd.read_csv(\"bank.csv\")\n```", "```py\ndf_bankdata.dtypes\n```", "```py\ndf_bankdata.isnull().sum()\n```", "```py\nprint(\"Total number of class labels: {}\".format(df_bankdata.shape[0]))\nprint(\"Number of people opted for Term Deposit: {}\".format(df_bankdata[df_bankdata.y == 'yes'].shape[0]))\nprint(\"Number of people not opted for Term Deposit: {}\".format(df_bankdata[df_bankdata.y == 'no'].shape[0]))\n```", "```py\ndf_bankdata['y'] = (df_bankdata['y']=='yes').astype(int)\n```", "```py\n# Using select_dtypes() to select only the non-numerical type variable\ncolumn_type = ['object']\ndf_bank_data_category_cols = df_bankdata.select_dtypes(column_type)\n\n# This will give you the names of the non-numerical variables\ncategory_column_names = df_bank_data_category_cols.columns.values.tolist()\ncategory_column_names\n```", "```py\nfor each_col in category_column_names:\n   dummy_var = pd.get_dummies(df_bank_data_category_cols[each_col], prefix=each_col)\n   df_joindata = df_bankdata.join(dummy_var)\n   df_joindata.drop([each_col], axis=1, inplace=True)\n   df_bankdata = df_joindata\n```", "```py\n# Separate features & response variable\nX=df_bankdata.iloc[:, :-1]\nY=df_bankdata['y']\n```", "```py\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n```", "```py\n# Note, you need not pass kernel='rbf' to the SVC() because its the default\nsvc_model = SVC(kernel='rbf') \nsvc_model.fit(X_train, Y_train)\n```", "```py\ntrain_predictedvalues=svc_model.predict(X_train)\ntest_predictedvalues=svc_model.predict(X_test)\n\nprint('Train Accuracy Score:')\nprint(metrics.accuracy_score(Y_train,train_predictedvalues))\n\nprint('Test Accuracy Score:')\nprint(metrics.accuracy_score(Y_test,test_predictedvalues))\n```", "```py\nsvc_model =SVC(kernel='poly') \nsvc_model.fit(X_train, Y_train)\n\ntrain_predictedvalues=svc_model.predict(X_train)\ntest_predictedvalues=svc_model.predict(X_test)\n\nprint('Train Accuracy Score:')\nprint(metrics.accuracy_score(Y_train,train_predictedvalues))\n\nprint('Test Accuracy Score:')\nprint(metrics.accuracy_score(Y_test,test_predictedvalues))\n```", "```py\nsvc_model =SVC(kernel='linear') \n```"]