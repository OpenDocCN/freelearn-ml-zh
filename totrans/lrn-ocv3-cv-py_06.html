<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Retrieving Images and Searching Using Image Descriptors"><div class="titlepage" id="aid-1CQAE2"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Retrieving Images and Searching Using Image Descriptors</h1></div></div></div><p>Similar to the human eyes and brain, OpenCV can detect the main features of an image and extract these into so-called <a id="id308" class="indexterm"/>image descriptors. These features can then be used as a database, enabling image-based searches. Moreover, we can use keypoints to stitch images together and compose a bigger image (think of putting together many pictures to form a 360 degree panorama).</p><p>This chapter shows you how to detect features of an image with OpenCV and make use of them to match and search images. Throughout the chapter, we will take sample images and detect their main features, and then try to find a sample image contained in another image using homography.</p><div class="section" title="Feature detection algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Feature detection algorithms</h1></div></div></div><p>There are a number <a id="id309" class="indexterm"/>of algorithms that can be used to detect and extract features, and we will explore most of them. The most common algorithms used in OpenCV are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Harris</strong></span>: This <a id="id310" class="indexterm"/>algorithm is useful to detect corners</li><li class="listitem"><span class="strong"><strong>SIFT</strong></span>: This <a id="id311" class="indexterm"/>algorithm is useful to detect blobs</li><li class="listitem"><span class="strong"><strong>SURF</strong></span>: This <a id="id312" class="indexterm"/>algorithm is useful to detect blobs</li><li class="listitem"><span class="strong"><strong>FAST</strong></span>: This <a id="id313" class="indexterm"/>algorithm is useful to detect corners</li><li class="listitem"><span class="strong"><strong>BRIEF</strong></span>: This <a id="id314" class="indexterm"/>algorithm is useful to detect blobs</li><li class="listitem"><span class="strong"><strong>ORB</strong></span>: This <a id="id315" class="indexterm"/>algorithm stands for <span class="strong"><strong>Oriented FAST and Rotated BRIEF</strong></span></li></ul></div><p>Matching features can be performed with the following methods:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Brute-Force matching</li><li class="listitem">FLANN-based matching</li></ul></div><p>Spatial verification can <a id="id316" class="indexterm"/>then be performed with homography.</p><div class="section" title="Defining features"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec31"/>Defining features</h2></div></div></div><p>What is a feature <a id="id317" class="indexterm"/>exactly? Why is a particular area of an image classifiable as a feature, while others are not? Broadly speaking, a feature is an area of interest in the image that is unique or easily recognizable. As you can imagine, corners and high-density areas are good features, while patterns that repeat themselves a lot or low-density areas (such as a blue sky) are not. Edges are good features as they tend to divide two regions of an image. A blob (an area of an image that greatly differs from its surrounding areas) is also an interesting feature.</p><p>Most feature detection algorithms revolve around the identification of corners, edges, and blobs, with some also focusing <a id="id318" class="indexterm"/>on the concept of a <span class="strong"><strong>ridge</strong></span>, which you can conceptualize as the symmetry axis of an elongated object (think, for example, about identifying a road in an image).</p><p>Some algorithms are better at identifying and extracting features of a certain type, so it's important to know what your input image is so that you can utilize the best tool in your OpenCV belt.</p><div class="section" title="Detecting features – corners"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec17"/>Detecting features – corners</h3></div></div></div><p>Let's start by <a id="id319" class="indexterm"/>identifying corners by utilizing <code class="literal">CornerHarris</code>, and let's do this with an example. If you continue studying OpenCV beyond this book, you'll find that—for many a reason—chessboards are a common subject of analysis in computer vision, partly because a chequered pattern is suited to many types of feature detections, and maybe because chess is pretty popular among geeks.</p><p>Here's our sample image:</p><div class="mediaobject"><img src="../Images/image00216.jpeg" alt="Detecting features – corners"/></div><p style="clear:both; height: 1em;"> </p><p>OpenCV has a very handy utility function called <code class="literal">cornerHarris</code>, which detects corners in an image. The code <a id="id320" class="indexterm"/>to illustrate this is incredibly simple:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('images/chess_board.png')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = np.float32(gray)
dst = cv2.cornerHarris(gray, 2, 23, 0.04)
img[dst&gt;0.01 * dst.max()] = [0, 0, 255] 
while (True):
  cv2.imshow('corners', img)
  if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
    break
cv2.destroyAllWindows()</pre></div><p>Let's analyze the code: after the usual imports, we load the chessboard image and turn it to grayscale so that <code class="literal">cornerHarris</code> can compute it. Then, we call the <code class="literal">cornerHarris</code> function:</p><div class="informalexample"><pre class="programlisting">dst = cv2.cornerHarris(gray, 2, 23, 0.04)</pre></div><p>The most important parameter here is the third one, which defines the aperture of the Sobel operator. The Sobel operator performs the change detection in the rows and columns of an image to detect edges, and it does this using a kernel. The OpenCV <code class="literal">cornerHarris</code> function uses a Sobel operator whose aperture is defined by this parameter. In plain english, it defines how sensitive corner detection is. It must be between 3 and 31 and be an odd value. At value 3, all those diagonal lines in the black squares of the chessboard will register as corners when they touch the border of the square. At 23, only the corners of each square will be detected as corners.</p><p>Consider the following line:</p><div class="informalexample"><pre class="programlisting">img[dst&gt;0.01 * dst.max()] = [0, 0, 255] </pre></div><p>Here, in places where a red mark in the corners is detected, tweaking the second parameter in <code class="literal">cornerHarris</code> will <a id="id321" class="indexterm"/>change this, that is, the smaller the value, the smaller the marks indicating corners.</p><p>Here's the final result:</p><div class="mediaobject"><img src="../Images/image00217.jpeg" alt="Detecting features – corners"/></div><p style="clear:both; height: 1em;"> </p><p>Great, we have corner points marked, and the result is meaningful at first glance; all the corners are marked in red.</p></div></div><div class="section" title="Feature extraction and description using DoG and SIFT"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec32"/>Feature extraction and description using DoG and SIFT</h2></div></div></div><p>The <a id="id322" class="indexterm"/>preceding technique, which uses <code class="literal">cornerHarris</code>, is <a id="id323" class="indexterm"/>great to detect corners and has a distinct advantage because corners are corners; they are detected even if the image is rotated.</p><p>However, if we reduce (or increase) the size of an image, some parts of the image may lose or even gain a corner quality.</p><p>For example, look at the following corner detections of the F1 Italian Grand Prix track:</p><div class="mediaobject"><img src="../Images/image00218.jpeg" alt="Feature extraction and description using DoG and SIFT"/></div><p style="clear:both; height: 1em;"> </p><p>Here's a smaller version of the same screenshot:</p><div class="mediaobject"><img src="../Images/image00219.jpeg" alt="Feature extraction and description using DoG and SIFT"/></div><p style="clear:both; height: 1em;"> </p><p>You will notice how corners are a lot more condensed; however, we didn't only gain corners, we also <a id="id324" class="indexterm"/>lost some! In particular, look at the <a id="id325" class="indexterm"/>
<span class="strong"><strong>Variante Ascari</strong></span> chicane that squiggles at the <a id="id326" class="indexterm"/>end of the NW/SE straight part of the track. In the larger version of the image, both the entrance and the apex of the double bend were detected as corners. In the smaller image, the apex is not detected as such. The more we reduce the image, the more likely it is that we're going to lose the entrance to that chicane too.</p><p>This loss of features raises an issue; we need an algorithm that will work regardless of the scale of the image. Enter <span class="strong"><strong>SIFT</strong></span>: while <span class="strong"><strong>Scale-Invariant Feature Transform</strong></span> may sound a bit <a id="id327" class="indexterm"/>mysterious, now that we know what problem we're trying to resolve, it actually makes sense. We need a function (a transform) that will detect features (a feature transform) and will not output different results depending on the scale of the image (a scale-invariant feature transform). Note that SIFT does not detect keypoints (which is done with Difference of Gaussians), but it describes the region surrounding them by means of a feature vector.</p><p>A quick introduction to <span class="strong"><strong>Difference of Gaussians</strong></span> (<span class="strong"><strong>DoG</strong></span>) is in order; we have already talked about <a id="id328" class="indexterm"/>low pass filters and blurring operations, specifically with the <code class="literal">cv2.GaussianBlur()</code> function. DoG is the result of different Gaussian filters applied to the same image. In <a class="link" title="Chapter 3. Processing Images with OpenCV 3" href="part0023.xhtml#aid-LTSU1">Chapter 3</a>, <span class="emphasis"><em>Processing Images with OpenCV 3</em></span>, we applied this technique to compute a very efficient edge detection, and the idea is the same. The final result of a DoG operation contains areas of interest (keypoints), which are then going to be described through SIFT.</p><p>Let's see how SIFT behaves in an image full of corners and features:</p><div class="mediaobject"><img src="../Images/image00220.jpeg" alt="Feature extraction and description using DoG and SIFT"/></div><p style="clear:both; height: 1em;"> </p><p>Now, the beautiful <a id="id329" class="indexterm"/>panorama of Varese (Lombardy, Italy) also <a id="id330" class="indexterm"/>gains a computer vision meaning. Here's the code used to obtain this processed image:</p><div class="informalexample"><pre class="programlisting">import cv2
import sys
import numpy as np

imgpath = sys.argv[1]
img = cv2.imread(imgpath)

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

sift = cv2.xfeatures2d.SIFT_create()
keypoints, descriptor = sift.detectAndCompute(gray,None)

img = cv2.drawKeypoints(image=img, outImage=img, keypoints = keypoints, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINT, color = (51, 163, 236))

cv2.imshow('sift_keypoints', img)
while (True):
  if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
    break
cv2.destroyAllWindows()</pre></div><p>After the usual imports, we load the image that we want to process. To make this script generic, we will <a id="id331" class="indexterm"/>take the image path as a command-line argument using the <code class="literal">sys</code> module of Python:</p><div class="informalexample"><pre class="programlisting">imgpath = sys.argv[1]
img = cv2.imread(imgpath)

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</pre></div><p>We then turn the <a id="id332" class="indexterm"/>image into grayscale. At this stage, you may have gathered that most processing algorithms in Python need a grayscale feed in order to work.</p><p>The next step is to create a SIFT object and compute the grayscale image:</p><div class="informalexample"><pre class="programlisting">sift = cv2.xfeatures2d.SIFT_create()
keypoints, descriptor = sift.detectAndCompute(gray,None)</pre></div><p>This is an interesting and important process; the SIFT object uses DoG to detect keypoints and computes a feature vector for the surrounding regions of each keypoint. As the name of the method clearly gives away, there are two main operations performed: detection and computation. The return value of the operation is a tuple containing keypoint information (keypoints) and the descriptor.</p><p>Finally, we process this image by drawing the keypoints on it and displaying it with the usual <code class="literal">imshow</code> function.</p><p>Note that in the <code class="literal">drawKeypoints</code> function, we pass a flag that has a value of 4. This is actually the <code class="literal">cv2</code> module property:</p><div class="informalexample"><pre class="programlisting">cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINT</pre></div><p>This code enables the drawing of circles and orientation of each keypoint.</p><div class="section" title="Anatomy of a keypoint"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec18"/>Anatomy of a keypoint</h3></div></div></div><p>Let's take a quick <a id="id333" class="indexterm"/>look at the definition, from the OpenCV documentation, of the keypoint class:</p><div class="informalexample"><pre class="programlisting">pt
size
angle
response
octave 
class_id</pre></div><p>Some properties are more self-explanatory than others, but let's not take anything for granted and go through each one:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The <code class="literal">pt</code> (point) property indicates the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> coordinates of the keypoint in the image.</li><li class="listitem">The <code class="literal">size</code> property indicates the diameter of the feature.</li><li class="listitem">The <code class="literal">angle</code> property indicates the orientation of the feature as shown in the preceding processed image.</li><li class="listitem">The <code class="literal">response</code> property indicates the strength of the keypoint. Some features are classified by SIFT as stronger than others, and <code class="literal">response</code> is the property you would check to evaluate the strength of a feature.</li><li class="listitem">The <code class="literal">octave</code> property indicates the layer in the pyramid where the feature was found. To <a id="id334" class="indexterm"/>fully explain this property, we would need to write an entire chapter on it, so I will only introduce the basic concept. The SIFT algorithm operates in a similar fashion to face detection algorithms in that, it processes the same image sequentially but alters the parameters of the computation.<p>For example, the scale of the image and neighboring pixels are parameters that change at each iteration (<code class="literal">octave</code>) of the algorithm. So, the <code class="literal">octave</code> property indicates the layer at which the keypoint was detected.</p></li><li class="listitem">Finally, the object ID is the ID of the keypoint.</li></ul></div></div></div><div class="section" title="Feature extraction and detection using Fast Hessian and SURF"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec33"/>Feature extraction and detection using Fast Hessian and SURF</h2></div></div></div><p>Computer <a id="id335" class="indexterm"/>vision is a relatively recent <a id="id336" class="indexterm"/>branch of computer science and many algorithms and techniques are of recent invention. SIFT is in fact only 16 years old, having been published by David Lowe in 1999.</p><p>SURF is a feature detection algorithm published in 2006 by Herbert Bay, which is several times faster than SIFT, and it is partially inspired by it.</p><div class="note" title="Note"><h3 class="title"><a id="note22"/>Note</h3><p>Note that both SIFT and SURF are patented algorithms and, for this reason, are made available in the <code class="literal">xfeatures2d</code> module of OpenCV.</p></div><p>It is not particularly relevant to this book to understand how SURF works under the hood, as much as we can use it in our applications and make the best of it. What is important to understand is that SURF is an OpenCV class that operates keypoint detection with the Fast Hessian algorithm and extraction with SURF, much like the SIFT class in OpenCV operating keypoint detection with DoG and extraction with SIFT.</p><p>Also, the good news is that as a feature detection algorithm, the API of SURF does not differ from SIFT. Therefore, we can simply edit the previous script to dynamically choose a feature detection algorithm instead of rewriting the entire program.</p><p>As we only support two algorithms for now, there is no need to find a particularly elegant solution to the evaluation of the algorithm to be used and we'll use the simple <code class="literal">if</code> blocks, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">import cv2
import sys
import numpy as np

imgpath = sys.argv[1]
img = cv2.imread(imgpath)
alg = sys.argv[2]

def fd(algorithm):
  if algorithm == "SIFT":
    return cv2.xfeatures2d.SIFT_create()
  if algorithm == "SURF":
    return cv2.xfeatures2d.SURF_create(float(sys.argv[3]) if len(sys.argv) == 4 else 4000)

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

fd_alg = fd(alg)
keypoints, descriptor = fd_alg.detectAndCompute(gray,None)

img = cv2.drawKeypoints(image=img, outImage=img, keypoints = keypoints, flags = 4, color = (51, 163, 236))

cv2.imshow('keypoints', img)
while (True):
  if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
    break
cv2.destroyAllWindows()</pre></div><p>Here's the <a id="id337" class="indexterm"/>result using SURF with a <a id="id338" class="indexterm"/>threshold:</p><div class="mediaobject"><img src="../Images/image00221.jpeg" alt="Feature extraction and detection using Fast Hessian and SURF"/></div><p style="clear:both; height: 1em;"> </p><p>This image has been obtained by processing it with a SURF algorithm using a Hessian threshold of 8000. To be precise, I ran the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; python feat_det.py images/varese.jpg SURF 8000</strong></span>
</pre></div><p>The higher the <a id="id339" class="indexterm"/>threshold, the less features <a id="id340" class="indexterm"/>identified, so play around with the values until you reach an optimal detection. In the preceding case, you can clearly see how individual buildings are detected as features.</p><p>In a process similar to the one we adopted in <a class="link" title="Chapter 4. Depth Estimation and Segmentation" href="part0036.xhtml#aid-12AK81">Chapter 4</a>, <span class="emphasis"><em>Depth Estimation and Segmentation</em></span>, when we were calculating disparity maps, try—as an exercise—to create a trackbar to feed the value of the Hessian threshold to the SURF instance, and see the number of features increase and decrease in an inversely proportional fashion.</p><p>Now, let's examine corner detection with FAST, the BRIEF keypoint descriptor, and ORB (which uses the two) and put feature detection to good use.</p></div><div class="section" title="ORB feature detection and feature matching"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec34"/>ORB feature detection and feature matching</h2></div></div></div><p>If SIFT is <a id="id341" class="indexterm"/>young, and SURF younger, ORB <a id="id342" class="indexterm"/>is in its infancy. ORB was first published in 2011 as a fast alternative to SIFT and SURF.</p><p>The algorithm was published in the paper, <span class="emphasis"><em>ORB: an efficient alternative to SIFT or SURF</em></span>, and is available in the PDF format at <a class="ulink" href="http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf">http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf</a>.</p><p>ORB mixes techniques used in the FAST keypoint detection and the BRIEF descriptor, so it is definitely worth taking a quick look at FAST and BRIEF first. We will then talk about Brute-Force <a id="id343" class="indexterm"/>matching—one <a id="id344" class="indexterm"/>of the algorithms used for feature matching—and show an example of feature matching.</p><div class="section" title="FAST"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec19"/>FAST</h3></div></div></div><p>The <a id="id345" class="indexterm"/>
<span class="strong"><strong>Features from Accelerated Segment Test</strong></span> (<span class="strong"><strong>FAST</strong></span>) algorithm works in a clever <a id="id346" class="indexterm"/>way; it draws a circle around including 16 pixels. It then marks each pixel brighter or darker than a particular threshold compared to the center of the circle. A corner is defined by identifying a number of contiguous pixels marked as brighter or darker.</p><p>FAST implements a high-speed test, which attempts at quickly skipping the whole 16-pixel test. To understand how this test works, let's take a look at this screenshot:</p><div class="mediaobject"><img src="../Images/image00222.jpeg" alt="FAST"/></div><p style="clear:both; height: 1em;"> </p><p>As you can see, three out of four of the test pixels (pixels number <span class="strong"><strong>1</strong></span>, <span class="strong"><strong>9</strong></span>, <span class="strong"><strong>5</strong></span>, and <span class="strong"><strong>13</strong></span>) must be within (or beyond) the threshold (and, therefore, marked as brighter or darker) and one must be in the opposite side of the threshold. If all four are marked as brighter or darker, or two are and two are not, the pixel is not a candidate corner.</p><p>FAST is <a id="id347" class="indexterm"/>an incredibly clever algorithm, but not devoid of weaknesses, and to compensate these weaknesses, developers analyzing images can implement a machine learning approach, feeding a set of images (relevant to your application) to the algorithm so that corner detection is optimized.</p><p>Despite this, FAST depends on a threshold, so the developer's input is always necessary (unlike SIFT).</p></div><div class="section" title="BRIEF"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec20"/>BRIEF</h3></div></div></div><p>
<span class="strong"><strong>Binary </strong></span>
<a id="id348" class="indexterm"/>
<span class="strong"><strong>Robust Independent Elementary Features</strong></span> (<span class="strong"><strong>BRIEF</strong></span>), on the other hand, is not a feature detection algorithm, but a descriptor. We have not yet explored this concept, so let's explain what a descriptor is, and then look at BRIEF.</p><p>You <a id="id349" class="indexterm"/>will notice that when we previously analyzed images with SIFT and SURF, the heart of the entire process is the call to the <code class="literal">detectAndCompute</code> function. This function operates two different steps: detection and computation, and they return two different results if coupled in a tuple.</p><p>The result of detection is a set of keypoints; the result of the computation is the descriptor. This means that the OpenCV's SIFT and SURF classes are both detectors and descriptors (although, remember that the original algorithms are not! OpenCV's SIFT is really DoG plus SIFT and OpenCV's SURF is really Fast Hessian plus SURF).</p><p>Keypoint descriptors are a representation of the image that serves as the gateway to feature matching because you can compare the keypoint descriptors of two images and find commonalities.</p><p>BRIEF is one of the fastest descriptors currently available. The theory behind BRIEF is actually quite complicated, but suffice to say that BRIEF adopts a series of optimizations that make it a very good choice for feature matching.</p></div><div class="section" title="Brute-Force matching"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec21"/>Brute-Force matching</h3></div></div></div><p>A <a id="id350" class="indexterm"/>Brute-Force matcher is a descriptor matcher that compares two descriptors and generates a result that is a list of matches. The <a id="id351" class="indexterm"/>reason why it's called Brute-Force is that there is little optimization involved in the algorithm; all the features in the first descriptor are compared to the features in the second descriptor, and each comparison is given a distance value and the best result is considered a match.</p><p>This is why it's called Brute-Force. In computing, the term, <span class="strong"><strong>brute-force</strong></span>, is often associated with <a id="id352" class="indexterm"/>an approach that prioritizes the exhaustion of all possible combinations (for example, all the possible combinations of <a id="id353" class="indexterm"/>characters to crack a password) over some clever and convoluted algorithmical logic. OpenCV provides a <code class="literal">BFMatcher</code> object that does just that.</p></div></div><div class="section" title="Feature matching with ORB"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec35"/>Feature matching with ORB</h2></div></div></div><p>Now that <a id="id354" class="indexterm"/>we have a general idea of what FAST and BRIEF are, we can understand why the team behind ORB (at the time composed by Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski) chose these two algorithms as a foundation for ORB.</p><p>In their paper, the authors aim at achieving the following results:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The addition of a fast and accurate orientation component to FAST</li><li class="listitem">The efficient computation of oriented BRIEF features</li><li class="listitem">Analysis of variance and correlation of oriented BRIEF features</li><li class="listitem">A learning method to decorrelate BRIEF features under rotational invariance, leading to better performance in nearest-neighbor applications.</li></ul></div><p>Aside from very technical jargon, the main points are quite clear; ORB aims at optimizing and speeding up operations, including the very important step of utilizing BRIEF in a rotation-aware fashion so that matching is improved even in situations where a training image has a very different rotation to the query image.</p><p>At this stage, though, I bet you have had enough of the theory and want to sink your teeth in some feature matching, so let's go look at some code.</p><p>As an avid listener of music, the first example that comes to my mind is to get the logo of a band and match it to one of the band's albums:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt

img1 = cv2.imread('images/manowar_logo.png',cv2.IMREAD_GRAYSCALE)
img2 = cv2.imread('images/manowar_single.jpg', cv2.IMREAD_GRAYSCALE)

orb = cv2.ORB_create()
kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(des1,des2)
matches = sorted(matches, key = lambda x:x.distance)
img3 = cv2.drawMatches(img1,kp1,img2,kp2, matches[:40], img2,flags=2)
plt.imshow(img3),plt.show()</pre></div><p>Let's now examine this code step by step.</p><p>After the <a id="id355" class="indexterm"/>usual imports, we load two images (the query image and the training image).</p><p>Note that you have probably seen the loading of images with a second parameter with the value of 0 being passed. This is because <code class="literal">cv2.imread</code> takes a second parameter that can be one of the following flags:</p><div class="informalexample"><pre class="programlisting">    IMREAD_ANYCOLOR = 4
    IMREAD_ANYDEPTH = 2
    IMREAD_COLOR = 1
    IMREAD_GRAYSCALE = 0
    IMREAD_LOAD_GDAL = 8
    IMREAD_UNCHANGED = -1</pre></div><p>As you can see, <code class="literal">cv2.IMREAD_GRAYSCALE</code> is equal to <code class="literal">0</code>, so you can pass the flag itself or its value; they are the same thing.</p><p>This is the image we've loaded:</p><div class="mediaobject"><img src="../Images/image00223.jpeg" alt="Feature matching with ORB"/></div><p style="clear:both; height: 1em;"> </p><p>This is another image that we've loaded:</p><div class="mediaobject"><img src="../Images/image00224.jpeg" alt="Feature matching with ORB"/></div><p style="clear:both; height: 1em;"> </p><p>Now, we <a id="id356" class="indexterm"/>proceed to creating the ORB feature detector and descriptor:</p><div class="informalexample"><pre class="programlisting">orb = cv2.ORB_create()
kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)</pre></div><p>In a similar fashion to what we did with SIFT and SURF, we detect and compute the keypoints and descriptors for both images.</p><p>The theory at this point is pretty simple; iterate through the descriptors and determine whether they are a match or not, and then calculate the quality of this match (distance) and sort the matches so that we can display the top <span class="emphasis"><em>n</em></span> matches with a degree of confidence that they are, in fact, matching features on both images.</p><p>
<code class="literal">BFMatcher</code>, as described in Brute-Force matching, does this for us:</p><div class="informalexample"><pre class="programlisting">bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(des1,des2)
matches = sorted(matches, key = lambda x:x.distance)</pre></div><p>At this stage, we already have all the information we need, but as computer vision enthusiasts, we place quite a bit of importance on visually representing data, so let's draw these matches in a <code class="literal">matplotlib</code> chart:</p><div class="informalexample"><pre class="programlisting">img3 = cv2.drawMatches(img1,kp1,img2,kp2, matches[:40], img2,flags=2)
plt.imshow(img3),plt.show()</pre></div><p>The result is as follows:</p><div class="mediaobject"><img src="../Images/image00225.jpeg" alt="Feature matching with ORB"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Using K-Nearest Neighbors matching"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec36"/>Using K-Nearest Neighbors matching</h2></div></div></div><p>There <a id="id357" class="indexterm"/>are a number of algorithms that can be used to detect matches so that we can draw them. One of them is <span class="strong"><strong>K-Nearest Neighbors</strong></span> (<span class="strong"><strong>KNN</strong></span>). Using different algorithms for different tasks can be really <a id="id358" class="indexterm"/>beneficial, because each algorithm has strengths and weaknesses. Some may be more accurate than others, some may be faster or less computationally expensive, so it's up to you to decide which one to use, depending on the task at hand.</p><p>For example, if you have hardware constraints, you may choose an algorithm that is less costly. If you're developing a real-time application, you may choose the fastest algorithm, regardless of how heavy it is on the processor or memory usage.</p><p>Among all the machine learning algorithms, KNN is probably the simplest, and while the theory behind it is interesting, it is well out of the scope of this book. Instead, we will simply show you how to use KNN in your application, which is not very different from the preceding example.</p><p>Crucially, the two points where the script differs to switch to KNN are in the way we calculate matches with the Brute-Force matcher, and the way we draw these matches. The preceding example, which has been edited to use KNN, looks like this:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt

img1 = cv2.imread('images/manowar_logo.png',0)
img2 = cv2.imread('images/manowar_single.jpg',0)

orb = cv2.ORB_create()
kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.knnMatch(des1,des2, k=2)
img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2, matches, img2,flags=2)
plt.imshow(img3),plt.show()</pre></div><p>The final result is somewhat similar to the previous one, so what is the difference between <code class="literal">match</code> and <code class="literal">knnMatch</code>? The difference is that <code class="literal">match</code> returns best matches, while KNN returns <span class="emphasis"><em>k</em></span> matches, giving the developer the option to further manipulate the matches obtained with <code class="literal">knnMatch</code>.</p><p>For <a id="id359" class="indexterm"/>example, you could iterate <a id="id360" class="indexterm"/>through the matches and apply a ratio test so that you can filter out matches that do not satisfy a user-defined condition.</p></div><div class="section" title="FLANN-based matching"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec37"/>FLANN-based matching</h2></div></div></div><p>Finally, we <a id="id361" class="indexterm"/>are going to take a look at <span class="strong"><strong>Fast Library for Approximate Nearest Neighbors</strong></span> (<span class="strong"><strong>FLANN</strong></span>). The official Internet home of FLANN is at <a class="ulink" href="http://www.cs.ubc.ca/research/flann/">http://www.cs.ubc.ca/research/flann/</a>.</p><p>Like <a id="id362" class="indexterm"/>ORB, FLANN has a more permissive license than SIFT or SURF, so you can freely use it in your project. Quoting the website of FLANN,</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"FLANN is a library for performing fast approximate nearest neighbor searches in high dimensional spaces. It contains a collection of algorithms we found to work best for nearest neighbor search and a system for automatically choosing the best algorithm and optimum parameters depending on the dataset.</em></span></p><p><span class="emphasis"><em>FLANN is written in C++ and contains bindings for the following languages: C, MATLAB and Python."</em></span></p></blockquote></div><p>In other words, FLANN possesses an internal mechanism that attempts at employing the best algorithm to process a dataset depending on the data itself. FLANN has been proven to be 10 times times faster than other nearest neighbors search software.</p><p>FLANN is even available on GitHub at <a class="ulink" href="https://github.com/mariusmuja/flann">https://github.com/mariusmuja/flann</a>. In my experience, I've found FLANN-based matching to be very accurate and fast as well as friendly to use.</p><p>Let's look <a id="id363" class="indexterm"/>at an example of FLANN-based feature matching:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt

queryImage = cv2.imread('images/bathory_album.jpg',0)
trainingImage = cv2.imread('images/vinyls.jpg',0)

# create SIFT and detect/compute
sift = cv2.xfeatures2d.SIFT_create()
kp1, des1 = sift.detectAndCompute(queryImage,None)
kp2, des2 = sift.detectAndCompute(trainingImage,None)

# FLANN matcher parameters
FLANN_INDEX_KDTREE = 0
indexParams = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
searchParams = dict(checks=50)   # or pass empty dictionary

flann = cv2.FlannBasedMatcher(indexParams,searchParams)

matches = flann.knnMatch(des1,des2,k=2)

# prepare an empty mask to draw good matches
matchesMask = [[0,0] for i in xrange(len(matches))]

# David G. Lowe's ratio test, populate the mask
for i,(m,n) in enumerate(matches):
    if m.distance &lt; 0.7*n.distance:
        matchesMask[i]=[1,0]

drawParams = dict(matchColor = (0,255,0),
                   singlePointColor = (255,0,0),
                   matchesMask = matchesMask,
                   flags = 0)

resultImage = cv2.drawMatchesKnn(queryImage,kp1,trainingImage,kp2,matches,None,**drawParams)

plt.imshow(resultImage,), plt.show()</pre></div><p>Some parts of the preceding script will be familiar to you at this stage (import of modules, image <a id="id364" class="indexterm"/>loading, and creation of a SIFT object).</p><div class="note" title="Note"><h3 class="title"><a id="note23"/>Note</h3><p>The interesting part is the declaration of the FLANN matcher, which follows the documentation at <a class="ulink" href="http://www.cs.ubc.ca/~mariusm/uploads/FLANN/flann_manual-1.6.pdf">http://www.cs.ubc.ca/~mariusm/uploads/FLANN/flann_manual-1.6.pdf</a>.</p></div><p>We find that the FLANN matcher takes two parameters: an <code class="literal">indexParams</code> object and a <code class="literal">searchParams</code> object. These parameters, passed in a dictionary form in Python (and a struct in C++), determine the behavior of the index and search objects used internally by FLANN to compute the matches.</p><p>In this <a id="id365" class="indexterm"/>case, we could have chosen between <code class="literal">LinearIndex</code>, <code class="literal">KTreeIndex</code>, <code class="literal">KMeansIndex</code>, <code class="literal">CompositeIndex</code>, and <code class="literal">AutotuneIndex</code>, and we chose <code class="literal">KTreeIndex</code>. Why? This is because it's a simple enough index to configure (only requires the user to specify the number of kernel density trees to be processed; a good value is between 1 and 16) and clever enough (the kd-trees are processed in parallel). The <code class="literal">searchParams</code> dictionary only contains one field (checks) that specifies the number of times an index tree should be traversed. The higher the value, the longer it takes to compute the matching, but it will also be more accurate.</p><p>In reality, a lot depends on the input that you feed the program with. I've found that 5 kd-trees and 50 checks always yield a respectably accurate result, while only taking a short time to complete.</p><p>After the creation of the FLANN matcher and having created the matches array, matches are then filtered according to the test described by Lowe in his paper, <span class="emphasis"><em>Distinctive Image Features from Scale-Invariant Keypoints</em></span>, available at <a class="ulink" href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</a>.</p><p>In its chapter, <span class="emphasis"><em>Application to object recognition</em></span>, Lowe explains that not all matches are "good" ones, and that filtering according to an arbitrary threshold would not yield good results all the time. Instead, Dr. Lowe explains,</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"The probability that a match is correct can be determined by taking the ratio of distance from the closest neighbor to the distance of the second closest."</em></span></p></blockquote></div><p>In the preceding example, discarding any value with a distance greater than 0.7 will result in just a few good matches being filtered out, while getting rid of around 90 percent of false matches.</p><p>Let's unveil the <a id="id366" class="indexterm"/>result of a practical example of FLANN. This is the query image that I've fed the script:</p><div class="mediaobject"><img src="../Images/image00226.jpeg" alt="FLANN-based matching"/></div><p style="clear:both; height: 1em;"> </p><p>This is the training image:</p><div class="mediaobject"><img src="../Images/image00227.jpeg" alt="FLANN-based matching"/></div><p style="clear:both; height: 1em;"> </p><p>Here, you <a id="id367" class="indexterm"/>may notice that the image contains the query image at position (5, 3) of this grid.</p><p>This is the <a id="id368" class="indexterm"/>FLANN processed result:</p><div class="mediaobject"><img src="../Images/image00228.jpeg" alt="FLANN-based matching"/></div><p style="clear:both; height: 1em;"> </p><p>A perfect <a id="id369" class="indexterm"/>match!!</p></div><div class="section" title="FLANN matching with homography"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec38"/>FLANN matching with homography</h2></div></div></div><p>First <a id="id370" class="indexterm"/>of all, what is <a id="id371" class="indexterm"/>homography? Let's read a definition from the Internet:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"A relation between two figures, such that to any point of the one corresponds one and but one point in the other, and vise versa. Thus, a tangent line rolling on a circle cuts two fixed tangents of the circle in two sets of points that are homographic."</em></span></p></blockquote></div><p>If you—like me—are none the wiser from the preceding definition, you will probably find this explanation a bit clearer: homography is a condition in which two figures find each other when one is a perspective distortion of the other.</p><p>Unlike all the previous examples, let's first take a look at what we want to achieve so that we can fully understand what homography is. Then, we'll go through the code. Here's the final result:</p><div class="mediaobject"><img src="../Images/image00229.jpeg" alt="FLANN matching with homography"/></div><p style="clear:both; height: 1em;"> </p><p>As you can see from the screenshot, we took a subject on the left, correctly identified in the image on the right-hand side, drew matching lines between keypoints, and even drew a white <a id="id372" class="indexterm"/>border showing the perspective deformation of the seed subject in the right-hand side of the image:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt

MIN_MATCH_COUNT = 10

img1 = cv2.imread('images/bb.jpg',0)
img2 = cv2.imread('images/color2_small.jpg',0)

sift = cv2.xfeatures2d.SIFT_create()
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)

FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)

flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(des1,des2,k=2)

# store all the good matches as per Lowe's ratio test.
good = []
for m,n in matches:
    if m.distance &lt; 0.7*n.distance:
        good.append(m)

if len(good)&gt;MIN_MATCH_COUNT:
    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)

    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)
    matchesMask = mask.ravel().tolist()

    h,w = img1.shape
    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
    dst = cv2.perspectiveTransform(pts,M)

    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)

else:
    print "Not enough matches are found - %d/%d" % (len(good),MIN_MATCH_COUNT)
    matchesMask = None

draw_params = dict(matchColor = (0,255,0), # draw matches in green color
                   singlePointColor = None,
                   matchesMask = matchesMask, # draw only inliers
                   flags = 2)

img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)

plt.imshow(img3, 'gray'),plt.show()</pre></div><p>When compared to the previous FLANN-based matching example, the only difference (and this is where all the action happens) is in the <code class="literal">if</code> block.</p><p>Here's <a id="id373" class="indexterm"/>what happens in this code step by step: firstly, we make sure that we have at least a certain number of <a id="id374" class="indexterm"/>good matches (the minimum required to compute a homography is four), which we will arbitrarily set at 10 (in real life, you would probably use a higher value than this):</p><div class="informalexample"><pre class="programlisting">if len(good)&gt;MIN_MATCH_COUNT:</pre></div><p>Then, we find the keypoints in the original image and the training image:</p><div class="informalexample"><pre class="programlisting">src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)</pre></div><p>Now, we find the homography:</p><div class="informalexample"><pre class="programlisting">M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)
matchesMask = mask.ravel().tolist()</pre></div><p>Note that <a id="id375" class="indexterm"/>we create <code class="literal">matchesMask</code>, which will be used in the final drawing of the matches so that only points lying within the homography will have matching lines drawn.</p><p>At this stage, we simply have to calculate the perspective distortion of the original object into the second picture so that we can draw the border:</p><div class="informalexample"><pre class="programlisting">h,w = img1.shape
pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
dst = cv2.perspectiveTransform(pts,M)
img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)</pre></div><p>And we then <a id="id376" class="indexterm"/>proceed to draw as per all our previous examples.</p></div><div class="section" title="A sample application – tattoo forensics"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec39"/>A sample application – tattoo forensics</h2></div></div></div><p>Let's conclude this chapter with a real-life (or kind of) example. Imagine that you're working for the Gotham forensics department and you need to identify a tattoo. You have the original picture of the tattoo (imagine this coming from a CCTV footage) belonging to a criminal, but you don't know the identity of the person. However, you possess a database of tattoos, indexed with the name of the person to whom the tattoo belongs.</p><p>So, let's divide the task in two parts: save image descriptors to files first, and then, scan these for matches against the picture we are using as a query image.</p><div class="section" title="Saving image descriptors to file"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec22"/>Saving image descriptors to file</h3></div></div></div><p>The first <a id="id377" class="indexterm"/>thing we will do is save image descriptors to an external file. This is so that we don't have to recreate the descriptors every time we want to scan two images for matches and homography.</p><p>In our application, we will scan a folder for images and create the corresponding descriptor files so that we have them readily available for future searches.</p><p>To create descriptors and save them to a file, we will use a process we have used a number of times in this chapter, namely load an image, create a feature detector, detect, and compute:</p><div class="informalexample"><pre class="programlisting"># generate_descriptors.py
import cv2
import numpy as np
from os import walk
from os.path import join
import sys

def create_descriptors(folder):
  files = []
  for (dirpath, dirnames, filenames) in walk(folder):
    files.extend(filenames)
  for f in files:
    save_descriptor(folder, f, cv2.xfeatures2d.SIFT_create())

def save_descriptor(folder, image_path, feature_detector):
  img = cv2.imread(join(folder, image_path), 0)
  keypoints, descriptors = feature_detector.detectAndCompute(img, None)
  descriptor_file = image_path.replace("jpg", "npy")
  np.save(join(folder, descriptor_file), descriptors)

dir = sys.argv[1]

create_descriptors(dir)</pre></div><p>In this script, we pass the folder name where all our images are contained, and then create all the descriptor files in the same folder.</p><p>NumPy has a <a id="id378" class="indexterm"/>very handy <code class="literal">save()</code> utility, which dumps array data into a file in an optimized way. To generate the descriptors in the folder containing your script, run this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; python generate_descriptors.py &lt;folder containing images&gt;</strong></span>
</pre></div><p>Note that <code class="literal">cPickle/pickle</code> are more popular libraries for Python object serialization. However, in this particular context, we are trying to limit ourselves to the usage of OpenCV and Python with NumPy and SciPy.</p></div><div class="section" title="Scanning for matches"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec23"/>Scanning for matches</h3></div></div></div><p>Now that we have <a id="id379" class="indexterm"/>descriptors saved to files, all we need to do is to repeat the homography process on all the descriptors and find a potential match to our query image.</p><p>This is the process we will put in place:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Loading a query image and creating a descriptor for it (<code class="literal">tattoo_seed.jpg</code>)</li><li class="listitem">Scanning the folder with descriptors</li><li class="listitem">For each descriptor, computing a FLANN-based match</li><li class="listitem">If the number of matches is beyond an arbitrary threshold, including the file of potential culprits (remember we're investigating a crime)</li><li class="listitem">Of all the culprits, electing the one with the highest number of matches as the potential suspect</li></ul></div><p>Let's inspect the <a id="id380" class="indexterm"/>code to achieve this:</p><div class="informalexample"><pre class="programlisting">from os.path import join
from os import walk
import numpy as np
import cv2
from sys import argv

# create an array of filenames
folder = argv[1]
query = cv2.imread(join(folder, "tattoo_seed.jpg"), 0)

# create files, images, descriptors globals
files = []
images = []
descriptors = []
for (dirpath, dirnames, filenames) in walk(folder):
  files.extend(filenames)
  for f in files:
    if f.endswith("npy") and f != "tattoo_seed.npy":
      descriptors.append(f)
  print descriptors

# create the sift detector
sift = cv2.xfeatures2d.SIFT_create()
query_kp, query_ds = sift.detectAndCompute(query, None)

# create FLANN matcher
FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)
flann = cv2.FlannBasedMatcher(index_params, search_params)

# minimum number of matches
MIN_MATCH_COUNT = 10

potential_culprits = {}

print "&gt;&gt; Initiating picture scan..."
for d in descriptors:
  print "--------- analyzing %s for matches ------------" % d
  matches = flann.knnMatch(query_ds, np.load(join(folder, d)), k =2)
  good = []
  for m,n in matches:
      if m.distance &lt; 0.7*n.distance:
          good.append(m)
  if len(good) &gt; MIN_MATCH_COUNT:
    print "%s is a match! (%d)" % (d, len(good))
  else:
    print "%s is not a match" % d
  potential_culprits[d] = len(good)

max_matches = None
potential_suspect = None
for culprit, matches in potential_culprits.iteritems():
  if max_matches == None or matches &gt; max_matches:
    max_matches = matches
    potential_suspect = culprit

print "potential suspect is %s" % potential_suspect.replace("npy", "").upper()</pre></div><p>I saved this <a id="id381" class="indexterm"/>script as <code class="literal">scan_for_matches.py</code>. The only element of novelty in this script is the use of <code class="literal">numpy.load(filename)</code>, which loads an <code class="literal">npy</code> file into an <code class="literal">np</code> array.</p><p>Running the script produces the following output:</p><div class="informalexample"><pre class="programlisting">&gt;&gt; Initiating picture scan...
--------- analyzing posion-ivy.npy for matches ------------
posion-ivy.npy is not a match
--------- analyzing bane.npy for matches ------------
bane.npy is not a match
--------- analyzing two-face.npy for matches ------------
two-face.npy is not a match
--------- analyzing riddler.npy for matches ------------
riddler.npy is not a match
--------- analyzing penguin.npy for matches ------------
penguin.npy is not a match
--------- analyzing dr-hurt.npy for matches ------------
dr-hurt.npy is a match! (298)
--------- analyzing hush.npy for matches ------------
hush.npy is a match! (301)
potential suspect is HUSH.</pre></div><p>If we were to <a id="id382" class="indexterm"/>represent this graphically, this is what we would see:</p><div class="mediaobject"><img src="../Images/image00230.jpeg" alt="Scanning for matches"/></div><p style="clear:both; height: 1em;"> </p></div></div></div></div>
<div class="section" title="Summary" id="aid-1DOR01"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Summary</h1></div></div></div><p>In this chapter, we learned about detecting features in images and extracting them into descriptors. We explored a number of algorithms available in OpenCV to accomplish this task, and then applied them to real-life scenarios to understand a real-world application of the concepts we explored.</p><p>We are now familiar with the concept of detecting features in an image (or a video frame), which is a good foundation for the next chapter.</p></div></body></html>