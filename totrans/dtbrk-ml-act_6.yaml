- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Searching for a Signal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover how to use data science to search for a signal
    hidden in the noise of data.
  prefs: []
  type: TYPE_NORMAL
- en: We will leverage the features we created within the Databricks platform during
    the previous chapter. We start by using **automated machine learning** (**AutoML**)
    for a basic modeling approach, which provides autogenerated code and quickly enables
    data scientists to establish a baseline model to beat. When searching for a signal,
    we experiment with different features, hyperparameters, and models. Historically,
    tracking these configurations and their corresponding evaluation metrics is a
    time-consuming project in and of itself. A low-overhead tracking mechanism, such
    as the tracking provided by MLflow, an open source platform for managing data
    science projects and supporting **ML operations** (**MLOps**) will reduce the
    burden of manually capturing configurations. More specifically, we’ll introduce
    MLflow Tracking, an MLflow component that significantly improves tracking each
    permutation’s many outputs. However, that is only the beginning. As data science
    teams are being pressed to leverage **generative artificial intelligence** (**GenAI**),
    we will also showcase how to leverage a **large language model** (**LLM**) to
    create a SQL bot and a **deep learning** (**DL**) model utilizing PyTorch. These
    examples demonstrate that in addition to building our own solutions, we can integrate
    external innovative solutions into our workflows. This openness allows us to pick
    from the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn as part of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Baselining with AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying beyond the basic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the technical requirements needed to complete the hands-on examples
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks ML Runtime:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pandas`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sklearn`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Torch`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For our LLM model, we will integrate with the **ChatGPT** model from **OpenAI**
    ([https://openai.com/](https://openai.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the **PyTorch Lightning AI** Python package ([https://www.pytorchlightning.ai/index.html](https://www.pytorchlightning.ai/index.html))
    and **TorchMetrics** ([https://torchmetrics.readthedocs.io/en/stable/](https://torchmetrics.readthedocs.io/en/stable/))
    while building the classification model for the Parkinson’s **Freezing of Gait**
    (**FOG**) problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baselining with AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A baseline model is a simple model used as a starting point for ML. Data scientists
    often use a baseline model to compare the performance of more complex models.
    Baseline models are typically simple or common algorithms, such as the majority
    class classifier or a random forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baseline models are valuable for several reasons, some of which are listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: They can help you understand the difficulty of finding a signal given your current
    dataset. If even the best baseline model performs poorly, it may indicate that
    more complex models will also struggle to find useful patterns (that is, garbage
    data in, garbage models out).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baseline models can help you to identify features that are most important for
    the ML task. If a baseline model performs well, it may be because it can learn
    from the most salient features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baseline models can help you avoid overfitting. Overfitting is a frequent problem
    with more complex models. It occurs when a model learns the training data too
    well and cannot generalize to new data. You can determine whether the more complex
    model is overfitting by comparing its performance to the baseline model. If the
    complex model performs better than the baseline on training data but worse on
    unseen test data, you know you’ve overfit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple ways to create a baseline model. One straightforward approach
    is to use a random model. A random model is created by randomly assigning labels
    to the data. This type of model helps you evaluate how other models perform compared
    to random guessing. Another common approach is to use the majority class classifier.
    The majority class classifier always predicts the most common class in the training
    data and gives you another simplistic algorithm against which you can compare
    more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: In the lakehouse, we have AutoML, which is another straightforward way to get
    a baseline. This is our personal favorite way to start an ML task because it gives
    us a head start on model selection compared to the simpler baseline options. Recall
    that we used AutoML in [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180) to explore
    the *Favorita sales* data. While generating that exploration notebook, AutoML
    also generated model experiments, which is what we are focusing on now.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML rapidly explores many model/hyperparameter permutations to find the best
    baseline model for your data, along with evaluation metrics and actual code. Once
    you have created a baseline model, you can evaluate its performance using accuracy,
    precision, recall, confusion matrices, **receiver operating characteristic** (**ROC**)
    curves, and more to choose the best experiment. However, it is essential to remember
    that AutoML is not a magic bullet. It can remove some of the overhead of coding
    multiple algorithms for experimentation, but you are still in charge of where
    to go next. Luckily, AutoML automatically tracks these model artifacts in MLflow,
    which is where MLflow shines. Tracking the many features, models, hyperparameters,
    and evaluation metrics is a real headache. Using MLflow to track everything natively
    is a lifesaver, so let’s explore that next.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking experiments with MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MLflow is an open source platform developed by Databricks for managing data
    science projects through the entire ML life cycle, from the experimentation phase
    to packaging code to model deployment in production. We’ll cover deployment in
    a later chapter. For now, let’s focus on the **Tracking** component of MLflow
    ([https://mlflow.org/docs/latest/tracking.html#tracking](https://mlflow.org/docs/latest/tracking.html#tracking)).
    Before we had MLflow Tracking, ML experiments required a lot of work outside of
    actual experimentation. MLflow Tracking handles the overhead of capturing configurations
    of features, algorithms, and hyperparameters during testing. Additionally, using
    MLflow in the lakehouse gives you access to Managed MLflow, which is built on
    top of MLflow. Databricks notebooks have built-in integrations that make it easy
    to manage and compare experiment results both programmatically via Mlflow’s lightweight
    APIs or through **user interfaces** (**UIs**). For example, *Figure 6**.1* shows
    how we can view our experiment runs while still in our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The in-notebook UI for viewing the experiment runs](img/B16865_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The in-notebook UI for viewing the experiment runs
  prefs: []
  type: TYPE_NORMAL
- en: 'In MLflow Tracking lingo, the execution of data science code is called a **run**.
    Each run can record the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters**: Key-value pairs of input parameters, such as the features used
    for a given run or the number of trees in a random forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**: Evaluation metrics such as **Root Mean Squared Error** (**RMSE**)
    or **Area Under the ROC** **Curve** (**AUC**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifacts**: Arbitrary output files in any format. This can include images,
    pickled models, and data files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source**: The code that originally ran the experiment and a reference to
    the exact version of the data used for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When you train models in the notebook, model training information is automatically
    tracked with MLflow Tracking. To customize the auto-logging configuration, call
    `mlflow.autolog()` before your training code. Please note that although many common
    libraries have auto-logging support (such as Scikit-learn, XGBoost, and Keras),
    check the documentation for a full list: [https://mlflow.org/docs/latest/models.html#built-in-model-flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors).'
  prefs: []
  type: TYPE_NORMAL
- en: When working on a particular ML task, it’s helpful to group your runs into “experiments.”
    This is an easy way to compare runs, either programmatically or via the Databricks
    Experiments UI.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180) and [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244),
    we used AutoML for traditional regression and classification models. In the next
    session, we will lean into more advanced classification techniques for more complex
    business problems.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying beyond the basic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Databricks AutoML product is a solid starting point for classification,
    regression, and forecasting models. There are more advanced classification techniques
    beyond tree-based models, gradient boost models, and logistic regression that
    you can use with the lakehouse, as it is designed to work with virtually any open
    source ML model.
  prefs: []
  type: TYPE_NORMAL
- en: The Databricks ML runtimes include pre-built DL infrastructure and libraries
    such as PyTorch, TensorFlow, and Hugging Face transformers. DL models are computationally
    intensive, and **distributed DL** (**DDL**) frameworks such as Horovod also work
    in conjunction with these DL libraries for more efficient DDL. Be sure to check
    out the new PyTorch on Databricks! There is a *PyTorch on Databricks – Introducing
    the Spark PyTorch Distributor* blog that is useful if you are working with PyTorch
    ([https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another exciting type of ML is **generative adversarial networks** (**GANs**).
    A quick introduction for those not familiar: GANs are a type of generative model
    that consists of two **neural networks** (**NNs**) – a generator and a discriminator.
    The generator network learns to generate synthetic data, such as images or text,
    that is similar to the real data, while the discriminator network tries to distinguish
    between the real and synthetic data. GANs have been used for image synthesis,
    data augmentation, and generating realistic deepfake videos. We used GANs in the
    past to thwart image classification algorithms. The goal was to alter an image
    just enough to confuse the DL algorithms but not so much that the human eye would
    recognize the image was altered. To see other applications of GANs, watch this
    awesome talk from *Data + AI Summit 2023*: *Generative AI at Scale Using GAN and
    Stable* *Diffusion* ([https://www.youtube.com/watch?v=YsWZDCsM9aE](https://www.youtube.com/watch?v=YsWZDCsM9aE)).'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating innovation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The world of data science and ML moves very fast. You will likely come across
    projects that will benefit from innovations outside the standard ML libraries.
    For example, if you want to work on a project using text data, you will want to
    explore LLMs. LLMs are a type of advanced language model trained using DL techniques
    on massive amounts of text data. Fortunately, the Databricks platform makes it
    easy to integrate with projects such as OpenAI’s ChatGPT and other available options
    from Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at an example of using an LLM to help business users or analysts
    get information from their tables without knowing SQL. We will build a chatbot
    using OpenAI’s **Generative Pre-trained Transformer 4** (**GPT-4**) as a data
    analyst. In this example, you create instructions on how it can ask for a list
    of tables, get information from those tables, and sample data from the tables.
    The chatbot is able to build a SQL query and then interpret the results. To run
    these notebooks in the example, you will need an account with OpenAI at the OpenAI
    developer site ([https://platform.openai.com](https://platform.openai.com)) and
    must request a key for the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: In the first notebook, `sql_resource`, we will create instructions and references
    for the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook starts with commented text with tips on responses and response
    format, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Instruction text for response and response format for this chatbot](img/B16865_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Instruction text for response and response format for this chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'The next lines are where you create text for invalid responses and the response
    format for your chatbot to share its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Text for invalid responses and the chatbot response format](img/B16865_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Text for invalid responses and the chatbot response format
  prefs: []
  type: TYPE_NORMAL
- en: 'For your chatbot to understand the landscape of data, you will need to create
    a catalog and define functions to identify your list of tables, table definitions,
    and schemas as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Defining tables and table locations for the chatbot](img/B16865_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Defining tables and table locations for the chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, your chatbot knows where to get information. To communicate with your
    chatbot, we need to teach it how to have a conversation. To do this, we define
    a log for a conversation and conversation function and the function to send the
    conversation to the OpenAI GPT-4 model. This is also where you can change which
    model your chatbot uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Defining function to submit conversation to OpenAI API](img/B16865_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Defining function to submit conversation to OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: 'We want our chatbot to build SQL queries to get data from our tables, so we
    create a function to teach it how to build a Spark SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Function to process SQL](img/B16865_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Function to process SQL
  prefs: []
  type: TYPE_NORMAL
- en: 'The function we created in *Figure 6**.6* is just a few lines of code, but
    it enables the chatbot to effectively build SQL queries against the tables defined
    in *Figure 6**.4*. Now, we need to tie it all together by defining how to process
    the conversation and create a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Function to process request and response](img/B16865_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Function to process request and response
  prefs: []
  type: TYPE_NORMAL
- en: We have now constructed the chatbot, created the initial language for the chatbot
    to interact with prompts, designated the data and tables available, and showed
    it how to assemble queries and respond to prompts. In the next section, we start
    working with the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next notebook is where we interact with the chatbot, and it starts by installing
    the OpenAI library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Installing OpenAI library](img/B16865_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Installing OpenAI library
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will pull in the functions that we defined in our `sql_resource` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Importing functions from sql_resources](img/B16865_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Importing functions from sql_resources
  prefs: []
  type: TYPE_NORMAL
- en: 'With the library installed and the functions loaded, we have all of the parts
    assembled that are needed to interact. We start by using the `startConversation()`
    function to initiate a conversation with our chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Starting a conversation with the chatbot](img/B16865_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Starting a conversation with the chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that we have all experienced when interacting with chatbots is they
    don’t always give you the information you want the first time, so with our chatbot,
    we can have a back-and-forth conversation. In the preceding conversation, we wanted
    to know which customer ordered the most, but we don’t know how many orders the
    customer ordered, so in *Figure 6**.11*, we ask the question in a different way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Continuing the conversation with the chatbot](img/B16865_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Continuing the conversation with the chatbot
  prefs: []
  type: TYPE_NORMAL
- en: As new versions of OpenAI’s GPT model are released, the results and behavior
    of your chatbot may change. In this case, GPT-3.5 asked more questions than the
    GPT-4 version, but the GPT-4 version was better at using the commands to list
    tables and request table definitions. As new models and approaches become available,
    it is good practice to test them and see how the changes impact your work and
    the results of your chatbot. Leveraging MLflow with your chatbot experiment will
    help you track and compare different features and configurations and assist in
    your production process.
  prefs: []
  type: TYPE_NORMAL
- en: In this next section, we will combine the features created in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244)
    to create models for our different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to create baseline models using AutoML,
    tracking our MLOps with MLflow, and even using more advanced language models in
    order to extract more information and ultimately business value from our data.
    Now, let’s take what we have learned and apply it to our datasets that we cleaned
    in [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180) and featurized in [*Chapter
    5*](B16865_05.xhtml#_idTextAnchor244).
  prefs: []
  type: TYPE_NORMAL
- en: We will start with creating and training a classification model for our Parkinson’s
    data so that, ultimately, we can classify hesitation using the patients’ tracking
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Parkinson’s FOG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in the *Technical requirements* section, we are using PyTorch.
    To use this, either install the packages in your notebook using `pip` or add it
    to your cluster configuration under `libraries`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Installing the PyTorch library](img/B16865_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Installing the PyTorch library
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have your libraries loaded, we import all the libraries we use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Importing libraries](img/B16865_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Importing libraries
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we create a model focused on one target label, namely `StartHesitation`.
    For ease of reuse, define feature and target variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Defining measures and target variable](img/B16865_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Defining measures and target variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a custom `FogDataset` class. The class is used by PyTorch and
    requires three specific class methods: `__init__`, `__len__`, `__getitem__`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `LightningModel` class, as earlier, the `__init__` class method sets
    the labels and converts the features to tensors. The `__len__` class method returns
    the total amount of samples in your dataset. The `__getitem__` class method returns,
    given an index, the i-th sample and label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Creating custom FogDataset class](img/B16865_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Creating custom FogDataset class
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to define the PyTorch model with functions. In the PyTorch
    model definitions, we call `self.log`, defining a `forward` and `test set()` function
    to surface scalars in TensorBoard. This will then be directly usable with PyTorch
    Lightning to make a lightning-fast model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Defining the PyTorch model up to the training step definition](img/B16865_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Defining the PyTorch model up to the training step definition
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.16* defines the PyTorch model, the forward feed, and the training
    step details. In the second half of the model code, we define test and validation
    steps, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – The second half of the PyTorch model definition](img/B16865_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – The second half of the PyTorch model definition
  prefs: []
  type: TYPE_NORMAL
- en: 'We are creating training data using SQL to join the `tdcsfog` data and `tdcsfog_metadata`.
    We check the label count and then convert the training data to Pandas to prepare
    it for the Sklearn library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Creating Parkinson’s training dataset](img/B16865_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Creating Parkinson’s training dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We will stratify the training data by subject, printing the label distribution
    to look for the most representative train/test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Stratifying the training dataset](img/B16865_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Stratifying the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a snippet of the output to illustrate why examining the folds
    is a necessary step in the process. There are some folds where there is a notable
    difference between the label distribution in the test and training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Reviewing the test and train labels of the folds](img/B16865_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Reviewing the test and train labels of the folds
  prefs: []
  type: TYPE_NORMAL
- en: 'We now implement our splits with fold `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Implementing the splits at fold 3](img/B16865_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Implementing the splits at fold 3
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have train test indices, we can clean our DataFrames by resetting
    the indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Resetting the indices after training](img/B16865_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Resetting the indices after training
  prefs: []
  type: TYPE_NORMAL
- en: 'Improperly indexed DataFrames cause issues with the `__getitem__` method in
    our `FogDataset` class. Now, we create custom datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Creating customer train, test, and validation datasets](img/B16865_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Creating customer train, test, and validation datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we build the model with the custom datasets we created and train using
    PyTorch’s `Trainer`:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.24 – Building and\uFEFF training the model](img/B16865_06_24.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Building and training the model
  prefs: []
  type: TYPE_NORMAL
- en: We have now used our Parkinson’s FOG data to build and train a classification
    PyTorch model to predict hesitation in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting Favorita sales
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), we used AutoML to jump-start
    our **exploratory data analysis** (**EDA**). Now, we’ll use AutoML to create a
    baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we create an aggregated table to feed into AutoML. This is
    not required but is an easy way to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Creating a table of aggregated sales data](img/B16865_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Creating a table of aggregated sales data
  prefs: []
  type: TYPE_NORMAL
- en: Note that the code creates a `favorita_autoML_agg` table, which includes the
    lag features we created in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244).
  prefs: []
  type: TYPE_NORMAL
- en: 'We create our AutoML experiment similarly to our previous one. See the experiment
    configurations in *Figure 6**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – AutoML experiment configuration for the Favorita forecasting
    sales example](img/B16865_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – AutoML experiment configuration for the Favorita forecasting sales
    example
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that during this experiment, we are treating the forecasting problem
    like a regression problem by selecting the ML problem type as `date` variable
    in the `time` column; see *Figure 6**.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27 – AutoML advanced configuration for the Favorita forecasting
    sales example](img/B16865_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – AutoML advanced configuration for the Favorita forecasting sales
    example
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiment created about 100 runs before reaching the point where it was
    no longer making progress against the metric of choice – in our case, **R-squared**,
    as shown in *Figure 6**.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28 – AutoML advanced configuration for the Favorita forecasting
    sales example](img/B16865_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – AutoML advanced configuration for the Favorita forecasting sales
    example
  prefs: []
  type: TYPE_NORMAL
- en: Out of the 100 combinations, only 6 have an R-squared value of 0.85 or higher.
    Using AutoML is saving us considerable time and effort. During the experiment,
    MLflow tried many model types and hyperparameter tuning utilizing **Hyperopt**.
    This experiment is also distributed with the power of Spark, meaning we have a
    solid model that was tuned efficiently. We have a baseline model, and we are sure
    a signal can be found. From here forward, we want to aim to beat the model. Beating
    the model at this point is done by brute force. We improve performance by creating
    new features, gathering more data points, or enriching the dataset. This point
    is brute force. We improve performance by creating new features, gathering more
    data points, or enriching the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed quick ways to create a baseline model and demonstrated
    how that increases productivity.
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrated MLflow functionality that supports MLOps and helps track model
    training and tuning. We also covered more complex classification frameworks that
    can be used in the lakehouse. Access to these frameworks made it possible to implement
    a DL model in PyTorch for the Parkinson’s FOG example. The openness of Databricks
    opens the doors for open source and proprietary innovations with API integrations,
    as shown by the SQL bot LLM. This integration saved time by not recreating the
    wheel and putting the SQL tool in the hands of our analysts sooner.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will focus on moving our models into production.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  prefs: []
  type: TYPE_NORMAL
- en: Why would you use a baseline model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are examples of more advanced classification techniques?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you use LLM models, such as OpenAI’s ChatGPT or **Dolly**, in your
    lakehouse?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a baseline model to have a simple model as a starting point to compare later
    and more complex models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some examples of more advanced classification techniques include DL and GANs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I would use an LLM model in my lakehouse if I needed to have more advanced language
    techniques with my data, such as a chatbot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please take a look at these resources to get deeper into areas that
    interest you most:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introducing AI Functions: Integrating Large Language Models with Databricks*
    *SQL*: [https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html](https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyTorch on Databricks – Introducing the Spark PyTorch* *Distributor*:[https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned* *LLM*:
    [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ray 2.3 release (**PyPI)*: [https://pypi.org/project/ray/](https://pypi.org/project/ray/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ray on Spark Databricks* *docs*: [https://docs.databricks.com/machine-learning/ray-integration.html](https://docs.databricks.com/machine-learning/ray-integration.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Announcing Ray support on Databricks and Apache Spark* *Clusters*: [https://www.databricks.com/blog/2023/02/28/announcing-ray-support-databricks-and-apache-spark-clusters.html](https://www.databricks.com/blog/2023/02/28/announcing-ray-support-databricks-and-apache-spark-clusters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ray* *docs*: [https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#deploying-on-spark-standalone-cluster](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#deploying-on-spark-standalone-cluster)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
