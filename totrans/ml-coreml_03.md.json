["```py\npublic protocol VideoCaptureDelegate: class {\n    func onFrameCaptured(\n      videoCapture: VideoCapture, \n      pixelBuffer:CVPixelBuffer?, \n      timestamp:CMTime)\n}\n```", "```py\npublic class VideoCapture : NSObject{\n    public weak var delegate: VideoCaptureDelegate?\n    public var fps = 15\n    var lastTimestamp = CMTime()\n    override init() {\n        super.init()\n    }\n    private func initCamera() -> Bool\n    {\n        return true\n    }\n    public func asyncStartCapturing(\n        completion: (() -> Void)? = nil)\n        {\n         }\n    public func asyncStopCapturing(\n        completion: (() -> Void)? = nil)\n        {\n        }\n}\n```", "```py\nextension VideoCapture : AVCaptureVideoDataOutputSampleBufferDelegate{\n    public func captureOutput(_ output: AVCaptureOutput,\n                              didOutput sampleBuffer: CMSampleBuffer,\n                              from connection: AVCaptureConnection)\n    {\n    }\n}\n```", "```py\nlet captureSession = AVCaptureSession()\nlet sessionQueue = DispatchQueue(label: \"session queue\")\n```", "```py\ncaptureSession.beginConfiguration()       \ncaptureSession.sessionPreset = AVCaptureSession.Preset.medium \n```", "```py\nguard let captureDevice = AVCaptureDevice.default(for: AVMediaType.video) else {\n    print(\"ERROR: no video devices available\")\n    return false\n}\n\nguard let videoInput = try? AVCaptureDeviceInput(device: captureDevice) else {\n    print(\"ERROR: could not create AVCaptureDeviceInput\")\n    return false\n}\n\nif captureSession.canAddInput(videoInput) {\n    captureSession.addInput(videoInput)\n}\n```", "```py\nlet videoOutput = AVCaptureVideoDataOutput()\n\nlet settings: [String : Any] = [\n    kCVPixelBufferPixelFormatTypeKey as String: NSNumber(value: kCVPixelFormatType_32BGRA)\n]\nvideoOutput.videoSettings = settings\nvideoOutput.alwaysDiscardsLateVideoFrames = true\nvideoOutput.setSampleBufferDelegate(self, queue: sessionQueue)\n\nif captureSession.canAddOutput(videoOutput) {\n    captureSession.addOutput(videoOutput)\n}\n\nvideoOutput.connection(with: AVMediaType.video)?.videoOrientation = .portrait\n```", "```py\ncaptureSession.commitConfiguration()\n```", "```py\nsessionQueue.async {\n    if !self.captureSession.isRunning{\n       self.captureSession.startRunning()\n    }\n\n    if let completion = completion{\n        DispatchQueue.main.async {\n            completion()\n        }\n    }\n }\n```", "```py\nsessionQueue.async {\n    if self.captureSession.isRunning{\n        self.captureSession.stopRunning()\n    }\n\n    if let completion = completion{\n        DispatchQueue.main.async {\n            completion()\n         }\n     }\n }\n```", "```py\nguard let delegate = self.delegate else{ return }\n\n let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)\n\n let elapsedTime = timestamp - lastTimestamp\n if elapsedTime >= CMTimeMake(1, Int32(fps)) {\n\n lastTimestamp = timestamp\n\n let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)\n\n delegate.onFrameCaptured(videoCapture: self,\n pixelBuffer:imageBuffer,\n timestamp: timestamp)\n }\n```", "```py\nimport AVFoundation\n import UIKit\n\n class CapturePreviewView: UIView {\n\n }\n```", "```py\noverride class var layerClass: AnyClass {\n    return AVCaptureVideoPreviewLayer.self\n}\n```", "```py\n@IBOutlet var previewView:CapturePreviewView!\n@IBOutlet var classifiedLabel:UILabel!\n\n let videoCapture : VideoCapture = VideoCapture()\n```", "```py\nif self.videoCapture.initCamera(){\n (self.previewView.layer as! AVCaptureVideoPreviewLayer).session = self.videoCapture.captureSession\n\n (self.previewView.layer as! AVCaptureVideoPreviewLayer).videoGravity = AVLayerVideoGravity.resizeAspectFill\n\n self.videoCapture.asyncStartCapturing()\n } else{\n fatalError(\"Failed to init VideoCapture\")\n }\n```", "```py\nextension ViewController : VideoCaptureDelegate{\n    func onFrameCaptured(videoCapture: VideoCapture,\n     pixelBuffer:CVPixelBuffer?,\n     timestamp:CMTime){\n     }\n }\n```", "```py\nself.videoCapture.delegate = self\n```", "```py\nfunc onFrameCaptured(videoCapture: VideoCapture,\n pixelBuffer:CVPixelBuffer?,\n timestamp:CMTime){\n // TODO\n }\n```", "```py\n@available(macOS 10.13, iOS 11.0, tvOS 11.0, watchOS 4.0, *)\n class Inceptionv3Input : MLFeatureProvider {\n\n /// Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 299 pixels wide by 299     pixels high\n var image: CVPixelBuffer\n\n var featureNames: Set<String> {\n     get {\n         return [\"image\"]\n     }\n }\n\n func featureValue(for featureName: String) -> MLFeatureValue? {\n     if (featureName == \"image\") {\n         return MLFeatureValue(pixelBuffer: image)\n     }\n     return nil\n }\n\n init(image: CVPixelBuffer) {\n     self.image = image\n     }\n }\n```", "```py\n@available(macOS 10.13, iOS 11.0, tvOS 11.0, watchOS 4.0, *)\n class Inceptionv3Output : MLFeatureProvider {\n\n /// Probability of each category as dictionary of strings to doubles\n let classLabelProbs: [String : Double]\n\n /// Most likely image category as string value\n let classLabel: String\n\n var featureNames: Set<String> {\n     get {\n         return [\"classLabelProbs\", \"classLabel\"]\n     }\n }\n\n func featureValue(for featureName: String) -> MLFeatureValue? {\n     if (featureName == \"classLabelProbs\") {\n         return try! MLFeatureValue(dictionary: classLabelProbs as [NSObject : NSNumber])\n     }\n     if (featureName == \"classLabel\") {\n         return MLFeatureValue(string: classLabel)\n     }\n     return nil\n}\n\n init(classLabelProbs: [String : Double], classLabel: String) {\n     self.classLabelProbs = classLabelProbs\n     self.classLabel = classLabel\n     }\n }\n```", "```py\n@available(macOS 10.13, iOS 11.0, tvOS 11.0, watchOS 4.0, *)\n class Inceptionv3 {\n var model: MLModel\n\n /**\n Construct a model with explicit path to mlmodel file\n - parameters:\n - url: the file url of the model\n - throws: an NSError object that describes the problem\n */\n init(contentsOf url: URL) throws {\n self.model = try MLModel(contentsOf: url)\n }\n\n /// Construct a model that automatically loads the model from the app's bundle\n convenience init() {\n let bundle = Bundle(for: Inceptionv3.self)\n let assetPath = bundle.url(forResource: \"Inceptionv3\", withExtension:\"mlmodelc\")\n try! self.init(contentsOf: assetPath!)\n }\n\n /**\n Make a prediction using the structured interface\n - parameters:\n - input: the input to the prediction as Inceptionv3Input\n - throws: an NSError object that describes the problem\n - returns: the result of the prediction as Inceptionv3Output\n */\n func prediction(input: Inceptionv3Input) throws -> Inceptionv3Output {\n let outFeatures = try model.prediction(from: input)\n let result = Inceptionv3Output(classLabelProbs: outFeatures.featureValue(for: \"classLabelProbs\")!.dictionaryValue as! [String : Double], classLabel: outFeatures.featureValue(for: \"classLabel\")!.stringValue)\n return result\n }\n\n /**\n Make a prediction using the convenience interface\n - parameters:\n - image: Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 299 pixels wide by 299 pixels high\n - throws: an NSError object that describes the problem\n - returns: the result of the prediction as Inceptionv3Output\n */\n func prediction(image: CVPixelBuffer) throws -> Inceptionv3Output {\n let input_ = Inceptionv3Input(image: image)\n return try self.prediction(input: input_)\n }\n }\n```", "```py\nextension CIImage{\n\n func resize(size: CGSize) -> CIImage {\n     fatalError(\"Not implemented\")\n }\n\n func toPixelBuffer(context:CIContext,\n size insize:CGSize? = nil,\n     gray:Bool=true) -> CVPixelBuffer?{\n         fatalError(\"Not implemented\")\n     }\n }\n```", "```py\nlet scale = min(size.width,size.height) / min(self.extent.size.width, self.extent.size.height)\n\n let resizedImage = self.transformed(\n by: CGAffineTransform(\n scaleX: scale,\n y: scale))\n```", "```py\nlet width = resizedImage.extent.width\n let height = resizedImage.extent.height\n let xOffset = (CGFloat(width) - size.width) / 2.0\n let yOffset = (CGFloat(height) - size.height) / 2.0\n let rect = CGRect(x: xOffset,\n y: yOffset,\n width: size.width,\n height: size.height)\n\n return resizedImage\n .clamped(to: rect)\n .cropped(to: CGRect(\n x: 0, y: 0,\n width: size.width,\n height: size.height))\n```", "```py\nfunc toPixelBuffer(context:CIContext, gray:Bool=true) -> CVPixelBuffer?{\n     fatalError(\"Not implemented\")\n }\n```", "```py\nlet attributes = [\n kCVPixelBufferCGImageCompatibilityKey:kCFBooleanTrue,\n kCVPixelBufferCGBitmapContextCompatibilityKey:kCFBooleanTrue\n ] as CFDictionary\n\n var nullablePixelBuffer: CVPixelBuffer? = nil\n let status = CVPixelBufferCreate(kCFAllocatorDefault,\n Int(self.extent.size.width),\n Int(self.extent.size.height),\n gray ? kCVPixelFormatType_OneComponent8 : kCVPixelFormatType_32ARGB,\n attributes,\n &nullablePixelBuffer)\n\n guard status == kCVReturnSuccess, let pixelBuffer = nullablePixelBuffer\n else { return nil }\n```", "```py\nCVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\n\n context.render(self,\n to: pixelBuffer,\n bounds: CGRect(x: 0,\n y: 0,\n width: self.extent.size.width,\n height: self.extent.size.height),\n colorSpace:gray ?\n CGColorSpaceCreateDeviceGray() :\n self.colorSpace)\n\n CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\n\n return pixelBuffer\n```", "```py\nlet context = CIContext()\n```", "```py\nguard let pixelBuffer = pixelBuffer else{ return }\n\n // Prepare our image for our model (resizing)\n guard let scaledPixelBuffer = CIImage(cvImageBuffer: pixelBuffer)\n .resize(size: CGSize(width: 299, height: 299))\n .toPixelBuffer(context: context) else{ return }\n```", "```py\nlet model = Inceptionv3()\n```", "```py\nlet prediction = try? self.model.prediction(image:scaledPixelBuffer)\n\n // Update label\n DispatchQueue.main.sync {\n classifiedLabel.text = prediction?.classLabel ?? \"Unknown\"\n }\n```"]