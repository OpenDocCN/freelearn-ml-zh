- en: '*Chapter 4*: Preparing Data for DataRobot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers tasks relating to preparing data for modeling. While the
    tasks themselves are relatively straightforward, they can take up a lot of time
    and can sometimes cause frustration. Just know that if you feel this way, you
    are not alone. This is pretty normal. This is also where you will begin to notice
    that things are a bit different from your experience in an academic setting. Data
    will almost never arrive in a form that's suitable for modeling, and it is a mistake
    to assume that the data you have received is in good condition and of good quality.
  prefs: []
  type: TYPE_NORMAL
- en: Most real-world problems do not come with a ready-made dataset that you can
    start processing and use to build models. Most likely you will need to stitch
    data together from multiple disparate sources. Depending on the data, **DataRobot**
    might perform data preparation and cleansing tasks automatically, or you might
    have to do some of these on your own. This chapter covers concepts and examples
    to show how to cleanse and prepare your data and the features that DataRobot provides
    to help with these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know how to set up data to hand it off
    to DataRobot and begin modeling. In the chapter, we''re going to cover the following
    main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating data for modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleansing the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with different types of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering features for modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some parts of this chapter require access to the DataRobot software, and some
    tools for data manipulation. Most of the examples deal with small datasets and
    therefore can be handled via Excel. The datasets that we will be using in the
    rest of this book are described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Automobile Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Automobile Dataset (source: Dua, D. and Graff, C. (2019). UCI Machine Learning
    Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]. Irvine,
    CA: University of California, School of Information and Computer Science) can
    be accessed at the UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)).
    Each row in this dataset represents a specific automobile. The features (columns)
    describe its characteristics, risk rating, and associated normalized losses. Even
    though it is a small dataset, it has many features that are numerical as well
    as categorical. Features are described on the web page, and the data is provided
    in `.csv` format.'
  prefs: []
  type: TYPE_NORMAL
- en: Appliances Energy Prediction Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This dataset (source: Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix,
    *Data driven prediction models of energy use of appliances in a low-energy house*,
    Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788) can
    be accessed at the UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#)).
    This dataset captures temperature and humidity data in various rooms in a house
    and in the outside environment, along with energy consumption by various devices
    over time. The data is captured every 10 minutes. This is a typical example of
    a time series dataset. Data is provided in `.csv` format, and the site also provides
    descriptions of the various features. All features in this dataset are numeric
    features. The dataset also includes two random variables to make the problem interesting.'
  prefs: []
  type: TYPE_NORMAL
- en: SQL
  prefs: []
  type: TYPE_NORMAL
- en: For some parts of this chapter, it will be helpful to know SQL, although you
    do not need to know SQL to go through the example problems.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By this point, you should have a list of data sources and an idea of what data
    is stored there. Depending on your use case, these sources could be real-time
    data streaming sources you need to tap into. Here are some typical sources of
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excel files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon S3 buckets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System** (**HDFS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data warehouses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the type of data source, you will use different mechanisms to access
    this data. These could be on-premises or in the cloud. Depending on the condition
    of the data, you can bring it directly into DataRobot, or you might have to do
    some preparation before you bring it into DataRobot. DataRobot has recently added
    capabilities in the form of **Paxata** to help with this process, but you might
    not have access to that add-on. Most of the processing work is done via **SQL**,
    **Python**, **pandas**, and **Excel**. For the purpose of this book, we will only
    focus on Excel.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not already familiar with SQL and pandas, then it will be helpful
    for you to start learning about them as soon as you get an opportunity:'
  prefs: []
  type: TYPE_NORMAL
- en: You can connect to a data source by going to the **Create New Project** menu,
    as shown in the following figure:![Figure 4.1 – Connecting to a data source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.1_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.1 – Connecting to a data source
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can search for an existing data source that has been defined, or you can
    add a new data connection. If you select the **add new data connection** option
    (shown in the preceding figure), you will see the following connection choices:![Figure
    4.2 – Types of data connection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.2_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.2 – Types of data connection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see the connection choices available for your organization. What you
    see here could be different from the preceding figure. Most databases with JDBC
    drivers are supported, but you might have to check with your administrator. As
    an example, let''s select the **MySQL** option, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Configuring a data connection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.3_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Configuring a data connection
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, you will see the configuration parameters for configuring
    a MySQL data source. Other data sources are similar in nature. Here, you will
    enter the configuration settings that can be obtained from your database administrator.
    You will need to create a similar connection if you are connecting to a database
    to get data into Python or Excel.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will need to have some working knowledge of SQL or work with someone who
    knows SQL to make use of these options.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating data for modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the previous chapters, you might remember that machine learning algorithms
    expect the dataset to be in a specific form and it needs to be in one table. The
    data needed for this table, however, could reside in multiple sources. Hence,
    one of the first things you need to do is to aggregate data from multiple sources.
    This is often done using SQL or Python. Recently, DataRobot has added the capability
    to add multiple datasets into a project and then aggregate this data within DataRobot.
    Please note that there are still some data cleansing operations that you might
    have to do outside of DataRobot, so if you want to use the aggregation capabilities
    of DataRobot, you need to do cleansing operations prior to bringing this data
    into DataRobot. We cover data cleansing in the following section. If you choose
    to do data aggregation inside DataRobot, you have to make sure to do this at the
    very start of the project (*Figure 4.4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Add secondary datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.4_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Add secondary datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding figure, just below the **Start** button, you can click on
    **Add datasets**. Once you click on it, you will see a window that lets you specify
    the additional dataset, as shown in *Figure 4.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Secondary datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.5_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Secondary datasets
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can add a new dataset and define the relationships between your main
    dataset and the secondary datasets. For time series problems, you can also use
    this capability to aggregate your data to the right timescale and join it with
    the main dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this does require some understanding of how relational tables
    work and some SQL concepts. If you are not familiar with these ideas and you are
    not sure what indexes to use, work with someone who understands databases to help
    you set this up.
  prefs: []
  type: TYPE_NORMAL
- en: Cleansing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This step can come before or after the data aggregation we talked about in
    the previous section. We introduced some concepts around data cleansing in [*Chapter
    2*](B17159_02_Final_NM_ePub.xhtml#_idTextAnchor039), *Machine Learning Basics*,
    so let''s look at how to actually do it on a dataset. For this, let''s start with
    the Automobile Dataset. Please refer to the *Technical requirements* section to
    access the UCI repository for this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download two files: `imports-85.data` and `imports-85.names`. The data
    file is in `.csv` format, so let''s rename the file with the `.csv` extension
    and open it using Excel (you can use any text editor). You will now see the data
    (*Figure 4.6*):![Figure 4.6 – Automobile data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_4.6_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.6 – Automobile data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will notice in the preceding screenshot that it is missing the header information.
    To retrieve the header information, open the `.names` file in any text editor.
    You will see the names of attributes as well as their definitions. Create an empty
    row at the top of your `.csv` file and you will have to manually type the names
    of these attributes as the first row of your file. Now let''s save this file as
    `autodata.csv`. It should now look as shown in *Figure 4.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Automobile data with headers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.7_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Automobile data with headers
  prefs: []
  type: TYPE_NORMAL
- en: 'Please review all the cells in this data file. You will have already noticed
    that many cells in the preceding figure have a `normalized-losses` where 20% of
    the total values are missing. Given that our dataset is very small, we do not
    want to drop the rows with missing data. Also, DataRobot has mechanisms to account
    for missing values, so we are going to leave most of them as is. The only one
    that we want to consider is `normalized-losses`. If `normalized-losses` is our
    target variable, then we have no choice but to drop those rows. If not, we can
    first try to go as is and let DataRobot build a model. We can then try an alternative
    strategy of using the average value of `normalized-losses` per **Symboling** value
    to see if that makes any difference. I will use Excel''s pivot table functionality
    to compute these averages (*Figure 4.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Pivot table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.8_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Pivot table
  prefs: []
  type: TYPE_NORMAL
- en: The reason for using **Symboling** is that it is an indicator of risk. Depending
    on the problem and what you are trying to accomplish, you can choose some other
    feature for this purpose. For now, we will use **Symboling** to illustrate how
    to do it. There are more sophisticated imputation methods available, such as a
    K-Nearest Neighbor-based imputation method, that you can explore if desired ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In reviewing the Appliances Energy Prediction Dataset, we see that the data
    looks very clean and no further cleansing is required. In real-world projects,
    you will almost never find a dataset that is free of problems. Typical problems
    in time series datasets to watch out for are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Very little data**: You need at least 35 or so datapoints for regression
    and 100 datapoints for classification problems to allow DataRobot to do something
    useful with your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data gaps**: Sometimes data might be missing for certain timesteps. In these
    cases, you can use values from the timesteps before or after to assign values
    for the missing time step. You can also let DataRobot do this for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interrelated series**: Often you will have multiple timeseries that you are
    trying to forecast. If the series are similar and are interrelated, then you can
    combine them into a single model. This can often improve the forecast accuracy.
    In these cases, you have to create a feature that tells DataRobot that these series
    are part of the same cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will revisit the data quality based on what DataRobot finds. Now that the
    dataset looks reasonably clean (which is very unusual by the way), let's investigate
    this data further.
  prefs: []
  type: TYPE_NORMAL
- en: Working with different types of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will have noticed that some of the features have numeric values while others
    have categorical values. For example, the `standard` as well as `std` in your
    datasets. In this case, DataRobot will treat them as different values, even though
    they are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some features that can be treated as categorical or as numerical.
    For example, `num-of-cylinders`; here, the values are expressed as text. Given
    that there is a numerical order here, it might be beneficial to turn this into
    a numeric variable, as shown in *Figure 4.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Categorical to numerical feature conversion'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.9_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Categorical to numerical feature conversion
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have created (in `cylinder-count`, that carries the numerical values
    for the number of cylinders. In this example, we are using Excel for the data
    manipulation, but this can be achieved via many methods, such as SQL, Python,
    and Paxata. You can do similar data manipulation and create a new column for `num-of-doors`
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `make` feature in the following figure. This seems
    to have 22 possible values, but we have very limited data available. If we count
    the number of rows for each make, we can see how much data is available for each
    make:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Data for each make'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.10_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Data for each make
  prefs: []
  type: TYPE_NORMAL
- en: We notice that some car types have very little data available, so it might be
    useful to combine some of them. For example, we can combine (using Excel) the
    highlighted rows into a make called `other`. Where you draw the line depends upon
    your understanding of the business problem or discussions with domain experts.
    Even with that knowledge, you might have to try out a few different options to
    see what works best. This is what makes machine learning an iterative and exploratory
    process. Also keep in mind that you have limited time available, so don't over-explore
    either. There is certainly a point of diminishing returns where additional tinkering
    will not produce many benefits.
  prefs: []
  type: TYPE_NORMAL
- en: DataRobot also allows special processing for images and geo-spatial data. We
    will cover them in [*Chapter 11*](B17159_11_Final_NM_ePub.xhtml#_idTextAnchor161),
    *Working with GeoSpatial Data, NLP, and Image Processing*. Now let's look at other
    transformations that can be done on data.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering features for modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As part of the system''s understanding you would have gained some insights
    into your problem and dataset that can be used to create new features in your
    dataset by combining the existing features in various ways. For example, we can
    create a new feature called `volume` by multiplying length, width, and height.
    Similarly, we can create a feature called `mpg-ratio` by dividing `highway-mpg`
    by `city-mpg`. Let''s also create a feature called `cylinder-size` by dividing
    `engine-size` by `cylinder-count`. The equations for these features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`volume = length * width * height`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mpg-ratio = highway-mpg / city-mpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cylinder-size = engine-size / cylinder-count`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 4.11* shows an example of what these feature values look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Engineered features for the Automobile Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.11_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Engineered features for the Automobile Dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can now see, many possibilities exist to create new features that could
    prove helpful in solving your problem. Many of these new features may not be useful,
    and it is OK to drop them later. Sometimes, such features will have meaning for
    the customers or stakeholders, and you might want to keep them instead of some
    other features that are redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the Appliances Energy Prediction Dataset file. With this
    dataset, we can create the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`total-energy = Appliances + lights`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`avg-temp-inside = (T1 + T2 + T3 + T4 + T5 + T7 + T8 + T9 ) / 8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`avg-rh-inside = (RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_7 + RH_8 + RH_9 ) /
    8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temp-inout-diff = T6 – avg-temp-inside`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rh-inout-diff = RH_6 – avg-rh-inside`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`windchill-factor` (I am creating an approximate windchill factor based on
    [https://www.weather.gov/media/epz/wxcalc/windChill.pdf](https://www.weather.gov/media/epz/wxcalc/windChill.pdf))
    `= T_out * (Windspeed0.16 )`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The new data features will appear as shown in *Figure 4.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Engineered features for the Appliances Energy Prediction Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.12_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Engineered features for the Appliances Energy Prediction Dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, these features use our knowledge about the domain that we can
    find by talking to domain experts or doing some research on the internet. You
    might be able to find even more such features by doing some research about dew
    points, pressure, and visibility. It will be hard for the automation to catch
    all of these on its own, but on the other hand, the automation might be able to
    find some additional interesting features based on them. Recently, DataRobot has
    also been adding capabilities to automatically do some feature engineering, but
    these capabilities are somewhat limited. One area where these capabilities are
    very useful is time series problems. In this particular area, these capabilities
    are extremely helpful in trying out a wide range of features that will be hard
    to match on your own. Having said that, it is still your responsibility to inject
    your domain knowledge into the model via engineered features.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered methods to help you prepare the dataset for building
    the models. Many of these methods have to be applied outside of DataRobot, although
    DataRobot is beginning to provide support for many of the data preparation tasks.
    As we discussed, many of these tasks cannot be automated at this point in time,
    and they require domain understanding to make appropriate decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in this chapter we have learned how to connect to various data
    sources and how to aggregate data from these sources. We looked at examples to
    address missing data issues and other data manipulation that should be done prior
    to modeling. We also covered several methods for creating new features that can
    be very important for improving the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: We are now at a stage where we will be working almost completely inside the
    DataRobot environment to analyze the data and build models. In the next chapter,
    we will use DataRobot to analyze the datasets.
  prefs: []
  type: TYPE_NORMAL
