- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Your First Machine Learning Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about **Redshift Machine Learning** (**ML**)
    benefits such as eliminating data movement and how models can be created using
    simple **Structured Query Language** (**SQL**) commands.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you are going to build your first machine learning model by
    using the standard SQL dialect. Amazon Redshift makes it very easy to use familiar
    SQL dialect to train, deploy, and run inferences against machine learning models.
    This approach makes it easy for different data personas, for example, database
    developers, database engineers, and citizen data scientists, to train and build
    machine learning models without moving data outside of their data warehouse platform
    and without having to learn a new programming language.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about using Amazon Redshift ML simple CREATE
    MODEL, which uses the `CREATE MODEL` command and different methods used to evaluate
    your ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, to build your first machine learning model, we will go through
    the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Redshift ML simple CREATE MODEL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a web browser and the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon Redshift Serverless endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift Query Editor v2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing the *Getting started with Amazon Redshift Serverless* section in[*Chapter
    1*](B19071_01.xhtml#_idTextAnchor015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code used in this chapter here: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Data files required for this chapter are located in a public S3 bucket: [s3://packt-serverless-ml-redshift/](https://s3://packt-serverless-ml-redshift/'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Redshift ML simple CREATE MODEL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Redshift ML simple CREATE MODEL is a feature in Amazon Redshift that allows
    users to create machine learning models using SQL commands, without the need for
    specialized skills or software. It simplifies the process of creating and deploying
    machine learning models by allowing users to use familiar SQL syntax to define
    the model structure and input data, and then automatically generates and trains
    the model using Amazon SageMaker. This feature can be used for a variety of machine
    learning tasks, including regression, classification, and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into building the first ML model, let us set the stage by defining
    a problem statement that will form the basis of our model-building solution.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use a customer sales dataset to build the first machine learning
    model. Business leaders at the fictitious *ABC Company* are grappling with dwindling
    sales. The data team at *ABC Company* has performed descriptive and diagnostic
    analytics and determined that the cause of decreasing sales is departing customers.
    To stop this problem, data analysts who are familiar with SQL language and some
    machine learning concepts have tapped into Redshift ML. Business users have documented
    which customers have and have not churned and teamed up with data analysts.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the business problem, the data analysts start by analyzing the sales
    dataset. With Redshift SQL commands, they will write SQL aggregate queries and
    create visualizations to understand the trends. The data analyst team then creates
    an ML model using the Redshift ML simple `CREATE MODEL` command. Finally, the
    data analysts evaluate the model performance to make sure the model is useful.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading and analyzing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset used for this chapter is located here: [s3://packt-serverless-ml-redshift/](https://s3://packt-serverless-ml-redshift/).
    We have modified the dataset to better fit the chapter’s requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is attributed to the University of California Irvine Repository
    of Machine Learning Datasets (Jafari-Marandi, R., Denton, J., Idris, A., Smith,
    B. K., & Keramati, A. (2020). *Optimum Profit-Driven Churn Decision Making: Innovative
    Artificial Neural Networks in Telecom Industry. Neural Computing* *and Applications*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset contains customer churn information. The following table lists
    the metadata of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Data Type** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| `state` | `varchar(2)` | US state in which the customer is located |'
  prefs: []
  type: TYPE_TB
- en: '| `account_length` | `int` | Length of customer account |'
  prefs: []
  type: TYPE_TB
- en: '| `area_code` | `int` | Area code or zip code of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `phone` | `varchar(8)` | Phone number of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `intl_plan` | `varchar(3)` | International plan subscriber |'
  prefs: []
  type: TYPE_TB
- en: '| `vMail_plan` | `varchar(3)` | Voicemail plan subscriber |'
  prefs: []
  type: TYPE_TB
- en: '| `vMail_message` | `int` | Voicemail message subscriber |'
  prefs: []
  type: TYPE_TB
- en: '| `day_mins` | `float` | Aggregated daily minutes |'
  prefs: []
  type: TYPE_TB
- en: '| `day_calls` | `int` | Aggregated daily calls |'
  prefs: []
  type: TYPE_TB
- en: '| `day_charge` | `float` | Aggregated daily charges |'
  prefs: []
  type: TYPE_TB
- en: '| `total_charge` | `float` | Total charges |'
  prefs: []
  type: TYPE_TB
- en: '| `eve_mins` | `float` | Evening minutes |'
  prefs: []
  type: TYPE_TB
- en: '| `eve_calls` | `int` | Evening calls |'
  prefs: []
  type: TYPE_TB
- en: '| `eve_charge` | `float` | Evening charges |'
  prefs: []
  type: TYPE_TB
- en: '| `night_mins` | `float` | Nightly minutes |'
  prefs: []
  type: TYPE_TB
- en: '| `night_calls` | `int` | Nightly calls |'
  prefs: []
  type: TYPE_TB
- en: '| `night_charge` | `float` | Nightly charges |'
  prefs: []
  type: TYPE_TB
- en: '| `intl_mins` | `float` | International minutes |'
  prefs: []
  type: TYPE_TB
- en: '| `intl_calls` | `int` | International calls |'
  prefs: []
  type: TYPE_TB
- en: '| `intl_charge` | `float` | International charges |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_serv_calls` | `int` | Number of calls to customer service |'
  prefs: []
  type: TYPE_TB
- en: '| `churn` | `varchar(6)` | Whether customer churned or not |'
  prefs: []
  type: TYPE_TB
- en: '| `record_date` | `date` | Record updated date |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Customer call data
  prefs: []
  type: TYPE_NORMAL
- en: 'After successfully connecting to Redshift as an admin or database developer,
    create the schema and load data into Amazon Redshift as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to **Redshift query editor v2**, connect to the **Serverless:default**
    endpoint, and connect to the **dev** database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new editor and rename the `untitled` query editor by saving it as
    `Chapter5`, as shown in *Figure 5**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Connecting to query editor v2](img/B19071_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Connecting to query editor v2
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Redshift schema named `Chapter5_buildfirstmodel`. Redshift schemas
    contain tables, views, and other named objects. For this chapter, tables and machine
    learning models will be created in this schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a Redshift table named `customer_calls_fact`. This table is used to
    load the dataset that has customer call information. This table is natively created
    in Redshift and used for training and validating the Redshift ML model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the customer call data into the Redshift table by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the Redshift `COPY` command to load the data into our table. `COPY` commands
    load data in parallel into a Redshift table. You can load terabytes of data by
    using the `COPY` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final step, we will analyze the customer churn fact table by creating
    a histogram for customer churn. To do this, let’s use the query editor v2 chart
    feature to create a histogram chart. In order to create the histogram, we need
    to count the number of customers who have churned and not churned. To get this
    information, first, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, click on the **Chart** option found on the right-hand side in the **Result**
    pane to view the histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.2 – Customers churned \uFEFFversus not churned histogram](img/B19071_05_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Customers churned versus not churned histogram
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding chart, you can see that the `customer_calls_fact` table has
    **3333** customers, of which **483** have churned.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we analyzed the dataset and found that there are customers who have churned.
    The next step is to create a machine learning model. For this, we will use the
    Redshift ML simple `CREATE` `MODEL` method.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deep into the Redshift ML CREATE MODEL syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this is the first time you are going to use the `CREATE MODEL` syntax,
    let’s refresh the basic constructs of the command here.
  prefs: []
  type: TYPE_NORMAL
- en: Redshift ML provides the easy-to-use `CREATE MODEL` syntax to create ML models.
    In this section, we will focus on a simple form of the `CREATE MODEL` command.
    In later chapters, you will learn about other forms of creating model statements.
  prefs: []
  type: TYPE_NORMAL
- en: Simple `CREATE MODEL` is the most basic form of Redshift `CREATE MODEL` statement.
    It is geared toward the personas who are not yet ready to deal with all the intricacies
    of the machine learning process. This form of model creation is also used by experienced
    personas such as citizen data scientists for its simplicity in creating a machine
    learning model. Data cleaning is an essential step for any ML problem, otherwise,
    it follows the principle of *garbage in, garbage out*. Data cleaning still remains
    a necessary task, however, with Redshift ML data transformation, standardization
    and model selection won’t be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the following command for simple model creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding `CREATE MODEL` syntax, as a user, you specify your dataset
    – in our case, `customer_calls_fact` – in the `FROM` clause. We set the variable
    that we are targeting to predict, in our case `churn`, in the `TARGET` parameter.
    As a user, you also give a name to the function, which you will use in select
    queries to run predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about simple `CREATE MODEL` parameters, please refer to
    the Redshift public document here: [https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_simple_create_model](https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_simple_create_model'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: We’ve learned about the generic simple `CREATE MODEL` syntax. Now, let’s create
    the syntax for our dataset and run it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your first machine learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we will now build our first ML model to predict customer churn events.
    As this is our first machine learning model, let’s use the simple `CREATE MODEL`
    command. This option uses Amazon SageMaker Autopilot, which means, without the
    heavy lifting of building ML models, you simply provide a tabular dataset and
    select the target column to predict and SageMaker Autopilot automatically explores
    different solutions to find the best model. This includes data preprocessing,
    model training, and model selection and deployment. AutoMode is the default mode:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Redshift ML shares training data and artifacts between Amazon Redshift and
    SageMaker through an S3 bucket. If you don’t have one already, you will need to
    create an S3 bucket. To do this, navigate to the Amazon S3 console and click on
    the **Create** **bucket** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – S3 console](img/B19071_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – S3 console
  prefs: []
  type: TYPE_NORMAL
- en: On the `serverlessmachinelearningwithredshift-<your account id>`, where `<your
    account id>` is your AWS account number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Creating an S3 bucket](img/B19071_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Creating an S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we send our dataset to the `CREATE MODEL` command, we will split the
    dataset into two parts – one is the training dataset, which is used to train the
    machine learning model, and the other one is for testing the model once it is
    created. We do this by filtering customer records that have `record_date` of less
    than `''2020-08-01''` for training and `record_date` greater than `''2020-07-31''`
    for testing. Run the following queries to check our record split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In *Figure 5**.5*, we can see we have **2714** records in the training set and
    **619** records in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Training and test dataset record count](img/B19071_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Training and test dataset record count
  prefs: []
  type: TYPE_NORMAL
- en: We apply the filtering condition when training and testing the model on our
    dataset. In the next step, we are going to create the model using this filter
    condition on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now run the following code to create `customer_churn_model`. Make sure to replace
    `<your account id>` with the correct AWS account number. Please note that since
    we are going to use simple `CREATE MODEL`, we set the max allowed time through
    the `MAX_RUNTIME` parameter. This is the maximum training time that Autopilot
    will take. We have set it to 1,800 seconds, which is 30 minutes. If you don’t
    specify a value for `MAX_RUNTIME` it will use the default value of 5,400 seconds
    (90 minutes):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us understand more about the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: The `SELECT` query in the `FROM` clause specifies the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TARGET` clause specifies which column is the label for which the `CREATE
    MODEL` statement builds a model to predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other columns in the training query are the features (input) used to predict
    the churn variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `predict_customer_churn` function is the name of an inference function used
    in `SELECT` queries to generate predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`S3_Bucket` is the location where Redshift ML saves artifacts when working
    with SageMaker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having `MAX_RUNTIME` set as 1,800 seconds specifies the maximum time that SageMaker
    will take to train our model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After you run the `CREATE MODEL` command, run the following command to check
    the status of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The Redshift ML `CREATE MODEL` statement is asynchronous, which means that when
    the model is under training, the query shows it is completed and the training
    is happening in Amazon SageMaker. To find out the status of the model, run the
    `SHOW` `MODEL` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see the `SHOW MODEL` output shows **Model
    State** as **TRAINING**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Model State TRAINING](img/B19071_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Model State TRAINING
  prefs: []
  type: TYPE_NORMAL
- en: When the same `SHOW MODEL` command is run after a while, **Model State** is
    displayed as **READY**, which means data processing, model training, model selection,
    and model deployment to Redshift is completed successfully. From the following
    screenshot, you can see that **Model Status** now shows **READY**. You can also
    see the **Estimated Cost** value, which represents Amazon SageMaker training hours.
    This value does not equal the elapsed training time as it is an accumulation of
    training time on the SageMaker instances used.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Model State READY](img/B19071_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Model State READY
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from `SHOW MODEL` command gives you other useful information about the
    model, for example, the query used, **Target Column**, **Model Type**, and **Function
    Name** to use when predicting. You can see that **Model Type** in our example
    is **xgboost**, which tells you that Amazon SageMaker has chosen the XGBoost algorithm
    to build the binary classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Model State READY continuation](img/B19071_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Model State READY continuation
  prefs: []
  type: TYPE_NORMAL
- en: 'If you read further into the output, Redshift ML has done the bulk of the work
    for you, for example, it has selected and set the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem Type** is set to **BinaryClassification**. This is true since our
    target variable has two distinct values in it, true and false. So, this is a binary
    classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation** and **Objective** is set to **F1**. F1 score is a recommended
    approach when evaluating binary scores since it considers both precision and recall.
    Other objectives that SageMaker Autopilot may select for a binary classification
    model are **accuracy** and **area under** **curve** (**AUC**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have created the model successfully as `SELECT` queries. The next sections
    show how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have created the model, let’s dive into the details of its performance.
  prefs: []
  type: TYPE_NORMAL
- en: When building machine learning models, it is very important to understand the
    model performance. You do this to make sure your model is useful and is not biased
    to one class over another and to make sure that the model is not under-trained
    or over-trained, which will mean the model is either not predicting classes correctly
    or is predicting only some instances and not others.
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, Redshift ML provides various objectives to measure
    the performance of the model. It is prudent that we test the model performance
    with the test dataset that we set aside in the previous section. This section
    explains how to review the Redshift ML objectives and also validate the model
    performance with our test data.
  prefs: []
  type: TYPE_NORMAL
- en: Redshift ML uses several objective methods to measure the predictive quality
    of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Redshift ML objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 5**.9* shows the `SHOW MODEL` output. It displays two values that are
    of interest to us. One is **Objective** and the other is **validation:f1_binary**.
    The first value to look at is **Objective**. It is set to **F1** for us. F1 or
    F-score is the most commonly used performance evaluation metric used for classification
    models. It is a measure for validating dataset accuracy. It is calculated from
    the precision and recall of the validations where precision is the number of true
    positive results divided by the number of all positive results included, and recall
    is the number of true positive results divided by the number of all records that
    should have been identified as positive. You can learn more about F-score here:
    [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command in query editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The output in *Figure 5**.9* shows the value of F1 is found in **validation:****f****1_binary**,
    which is **0.90**. The highest possible value for an F1 score is 1 and the lowest
    is 0\. The highest score of 1 would signify perfect precision and recall by a
    model. In our case, it is 90%, which is really good.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Model objective values](img/B19071_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Model objective values
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that the model created by Autopilot has a good F-score and is ready
    to use to predict whether customers are going to churn or not. In the next section,
    we will use the prediction function to generate the prediction values along with
    probability scores.
  prefs: []
  type: TYPE_NORMAL
- en: Running predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s invoke our `predict_customer_churn` and `predict_customer_churn_prob`
    prediction functions through the `SELECT` command. Redshift ML creates two functions
    for us to use:'
  prefs: []
  type: TYPE_NORMAL
- en: One is created with the same name as the one we gave when creating the model,
    in this case, `predict_customer_churn`, which returns the class label or predicted
    value, for example, `0` or `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other function, `predict_customer_churn_prob`, in addition to returning
    the class label or predicted value, also returns the probability that the predicted
    value is correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To test these functions, run the following query. In the following query, you’ll
    notice that we are using two prediction functions inside a `SELECT` command and
    passing all the input columns that were passed when creating the ML model. These
    two functions will return a label and probability score as output. We are also
    testing the prediction function by filtering rows where `record_date` is greater
    than `'2022-07-31'`. Since this is an unseen dataset, it should act as a challenging
    dataset for our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also important to note that all the predictions are happening locally
    on a Redshift cluster. When the `SELECT` query is run, there are no calls made
    to Amazon SageMaker. This makes all predictions free of cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the output in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Running predictions](img/B19071_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Running predictions
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, observe that the `predicted_class` values and `probability_score`
    values for each customer are shown. From the `predicted_class` column, you can
    understand that our model is predicting whether the customer is going to churn
    or not, and from the `probability_score` column, you can understand that the model
    is, for example, for the first row, 99% confident that the customer with account
    ID **415382-4657** is not going to churn.
  prefs: []
  type: TYPE_NORMAL
- en: We have witnessed that prediction is working without any issues. In the next
    section, let’s check how the model is performing compared to ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing ground truth to predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following query to compare actual versus predicted customer churn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the customers where the ML model made a mistake:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Results will vary as each trained model will have slight differences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Incorrect predictions](img/B19071_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Incorrect predictions
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the model predictions and compared them with ground truth. In the
    next section, we will learn about feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`explain_model` functions to retrieve feature importance. This will help you
    to understand which features are strongly related to the target variable, which
    features are important to the model and which are not, and from this you can reduce
    the number of dimensions that you feed into your machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the SQL code that you can run to retrieve the feature importance
    of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The following is the JSON format output of feature importance. You can read
    and understand the importance of each feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Feature importance raw output](img/B19071_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Feature importance raw output
  prefs: []
  type: TYPE_NORMAL
- en: 'For better readability of the feature importance, you may execute the following
    SQL code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.13 – Feature Importance](img/B19071_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Feature Importance
  prefs: []
  type: TYPE_NORMAL
- en: You can use feature importance to understand the relationship between each feature
    and target variable and the features that are not important.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen what features contribute highly to the model, now let’s look at
    how model performance metrics are calculated on our test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s use Redshift SQL to compute a **confusion matrix** to evaluate the performance
    of the classification model. Using a confusion matrix, you can identify true positives,
    true negatives, false positives, and false negatives, based on which various statistical
    measures such as accuracy, precision, recall, sensitivity, specificity, and finally,
    F1 score are calculated. You can read more about the concept of the confusion
    matrix here: [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following query uses a `WITH` clause, which implements a common table expression
    in Redshift. This query has the following three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first part is about the `SELECT` statement within the `WITH` clause, where
    we predict customer churn and save it in memory. This dataset is named `infer_data`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second part, which is below the first `SELECT` statement, reads `infer_data`
    and builds the confusion matrix, and these details are stored in memory in a dataset
    called `confusionmatrix`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the third part of the statement, note that the `SELECT` statement builds
    the model performance metrics such as F1 score, accuracy, recall, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run the following query to build a confusion matrix for the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.14 – Confusion \uFEFFmatrix for the test dataset](img/B19071_05_14.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Confusion matrix for the test dataset
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the `record_date` > `'2020-07-31'`). These records have not been
    seen by the model before, but 97% of the time, the model is able to correctly
    predict the class value. This proves that the model is useful and correctly predicts
    both classes – churn and no churn. This model can now be given to the business
    units so it can be used to proactively predict the customers who are about to
    churn and build marketing campaigns for them.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned how to create your first machine learning
    model using a simple `CREATE MODEL` statement. While doing so, you explored `customer_calls_fact`
    table data using query editor v2, learned about the basic syntax of the `CREATE
    MODEL` statement, created a simple ML model, learned how to read the model’s output,
    and finally, used Redshift SQL to compute some of the model evaluation metrics
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will use the basics that you have learned in this chapter
    to build various classification models using Redshift ML.
  prefs: []
  type: TYPE_NORMAL
