<html><head></head><body>
		<div id="_idContainer068">
			<h1 id="_idParaDest-81"><a id="_idTextAnchor104"/>Chapter 5: Centralized Feature Repository with Amazon SageMaker Feature Store</h1>
			<p>Let's begin with the basic questions – what is a feature store and why is it necessary? A feature store<a id="_idIndexMarker172"/> is a repository that persists engineered features. A lot of time goes into feature engineering, sometimes involving multi-step data processing pipelines executed over hours of compute time. ML models depend on these engineered features that often come from a variety of data sources. A feature store accelerates this process by reducing repetitive data processing that is required to convert raw data into features. A feature store not only allows you to share engineered features during model-building activities, but also allows consistency in using engineered features for inference. </p>
			<p><strong class="bold">Amazon SageMaker Feature Store</strong> is a <a id="_idIndexMarker173"/>managed repository with capabilities to store, update, retrieve, and share features. SageMaker Feature Store provides the ability to reuse the engineered features in two different scenarios. First, the features can be shared between the training and inference phases of a single ML project resulting in consistent model inputs and reduced training-serving skew. Second, features from SageMaker </p>
			<p>Feature Store can also be shared across multiple ML projects, leading to improved data scientist productivity.</p>
			<p>By the end of this chapter, you will be able to use Amazon SageMaker Feature Store capabilities and apply best practices to implement solutions to address the challenges of reducing data processing time and architecting features for near real-time ML inferences.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Basic concepts of Amazon SageMaker Feature Store</li>
				<li>Creating reusable features to reduce feature inconsistencies and inference latency</li>
				<li>Designing solutions for near real-time ML predictions</li>
			</ul>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor105"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you have not set up the data science environment yet, please refer to <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Science Environments</em>, which provides a walk-through of the setup process.</p>
			<p>Code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter05">https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter05</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor106"/>Amazon SageMaker Feature Store essentials </h1>
			<p>In this section, you will<a id="_idIndexMarker174"/> learn the basic terminology and capabilities of Amazon SageMaker Feature Store. Amazon SageMaker Feature Store provides a centralized repository with capabilities to store, update, retrieve, and share features. Scalable storage and near real-time feature retrieval are at the heart of Amazon SageMaker Feature Store. Utilizing Amazon SageMaker Feature Store involves three high-level steps, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B17249_05_01.jpg" alt="Figure 5.1 – High-level steps with Amazon SageMaker Feature Store&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – High-level steps with Amazon SageMaker Feature Store</p>
			<p>Let's see what is involved in each of these steps in a bit more detail.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor107"/>Creating feature groups</h1>
			<p>In Amazon SageMaker<a id="_idIndexMarker175"/> Feature Store, features are stored in a collection called a <strong class="bold">feature group</strong>. A feature group, in turn, is composed of records of features and feature values. Each record is a collection of feature values, identified by a unique <strong class="source-inline">RecordIdentifier</strong> value. Every record belonging to a feature group will use the same feature as <strong class="source-inline">RecordIdentifier</strong>. For example, the record identifier for the feature store created for the weather data could be <strong class="source-inline">parameter_id</strong> or <strong class="source-inline">location_id</strong>. Think of <strong class="source-inline">RecordIdentifier</strong> as a primary key for the feature group. Using this primary key, you can query feature groups for the fast lookup of features. It's also important to note that each record of a feature group must, at a minimum, contain a <strong class="source-inline">RecordIdentifier</strong> and an event time feature. The event time feature is identified by <strong class="source-inline">EventTimeFeatureName</strong> when a feature group is set up. When a feature record is ingested into a feature group, SageMaker adds three features – <strong class="source-inline">is_deleted</strong>, <strong class="source-inline">api_invocation</strong> time, and <strong class="source-inline">write_time</strong> – for each feature record. <strong class="source-inline">is_deleted</strong> is used to manage the deletion of records, <strong class="source-inline">api_invocation_time</strong> is the time when the API call is invoked to write a record to a feature store, and <strong class="source-inline">write_time</strong> is the time when the feature record is persisted to the offline store. </p>
			<p><em class="italic">Figure 5.2</em> shows a high-level view of how a <a id="_idIndexMarker176"/>feature store is structured: </p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B17249_05_02.jpg" alt="Figure 5.2 – Amazon SageMaker feature store structure&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Amazon SageMaker feature store structure</p>
			<p>While each feature group is managed and scaled independently, you can search and discover features from multiple feature groups as long as the appropriate access is in place.</p>
			<p>When you create a feature store group with SageMaker, you can choose to enable an offline store, online store, or both. When both online and offline stores are enabled, the service replicates the online store contents into the offline store maintained in Amazon S3. </p>
			<p>The following code blocks show<a id="_idIndexMarker177"/> the process of creating a feature store: </p>
			<ol>
				<li>First define the feature group name:<p class="source-code">#Feature group name</p><p class="source-code">weather_feature_group_name_offline = 'weather-feature-group-offline' + strftime('%d-%H-%M-%S', gmtime())</p></li>
				<li>Then, create the feature definitions that capture the feature name and the type:<p class="source-code">##Create FeatureDefinitions</p><p class="source-code">fd_location=FeatureDefinition(feature_name='location', feature_type=FeatureTypeEnum('Integral'))</p><p class="source-code">fd_event_time=FeatureDefinition(feature_name='EventTime', feature_type=FeatureTypeEnum('Fractional'))</p><p class="source-code">…</p><p class="source-code">weather_feature_definitions = []</p><p class="source-code">weather_feature_definitions.append(fd_location)</p><p class="source-code">weather_feature_definitions.append(fd_event_time)</p><p class="source-code">…</p></li>
				<li>Next, define the record identifier feature:<p class="source-code">##Define unique identifier</p><p class="source-code">record_identifier_feature_name = "location"</p></li>
				<li>Finally, create the feature group using the <strong class="source-inline">create()</strong> API, which, by default, creates a feature group<a id="_idIndexMarker178"/> with an offline store:<p class="source-code">#Create offline feature group</p><p class="source-code">weather_feature_group_offline =     \ </p><p class="source-code">    FeatureGroup(name=weather_feature_group_name_offline,</p><p class="source-code">         feature_definitions=weather_feature_definitions,</p><p class="source-code">                 sagemaker_session=sagemaker_session) </p><p class="source-code">weather_feature_group_offline.create(</p><p class="source-code">             s3_uri=f"s3://{s3_bucket_name}/{prefix}",</p><p class="source-code">            record_identifier_name="location",</p><p class="source-code">            event_time_feature_name="EventTime",</p><p class="source-code">            role_arn=role</p><p class="source-code">)</p></li>
				<li>To enable an online store in addition to an offline store, use <strong class="source-inline">enable_online_store</strong>, as shown in the following code:<p class="source-code">weather_feature_group_offline_online.create(</p><p class="source-code">            s3_uri=f"s3://{s3_bucket_name}/{prefix}",</p><p class="source-code">    record_identifier_name="location", </p><p class="source-code">           event_time_feature_name="EventTime",</p><p class="source-code">           role_arn=role,</p><p class="source-code">          enable_online_store=True</p><p class="source-code">)</p></li>
				<li>To create a feature group with only an online store enabled, set <strong class="source-inline">s3_uri</strong> to <strong class="source-inline">False</strong>, as shown in the<a id="_idIndexMarker179"/> following code:<p class="source-code">weather_feature_group_online.create(</p><p class="source-code">            s3_uri=False,</p><p class="source-code">            record_identifier_name="location", </p><p class="source-code">           event_time_feature_name="EventTime",</p><p class="source-code">           role_arn=role,</p><p class="source-code">           enable_online_store=True</p><p class="source-code">)</p></li>
			</ol>
			<p>Note that you can also <a id="_idIndexMarker180"/>create a feature group using <strong class="bold">SageMaker Studio</strong>. Once feature groups are created either using the APIs or<a id="_idIndexMarker181"/> SageMaker Studio, you can view them along with their status in SageMaker Studio. <em class="italic">Figure 5.3</em> shows a list of feature groups in SageMaker Studio:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B17249_05_03.jpg" alt="Figure 5.3 – Feature groups list in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Feature groups list in SageMaker Studio</p>
			<p>To wrap up the feature group<a id="_idIndexMarker182"/> creation discussion, the following table summarizes the differences between the online and offline feature stores:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B17249_05_04.jpg" alt="Figure 5.4 – Comparison of online and offline feature stores&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Comparison of online and offline feature stores</p>
			<p>Now that you can create feature groups in the feature store, let's take a look at how to populate them. </p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor108"/>Populating feature groups</h1>
			<p>After <a id="_idIndexMarker183"/>creating the feature groups, you will populate them with features. You can ingest features into a feature group using either <strong class="bold">batch ingestion</strong> or <strong class="bold">streaming ingestion</strong>, as shown in <em class="italic">Figure 5.5</em>: </p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B17249_05_05.jpg" alt="Figure 5.5 – Ingesting features into feature groups&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Ingesting features into feature groups</p>
			<p>To ingest features<a id="_idIndexMarker184"/> into the feature store, you create a feature pipeline that can populate the feature store. A <strong class="bold">feature pipeline</strong> can include<a id="_idIndexMarker185"/> any service or capability that accepts raw data and then transforms that raw data into engineered features and puts the features in a designated feature group. Features can be ingested either in bulk in batches or streamed individually. The <strong class="source-inline">PutRecord</strong> API call is the core SageMaker API for <a id="_idIndexMarker186"/>ingesting features. This is used for both online and offline feature stores as well as ingesting through batch or streaming methods. </p>
			<p>The following code block shows the usage of the <strong class="source-inline">PutRecord</strong> API:</p>
			<p class="source-code">##Create a record to ingest into the feature group</p>
			<p class="source-code">record = []</p>
			<p class="source-code">event_time_feature = {'FeatureName': 'EventTime','ValueAsString': str(int(round(time.time())))}</p>
			<p class="source-code">location_feature =   {'FeatureName': 'location','ValueAsString': str('200.0')}</p>
			<p class="source-code">ismobile_feature =   {'FeatureName':   'ismobile','ValueAsString': str('0')}</p>
			<p class="source-code">value_feature ={'FeatureName': 'value','ValueAsString': str('34234.0')}</p>
			<p class="source-code"> </p>
			<p class="source-code">record.append(event_time_feature)</p>
			<p class="source-code">record.append(location_feature)</p>
			<p class="source-code">record.append(ismobile_feature)</p>
			<p class="source-code">record.append(value_feature)</p>
			<p class="source-code"> </p>
			<p class="source-code">response = sagemaker_fs_runtime_client.put_record(</p>
			<p class="source-code">       FeatureGroupName=weather_feature_group_online,</p>
			<p class="source-code">                                             Record=record)</p>
			<p>You can also use a wrapper API, <strong class="source-inline">fg.ingest</strong>, which takes in a pandas <strong class="source-inline">dataframe</strong> as input and allows you to<a id="_idIndexMarker187"/> configure multiple workers and processes to ingest features in parallel. The following code block shows how to use the <strong class="source-inline">ingest()</strong> API:</p>
			<p class="source-code">#Read csv directly from S3 into a dataframe</p>
			<p class="source-code">weather_df = pd.read_csv(s3_path)</p>
			<p class="source-code"> </p>
			<p class="source-code">#Ingest features into the feature group</p>
			<p class="source-code">weather_feature_group_offline.ingest(</p>
			<p class="source-code">          data_frame=weather_df, max_workers=3, wait=True</p>
			<p class="source-code">)</p>
			<p>For batch ingestion, you can<a id="_idIndexMarker188"/> author features (for example, using <strong class="bold">Amazon Data Wrangler</strong>) and ingest features in batches using a <strong class="bold">SageMaker Processing</strong> job. This allows batch ingestion into the offline store and the online store. For streaming ingestion, records can be pushed synchronously using the <strong class="source-inline">PutRecord</strong> API call. When ingesting records to the online feature store, you maintain only the latest feature values for a given record identifier. Historical values are only maintained in the replicated offline store if the<a id="_idIndexMarker189"/> feature group is configured for both online and offline stores. <em class="italic">Figure 5.6</em> outlines the <a id="_idIndexMarker190"/>methods to ingest features as they relate to the online and offline feature stores:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B17249_05_06.jpg" alt="Figure 5.6 – Ingesting feature store records&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Ingesting feature store records</p>
			<p>With the ingestion APIs in hand, let's <a id="_idIndexMarker191"/>take a look at a generic batch ingestion architecture. <em class="italic">Figure 5.7</em> shows the architecture for batch ingestion with <strong class="bold">Amazon SageMaker Processing</strong>: </p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B17249_05_07.jpg" alt="Figure 5.7 – Batch ingestion with SageMaker Processing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Batch ingestion with SageMaker Processing</p>
			<p>Here are the high-level steps<a id="_idIndexMarker192"/> involved in the batch ingestion architecture:</p>
			<ol>
				<li value="1">Bulk raw data is available in an S3 bucket. </li>
				<li>The Amazon SageMaker Processing job takes raw data as input and applies feature engineering techniques to the data. The processing job can be configured to run on a distributed cluster of instances to process data at scale. </li>
				<li>The processing job also ingests the engineered features ingested into the online store of the feature group, using the <strong class="source-inline">PutRecord</strong> API. Features are then automatically replicated to the offline store of the feature group.</li>
				<li>Features from the offline store can then be used for training other models and by other data science teams to address a wide variety of other use cases. Features from the <a id="_idIndexMarker193"/>online store can be used for feature lookup during real-time predictions.</li>
			</ol>
			<p>Note that if the feature store used in this architecture is offline only, the processing job can directly write into the offline store using the <strong class="source-inline">PutRecord</strong> API.</p>
			<p>Next, let's take a look at a possible streaming ingestion architecture in <em class="italic">Figure 5.8</em>. This should look very similar to batch ingestions, except instead of using a processing job, you use a single compute instance or an <strong class="bold">AWS Lambda function</strong>:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B17249_05_08.jpg" alt="Figure 5.8 – Streaming ingestion with AWS Lambda &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Streaming ingestion with AWS Lambda </p>
			<p>Here are the high-level steps<a id="_idIndexMarker194"/> involved in the streaming ingestion architecture:</p>
			<ol>
				<li value="1">Raw data lands in an S3 bucket, which triggers an AWS Lambda function.</li>
				<li>The Lambda function processes data and inserts features into the online store of the feature group, using the <strong class="source-inline">PutRecord</strong> API.</li>
				<li>Features are then automatically replicated to the offline store of the feature group.</li>
				<li>Features from the <a id="_idIndexMarker195"/>offline store can then be used for training other models and by other data science teams to address a wide variety of other use cases. Features from the online store can be used for feature lookup during real-time predictions.</li>
			</ol>
			<p>In addition to using the ingestion APIs to populate the offline store, you can populate the underlying S3 bucket directly. If you don't have a need for real-time inference and have huge volumes of historical feature data (terabytes or even hundreds of gigabytes) that you want to migrate to an offline feature store to be used for training models, you can directly upload them to the underlying S3 bucket. To do this effectively, it is important to understand the S3 folder structure of the offline bucket. Feature groups in the offline store are organized in the structure <strong class="source-inline">s3</strong>:</p>
			<p class="source-code">s3://&lt;bucket-name&gt;/&lt;customer-prefix&gt;/&lt;account-id&gt;/sagemaker/&lt;aws-region&gt;/offline-store/&lt;feature-group-name&gt;-&lt;feature-group-creation-time&gt;/data/year=&lt;event-time-year&gt;/month=&lt;event-time-month&gt;/day=&lt;event-time-day&gt;/hour=&lt;event-time-hour&gt;/&lt;timestamp_of_latest_event_time_in_file&gt;_&lt;16-random-alphanumeric-digits&gt;.parquet</p>
			<p>Also note that, when you use ingestion APIs, the features <strong class="source-inline">isdeleted</strong>, <strong class="source-inline">api_invocation_time</strong>, and <strong class="source-inline">write-time</strong> are included automatically in the feature record, but when you write directly to the offline store, you are responsible for including them. </p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor109"/>Retrieving features from feature groups</h1>
			<p>Once feature groups<a id="_idIndexMarker196"/> are populated, to retrieve features from the feature store, there are two APIs available – <strong class="source-inline">get_record</strong> and <strong class="source-inline">batch_get_record</strong>. The following code block shows retrieving a single record from a feature group using the <strong class="source-inline">get_record</strong> API:</p>
			<p class="source-code">record_identifier_value = str('300')</p>
			<p class="source-code">response = sagemaker_fs_runtime_client.get_record</p>
			<p class="source-code">(FeatureGroupName=weather_feature_group_name_online,</p>
			<p class="source-code">RecordIdentifierValueAsString=record_identifier_value)</p>
			<p class="source-code">response</p>
			<p class="source-code">Response from the code block looks similar to the following figure:</p>
			<p class="source-code">{'ResponseMetadata': {'RequestId': '195debf2-3b10-4116-98c7-142dc13e9df3',</p>
			<p class="source-code">  'HTTPStatusCode': 200,</p>
			<p class="source-code">  'HTTPHeaders': {'x-amzn-requestid': '195debf2-3b10-4116-98c7-142dc13e9df3',</p>
			<p class="source-code">   'content-type': 'application/json',</p>
			<p class="source-code">   'content-length': '214',</p>
			<p class="source-code">   'date': 'Wed, 14 Jul 2021 04:27:11 GMT'},</p>
			<p class="source-code">  'RetryAttempts': 0},</p>
			<p class="source-code"> 'Record': [{'FeatureName': 'value', 'ValueAsString': '4534.0'},</p>
			<p class="source-code">  {'FeatureName': 'ismobile', 'ValueAsString': '0'},</p>
			<p class="source-code">  {'FeatureName': 'location', 'ValueAsString': '300'},</p>
			<p class="source-code">  {'FeatureName': 'EventTime', 'ValueAsString': '1626236799'}]}</p>
			<p>Similarly, the <a id="_idIndexMarker197"/>following code shows retrieving multiple records from one or more feature groups using the <strong class="source-inline">batch_get_record</strong> API:</p>
			<p class="source-code">record_identifier_values = ["200", "250", "300"]</p>
			<p class="source-code">response=sagemaker_fs_runtime_client.batch_get_record(</p>
			<p class="source-code">            Identifiers=[</p>
			<p class="source-code">           {"FeatureGroupName": weather_feature_group_name_online, "RecordIdentifiersValueAsString": record_identifier_values}</p>
			<p class="source-code">            ]</p>
			<p class="source-code">)</p>
			<p class="source-code">response</p>
			<p>The response from <a id="_idIndexMarker198"/>the code block should look similar to the following response:</p>
			<p class="source-code">{'ResponseMetadata': {'RequestId': '3c3e1f5f-3a65-4b54-aa18-8683c83962c5',</p>
			<p class="source-code">  'HTTPStatusCode': 200,</p>
			<p class="source-code">  'HTTPHeaders': {'x-amzn-requestid': '3c3e1f5f-3a65-4b54-aa18-8683c83962c5',</p>
			<p class="source-code">   'content-type': 'application/json',</p>
			<p class="source-code">   'content-length': '999',</p>
			<p class="source-code">   'date': 'Wed, 14 Jul 2021 04:29:47 GMT'},</p>
			<p class="source-code">  'RetryAttempts': 0},</p>
			<p class="source-code"> 'Records': [{'FeatureGroupName': 'weather-feature-group-online-13-19-23-46',</p>
			<p class="source-code">   'RecordIdentifierValueAsString': '300',</p>
			<p class="source-code">   'Record': [{'FeatureName': 'value', 'ValueAsString': '4534.0'},</p>
			<p class="source-code">           {'FeatureName': 'ismobile', 'ValueAsString': '0'},</p>
			<p class="source-code">           {'FeatureName': 'location', 'ValueAsString': '300'},</p>
			<p class="source-code">          {'FeatureName': 'EventTime', 'ValueAsString': '1626236799'}]},</p>
			<p class="source-code">  {'FeatureGroupName': 'weather-feature-group-online-13-19-23-46',</p>
			<p class="source-code">   'RecordIdentifierValueAsString': '200',</p>
			<p class="source-code">   'Record': [{'FeatureName': 'value', 'ValueAsString': '34234.0'},</p>
			<p class="source-code">            {'FeatureName': 'ismobile', 'ValueAsString': '0'},</p>
			<p class="source-code">           {'FeatureName': 'location', 'ValueAsString': '200'},</p>
			<p class="source-code">          {'FeatureName': 'EventTime', 'ValueAsString': '1626236410'}]}],</p>
			<p class="source-code"> 'Errors': [],</p>
			<p class="source-code"> 'UnprocessedIdentifiers': []}</p>
			<p>The <strong class="source-inline">get_record</strong> and <strong class="source-inline">batch_get_record</strong> APIs should be used with online stores. Additionally, since the underlying storage for an offline store is an S3 bucket, you can query the offline store directly using Athena or other ways of accessing S3. The following code shows<a id="_idIndexMarker199"/> a sample Athena query that retrieves all feature records directly from the S3 bucket supporting the offline store:</p>
			<p class="source-code">weather_data_query = weather_feature_group.athena_query()</p>
			<p class="source-code">weather_table = weather_data_query.table_name</p>
			<p class="source-code"> </p>
			<p class="source-code">#Query string</p>
			<p class="source-code">query_string = 'SELECT * FROM "'+ weather_table + '"'</p>
			<p class="source-code">print('Running ' + query_string)</p>
			<p class="source-code"> </p>
			<p class="source-code">#run Athena query. The output is loaded to a Pandas dataframe.</p>
			<p class="source-code">weather_data_query.run(query_string=query_string, output_location='s3://'+s3_bucket_name+'/'+prefix+'/query_results/')</p>
			<p class="source-code">weather_data_query.wait()</p>
			<p class="source-code">dataset = weather_data_query.as_dataframe()</p>
			<p>For the dataset <a id="_idIndexMarker200"/>used in this book, we will use two feature groups – location and weather data. The location feature group will have <strong class="source-inline">location_id</strong> as the record identifier and capture features related to the location such as the city name. The weather data feature group will also have <strong class="source-inline">location_id</strong> as the record identifier and capture weather quality measurements such as pm25. This allows us to use the feature groups across multiple ML projects. </p>
			<p>For example, features from both location and weather data feature groups are used for a regression model to predict future weather measurements for a given location. On the other hand, features from the weather data feature group can also be used for a clustering model to find stations with similar measurements.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The example notebook provides a walk-through of the key Amazon SageMaker Feature Store APIs for creating a feature group, ingesting features into feature groups, and retrieving features from a feature group. To see all the feature store capabilities in action, we recommend that you execute the sample notebook in the data science environment you set up in <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Science Environments</em>:</p>
			<p class="callout"><a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/feature_store_apis.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/feature_store_apis.ipynb</a>. </p>
			<p>Now that you have learned the capabilities of SageMaker Feature Store, in the next two sections, you will learn how to use these capabilities to solve feature design challenges that data scientists and organizations face.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor110"/>Creating reusable features to reduce feature inconsistencies and inference latency</h1>
			<p>One of the <a id="_idIndexMarker201"/>challenges data scientists face is the long data processing time – hours and sometimes days – necessary for preparing features to be used for ML training. Additionally, the data processing steps applied in feature engineering need to be applied to the inference requests during prediction time, which increases the inference latency. Each data science team will need to spend this data processing time even when they use the same raw data for different models. In this section, we will discuss best practices to address these challenges by using Amazon SageMaker Feature Store.</p>
			<p>For use cases that require low latency features for inference, an online feature store should be configured, and it's generally recommended to enable both the online and offline feature store. A feature store enabled with both online and offline stores allows you to reuse the same feature values for the training and inference phases. This configuration reduces the inconsistencies between the two phases and minimizes training and inference skew. In this mode, to populate the store, ingest features into the online store either using batch or streaming. </p>
			<p>As you ingest features into an online store, SageMaker automatically replicates feature values to an offline store, continuously appending the latest values. It's important to note that for the online feature store, only the most current feature record is maintained and the <strong class="source-inline">PutRecord</strong> API is always processed as <strong class="source-inline">insert</strong>/<strong class="source-inline">upsert</strong>. This is key because if you need to update a feature record, the process to do so is to re-insert or overlay the existing record. This is to allow the retrieval of features with the minimum possible latency for inference use cases. </p>
			<p>Although the online feature store maintains only the latest record, the offline store will provide a full history of feature values over time. Records will stay in the offline store until they are explicitly removed. As a result, you should establish a process to prune unnecessary records in the offline feature store using the standard mechanisms provided for S3 archival. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The example notebook from the GitHub repository shows the end-to-end flow of creating a feature store, ingesting features, retrieving features, and further using the features for training the model, deploying the model, and using the features from the feature store during inference: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH04/feature_store_train_deploy_models.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH04/feature_store_train_deploy_models.ipynb</a>. </p>
			<p>Another best<a id="_idIndexMarker202"/> practice is to set up standards for versioning features. As features evolve, it is important to keep track of feature versions. Consider versioning at two levels – versions of the feature group itself and versions of features within a feature group. You need to create a new version of the feature group for when the schema of the features change, such as when feature definitions need to be added or deleted.</p>
			<p>At the time of this book's publication, feature groups are immutable. To add or remove features, you will need to create a new feature group. To address the requirement of multiple versions of a feature group with different numbers of features, establish and stick to naming conventions. For example, you could create a <strong class="source-inline">weather-conditions-v1</strong> feature group initially. When that feature group needs to be updated, you can create a new <strong class="source-inline">weather-conditions-v2</strong> feature group. You can also consider adding descriptive labels on data readiness or usage, such as <strong class="source-inline">weather-conditions-latest-v2</strong> or <strong class="source-inline">weather-conditions-stable-v2</strong>. You also can tag feature groups to provide metadata. Additionally, you should also establish standards for how many concurrent versions to support and when to deprecate old versions.</p>
			<p>For the versioning of the individual features, the offline store keeps a history of all values of the features in a feature group. Each feature record is required to have an <strong class="source-inline">eventTime</strong>, which supports the ability to access feature versions by date. To retrieve previous version values of features from the offline store, use an Athena query with a specific timestamp, as shown in the following code block: </p>
			<p class="source-code">#Query string with specific date/time</p>
			<p class="source-code">timestamp = int(round(time.time()))</p>
			<p class="source-code">time_based_query_string = f"""</p>
			<p class="source-code">SELECT *</p>
			<p class="source-code">FROM "{weather_table}"</p>
			<p class="source-code">where eventtime &lt;= {timestamp} and city=1080.0</p>
			<p class="source-code">"""</p>
			<p class="source-code"># Run Athena query. The output is loaded to a Pandas dataframe.</p>
			<p class="source-code">weather_query.run(query_string=time_based_query_string, output_location='s3://'+s3_bucket_name+'/'+prefix+'/query_results/')</p>
			<p class="source-code">weather_query.wait()</p>
			<p class="source-code">dataset = weather_query.as_dataframe()</p>
			<p>Note that you can further fine-tune the Athena query to include <strong class="source-inline">write-time</strong> and <strong class="source-inline">api_call_time</strong> to extract very specific versions of the features. Please see the references section for a link to a detailed blog on point-in-time queries with SageMaker Feature Store. </p>
			<p>Additionally, when a <a id="_idIndexMarker203"/>record is deleted from the online store, the corresponding record in the offline store is only logically deleted, which is typically referred to as a tombstone. When you query the offline store, you may see a tombstone in the results. Use the <strong class="source-inline">is_deleted</strong> feature of the record to filter these records from the results. </p>
			<p>Now that you have the feature groups created and populated, how do teams in your organization discover and reuse the features? All authorized users of the Amazon SageMaker Feature Store can view and browse through a list of feature groups in a feature store in a SageMaker Studio environment. You can also search for specific feature groups by name, description, record identifier, creation date, and tags, as shown in <em class="italic">Figure 5.9</em>:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B17249_05_09.jpg" alt="Figure 5.9 – Search and discover feature groups&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Search and discover feature groups</p>
			<p>You can go a step <a id="_idIndexMarker204"/>further, view feature definitions of the feature group, and search for specific features as shown in <em class="italic">Figure 5.10</em>:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B17249_05_10.jpg" alt="Figure 5.10 – Search and discover features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – Search and discover features</p>
			<p>In the next section, you will learn about designing an ML system that provides near real-time predictions.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor111"/>Designing solutions for near real-time ML predictions</h1>
			<p>Sometimes <a id="_idIndexMarker205"/>machine learning applications demand high-throughput updates to features and near real-time access to the updated features. Timely access to fast-changing features is critical for the accuracy of predictions made by these applications. As an example, consider a machine learning application in a call center that predicts how to route the incoming customer calls to available agents. This application needs to have knowledge of the customer's latest web session clicks to make accurate routing decisions. If you capture a customer's web-click behavior as features, the features need to be updated instantly and the application needs access to the updated features in near-real time. Similarly, for weather prediction problems, you may want to capture the weather measurement features frequently for accurate weather predictions and need the ability to look up features in real time.</p>
			<p>Let's look at some best practices in designing a reliable solution that meets the requirement of high-throughput writes and low-latency reads. At a high level, this solution will couple streaming ingestion into a feature group with streaming predictions. We will discuss the best practices to apply to ingestion into and serving from a feature store.</p>
			<p>For ingesting features, the decision to choose between batch and streaming ingestion should be based on how often feature values in the feature store need to be updated for use by downstream training or inference. While simple machine models may need features from a single feature group, if you are working with data from multiple sources, you will find yourself using features from multiple feature groups. Some of these features need to be updated on a periodic basis (hourly, daily, weekly) and others must be streamed in near-real time. </p>
			<p>Feature update frequency and inference access patterns should also be used as a consideration for creating different feature groups and isolating features. By isolating features that need to be inserted on different schedules, the ingestion throughput for streaming features can be improved independently. However, retrieving values from multiple feature groups increases the number of API calls and can increase overall retrieval times.</p>
			<p>Your solution needs to balance feature isolation and retrieval performance. If your models require features from a large number of different feature groups at inference, design the solution to utilize larger feature groups or to retrieve from the feature store in parallel to meet the near real-time SLAs for predictions. For example, if your model requires features from three feature groups for inference, you can issue three API calls to get the feature record data in parallel before merging that data for model inference. This can be done through a typical inference workflow executing through an AWS service such <a id="_idIndexMarker206"/>as <strong class="bold">AWS Step Functions</strong>. Optionally, if that same set of features are always used together for inference, you may want to consider <a id="_idIndexMarker207"/>combining those into a single feature group. </p>
			<p><em class="italic">Figure 5.11</em> shows the end-to-end architecture for streaming ingestion and streaming inferences to support high-throughput writes and low-latency reads:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B17249_05_11.jpg" alt="Figure 5.11 – End-to-end architecture for real-time feature ingestion and retrieval&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – End-to-end architecture for real-time feature ingestion and retrieval</p>
			<p>Here are the high-level steps involved in this architecture:</p>
			<p>On the ingestion side:</p>
			<ol>
				<li value="1">The client application collects and processes the live data. For streaming applications, one option is to <a id="_idIndexMarker208"/>use <strong class="bold">Kinesis Data Streams</strong>. To ingest features, the client application calls an ingestion API hosted by an API Gateway.</li>
				<li>An API Gateway<a id="_idIndexMarker209"/> invokes the lambda function that uses the <strong class="source-inline">put_record</strong> API to push features into the online feature store. As necessary, the lambda function can also perform additional processing on the raw data before pushing features to the feature store. </li>
			</ol>
			<p>On the prediction side:</p>
			<ol>
				<li value="1">A model-consuming client application calls a prediction API hosted by an API Gateway. An API Gateway invokes a lambda function that looks up the features related to inference requests from the online feature store and creates an enhanced request. </li>
				<li>The enhanced request is sent to the SageMaker deployed endpoint. The prediction from the endpoint traverses back to the client application.</li>
			</ol>
			<p>Using these techniques and best practices, you can design real-time ML systems.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor112"/>Summary</h1>
			<p>In this chapter, you reviewed the basic capabilities of Amazon SageMaker Feature Store along with the APIs to use. By combining different capabilities, you learned how to reuse engineered features across training and inference phases of a single machine learning project and across multiple ML projects. Finally, you combined streaming ingestion and serving to design near real-time inference solutions. In the next chapter, you will use these engineered features to train and tune machine learning models at scale.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor113"/>References </h1>
			<p>For additional reading material, please review these references:</p>
			<ul>
				<li>Using streaming ingestion with Amazon SageMaker Feature Store to make ML-backed decisions in near-real time:<p><a href="https://aws.amazon.com/blogs/machine-learning/using-streaming-ingestion-with-amazon-sagemaker-feature-store-to-make-ml-backed-decisions-in-near-real-time/ ">https://aws.amazon.com/blogs/machine-learning/using-streaming-ingestion-with-amazon-sagemaker-feature-store-to-make-ml-backed-decisions-in-near-real-time/</a></p></li>
				<li>Enable feature reuse across accounts and teams using Amazon SageMaker Feature Store: <p><a href="https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/ ">https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-f<span id="_idTextAnchor114"/>eature-store/</a></p></li>
				<li>Build accurate ML training datasets using point-in-time queries with Amazon SageMaker Feature Store and Apache Spark:<p><a href="https://aws.amazon.com/blogs/machine-learning/build-accurate-ml-training-datasets-using-point-in-time-queries-with-amazon-sagemaker-feature-store-and-apache-spark/ ">https://aws.amazon.com/blogs/machine-learning/build-accurate-ml-training-datasets-using-point-in-time-queries-with-amazon-sagemaker-feature-store-and-<span id="_idTextAnchor115"/>apache-spark/</a></p></li>
				<li>Ingesting historical feature data into Amazon SageMaker Feature Store:<p><a href="https://towardsdatascience.com/ingesting-historical-feature-data-into-sagemaker-feature-store-5618e41a11e6">https://towardsdatascience.com/ingesting-historical-feature-data-into-sagemaker-feature-store-5618e41a11e6</a></p></li>
			</ul>
		</div>
	</body></html>