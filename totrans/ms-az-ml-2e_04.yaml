- en: '*Chapter 3*: Preparing the Azure Machine Learning Workspace'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to navigate different Azure services
    for implementing ML solutions in the cloud. We realized that the best service
    for training custom ML models programmatically and automating infrastructure and
    deployments is the Azure Machine Learning service. In this chapter, we will set
    up and explore the Azure Machine Learning workspace, create a cloud training cluster,
    and perform data experimentation locally and on cloud compute, while collecting
    all the artifacts of the ML runs in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we will learn how to manage Azure resources using different
    tools such as the Azure **Command-Line Interface (CLI)**, the Azure SDKs, and
    **Azure Resource Manager (ARM)** templates. We will set up and explore the Azure
    CLI, as well as Azure Machine Learning extensions, and subsequently deploy an
    Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: We will then look under the hood of Azure Machine Learning by exploring the
    resources that were deployed as part of Azure Machine Learning, such as the storage
    account, Azure Key Vault, Azure Application Insights, and Azure Container Registry.
    Following that, we will dive into Azure Machine Learning and explore the workspace
    to better understand the individual components.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the last section, we will put all this knowledge into practice and
    run our first experiment with Azure Machine Learning. After setting up our environment,
    we will enhance a simple ML Keras training script to log metrics, logs, models,
    and code snapshots into Azure Machine Learning. We will then progress to schedule
    training runs on our local machine as well as on a training cluster in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will see all your successful training runs,
    metrics, and tracked models in your Azure Machine Learning workspace, and you
    will have a good understanding of Azure Machine Learning to start your ML journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an Azure Machine Learning workspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Azure Machine Learning service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running ML experiments with Azure Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    perform and manage experiment runs on Azure Machine Learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-core 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-widgets 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow 2.6.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can run this code using either a local Python interpreter or a notebook
    environment hosted in Azure Machine Learning. However, some scripts need to be
    scheduled to execute in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter03](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter03).'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an Azure Machine Learning workspace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can start delving deep into ML on Azure itself, we need to understand
    how to deploy an Azure Machine Learning workspace or Azure services in general,
    what tooling is supported, and which one of those we will use to work with throughout
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we will require an Azure subscription.
  prefs: []
  type: TYPE_NORMAL
- en: If you are working in an organization and want to use your work account, you
    can go to portal.azure.com and log in with your work account. If the login works,
    you will land on the portal itself, and your work account is shown at the top
    right. This means that your company already has an **Azure Active Directory**
    (**AAD**) instance set up. In this case, talk to your Azure Global Administrator,
    if you haven't already, to discuss which Azure subscription to use for your purpose.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to Azure and want to use your private account, go to azure.com
    and click on **Free Account** to create an AAD for yourself with a free trial
    subscription. This trial gives you a certain amount of money to spend for 30 days
    on Azure services.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, in the end, you should have the capability to log in to the Azure
    portal with your identity, and you should know which Azure subscription (name
    and/or subscription ID) you want to deploy your ML services to.
  prefs: []
  type: TYPE_NORMAL
- en: With this all done, we will now have a look at how to deploy and manage our
    Azure environment in general and what options and tooling there are to choose
    from.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the available tooling for Azure deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Azure, any action that deploys or changes an Azure service goes through
    the so-called ARM. As shown in *Figure 3.1*, ARM accepts requests from either
    the **Azure portal**, **Azure PowerShell** (a PowerShell extension), the **Azure
    CLI**, or the **Azure REST API**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Azure Resource Manager ](img/B17928_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Azure Resource Manager
  prefs: []
  type: TYPE_NORMAL
- en: In the Azure portal, you can select `machine learning`, the set of results set
    will show a service called **Machine Learning** from Microsoft. Clicking on this
    card and then **Create** will open the deployment wizard for this service. This
    will give you a sense of what is required to deploy this service.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we will not go any further on the portal itself, as we want to facilitate
    a more programmatic approach in this book. Using this approach will greatly enable
    the reproducibility and automation of all the tasks performed in Azure. Therefore,
    we will concentrate on the latter solutions – let''s take a look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure CLI**: This is a fully fledged command-line environment that you can
    install on every major operating system. The latest version can be downloaded
    from [https://docs.microsoft.com/en-us/cli/azure/install-azure-cli](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Power Shell**: As the name suggests, this is a library of PowerShell
    modules, which can be added to a PowerShell environment. Previously, PowerShell
    was only available on Windows, but the new PowerShell Core 7.x now officially
    supports the major Linux releases and macOS. The following description shows how
    to install it on your system: https://docs.microsoft.com/en-us/powershell/azure/install-az-ps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`curl` or the popular Python `requests` library. The following article describes
    the given syntax: https://docs.microsoft.com/en-us/rest/api/resources/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these options allow the use of so-called **ARM templates** (https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview),
    Azure's version of **Infrastructure as Code** (**IaC**). It gives you the ability
    to save and version-control infrastructure definitions in files. This way is highly
    recommended when dealing with complex infrastructure deployment, but we will not
    dive any further into this topic. The only additional point to make here is that
    there are other tools on the market for IaC management. The most prominent tool
    is called **Terraform** (https://www.terraform.io/), which allows infrastructure
    management of any cloud vendor or on-premises environment, including Azure. To
    achieve this, Terraform utilizes the Azure CLI under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, you can choose any of the aforementioned options for the tasks at
    hand, especially if you have a strong preference for one of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will not manage complex infrastructure and want to avoid any unnecessary
    additional levels of complexity, we will utilize the Azure CLI throughout the
    rest of the book. Furthermore, the new ML CLI extension offers a couple of neat
    features for Azure Machine Learning, which we will discover throughout the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The Azure CLI ](img/B17928_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The Azure CLI
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't already, please feel free to download and install or update the
    CLI with the latest version. When you are ready, open your favorite command line
    or terminal and type `az` into the console. You should be greeted by the screen
    shown in *Figure 3.2*.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the workspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After this short introduction to ARM, let''s deploy our first ML workspace.
    We will deploy a workspace using the Azure CLI. If you would like to rather deploy
    it via the Azure portal, you can follow this tutorial: https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you had a short look through the list of commands in the CLI, you might
    have noticed that there seems to be no command referencing ML. Let''s rectify
    this and set up our first Azure Machine Learning workspace via the CLI following
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to your Azure environment through the CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will open a website with an AAD login screen. After you have done
    this, return to the console. The screen will now show you some information about
    your AAD tenant (`homeTenantId`), your subscriptions (`id`, `name`), and your
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have more than one subscription shown to you and need to check which
    subscription is active, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the output, check whether the `IsDefault` column shows `True` for your preferred
    subscription. If not, use the following command to set it to your chosen one by
    typing in the name of it – `<yoursub>` – and checking again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we are deploying to the correct subscription in the correct tenant,
    let''s check the situation with the installed extension. Type in the following
    command in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If neither `azure-cli-ml` nor `ml` is shown in the list, you are missing an
    extension for using Azure Machine Learning via the CLI. The first of them denotes
    `Azure ML CLI 1.0`, the second one `Azure ML CLI 2.0`. Version 2 of the ML CLI
    was announced at Microsoft Build 2021 ([https://techcommunity.microsoft.com/t5/azure-ai/announcing-the-new-cli-and-arm-rest-apis-for-azure-machine/ba-p/2393447](https://techcommunity.microsoft.com/t5/azure-ai/announcing-the-new-cli-and-arm-rest-apis-for-azure-machine/ba-p/2393447)),
    offering fine-grained control of the ML workspace. Therefore, we will be using
    the new version of the CLI extension.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Azure ML CLI 2.0 offers new abilities to directly control the jobs, clusters,
    and pipelines of the ML workspace from the command line. It also offers support
    for YAML configuration files, which are crucial for MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running the old version, you should remove that version, but be
    aware that, as some commands are slightly different, you might break a script
    you are already using. To clean up the namespace and remove the previous version,
    you can use the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s install the ML extension using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After that, feel free to check the installed extensions again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will be able to use it. First off, we will have a look at the help
    page for the extension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you the following subgroups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have a lot of options to control our workspace from the CLI.
    We will come back to many of them later in the book. For now, we are interested
    in managing our workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you type the following command, we will have a look to see whether we are
    still missing requirements for the creation of the ML workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Going through the arguments, you will see that a **resource group** is required.
    A resource group in Azure is a logical construct where resources need to be deployed
    to. It is one vital part of the **Azure management hierarchy**. For further reading,
    have a look at access management in Azure: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, if you scroll down to the examples in the console output, you will
    also see that the new version of the CLI has a neat property that lets us deploy
    the workspace from a **Yet Another Markup Language** (**YAML**) file. We will
    not do this now, but it is something to keep in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The Azure Machine Learning service can be completely operated using the Azure
    ML CLI 2.0 extension, YAML configuration files, and a training or inference script.
  prefs: []
  type: TYPE_NORMAL
- en: 'A resource group in Azure also requires a location. Therefore, let''s have
    a look at the available data center locations for the Azure cloud by running this
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Have a look at the name of your preferred region and use it in the following
    command to create the resource group. Our example here will create a resource
    group in West US 2 with the name `mldemo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Even though we define the resource group to be in West US 2, resources inside
    a resource group can be in different regions. It is just best practice to define
    a group in a specific region and let the resources inside that group be in the
    same region.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create the workspace itself by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create a workspace named `mldemows` in the `mldemo` resource group.
    If we remove the location setting, it will take the location of the resource group.
  prefs: []
  type: TYPE_NORMAL
- en: 'This command can take a bit of time. When it is done, you will see output like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding command created multiple resources, together with
    the Azure Machine Learning workspace, that are required for running ML experiments.
    We will come back to the reasons in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to have a look at the deployment at any point, you can run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have created our first Azure Machine Learning workspace. Good work! In the
    next section, we will have a look at what this entails.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Azure Machine Learning service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we continue to set up our own development environment and do some ML,
    we will have a look at what was just deployed besides the main workspace, get
    a base understanding of all features available in the service, which we will utilize
    throughout the book, and have a first short look at **Azure Machine Learning**
    **Studio**.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the deployed services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by navigating to the Azure portal again. There, type the name
    of the workspace as `mldemows` in the top search bar. You should see something
    like the result shown in *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – An Azure portal search for an ML workspace ](img/B17928_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – An Azure portal search for an ML workspace
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, besides the main `mldemows` workspace, three other services
    were deployed, namely **Storage account**, **Key vault**, and **Application Insights**.
    As most of them require unique names, you will see a random alphanumeric code
    at the end of each name. For each one of these additional services, we can provide
    our own already existing service when we deploy the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, an **Azure container registry** will be required at a later stage
    but does not need to be there during the initial deployment of the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing now what additional services were deployed, let's discuss why they are
    there.
  prefs: []
  type: TYPE_NORMAL
- en: The storage account for an ML workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The storage account, typically referred to as the **default storage account**,
    is the main datastore for the workspace. This storage is vital for the operation
    of the service. It stores among other things experiment runs, models, snapshots,
    and even source files, such as Jupyter notebooks. We will have a more in-depth
    look at default workspace storage, many other datastores in and around Azure,
    and how they can be integrated in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that if you would want to use your own storage account as default storage
    when deploying the workspace, it cannot have a hierarchical namespace (Azure Data
    Lake) and it cannot be premium storage (high-performant SSDs).
  prefs: []
  type: TYPE_NORMAL
- en: Azure Key Vault for an ML workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Key Vault is a cloud-managed service that can store *secrets* such as passwords,
    API keys, certificates, and cryptographic keys. Secrets in the service are held
    either in a software vault or a managed **Hardware Security Module** (**HSM**).
    For the ML workspace, and any other service for that matter, it is crucial to
    store your access keys in a secure environment.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only handled relatively unimportant information such as a subscription
    ID, but if we want, for example, to pull data from external storage, we will either
    need a key to access it or call a function to another service, where this information
    is stored securely. You can be the judge of what is the better choice.
  prefs: []
  type: TYPE_NORMAL
- en: The developers of the ML workspace chose the latter options. Due to that, an
    Azure key vault is required to store the internal secrets for the workspace and
    give you the possibility to store any secret necessary to read out datasets, perform
    ML training on compute targets, and deploy your final models to internal or external
    targets.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the question might arise of how to get secure access to Key Vault itself.
    This is done through a so-called **managed identity**, which gives the workspace
    (the app) itself an identity to assign rights to.
  prefs: []
  type: TYPE_NORMAL
- en: Managed Identities on Azure
  prefs: []
  type: TYPE_NORMAL
- en: A managed identity is an identity given to an application that behaves the same
    way as a user identity.
  prefs: []
  type: TYPE_NORMAL
- en: As with the other services, you could have linked an already existing key vault
    during deployment without any restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Application Insights for an ML workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Applications Insights** is a module of **Azure Monitor**, which in turn is
    a suite in Azure to monitor infrastructure and applications, which stores and
    surfaces infrastructure metrics such as CPU usage and log files of applications.'
  prefs: []
  type: TYPE_NORMAL
- en: The Azure Machine Learning workspace uses Application Insights to store compute
    infrastructure logs, ML script logs, and defined metrics of the ML model runs
    and is therefore required for the operation of the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Container Registry for an ML workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Azure Container Registry** (**ACR**) is a service based on the **Docker Registry**.
    It is used to store and manage Docker container images and artifacts. For the
    workspace, the registry is required at the point when we start running training
    on or deploying models to a compute that is not our local machine. In this process,
    a container is packed and registered to ACR, which then can be tracked and utilized
    in ML scripts or by deployment pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Please be aware that the ML service by default deploys ACR in the basic service
    tier. To reduce the time for building and deploying an image to a compute target,
    you might want to change the Container Registry service level to Standard or Premium.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the workspace interior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we understand the additional deployed service, we will have a look
    at the interior of the workspace itself. *Figure 3.4* shows nearly every aspect
    of note of an Azure Machine Learning workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – A structural view of an Azure Machine Learning workspace ](img/B17928_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – A structural view of an Azure Machine Learning workspace
  prefs: []
  type: TYPE_NORMAL
- en: Let's get an understanding of each of these aspects, except for **Associated
    Azure resources**, as we already discussed that in the *Analyzing the deployed
    services* section.
  prefs: []
  type: TYPE_NORMAL
- en: User roles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with any other service in Azure, user authentication and authorization are
    performed through AAD and so-called **Azure Role-Based Access Control** (**Azure
    RBAC**).
  prefs: []
  type: TYPE_NORMAL
- en: Role-based Access Control on Azure
  prefs: []
  type: TYPE_NORMAL
- en: Azure RBAC is used to assign to an identity from AAD (a user, a service principal,
    or a managed identity) a specific role on a resource, which defines the level
    of access to the resource and the type of granular action that can be performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the ML workspace, we can assign an identity the Azure predefined
    base roles (**Owner**, **Contributor**, or **Reader**) and two custom roles named
    **AzureML Data Scientist** and **AzureML Metrics Writer**. Here are their details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reader**: This role is allowed to look at everything but cannot change any
    data or action anything that would change the state of the resource (for example,
    deploying a compute or changing a network configuration).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contributor**: This role is allowed to look at and change everything but
    is not allowed to change the user roles and rights on the resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Owner**: This role is allowed to do any action on a specific resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureML Data Scientist**: This role is not allowed any action in the workspace
    except creating or deleting compute resources or modifying the workspace settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureML Metrics Writer**: This role is only allowed to write metrics to the
    workspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides these, the ML workspace does not offer additional custom roles.
  prefs: []
  type: TYPE_NORMAL
- en: To give you more fine-grained control in this matter, RBAC lets you build your
    own custom roles, as a lot of actions a user can perform in the ML workspace are
    defined as so-called **actions** in RBAC. All available actions for the Azure
    Machine Learning service can be found in this list of resource providers, [https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations](https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations),
    under the operation group named **Microsoft.MachineLearningServices**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get some inspiration for different roles, have a look at common scenarios
    and custom roles suggested by Microsoft: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles#common-scenarios](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles#common-scenarios).
    We will have a look in the next section where you can define and assign them.'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of ML – in a nutshell – is to find a mathematical function, which would
    be hard to find algorithmically, that when given specific input results in as
    many cases as possible in the expected output. This function is typically referred
    to as an **ML model**. A model we train might be a function that assigns voices
    in a sound file to specific speakers or that recommends products for customers
    on a web shop based on the buying behavior of similar buyers (see [*Chapter 13*](B17928_13_ePub.xhtml#_idTextAnchor202),
    *Building a Recommendation Engine in Azure*).
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we need to train ML models utilizing already existing ML algorithms,
    with the goal to lower the output of the so-called **loss function** of said model.
    This requires tweaking the settings of our models and, mathematically speaking,
    in the best case, finding the global minimum of the loss function on the *n*-dimensional
    room of all possible functions. Depending on the complexity of our model, this
    requires a lot of reiterations.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to keep track of the iterations of our model training, we define
    them as **runs** and align them to a construct called an **experiment**, which
    collects all information concerning a specific model we want to train. To do this,
    we will connect any training script run we perform to a specific experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and datastores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any ML model requires data to operate with, either for training or for testing
    purposes. Instead of linking data sources and different data files directly in
    our scripts, we can reference **datasets**, which we can define inside the workspace.
    Datasets, in turn, curate data from **datastores**, which we can define and attach
    in the workspace. We will go into more detail on how to handle data, datasets,
    and datastores in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071), *Ingesting
    Data and Managing Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: Compute targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to run experiments and, later on, host models for inferencing, we
    require a **compute target**. The ML service comes with two options in this area,
    namely the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute instance**: A single virtual machine typically used for development,
    as a notebook server, or as a target for training and inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute cluster**: A multi-node cluster of machines typically used for complex
    training and production environments for inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find a list of supported compute targets (virtual machines) here: [https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target#supported-vm-series-and-sizes](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target#supported-vm-series-and-sizes).
    There are more details concerning their pricing in the following overview: [https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/).'
  prefs: []
  type: TYPE_NORMAL
- en: Besides these two options, the workspace offers a bunch of other possible targets
    for both training and inferencing. Popular compute options are your own local
    computer, any type of Spark engine (**Apache Spark**, **Azure Databricks**, or
    **Synapse**) for training, and **Azure Kubernetes Service** (**AKS**) for inferencing.
    For a full updated list of options, refer to [https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target).
  prefs: []
  type: TYPE_NORMAL
- en: Environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you write a simple Python script and run it in the Python interpreter,
    you run it in a so-called `numpy`), and certainly the operating system you are
    running it on. This is also true for any ML script that we run.
  prefs: []
  type: TYPE_NORMAL
- en: For our purpose, we operate in an environment that requires a specific Python
    version and certain libraries such as the Azure Machine Learning Python SDK and
    libraries containing ML algorithms and tooling, such as **TensorFlow**. For our
    own local machine, and especially if we want to run our script on a much faster
    compute cluster in the workspace, we need a good way to define the environment
    for the compute target.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate this, the workspace gives us the ability to define and register
    ML environments. These are typically **Docker containers** encompassing the OS
    and every runtime, library, and dependency required. For defining libraries and
    dependencies for Python inside the container, the package manager **Conda** ([https://conda.io/](https://conda.io/))
    is used in most cases under the hood. Speaking of that, let''s classify the types
    of environments we can work with or create:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Curated environments** use predefined environments containing typical runtimes
    and ML frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System-managed environments** (using default behavior) build environments
    starting from a base image with dependency management through Conda.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-managed environments** build environments by either starting from a
    base image but allowing you to handle all libraries and dependencies yourself
    through Docker steps, or by creating a complete custom Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we start our first experiments at the end of this chapter, we will see
    how to use environments in our ML runs.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning Environments
  prefs: []
  type: TYPE_NORMAL
- en: An environment in Azure Machine Learning is a Docker container encompassing
    an OS and any runtimes, libraries, and additional dependencies required.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that we require a defined environment to run experiments on
    compute clusters in the workspace. For our local computer, on the other hand,
    we could just run on the *environment* we curated on the machine and ignore the
    ML workspace environments. But if we were to use the environment methods of the
    Azure Machine Learning Python SDK in our ML scripts, the run would require some
    type of defined environment. This can either be the given environment our machine
    exists in, a local Docker runtime, or a runtime powered by a Conda environment
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: Runs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **run** is the actual execution of a model training on a compute target.
    Before executing a run, it requires (in most cases) a so-called **run configuration**.
    This configuration is composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A training script**: The training script that performs the actual ML training
    (which basically takes your source folder with all source files, zips it, and
    sends it to the compute target)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An environment**: The ML environment described previously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A compute target**: The target compute instance or cluster that the run will
    be executed in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see later in the chapter when we do our first experiments that there
    is a `RunConfiguration` class in the Azure Machine Learning Python library that
    needs to be used to execute the run.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning Experiment Runs
  prefs: []
  type: TYPE_NORMAL
- en: A run is the execution of a training script in a given environment on a specified
    compute target.
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of that, during and after the execution of the run, it tracks and collects
    the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log files**: Includes the log files generated during the execution and any
    statement we add to the logging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**: Includes standard run metrics and any type of object (values,
    images, and tables) that we want to track specifically during the run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snapshots**: Includes a copy of the source directory containing our training
    scripts (using the ZIP file that we already required for the run configuration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output files**: Includes the files generated by the algorithm (the model)
    and any file we additionally want to attach to the run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see later that we can utilize the `Run` class in the Azure Machine Learning
    Python library to influence what is tracked.
  prefs: []
  type: TYPE_NORMAL
- en: Registered models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As said before, the output of our experiment runs is an ML model. This model
    is basically a mathematical function or, to be more precise, a piece of code implementing
    a function. Depending on the ML framework we utilize, the function is stored in
    binary format in one or multiple output files found in the identically named folder.
    Popular formats for serialized ML models are **pickle** (Python), **H5** (Keras),
    **Protobuf** (TensorFlow and Caffe), and other custom formats.
  prefs: []
  type: TYPE_NORMAL
- en: As all models from different runs would *just* be stored in the output files
    of the run itself, the workspace offers the ability to register a model to the
    *model registry*. In the registry, the models are stored with a name and a version.
    Each time you add a model with the same name, the registry adds a new version
    of the existing model with a new version number. In addition, you can tag each
    model with metainformation, such as the framework utilized.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning Model Registry
  prefs: []
  type: TYPE_NORMAL
- en: The model registry in Azure Machine Learning stores names and versions of registered
    models for tracking and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the model registry helps you to keep track of the different results
    you achieved through training and allows you to deploy different versions of the
    model for production, development, and test environments.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments and deployment endpoints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once a model is trained and registered, it can be packaged as a service – by
    defining an entry script and environment – and deployed to a compute target. The
    entry script's job is to load the model during initialization, as well as parse
    user inputs, evaluate the model, and return the results for a user request. This
    process is called **deployment** in Azure Machine Learning. Compute targets for
    deployments can be either managed services such as **Azure Container Instances**
    (**ACI**) or **Azure Kubernetes Service** (**AKS**), or a completely custom user-managed
    AKS cluster. Every deployment typically serves a single model.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to abstract multiple model deployments behind a common endpoint,
    you can define an **endpoint service**. This is a common requirement for rolling
    out multiple model versions, performing **blue-green deployments**, or **A/B testing**.
    An endpoint is a separate service in Azure Machine Learning that provides a common
    domain for multiple model deployments, performs **Secure Socket Layer (SSL)**/**Transport
    Layer Security (TLS)** termination, and allows traffic allocation between deployments.
    Endpoints can also be deployed to multiple compute targets, including ACI and
    AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning Endpoints
  prefs: []
  type: TYPE_NORMAL
- en: A deployment endpoint in Azure Machine Learning is a service offering a common
    domain for accessing and testing multiple versions of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For both deployments and endpoints, we differentiate between **online scoring**
    and **batch scoring**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Online scoring**: A model is evaluated synchronously for a single input record
    (or small batch of input records) where the input data, as well as the scoring
    results, are passed directly in the request and response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch scoring**: A user typically passes a location to the input data instead
    of sending input data with the request. In this case, the model is evaluated asynchronously
    and provides the results in an output location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss the deployment of models and endpoints in more detail in [*Chapter
    14*](B17928_14_ePub.xhtml#_idTextAnchor217), *Model Deployments, Endpoints, and
    Operations*.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final part to mention is **ML pipelines**. Everything we have discussed
    so far might be enough to do some data preparation, model training, model deployment,
    and inferencing for ourselves. But even that would entail multiple manual steps.
    Certainly, we can automate most parts of this using the Azure CLI through some
    scripting and be quite happy with our setup.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine that we want to work with a team and build automated retraining
    and deployment of our model whenever there is new data to train on. We would have
    to run similar steps again, such as preprocessing, training, and optimization
    – just this time with new training data. This process is typically repeated whenever
    there is significant data drift between the training data and the inferencing
    data. This is the point where we need to think about bringing in ideas and proven
    solutions from DevOps, as in the end, we will also write code and deploy infrastructure
    into a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, pipelines are used to facilitate workflows and bring automation to
    every step of the ML chain; we will take a closer look at them in [*Chapter 8*](B17928_08_ePub.xhtml#_idTextAnchor135),
    *Azure Machine Learning Pipelines*. Pipelines are also one of the integral parts
    of MLOps, and we will see them in action in [*Chapter 16*](B17928_16_ePub.xhtml#_idTextAnchor252),
    *Bringing Models into Production with MLOps*.
  prefs: []
  type: TYPE_NORMAL
- en: Surveying Azure Machine Learning Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a good understanding of the features of the workspace, let''s
    continue where we left off before and have a look into the Azure portal and **Azure
    Machine Learning Studio**, the web service to operate every aspect of the ML process.
    This time, search again for our workspace name and click on **mldemows**, the
    ML workspace. You will be shown the typical menu structure for an Azure resource
    on the left and the **Overview** page of the service on the right, as shown in
    *Figure 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – The Azure resource view ](img/B17928_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The Azure resource view
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the administration view from an infrastructure perspective. The major
    points of interest for you to keep in mind are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview**: The panel showing the names and attached services of the workspace
    and the button to launch the ML studio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access control (IAM)**: The panel to set user access rights on every aspect
    of the workspace, as discussed in the last section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Networking**: The panel to integrate the service into a private virtual network
    by activating a **private endpoint** for the workspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity**: The panel showing the already created managed identity of the
    workspace, which can be used to give the workspace access to external Azure services,
    such as a storage account using RBAC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage + quotas**: The panel to access the available quota on the subscription,
    which defines how many cores of which type of virtual machine the user is allowed
    to deploy within the subscription.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By clicking on the **Launch studio** button on the overview page, the actual
    Azure Machine Learning Studio will open in a new tab, greeting you with the view
    shown in *Figure 3.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – The Azure Machine Learning Studio home page ](img/B17928_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – The Azure Machine Learning Studio home page
  prefs: []
  type: TYPE_NORMAL
- en: You can theoretically do everything we will do in this book through this web
    application, but in certain areas, this can be cumbersome. We will discuss in
    detail how we set up and operate our development environment in the next section,
    but it is a good idea to get an understanding of this web service, as we will
    come back to it throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the menu to the left, there are three major categories, namely **Author**,
    **Assets**, and **Manage**. Let's match what we already know about the workspace
    to what is shown to us in the web service.
  prefs: []
  type: TYPE_NORMAL
- en: Author
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first section of the menu shows you the options for authoring your ML experiments.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebooks**: Create and author Jupyter notebooks utilizing a notebook **virtual
    machine (VM)** (compute instance) in the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated ML**: Create ML models through a wizard, offering insights and
    suggestions based on your given dataset and problem to solve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designer**: Build ML models through a GUI interface using logical building
    blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already discussed why we prefer using code and notebooks in [*Chapter
    2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing the Right Machine Learning
    Service in Azure*. We will come back to automated ML later in this book in [*Chapter
    11*](B17928_11_ePub.xhtml#_idTextAnchor178), *Hyperparameter Tuning and Automated
    Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: For now, the options to author our notebooks are to either work in the web service
    environment and utilize a Jupyter server on a compute instance in the cloud, or
    to work from our local computer with a local Jupyter server.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: We will stay in our own local environment for most of the book, but be aware
    that in a bigger team, it might be of value to have a notebook server in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Assets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second section of the menu shows you the assets available to utilize in
    your scripts. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets**: View and create datasets in the workspace and configure dataset
    monitoring for understanding data drift between your training data and the inference
    data from a deployed model (imaging a sensor that is placed differently in production
    than when gathering test data or that is suddenly broken).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiments**: View all experiments and all runs that have been tracked,
    including their detailed run statistics (metrics, snapshots, logs, and outputs)
    and infrastructure monitoring logs of the compute target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipelines**: Create pipelines, view pipeline runs, and define endpoints for
    pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: Register models and view their properties, including their version,
    the datasets they are using, the artifacts they are made of, and the endpoints
    they are actively deployed to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoints**: View and create web service endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going through these pages, we can see a lot of the workspace items we already
    discussed, from datasets to model training through experiments and their runs,
    registering models, and surfacing service endpoints for our deployments, up to
    managing all of this through ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: You might have seen some other additional features, such as **Dataset Monitoring**,
    which we will come back to in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingestion Data and Managing Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: We will have a closer look at the experiment and run statistics at the end of
    this chapter when we have an experiment and a run has been shown in Azure Machine
    Learning Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Manage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final section of the menu shows us the machines and services that we can
    manage in our workspace. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute**: Create, view, and manage compute instances, compute clusters,
    inference clusters, and other attached computes (for example, external VMs or
    Databricks clusters), including performed runs, distribution of runs on nodes
    (if existing), and monitoring of the infrastructure itself (for example, CPU usage).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environments**: View available curated environments and create your own custom
    environments from a Python virtual environment, a Conda YAML configuration, a
    Docker image stored in the container registry, or from your own Docker file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workspacefilestore` and `workspaceblobstore`), the global Azure Machine Learning
    dataset repository (`azureml_globaldatasets`), and any already attached external
    storage or attach new ones, including Azure Data Lake, Azure Blob storage, Azure
    file shares, and Azure SQL, MySQL, and PostgreSQL databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Labeling**: Create labeling projects for image classification and object
    detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linked Services**: Link an Azure Synapse Spark pool to the workspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these views, we find the final missing pieces, the compute targets in the
    workspace, the environments, and our available datastores, from which we source
    our datasets for modeling. Furthermore, we find a service to help us with data
    labeling of source files (typically images) and the possibility to link Azure
    Synapse to our workspace.
  prefs: []
  type: TYPE_NORMAL
- en: We will go into more detail on the datastores in the next chapter and on data
    labeling in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102), *Feature Engineering
    and Labeling*. We will not cover the Azure Synapse integration in detail in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good overview of the features and tooling of the Azure Machine
    Learning service, we can now return to our local machine and start our first experiments
    with Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Running ML experiments with Azure Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have installed the Azure CLI locally, deployed our ML workspace to
    our Azure subscription, and had a look through the features and functionalities
    of the Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: In this final section of the chapter, we will set up our local environment,
    including Python, the Azure Machine Learning Python SDK, and optionally Visual
    Studio Code, and embark on our first experiments locally and with compute targets
    in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a local environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the beginning, we discussed briefly the tooling available for deploying
    Azure resources through Azure Resource Manager. In the same vein, let''s have
    a look at the options for authoring and orchestrating the workspace from our local
    environment. The options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Python 3, the Azure Machine Learning Python SDK, a Jupyter Python extension,
    and the Azure ML CLI (1.0/2.0) extension (and an editor of choice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Python3, the Azure Machine Learning Python SDK, an Azure ML CLI (1.0/2.0)
    extension, **Visual Studio Code (VS Code)**, and VS Code extensions (Azure, Azure
    Machine Learning, Jupyter, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Python3, an Azure ML CLI 2.0 extension, YAML, and VS Code (or an editor
    of choice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using R, an Azure ML CLI 2.0 extension, YAML, and VS Code (or an editor of choice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two options are the de facto standard at the time of writing and the
    ones we will focus on primarily in this book. We will use the Azure Machine Learning
    Python SDK with Python 3 and leave it to you if you prefer to work mostly from
    the console with source files and optionally an editor of choice, or if you want
    to use an **integrated development environment (IDE)** such as VS Code, which
    comes with a feature-rich editor and helpful extensions for Azure, Azure Machine
    Learning, and Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, we will author a Jupyter notebook to orchestrate our ML experiments
    on the workspace and one or more Python source files to implement the training
    procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latter two options were introduced with the more extensive **Azure ML CLI
    2.0**. Instead of writing a Jupyter notebook, we completely detach the orchestration
    of the workspace (run configuration, environments, deployments, and endpoints)
    from the training and inference source code. This is done through YAML configuration
    files. An example of an ML experiment run looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this YAML structure references the actual code to be executed
    (`code`), the runtime to use (`command`), and defines every part (`environment`,
    `compute`, and `data`) necessary for the training run in a descriptive manner.
  prefs: []
  type: TYPE_NORMAL
- en: YAML Configurations
  prefs: []
  type: TYPE_NORMAL
- en: YAML configuration files are a descriptive way to run experiments, create compute
    services and endpoints, and deploy models in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: This is a more structural way of thinking about the task we will perform and
    will come in handy when we talk about production systems and MLOps in [*Chapter
    16*](B17928_16_ePub.xhtml#_idTextAnchor252), *Bringing Models into Production
    with MLOps*. Finally, this option is the only one allowing source files to be
    written in **R**, the domain-specific language for data science, and is highly
    supported in VS Code through the Azure Machine Learning VS Code extension.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Python environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have a good idea about the possible local development environments
    we can work with, let''s set up our Python environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The following actions only have to be done if you run your experiments on your
    own local machine and not if you are using a notebook compute instance in the
    Azure Machine Learning Studio authoring environment or a **Data Science Virtual
    Machine** (**DSVM**) in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, check whether there is already a Python version installed on your system
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, please check the metadata of the Azure Machine Learning Python extension
    on https://pypi.org/project/azureml-sdk/. There are certain times when the extension
    is behind the most recent Python release. If you already have an unsupported Python
    version on your system, either uninstall that version or read up on how to operate
    multiple Python environments on the same machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After you have verified the supported Python release, either go to [https://www.python.org/](https://www.python.org/)
    and find the supported version for Windows and macOS or use the Terminal and the
    `apt-get` command under your Linux distribution. An example for Python 3.8 would
    look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you have installed Python for the first time or reinstalled it again, please
    check that Python is correctly integrated into the path environment variable by
    checking for the Python version (see *step 1*). If all is good, we can move forward
    and install the SDK by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If this command is trying to resolve a lot of dependencies, you might still
    be operating with an unsupported version of Python or the package installer **PIP**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to work with VS Code, you can jump to the next paragraph now. If
    you prefer to work primarily with the command line, please install either a local
    JupyterLab or a local Jupyter notebook server ([https://jupyter.org/index.html](https://jupyter.org/index.html))
    with one of the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, you can start either environment from the command line, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With this version of the setup, you can now proceed to the *Running a simple
    experiment with Azure Machine Learning* section.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Visual Studio Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VS Code is a lightweight but very powerful IDE. It is highly integrated with
    Azure, Azure Machine Learning, and Git, and has a very good editor, an integrated
    terminal, and a long list of useful extensions to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the tool either from [https://code.visualstudio.com/](https://code.visualstudio.com/)
    or through Azure Marketplace and install it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After you open it, you will be greeted by the view shown in *Figure 3.7* (probably
    with a darker theme):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – The VS Code interface ](img/B17928_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – The VS Code interface
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the top menu on `>Theme` and look for `>Preferences: Color
    Themes`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicking on it will give you a quick way to set the theme of the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to open the terminal, you can click on the top menu on `az` again to see
    the same as shown in *Figure 3.7*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the left menu, you will find an **EXPLORER** tab, where you can add
    your source folders and files, a **Source Control** tab to connect to Git, a **Run
    and Debug** tab that lets you handle the debugging of your code, and an **Extensions**
    tab where you can search for VS Code extensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to the **Extensions** tab and search and install the following extensions,
    if they are not already installed: **Azure Tools**, **Azure Machine Learning**,
    **Python**, **Pylance**, **YAML**, and **Jupyter**.'
  prefs: []
  type: TYPE_NORMAL
- en: After the installation, you will find a new tab in the left menu called `sign
    in azure`, you will find a way to sign in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After you are through with signing in to Azure, the **Azure** tab will populate
    with your subscription names, resource groups, and any resource you might have.
    If you look under the **MACHINE LEARNING** headline, you will also find your previously
    deployed workspace, as shown in *Figure 3.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – The VS Code Azure Machine Learning extension ](img/B17928_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – The VS Code Azure Machine Learning extension
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, download the files for this chapter to work with. Just
    open the folder via **File** | **Open Folder…**, which will add them to the **Explorer**
    tab, from where you can start the journey.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: VS Code has much more to offer, but we will concentrate primarily on understanding
    ML and the Azure Machine Learning workspace from now on, not on operating every
    aspect of this editor. If you need more help using VS Code, please feel free to
    visit [https://code.visualstudio.com/docs/introvideos/basics](https://code.visualstudio.com/docs/introvideos/basics)
    or any other resource that can help you with it.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing a simple experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One great use case for starting with Azure Machine Learning is to add advanced
    logging, tracking, and monitoring capabilities to your existing ML scripts and
    pipelines. Imagine you have a central place to track all ML experiments from all
    your data scientists, monitor training, and validation metrics, upload your trained
    models and other output files, and save a snapshot of the current environment
    every time a new training run is executed. You can achieve this with Azure Machine
    Learning by simply adding a few lines of code to your training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by adding Azure Machine Learning workspace functionality to a
    **Keras** ([https://keras.io](https://keras.io)) ML training script. Keras is
    one of many ML libraries we can choose from, depending on the ML algorithms we
    require.
  prefs: []
  type: TYPE_NORMAL
- en: A working directory and preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we begin, please download the code files for this chapter from the repository
    and extract them to your preferred working directory. After that, either switch
    to this directory in the console or open it as a folder in VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In either case, you will find the following files in the directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.azureml/config.json`: The Azure Machine Learning workspace configuration
    file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.azureml/requirements.txt`: The Python PIP environment requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`00_setup_env.sh`: A shell script to set up the Azure CLI and Python environment
    from scratch (as we already did)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`01_setup_azure_ml_ws.sh`: A shell script to set up the Azure Machine Learning
    workspace (as we did already)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0x_run_experiment_*.ipynb`: Multiple Jupyter notebooks for the upcoming experiments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`04_setup_azure_ml_compute.sh`: A shell script to create a workspace compute
    instance from a YAML configuration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compute.yml`: A YAML configuration file for a workspace compute instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`code/*.py`: A folder containing the Python model training scripts we will
    use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.amlignore`: A file denoting everything that should be ignored by the run
    snapshot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with our first experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the missing Python package we will need for the following
    experiments. Run the following command, which will install the packages defined
    in the PIP requirements file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: PIP will point out that the Azure Machine Learning SDK is already installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open the `config.json` file and enter your subscription ID after the
    `subscription_id` key. This is necessary, as we will load this configuration in
    all notebooks using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `from_config()` method looks for a file called `config.json` either in the
    current working directory or in a directory called `.azureml`. We will choose
    to add it to the folder, as it is part of the `.amlignore` file.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `02_run_experiment_keras_base.ipynb` notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following, we will have a look through the notebook in order to understand
    the actual model training script, how we can add snapshots, outputs, and logs
    to the Azure Machine Learning experiment, and how we can catalog the best model
    in the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: A training script for Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Navigate to the second block in the notebook. Imagine this part to be your original
    ML training file (plus the `model.fit()` function that you will find in the final
    block).
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand the actual training code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the classes we require for the training from the `tensorflow`
    library (Keras is a part of TensorFlow):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to get our training and test data from the CIFAR-10 dataset
    and change it into a useful format. The `cifar10.load_data()` function will fill
    the training set with 50,000 datapoints and the test set with 10,000 data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Test and Training Datasets
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset is made up of the data points we train our model on; the
    test dataset is made up of the data points we will evaluate our model against
    after it has been trained. These should be completely distinct from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we start defining our model – in this case, a `Sequential` model
    ([https://keras.io/guides/sequential_model/](https://keras.io/guides/sequential_model/))
    – and we set the name of the model and the location for the output. We will use
    the **HDF5** file format (or H5 for short) for Keras, as mentioned before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we define an optimizer (`RMSProp` in this case), a checkpoint `loss`
    function, `optimizer`, and additional `metrics` to track during the training run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The part that would otherwise complete our original script is the one found
    in the last block of the notebook, which we will discuss in a moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is most of the notebook code. The rest of the code you
    can see is what you need to add to your script to enable tracking of your experiment
    runs, which we will analyze next.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking snapshots, output, and logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now have a look at the code we have ignored so far. First, return to
    the first block of the notebook we skipped before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we define a workspace object called `ws` using our config file,
    and as a second step, we define an experiment object, `exp`, to be tracked in
    the defined workspace under a chosen name. As you can see, we name it `cifar10_cnn_local`
    because we will utilize the CIFAR-10 dataset ([https://www.kaggle.com/c/cifar-10](https://www.kaggle.com/c/cifar-10)),
    we will run a **Convolutional Neural Network** (**CNN**), and we will do so on
    a local machine. If an experiment with the same name already exists, this invocation
    returns the existing experiment as a handle; otherwise, a new experiment will
    be created. Through the given name, all the runs in this experiment are now grouped
    together and can be displayed and analyzed on a single dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code block might open a website to log in to your Azure account.
    This is called interactive authentication. Please do this to grant your current
    execution environment access to your Azure Machine Learning workspace. If you
    run a non-interactive Python script rather than a notebook environment, you can
    provide the Azure CLI credentials through other means described here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#use-interactive-authentication.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have successfully linked the workspace into the `ws` object, you can
    continue adding tracking capabilities to your ML experiments. We will use this
    object to create experiments, runs, and log metrics, and register models in our
    Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's jump to the final block, where we will perform a run of the experiment.
    As described before, a run is a single execution of your experiment (your training
    script), with different settings, models, code, and data but the same comparable
    metric. You use runs to test multiple hypotheses for a given experiment and track
    all the results within the same experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, we can create a `run` object and start logging this run here by
    invoking the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code not only creates and initializes a new run; it also takes
    a snapshot of the current environment, defined through the `snapshot_directory`
    argument, and uploads it to the Azure Machine Learning workspace. To disable this
    feature, you need to explicitly pass `snapshot_directory=None` to the `start_logging()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the snapshot will take every file and folder existing in the current
    directory. To restrict this, we can specify the files and folders to ignore using
    a `.amlignore` file.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the code itself in the last notebook block, you can see that this
    is not the same line of code shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because it is good practice to wrap your training code in a `try` and
    `except` block in order to propagate the status of your run in Azure. If the training
    run fails, then the run will be reported as a failed run in Azure. You can achieve
    this by using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We included the `raise` statement in order to fail the script when an error
    occurs. This would normally not happen, as all exceptions are caught. You can
    simplify the preceding code by using the `with` statement in Python. This will
    yield the same result and is much easier to read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: By using only this single line of code, you can track a snapshot for each execution
    of your experimentation runs automatically and, hence, never lose code or configurations
    and always come back to specific code, parameters, or models used for one of your
    ML runs. This is not very impressive yet, but we are just getting started using
    the features of Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now, execute every code block in this notebook and wait for completion.
  prefs: []
  type: TYPE_NORMAL
- en: Once executed, go back to Azure Machine Learning Studio and navigate to the
    `cifar10_cnn_local`. When you click on it, you will see some metrics in a graph
    and a list of runs associated with the experiment. Click on the most recent run
    and then on `.azureml`).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.9* shows the uploaded snapshot files of a run in our experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – A snapshot view of an experiment run ](img/B17928_03_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – A snapshot view of an experiment run
  prefs: []
  type: TYPE_NORMAL
- en: Besides the `snapshot` directory, which is uploaded before the run starts, we
    also end up with two additional directories after the run created by the ML script,
    namely `outputs` and `logs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a run is completed using `run.complete()`, all content of the `outputs`
    directory is automatically uploaded to the Azure Machine Learning workspace. In
    our simple example using Keras, we can use a checkpoint callback to only store
    the *best model* of all epochs to the `outputs` directory, which then is tracked
    with our run. Have a look at this sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we trained a Keras model for five epochs. The process
    sets apart 20% (`validation_split`) of the training data as a so-called validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Validation Datasets
  prefs: []
  type: TYPE_NORMAL
- en: The validation set is the third set of datapoints, which the model is evaluated
    against during model training. It should neither be a subset of the training data
    nor the test data.
  prefs: []
  type: TYPE_NORMAL
- en: After that, the function runs through every epoch with a shuffled (`shuffle=True`)
    training dataset. In every epoch, it takes and overwrites the model file in the
    defined `output` folder if the model of this epoch is performing better on the
    validation set, which we defined by having a lower validation loss (`monitor='val_loss'`).
    Therefore, we will only have the best model stored in the `output` folder at the
    end. Hence, whenever we run the training with the previous experiment tracking,
    the model gets uploaded automatically once the run is completed.
  prefs: []
  type: TYPE_NORMAL
- en: If you go back to the second code block in the notebook, you will see that we
    already added the checkpoint callback in our code. Let's check what we got then.
  prefs: []
  type: TYPE_NORMAL
- en: In Azure Machine Learning Studio, navigate to `keras_cifar10_trained_model.h5`,
    was uploaded to the Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: This is also very convenient, as you won't lose track of your trained models
    anymore. On top of that, all artifacts you see here are stored in the workspace
    Blob storage, which is highly scalable and inexpensive.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.10* shows the additional output and log information of a run in our
    experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Outputs and logs of an experiment run ](img/B17928_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Outputs and logs of an experiment run
  prefs: []
  type: TYPE_NORMAL
- en: The `logs` directory contains the log output from Keras, which you also saw
    in the Jupyter notebook when executing the last block. In the current run, this
    was uploaded after the run, together with the `output` folder and the model.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning Log Streaming
  prefs: []
  type: TYPE_NORMAL
- en: Log streaming in Azure Machine Learning allows you to see logs in Azure Machine
    Learning Studio while a run is being executed.
  prefs: []
  type: TYPE_NORMAL
- en: We will see later that if the training script run is invoked through `ScriptRunConfig`
    rather than being executed directly, the logging will **stream** to the workspace
    (see also the **Enable log streaming** button). This will allow you to see the
    logs here while the run is still going on.
  prefs: []
  type: TYPE_NORMAL
- en: Cataloging models to the model registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a final step, we want to register our best model, which we have stored in
    the `output` folder, to the model registry in the Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we navigate to the final block of the notebook again, we can see that the
    last lines read like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first force the upload of the model. This is needed because all output
    resources are only uploaded when the run is completed and not immediately. Hence,
    after uploading the model, we can simply register it in the model registry by
    invoking the `run.register_model()` method.
  prefs: []
  type: TYPE_NORMAL
- en: If you navigate in Azure Machine Learning Studio to `keras_cifar10_trained_model.h5`
    from the `cifar10_cnn_local` experiment. If you click on it, you will find details
    about the model under **Details**, including the version number, and you will
    find the actual model file we created under **Artifacts**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.11* shows the model details of the registered model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – A registered model in the Azure Machine Learning model registry
    ](img/B17928_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – A registered model in the Azure Machine Learning model registry
  prefs: []
  type: TYPE_NORMAL
- en: The model can then be used for automatic deployments from the Azure Machine
    Learning service. We will look at this in a lot more detail in [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217),
    *Model Deployments, Endpoints, and Operations*, and [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to run a simple experiment, let's learn how to log metrics
    and track results in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Logging metrics and tracking results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already saw three useful features to track snapshot code, upload output artifacts,
    and register trained model files in our Azure Machine Learning workspace. As we
    saw, these features can be added to any existing experimentation and training
    Python script or notebook with a few lines of code. In a similar way, we can extend
    the experimentation script to also track all kinds of variables, such as training
    accuracy and validation loss per epoch, as well as the test set accuracy of the
    best model.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `run.log()` method, you can track any parameter during training and
    experimentation. You simply supply a name and a value, and Azure will do the rest
    for you. The backend automatically detects whether you send a list of values –
    hence multiple values with the same key when you log the same value multiple times
    in the same run – or a single value per run, such as the test performance. In
    Azure Machine Learning Studio, these values will be used automatically to visualize
    your overall training performance.
  prefs: []
  type: TYPE_NORMAL
- en: Our Keras model so far is tracking the *loss* as a metric by default and the
    *accuracy* of the model through our model compilation. We just don't log them
    to the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: We previously talked about the different datasets we are using in the script,
    namely the training dataset, the validation dataset, and the test dataset. Remember
    that the validation dataset is evaluated at the end of each epoch, which also
    means we can get the **validation loss** and the **validation accuracy** at the
    end of each epoch. Further, after we have found the best model of all epochs,
    we want to evaluate this model against the test data, which we have not done yet.
    This then results in the *test loss* and *test accuracy* of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the following, we will first add the test metrics to our run, then the validation
    metrics, and then have a look at them in Azure Machine Learning Studio. Finally,
    we will enhance the code so that we only register a model if it is better than
    all of the models from previous runs. Feel free to have the `02_run_experiment_keras_enhanced.ipynb`
    notebook open to follow along.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of the best model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal is to evaluate the best training model of all epochs against the test
    dataset to get the overall test metrics. In order to do this, we need to load
    it back into our model object. Luckily, we already only stored the best model
    of the whole run in our `output` folder using the checkpoint callback that we
    defined before. Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we get back the best model and then evaluate it, extracting
    the loss (`scores[0]`) and the accuracy (`scores[1]`). Having done this part,
    let's have a look at the validation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: A Keras callback for validation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal is to evaluate the model created in each epoch against the validation
    dataset to get the validation metrics for each epoch. We already used an existing
    callback to check for the best model in each epoch, so it might be a good idea
    to write one ourselves to track the metrics in each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `keras_azure_ml_cb.py` file in the `code` directory. You will be greeted
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code implements a simple Keras callback function. When the callback
    is executed, Keras passes the current epoch as well as all training and validation
    metrics as a dictionary (`logs`).
  prefs: []
  type: TYPE_NORMAL
- en: What then happens is that for all dictionary entries, we pull out the name and
    the value to log them to the experiment run with the `run.log(metric_name,metric_val)`
    function. We only have to check whether the value is a single value or an array
    type, as the Azure Machine Learning SDK has a different function called `run.log_list()`
    for multi-value entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use this callback in our model training the same way as we did with
    the previous callback, by adding it to the `model.fit()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This extends Keras naturally using a callback function to track the training
    and validation loss and accuracy in the Azure Machine Learning service. Any metric
    defined on the model itself will now be tracked automatically in the experiment
    run.
  prefs: []
  type: TYPE_NORMAL
- en: Running metric visualization in Azure Machine Learning Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After we have added a bunch of metrics to the experiment run, let's run the
    notebook as is and have a look at the run statistics in Azure Machine Learning
    Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you open the run, the **Metrics** list of types, as with both validation
    metrics, are automatically converted into line charts and plotted, as shown in
    *Figure 3.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – The metrics view of an experiment run ](img/B17928_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – The metrics view of an experiment run
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the test metrics and validation metrics are all accounted for.
    In addition, we can see **Test loss** and **Test accuracy** as metrics, which
    are also provided by Keras for each epoch as the evaluation of the model against
    the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Another nifty feature is that the ML workspace experiment gives you an overview
    of all your runs. It automatically uses both the scalar values and training and
    validation metrics that were logged per run and displays them on a dashboard.
    You can modify the displayed values and the aggregation method used to aggregate
    those values over the individual runs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.13* shows the accuracy and the validation accuracy of all experiment
    runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – The visualized metrics of all experiment runs ](img/B17928_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – The visualized metrics of all experiment runs
  prefs: []
  type: TYPE_NORMAL
- en: This is the simplest method of tracking values from the runs and displaying
    them with the corresponding experiments. Adding a few lines of code to your existing
    ML training scripts – independent of which framework you are using – automatically
    tracks your model scores and displays all experiments in a dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the registration of models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have metrics to read out and work with, we can, as a final step,
    enhance the way we save the best model to the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we always update the model with a new version as soon as a new model
    is available. However, this doesn't automatically mean that the new model has
    a better performance than the last model we registered in the workspace. As we
    want a new **version** of the model to actually be better than the last version,
    we need to check for that.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a common approach is to register the new model only if the specified
    metric is better than the highest previously stored metric for the experiment.
    Let's implement this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a function that returns a generator of metrics from an experiment,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding generator function yields the specified tracked metric for each
    run that is completed. We can use this function to return the best metric from
    all previous experiment runs to compare the evaluated score from the current model
    and decide whether we should register a new version of the model. We should do
    this only if the current model performs better than the previous recorded model.
    For that, we need to compare a metric. Using the **test accuracy** is a good idea,
    as it is the model tested against unknown data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we get the result for the test accuracy metric of all previously
    runs tracked in this experiment and select the largest. We then register the model
    only if the test accuracy of the new model is higher than the previously stored
    best score. Nevertheless, we still upload and track the model binaries with the
    experiment run.
  prefs: []
  type: TYPE_NORMAL
- en: We now have an enhanced version of our notebook, including metrics tracking
    and a better version to register a model in the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling the script execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we saw how you can annotate your existing ML experimentation
    and training code with a few lines of code in order to track relevant metrics
    and run artifacts in your workspace. In this section, we move from invoking the
    training script directly to scheduling the training script on the local machine.
    You might ask why this extra step is useful because there are not many differences
    between invoking the training script directly and scheduling the training script
    to run locally.
  prefs: []
  type: TYPE_NORMAL
- en: The main motivation behind this exercise is that in the subsequent step, we
    can change the execution target to a remote compute target and run the training
    code on a compute cluster in the cloud instead of the local machine. This will
    be a huge benefit, as we can now easily test code locally and later deploy the
    same code to a highly scalable compute environment in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing to note is that when scheduling the training script instead of
    invoking it, the standard output and error streams, as well as all files in the
    **logs** directory, will be streamed directly to the Azure Machine Learning workspace
    run. This has the benefit of tracking the script output in real time in your ML
    workspace, even if your code is running on the remote compute cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement this in a so-called **authoring script**. We call it an authoring
    script (or authoring environment) when the script or environment's job is to schedule
    another training or experimentation script. In addition, we will now refer to
    the script that runs and executes the training as the **execution script** (or
    execution environment).
  prefs: []
  type: TYPE_NORMAL
- en: We need to define two things in the authoring script – an environment we will
    run on and a run configuration, to which we will hand over the execution script,
    the environment, and a possible compute target.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `03_run_experiment_local.ipynb` notebook file. Compared to our previous
    notebooks, you can see that this is a very short file, as the actual Keras training
    is happening now in the execution script, which you can find in the `cifar10_cnn_remote.py`
    file in the `code` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define an environment. As we are still running locally, we
    create an environment with `user-managed-env`. This will just take our environment
    as is from our local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next block, we define the location and name of the execution script
    we want to run locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define a run configuration using a `ScriptRunConfig` object and
    attach to it the source directory, the script name, and our previously defined
    local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, execute the whole notebook, and while doing so, navigate to Azure Machine
    Learning Studio and look for the current run for our experiment called `cifar10_cnn_remote`.
    When it is visible, go to the `azureml-logs` and `logs/azureml` folders will now
    be populated with the logging output during the run.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.14* shows an example of the ingested streaming logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14  – The streaming logs of an Azure Machine Learning experiment
    run ](img/B17928_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – The streaming logs of an Azure Machine Learning experiment run
  prefs: []
  type: TYPE_NORMAL
- en: This is very handy, as now we don't really need to know where the code is ultimately
    executed. All we care about is seeing the output, the progress of the run while
    tracking all metrics, generated models, and all other artifacts. The link to the
    current run can be retrieved by calling the `print(run.get_portal_url())` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, instead of navigating to the Azure portal every time we run a training
    script, we can embed a widget in our notebook environment to give us the same
    (and more) functionality, directly within Jupyter, JupyterLab, or VS Code. To
    do so, we need to replace the `run.wait_for_completion()` line with the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Please be aware that you need to add the **Azure Widgets Python extension**
    to your environment. Please refer to this installation guide for the extension:
    [https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets.rundetails?view=azure-ml-py](https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets.rundetails?view=azure-ml-py).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's have a look at the execution script we are using. Open the file
    named `cifar10_cnn_remote.py` in the `code` directory. Scanning through this,
    you should find two additional parts that we added to the original model training
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is the part where we write debug logs into the `logs` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this call is that when we want to move to a remote execution
    environment, we need to infer the run context. Therefore, we need to load the
    `run` object from the current execution context instead of creating a new run,
    as shown in the previous sections, where we used the `exp.start_logging()` call.
  prefs: []
  type: TYPE_NORMAL
- en: The `run` object will be automatically linked with the experiment when it was
    scheduled through the authoring script. This is handy for remote execution, as
    we don't need to explicitly specify the `run` object in the execution script anymore.
    Using this inferred `run` object, we can log values, upload files and folders,
    and register models exactly as in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Running experiments on a cloud compute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After running our experiments so far on our local machine, let's proceed now
    as a final step in this chapter to run the same ML model on a compute target in
    the ML workspace.
  prefs: []
  type: TYPE_NORMAL
- en: The recommended compute target for training ML models in Azure is the managed
    Azure Machine Learning compute cluster, an auto-scaling compute cluster that is
    directly managed within your Azure subscription. If you have already used Azure
    for batch workloads, you will find it similar to Azure Batch and Azure Batch AI,
    with less configuration and tightly embedded in the Azure Machine Learning service.
  prefs: []
  type: TYPE_NORMAL
- en: There are three options to deploy a cluster, either through the Azure CLI and
    YAML, through the Python SDK, or through Azure Machine Learning Studio. In the
    following steps, we will use the first options, as they are becoming more prevalent,
    especially with MLOps. After that, we will see how with Python code the second
    option works as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `compute.yml` file in the working directory. You will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: compute.yml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This describes a compute cluster named `mldemocompute` that we want to deploy.
    This configuration defines a compute type (`amlcompute`) in the ML workspace with
    0–2 nodes with a VM size of **Standard D2v2** (2 CPUs, 7 GB of RAM, and 100 GB
    HDD) in the West US 2 Azure region. In addition, we define the idle time before
    the cluster scales down (shuts off) to be 15 minutes (which equals 900 seconds).
  prefs: []
  type: TYPE_NORMAL
- en: There are many other settings for compute clusters, including diverse network
    and load balancing settings. You can also define VM types with GPUs as your worker
    nodes – for example, **Standard_NC6** (6 CPUs, 56 GB of RAM, 340 GB SSD, 1 GPU,
    and 12 GB GPU memory) – by simply changing the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to other managed clusters, such as Azure Databricks, you don't pay
    for a head or master node, just for worker nodes. We will go into a lot more detail
    about VM types for deep learning in [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, and run distributed training on GPU
    clusters in [*Chapter 12*](B17928_12_ePub.xhtml#_idTextAnchor189), *Distributed
    Machine Learning on Azure*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are working with VS Code, the **Azure ML** extension (reachable in the
    Azure tab on the left) can show you YAML templates. Just go to your ML workspace,
    and under **mldemows** | **Compute** | **Compute clusters**, click on the **+**
    sign on the right. It will generate a template file, which looks like a bare version
    of the preceding one. In addition, if you have installed the YAML extension, it
    will understand the schema link in the file and will autocomplete your typing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the console and run the following CLI command to create the compute instance
    from the YAML file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can also call the shell script in the working directory called `04_setup_azure_ml_compute.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: After a short while, it will give you an output showing the properties of the
    created compute cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Open the notebook called `05_run_experiment_remote.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The second block in that notebook shows you the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The `except` clause of the `try` construct shows you the way you can create
    a compute cluster through the Python SDK. As the name of the cluster is the same
    as the one we already deployed via the CLI, when executing this block, it will
    just link our compute to the `aml_cluster` object through the `try` clause.
  prefs: []
  type: TYPE_NORMAL
- en: Either way, this `try..except` clause is very handy, as it either gives us back
    the already existing cluster or creates a new one for us. The final line of code
    is necessary if the compute target does not already exist, as we need to wait
    for the compute target to be ready to receive the run configuration in the next
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now have a look at the environment definition and the run configuration,
    we will see some minor changes to the code from the `03_run_experiment_local.ipynb`
    notebook. Our environment definition now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we attach to the environment our PIP configuration file we
    worked with locally. In the backend, the SDK will convert this to a **Conda properties
    file** and create a container from a Docker base image. If you run the cells up
    to this one, you will see which base image and configuration Azure Machine Learning
    builds based on this input. A small excerpt of this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Having a look at the final block in the notebook, we can see that the only difference
    is that we now define the compute target to be our `aml_cluster` in the run configuration
    and pass the new environment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we now run the whole notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training script is now executed in the remote compute target on Azure.
    In the experiment run in Azure Machine Learning Studio, the snapshot, outputs,
    and logs look very similar to the local run. However, we can now also see the
    logs of the Docker environment build process for the compute target, as shown
    in *Figure 3.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – The Docker build phase for a remote experiment run ](img/B17928_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – The Docker build phase for a remote experiment run
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final exercise, let''s understand the steps that are performed when we
    submit this run to the Azure Machine Learning workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: The Azure Machine Learning service builds a Docker container from the defined
    environment if it doesn't exist already.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning service registers your environment in the private
    container registry so that it can be reused for other scripts and deployments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning service queues your script execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning compute initializes and scales up a compute node
    using the defined container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning compute executes the script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning compute captures logs, artifacts, and metrics and
    streams them to the Azure Machine Learning service, and inlines the logs in the
    Jupyter notebook through the widget.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning service stores all artifacts in the workspace storage
    and your metrics in Application Insights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning service provides you with all the information about
    the run through Azure Machine Learning Studio or the Python SDK.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure Machine Learning compute automatically scales itself down after 15
    minutes (in our case) of inactivity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations on following along with this exercise. Given that it took us
    maybe 5 minutes to set up the Azure Machine Learning workspace, we get a fully
    fledged batch compute scheduling and execution environment for all our ML workloads.
    Many bits and pieces of this environment can be tuned and configured to our liking,
    and best of all, everything can be automated through the Azure CLI or the Azure
    Python SDK. Throughout the book, we will use these tools to configure, start,
    scale, and delete clusters for training and scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes the first part of this book. By now, you should have a good idea
    of what ML in general entails, what services and options are available in Azure,
    and how to utilize the Azure Machine Learning service to do ML experimentation
    and enhance your existing ML modeling scripts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next part of the book, we will concentrate on one of the aspects of ML
    often overlooked, the data itself. It is extremely vital to get this right. You
    might have heard the phrase *garbage in, garbage out* before, which holds true.
    Therefore, we will be working on removing as many pitfalls as possible by running
    automated data ingestion, cleaning and preparing data, extracting features, and
    performing labeling. In the end, we will bring all our knowledge together to discuss
    how to set up an ingestion and training ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: As the first step of this process, we need to understand different data sources
    and formats and bring our data to the Azure Machine Learning workspace, which
    we will discuss in the next chapter.
  prefs: []
  type: TYPE_NORMAL
