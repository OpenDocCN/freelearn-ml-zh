<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer033">
			<h1 class="chapter-number"><a id="_idTextAnchor336"/>3</h1>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor337"/>AWS Services for Data Migration and Processing</h1>
			<p>In the previous chapter, you learned about several ways of storing data in AWS. In this chapter, you will explore the techniques for using that data and gaining some insight from the data. There are use cases where you have to process your data or load the data to a hive data warehouse to query and analyze the data. If you are on AWS and your data is in S3, then you have to create a table in hive on AWS EMR to query the data in the hive table. To provide the same functionality as a managed service, AWS has a product called Athena, where you create a data catalog and query your data on S3. If you need to transform the data, then AWS Glue is the best option to transform and restore it to S3. Imagine a use case where you need to stream data and create analytical reports on that data. For this, you can opt for AWS Kinesis Data Streams to stream data and store it in S3. Using Glue, the same data can be copied to Redshift for further analytical utilization. AWS <strong class="bold">Database Migration Service</strong> (<strong class="bold">DMS</strong>) provides seamless migration of heterogeneous and homogeneous databases. This chapter will cover the following topics that are required for the purpose of <span class="No-Break">the certification:</span></p>
			<ul>
				<li>Using Glue to design <span class="No-Break">ETL jobs</span></li>
				<li>Querying S3 data <span class="No-Break">using Athena</span></li>
				<li>Streaming data through AWS Kinesis Data Streams and storing it using <span class="No-Break">Kinesis Firehose</span></li>
				<li>Ingesting data from on-premises locations <span class="No-Break">to AWS</span></li>
				<li>Migrating data to AWS and extending on-premises data centers <span class="No-Break">to AWS</span></li>
				<li>Processing data <span class="No-Break">on AWS</span></li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor338"/><a id="_idTextAnchor339"/>Technical requirements</h1>
			<p>You can download the data used in the examples from GitHub, available <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-66"><strong class="bold"><a id="_idTextAnchor340"/><a id="_idTextAnchor341"/>Creating ETL jobs o</strong><strong class="bold">n AWS Glue</strong></h1>
			<p>In a modern data pipeline, there are<a id="_idTextAnchor342"/> multiple stages, such as generating data, collecting data, storing data, performing ETL, analyzing, and visualizing. In this section, you will cover each of these at <a id="_idTextAnchor343"/>a high level and <a id="_idTextAnchor344"/>understand the <strong class="bold">extract, transform, load (ETL)</strong> process <span class="No-Break">in depth:</span></p>
			<ul>
				<li>Data can be generated from several devices, including mobile devices or IoT, weblogs, social media, transactional data, and <span class="No-Break">online games.</span></li>
				<li>This huge amount of generated data can be collected by using polling services, through API gateways integrated with AWS Lambda to collect the data, or via streams such as AWS Kinesis, AWS-managed Kafka, or Kinesis Firehose. If you have an on-premises database and you want to bring that data to AWS, then you would choose AWS DMS for that. You can sync your on-premises data to Amazon S3, Amazon EFS, or Amazon FSx via AWS DataSync. AWS Snowball is used to collect/transfer data into and out <span class="No-Break">of AWS.</span></li>
				<li>The next step involves storing data. You learned about some of the services to do this in the previous chapter, such as S3, EBS, EFS, RDS, Redshift, <span class="No-Break">and DynamoDB.</span></li>
				<li>Once you know your data storage requirements, an ETL job can be designed to extract-transform-load or extract-load-transform your structured or unstructured data into the format you desire for further analysis. For example, you can use AWS Lambda to transform the data on the fly and store the transformed data in S3, or you can run a Spark application on an EMR cluster to transform the data and store it in S3 or Redshift <span class="No-Break">or RDS.</span></li>
				<li>There are many services available in AWS for performing an analysis on transformed data, for example, EMR, Athena, Redshift, Redshift Spectrum, and <span class="No-Break">Kinesis Analytics.</span></li>
				<li>Once the data is analyzed, you can visualize it using AWS QuickSight to understand the patterns or trends. Data scientists or machine learning professionals would want to apply statistical analysis to understand data distribution in a better way. Business users use statistical analysis to prepare reports. You will learn and explore various ways to present and visualize data in <a href="B21197_05.xhtml#_idTextAnchor638"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Data Understanding </em><span class="No-Break"><em class="italic">and Visualization.</em></span></li>
			</ul>
			<p>What you understood<a id="_idTextAnchor345"/> from the traditional data pipeline is that ETL is all about coding and maintaining code on the servers so that everything runs smoothly. If the data format changes in any way, then the code needs to be changed, and that results in a <a id="_idTextAnchor346"/>change to the target schema. If the data source changes, then the code must be able to handle that too, and it’s an overhead. <em class="italic">Should you write code to recognize these changes in data sources? Do you need a system to adapt to the change and discover the data for you?</em> The answer to these questions is yes, and to do so, you can use <strong class="bold">AWS Glue</strong>. Now, you will learn why AWS Glue is <span class="No-Break">so popular.</span></p>
			<h2 id="_idParaDest-67"><strong class="bold"><a id="_idTextAnchor347"/><a id="_idTextAnchor348"/>Features of AWS Glue</strong></h2>
			<p>AWS Glue is a completely managed <a id="_idTextAnchor349"/>serverless ETL service on AWS. It has the <span class="No-Break">following features:</span></p>
			<ul>
				<li>It automatically discovers and categorizes your data by connecting to the data sources and generates a <span class="No-Break">data catalog.</span></li>
				<li>Services such as Amazon Athena, Amazon Redshift, and Amazon EMR can use the data catalog to query <span class="No-Break">the data.</span></li>
				<li>AWS Glue generates the ETL code, which is an extension to Spark in Python or Scala, which can be <span class="No-Break">modified, too.</span></li>
				<li>It scales out automatically to match your Spark application requirements for running the ETL job and loading the data into <span class="No-Break">the destination.</span></li>
			</ul>
			<p>AWS Glue has the <strong class="bold">Data Catalog</strong>, and that’s the<a id="_idTextAnchor350"/> secret to its success. It helps with discovering data from data sources and understanding a bit <span class="No-Break">about it:</span></p>
			<ul>
				<li>The Data Catalog automatically discovers new data and extracts schema definitions. It detects schema changes and version tables. It detects Apache Hive-style partitions on <span class="No-Break">Amazon S3.</span></li>
				<li>The Data Catalog comes with built-in classifiers for popular data types. Custom classifiers can be<a id="_idTextAnchor351"/> written using <strong class="bold">Grok expressions</strong>. The classifiers help to detect <span class="No-Break">the schema.</span></li>
				<li>Glue crawlers can be run ad hoc or in a scheduled fashion to update the metadata in the Glue Data Catalog. Glue crawlers must be associated with an IAM role with sufficient access to read the data sources, such as Amazon RDS, Redshift, <span class="No-Break">and S3.</span></li>
			</ul>
			<p>As you now have a<a id="_idTextAnchor352"/> brief idea of what AWS Glue is used for, move on to run the following example to get your <span class="No-Break">hands dirty.</span></p>
			<h2 id="_idParaDest-68"><strong class="bold"><a id="_idTextAnchor353"/><a id="_idTextAnchor354"/>Getting hands-on with AWS Glue Data Catalog components</strong></h2>
			<p>In this example, you will <a id="_idTextAnchor355"/>create a job to copy data from S3 to Redshift by using AWS Glue. All my components were created in the <strong class="source-inline">us-east-1</strong> region. Start by creating <span class="No-Break">a bucket:</span></p>
			<ol>
				<li>Navigate to the AWS S3 console and create a bucket. I have named the <span class="No-Break">bucket </span><span class="No-Break"><strong class="source-inline">aws-glue-example-01</strong></span><span class="No-Break">.</span></li>
				<li>Click on <strong class="bold">Create Folder</strong> and name <span class="No-Break">it </span><span class="No-Break"><strong class="source-inline">input-data</strong></span><span class="No-Break">.</span></li>
				<li>Navigate inside the folder and click on the <strong class="bold">Upload</strong> button to upload the <strong class="source-inline">sales-records.csv</strong> dataset. The data is available in the following GitHub <span class="No-Break">location: </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data</span></a><span class="No-Break">.</span><p class="list-inset">As you have the data uploaded in the S3 bucket, now create a VPC in which you will create your <span class="No-Break">Redshift cluster.</span></p></li>
				<li>Navigate to the VPC console by accessing the <a href="https://console.aws.amazon.com/vpc/home?region=us-east-1#">https://console.aws.amazon.com/vpc/home?region=us-east-1#</a> URL and click on <strong class="bold">Endpoints</strong> on the left-hand side menu. Click on <strong class="bold">Create Endpoint</strong> and then fill in the fields as <span class="No-Break">shown here:</span><ol><li class="Alphabets"><strong class="bold">Service Category</strong>: <span class="No-Break"><strong class="source-inline">AWS services</strong></span></li><li class="Alphabets"><strong class="bold">Select a service</strong>: <strong class="source-inline">com.amazonaws.us-east-1.s3</strong> (<span class="No-Break">gateway type)</span></li><li class="Alphabets"><strong class="bold">VPC</strong>: Select <strong class="bold">the Default VPC</strong> (use this default VPC in which your Redshift cluster will <a id="_idTextAnchor356"/><span class="No-Break">be created)</span></li></ol></li>
				<li>Leave the other fields as is and click on <span class="No-Break"><strong class="bold">Create Endpoint</strong></span><span class="No-Break">.</span></li>
				<li>Click on <strong class="bold">Security Groups</strong> from the VPC console. Give a name to your security group, such as <strong class="source-inline">redshift-self</strong>, and choose the default VPC drop-down menu. Provide an appropriate description, such as <strong class="source-inline">Redshift Security Group</strong>. Click on <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">security group</strong></span><span class="No-Break">.</span></li>
				<li>Click on the <strong class="bold">Actions</strong> dropdown and select <strong class="bold">Edit Inbound rules</strong>. Click on <strong class="bold">Add rule</strong> and complete the fields as <span class="No-Break">shown here:</span><ol><li class="Alphabets"><strong class="bold">Type</strong>: <span class="No-Break"><strong class="source-inline">All traffic</strong></span></li><li class="Alphabets"><span class="No-Break"><strong class="bold">Source</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">Custom</strong></span></li><li class="Alphabets">In the search field, select the same security <span class="No-Break">group (</span><span class="No-Break"><strong class="source-inline">redshift-self</strong></span><span class="No-Break">)</span></li></ol></li>
				<li>Click on <span class="No-Break"><strong class="bold">Save Rules</strong></span><span class="No-Break">.</span><p class="list-inset">Now, create your <span class="No-Break">Redshift cluster.</span></p></li>
				<li>Navigate to the Amazon Redshift console. Click on <strong class="bold">Create Cluster</strong> and complete the highlighted fields, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span><div id="_idContainer023" class="IMG---Figure"><img src="image/B21197_03_01.jpg" alt="Figure 3.1 – A screenshot of Amazon Redshift’s Create cluster screen" width="1108" height="680"/></div></li>
			</ol>
			<p class="IMG---Figure"><strong class="bold"><a id="_idTextAnchor357"/></strong></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A screenshot of Amazon Redshift’s Create cluster screen</p>
			<ol>
				<li value="10">Scroll down a<a id="_idTextAnchor358"/>nd fill in the highlighted fields shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2 </em>with your <span class="No-Break">own values:</span><div id="_idContainer024" class="IMG---Figure"><img src="image/B21197_03_02.jpg" alt="Figure 3.2 – A screenshot of an Amazon Redshift cluster’s Database configurations section" width="1053" height="644"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor359"/>Figure 3.2 – A screenshot of an Amazon Redshift cluster’s Database configurations section</p>
			<ol>
				<li value="11">Scroll down and ch<a id="_idTextAnchor360"/>ange the <strong class="bold">Additional configurations</strong> settings, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span><div id="_idContainer025" class="IMG---Figure"><img src="image/B21197_03_03.jpg" alt="Figure 3.3 – A screenshot of an Amazon Redshift cluster’s Additional configurations section" width="1221" height="749"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">F<a id="_idTextAnchor361"/>igure 3.3 – A screenshot of an Amazon Redshift cluster’s Additional configurations section</p>
			<ol>
				<li value="12">Change the IAM pe<a id="_idTextAnchor362"/>rmissions too, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span><div id="_idContainer026" class="IMG---Figure"><img src="image/B21197_03_04.jpg" alt="Figure 3.4 – A screenshot of an Amazon Redshift cluster’s Cluster permissions section" width="1218" height="461"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fi<a id="_idTextAnchor363"/>gure 3.4 – A screenshot of an Amazon Redshift cluster’s Cluster permissions section</p>
			<ol>
				<li value="13">Scroll down and click on <strong class="bold">Create Cluster</strong>. It will take a minute or two to get the cluster in the <span class="No-Break">available state.</span><p class="list-inset">Next, you will crea<a id="_idTextAnchor364"/>te an <span class="No-Break">IAM role.</span></p></li>
				<li>Navigate to the AWS IAM console and select <strong class="bold">Roles</strong> in the <strong class="bold">Access Management</strong> section on <span class="No-Break">the screen.</span></li>
				<li>Click on the <strong class="bold">Create role</strong> button and choose <strong class="bold">Glue</strong> from the services. Click on the <strong class="bold">Next: permissions</strong> button to navigate to the <span class="No-Break">next page.</span></li>
				<li>Search for <strong class="source-inline">AmazonS3FullAccess</strong> and select it. Then, search for <strong class="source-inline">AWSGlueServiceRole</strong> and select it. As you are writing your data to Redshift as part of this example, select <strong class="bold">AmazonRedshiftFullAccess</strong>. Click on <strong class="bold">Next: Tags</strong>, followed by the <strong class="bold">Next: </strong><span class="No-Break"><strong class="bold">Review</strong></span><span class="No-Break"> button.</span></li>
				<li>Provide a name, <strong class="source-inline">Glue-IAM-Role</strong>, and then click on the <strong class="bold">Create role</strong> button. The role appears as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span><div id="_idContainer027" class="IMG---Figure"><img src="image/B21197_03_05.jpg" alt="Figure 3.5 – A screenshot of the IAM role" width="1219" height="663"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fig<a id="_idTextAnchor365"/>ure 3.5 – A screenshot of the IAM role</p>
			<p class="list-inset">Now, you have the <a id="_idTextAnchor366"/>input data source and the output data storage handy. The next step is to create the Glue crawler from the AWS <span class="No-Break">Glue console.</span></p>
			<ol>
				<li value="18">Select <strong class="bold">Connections</strong> under <strong class="bold">Databases</strong>. Click on the <strong class="bold">Add connection</strong> button and complete the fields as <span class="No-Break">shown here:</span><ol><li class="Alphabets"><strong class="bold">Connection </strong><span class="No-Break"><strong class="bold">name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">glue-redshift-connection</strong></span></li><li class="Alphabets"><strong class="bold">Connection type</strong>: <span class="No-Break"><strong class="source-inline">Amazon Redshift</strong></span></li></ol></li>
				<li>Click on <strong class="bold">Next</strong> and then fill in the fields as <span class="No-Break">shown here:</span><ol><li class="Alphabets"><span class="No-Break"><strong class="bold">Cluster</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">redshift-glue-example</strong></span></li><li class="Alphabets"><strong class="bold">Database </strong><span class="No-Break"><strong class="bold">name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">glue-dev</strong></span></li><li class="Alphabets"><span class="No-Break"><strong class="bold">Username</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">awsuser</strong></span></li><li class="Alphabets"><strong class="bold">Password</strong>: <strong class="source-inline">********</strong> (enter the value chosen in <span class="No-Break"><em class="italic">step 10</em></span><span class="No-Break">)</span></li></ol></li>
				<li>Click on <strong class="bold">Next</strong> and then <strong class="bold">Finish</strong>. To verify that it’s working, click on <strong class="bold">Test Connection</strong>, select <strong class="bold">Glue-IAM-Role</strong> in the IAM role section, and then click <span class="No-Break"><strong class="bold">Test Connection</strong></span><span class="No-Break">.</span></li>
				<li>Go to Crawler and select Add Crawler. Provide a name for the crawler, S3-glue-crawler, and then click Next. On the Specify crawler source type page, leave everything as thei<a id="_idTextAnchor367"/>r default settings and then <span class="No-Break">click Next.</span></li>
				<li>On the<strong class="bold"> Add a data store</strong> page, choose <strong class="bold">Include path</strong> option and <span class="No-Break">enter </span><span class="No-Break"><strong class="source-inline">s3://aws-glue-example-01/input-data/sales-records.csv</strong></span><span class="No-Break">.</span></li>
				<li><span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Set<strong class="bold"> Add another datastore</strong> to <strong class="source-inline">No</strong>. <span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>For <strong class="bold">Choose an existing IAM Role</strong>, set <strong class="source-inline">Glue-IAM-Role</strong>. Then, <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Set<strong class="bold"> Frequency</strong> to <strong class="bold">Run on demand</strong>. <span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>No database has been created, so click on <strong class="bold">Add database</strong>, provide a name, <strong class="source-inline">s3-data</strong>, click <strong class="bold">Next</strong>, and then <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Finish</strong></span><span class="No-Break">.</span></li>
				<li>Select the crawler, <strong class="bold">S3-glue-crawler</strong>, and then click on <strong class="bold">Run Crawler</strong>. Once the run is complete, you can see that there is a <em class="italic">1</em> in the <strong class="bold">Tables Added</strong> column. This means that a database, <strong class="source-inline">s3-data</strong>, has been created, as mentioned in the previous step, and a table has been added. Click on <strong class="bold">Tables</strong> and select the newly created table, <strong class="bold">sales_records_csv</strong>. You can see that the schema has been discovered now. You can change the data type if the inferred data type does not meet <span class="No-Break">your requirements.</span></li>
			</ol>
			<p>In this hands-on section, you learned about database tables, database connections, crawlers in S3, and the creation of a Redshift cluster. In the next hands-on section, you will learn about creating ETL jobs <span class="No-Break">using Glue.</span></p>
			<h2 id="_idParaDest-69"><strong class="bold">Getti<a id="_idTextAnchor368"/><a id="_idTextAnchor369"/>ng hands-on with AWS Glue ETL components</strong></h2>
			<p>In this section, you will use the Data <a id="_idTextAnchor370"/>Catalog components created earlier to build a job. You will start by creating <span class="No-Break">a job:</span></p>
			<ol>
				<li>Navigate to the AWS Glue console and click on <strong class="bold">Jobs</strong> under the <span class="No-Break"><strong class="bold">ETL</strong></span><span class="No-Break"> section.</span></li>
				<li>Click on the <strong class="bold">Add Job</strong> button and complete the fields as <span class="No-Break">shown here:</span><ul><li><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">s3-glue-redshift</strong></span></li><li><strong class="bold">IAM role</strong>: <strong class="bold">Glue-IAM-Role</strong> (this is the same role you created in the <span class="No-Break">previous section)</span></li><li><span class="No-Break"><strong class="bold">Type</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">Spark</strong></span></li><li><strong class="bold">Glue version</strong>: <strong class="bold">Spark 2.4, Python 3 with improved job start up times (Glue </strong><span class="No-Break"><strong class="bold">version 2.0)</strong></span></li></ul></li>
				<li>Leave the other fields as they are and then click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Select <strong class="bold">sales_records_csv</strong> and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Select <strong class="bold">Change Schema</strong> by default and then click <strong class="bold">Next</strong> (at the time of writing this book, machine learning transformations are not supported for <span class="No-Break">Glue 2.0).</span></li>
				<li>Select <strong class="bold">Create tables in your data target</strong>. Choose <strong class="bold">JDBC</strong> as the data store and <strong class="bold">glue-redshift-connection</strong> as the connection. Provide <strong class="source-inline">glue-dev</strong> as the database name (as created in the previous section) and then <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Next comes the <strong class="bold">Output Schema Definition</strong> page, where you can choose the desired columns to be removed from the target schema. Scroll down and click on <strong class="bold">Save job and </strong><span class="No-Break"><strong class="bold">edit script</strong></span><span class="No-Break">.</span></li>
				<li>You can now see the pipeline being created on the left-hand side of the screen and the suggested code on the right-hand side, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6</em>. You can modify the code based on your requirements. Click on the <strong class="bold">Run job</strong> button. A pop-up window appears, asking you to edit any details that you wish to change. <p class="list-inset">This is optional. Then, click on the <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">job</strong></span><span class="No-Break"> button:</span></p><div id="_idContainer028" class="IMG---Figure"><img src="image/B21197_03_06.jpg" alt="Figure 3.6 – A screenshot of the AWS Glue ETL job" width="1229" height="1020"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figu<a id="_idTextAnchor371"/>re 3.6 – A screenshot of the AWS Glue ETL job</p>
			<ol>
				<li value="9">Once the job is succe<a id="_idTextAnchor372"/>ssful, navigate to Amazon Redshift and click on <span class="No-Break"><strong class="bold">Query editor</strong></span><span class="No-Break">.</span></li>
				<li>Set the database name as <strong class="source-inline">glue-dev</strong> and then provide the username and password to create <span class="No-Break">a connection.</span></li>
				<li>Select the <strong class="source-inline">public</strong> schema, and now you can query the table to see the records, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span><div id="_idContainer029" class="IMG---Figure"><img src="image/B21197_03_07.jpg" alt="Figure 3.7 – A screenshot of Amazon Redshift’s Query editor" width="900" height="444"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figur<a id="_idTextAnchor373"/>e 3.7 – A screenshot of Amazon Redshift’s Query editor</p>
			<p>You now understand how to create<a id="_idTextAnchor374"/> an ETL job using AWS Glue to copy the data from an S3 bucket to Amazon Redshift. You also queried data in Amazon Redshift using the query editor from the UI console. It is recommended to delete the Redshift cluster and AWS Glue job once you have completed the steps successfully. AWS creates two buckets in your account to store AWS Glue scripts and temporary results from AWS Glue. Delete these as well to save costs. You will use the data catalog created on S3 data in the <span class="No-Break">next section.</span></p>
			<p>In the following section, you will learn about querying S3 data <span class="No-Break">using Athena.</span></p>
			<h1 id="_idParaDest-70"><strong class="bold">Queryin<a id="_idTextAnchor375"/><a id="_idTextAnchor376"/>g S3 data us</strong><strong class="bold">ing Athena</strong></h1>
			<p>Athena is a serverless service design<a id="_idTextAnchor377"/>ed for querying data stored in S3. It is serverless because the client doesn’t manage the servers that are used <span class="No-Break">for computation:</span></p>
			<ul>
				<li>Athena uses a schema to present the result<a id="_idTextAnchor378"/>s against a query on the data stored in S3. You define how (the way or the structure) you want your data to appear in the form of a schema and Athena reads the raw data from S3 to show the results as per the <span class="No-Break">defined schema.</span></li>
				<li>The output can be used by other services for visualization, storage, or various analytics purposes. The source data in S3 can be in any of the following structured, semi-structured, or unstructured data formats: XML, JSON, CSV/TSV, AVRO, Parquet, or ORC (as well as others). CloudTrail, ELB logs, and VPC flow logs can also be stored in S3 and analyzed <span class="No-Break">by Athena.</span></li>
				<li>This follows the schema-on-read technique. Unlike traditional techniques, tables are defined in advance in a data catalog, and the data’s structure is validated against the table’s schema while reading the data from the tables. SQL-like queries can be carried out on data without transforming the <span class="No-Break">source data.</span></li>
			</ul>
			<p>Now, to help you understand this, here’s an example, where you will use <strong class="bold">AWSDataCatalog</strong> created in AWS Glue on the S3 data and query them <span class="No-Break">using Athena:</span></p>
			<ol>
				<li>Navigate to the AWS At<a id="_idTextAnchor379"/>hena console. Select <strong class="source-inline">AWSDataCatalog</strong> from <strong class="bold">Data source</strong> (if you are doing this for the first time, then a <strong class="source-inline">sampledb</strong> database will be created with a table, <strong class="source-inline">elb_logs</strong>, in the AWS Glue <span class="No-Break">Data Catalog).</span></li>
				<li>Select <strong class="source-inline">s3-data</strong> as <span class="No-Break">the database.</span></li>
				<li>Click on <strong class="bold">Settings</strong> in the top-right corner and fill in the details as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em> (I have used the same bucket as in the previous example and a <span class="No-Break">different folder):</span><div id="_idContainer030" class="IMG---Figure"><img src="image/B21197_03_08.jpg" alt="Figure 3.8 – A screenshot of Amazon Athena’s settings" width="1285" height="445"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure<a id="_idTextAnchor380"/> 3.8 – A screenshot of Amazon Athena’s settings</p>
			<ol>
				<li value="4">The next step is<a id="_idTextAnchor381"/> to write your query in the query editor and execute<a id="_idTextAnchor382"/> it. Once your execution is complete, please delete your S3 buckets and AWS Glue data catalogs. This will save <span class="No-Break">you money.</span></li>
			</ol>
			<p>In this section, you learned how to query S3 data using Amazon Athena through the AWS Glue Data Catalog. You also learned how to create a schema and query data from S3. In the next section, you will learn about Amazon Kinesis <span class="No-Break">Data Streams.</span></p>
			<h1 id="_idParaDest-71"><strong class="bold">Processi<a id="_idTextAnchor383"/><a id="_idTextAnchor384"/>ng real-time data using Kinesis Da</strong><strong class="bold">ta Streams</strong></h1>
			<p>Kinesis is Amazon’s streaming service<a id="_idTextAnchor385"/> and can be scaled based on requirements. It has a level of persistence that retains data for 24 hours by default or optionally up to 365 days. Kinesis Data Streams is used for large-scale data ingestion, analytics, <span class="No-Break">and monitoring:</span></p>
			<ul>
				<li>Kinesis streams can be ingested by multiple producers and multiple consumers can also read data from the streams. The following is an example to help you understand this. Suppose you have a producer ingesting data to a Kinesis stream and the default retention period is 24 hours, which means data ingested at 05:00:00 A.M. today will be available in the stream until 04:59:59 A.M. tomorrow. This data won’t be available beyond that point, and ideally, it should be consumed before it expires; otherwise, it can be stored somewhere if it’s critical. The retention period can be extended to a maximum of 365 days, at an <span class="No-Break">extra cost.</span></li>
				<li>Kinesis can be used<a id="_idTextAnchor386"/> for real-time analytics or dashboard visualization. Producers can be imagined as a piece of code pushing data into the Kinesis stream, and it can be an EC2 instance, a Lambda function, an IoT device, on-premises servers, mobile applications or devices, and so on running <span class="No-Break">the code.</span></li>
				<li>Similarly, the consumer can also be a piece of code running on an EC2 instance, Lambda function, or on-premises servers that know how to connect to a Kinesis stream, read the data, and apply some action to the data. AWS provides triggers to invoke a Lambda consumer as soon as data arrives in the <span class="No-Break">Kinesis stream.</span></li>
				<li>Kinesis is scalable due to its shard ar<a id="_idTextAnchor387"/>chitecture, which is the fundamental throughput unit of a Kinesis stream. <em class="italic">What is a shard?</em> A shard is a logical structure that partitions the data based on a partition key. A shard supports a writing capacity of <em class="italic">1 MB/sec</em> and a reading capacity of <em class="italic">2 MB/sec</em>. 1,000 <strong class="source-inline">PUT</strong> records per second are supported by a single shard. If you have created a stream with <em class="italic">three shards</em>, then <em class="italic">3 MB/sec write throughput</em> and <em class="italic">6 MB/sec read throughput</em> can be achieved, and this allows 3,000 <strong class="source-inline">PUT</strong> records. So, with more shards, you have to pay an extra amount to get <span class="No-Break">higher performance.</span></li>
				<li>The data in a shard is stored via a Kinesis data record and can be a maximum of 1 MB. Kinesis data records are stored across the shard based on the partition key. They also have a sequence number. A sequence number is assigned by Kinesis as soon as a <strong class="source-inline">putRecord</strong> or <strong class="source-inline">putRecords</strong> API operation is performed so as to uniquely identify a record. The partition key is specified by the producer while adding the data to the Kinesis data stream, and the partition key is responsible for segregating and routing the record to different shards in the stream to balance <span class="No-Break">the load.</span></li>
				<li>There are two ways to encrypt<a id="_idTextAnchor388"/> the data in a Kinesis stream: server-side encryption and client-side encryption. Client-side encryption makes it hard to implement and manage the keys because the client has to encrypt the data before putting it into the stream and decrypt the data after reading it from the stream. With server-side encryption enabled via <strong class="bold">AWS KMS</strong>, the data is automatically encrypted and decrypted as you put the data and get it from <span class="No-Break">a stream.</span></li>
			</ul>
			<p class="callout-heading"><strong class="bold">Note</strong></p>
			<p class="callout">Amazon Kinesis shouldn’t be confused with Amazon SQS. Amazon SQS supports one production group and one consumption group. If your use case demands multiple users sending data and receiving data, then Kinesis is <span class="No-Break">the solution.</span></p>
			<p class="callout">For decoupling and asynchronous communications, SQS is the solution, because the sender and receiver do not need to be aware of <span class="No-Break">one another.</span></p>
			<p class="callout">In SQS, there is no concept of persistence. Once the message is read, the next step is deletion. There’s no concept of a retention time window for Amazon SQS. If your use case demands large-scale ingestion, then Kinesis should <span class="No-Break">be used.</span></p>
			<p>In the next section, you will learn about storing the streamed data for <span class="No-Break">further analysis.</span></p>
			<h1 id="_idParaDest-72"><strong class="bold">Storing <a id="_idTextAnchor389"/><a id="_idTextAnchor390"/>and transforming real-time data using Kinesis Dat</strong><strong class="bold">a Firehose</strong></h1>
			<p>There are a lot of use cases t<a id="_idTextAnchor391"/>hat require data to be streamed and stored for future analytics purposes. To overcome such problems, you can write a Kinesis consumer to read the Kinesis stream and store the data in S3. This solution needs an instance or a machine to run the code with the required access to read from the stream and write to S3. The other possible option would be to run a Lambda function that gets triggered on the <strong class="source-inline">putRecord</strong> or <strong class="source-inline">putRecords</strong> API made to the stream and reads the data from the stream to store in the <span class="No-Break">S3 bucket:</span></p>
			<ul>
				<li>To make this easy, Amazon provides<a id="_idTextAnchor392"/> a separate service called Kinesis Data Firehose. This can easily be plugged into a Kinesis data stream and will require essential IAM roles to write data into S3. It is a fully managed service to reduce the load of managing servers and code. It also supports loading the streamed data into Amazon Redshift, Elasticsearch, and Splunk. Kinesis Data Firehose scales automatically to match the throughput of <span class="No-Break">the data.</span></li>
				<li>Data can be transformed via an AWS Lambda function before storing or delivering it to the destination. If you want to build a raw data lake with the untransformed data, then by enabling source record backup, you can store it in another S3 bucket prior to <span class="No-Break">the transformation.</span></li>
				<li>With the help of AWS KMS, data can be encrypted following delivery to the S3 bucket. It has to be enabled while creating the delivery stream. Data can also be compressed in supported formats, such as gzip, ZIP, <span class="No-Break">or Snappy.</span></li>
			</ul>
			<p>In the next section, you will learn about different AWS services used for ingesting data from on-premises servers <span class="No-Break">to AWS.</span></p>
			<h1 id="_idParaDest-73"><strong class="bold">Differen<a id="_idTextAnchor393"/><a id="_idTextAnchor394"/>t ways of ingesting data from on</strong><strong class="bold">-premises </strong><strong class="bold">
into AWS</strong></h1>
			<p>With the increa<a id="_idTextAnchor395"/>sing demand for data-dri<a id="_idTextAnchor396"/>ven use cases, managing data on on-premises servers is pretty tough at the moment. Taking backups is not easy when you deal with a huge amount of data. This data in data lakes is used to build deep neural networks, create a data warehouse to extract meaningful information from it, run analytics, and <span class="No-Break">generate reports.</span></p>
			<p>Now, if you look at the available options to migrate data into AWS, this comes with various challenges too. For example, if you want to send data to S3, then you have to write a few lines of code to send your data to AWS. You will have to manage the code and servers to run the code. It has to be ensured that the data is commuting via the HTTPS network. You need to verify whether the data transfer was successful. This adds complexity as well as time and effort challenges to the process. To avoid such scenarios, AWS provides services to match or<a id="_idTextAnchor397"/> solve your use cases by designing a hybrid infrastructure that allows data sharing between the on-premises data centers and AWS. You will learn about these in the <span class="No-Break">following sections.</span></p>
			<h2 id="_idParaDest-74"><strong class="bold">AWS Stor<a id="_idTextAnchor398"/><a id="_idTextAnchor399"/>age Gateway</strong></h2>
			<p>Storage Gateway is a hybrid storage virtual <a id="_idTextAnchor400"/>appliance. It can run in three different modes – <strong class="bold">File Gateway</strong>, <strong class="bold">Tape Gateway</strong>, and <strong class="bold">Volume Gateway</strong>. It can be used for the extension, migration, and backups <a id="_idTextAnchor401"/>of an on-premises data center <span class="No-Break">to AWS:</span></p>
			<ul>
				<li>In Tape Gateway mode, Storage Gateway stores virtual tapes on S3, and when ejected and archived, the tapes are moved from S3 to Glacier. Active tapes are stored in S3 for storage and retrieval. Archived or exported tapes are stored in <strong class="bold">Virtual Tape Shelf</strong> (<strong class="bold">VTS</strong>) in Glacier. Virtual tapes ca<a id="_idTextAnchor402"/>n be created and can range in size from 100 GiB to 5 TiB. A total of 1 petabyte of storage can be configured locally and an unlimited number of tapes can be archived to Glacier. This is ideal for an existing backup system on tape and where there is a need to migrate backup data into AWS. You can decommission the physical tape <span class="No-Break">hardware later.</span></li>
				<li>In File Gateway mode, Storage Gateway maps files onto S3 objects, which can be stored using one of the available storage classes. This helps you to extend the data center into AWS. You can load more files to your file gateway and these are stored as S3 objects. It can run on your on-premises virtual server, which connects to various devices <a id="_idTextAnchor403"/>using <strong class="bold">Server Message Block (SMB)</strong> or <strong class="bold">Network File System (NFS)</strong>. File Gateway connects to AWS using an HTTPS public endpoint to store the data on S3 objects. Life cycle policies can be applied to those S3 objects. You can easily integrate your <strong class="bold">Active</strong> <strong class="bold">Directory</strong> (<strong class="bold">AD</strong>) with File Gateway to control access to the files on the <span class="No-Break">file share.</span></li>
				<li>In Volume Gateway mode, the storage gateway presents block storage. There are two ways of using this; one is <strong class="bold">Gateway Cached</strong> and the other is <span class="No-Break"><strong class="bold">Gateway Stored</strong></span><span class="No-Break">:</span></li>
				<li><strong class="bold">Gateway Stored</strong> is a volume storage gateway running locally on-premises. It has local storage <a id="_idTextAnchor404"/>and an upload buffer. A total of 32 volumes can be created, and each volume can be up to 16 TB in size for a total capacity of 512 TB. Primary data is store<a id="_idTextAnchor405"/>d on-premises and backup data is asynchronously replicated to AWS in the background. Volumes are made available via <strong class="bold">Internet Small Computer Systems Interface (iSCSI)</strong> for network-based servers to access. It connects to a Storage Gateway endpoint via an HTTPS public endpoint and creates EBS snap<a id="_idTextAnchor406"/>shots from backup data. These snapshots can be used to create standard EBS volumes. This option is ideal for migration to AWS, disaster recovery, or business continuity. The local system will still use the local volume, but the EBS snapshots are in AWS, which can be used instead of backups. It’s not the best option for data center extensions because you require a huge amount of <span class="No-Break">local storage.</span></li>
				<li><strong class="bold">Gateway Cached</strong> is a volume storage <a id="_idTextAnchor407"/>gateway running locally on-premises. It has cache storage and an upload buffer. The difference is that the data that is added to Storage Gateway is not local but uploaded to AWS. Primary data is stored in AWS. Frequently accessed data is cached locally. This is an ideal option for extending an on-pr<a id="_idTextAnchor408"/>emises data center to AWS. It connects to a Storage Gateway endpoint via an HTTPS public endpoint and creates S3-backed volume (AWS-managed bucket) snapshots that are stored as standard <span class="No-Break">EBS snapshots.</span></li>
			</ul>
			<h2 id="_idParaDest-75"><strong class="bold">Snowball<a id="_idTextAnchor409"/><a id="_idTextAnchor410"/>, Snowball Edge, and Snowmobile</strong></h2>
			<p>These belong to the same product category or family for the physical transfer of data between business operating locations and AWS. To move a large amount of data into and out of AWS, you can use any of <span class="No-Break">the three:</span></p>
			<ul>
				<li><strong class="bold">Snowball</strong>: This physical device can be ordered from AWS by l<a id="_idTextAnchor411"/>ogging a job. AWS delivers a device for you to load your data onto before sending it back. Data in Snowball<a id="_idTextAnchor412"/> is encrypted using KMS. It comes with two capacity ranges: 50 TB and 80 TB. It is economical to order one or more Snowball devices for data between 10 TB and 10 PB. The device can be sent to different premises. It does not have any compute capability; it only comes with <span class="No-Break">storage capability.</span></li>
				<li><strong class="bold">Snowball Edge</strong>: This is like Snowbal<a id="_idTextAnchor413"/>l, but it comes with both storage and compute capability. It has a larger capacity than Snowbal<a id="_idTextAnchor414"/>l. It offers fastened networking, such as 10 Gbps over RJ45, 10/25 Gb over SFP28, and 40/100 Gb+ over QSFP+ copper. This is ideal for the secure and quick transfer of terabytes to petabytes of data <span class="No-Break">into AWS.</span></li>
				<li><strong class="bold">Snowmobile</strong>: This is a portable data center w<a id="_idTextAnchor415"/>ithin a shipping container on a truck. It allows you to move exabytes of data fr<a id="_idTextAnchor416"/>om on-premises to AWS. If your data size exceeds 10 PB, then Snowmobile is preferred. Essentially, upon requesting to use the Snowmobile service, a truck is driven to your location and you plug your data center into the truck and transfer the data. If you have multiple sites, choosing Snowmobile for data transfer is not an <span class="No-Break">ideal option.</span></li>
			</ul>
			<h2 id="_idParaDest-76"><strong class="bold">AWS Data<a id="_idTextAnchor417"/><a id="_idTextAnchor418"/>Sync</strong></h2>
			<p>AWS DataSync is designed to move dat<a id="_idTextAnchor419"/>a from on-premises storage to AWS, or <span class="No-Break">vice versa:</span></p>
			<ul>
				<li>It is an ideal product from AWS for data processing transfers, archival or cost-effective storage, disaster recovery, business continuity, and <span class="No-Break">data migrations.</span></li>
				<li>It has a special data validation feature that verifies the original data with the data in AWS, as soon as the data arrives in AWS. In other words, it checks the integrity of <span class="No-Break">the data.</span></li>
				<li>To understand this product in depth, consider an example of an on-premises data center that has SAN/NAS storage. When you run the AWS DataSync agent on a VMWare platform, this agent is capable of communicating with the NAS/SAN storage via an NFS/SMB protocol. Once it is on, it communicates with the AWS DataSync endpoint, and from there, it can connect with several different types of locations, including various S3 storage classes<a id="_idTextAnchor420"/> or VPC-based resources, such as <strong class="bold">Elastic</strong> <strong class="bold">File</strong> <strong class="bold">System</strong> (<strong class="bold">EFS</strong>) and FSx for <span class="No-Break">Windows Server.</span></li>
				<li>It allows you to schedule data transfers during specific periods. By configuring the built-in bandwidth throttle, you can limit the amount of network bandwidth that <span class="No-Break">DataSync uses.</span></li>
			</ul>
			<h2 id="_idParaDest-77"><strong class="bold"><a id="_idTextAnchor421"/>AWS Database Migration Service</strong></h2>
			<p>There are several situations when an organization might decide to migrate their databases from one to another, such as the need for better performance, enhanced security, or advanced features or to avoid licensing costs from vendors. If an organization wants to expand its business to a different geolocation, it will need to carry out database migration, disaster recovery improvements, and database sync in a cost-effective manner. AWS DMS allows you to leverage the benefits of scalability, flexibility, and cost-efficiency when migrating databases from an <strong class="source-inline">on-premises/EC2 instance/Amazon RDS</strong> to Amazon RDS or <span class="No-Break">Amazon Aurora.</span></p>
			<p>In scenarios where multiple databases need to be consolidated into a single database or data needs to be integrated across multiple databases, AWS DMS can be a valuable tool. AWS DMS is designed to move data from a source to a target provided one of the endpoints is <span class="No-Break">on AWS:</span></p>
			<ul>
				<li>DMS supports both homogenous and heterogeneous database migrations, allowing you to migrate between different database engines, such as Oracle, MySQL, PostgreSQL, and Microsoft <span class="No-Break">SQL Server.</span></li>
				<li>DMS simplifies the database migration process by handling schema conversion, data replication, and ongoing synchronization between the source and <span class="No-Break">target databases.</span></li>
				<li>DMS supports both full-load and ongoing <strong class="bold">Change Data Capture</strong> (<strong class="bold">CDC</strong>) replication methods. Full-load migration copies the entire source database to the target, while CDC captures and replicates only the changes made after the initial load. For example, for a database migration with minimal downtime, DMS performs an initial full-load migration, followed by CDC replication to keep the target database up to date with changes in the <span class="No-Break">source database.</span></li>
				<li>DMS provides a user-friendly console and API for the easy configuration, monitoring, and management of database migrations. It offers detailed logging and error handling to help diagnose and resolve migration issues. For example, during a migration from Oracle to Amazon Aurora, DMS can automatically convert Oracle-specific data types and modify table structures to align with the Aurora <span class="No-Break">database schema.</span></li>
				<li>DMS supports continuous data replication, allowing you to keep the source and target databases in sync even after the initial migration. This is particularly useful in scenarios requiring ongoing data synchronization or database replication. An example is if a company maintains an active-active database setup for high availability, where DMS replicates data changes between multiple database instances located in different regions for <span class="No-Break">real-time synchronization.</span></li>
				<li>DMS offers built-in validation and testing capabilities to ensure the integrity and consistency of the migrated data. It performs data validation checks and generates reports to verify the success of the migration process. For example, after migrating a large database from Microsoft SQL Server to Amazon RDS for PostgreSQL, DMS validates the migrated data by comparing row counts, data types, and other metrics to ensure <span class="No-Break">data accuracy.</span></li>
				<li>DMS supports both one-time migrations and continuous replication for database consolidation and integration scenarios. It enables organizations to consolidate data from multiple databases into a single target database or distribute data across multiple databases as needed. For example, say a company with several subsidiary databases wants to consolidate all the data into a centralized database for unified reporting and analysis. DMS facilitates the migration and ongoing synchronization of data from multiple sources to the <span class="No-Break">target database.</span></li>
			</ul>
			<h1 id="_idParaDest-78"><strong class="bold">Processi<a id="_idTextAnchor422"/><a id="_idTextAnchor423"/>ng stored d</strong><strong class="bold">ata on AWS</strong></h1>
			<p>There are several services for processing the data stored in AWS. You will learn about AWS Batch and AWS <strong class="bold">Elastic MapReduce</strong> (<strong class="bold">EMR</strong>) in this section. EMR is a product from AWS that primarily runs <strong class="source-inline">MapRedu<a id="_idTextAnchor424"/>ce</strong> jobs and Spark applications in a managed way. AWS Batch is used for long-running, <span class="No-Break">compute-heavy workloads.</span></p>
			<h2 id="_idParaDest-79"><strong class="bold"><a id="_idTextAnchor425"/>AWS EMR</strong></h2>
			<p><a id="_idTextAnchor426"/><a id="_idTextAnchor427"/>EMR is a managed impleme<a id="_idTextAnchor428"/>ntation of Apache Hadoop provided as a service by AWS. It includes other components of the Hadoop ecosystem, such as Spark, HBase, Flink, Presto, Hive, and Pig. You will not need to<a id="_idTextAnchor429"/> learn about these in detail for the certification exam, but here’s some information <span class="No-Break">about EMR:</span></p>
			<ul>
				<li>EMR clusters can be launched from the AWS console or via the AWS CLI with a specific number of nodes. The cluster can be a long-term cluster or an ad hoc cluster. In a long-running traditional cluster, you have to configure the machines and manage them yourself. If you have jobs that need to be executed faster, then you need to manually add a cluster. In the case of EMR, these admin overheads disappear. You can request any number of nodes from EMR and it manages and launches the nodes for you. If you have autoscaling enabled on the cluster, EMR regulates nodes according to the requirement. That means, EMR launches new nodes in the cluster when the load is high and decommissions the nodes once the load <span class="No-Break">is reduced.</span></li>
				<li>EMR uses EC2 instances in the background and runs in one Availability Zone in a VPC. This enables faster network speeds between the nodes. AWS Glue uses EMR clusters in the background, where users do not need to worry about having an operational understanding of <span class="No-Break">AWS EMR.</span></li>
				<li>From a use case standpoint, EMR can be used to process or transform the data stored in S3 and output data to be stored in S3. EMR uses nodes (EC2 instances) as the computing units for data processing. EMR nodes come in different variants, including master nodes, core nodes, and <span class="No-Break">task nodes.</span></li>
				<li>The EMR master node acts as a Hadoop NameNode and manages the cluster and its health. It is responsible for distributing the job workload among the other core nodes and task nodes. If you have SSH enabled, then you can connect to the master node instance and access <span class="No-Break">the cluster.</span></li>
				<li>An EMR cluster can have one or more core nodes. If you relate it to the Hadoop ecosystem, then core nodes are similar to Hadoop data nodes for HDFS and they are responsible for running the tasks <span class="No-Break">within them.</span></li>
				<li>Task nodes are optional and they don’t have HDFS storage. They are responsible for running tasks. If a task node fails for some reason, then this does not impact HDFS storage, but a core node failure causes HDFS <span class="No-Break">storage interruptions.</span></li>
				<li>EMR has a filesystem called EMRFS. I<a id="_idTextAnchor430"/>t is backed by S3, which makes it regionally resilient. If a core node fails, the data is still safe in S3. HDFS is efficient in terms of I/O and faster <span class="No-Break">than EMRFS.</span></li>
			</ul>
			<p>In the following section<a id="_idTextAnchor431"/>, you will learn about AWS Batch, which is a managed batch-processing compute service that can be used for <span class="No-Break">long-running services.</span></p>
			<h2 id="_idParaDest-80"><strong class="bold">AWS Batc<a id="_idTextAnchor432"/><a id="_idTextAnchor433"/>h</strong></h2>
			<p>This is a managed batch-processing product<a id="_idTextAnchor434"/>. If you are using AWS Batc<a id="_idTextAnchor435"/>h, then jobs can be run without end user interaction or can be scheduled <span class="No-Break">to run:</span></p>
			<ul>
				<li>Imagine an event-driven application that launches a Lambda function to process the data stored in S3. If the processing time goes beyond 15 minutes, then Lambda stops the execution and fails. For such scenarios, AWS Batch is a better solution, where computation-heavy workloads can be scheduled or driven through <span class="No-Break">API events.</span></li>
				<li>AWS Batch is a good fit for use cases where a longer processing time is required or more computation resources <span class="No-Break">are needed.</span></li>
				<li>AWS Batch jobs can be a script or an executable. One job can depend on another job. A job needs to be defined, such as who can run the job (with IAM permissions), where the job can be run (resources to be used), mount points, and <span class="No-Break">other metadata.</span></li>
				<li>Jobs are submitted to queues, where they wait for compute environment capacity. These queues are associated with one or more <span class="No-Break">compute environments.</span></li>
				<li>Compute environments do the actual work of executing the jobs. These can be ECS or EC2 instances, or any computing resources. You can define their sizes and <span class="No-Break">capacities too.</span></li>
				<li>Environments receive<a id="_idTextAnchor436"/> jobs from the queues based on their priority and execute them. They can be managed or unmanaged <span class="No-Break">compute environments.</span></li>
				<li>AWS Batch can store t<a id="_idTextAnchor437"/>he metadata in DynamoDB for further use and can also store the output in an <span class="No-Break">S3 bucket.</span></li>
			</ul>
			<p class="callout-heading"><strong class="bold">Note</strong></p>
			<p class="callout">If you get a question in the exam on an event-style workload that requires flexible compute, a higher disk space, no time limit (more than 15 minutes), or an effective resource limit, then the answer is likely to be <span class="No-Break">AWS Batch.</span></p>
			<h1 id="_idParaDest-81"><strong class="bold"><a id="_idTextAnchor438"/>Summary</strong></h1>
			<p><a id="_idTextAnchor439"/><a id="_idTextAnchor440"/>In this chapter, you learned about different ways of processing data in AWS. You also learned the capabilities in terms of extending your data centers to AWS, migrating data to AWS, and the ingestion process. You learned about the various ways of using data to process it and make it ready for analysis. You understood the magic of using a data catalog, which helps you to query your data via AWS Glue <span class="No-Break">and Athena.</span></p>
			<p>In the next chapter, you will learn about various machine learning algorithms and <span class="No-Break">their usage.</span></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor441"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH03"><span class="No-Break">https://packt.link/MLSC01E2_CH03</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B21197_03_09.jpg" alt="Figure 3.9 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B21197_03_10.jpg" alt="Figure 3.10 – Chapter Review Questions for Chapter 3" width="1448" height="757"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Chapter Review Questions for Chapter 3</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor442"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-84"><a id="_idTextAnchor443"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-85"><a id="_idTextAnchor444"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor445"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor446"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>