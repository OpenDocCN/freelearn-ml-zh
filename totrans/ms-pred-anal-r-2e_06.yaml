- en: Chapter 6. Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 支持向量机
- en: In this chapter, we are going to take a fresh look at nonlinear predictive models
    by introducing support vector machines. Support vector machines, often abbreviated
    as SVMs, are very commonly used for classification problems, although there are
    certainly ways to perform function approximation and regression tasks with them.
    In this chapter, we will focus on the more typical case of their role in classification.
    To do this, we'll first present the notion of maximal margin classification, which
    presents an alternative formulation of how to choose between many possible classification
    boundaries and differs from approaches we have seen thus far, such as maximum
    likelihood. We'll introduce the related idea of support vectors and how, together
    with maximal margin classification, we can obtain a linear model in the form of
    a support vector classifier. Finally, we'll present how we can generalize these
    ideas in order to introduce nonlinearity through the use of certain functions
    known as kernels to finally arrive at our destination, the support vector machine.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过介绍支持向量机来重新审视非线性预测模型。支持向量机，通常缩写为SVMs，在分类问题中非常常用，尽管当然有方法使用它们进行函数逼近和回归任务。在本章中，我们将重点关注它们在分类中更典型的角色。为此，我们首先将介绍最大间隔分类的概念，它提出了如何在许多可能的分类边界之间进行选择的另一种公式化方法，并且与迄今为止我们所看到的方法不同，例如最大似然。我们将介绍相关的支持向量概念以及如何，与最大间隔分类一起，我们可以获得一个以支持向量分类器形式存在的线性模型。最后，我们将展示如何通过使用某些称为核的函数来引入非线性，最终达到我们的目的地，即支持向量机。
- en: Maximal margin classification
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大间隔分类
- en: 'We''ll begin this chapter by returning to a situation that should be very familiar
    by now: the binary classification task. Once again, we''ll be thinking about the
    problem of how to design a model that will correctly predict whether an observation
    belongs to one of two possible classes. We''ve already seen that this task is
    simplest when the two classes are linearly separable; that is, when we can find
    a *separating hyperplane* (a plane in a multidimensional space) in the space of
    our features so that all the observations on one side of the hyperplane belong
    to one class and all the observations that lie on the other side belong to the
    second class. Depending on the structure, assumptions, and optimizing criterion
    that our particular model uses, we could end up with one of infinite such hyperplanes.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，回到现在应该非常熟悉的情况：二元分类任务。再一次，我们将思考如何设计一个模型来正确预测一个观察值属于两个可能类别之一的问题。我们已经看到，当两个类别是线性可分的时候，这个任务是最简单的；也就是说，当我们可以在我们的特征空间中找到一个*分离超平面*（一个多维空间中的平面）时，超平面一侧的所有观察值属于一个类别，而位于另一侧的所有观察值属于第二个类别。根据我们特定模型使用的结构、假设和优化标准，我们可能会得到无限多个这样的超平面。
- en: 'Let''s visualize this scenario using some data in a two-dimensional feature
    space, where the separating hyperplane is just a separating line:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用二维特征空间中的某些数据来可视化这个场景，其中分离超平面仅仅是一条分离线：
- en: '![Maximal margin classification](img/00108.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00108.jpeg)'
- en: 'In the preceding diagram, we can see two clusters of observations, each of
    which belongs to a different class. We''ve used different symbols for the two
    classes to denote this explicitly. Next, we show three different lines that could
    serve as the decision boundary of a classifier, all of which would generate 100
    percent classification accuracy on the entire dataset. We''ll remind ourselves
    that the equation of a hyperplane can be expressed as a linear combination of
    the input features, which are the dimensions of the space in which the hyperplane
    resides:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到两个属于不同类别的观察值簇。我们使用了不同的符号来明确表示这一点。接下来，我们展示了三条可以作为分类器决策边界的不同线，所有这些线在整个数据集上都会产生100%的分类准确率。我们将提醒自己，超平面的方程可以表示为输入特征的线性组合，这些特征是超平面所在空间中的维度：
- en: '![Maximal margin classification](img/00109.jpeg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00109.jpeg)'
- en: 'A separating hyperplane has this property:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分离超平面具有以下属性：
- en: '![Maximal margin classification](img/00110.jpeg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00110.jpeg)'
- en: 'The first equation simply says that the data points that belong to class 1
    all lie above the hyperplane, and the second equation says that the data points
    that belong to class -1 all lie below the hyperplane. The subscript *i* is used
    to index observations, and the subscript *k* is used to index features, so that
    *x[ik]* means the value of the *k^(th)* feature in the *i^(th)* observation. We
    can combine these two equations into a single equation for simplicity, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程简单地说明属于类别 1 的数据点都位于超平面之上，第二个方程说明属于类别 -1 的数据点都位于超平面之下。下标 *i* 用于索引观察，下标 *k*
    用于索引特征，因此 *x[ik]* 表示第 *i* 个观察的第 *k* 个特征的值。为了简化，我们可以将这两个方程合并为一个方程，如下所示：
- en: '![Maximal margin classification](img/00111.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00111.jpeg)'
- en: To see why this simplification works, consider an observation of the class -1
    (*y[i]* *= -1*). This observation will lie below the separating hyperplane, so
    the linear combination in brackets will produce a negative value. Multiplying
    this with its *y[i]* value of -1 results in a positive value. A similar argument
    works for observations of class 1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这种简化的原因，考虑对类别 -1 的观察（*y[i]* = -1*）。这个观察将位于分离超平面下方，因此括号中的线性组合将产生一个负值。将其与其
    *y[i]* 的 -1 值相乘，结果为正值。对于类别 1 的观察，有类似的论点。
- en: Looking back at our diagram, note that the two dashed lines come quite close
    to certain observations. The solid line intuitively feels better than the other
    two lines as a decision boundary, as it separates the two classes without coming
    too close to either by traversing the center of the space between them. In this
    way, it distributes the space between the two classes equally. We can define a
    quantity known as the **margin** that a particular separating hyperplane generates,
    as the smallest perpendicular distance from any point in the dataset to the hyperplane.
    In two dimensions and two classes, we will always have at least two points that
    lie at a perpendicular distance equal to the margin from the separating line,
    one on each side of the line. Sometimes, as is the case with our data, we may
    have more than two points whose perpendicular distance from the separating line
    equals the margin.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的图，注意两条虚线与某些观察相当接近。直观上，实线作为决策边界比其他两条线更好，因为它在它们之间的空间中心穿越，将两个类别分开，而不靠近任何一个类别。这样，它在两个类别之间平均分配空间。我们可以定义一个称为
    **间隔** 的量，它是一个特定分离超平面产生的，即从数据集中任何点到超平面的最小垂直距离。在二维和两个类别的情况下，我们总是至少有两个点，它们与分离线的垂直距离等于间隔，一个在线的每一侧。有时，正如我们的数据那样，我们可能有超过两个点，它们与分离线的垂直距离等于间隔。
- en: 'The next plot shows the margin of the solid line from the previous plot, demonstrating
    that we have three points at a distance equal to the margin from this separating
    line:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图显示了前一个图中实线的间隔，表明我们有三个点与这个分离线的间隔相等：
- en: '![Maximal margin classification](img/00112.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00112.jpeg)'
- en: Now that we have the definition of the margin under our belt, we have a way
    to codify our intuition that led us to choose the solid line as the better decision
    boundary among the three lines that we saw in the first plot. We can go a step
    further and define the **maximal margin hyperplane** as the hyperplane whose margin
    is the largest amongst all possible separating hyperplanes. In our 2D example,
    we are essentially looking for the line that will separate the two classes while
    at the same time being as far away from the observations as possible. It turns
    out that the solid line from our example is actually the maximal margin line,
    so that there is no other line that can be drawn with a higher margin than two
    units. This explains why we chose to label it as the line of maximum margin separation
    in our first plot.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了间隔的定义，我们就有了一种方法来编码我们选择实线作为三个线中较好决策边界的直觉。我们可以更进一步，定义 **最大间隔超平面** 为所有可能的分离超平面中间隔最大的超平面。在我们的二维例子中，我们实际上是在寻找一条线，它将两个类别分开，同时尽可能远离观察点。结果证明，我们例子中的实线实际上是最大间隔线，因此没有其他线可以画出比两个单位更高的间隔。这解释了为什么我们在第一个图中将其标记为最大间隔分离线。
- en: 'In order to understand how we found the maximal margin hyperplane in our simple
    example, we need to formalize the problem as an optimization problem with *p*
    features by using the following equations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们如何在简单示例中找到最大间隔超平面，我们需要使用以下方程将问题形式化为一个具有*p*个特征的优化问题：
- en: '![Maximal margin classification](img/00113.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00113.jpeg)'
- en: Together, these two constraints in our optimization problem express the idea
    that observations in our data need to not only be correctly classified, but also
    lie at least *M* units away from the separating hyperplane. The goal is to maximize
    this distance *M* by appropriately choosing the coefficients *β[i]*. Thus, we
    need an optimization procedure that handles this type of problem. The details
    of how the optimization is actually implemented in practice are beyond the scope
    of this book, but we will see them in action later on when we do some programming
    with R.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个约束条件共同表达了我们的优化问题中的想法，即我们的数据中的观察点不仅需要被正确分类，而且至少需要位于分离超平面至少*M*个单位之外。目标是通过对系数*β[i]*进行适当的选取来最大化这个距离*M*。因此，我们需要一个处理这类问题的优化过程。优化实际上如何在实践中实现的具体细节超出了本书的范围，但当我们用R进行编程时，我们将在后面看到它们是如何发挥作用的。
- en: 'We have a natural way forward now, which is to start looking at how the situation
    changes when our data is not linearly separable, something that we know by now
    is the typical scenario for a real-world dataset. Let us take a step back before
    doing this. We''ve already studied two different methods for estimating the parameters
    of a model: namely, maximum likelihood estimation and the least squared error
    criterion for linear regression. For example, when we looked at classification
    with logistic regression, we considered the idea of maximizing the likelihood
    of our data. This takes into account all of the available data points. This is
    also the case when classifying with multilayer perceptrons. With the maximum margin
    classifier, however, the construction of our decision boundary is only supported
    by the points that lie on the margin. Put differently, with the data in our 2D
    example, we can freely adjust the position of any observation except the three
    on the margin, and as long as the adjustment does not result in the observation
    falling inside the margin, our separating line will stay exactly in the same position.
    For this reason, we define the perpendicular vectors from the points that lie
    on the margin to the separating hyperplane as the **support vectors**. Thus, we''ve
    seen that our 2D example has three support vectors. The fact that only a subset
    of all the points in our data essentially determines the placement of the separating
    hyperplane means that we have the potential to overfit our training data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个自然的前进方式，那就是开始研究当我们的数据不是线性可分时，情况会如何变化，我们知道这是现实世界数据集的典型场景。在这样做之前，让我们退一步。我们已经研究了两种估计模型参数的方法：即最大似然估计和线性回归的最小二乘误差标准。例如，当我们研究逻辑回归的分类时，我们考虑了最大化我们数据似然的想法。这考虑了所有可用的数据点。在用多层感知器进行分类时也是如此。然而，对于最大间隔分类器，我们的决策边界的构建只由位于边缘的点支持。换句话说，在我们的二维示例中，我们可以自由调整除边缘上的三个点外的任何观察点的位置，只要调整不会导致观察点落在边缘内，我们的分离线将保持在完全相同的位置。因此，我们将从位于边缘的点到分离超平面的垂直向量定义为**支持向量**。因此，我们已经看到我们的二维示例有三个支持向量。只有数据集中所有点的子集实际上决定了分离超平面的位置这一事实意味着我们有过度拟合训练数据的潜力。
- en: On the other hand, this approach does yield a couple of nice properties. We
    split the space between the two classes equally between them without applying
    any bias toward either. Points that clearly lie well inside the area of space
    occupied by a particular class do not play such a big role in the model compared
    to points on the fringes, which is where we need to place our decision boundary.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这种方法确实产生了一些很好的性质。我们在两个类别之间平等地分割空间，而不对任何一个类别施加任何偏差。显然位于特定类别占据的空间内的点的点在模型中的作用不如边缘上的点大，这是我们放置决策边界的区域。
- en: Support vector classification
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量分类
- en: 'We need our data to be linearly separable in order to classify it with a maximal
    margin classifier. When our data is not linearly separable, we can still use the
    notion of support vectors that define a margin, but this time, we will allow some
    examples to be misclassified. Thus, we essentially define a **soft margin**, in
    that some of the observations in our dataset can violate the constraint that they
    need to be at least as far as the margin from the separating hyperplane. It is
    also important to note that sometimes we may want to use a soft margin even for
    linearly separable data. The reason for this is in order to limit the degree of
    overfitting the data. Note that the larger the margin, the more confident we are
    about our ability to correctly classify new observations, because the classes
    are further apart from each other in our training data. If we achieve separation
    using a very small margin, we are less confident about our ability to correctly
    classify our data and we may, instead, want to allow a few errors and come up
    with a larger margin that is more robust. Study the following plot:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要我们的数据是线性可分的，以便使用最大边界分类器对其进行分类。当我们的数据不是线性可分时，我们仍然可以使用定义边界的支持向量的概念，但这次，我们将允许一些示例被误分类。因此，我们本质上定义了一个**软边界**，即我们数据集中的某些观测可能违反了它们需要至少与分离超平面保持一定距离的约束。同样重要的是要注意，有时我们可能即使在数据是线性可分的情况下也想使用软边界。这样做的原因是为了限制数据过度拟合的程度。请注意，边界越大，我们对正确分类新观测的信心就越大，因为在我们训练数据中，类别彼此之间的距离越远。如果我们使用非常小的边界实现分离，我们对正确分类数据的信心就会降低，我们可能更愿意允许一些错误，并提出一个更大的、更稳健的边界。研究以下图表：
- en: '![Support vector classification](img/00114.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量分类](img/00114.jpeg)'
- en: 'In order to get a firmer grasp of the reason why a soft margin may be preferable
    to a hard margin, even for linearly separable data, we''ve changed our data slightly.
    We used the same data that we had previously, but we added an extra observation
    to class 1 and placed it close to the boundary of class -1\. Note that with the
    addition of this single new data point, with feature values f1=16 and f2=40, our
    maximal margin line has moved drastically! The margin has been reduced from two
    units to 0.29 units. Looking at this graph, we are tempted to feel that the new
    point might either be an outlier or a mislabeling in our dataset. If we were to
    allow our model to make one single misclassification using a soft margin, we would
    go back to our previous line, which separates the two classes with a much wider
    margin and is less likely to have overfit on the data. We formalize the notion
    of our soft classifier by modifying our optimization problem set up:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更牢固地掌握为什么即使是对于线性可分的数据，软边界可能比硬边界更可取的原因，我们稍微改变了我们的数据。我们使用了之前相同的数据，但我们在类别1中添加了一个额外的观测点，并将其放置在类别-1的边界附近。请注意，随着这个单一新数据点的添加，特征值f1=16和f2=40，我们的最大边界线发生了巨大变化！边界从两个单位减少到0.29单位。看着这张图，我们可能会觉得这个新点可能是我们的数据集中的异常值或误标记。如果我们允许我们的模型使用软边界进行一次误分类，我们会回到我们之前的线，这条线以更宽的边界分隔两个类别，并且不太可能对数据进行过度拟合。我们通过修改我们的优化问题设置来形式化我们的软分类器的概念：
- en: '![Support vector classification](img/00115.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量分类](img/00115.jpeg)'
- en: 'Under this new setup, we''ve introduced a new set of variables *ξ*i, known
    as the **slack variables**. There is one slack variable for every observation
    in our dataset and the value of the *ξ*i slack variable depends on where the *i*th
    observation falls with respect to the margin. When an observation is on the correct
    side of the separating hyperplane and outside the margin, the slack variable for
    that observation takes the value 0\. This is the ideal situation that we have
    seen for all observations under a hard boundary. When an observation is correctly
    classified but falls at a distance within the margin, the corresponding slack
    variable takes a small positive value less than 1\. When an observation is actually
    misclassified, thus falling on the wrong side of the hyperplane altogether, then
    its associated slack variable takes a value greater than 1\. In summary, take
    a look at the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的设置下，我们引入了一组新的变量 *ξ*i，被称为**松弛变量**。对于我们的数据集中的每一个观测值，都有一个松弛变量，而 *ξ*i 松弛变量的值取决于第
    *i* 个观测值相对于边界的位置。当一个观测值位于分离超平面的正确一侧且在边界之外时，该观测值的松弛变量取值为 0。这是我们对于所有观测值在硬边界下看到的最理想的情况。当一个观测值被正确分类但落在边界内的一定距离处时，相应的松弛变量取一个小于
    1 的小正数。当一个观测值实际上被错误分类，因此完全落在超平面的错误一侧时，其关联的松弛变量取值大于 1。总的来说，看看以下内容：
- en: '![Support vector classification](img/00116.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量分类](img/00116.jpeg)'
- en: When an observation is incorrectly classified, the magnitude of the slack variables
    is proportional to the distance between that observation and the boundary of the
    separating hyperplane. The fact that the sum of the slack variables must be less
    than a constant *C* means that we can think of this constant as an error budget
    that we are prepared to tolerate. As a misclassification of a single particular
    observation results in a slack variable taking at least the value 1, and our constant
    *C* is the sum of all the slack variables, setting a value of *C* less than 1
    means that our model will tolerate a few observations falling inside the margin,
    but no misclassifications. A high value of *C* often results in many observations
    either falling inside the margin or being misclassified, and as these are all
    support vectors, we end up having a greater number of support vectors. This results
    in a model that has a lower variance, but because we have shifted our boundary
    in a way that has increased tolerance to margin violations and errors, we may
    have a higher bias. By contrast, depending on fewer support vectors caused by
    having a much stricter model (and hence a lower value of *C*) may result in a
    lower bias in our model. These support vectors, however, will individually affect
    the position of our boundary to a much higher degree. Consequently, we will experience
    a higher variance in our model performance across different training sets. Once
    again, the interplay between model bias and variance resurfaces in the design
    decisions that we must make as predictive modelers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个观测值被错误分类时，松弛变量的幅度与该观测值与分离超平面边界的距离成正比。由于松弛变量的总和必须小于一个常数 *C*，我们可以将这个常数视为我们准备容忍的错误预算。由于单个特定观测值的错误分类会导致松弛变量至少取值为
    1，而我们的常数 *C* 是所有松弛变量的总和，将 *C* 的值设为小于 1 意味着我们的模型将容忍一些观测值落在边界内，但不会出现错误分类。*C* 的值较高通常会导致许多观测值要么落在边界内，要么被错误分类，而这些都是支持向量，我们最终会有更多的支持向量。这导致了一个具有较低方差但因为我们已经通过增加对边界违规和错误的容忍度而改变了边界，我们可能会有更高的偏差。相比之下，由于模型（因此
    *C* 的值较低）非常严格而导致的支持向量数量减少，可能会在我们的模型中产生较低的偏差。然而，这些支持向量将分别以更高的程度影响我们边界的位置。因此，我们将在不同的训练集上经历模型性能的更高方差。再次强调，模型偏差和方差之间的相互作用再次出现在我们作为预测模型制定者必须做出的设计决策中。
- en: Inner products
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内积
- en: 'The exact details of how the parameters of the support vector classifier model
    are computed are beyond the scope of this book. However, it turns out that the
    model itself can be simplified into a more convenient form that uses the **inner
    products** of the observations. An inner product of two vectors of identical length,
    *v1* and *v2*, is computed by first computing the element-wise multiplication
    of the two vectors and then taking the sum of the resulting elements. In R, we
    obtain an element-wise multiplication of two vectors by simply using the multiplication
    symbol. So we can compute the inner product of two vectors as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机模型参数计算的详细过程超出了本书的范围。然而，结果表明，该模型本身可以被简化为一个更方便的形式，该形式使用观测值的**内积**。两个长度相同的向量
    *v1* 和 *v2* 的内积是通过首先计算两个向量的逐元素乘积，然后取这些结果的和来计算的。在 R 中，我们只需使用乘号即可获得两个向量的逐元素乘积。因此，我们可以按以下方式计算两个向量的内积：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In mathematical terms, we use triangular brackets to denote the inner product
    operation, and we represent the process as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，我们使用三角括号来表示内积运算，并按以下方式表示这个过程：
- en: '![Inner products](img/00117.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00117.jpeg)'
- en: 'In the preceding equation, for the two vectors *v[1]* and *v[2]*, the index
    *i* is iterating over the *p* features or dimensions. Now, here is the original
    form of our support vector classifier:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，对于两个向量 *v[1]* 和 *v[2]*，索引 *i* 正在遍历 *p* 个特征或维度。现在，这是我们的支持向量机分类器的原始形式：
- en: '![Inner products](img/00118.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00118.jpeg)'
- en: 'This is just the standard equation for a linear combination of the input features.
    It turns out that for the support vector classifier, the model''s solution can
    be expressed in terms of the inner product between the *x* observation that we
    are trying to classify and all other *x*i observations that are in our training
    dataset. More concretely, the form of our support vector classifier can also be
    written as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是输入特征线性组合的标准方程。结果表明，对于支持向量机，模型解可以用我们试图分类的 *x* 观测值与其他所有训练数据集中的 *x*i 观测值之间的内积来表示。更具体地说，我们的支持向量机的形式也可以写成：
- en: '![Inner products](img/00119.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00119.jpeg)'
- en: For this equation, we have explicitly indicated that our model predicts *y*
    as a function of an input observation *x*. The summing function now computes a
    weighted sum of all the inner products of the current observation with every other
    observation in the dataset, which is why we are now summing across the *n* observations.
    We want to make it very clear that we haven't changed anything in the original
    model itself; we have simply written two different representations of the same
    model. Note that we cannot assume that a linear model takes this form in general;
    this is only true for the support vector classifier. Now, in a real-world scenario,
    the number of observations in our data set, *n*, is typically much greater than
    the number of parameters, *p*, so the number of *α* coefficients is seemingly
    larger than the number of *β* coefficients.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方程，我们明确指出，我们的模型将 *y* 预测为输入观测值 *x* 的函数。求和函数现在计算当前观测值与数据集中每个其他观测值的内积的加权和，这就是为什么我们现在要对
    *n* 个观测值进行求和。我们想非常清楚地说明，我们没有改变原始模型本身；我们只是写了同一模型的两种不同表示。请注意，我们不能假设线性模型在一般情况下都采取这种形式；这仅适用于支持向量机。现在，在现实世界的场景中，我们数据集中观测值的数量
    *n* 通常远大于参数的数量 *p*，因此 *α* 系数的数量似乎比 *β* 系数的数量大。
- en: Additionally, whereas in the first equation we were considering observations
    independently of each other, the form of the second equation shows us that to
    classify all our observations, we need to consider all possible pairs and compute
    their inner product. There are such pairs, which are of the order of *n*2\. Thus,
    it would seem like we are introducing complexity rather than producing a representation
    that is simpler. It turns out, however, that all *α* coefficients are zero for
    all observations in our dataset, except those that are support vectors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然在第一个方程中我们是独立考虑每个观测值，但第二个方程的形式表明，为了分类所有观测值，我们需要考虑所有可能的成对组合并计算它们的内积。这样的成对组合有
    *n*2 个，这似乎是在引入复杂性而不是产生一个更简单的表示。然而，实际上，在我们的数据集中，除了支持向量之外，所有 *α* 系数都是零。
- en: 'The number of support vectors in our dataset is typically much smaller than
    the total number of observations. Thus, we can simplify our new representation
    by explicitly showing that we sum over elements from the set of support vectors,
    *S*, in our dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，支持向量的数量通常远小于总观察数量。因此，我们可以通过明确显示我们在数据集中的支持向量集合 *S* 上求和来简化我们的新表示：
- en: '![Inner products](img/00120.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00120.jpeg)'
- en: Kernels and support vector machines
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核和支持向量机
- en: So far, we've introduced the notion of maximum margin classification under linearly
    separable conditions and its extension to the support vector classifier, which
    still uses a hyperplane as the separating boundary but handles datasets that are
    not linearly separable by specifying a budget for tolerating errors. The observations
    that are on or within the margin, or are misclassified by the support vector classifier,
    are support vectors. The critical role that these play in the positioning of the
    decision boundary was also seen in an alternative model representation of the
    support vector classifier that uses inner products.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们介绍了在线性可分条件下最大间隔分类的概念及其扩展到支持向量分类器，它仍然使用超平面作为分离边界，但通过指定容错预算来处理非线性可分的数据集。位于或位于间隔内，或被支持向量分类器错误分类的观察值是支持向量。这些在决策边界定位中发挥的关键作用也在使用内积的替代模型表示支持向量分类器中得到了体现。
- en: 'What is common in the situations that we have seen so far in this chapter is
    that our model is always linear in terms of the input features. We''ve seen that
    the ability to create models that implement nonlinear boundaries between the classes
    to be separated is far more flexible in terms of the different kinds of underlying
    target functions that they can handle. One way to introduce nonlinearity in our
    model that uses our new representation involving inner products is to apply a
    nonlinear transformation to this result. We can define a general function *K*,
    which we''ll call a **kernel function**, that operates on two vectors and produces
    a scalar result. This allows us to generalize our model as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们迄今为止看到的情况中，共同点是我们的模型在输入特征方面总是线性的。我们已经看到，创建实现非线性边界的模型的能力，在处理不同类型的潜在目标函数方面要灵活得多。在我们的模型中引入非线性的一种方法是对这个结果应用非线性变换。我们可以定义一个通用函数
    *K*，我们将它称为**核函数**，它作用于两个向量并产生一个标量结果。这允许我们如下泛化我们的模型：
- en: '![Kernels and support vector machines](img/00121.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00121.jpeg)'
- en: Our model now has as many features as there are support vectors, and each feature
    is defined as the result of a kernel acting upon the current observation and one
    of the support vectors. For the support vector classifier, the kernel we applied
    is known as the **linear kernel,** as this just uses the inner product itself,
    producing a linear model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的模型具有与支持向量一样多的特征，每个特征都被定义为核函数作用于当前观察结果和其中一个支持向量的结果。对于支持向量机分类器，我们应用的核函数被称为**线性核**，因为这仅仅使用内积本身，产生一个线性模型。
- en: '![Kernels and support vector machines](img/00122.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00122.jpeg)'
- en: 'Kernel functions are also known as similarity functions, as we can consider
    the output they produce as a measure of the similarity between the two input vectors
    provided. We introduce nonlinearity in our model using nonlinear kernels, and
    when we do this, our model is known as a **support vector machine**. There are
    a number of different types of nonlinear kernels. The two most common ones are
    the **polynomial kernel** and the **radial basis function kernel**. The polynomial
    kernel uses a power expansion of the inner product between two vectors. For a
    polynomial of degree *d*, the form of the polynomial kernel is:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数也被称为相似度函数，因为我们可以将它们产生的输出视为两个输入向量之间相似度的度量。我们使用非线性核在我们的模型中引入非线性，当我们这样做时，我们的模型被称为**支持向量机**。有几种不同类型的非线性核。最常见的是**多项式核**和**径向基函数核**。多项式核使用两个向量之间内积的幂展开。对于度数为
    *d* 的多项式，多项式核的形式如下：
- en: '![Kernels and support vector machines](img/00123.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00123.jpeg)'
- en: 'Using this kernel, we are essentially transforming our feature space into a
    higher dimensional space. Computing the kernel applied to the inner product is
    much more efficient than first transforming all the features into a high-dimensional
    space and then trying to fit a linear model into that space. This is especially
    true when we use the **radial basis function kernel**, often referred to simply
    as the **radial kernel**, where the number of dimensions of the transformed feature
    space is actually infinite due to the infinite number of terms in the expansion.
    The form of the radial kernel is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个核函数，我们实际上是将我们的特征空间转换到了一个更高维的空间。计算应用于内积的核函数比首先将所有特征转换到高维空间，然后尝试在那个空间中拟合线性模型要高效得多。这在我们使用**径向基函数核**时尤其正确，通常简称为**径向核**，因为由于展开中的项数无限，转换后的特征空间的维度实际上是无限的。径向核的形式是：
- en: '![Kernels and support vector machines](img/00124.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00124.jpeg)'
- en: Upon close inspection, we should be able to spot that the radial kernel does
    not use the inner product between two vectors. Instead, the summation in the exponent
    is just the square of the Euclidean distance between these two vectors. The radial
    kernel is often referred to as a **local kernel**, because when the Euclidean
    distance between the two input vectors is large, the resulting value that the
    kernel computes is very small because of the negative sign in the exponent. Consequently,
    when we use a radial kernel, only vectors close to the current observation for
    which we want to get a prediction play a significant role in the computation.
    We're now ready to put all this to practice with some real-world datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察后，我们应该能够发现径向核不使用两个向量之间的内积。相反，指数中的求和只是这两个向量之间欧几里得距离的平方。径向核通常被称为**局部核**，因为当两个输入向量之间的欧几里得距离很大时，由于指数中的负号，核计算出的结果非常小。因此，当我们使用径向核时，只有接近当前观察值的向量在计算中起重要作用。我们现在已经准备好使用一些真实世界的数据集来实践所有这些内容。
- en: Predicting chemical biodegration
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测化学物质生物降解
- en: 'In this section, we are going to use R''s `e1071` package to try out the models
    we''ve discussed on a real-world dataset. As our first example, we have chosen
    the *QSARbiodegration data set*, which can be found at [https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation).
    This is a dataset containing 41 numerical variables that describe the molecular
    composition and properties of 1,055 chemicals. The modeling task is to predict
    whether a particular chemical will be biodegradable based on these properties.
    Example properties are the percentages of carbon, nitrogen, and oxygen atoms,
    as well as the number of heavy atoms in the molecule. These features are highly
    specialized and sufficiently numerous, so a full listing won''t be given here.
    The complete list and further details of the quantities involved can be found
    on the website. For now, we''ve downloaded the data into a `bdf` data frame:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用R语言的`e1071`包，在一个真实世界的数据集上尝试我们讨论过的模型。作为第一个例子，我们选择了*QSARbiodegration数据集*，可以在[https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation)找到。这是一个包含41个数值变量，描述了1,055种化学物质的分子组成和性质的数据集。建模任务是预测特定化学物质是否可生物降解，基于这些性质。示例性质包括碳、氮、氧原子的百分比，以及分子中的重原子数量。这些特征非常专业且数量充足，因此这里不会给出完整的列表。涉及到的完整列表和更多细节可以在网站上找到。目前，我们已经将数据下载到了一个`bdf`数据框中：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The final column, `V42`, contains the output variable, which takes the value
    `NRB` for chemicals that are not biodegradable and `RB` for those that are. We''ll
    recode this into the familiar labels of `0` and `1`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个列，`V42`，包含输出变量，对于不可生物降解的化学物质取值为`NRB`，对于可生物降解的化学物质取值为`RB`。我们将将其重新编码为熟悉的标签`0`和`1`：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have our data ready, we''ll begin, as usual, by splitting them
    into training and testing sets, with an 80-20 split:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，我们将像往常一样，将它们分为训练集和测试集，比例为80-20：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are a number of packages available in R that implement support vector
    machines. In this chapter, we''ll explore the use of the `e1071` package, which
    provides us with the `svm()` function. If we examine our training data, we''ll
    quickly notice that on the one hand, the scales of the various features are quite
    different, and on the other hand, many features are sparse features, which means
    that for many entries they take a zero value. It is a good idea to scale features
    as we did with neural networks, especially if we want to work with radial kernels.
    Fortunately for us, the `svm()` function has a `scale` parameter, which is set
    to `TRUE` by default. This normalizes the input features so that they have zero
    mean and unit variance before the model is trained. This circumvents the need
    for us to manually carry out this preprocessing step. The first model that we
    will investigate will use a linear kernel:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，有多个包实现了支持向量机。在本章中，我们将探讨使用`e1071`包，它为我们提供了`svm()`函数。如果我们检查我们的训练数据，我们会很快注意到一方面，各种特征的比例相差很大，另一方面，许多特征是稀疏特征，这意味着对于许多条目，它们取零值。在神经网络中我们这样做，将特征进行缩放是一个好主意，尤其是如果我们想使用径向核。幸运的是，`svm()`函数有一个`scale`参数，默认设置为`TRUE`。在模型训练之前，这个参数将标准化输入特征，使它们具有零均值和单位方差。这避免了我们需要手动执行此预处理步骤的需要。我们将要研究的第一个模型将使用线性核：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The call to the `svm()` function follows the familiar paradigm of first providing
    a formula, then providing the name of the data frame, and finally, other parameters
    relevant to the model. In our case, we want to train a model where the final `V42`
    column is the predictor column and all other columns are to be used as features.
    For this reason, we can just use the simple formula `V42 ~` instead of having
    to fully enumerate all the other columns. After specifying our data frame, we
    then specify the type of kernel we will use, and in this case, we''ve opted for
    a linear kernel. We''ll also specify a value for the `cost` parameter, which is
    related to the error budget C in our model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`svm()`函数遵循熟悉的范式，首先提供一个公式，然后提供数据框的名称，最后提供与模型相关的其他参数。在我们的情况下，我们想要训练一个模型，其中最终的`V42`列是预测列，所有其他列都用作特征。因此，我们可以只使用简单的公式`V42
    ~`，而不是必须完全列出所有其他列。指定我们的数据框后，我们再指定我们将使用的核的类型，在这种情况下，我们选择了线性核。我们还将指定`cost`参数的值，这与我们的模型中的错误预算C相关：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our model doesn''t provide us with too much information on its performance
    beyond the details of the parameters that we specified. One interesting piece
    of information is the number of data points that were support vectors in our model;
    in this case, `272`. If we use the `str()` function to examine the structure of
    the fitted model, however, we will find that it contains a number of useful attributes.
    For example, the fitted attribute contains the model''s predictions on the training
    data. We''ll use this to gauge the quality of model fit by computing the accuracy
    of the training data and the confusion matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型没有提供太多关于其性能的信息，除了我们指定的参数细节。一个有趣的信息是，在我们的模型中作为支持向量的数据点的数量；在这种情况下，`272`。然而，如果我们使用`str()`函数来检查拟合模型的架构，我们会发现它包含许多有用的属性。例如，拟合属性包含模型对训练数据的预测。我们将使用这些预测来评估模型拟合的质量，通过计算训练数据的准确率和混淆矩阵：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have a training accuracy of just under 89 percent, which is a decent start.
    Next, we''ll examine the performance of the test data using the `predict()` function
    to see whether we can get a test accuracy close to this or whether we have ended
    up overfitting the data:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练准确率略低于89%，这是一个不错的开始。接下来，我们将使用`predict()`函数检查测试数据的性能，看看我们是否能得到接近这个准确率的测试准确率，或者我们是否最终过度拟合了数据：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We do have a slightly lower test accuracy than we''d expect, but we are sufficiently
    close to the training accuracy we obtained earlier in order to be relatively confident
    that we are not in a position where we are overfitting the training data. Now,
    we''ve seen that the `cost` parameter plays an important role in our model, and
    that choosing this involves a trade-off in model bias and variance. Consequently,
    we want to try different values of our `cost` parameter before settling on a final
    model. After manually repeating the preceding code for a few values of this parameter,
    we obtained the following set of results:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实比预期的测试准确率略低，但与我们在之前训练中获得的准确率足够接近，因此我们可以相对有信心地认为我们并没有过度拟合训练数据。现在，我们已经看到`cost`参数在我们的模型中起着重要作用，选择这个参数涉及到模型偏差和方差的权衡。因此，在确定最终模型之前，我们想要尝试`cost`参数的不同值。在手动重复前述代码的几个参数值之后，我们得到了以下结果集：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Sometimes, when building a model, we may see a warning informing us that the
    maximum number of iterations has been reached. If this happens, we should be doubtful
    of the model that we produced, as it may be an indication that a solution was
    not found and the optimization procedure did not converge. In such a case, it
    is best to experiment with a different `cost` value and/or kernel type.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，在构建模型时，我们可能会看到一个警告，告知我们已达到最大迭代次数。如果发生这种情况，我们应该对我们的模型持怀疑态度，因为这可能是没有找到解决方案并且优化过程没有收敛的迹象。在这种情况下，最好是尝试不同的`cost`值和/或核类型。
- en: These results show that for most values of the `cost` parameter, we are seeing
    a very similar level of quality of fit on our training data, roughly 88 percent.
    Ironically, the best performance on the test data was obtained using the model
    whose fit on the training data was the worst, using a cost of 0.01\. In short,
    although we have reasonable performance on our training and test datasets, the
    low variance in the results shown in the table essentially tells us that we are
    not going to get a significant improvement in the quality of fit by tweaking the
    `cost` parameter on this particular dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，对于`cost`参数的大多数值，我们在训练数据上看到的是非常相似的质量拟合水平，大约88%。具有讽刺意味的是，在测试数据上获得最佳性能的是在训练数据拟合最差的模型，使用了0.01的成本。简而言之，尽管我们在训练和测试数据集上都有合理的性能，但表格中显示的结果的低方差实际上告诉我们，通过调整这个特定数据集上的`cost`参数，我们不太可能显著提高拟合质量。
- en: 'Now let''s try using a radial kernel to see whether introducing some nonlinearity
    can allow us to improve our performance. When we specify a radial kernel, we must
    also specify a positive `gamma` parameter. This corresponds to the *1/2σ2* parameter
    in the equation of a radial kernel. The role that this parameter plays is that
    it controls the locality of the similarity computation between its two vector
    inputs. A large `gamma` means that the kernel will produce values that are close
    to zero, unless the two vectors are very close together. A smaller `gamma` results
    in a smoother kernel and takes into account pairs of vectors that are farther
    away. Again, this choice boils down to a trade-off between bias and variance,
    so just as with the `cost` parameter, we''ll have to try out different values
    of `gamma`. For now, let''s see how we can create a support vector machine model
    using a radial kernel with a specific configuration:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用径向核来查看是否引入一些非线性可以让我们提高性能。当我们指定径向核时，我们还必须指定一个正的`gamma`参数。这对应于径向核方程中的*1/2σ2*参数。这个参数所起的作用是控制其两个向量输入之间的相似度计算的局部性。大的`gamma`值意味着核将产生接近零的值，除非两个向量非常接近。较小的`gamma`值会导致核更加平滑，并考虑距离较远的向量对。同样，这个选择归结为偏差和方差的权衡，所以就像`cost`参数一样，我们不得不尝试`gamma`的不同值。现在，让我们看看如何使用特定配置的径向核创建支持向量机模型：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that the radial kernel under these settings is able to fit the training
    data much more closely, as indicated by the nearly 100 percent training accuracy;
    but when we see the performance on the test dataset, the result is substantially
    lower than what we obtained on the training data. Consequently, we have a very
    clear indication that this model is overfitting the data. To get around this problem,
    we will manually experiment with a few different settings of the `gamma` and `cost`
    parameters to see whether we can improve the fit:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这些设置下，径向核能够更紧密地拟合训练数据，这从几乎100%的训练准确率中可以看出；但是当我们看到测试数据集上的性能时，结果实际上比我们在训练数据上获得的结果要低得多。因此，我们有一个非常明确的迹象表明，这个模型正在过度拟合数据。为了解决这个问题，我们将手动尝试调整`gamma`和`cost`参数的几个不同设置，看看我们是否可以提高拟合度：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, the combination of the two parameters, `cost` and `gamma`, yields
    a much wider range of results using the radial kernel. From the data frame we
    built previously, we can see that some combinations, such as *cost = 1* and *gamma
    = 0.05*, have brought our accuracy up to 89 percent on the test data, while still
    maintaining an analogous performance on our training data. Also, in the data frame,
    we see many examples of settings in which the training accuracy is nearly 100
    percent, but the test accuracy is well below this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这两个参数`cost`和`gamma`的组合，使用径向核可以产生更广泛的结果。从我们之前构建的数据框中，我们可以看到一些组合，例如`cost
    = 1`和`gamma = 0.05`，将我们的测试数据准确率提高到89%，同时仍然在训练数据上保持类似的表现。此外，在数据框中，我们看到许多设置，其中训练准确率几乎达到100%，但测试准确率却远低于这个水平。
- en: As a result, we conclude that using a nonlinear kernel, such as the radial kernel,
    needs to be done with care in order to avoid overfitting. Nonetheless, radial
    kernels are very powerful and can be quite effective at modeling a highly nonlinear
    decision boundary, often allowing us to achieve higher rates of classification
    accuracy than a linear kernel. At this point in our analysis, we would usually
    want to settle on a particular value for the `cost` and `gamma` parameters and
    then retrain our model using the entire data available before deploying it in
    the real world.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出结论，在使用非线性核，如径向核时，需要谨慎，以避免过度拟合。尽管如此，径向核非常强大，在建模高度非线性决策边界时可以非常有效，通常允许我们比线性核实现更高的分类准确率。在我们分析的这个阶段，我们通常会希望确定`cost`和`gamma`参数的特定值，然后使用可用的全部数据重新训练我们的模型，在现实世界中部署之前。
- en: Unfortunately, after using the test set to guide our decision on what parameters
    to use, it no longer represents an unseen dataset that would enable us to predict
    the model's accuracy in the real world. One possible solution to this problem
    is to use a validation set, but this would require us to set aside some of our
    data, resulting in smaller training and test set sizes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在用测试集指导我们决定使用哪些参数之后，它就不再代表一个未知的测试数据集，这将使我们能够预测模型在现实世界中的准确率。解决这个问题的可能方法之一是使用验证集，但这将需要我们留出一部分数据，从而导致训练集和测试集的大小减小。
- en: '*Cross-validation*, which we covered in [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Tidying Data and Measuring
    Performance*, should be considered as a practical way out of this dilemma.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉验证*，我们在[第2章](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "第2章。整理数据和衡量性能")、*整理数据和衡量性能*中讨论过，应被视为解决这一困境的实用方法。'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A very readable book on support vector machines is *An Introduction to Support
    Vector Machines and Other Kernel-based Learning Methods* by Nello Christiani and
    John Shawe-Taylor. Another good reference that presents an insightful link between
    SVMs and a related type of neural network known as a **Radial Basis Function Network**
    is *Neural Networks and Learning Machines* by Simon Haykin, which we also referenced
    in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Neural Networks*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一本关于支持向量机的非常易读的书籍是Nello Christiani和John Shawe-Taylor合著的《支持向量机及其核学习方法导论》。另一个很好的参考资料是Simon
    Haykin的《神经网络与学习机器》，它展示了SVMs与一种称为**径向基函数网络**的相关神经网络之间的洞察力链接，我们也在[第5章](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "第5章。神经网络")、*神经网络*中引用了它。
- en: Predicting credit scores
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测信用评分
- en: In this section, we will explore another dataset, this time in the field of
    banking and finance. The particular dataset in question is known as the *German
    Credit Dataset* and is also hosted by the UCI Machine Learning Repository. The
    link to the data is [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨另一个数据集，这次是在银行和金融领域。具体的数据集被称为*德国信用数据集*，并由UCI机器学习存储库托管。数据的链接是[https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)。
- en: The observations in the dataset are loan applications made by individuals at
    a bank. The goal of the data is to determine whether an application constitutes
    a high credit risk.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的观测值是银行个人提交的贷款申请。数据的目标是确定一个申请是否构成高信用风险。
- en: '| Column name | Type | Definition |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `checking` | Categorical | The status of the existing checking account |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `checking` | 分类 | 现有支票账户的状态 |'
- en: '| `duration` | Numerical | The duration in months |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `duration` | 数值 | 持续时间（以月为单位） |'
- en: '| `creditHistory` | Categorical | The applicant''s credit history |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `creditHistory` | 分类 | 申请人的信用历史 |'
- en: '| `purpose` | Categorical | The purpose of the loan |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `purpose` | 分类 | 贷款目的 |'
- en: '| `credit` | Numerical | The credit amount |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `credit` | 数值 | 信用额度 |'
- en: '| `savings` | Categorical | Savings account/bonds |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `savings` | 分类 | 储蓄账户/债券 |'
- en: '| `employment` | Categorical | Present employment since |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `employment` | 分类 | 自从现在起有现职 |'
- en: '| `installmentRate` | Numerical | The installment rate (as a percentage of
    disposable income) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `installmentRate` | 数值 | 分期付款率（作为可支配收入的百分比） |'
- en: '| `personal` | Categorical | Personal status and gender |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `personal` | 分类 | 个人状况和性别 |'
- en: '| `debtors` | Categorical | Other debtors/guarantors |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `debtors` | 分类 | 其他债务人/担保人 |'
- en: '| `presentResidence` | Numerical | Present residence since |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `presentResidence` | 数值 | 现居住地时间 |'
- en: '| `property` | Categorical | The type of property |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `property` | 分类 | 财产类型 |'
- en: '| `age` | Numerical | The applicant''s age in years |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `age` | 数值 | 申请人的年龄（以年为单位） |'
- en: '| `otherPlans` | Categorical | Other installment plans |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `otherPlans` | 分类 | 其他分期付款计划 |'
- en: '| `housing` | Categorical | The applicant''s housing situation |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `housing` | 分类 | 申请人的住房状况 |'
- en: '| `existingBankCredits` | Numerical | The number of existing credits at this
    bank |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `existingBankCredits` | 数值 | 在这家银行现有的信用数量 |'
- en: '| `job` | Categorical | The applicant''s job situation |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `job` | 分类 | 申请人的工作状况 |'
- en: '| `dependents` | Numerical | The number of dependents |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `dependents` | 数值 | 受抚养人数 |'
- en: '| `telephone` | Categorical | The status of the applicant''s telephone |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `telephone` | 分类 | 申请人的电话状态 |'
- en: '| `foreign` | Categorical | Foreign worker |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `foreign` | 分类 | 外籍工人 |'
- en: '| `risk` | Binary | Credit risk (1 = good, 2 = bad) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `risk` | 二进制 | 信用风险（1 = 好，2 = 差） |'
- en: 'First, we will load the data into a data frame called `german_raw` and provide
    it with column names that match the previous table:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据加载到名为`german_raw`的数据框中，并为其提供与上一表格匹配的列名：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note from the table describing the features that we have a lot of categorical
    features to deal with. For this reason, we will employ `dummyVars()` once again
    to create dummy binary variables for these. In addition, we will record the `risk`
    variable, our output, as a factor with level 0 for good credit and level 1 for
    bad credit:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中的注释说明了我们有很多分类特征需要处理。因此，我们将再次使用`dummyVars()`来为这些特征创建虚拟的二进制变量。此外，我们将`risk`变量，我们的输出，记录为一个因子，其中0级表示良好的信用，1级表示不良的信用：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As a result of this processing, we now have a data frame with 61 features,
    because several of the categorical input features had many levels. Next, we will
    partition our data into training and test sets:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 经过此处理，我们现在有一个包含61个特征的数据框，因为几个分类输入特征有很多级别。接下来，我们将数据分为训练集和测试集：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One particularity of this dataset that is mentioned on the website is that the
    data comes from a scenario where the two different types of errors have different
    costs. Specifically, the cost of misclassifying a high-risk customer as a low-risk
    customer is five times more expensive for the bank than misclassifying a low-risk
    customer as a high-risk customer. This is understandable, as in the first case,
    the bank stands to lose a lot of money from a loan it gives out that cannot be
    repaid, whereas in the second case, the bank misses out on an opportunity to give
    out a loan that will yield interest for the bank.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的一个特定之处，在网站上提到的是，数据来自一个两种不同类型的错误具有不同成本的场景。具体来说，将高风险客户错误分类为低风险客户的成本，对于银行来说比将低风险客户错误分类为高风险客户的成本高出五倍。这是可以理解的，因为在第一种情况下，银行可能会从无法偿还的贷款中损失大量资金，而在第二种情况下，银行会错过发放能够为银行带来利息的贷款的机会。
- en: 'The `svm()` function has a `class.weights` parameter, which we use to specify
    the cost of misclassifying an observation to each class. This is how we will incorporate
    our asymmetric error cost information into our model. First, we''ll create a vector
    of class weights, noting that we need to specify names that correspond to the
    output factor levels. Then, we will use the `tune()` function to train various
    SVM models with a radial kernel:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`svm()` 函数有一个 `class.weights` 参数，我们用它来指定将观察值错误分类到每个类的成本。这就是我们将非对称错误成本信息纳入模型的方式。首先，我们将创建一个类权重向量，注意我们需要指定与输出因子水平相对应的名称。然后，我们将使用
    `tune()` 函数训练具有径向核的各种SVM模型：'
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The suggested best model has *cost = 10* and *gamma = 0.05* and achieves 74
    percent training accuracy. Let''s see how this model fares on our test dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 建议的最佳模型具有 *cost = 10* 和 *gamma = 0.05*，并在训练中达到74%的准确率。让我们看看这个模型在我们的测试数据集上的表现：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The performance on our test set is 73.5 percent and very close to what we saw
    in training. As expected, our model tends to make many more errors that misclassify
    a low risk customer as a high risk customer. Predictably, this takes a toll on
    the overall classification accuracy, which just computes the ratio of correctly
    classified observations to the overall number of observations. In fact, were we
    to remove this cost imbalance, we would actually select a different set of parameters
    for our model, and our performance, from the perspective of the unbiased classification
    accuracy, would be better:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试集上的性能是73.5%，非常接近我们在训练中看到的结果。正如预期的那样，我们的模型倾向于犯更多的错误，将低风险客户错误分类为高风险客户。可以预见，这会对整体分类准确率产生负面影响，因为整体分类准确率只是正确分类的观察值与总观察值之比。实际上，如果我们消除这种成本不平衡，我们实际上会选择一组不同的模型参数，并且从无偏分类准确率的角度来看，我们的性能会更好：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Of course, this last model will tend to make a greater number of costly misclassifications
    of high-risk customers as low-risk customers, which we know is very undesirable.
    We'll conclude this section with two final thoughts. Firstly, we have used relatively
    small ranges for the `gamma` and `cost` parameters. It is left as an exercise
    for the reader to rerun our analysis with a greater spread of values for these
    two in order to see whether we can get even better performance. This will, however,
    necessarily result in longer training times. Secondly, this particular dataset
    is quite challenging in that its baseline accuracy is actually 70 percent. This
    is because 70 percent of the customers in the data are low-risk customers (the
    two output classes are not balanced). For this reason, computing the Kappa statistic,
    which we saw in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    might be a better metric to use instead of classification accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个最后的模型可能会犯更多的代价高昂的错误，将高风险客户错误分类为低风险客户，这是我们知道的非常不希望看到的。我们将以两个最后的想法来结束本节。首先，我们为
    `gamma` 和 `cost` 参数使用了相对较小的范围。读者可以将这两个参数的值范围扩大，重新运行我们的分析，以查看我们是否可以获得更好的性能。然而，这必然会导致更长的训练时间。其次，这个特定的数据集相当具有挑战性，因为其基线准确率实际上是70%。这是因为数据中的70%的客户是低风险客户（两个输出类别不平衡）。因此，计算我们在[第1章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第1章。为预测建模做准备")中看到的Kappa统计量，*为预测建模做准备*，可能是一个更好的指标，而不是分类准确率。
- en: Multiclass classification with support vector machines
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机进行多类分类
- en: Just like with logistic regression, we've seen that the basic premise behind
    the support vector machine is that it is designed to handle two classes. Of course,
    we often have situations where we would like to be able to handle a greater number
    of classes, such as when classifying different plant species based on a variety
    of physical characteristics. One way to do this is the **one versus all** approach.
    Here, if we have *K* classes, we create *K* SVM classifiers, and for each classifier,
    we are attempting to distinguish one particular class from all the rest.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就像逻辑回归一样，我们看到了支持向量机背后的基本前提是它被设计来处理两类。当然，我们经常遇到我们希望能够处理更多类的情况，例如根据各种物理特征对不同的植物物种进行分类。一种方法是**一对多**的方法。在这里，如果我们有*K*个类别，我们创建*K*个SVM分类器，并且对于每个分类器，我们试图将一个特定的类别与所有其他类别区分开来。
- en: To determine the best class to pick, we assign the class for which the observation
    produces the highest distance from the separating hyperplane, thus lying farthest
    away from all other classes. More formally, we pick the class for which our linear
    feature combination has a maximum value across all the different classifiers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳类别，我们分配给观察结果产生与分离超平面最大距离的类别，即离所有其他类别最远的类别。更正式地说，我们选择我们的线性特征组合在所有不同分类器中具有最大值的类别。
- en: An alternative approach is known as the (balanced) **one versus one** approach.
    We create a classifier for all possible pairs of output classes. We then classify
    our observation with every one of these classifiers and tally up the totals for
    every winning class. Finally, we pick the class that has the most votes. This
    latter approach is actually what is implemented by the `svm()` function in the
    `e1071` package. We can, therefore, use this function when we have a problem with
    multiple classes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法被称为（平衡的）**一对多**方法。我们为所有可能的输出类别对创建一个分类器。然后，我们用每个这样的分类器对每个观察结果进行分类，并统计每个获胜类别的总数。最后，我们选择获得最多票数的类别。这种后一种方法实际上是`e1071`包中的`svm()`函数所实现的。因此，当我们有多个类的问题时，我们可以使用这个函数。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we presented the maximal margin hyperplane as a decision boundary
    that is designed to separate two classes by finding the maximum distance from
    either of them. When the two classes are linearly separable, this creates a situation
    where the space between the two classes is evenly split.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了最大间隔超平面作为决策边界，它是通过找到与两个类别之一的最大距离来设计用来分离两个类别的。当两个类别是线性可分时，这创造了一个两个类别之间的空间均匀分割的情况。
- en: We've seen that there are circumstances where this is not always desirable,
    such as when the classes are close to each other because of a few observations.
    An improvement to this approach is the support vector classifier that allows us
    to tolerate a few margin violations, or even misclassifications, in order to obtain
    a more stable result. This also allows us to handle classes that aren't linearly
    separable. The form of the support vector classifier can be written in terms of
    inner products between the observation that is being classified and the support
    vectors. This transforms our feature space from *p* features into as many features
    as we have support vectors. Using kernel functions on these new features, we can
    introduce nonlinearity in our model and thus obtain a support vector machine.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在某些情况下，这并不总是理想的，例如当类别由于一些观察结果而彼此接近时。这种方法的改进是支持向量分类器，它允许我们容忍一些边界违规，甚至误分类，以获得更稳定的结果。这也允许我们处理非线性可分的类别。支持向量分类器的形式可以用被分类的观察结果和支持向量之间的内积来表示。这把我们的特征空间从*p*个特征转换为我们有支持向量的那么多特征。使用这些新特征上的核函数，我们可以在模型中引入非线性，从而获得支持向量机。
- en: In practice, we saw that training a support vector classifier, which is a support
    vector machine with a linear kernel, involves adjusting the `cost` parameter.
    The performance we obtain on our training data can be close to what we get in
    our test data. By contrast, we saw that by using a radial kernel, we have the
    potential to fit our training data much more closely, but we are far more likely
    to fall into the trap of overfitting.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们发现训练一个支持向量分类器，这是一个具有线性核的支持向量机，涉及到调整`成本`参数。我们在训练数据上获得的表现可以接近我们在测试数据上获得的表现。相比之下，我们发现使用径向核，我们有可能使我们的训练数据拟合得更紧密，但我们更有可能陷入过拟合的陷阱。
- en: To deal with this, it is useful to try different combinations of the `cost`
    and the `gamma` parameters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，尝试不同的`cost`和`gamma`参数组合是有用的。
- en: 'In the next chapter, we are going to explore another cornerstone of machine
    learning: tree-based models. Also known as decision trees, they can handle regression
    and classification problems with many classes, are highly interpretable, and have
    a built-in way of handling missing data.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨机器学习的另一个基石：基于树的模型。也称为决策树，它们可以处理具有许多类别的回归和分类问题，具有高度的可解释性，并且内置了处理缺失数据的方式。
