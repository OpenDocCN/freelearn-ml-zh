- en: Chapter 6. Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 支持向量机
- en: In this chapter, we are going to take a fresh look at nonlinear predictive models
    by introducing support vector machines. Support vector machines, often abbreviated
    as SVMs, are very commonly used for classification problems, although there are
    certainly ways to perform function approximation and regression tasks with them.
    In this chapter, we will focus on the more typical case of their role in classification.
    To do this, we'll first present the notion of maximal margin classification, which
    presents an alternative formulation of how to choose between many possible classification
    boundaries and differs from approaches we have seen thus far, such as maximum
    likelihood. We'll introduce the related idea of support vectors and how, together
    with maximal margin classification, we can obtain a linear model in the form of
    a support vector classifier. Finally, we'll present how we can generalize these
    ideas in order to introduce nonlinearity through the use of certain functions
    known as kernels to finally arrive at our destination, the support vector machine.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过介绍支持向量机来重新审视非线性预测模型。支持向量机，通常缩写为SVMs，在分类问题中非常常用，尽管当然有方法使用它们进行函数逼近和回归任务。在本章中，我们将重点关注它们在分类中更典型的角色。为此，我们首先将介绍最大间隔分类的概念，它提出了如何在许多可能的分类边界之间进行选择的另一种公式化方法，并且与迄今为止我们所看到的方法不同，例如最大似然。我们将介绍相关的支持向量概念以及如何，与最大间隔分类一起，我们可以获得一个以支持向量分类器形式存在的线性模型。最后，我们将展示如何通过使用某些称为核的函数来引入非线性，最终达到我们的目的地，即支持向量机。
- en: Maximal margin classification
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大间隔分类
- en: 'We''ll begin this chapter by returning to a situation that should be very familiar
    by now: the binary classification task. Once again, we''ll be thinking about the
    problem of how to design a model that will correctly predict whether an observation
    belongs to one of two possible classes. We''ve already seen that this task is
    simplest when the two classes are linearly separable; that is, when we can find
    a *separating hyperplane* (a plane in a multidimensional space) in the space of
    our features so that all the observations on one side of the hyperplane belong
    to one class and all the observations that lie on the other side belong to the
    second class. Depending on the structure, assumptions, and optimizing criterion
    that our particular model uses, we could end up with one of infinite such hyperplanes.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，回到现在应该非常熟悉的情况：二元分类任务。再一次，我们将思考如何设计一个模型来正确预测一个观察值属于两个可能类别之一的问题。我们已经看到，当两个类别是线性可分的时候，这个任务是最简单的；也就是说，当我们可以在我们的特征空间中找到一个*分离超平面*（一个多维空间中的平面）时，超平面一侧的所有观察值属于一个类别，而位于另一侧的所有观察值属于第二个类别。根据我们特定模型使用的结构、假设和优化标准，我们可能会得到无限多个这样的超平面。
- en: 'Let''s visualize this scenario using some data in a two-dimensional feature
    space, where the separating hyperplane is just a separating line:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用二维特征空间中的某些数据来可视化这个场景，其中分离超平面仅仅是一条分离线：
- en: '![Maximal margin classification](img/00108.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00108.jpeg)'
- en: 'In the preceding diagram, we can see two clusters of observations, each of
    which belongs to a different class. We''ve used different symbols for the two
    classes to denote this explicitly. Next, we show three different lines that could
    serve as the decision boundary of a classifier, all of which would generate 100
    percent classification accuracy on the entire dataset. We''ll remind ourselves
    that the equation of a hyperplane can be expressed as a linear combination of
    the input features, which are the dimensions of the space in which the hyperplane
    resides:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到两个属于不同类别的观察值簇。我们使用了不同的符号来明确表示这一点。接下来，我们展示了三条可以作为分类器决策边界的不同线，所有这些线在整个数据集上都会产生100%的分类准确率。我们将提醒自己，超平面的方程可以表示为输入特征的线性组合，这些特征是超平面所在空间中的维度：
- en: '![Maximal margin classification](img/00109.jpeg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00109.jpeg)'
- en: 'A separating hyperplane has this property:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分离超平面具有以下属性：
- en: '![Maximal margin classification](img/00110.jpeg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00110.jpeg)'
- en: 'The first equation simply says that the data points that belong to class 1
    all lie above the hyperplane, and the second equation says that the data points
    that belong to class -1 all lie below the hyperplane. The subscript *i* is used
    to index observations, and the subscript *k* is used to index features, so that
    *x[ik]* means the value of the *k^(th)* feature in the *i^(th)* observation. We
    can combine these two equations into a single equation for simplicity, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程简单地说明属于类别 1 的数据点都位于超平面之上，第二个方程说明属于类别 -1 的数据点都位于超平面之下。下标 *i* 用于索引观察，下标 *k*
    用于索引特征，因此 *x[ik]* 表示第 *i* 个观察的第 *k* 个特征的值。为了简化，我们可以将这两个方程合并为一个方程，如下所示：
- en: '![Maximal margin classification](img/00111.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00111.jpeg)'
- en: To see why this simplification works, consider an observation of the class -1
    (*y[i]* *= -1*). This observation will lie below the separating hyperplane, so
    the linear combination in brackets will produce a negative value. Multiplying
    this with its *y[i]* value of -1 results in a positive value. A similar argument
    works for observations of class 1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这种简化的原因，考虑对类别 -1 的观察（*y[i]* = -1*）。这个观察将位于分离超平面下方，因此括号中的线性组合将产生一个负值。将其与其
    *y[i]* 的 -1 值相乘，结果为正值。对于类别 1 的观察，有类似的论点。
- en: Looking back at our diagram, note that the two dashed lines come quite close
    to certain observations. The solid line intuitively feels better than the other
    two lines as a decision boundary, as it separates the two classes without coming
    too close to either by traversing the center of the space between them. In this
    way, it distributes the space between the two classes equally. We can define a
    quantity known as the **margin** that a particular separating hyperplane generates,
    as the smallest perpendicular distance from any point in the dataset to the hyperplane.
    In two dimensions and two classes, we will always have at least two points that
    lie at a perpendicular distance equal to the margin from the separating line,
    one on each side of the line. Sometimes, as is the case with our data, we may
    have more than two points whose perpendicular distance from the separating line
    equals the margin.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的图，注意两条虚线与某些观察相当接近。直观上，实线作为决策边界比其他两条线更好，因为它在它们之间的空间中心穿越，将两个类别分开，而不靠近任何一个类别。这样，它在两个类别之间平均分配空间。我们可以定义一个称为
    **间隔** 的量，它是一个特定分离超平面产生的，即从数据集中任何点到超平面的最小垂直距离。在二维和两个类别的情况下，我们总是至少有两个点，它们与分离线的垂直距离等于间隔，一个在线的每一侧。有时，正如我们的数据那样，我们可能有超过两个点，它们与分离线的垂直距离等于间隔。
- en: 'The next plot shows the margin of the solid line from the previous plot, demonstrating
    that we have three points at a distance equal to the margin from this separating
    line:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图显示了前一个图中实线的间隔，表明我们有三个点与这个分离线的间隔相等：
- en: '![Maximal margin classification](img/00112.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00112.jpeg)'
- en: Now that we have the definition of the margin under our belt, we have a way
    to codify our intuition that led us to choose the solid line as the better decision
    boundary among the three lines that we saw in the first plot. We can go a step
    further and define the **maximal margin hyperplane** as the hyperplane whose margin
    is the largest amongst all possible separating hyperplanes. In our 2D example,
    we are essentially looking for the line that will separate the two classes while
    at the same time being as far away from the observations as possible. It turns
    out that the solid line from our example is actually the maximal margin line,
    so that there is no other line that can be drawn with a higher margin than two
    units. This explains why we chose to label it as the line of maximum margin separation
    in our first plot.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了间隔的定义，我们就有了一种方法来编码我们选择实线作为三个线中较好决策边界的直觉。我们可以更进一步，定义 **最大间隔超平面** 为所有可能的分离超平面中间隔最大的超平面。在我们的二维例子中，我们实际上是在寻找一条线，它将两个类别分开，同时尽可能远离观察点。结果证明，我们例子中的实线实际上是最大间隔线，因此没有其他线可以画出比两个单位更高的间隔。这解释了为什么我们在第一个图中将其标记为最大间隔分离线。
- en: 'In order to understand how we found the maximal margin hyperplane in our simple
    example, we need to formalize the problem as an optimization problem with *p*
    features by using the following equations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们如何在简单示例中找到最大间隔超平面，我们需要使用以下方程将问题形式化为一个具有*p*个特征的优化问题：
- en: '![Maximal margin classification](img/00113.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类](img/00113.jpeg)'
- en: Together, these two constraints in our optimization problem express the idea
    that observations in our data need to not only be correctly classified, but also
    lie at least *M* units away from the separating hyperplane. The goal is to maximize
    this distance *M* by appropriately choosing the coefficients *β[i]*. Thus, we
    need an optimization procedure that handles this type of problem. The details
    of how the optimization is actually implemented in practice are beyond the scope
    of this book, but we will see them in action later on when we do some programming
    with R.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个约束条件共同表达了我们的优化问题中的想法，即我们的数据中的观察点不仅需要被正确分类，而且至少需要位于分离超平面至少*M*个单位之外。目标是通过对系数*β[i]*进行适当的选取来最大化这个距离*M*。因此，我们需要一个处理这类问题的优化过程。优化实际上如何在实践中实现的具体细节超出了本书的范围，但当我们用R进行编程时，我们将在后面看到它们是如何发挥作用的。
- en: 'We have a natural way forward now, which is to start looking at how the situation
    changes when our data is not linearly separable, something that we know by now
    is the typical scenario for a real-world dataset. Let us take a step back before
    doing this. We''ve already studied two different methods for estimating the parameters
    of a model: namely, maximum likelihood estimation and the least squared error
    criterion for linear regression. For example, when we looked at classification
    with logistic regression, we considered the idea of maximizing the likelihood
    of our data. This takes into account all of the available data points. This is
    also the case when classifying with multilayer perceptrons. With the maximum margin
    classifier, however, the construction of our decision boundary is only supported
    by the points that lie on the margin. Put differently, with the data in our 2D
    example, we can freely adjust the position of any observation except the three
    on the margin, and as long as the adjustment does not result in the observation
    falling inside the margin, our separating line will stay exactly in the same position.
    For this reason, we define the perpendicular vectors from the points that lie
    on the margin to the separating hyperplane as the **support vectors**. Thus, we''ve
    seen that our 2D example has three support vectors. The fact that only a subset
    of all the points in our data essentially determines the placement of the separating
    hyperplane means that we have the potential to overfit our training data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个自然的前进方式，那就是开始研究当我们的数据不是线性可分时，情况会如何变化，我们知道这是现实世界数据集的典型场景。在这样做之前，让我们退一步。我们已经研究了两种估计模型参数的方法：即最大似然估计和线性回归的最小二乘误差标准。例如，当我们研究逻辑回归的分类时，我们考虑了最大化我们数据似然的想法。这考虑了所有可用的数据点。在用多层感知器进行分类时也是如此。然而，对于最大间隔分类器，我们的决策边界的构建只由位于边缘的点支持。换句话说，在我们的二维示例中，我们可以自由调整除边缘上的三个点外的任何观察点的位置，只要调整不会导致观察点落在边缘内，我们的分离线将保持在完全相同的位置。因此，我们将从位于边缘的点到分离超平面的垂直向量定义为**支持向量**。因此，我们已经看到我们的二维示例有三个支持向量。只有数据集中所有点的子集实际上决定了分离超平面的位置这一事实意味着我们有过度拟合训练数据的潜力。
- en: On the other hand, this approach does yield a couple of nice properties. We
    split the space between the two classes equally between them without applying
    any bias toward either. Points that clearly lie well inside the area of space
    occupied by a particular class do not play such a big role in the model compared
    to points on the fringes, which is where we need to place our decision boundary.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这种方法确实产生了一些很好的性质。我们在两个类别之间平等地分割空间，而不对任何一个类别施加任何偏差。显然位于特定类别占据的空间内的点的点在模型中的作用不如边缘上的点大，这是我们放置决策边界的区域。
- en: Support vector classification
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量分类
- en: 'We need our data to be linearly separable in order to classify it with a maximal
    margin classifier. When our data is not linearly separable, we can still use the
    notion of support vectors that define a margin, but this time, we will allow some
    examples to be misclassified. Thus, we essentially define a **soft margin**, in
    that some of the observations in our dataset can violate the constraint that they
    need to be at least as far as the margin from the separating hyperplane. It is
    also important to note that sometimes we may want to use a soft margin even for
    linearly separable data. The reason for this is in order to limit the degree of
    overfitting the data. Note that the larger the margin, the more confident we are
    about our ability to correctly classify new observations, because the classes
    are further apart from each other in our training data. If we achieve separation
    using a very small margin, we are less confident about our ability to correctly
    classify our data and we may, instead, want to allow a few errors and come up
    with a larger margin that is more robust. Study the following plot:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要我们的数据是线性可分的，以便使用最大边界分类器对其进行分类。当我们的数据不是线性可分时，我们仍然可以使用定义边界的支持向量的概念，但这次，我们将允许一些示例被误分类。因此，我们本质上定义了一个**软边界**，即我们数据集中的某些观测可能违反了它们需要至少与分离超平面保持一定距离的约束。同样重要的是要注意，有时我们可能即使在数据是线性可分的情况下也想使用软边界。这样做的原因是为了限制数据过度拟合的程度。请注意，边界越大，我们对正确分类新观测的信心就越大，因为在我们训练数据中，类别彼此之间的距离越远。如果我们使用非常小的边界实现分离，我们对正确分类数据的信心就会降低，我们可能更愿意允许一些错误，并提出一个更大的、更稳健的边界。研究以下图表：
- en: '![Support vector classification](img/00114.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量分类](img/00114.jpeg)'
- en: 'In order to get a firmer grasp of the reason why a soft margin may be preferable
    to a hard margin, even for linearly separable data, we''ve changed our data slightly.
    We used the same data that we had previously, but we added an extra observation
    to class 1 and placed it close to the boundary of class -1\. Note that with the
    addition of this single new data point, with feature values f1=16 and f2=40, our
    maximal margin line has moved drastically! The margin has been reduced from two
    units to 0.29 units. Looking at this graph, we are tempted to feel that the new
    point might either be an outlier or a mislabeling in our dataset. If we were to
    allow our model to make one single misclassification using a soft margin, we would
    go back to our previous line, which separates the two classes with a much wider
    margin and is less likely to have overfit on the data. We formalize the notion
    of our soft classifier by modifying our optimization problem set up:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更牢固地掌握为什么即使是对于线性可分的数据，软边界可能比硬边界更可取的原因，我们稍微改变了我们的数据。我们使用了之前相同的数据，但我们在类别1中添加了一个额外的观测点，并将其放置在类别-1的边界附近。请注意，随着这个单一新数据点的添加，特征值f1=16和f2=40，我们的最大边界线发生了巨大变化！边界从两个单位减少到0.29单位。看着这张图，我们可能会觉得这个新点可能是我们的数据集中的异常值或误标记。如果我们允许我们的模型使用软边界进行一次误分类，我们会回到我们之前的线，这条线以更宽的边界分隔两个类别，并且不太可能对数据进行过度拟合。我们通过修改我们的优化问题设置来形式化我们的软分类器的概念：
- en: '![Support vector classification](img/00115.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量分类](img/00115.jpeg)'
- en: 'Under this new setup, we''ve introduced a new set of variables *ξ*i, known
    as the **slack variables**. There is one slack variable for every observation
    in our dataset and the value of the *ξ*i slack variable depends on where the *i*th
    observation falls with respect to the margin. When an observation is on the correct
    side of the separating hyperplane and outside the margin, the slack variable for
    that observation takes the value 0\. This is the ideal situation that we have
    seen for all observations under a hard boundary. When an observation is correctly
    classified but falls at a distance within the margin, the corresponding slack
    variable takes a small positive value less than 1\. When an observation is actually
    misclassified, thus falling on the wrong side of the hyperplane altogether, then
    its associated slack variable takes a value greater than 1\. In summary, take
    a look at the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的设置下，我们引入了一组新的变量 *ξ*i，被称为**松弛变量**。对于我们的数据集中的每一个观测值，都有一个松弛变量，而 *ξ*i 松弛变量的值取决于第
    *i* 个观测值相对于边界的位置。当一个观测值位于分离超平面的正确一侧且在边界之外时，该观测值的松弛变量取值为 0。这是我们对于所有观测值在硬边界下看到的最理想的情况。当一个观测值被正确分类但落在边界内的一定距离处时，相应的松弛变量取一个小于
    1 的小正数。当一个观测值实际上被错误分类，因此完全落在超平面的错误一侧时，其关联的松弛变量取值大于 1。总的来说，看看以下内容：
- en: '![Support vector classification](img/00116.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量分类](img/00116.jpeg)'
- en: When an observation is incorrectly classified, the magnitude of the slack variables
    is proportional to the distance between that observation and the boundary of the
    separating hyperplane. The fact that the sum of the slack variables must be less
    than a constant *C* means that we can think of this constant as an error budget
    that we are prepared to tolerate. As a misclassification of a single particular
    observation results in a slack variable taking at least the value 1, and our constant
    *C* is the sum of all the slack variables, setting a value of *C* less than 1
    means that our model will tolerate a few observations falling inside the margin,
    but no misclassifications. A high value of *C* often results in many observations
    either falling inside the margin or being misclassified, and as these are all
    support vectors, we end up having a greater number of support vectors. This results
    in a model that has a lower variance, but because we have shifted our boundary
    in a way that has increased tolerance to margin violations and errors, we may
    have a higher bias. By contrast, depending on fewer support vectors caused by
    having a much stricter model (and hence a lower value of *C*) may result in a
    lower bias in our model. These support vectors, however, will individually affect
    the position of our boundary to a much higher degree. Consequently, we will experience
    a higher variance in our model performance across different training sets. Once
    again, the interplay between model bias and variance resurfaces in the design
    decisions that we must make as predictive modelers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个观测值被错误分类时，松弛变量的幅度与该观测值与分离超平面边界的距离成正比。由于松弛变量的总和必须小于一个常数 *C*，我们可以将这个常数视为我们准备容忍的错误预算。由于单个特定观测值的错误分类会导致松弛变量至少取值为
    1，而我们的常数 *C* 是所有松弛变量的总和，将 *C* 的值设为小于 1 意味着我们的模型将容忍一些观测值落在边界内，但不会出现错误分类。*C* 的值较高通常会导致许多观测值要么落在边界内，要么被错误分类，而这些都是支持向量，我们最终会有更多的支持向量。这导致了一个具有较低方差但因为我们已经通过增加对边界违规和错误的容忍度而改变了边界，我们可能会有更高的偏差。相比之下，由于模型（因此
    *C* 的值较低）非常严格而导致的支持向量数量减少，可能会在我们的模型中产生较低的偏差。然而，这些支持向量将分别以更高的程度影响我们边界的位置。因此，我们将在不同的训练集上经历模型性能的更高方差。再次强调，模型偏差和方差之间的相互作用再次出现在我们作为预测模型制定者必须做出的设计决策中。
- en: Inner products
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内积
- en: 'The exact details of how the parameters of the support vector classifier model
    are computed are beyond the scope of this book. However, it turns out that the
    model itself can be simplified into a more convenient form that uses the **inner
    products** of the observations. An inner product of two vectors of identical length,
    *v1* and *v2*, is computed by first computing the element-wise multiplication
    of the two vectors and then taking the sum of the resulting elements. In R, we
    obtain an element-wise multiplication of two vectors by simply using the multiplication
    symbol. So we can compute the inner product of two vectors as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机模型参数计算的详细过程超出了本书的范围。然而，结果表明，该模型本身可以被简化为一个更方便的形式，该形式使用观测值的**内积**。两个长度相同的向量
    *v1* 和 *v2* 的内积是通过首先计算两个向量的逐元素乘积，然后取这些结果的和来计算的。在 R 中，我们只需使用乘号即可获得两个向量的逐元素乘积。因此，我们可以按以下方式计算两个向量的内积：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In mathematical terms, we use triangular brackets to denote the inner product
    operation, and we represent the process as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，我们使用三角括号来表示内积运算，并按以下方式表示这个过程：
- en: '![Inner products](img/00117.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00117.jpeg)'
- en: 'In the preceding equation, for the two vectors *v[1]* and *v[2]*, the index
    *i* is iterating over the *p* features or dimensions. Now, here is the original
    form of our support vector classifier:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，对于两个向量 *v[1]* 和 *v[2]*，索引 *i* 正在遍历 *p* 个特征或维度。现在，这是我们的支持向量机分类器的原始形式：
- en: '![Inner products](img/00118.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00118.jpeg)'
- en: 'This is just the standard equation for a linear combination of the input features.
    It turns out that for the support vector classifier, the model''s solution can
    be expressed in terms of the inner product between the *x* observation that we
    are trying to classify and all other *x*i observations that are in our training
    dataset. More concretely, the form of our support vector classifier can also be
    written as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是输入特征线性组合的标准方程。结果表明，对于支持向量机，模型解可以用我们试图分类的 *x* 观测值与其他所有训练数据集中的 *x*i 观测值之间的内积来表示。更具体地说，我们的支持向量机的形式也可以写成：
- en: '![Inner products](img/00119.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00119.jpeg)'
- en: For this equation, we have explicitly indicated that our model predicts *y*
    as a function of an input observation *x*. The summing function now computes a
    weighted sum of all the inner products of the current observation with every other
    observation in the dataset, which is why we are now summing across the *n* observations.
    We want to make it very clear that we haven't changed anything in the original
    model itself; we have simply written two different representations of the same
    model. Note that we cannot assume that a linear model takes this form in general;
    this is only true for the support vector classifier. Now, in a real-world scenario,
    the number of observations in our data set, *n*, is typically much greater than
    the number of parameters, *p*, so the number of *α* coefficients is seemingly
    larger than the number of *β* coefficients.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方程，我们明确指出，我们的模型将 *y* 预测为输入观测值 *x* 的函数。求和函数现在计算当前观测值与数据集中每个其他观测值的内积的加权和，这就是为什么我们现在要对
    *n* 个观测值进行求和。我们想非常清楚地说明，我们没有改变原始模型本身；我们只是写了同一模型的两种不同表示。请注意，我们不能假设线性模型在一般情况下都采取这种形式；这仅适用于支持向量机。现在，在现实世界的场景中，我们数据集中观测值的数量
    *n* 通常远大于参数的数量 *p*，因此 *α* 系数的数量似乎比 *β* 系数的数量大。
- en: Additionally, whereas in the first equation we were considering observations
    independently of each other, the form of the second equation shows us that to
    classify all our observations, we need to consider all possible pairs and compute
    their inner product. There are such pairs, which are of the order of *n*2\. Thus,
    it would seem like we are introducing complexity rather than producing a representation
    that is simpler. It turns out, however, that all *α* coefficients are zero for
    all observations in our dataset, except those that are support vectors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然在第一个方程中我们是独立考虑每个观测值，但第二个方程的形式表明，为了分类所有观测值，我们需要考虑所有可能的成对组合并计算它们的内积。这样的成对组合有
    *n*2 个，这似乎是在引入复杂性而不是产生一个更简单的表示。然而，实际上，在我们的数据集中，除了支持向量之外，所有 *α* 系数都是零。
- en: 'The number of support vectors in our dataset is typically much smaller than
    the total number of observations. Thus, we can simplify our new representation
    by explicitly showing that we sum over elements from the set of support vectors,
    *S*, in our dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，支持向量的数量通常远小于总观察数量。因此，我们可以通过明确显示我们在数据集中的支持向量集合 *S* 上求和来简化我们的新表示：
- en: '![Inner products](img/00120.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![内积](img/00120.jpeg)'
- en: Kernels and support vector machines
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核和支持向量机
- en: So far, we've introduced the notion of maximum margin classification under linearly
    separable conditions and its extension to the support vector classifier, which
    still uses a hyperplane as the separating boundary but handles datasets that are
    not linearly separable by specifying a budget for tolerating errors. The observations
    that are on or within the margin, or are misclassified by the support vector classifier,
    are support vectors. The critical role that these play in the positioning of the
    decision boundary was also seen in an alternative model representation of the
    support vector classifier that uses inner products.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们介绍了在线性可分条件下最大间隔分类的概念及其扩展到支持向量分类器，它仍然使用超平面作为分离边界，但通过指定容错预算来处理非线性可分的数据集。位于或位于间隔内，或被支持向量分类器错误分类的观察值是支持向量。这些在决策边界定位中发挥的关键作用也在使用内积的替代模型表示支持向量分类器中得到了体现。
- en: 'What is common in the situations that we have seen so far in this chapter is
    that our model is always linear in terms of the input features. We''ve seen that
    the ability to create models that implement nonlinear boundaries between the classes
    to be separated is far more flexible in terms of the different kinds of underlying
    target functions that they can handle. One way to introduce nonlinearity in our
    model that uses our new representation involving inner products is to apply a
    nonlinear transformation to this result. We can define a general function *K*,
    which we''ll call a **kernel function**, that operates on two vectors and produces
    a scalar result. This allows us to generalize our model as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们迄今为止看到的情况中，共同点是我们的模型在输入特征方面总是线性的。我们已经看到，创建实现非线性边界的模型的能力，在处理不同类型的潜在目标函数方面要灵活得多。在我们的模型中引入非线性的一种方法是对这个结果应用非线性变换。我们可以定义一个通用函数
    *K*，我们将它称为**核函数**，它作用于两个向量并产生一个标量结果。这允许我们如下泛化我们的模型：
- en: '![Kernels and support vector machines](img/00121.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00121.jpeg)'
- en: Our model now has as many features as there are support vectors, and each feature
    is defined as the result of a kernel acting upon the current observation and one
    of the support vectors. For the support vector classifier, the kernel we applied
    is known as the **linear kernel,** as this just uses the inner product itself,
    producing a linear model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的模型具有与支持向量一样多的特征，每个特征都被定义为核函数作用于当前观察结果和其中一个支持向量的结果。对于支持向量机分类器，我们应用的核函数被称为**线性核**，因为这仅仅使用内积本身，产生一个线性模型。
- en: '![Kernels and support vector machines](img/00122.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00122.jpeg)'
- en: 'Kernel functions are also known as similarity functions, as we can consider
    the output they produce as a measure of the similarity between the two input vectors
    provided. We introduce nonlinearity in our model using nonlinear kernels, and
    when we do this, our model is known as a **support vector machine**. There are
    a number of different types of nonlinear kernels. The two most common ones are
    the **polynomial kernel** and the **radial basis function kernel**. The polynomial
    kernel uses a power expansion of the inner product between two vectors. For a
    polynomial of degree *d*, the form of the polynomial kernel is:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数也被称为相似度函数，因为我们可以将它们产生的输出视为两个输入向量之间相似度的度量。我们使用非线性核在我们的模型中引入非线性，当我们这样做时，我们的模型被称为**支持向量机**。有几种不同类型的非线性核。最常见的是**多项式核**和**径向基函数核**。多项式核使用两个向量之间内积的幂展开。对于度数为
    *d* 的多项式，多项式核的形式如下：
- en: '![Kernels and support vector machines](img/00123.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00123.jpeg)'
- en: 'Using this kernel, we are essentially transforming our feature space into a
    higher dimensional space. Computing the kernel applied to the inner product is
    much more efficient than first transforming all the features into a high-dimensional
    space and then trying to fit a linear model into that space. This is especially
    true when we use the **radial basis function kernel**, often referred to simply
    as the **radial kernel**, where the number of dimensions of the transformed feature
    space is actually infinite due to the infinite number of terms in the expansion.
    The form of the radial kernel is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个核函数，我们实际上是将我们的特征空间转换到了一个更高维的空间。计算应用于内积的核函数比首先将所有特征转换到高维空间，然后尝试在那个空间中拟合线性模型要高效得多。这在我们使用**径向基函数核**时尤其正确，通常简称为**径向核**，因为由于展开中的项数无限，转换后的特征空间的维度实际上是无限的。径向核的形式是：
- en: '![Kernels and support vector machines](img/00124.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![核和支持向量机](img/00124.jpeg)'
- en: Upon close inspection, we should be able to spot that the radial kernel does
    not use the inner product between two vectors. Instead, the summation in the exponent
    is just the square of the Euclidean distance between these two vectors. The radial
    kernel is often referred to as a **local kernel**, because when the Euclidean
    distance between the two input vectors is large, the resulting value that the
    kernel computes is very small because of the negative sign in the exponent. Consequently,
    when we use a radial kernel, only vectors close to the current observation for
    which we want to get a prediction play a significant role in the computation.
    We're now ready to put all this to practice with some real-world datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察后，我们应该能够发现径向核不使用两个向量之间的内积。相反，指数中的求和只是这两个向量之间欧几里得距离的平方。径向核通常被称为**局部核**，因为当两个输入向量之间的欧几里得距离很大时，由于指数中的负号，核计算出的结果非常小。因此，当我们使用径向核时，只有接近当前观察值的向量在计算中起重要作用。我们现在已经准备好使用一些真实世界的数据集来实践所有这些内容。
- en: Predicting chemical biodegration
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测化学物质生物降解
- en: 'In this section, we are going to use R''s `e1071` package to try out the models
    we''ve discussed on a real-world dataset. As our first example, we have chosen
    the *QSARbiodegration data set*, which can be found at [https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation).
    This is a dataset containing 41 numerical variables that describe the molecular
    composition and properties of 1,055 chemicals. The modeling task is to predict
    whether a particular chemical will be biodegradable based on these properties.
    Example properties are the percentages of carbon, nitrogen, and oxygen atoms,
    as well as the number of heavy atoms in the molecule. These features are highly
    specialized and sufficiently numerous, so a full listing won''t be given here.
    The complete list and further details of the quantities involved can be found
    on the website. For now, we''ve downloaded the data into a `bdf` data frame:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用R语言的`e1071`包，在一个真实世界的数据集上尝试我们讨论过的模型。作为第一个例子，我们选择了*QSARbiodegration数据集*，可以在[https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation)找到。这是一个包含41个数值变量，描述了1,055种化学物质的分子组成和性质的数据集。建模任务是预测特定化学物质是否可生物降解，基于这些性质。示例性质包括碳、氮、氧原子的百分比，以及分子中的重原子数量。这些特征非常专业且数量充足，因此这里不会给出完整的列表。涉及到的完整列表和更多细节可以在网站上找到。目前，我们已经将数据下载到了一个`bdf`数据框中：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The final column, `V42`, contains the output variable, which takes the value
    `NRB` for chemicals that are not biodegradable and `RB` for those that are. We''ll
    recode this into the familiar labels of `0` and `1`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个列，`V42`，包含输出变量，对于不可生物降解的化学物质取值为`NRB`，对于可生物降解的化学物质取值为`RB`。我们将将其重新编码为熟悉的标签`0`和`1`：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have our data ready, we''ll begin, as usual, by splitting them
    into training and testing sets, with an 80-20 split:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，我们将像往常一样，将它们分为训练集和测试集，比例为80-20：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are a number of packages available in R that implement support vector
    machines. In this chapter, we''ll explore the use of the `e1071` package, which
    provides us with the `svm()` function. If we examine our training data, we''ll
    quickly notice that on the one hand, the scales of the various features are quite
    different, and on the other hand, many features are sparse features, which means
    that for many entries they take a zero value. It is a good idea to scale features
    as we did with neural networks, especially if we want to work with radial kernels.
    Fortunately for us, the `svm()` function has a `scale` parameter, which is set
    to `TRUE` by default. This normalizes the input features so that they have zero
    mean and unit variance before the model is trained. This circumvents the need
    for us to manually carry out this preprocessing step. The first model that we
    will investigate will use a linear kernel:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，有多个包实现了支持向量机。在本章中，我们将探讨使用`e1071`包，它为我们提供了`svm()`函数。如果我们检查我们的训练数据，我们会很快注意到一方面，各种特征的比例相差很大，另一方面，许多特征是稀疏特征，这意味着对于许多条目，它们取零值。在神经网络中我们这样做，将特征进行缩放是一个好主意，尤其是如果我们想使用径向核。幸运的是，`svm()`函数有一个`scale`参数，默认设置为`TRUE`。在模型训练之前，这个参数将标准化输入特征，使它们具有零均值和单位方差。这避免了我们需要手动执行此预处理步骤的需要。我们将要研究的第一个模型将使用线性核：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The call to the `svm()` function follows the familiar paradigm of first providing
    a formula, then providing the name of the data frame, and finally, other parameters
    relevant to the model. In our case, we want to train a model where the final `V42`
    column is the predictor column and all other columns are to be used as features.
    For this reason, we can just use the simple formula `V42 ~` instead of having
    to fully enumerate all the other columns. After specifying our data frame, we
    then specify the type of kernel we will use, and in this case, we''ve opted for
    a linear kernel. We''ll also specify a value for the `cost` parameter, which is
    related to the error budget C in our model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`svm()`函数遵循熟悉的范式，首先提供一个公式，然后提供数据框的名称，最后提供与模型相关的其他参数。在我们的情况下，我们想要训练一个模型，其中最终的`V42`列是预测列，所有其他列都用作特征。因此，我们可以只使用简单的公式`V42
    ~`，而不是必须完全列出所有其他列。指定我们的数据框后，我们再指定我们将使用的核的类型，在这种情况下，我们选择了线性核。我们还将指定`cost`参数的值，这与我们的模型中的错误预算C相关：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our model doesn''t provide us with too much information on its performance
    beyond the details of the parameters that we specified. One interesting piece
    of information is the number of data points that were support vectors in our model;
    in this case, `272`. If we use the `str()` function to examine the structure of
    the fitted model, however, we will find that it contains a number of useful attributes.
    For example, the fitted attribute contains the model''s predictions on the training
    data. We''ll use this to gauge the quality of model fit by computing the accuracy
    of the training data and the confusion matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型没有提供太多关于其性能的信息，除了我们指定的参数细节。一个有趣的信息是，在我们的模型中作为支持向量的数据点的数量；在这种情况下，`272`。然而，如果我们使用`str()`函数来检查拟合模型的架构，我们会发现它包含许多有用的属性。例如，拟合属性包含模型对训练数据的预测。我们将使用这些预测来评估模型拟合的质量，通过计算训练数据的准确率和混淆矩阵：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have a training accuracy of just under 89 percent, which is a decent start.
    Next, we''ll examine the performance of the test data using the `predict()` function
    to see whether we can get a test accuracy close to this or whether we have ended
    up overfitting the data:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练准确率略低于89%，这是一个不错的开始。接下来，我们将使用`predict()`函数检查测试数据的性能，看看我们是否能得到接近这个准确率的测试准确率，或者我们是否最终过度拟合了数据：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We do have a slightly lower test accuracy than we''d expect, but we are sufficiently
    close to the training accuracy we obtained earlier in order to be relatively confident
    that we are not in a position where we are overfitting the training data. Now,
    we''ve seen that the `cost` parameter plays an important role in our model, and
    that choosing this involves a trade-off in model bias and variance. Consequently,
    we want to try different values of our `cost` parameter before settling on a final
    model. After manually repeating the preceding code for a few values of this parameter,
    we obtained the following set of results:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实比预期的测试准确率略低，但与我们在之前训练中获得的准确率足够接近，因此我们可以相对有信心地认为我们并没有过度拟合训练数据。现在，我们已经看到`cost`参数在我们的模型中起着重要作用，选择这个参数涉及到模型偏差和方差的权衡。因此，在确定最终模型之前，我们想要尝试`cost`参数的不同值。在手动重复前述代码的几个参数值之后，我们得到了以下结果集：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Sometimes, when building a model, we may see a warning informing us that the
    maximum number of iterations has been reached. If this happens, we should be doubtful
    of the model that we produced, as it may be an indication that a solution was
    not found and the optimization procedure did not converge. In such a case, it
    is best to experiment with a different `cost` value and/or kernel type.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，在构建模型时，我们可能会看到一个警告，告知我们已达到最大迭代次数。如果发生这种情况，我们应该对我们的模型持怀疑态度，因为这可能是没有找到解决方案并且优化过程没有收敛的迹象。在这种情况下，最好是尝试不同的`cost`值和/或核类型。
- en: These results show that for most values of the `cost` parameter, we are seeing
    a very similar level of quality of fit on our training data, roughly 88 percent.
    Ironically, the best performance on the test data was obtained using the model
    whose fit on the training data was the worst, using a cost of 0.01\. In short,
    although we have reasonable performance on our training and test datasets, the
    low variance in the results shown in the table essentially tells us that we are
    not going to get a significant improvement in the quality of fit by tweaking the
    `cost` parameter on this particular dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，对于`cost`参数的大多数值，我们在训练数据上看到的是非常相似的质量拟合水平，大约88%。具有讽刺意味的是，在测试数据上获得最佳性能的是在训练数据拟合最差的模型，使用了0.01的成本。简而言之，尽管我们在训练和测试数据集上都有合理的性能，但表格中显示的结果的低方差实际上告诉我们，通过调整这个特定数据集上的`cost`参数，我们不太可能显著提高拟合质量。
- en: 'Now let''s try using a radial kernel to see whether introducing some nonlinearity
    can allow us to improve our performance. When we specify a radial kernel, we must
    also specify a positive `gamma` parameter. This corresponds to the *1/2σ2* parameter
    in the equation of a radial kernel. The role that this parameter plays is that
    it controls the locality of the similarity computation between its two vector
    inputs. A large `gamma` means that the kernel will produce values that are close
    to zero, unless the two vectors are very close together. A smaller `gamma` results
    in a smoother kernel and takes into account pairs of vectors that are farther
    away. Again, this choice boils down to a trade-off between bias and variance,
    so just as with the `cost` parameter, we''ll have to try out different values
    of `gamma`. For now, let''s see how we can create a support vector machine model
    using a radial kernel with a specific configuration:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用径向核来查看是否引入一些非线性可以让我们提高性能。当我们指定径向核时，我们还必须指定一个正的`gamma`参数。这对应于径向核方程中的*1/2σ2*参数。这个参数所起的作用是控制其两个向量输入之间的相似度计算的局部性。大的`gamma`值意味着核将产生接近零的值，除非两个向量非常接近。较小的`gamma`值会导致核更加平滑，并考虑距离较远的向量对。同样，这个选择归结为偏差和方差的权衡，所以就像`cost`参数一样，我们不得不尝试`gamma`的不同值。现在，让我们看看如何使用特定配置的径向核创建支持向量机模型：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that the radial kernel under these settings is able to fit the training
    data much more closely, as indicated by the nearly 100 percent training accuracy;
    but when we see the performance on the test dataset, the result is substantially
    lower than what we obtained on the training data. Consequently, we have a very
    clear indication that this model is overfitting the data. To get around this problem,
    we will manually experiment with a few different settings of the `gamma` and `cost`
    parameters to see whether we can improve the fit:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, the combination of the two parameters, `cost` and `gamma`, yields
    a much wider range of results using the radial kernel. From the data frame we
    built previously, we can see that some combinations, such as *cost = 1* and *gamma
    = 0.05*, have brought our accuracy up to 89 percent on the test data, while still
    maintaining an analogous performance on our training data. Also, in the data frame,
    we see many examples of settings in which the training accuracy is nearly 100
    percent, but the test accuracy is well below this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we conclude that using a nonlinear kernel, such as the radial kernel,
    needs to be done with care in order to avoid overfitting. Nonetheless, radial
    kernels are very powerful and can be quite effective at modeling a highly nonlinear
    decision boundary, often allowing us to achieve higher rates of classification
    accuracy than a linear kernel. At this point in our analysis, we would usually
    want to settle on a particular value for the `cost` and `gamma` parameters and
    then retrain our model using the entire data available before deploying it in
    the real world.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, after using the test set to guide our decision on what parameters
    to use, it no longer represents an unseen dataset that would enable us to predict
    the model's accuracy in the real world. One possible solution to this problem
    is to use a validation set, but this would require us to set aside some of our
    data, resulting in smaller training and test set sizes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-validation*, which we covered in [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Tidying Data and Measuring
    Performance*, should be considered as a practical way out of this dilemma.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very readable book on support vector machines is *An Introduction to Support
    Vector Machines and Other Kernel-based Learning Methods* by Nello Christiani and
    John Shawe-Taylor. Another good reference that presents an insightful link between
    SVMs and a related type of neural network known as a **Radial Basis Function Network**
    is *Neural Networks and Learning Machines* by Simon Haykin, which we also referenced
    in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Neural Networks*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit scores
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore another dataset, this time in the field of
    banking and finance. The particular dataset in question is known as the *German
    Credit Dataset* and is also hosted by the UCI Machine Learning Repository. The
    link to the data is [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The observations in the dataset are loan applications made by individuals at
    a bank. The goal of the data is to determine whether an application constitutes
    a high credit risk.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| `checking` | Categorical | The status of the existing checking account |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| `duration` | Numerical | The duration in months |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| `creditHistory` | Categorical | The applicant''s credit history |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| `purpose` | Categorical | The purpose of the loan |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| `credit` | Numerical | The credit amount |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| `savings` | Categorical | Savings account/bonds |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: '| `employment` | Categorical | Present employment since |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| `installmentRate` | Numerical | The installment rate (as a percentage of
    disposable income) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| `personal` | Categorical | Personal status and gender |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| `debtors` | Categorical | Other debtors/guarantors |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| `presentResidence` | Numerical | Present residence since |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: '| `property` | Categorical | The type of property |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '| `age` | Numerical | The applicant''s age in years |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: '| `otherPlans` | Categorical | Other installment plans |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| `housing` | Categorical | The applicant''s housing situation |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| `existingBankCredits` | Numerical | The number of existing credits at this
    bank |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| `job` | Categorical | The applicant''s job situation |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| `dependents` | Numerical | The number of dependents |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| `telephone` | Categorical | The status of the applicant''s telephone |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| `foreign` | Categorical | Foreign worker |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| `risk` | Binary | Credit risk (1 = good, 2 = bad) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: 'First, we will load the data into a data frame called `german_raw` and provide
    it with column names that match the previous table:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note from the table describing the features that we have a lot of categorical
    features to deal with. For this reason, we will employ `dummyVars()` once again
    to create dummy binary variables for these. In addition, we will record the `risk`
    variable, our output, as a factor with level 0 for good credit and level 1 for
    bad credit:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As a result of this processing, we now have a data frame with 61 features,
    because several of the categorical input features had many levels. Next, we will
    partition our data into training and test sets:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One particularity of this dataset that is mentioned on the website is that the
    data comes from a scenario where the two different types of errors have different
    costs. Specifically, the cost of misclassifying a high-risk customer as a low-risk
    customer is five times more expensive for the bank than misclassifying a low-risk
    customer as a high-risk customer. This is understandable, as in the first case,
    the bank stands to lose a lot of money from a loan it gives out that cannot be
    repaid, whereas in the second case, the bank misses out on an opportunity to give
    out a loan that will yield interest for the bank.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'The `svm()` function has a `class.weights` parameter, which we use to specify
    the cost of misclassifying an observation to each class. This is how we will incorporate
    our asymmetric error cost information into our model. First, we''ll create a vector
    of class weights, noting that we need to specify names that correspond to the
    output factor levels. Then, we will use the `tune()` function to train various
    SVM models with a radial kernel:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The suggested best model has *cost = 10* and *gamma = 0.05* and achieves 74
    percent training accuracy. Let''s see how this model fares on our test dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The performance on our test set is 73.5 percent and very close to what we saw
    in training. As expected, our model tends to make many more errors that misclassify
    a low risk customer as a high risk customer. Predictably, this takes a toll on
    the overall classification accuracy, which just computes the ratio of correctly
    classified observations to the overall number of observations. In fact, were we
    to remove this cost imbalance, we would actually select a different set of parameters
    for our model, and our performance, from the perspective of the unbiased classification
    accuracy, would be better:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Of course, this last model will tend to make a greater number of costly misclassifications
    of high-risk customers as low-risk customers, which we know is very undesirable.
    We'll conclude this section with two final thoughts. Firstly, we have used relatively
    small ranges for the `gamma` and `cost` parameters. It is left as an exercise
    for the reader to rerun our analysis with a greater spread of values for these
    two in order to see whether we can get even better performance. This will, however,
    necessarily result in longer training times. Secondly, this particular dataset
    is quite challenging in that its baseline accuracy is actually 70 percent. This
    is because 70 percent of the customers in the data are low-risk customers (the
    two output classes are not balanced). For this reason, computing the Kappa statistic,
    which we saw in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    might be a better metric to use instead of classification accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification with support vector machines
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like with logistic regression, we've seen that the basic premise behind
    the support vector machine is that it is designed to handle two classes. Of course,
    we often have situations where we would like to be able to handle a greater number
    of classes, such as when classifying different plant species based on a variety
    of physical characteristics. One way to do this is the **one versus all** approach.
    Here, if we have *K* classes, we create *K* SVM classifiers, and for each classifier,
    we are attempting to distinguish one particular class from all the rest.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: To determine the best class to pick, we assign the class for which the observation
    produces the highest distance from the separating hyperplane, thus lying farthest
    away from all other classes. More formally, we pick the class for which our linear
    feature combination has a maximum value across all the different classifiers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach is known as the (balanced) **one versus one** approach.
    We create a classifier for all possible pairs of output classes. We then classify
    our observation with every one of these classifiers and tally up the totals for
    every winning class. Finally, we pick the class that has the most votes. This
    latter approach is actually what is implemented by the `svm()` function in the
    `e1071` package. We can, therefore, use this function when we have a problem with
    multiple classes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the maximal margin hyperplane as a decision boundary
    that is designed to separate two classes by finding the maximum distance from
    either of them. When the two classes are linearly separable, this creates a situation
    where the space between the two classes is evenly split.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: We've seen that there are circumstances where this is not always desirable,
    such as when the classes are close to each other because of a few observations.
    An improvement to this approach is the support vector classifier that allows us
    to tolerate a few margin violations, or even misclassifications, in order to obtain
    a more stable result. This also allows us to handle classes that aren't linearly
    separable. The form of the support vector classifier can be written in terms of
    inner products between the observation that is being classified and the support
    vectors. This transforms our feature space from *p* features into as many features
    as we have support vectors. Using kernel functions on these new features, we can
    introduce nonlinearity in our model and thus obtain a support vector machine.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we saw that training a support vector classifier, which is a support
    vector machine with a linear kernel, involves adjusting the `cost` parameter.
    The performance we obtain on our training data can be close to what we get in
    our test data. By contrast, we saw that by using a radial kernel, we have the
    potential to fit our training data much more closely, but we are far more likely
    to fall into the trap of overfitting.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, it is useful to try different combinations of the `cost`
    and the `gamma` parameters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，尝试不同的`cost`和`gamma`参数组合是有用的。
- en: 'In the next chapter, we are going to explore another cornerstone of machine
    learning: tree-based models. Also known as decision trees, they can handle regression
    and classification problems with many classes, are highly interpretable, and have
    a built-in way of handling missing data.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨机器学习的另一个基石：基于树的模型。也称为决策树，它们可以处理具有许多类别的回归和分类问题，具有高度的可解释性，并且内置了处理缺失数据的方式。
