- en: Chapter 6. Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to take a fresh look at nonlinear predictive models
    by introducing support vector machines. Support vector machines, often abbreviated
    as SVMs, are very commonly used for classification problems, although there are
    certainly ways to perform function approximation and regression tasks with them.
    In this chapter, we will focus on the more typical case of their role in classification.
    To do this, we'll first present the notion of maximal margin classification, which
    presents an alternative formulation of how to choose between many possible classification
    boundaries and differs from approaches we have seen thus far, such as maximum
    likelihood. We'll introduce the related idea of support vectors and how, together
    with maximal margin classification, we can obtain a linear model in the form of
    a support vector classifier. Finally, we'll present how we can generalize these
    ideas in order to introduce nonlinearity through the use of certain functions
    known as kernels to finally arrive at our destination, the support vector machine.
  prefs: []
  type: TYPE_NORMAL
- en: Maximal margin classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll begin this chapter by returning to a situation that should be very familiar
    by now: the binary classification task. Once again, we''ll be thinking about the
    problem of how to design a model that will correctly predict whether an observation
    belongs to one of two possible classes. We''ve already seen that this task is
    simplest when the two classes are linearly separable; that is, when we can find
    a *separating hyperplane* (a plane in a multidimensional space) in the space of
    our features so that all the observations on one side of the hyperplane belong
    to one class and all the observations that lie on the other side belong to the
    second class. Depending on the structure, assumptions, and optimizing criterion
    that our particular model uses, we could end up with one of infinite such hyperplanes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize this scenario using some data in a two-dimensional feature
    space, where the separating hyperplane is just a separating line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximal margin classification](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can see two clusters of observations, each of
    which belongs to a different class. We''ve used different symbols for the two
    classes to denote this explicitly. Next, we show three different lines that could
    serve as the decision boundary of a classifier, all of which would generate 100
    percent classification accuracy on the entire dataset. We''ll remind ourselves
    that the equation of a hyperplane can be expressed as a linear combination of
    the input features, which are the dimensions of the space in which the hyperplane
    resides:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximal margin classification](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A separating hyperplane has this property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximal margin classification](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first equation simply says that the data points that belong to class 1
    all lie above the hyperplane, and the second equation says that the data points
    that belong to class -1 all lie below the hyperplane. The subscript *i* is used
    to index observations, and the subscript *k* is used to index features, so that
    *x[ik]* means the value of the *k^(th)* feature in the *i^(th)* observation. We
    can combine these two equations into a single equation for simplicity, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximal margin classification](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To see why this simplification works, consider an observation of the class -1
    (*y[i]* *= -1*). This observation will lie below the separating hyperplane, so
    the linear combination in brackets will produce a negative value. Multiplying
    this with its *y[i]* value of -1 results in a positive value. A similar argument
    works for observations of class 1.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at our diagram, note that the two dashed lines come quite close
    to certain observations. The solid line intuitively feels better than the other
    two lines as a decision boundary, as it separates the two classes without coming
    too close to either by traversing the center of the space between them. In this
    way, it distributes the space between the two classes equally. We can define a
    quantity known as the **margin** that a particular separating hyperplane generates,
    as the smallest perpendicular distance from any point in the dataset to the hyperplane.
    In two dimensions and two classes, we will always have at least two points that
    lie at a perpendicular distance equal to the margin from the separating line,
    one on each side of the line. Sometimes, as is the case with our data, we may
    have more than two points whose perpendicular distance from the separating line
    equals the margin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next plot shows the margin of the solid line from the previous plot, demonstrating
    that we have three points at a distance equal to the margin from this separating
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximal margin classification](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have the definition of the margin under our belt, we have a way
    to codify our intuition that led us to choose the solid line as the better decision
    boundary among the three lines that we saw in the first plot. We can go a step
    further and define the **maximal margin hyperplane** as the hyperplane whose margin
    is the largest amongst all possible separating hyperplanes. In our 2D example,
    we are essentially looking for the line that will separate the two classes while
    at the same time being as far away from the observations as possible. It turns
    out that the solid line from our example is actually the maximal margin line,
    so that there is no other line that can be drawn with a higher margin than two
    units. This explains why we chose to label it as the line of maximum margin separation
    in our first plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand how we found the maximal margin hyperplane in our simple
    example, we need to formalize the problem as an optimization problem with *p*
    features by using the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximal margin classification](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Together, these two constraints in our optimization problem express the idea
    that observations in our data need to not only be correctly classified, but also
    lie at least *M* units away from the separating hyperplane. The goal is to maximize
    this distance *M* by appropriately choosing the coefficients *β[i]*. Thus, we
    need an optimization procedure that handles this type of problem. The details
    of how the optimization is actually implemented in practice are beyond the scope
    of this book, but we will see them in action later on when we do some programming
    with R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a natural way forward now, which is to start looking at how the situation
    changes when our data is not linearly separable, something that we know by now
    is the typical scenario for a real-world dataset. Let us take a step back before
    doing this. We''ve already studied two different methods for estimating the parameters
    of a model: namely, maximum likelihood estimation and the least squared error
    criterion for linear regression. For example, when we looked at classification
    with logistic regression, we considered the idea of maximizing the likelihood
    of our data. This takes into account all of the available data points. This is
    also the case when classifying with multilayer perceptrons. With the maximum margin
    classifier, however, the construction of our decision boundary is only supported
    by the points that lie on the margin. Put differently, with the data in our 2D
    example, we can freely adjust the position of any observation except the three
    on the margin, and as long as the adjustment does not result in the observation
    falling inside the margin, our separating line will stay exactly in the same position.
    For this reason, we define the perpendicular vectors from the points that lie
    on the margin to the separating hyperplane as the **support vectors**. Thus, we''ve
    seen that our 2D example has three support vectors. The fact that only a subset
    of all the points in our data essentially determines the placement of the separating
    hyperplane means that we have the potential to overfit our training data.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, this approach does yield a couple of nice properties. We
    split the space between the two classes equally between them without applying
    any bias toward either. Points that clearly lie well inside the area of space
    occupied by a particular class do not play such a big role in the model compared
    to points on the fringes, which is where we need to place our decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need our data to be linearly separable in order to classify it with a maximal
    margin classifier. When our data is not linearly separable, we can still use the
    notion of support vectors that define a margin, but this time, we will allow some
    examples to be misclassified. Thus, we essentially define a **soft margin**, in
    that some of the observations in our dataset can violate the constraint that they
    need to be at least as far as the margin from the separating hyperplane. It is
    also important to note that sometimes we may want to use a soft margin even for
    linearly separable data. The reason for this is in order to limit the degree of
    overfitting the data. Note that the larger the margin, the more confident we are
    about our ability to correctly classify new observations, because the classes
    are further apart from each other in our training data. If we achieve separation
    using a very small margin, we are less confident about our ability to correctly
    classify our data and we may, instead, want to allow a few errors and come up
    with a larger margin that is more robust. Study the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector classification](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to get a firmer grasp of the reason why a soft margin may be preferable
    to a hard margin, even for linearly separable data, we''ve changed our data slightly.
    We used the same data that we had previously, but we added an extra observation
    to class 1 and placed it close to the boundary of class -1\. Note that with the
    addition of this single new data point, with feature values f1=16 and f2=40, our
    maximal margin line has moved drastically! The margin has been reduced from two
    units to 0.29 units. Looking at this graph, we are tempted to feel that the new
    point might either be an outlier or a mislabeling in our dataset. If we were to
    allow our model to make one single misclassification using a soft margin, we would
    go back to our previous line, which separates the two classes with a much wider
    margin and is less likely to have overfit on the data. We formalize the notion
    of our soft classifier by modifying our optimization problem set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector classification](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Under this new setup, we''ve introduced a new set of variables *ξ*i, known
    as the **slack variables**. There is one slack variable for every observation
    in our dataset and the value of the *ξ*i slack variable depends on where the *i*th
    observation falls with respect to the margin. When an observation is on the correct
    side of the separating hyperplane and outside the margin, the slack variable for
    that observation takes the value 0\. This is the ideal situation that we have
    seen for all observations under a hard boundary. When an observation is correctly
    classified but falls at a distance within the margin, the corresponding slack
    variable takes a small positive value less than 1\. When an observation is actually
    misclassified, thus falling on the wrong side of the hyperplane altogether, then
    its associated slack variable takes a value greater than 1\. In summary, take
    a look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector classification](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When an observation is incorrectly classified, the magnitude of the slack variables
    is proportional to the distance between that observation and the boundary of the
    separating hyperplane. The fact that the sum of the slack variables must be less
    than a constant *C* means that we can think of this constant as an error budget
    that we are prepared to tolerate. As a misclassification of a single particular
    observation results in a slack variable taking at least the value 1, and our constant
    *C* is the sum of all the slack variables, setting a value of *C* less than 1
    means that our model will tolerate a few observations falling inside the margin,
    but no misclassifications. A high value of *C* often results in many observations
    either falling inside the margin or being misclassified, and as these are all
    support vectors, we end up having a greater number of support vectors. This results
    in a model that has a lower variance, but because we have shifted our boundary
    in a way that has increased tolerance to margin violations and errors, we may
    have a higher bias. By contrast, depending on fewer support vectors caused by
    having a much stricter model (and hence a lower value of *C*) may result in a
    lower bias in our model. These support vectors, however, will individually affect
    the position of our boundary to a much higher degree. Consequently, we will experience
    a higher variance in our model performance across different training sets. Once
    again, the interplay between model bias and variance resurfaces in the design
    decisions that we must make as predictive modelers.
  prefs: []
  type: TYPE_NORMAL
- en: Inner products
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The exact details of how the parameters of the support vector classifier model
    are computed are beyond the scope of this book. However, it turns out that the
    model itself can be simplified into a more convenient form that uses the **inner
    products** of the observations. An inner product of two vectors of identical length,
    *v1* and *v2*, is computed by first computing the element-wise multiplication
    of the two vectors and then taking the sum of the resulting elements. In R, we
    obtain an element-wise multiplication of two vectors by simply using the multiplication
    symbol. So we can compute the inner product of two vectors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In mathematical terms, we use triangular brackets to denote the inner product
    operation, and we represent the process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inner products](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, for the two vectors *v[1]* and *v[2]*, the index
    *i* is iterating over the *p* features or dimensions. Now, here is the original
    form of our support vector classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inner products](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is just the standard equation for a linear combination of the input features.
    It turns out that for the support vector classifier, the model''s solution can
    be expressed in terms of the inner product between the *x* observation that we
    are trying to classify and all other *x*i observations that are in our training
    dataset. More concretely, the form of our support vector classifier can also be
    written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inner products](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For this equation, we have explicitly indicated that our model predicts *y*
    as a function of an input observation *x*. The summing function now computes a
    weighted sum of all the inner products of the current observation with every other
    observation in the dataset, which is why we are now summing across the *n* observations.
    We want to make it very clear that we haven't changed anything in the original
    model itself; we have simply written two different representations of the same
    model. Note that we cannot assume that a linear model takes this form in general;
    this is only true for the support vector classifier. Now, in a real-world scenario,
    the number of observations in our data set, *n*, is typically much greater than
    the number of parameters, *p*, so the number of *α* coefficients is seemingly
    larger than the number of *β* coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, whereas in the first equation we were considering observations
    independently of each other, the form of the second equation shows us that to
    classify all our observations, we need to consider all possible pairs and compute
    their inner product. There are such pairs, which are of the order of *n*2\. Thus,
    it would seem like we are introducing complexity rather than producing a representation
    that is simpler. It turns out, however, that all *α* coefficients are zero for
    all observations in our dataset, except those that are support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of support vectors in our dataset is typically much smaller than
    the total number of observations. Thus, we can simplify our new representation
    by explicitly showing that we sum over elements from the set of support vectors,
    *S*, in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inner products](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Kernels and support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've introduced the notion of maximum margin classification under linearly
    separable conditions and its extension to the support vector classifier, which
    still uses a hyperplane as the separating boundary but handles datasets that are
    not linearly separable by specifying a budget for tolerating errors. The observations
    that are on or within the margin, or are misclassified by the support vector classifier,
    are support vectors. The critical role that these play in the positioning of the
    decision boundary was also seen in an alternative model representation of the
    support vector classifier that uses inner products.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is common in the situations that we have seen so far in this chapter is
    that our model is always linear in terms of the input features. We''ve seen that
    the ability to create models that implement nonlinear boundaries between the classes
    to be separated is far more flexible in terms of the different kinds of underlying
    target functions that they can handle. One way to introduce nonlinearity in our
    model that uses our new representation involving inner products is to apply a
    nonlinear transformation to this result. We can define a general function *K*,
    which we''ll call a **kernel function**, that operates on two vectors and produces
    a scalar result. This allows us to generalize our model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernels and support vector machines](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our model now has as many features as there are support vectors, and each feature
    is defined as the result of a kernel acting upon the current observation and one
    of the support vectors. For the support vector classifier, the kernel we applied
    is known as the **linear kernel,** as this just uses the inner product itself,
    producing a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernels and support vector machines](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Kernel functions are also known as similarity functions, as we can consider
    the output they produce as a measure of the similarity between the two input vectors
    provided. We introduce nonlinearity in our model using nonlinear kernels, and
    when we do this, our model is known as a **support vector machine**. There are
    a number of different types of nonlinear kernels. The two most common ones are
    the **polynomial kernel** and the **radial basis function kernel**. The polynomial
    kernel uses a power expansion of the inner product between two vectors. For a
    polynomial of degree *d*, the form of the polynomial kernel is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernels and support vector machines](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this kernel, we are essentially transforming our feature space into a
    higher dimensional space. Computing the kernel applied to the inner product is
    much more efficient than first transforming all the features into a high-dimensional
    space and then trying to fit a linear model into that space. This is especially
    true when we use the **radial basis function kernel**, often referred to simply
    as the **radial kernel**, where the number of dimensions of the transformed feature
    space is actually infinite due to the infinite number of terms in the expansion.
    The form of the radial kernel is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernels and support vector machines](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Upon close inspection, we should be able to spot that the radial kernel does
    not use the inner product between two vectors. Instead, the summation in the exponent
    is just the square of the Euclidean distance between these two vectors. The radial
    kernel is often referred to as a **local kernel**, because when the Euclidean
    distance between the two input vectors is large, the resulting value that the
    kernel computes is very small because of the negative sign in the exponent. Consequently,
    when we use a radial kernel, only vectors close to the current observation for
    which we want to get a prediction play a significant role in the computation.
    We're now ready to put all this to practice with some real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting chemical biodegration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to use R''s `e1071` package to try out the models
    we''ve discussed on a real-world dataset. As our first example, we have chosen
    the *QSARbiodegration data set*, which can be found at [https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation).
    This is a dataset containing 41 numerical variables that describe the molecular
    composition and properties of 1,055 chemicals. The modeling task is to predict
    whether a particular chemical will be biodegradable based on these properties.
    Example properties are the percentages of carbon, nitrogen, and oxygen atoms,
    as well as the number of heavy atoms in the molecule. These features are highly
    specialized and sufficiently numerous, so a full listing won''t be given here.
    The complete list and further details of the quantities involved can be found
    on the website. For now, we''ve downloaded the data into a `bdf` data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The final column, `V42`, contains the output variable, which takes the value
    `NRB` for chemicals that are not biodegradable and `RB` for those that are. We''ll
    recode this into the familiar labels of `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our data ready, we''ll begin, as usual, by splitting them
    into training and testing sets, with an 80-20 split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of packages available in R that implement support vector
    machines. In this chapter, we''ll explore the use of the `e1071` package, which
    provides us with the `svm()` function. If we examine our training data, we''ll
    quickly notice that on the one hand, the scales of the various features are quite
    different, and on the other hand, many features are sparse features, which means
    that for many entries they take a zero value. It is a good idea to scale features
    as we did with neural networks, especially if we want to work with radial kernels.
    Fortunately for us, the `svm()` function has a `scale` parameter, which is set
    to `TRUE` by default. This normalizes the input features so that they have zero
    mean and unit variance before the model is trained. This circumvents the need
    for us to manually carry out this preprocessing step. The first model that we
    will investigate will use a linear kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The call to the `svm()` function follows the familiar paradigm of first providing
    a formula, then providing the name of the data frame, and finally, other parameters
    relevant to the model. In our case, we want to train a model where the final `V42`
    column is the predictor column and all other columns are to be used as features.
    For this reason, we can just use the simple formula `V42 ~` instead of having
    to fully enumerate all the other columns. After specifying our data frame, we
    then specify the type of kernel we will use, and in this case, we''ve opted for
    a linear kernel. We''ll also specify a value for the `cost` parameter, which is
    related to the error budget C in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model doesn''t provide us with too much information on its performance
    beyond the details of the parameters that we specified. One interesting piece
    of information is the number of data points that were support vectors in our model;
    in this case, `272`. If we use the `str()` function to examine the structure of
    the fitted model, however, we will find that it contains a number of useful attributes.
    For example, the fitted attribute contains the model''s predictions on the training
    data. We''ll use this to gauge the quality of model fit by computing the accuracy
    of the training data and the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a training accuracy of just under 89 percent, which is a decent start.
    Next, we''ll examine the performance of the test data using the `predict()` function
    to see whether we can get a test accuracy close to this or whether we have ended
    up overfitting the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We do have a slightly lower test accuracy than we''d expect, but we are sufficiently
    close to the training accuracy we obtained earlier in order to be relatively confident
    that we are not in a position where we are overfitting the training data. Now,
    we''ve seen that the `cost` parameter plays an important role in our model, and
    that choosing this involves a trade-off in model bias and variance. Consequently,
    we want to try different values of our `cost` parameter before settling on a final
    model. After manually repeating the preceding code for a few values of this parameter,
    we obtained the following set of results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, when building a model, we may see a warning informing us that the
    maximum number of iterations has been reached. If this happens, we should be doubtful
    of the model that we produced, as it may be an indication that a solution was
    not found and the optimization procedure did not converge. In such a case, it
    is best to experiment with a different `cost` value and/or kernel type.
  prefs: []
  type: TYPE_NORMAL
- en: These results show that for most values of the `cost` parameter, we are seeing
    a very similar level of quality of fit on our training data, roughly 88 percent.
    Ironically, the best performance on the test data was obtained using the model
    whose fit on the training data was the worst, using a cost of 0.01\. In short,
    although we have reasonable performance on our training and test datasets, the
    low variance in the results shown in the table essentially tells us that we are
    not going to get a significant improvement in the quality of fit by tweaking the
    `cost` parameter on this particular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s try using a radial kernel to see whether introducing some nonlinearity
    can allow us to improve our performance. When we specify a radial kernel, we must
    also specify a positive `gamma` parameter. This corresponds to the *1/2σ2* parameter
    in the equation of a radial kernel. The role that this parameter plays is that
    it controls the locality of the similarity computation between its two vector
    inputs. A large `gamma` means that the kernel will produce values that are close
    to zero, unless the two vectors are very close together. A smaller `gamma` results
    in a smoother kernel and takes into account pairs of vectors that are farther
    away. Again, this choice boils down to a trade-off between bias and variance,
    so just as with the `cost` parameter, we''ll have to try out different values
    of `gamma`. For now, let''s see how we can create a support vector machine model
    using a radial kernel with a specific configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the radial kernel under these settings is able to fit the training
    data much more closely, as indicated by the nearly 100 percent training accuracy;
    but when we see the performance on the test dataset, the result is substantially
    lower than what we obtained on the training data. Consequently, we have a very
    clear indication that this model is overfitting the data. To get around this problem,
    we will manually experiment with a few different settings of the `gamma` and `cost`
    parameters to see whether we can improve the fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the combination of the two parameters, `cost` and `gamma`, yields
    a much wider range of results using the radial kernel. From the data frame we
    built previously, we can see that some combinations, such as *cost = 1* and *gamma
    = 0.05*, have brought our accuracy up to 89 percent on the test data, while still
    maintaining an analogous performance on our training data. Also, in the data frame,
    we see many examples of settings in which the training accuracy is nearly 100
    percent, but the test accuracy is well below this.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we conclude that using a nonlinear kernel, such as the radial kernel,
    needs to be done with care in order to avoid overfitting. Nonetheless, radial
    kernels are very powerful and can be quite effective at modeling a highly nonlinear
    decision boundary, often allowing us to achieve higher rates of classification
    accuracy than a linear kernel. At this point in our analysis, we would usually
    want to settle on a particular value for the `cost` and `gamma` parameters and
    then retrain our model using the entire data available before deploying it in
    the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, after using the test set to guide our decision on what parameters
    to use, it no longer represents an unseen dataset that would enable us to predict
    the model's accuracy in the real world. One possible solution to this problem
    is to use a validation set, but this would require us to set aside some of our
    data, resulting in smaller training and test set sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-validation*, which we covered in [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Tidying Data and Measuring
    Performance*, should be considered as a practical way out of this dilemma.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very readable book on support vector machines is *An Introduction to Support
    Vector Machines and Other Kernel-based Learning Methods* by Nello Christiani and
    John Shawe-Taylor. Another good reference that presents an insightful link between
    SVMs and a related type of neural network known as a **Radial Basis Function Network**
    is *Neural Networks and Learning Machines* by Simon Haykin, which we also referenced
    in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit scores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore another dataset, this time in the field of
    banking and finance. The particular dataset in question is known as the *German
    Credit Dataset* and is also hosted by the UCI Machine Learning Repository. The
    link to the data is [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).
  prefs: []
  type: TYPE_NORMAL
- en: The observations in the dataset are loan applications made by individuals at
    a bank. The goal of the data is to determine whether an application constitutes
    a high credit risk.
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `checking` | Categorical | The status of the existing checking account |'
  prefs: []
  type: TYPE_TB
- en: '| `duration` | Numerical | The duration in months |'
  prefs: []
  type: TYPE_TB
- en: '| `creditHistory` | Categorical | The applicant''s credit history |'
  prefs: []
  type: TYPE_TB
- en: '| `purpose` | Categorical | The purpose of the loan |'
  prefs: []
  type: TYPE_TB
- en: '| `credit` | Numerical | The credit amount |'
  prefs: []
  type: TYPE_TB
- en: '| `savings` | Categorical | Savings account/bonds |'
  prefs: []
  type: TYPE_TB
- en: '| `employment` | Categorical | Present employment since |'
  prefs: []
  type: TYPE_TB
- en: '| `installmentRate` | Numerical | The installment rate (as a percentage of
    disposable income) |'
  prefs: []
  type: TYPE_TB
- en: '| `personal` | Categorical | Personal status and gender |'
  prefs: []
  type: TYPE_TB
- en: '| `debtors` | Categorical | Other debtors/guarantors |'
  prefs: []
  type: TYPE_TB
- en: '| `presentResidence` | Numerical | Present residence since |'
  prefs: []
  type: TYPE_TB
- en: '| `property` | Categorical | The type of property |'
  prefs: []
  type: TYPE_TB
- en: '| `age` | Numerical | The applicant''s age in years |'
  prefs: []
  type: TYPE_TB
- en: '| `otherPlans` | Categorical | Other installment plans |'
  prefs: []
  type: TYPE_TB
- en: '| `housing` | Categorical | The applicant''s housing situation |'
  prefs: []
  type: TYPE_TB
- en: '| `existingBankCredits` | Numerical | The number of existing credits at this
    bank |'
  prefs: []
  type: TYPE_TB
- en: '| `job` | Categorical | The applicant''s job situation |'
  prefs: []
  type: TYPE_TB
- en: '| `dependents` | Numerical | The number of dependents |'
  prefs: []
  type: TYPE_TB
- en: '| `telephone` | Categorical | The status of the applicant''s telephone |'
  prefs: []
  type: TYPE_TB
- en: '| `foreign` | Categorical | Foreign worker |'
  prefs: []
  type: TYPE_TB
- en: '| `risk` | Binary | Credit risk (1 = good, 2 = bad) |'
  prefs: []
  type: TYPE_TB
- en: 'First, we will load the data into a data frame called `german_raw` and provide
    it with column names that match the previous table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note from the table describing the features that we have a lot of categorical
    features to deal with. For this reason, we will employ `dummyVars()` once again
    to create dummy binary variables for these. In addition, we will record the `risk`
    variable, our output, as a factor with level 0 for good credit and level 1 for
    bad credit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of this processing, we now have a data frame with 61 features,
    because several of the categorical input features had many levels. Next, we will
    partition our data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: One particularity of this dataset that is mentioned on the website is that the
    data comes from a scenario where the two different types of errors have different
    costs. Specifically, the cost of misclassifying a high-risk customer as a low-risk
    customer is five times more expensive for the bank than misclassifying a low-risk
    customer as a high-risk customer. This is understandable, as in the first case,
    the bank stands to lose a lot of money from a loan it gives out that cannot be
    repaid, whereas in the second case, the bank misses out on an opportunity to give
    out a loan that will yield interest for the bank.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `svm()` function has a `class.weights` parameter, which we use to specify
    the cost of misclassifying an observation to each class. This is how we will incorporate
    our asymmetric error cost information into our model. First, we''ll create a vector
    of class weights, noting that we need to specify names that correspond to the
    output factor levels. Then, we will use the `tune()` function to train various
    SVM models with a radial kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The suggested best model has *cost = 10* and *gamma = 0.05* and achieves 74
    percent training accuracy. Let''s see how this model fares on our test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance on our test set is 73.5 percent and very close to what we saw
    in training. As expected, our model tends to make many more errors that misclassify
    a low risk customer as a high risk customer. Predictably, this takes a toll on
    the overall classification accuracy, which just computes the ratio of correctly
    classified observations to the overall number of observations. In fact, were we
    to remove this cost imbalance, we would actually select a different set of parameters
    for our model, and our performance, from the perspective of the unbiased classification
    accuracy, would be better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this last model will tend to make a greater number of costly misclassifications
    of high-risk customers as low-risk customers, which we know is very undesirable.
    We'll conclude this section with two final thoughts. Firstly, we have used relatively
    small ranges for the `gamma` and `cost` parameters. It is left as an exercise
    for the reader to rerun our analysis with a greater spread of values for these
    two in order to see whether we can get even better performance. This will, however,
    necessarily result in longer training times. Secondly, this particular dataset
    is quite challenging in that its baseline accuracy is actually 70 percent. This
    is because 70 percent of the customers in the data are low-risk customers (the
    two output classes are not balanced). For this reason, computing the Kappa statistic,
    which we saw in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    might be a better metric to use instead of classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification with support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like with logistic regression, we've seen that the basic premise behind
    the support vector machine is that it is designed to handle two classes. Of course,
    we often have situations where we would like to be able to handle a greater number
    of classes, such as when classifying different plant species based on a variety
    of physical characteristics. One way to do this is the **one versus all** approach.
    Here, if we have *K* classes, we create *K* SVM classifiers, and for each classifier,
    we are attempting to distinguish one particular class from all the rest.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the best class to pick, we assign the class for which the observation
    produces the highest distance from the separating hyperplane, thus lying farthest
    away from all other classes. More formally, we pick the class for which our linear
    feature combination has a maximum value across all the different classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach is known as the (balanced) **one versus one** approach.
    We create a classifier for all possible pairs of output classes. We then classify
    our observation with every one of these classifiers and tally up the totals for
    every winning class. Finally, we pick the class that has the most votes. This
    latter approach is actually what is implemented by the `svm()` function in the
    `e1071` package. We can, therefore, use this function when we have a problem with
    multiple classes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the maximal margin hyperplane as a decision boundary
    that is designed to separate two classes by finding the maximum distance from
    either of them. When the two classes are linearly separable, this creates a situation
    where the space between the two classes is evenly split.
  prefs: []
  type: TYPE_NORMAL
- en: We've seen that there are circumstances where this is not always desirable,
    such as when the classes are close to each other because of a few observations.
    An improvement to this approach is the support vector classifier that allows us
    to tolerate a few margin violations, or even misclassifications, in order to obtain
    a more stable result. This also allows us to handle classes that aren't linearly
    separable. The form of the support vector classifier can be written in terms of
    inner products between the observation that is being classified and the support
    vectors. This transforms our feature space from *p* features into as many features
    as we have support vectors. Using kernel functions on these new features, we can
    introduce nonlinearity in our model and thus obtain a support vector machine.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we saw that training a support vector classifier, which is a support
    vector machine with a linear kernel, involves adjusting the `cost` parameter.
    The performance we obtain on our training data can be close to what we get in
    our test data. By contrast, we saw that by using a radial kernel, we have the
    potential to fit our training data much more closely, but we are far more likely
    to fall into the trap of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, it is useful to try different combinations of the `cost`
    and the `gamma` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we are going to explore another cornerstone of machine
    learning: tree-based models. Also known as decision trees, they can handle regression
    and classification problems with many classes, are highly interpretable, and have
    a built-in way of handling missing data.'
  prefs: []
  type: TYPE_NORMAL
