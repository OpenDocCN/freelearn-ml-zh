- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed and GPU-Based Learning with LightGBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter looks at training LightGBM models on distributed computing clusters
    and GPUs. Distributed computing can significantly speed up training workloads
    and enable the training of much larger datasets than the memory available on a
    single machine. We’ll look at leveraging Dask for distributed computing and LightGBM’s
    support for GPU-based training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed learning with LightGBM and Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU training for LightGBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples of training and running LightGBM models on distributed
    computing clusters and GPUs. A Dask environment and GPUs are required to run the
    examples. Complete code examples are available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed learning with LightGBM and Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask is an open-source Python library for distributed computing. It’s designed
    to integrate seamlessly with existing Python libraries and tools, including scikit-learn
    and LightGBM. This section looks at running distributed training workloads for
    LightGBM using Dask.
  prefs: []
  type: TYPE_NORMAL
- en: Dask ([https://www.dask.org/](https://www.dask.org/)) allows you to set up clusters
    on both a single machine and across many machines. Running Dask on a single machine
    is the default and requires no setup. However, workloads that run on a single-machine
    cluster (or scheduler) can readily be run with a distributed scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Dask offers many ways to run a distributed cluster, including integrating Kubernetes,
    MPI, or automatic provisioning into a hyperscalar such as AWS or Google Cloud
    Platform.
  prefs: []
  type: TYPE_NORMAL
- en: When running on a single machine, Dask still distributes the workload across
    multiple threads, which can significantly speed up workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask provides cluster management utility classes to set up a cluster easily.
    A local cluster can be run as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates a local cluster with four workers, configuring each
    worker to run two threads. The cluster runs on localhost, with the scheduler running
    on port `8786` by default. The host IP and port can be configured using parameters.
    In addition to running the scheduler, Dask also starts a diagnostic dashboard
    that’s implemented using Bokeh ([https://docs.bokeh.org/en/latest/](https://docs.bokeh.org/en/latest/)).
    By default, the dashboard runs on port `8787`. We can check the **Workers** page
    to see the status of our running cluster, as shown in *Figure 11**.1.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Dask diagnostics dashboard showing four running workers with
    some technical statistics for each](img/B16690_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Dask diagnostics dashboard showing four running workers with some
    technical statistics for each
  prefs: []
  type: TYPE_NORMAL
- en: With a cluster up and running, we can prepare our data for use on the distributed
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Dask offers its own implementation of a data frame called the Dask DataFrame.
    A Dask DataFrame comprises many smaller pandas DataFrames, which are split based
    on the index. Each part can be stored on disk or distributed across a network,
    which allows working with much larger datasets than can fit into a single machine’s
    memory. Operations performed on the Dask DataFrame are automatically distributed
    to the pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When your dataset fits into RAM, using standard pandas DataFrames instead of
    a Dask DataFrame is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a Dask DataFrame by loading a CSV file. Note that the CSV file
    may be located on S3 or HDFS and can be very large. The following code creates
    a Dask DataFrame from a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we also specify the block size for loading the CSV file. The block size
    sets the chunks the dataset is divided into and gives us granular control over
    the memory of individual DataFrame parts. When calling `df.shape`, we get an interesting
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of columns is returned as a number. However, looking at the number
    of rows, we got a wrapper class called `Delayed`. This illustrates that even though
    we’ve created the DataFrame, the data is not loaded into memory. Instead, Dask
    loads the data as needed on the workers that use the data. We can force Dask to
    compute the row count as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data available in a DataFrame, we can prepare it for training. We
    split our data into a training and test set using the `train_test_split` function
    from `dask_ml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Although the function from `dask_ml` mirrors the functionality of scikit-learn’s
    `train_test_split`, the Dask version maintains the distributed nature of the underlying
    Dask DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Our cluster is now set up, and the data is prepared for training. We can now
    look toward training our LightGBM model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LightGBM library team offers and maintains Dask versions of each available
    learning algorithm: `DaskLGBMRegressor`, `DaskLGBMClassifier`, and `DaskLGBMRanker`.
    These are wrappers around the standard LightGBM scikit-learn interface with additional
    functionality to specify the Dask cluster client to use.'
  prefs: []
  type: TYPE_NORMAL
- en: When LightGBM runs on a Dask cluster, training occurs with one LightGBM worker
    per Dask worker. LightGBM concatenates all data partitions on a single worker
    into a single dataset, and each LightGBM worker uses the local dataset independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each LightGBM worker then works in concert to train a single LightGBM model,
    using the Dask cluster to communicate. When data parallel training is performed
    (as is the case with Dask), LightGBM uses a **Reduce-Scatter** strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the histogram-building phase, each worker builds histograms for different
    non-overlapping features. Then, a **Reduce-Scatter** operation is performed: each
    worker shares a part of its histogram with each other worker.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the **Reduce-Scatter**, each worker has a complete histogram for a subset
    of features and then finds the best split for these features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, a gathering operation is performed: each worker shares its best split
    with all other workers, so all workers have all the best splits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best feature split is chosen, and the data is partitioned accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the complexity of the distributed algorithm is hidden from us,
    and the training code is identical to the scikit-learn training code we’re used
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code trains the LightGBM model, and we can see the progress
    by checking the **Status** page of the Dask dashboard, as shown in *Figure 11**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Status page of the Dask dashboard showing the task stream while
    a LightGBM model is training](img/B16690_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Status page of the Dask dashboard showing the task stream while
    a LightGBM model is training
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask LightGBM models can be fully serialized using **Pickle** or **joblib**,
    and we can save the model to disk as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Predictions can be made by calling the `predict` method of the model. Note
    that the Dask model expects a Dask DataFrame or array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to getting the shape of a Dask DataFrame, the prediction operation
    is also delayed and only calculated when needed. We can use `compute` to get the
    prediction values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our look at leveraging Dask for distributed training with LightGBM.
    With Dask, LightGBM can train models on massive datasets well beyond the computing
    power of a single server. Dask scales alongside your needs, so you can start with
    your local laptop and move to a high-performance computing environment or cloud
    infrastructure as your data grows. Further, as shown previously, Dask is designed
    to work harmoniously with established Python libraries such as pandas, NumPy,
    and scikit-learn, providing a familiar environment for data scientists while extending
    the capabilities of these tools.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at speeding up LightGBM training when large models need to
    be trained using the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: GPU training for LightGBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LightGBM library has native support for training the model on a GPU [*1*].
    Two GPU platforms are supported: GPU via OpenCL and CUDA. Leveraging the GPU via
    OpenCL offers support for the broadest range of GPUs (including AMD GPUs) and
    is significantly faster than running the model on a CPU. However, the CUDA platform
    offers the fastest runtime if you have an NVIDIA GPU available.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up LightGBM for the GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up your environment to use the GPU can be a bit tricky, but we'll review
    the core steps here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPU setup steps discussed here are offered as a guide and overview of the
    process of setting up your environment. The exact version number of libraries
    and drivers listed here may be outdated, and it’s recommended that you review
    the official documentation for up-to-date versions: [https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml](https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: In order to use the GPU, we have to *compile and build the LightGBM library
    from the source code*. The following instructions assume an Ubuntu Linux build
    environment; steps for other platforms are similar. Before we can build the library,
    we must install a few dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, first, install the GPU drivers for your environment. If you have
    an NVIDIA GPU, also install CUDA. Instructions for doing so are available from
    the respective vendor sites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://www.amd.com/en/support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we need to install the OpenCL headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, install the library build dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to compile the LightGBM library with GPU support. Clone the
    repository and build the library, setting the `USE_GPU` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036), *Ensemble
    Learning – Bagging and Boosting,* LightGBM is a C++ library with a Python interface.
    With the preceding instructions, we have built the library with GPU support, but
    we must build and install the Python package to use the library from Python (including
    the scikit-learn API):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Running LightGBM on the GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running training code on the GPU is straightforward. We set the device parameter
    to either `gpu` or `cuda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, we turn off LightGBM’s sparse matrix optimization
    by setting `is_enable_sparse` to `False`. LightGBM’s sparse features are not supported
    on GPU devices. Further, depending on your dataset, you might get the following
    warning stating that `multi_logloss` is not implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Notably, the fallback performed is only for evaluation and not training; training
    is still performed on the GPU. We can validate that the GPU is used by checking
    `nvidia-smi` (for NVIDIA GPUs):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – nvidia-smi output while LightGBM training is running (as we
    can see, the GPU utilization is at 40%)](img/B16690_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – nvidia-smi output while LightGBM training is running (as we can
    see, the GPU utilization is at 40%)
  prefs: []
  type: TYPE_NORMAL
- en: The speed-up achieved depends on your GPU. The training time was reduced from
    171 s to 11 s (a 15-times speed-up) for 150 iterations on the Forest Cover dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The immense performance gain stemming from using a GPU is especially useful
    when performing parameter tuning. We can use GPU-based training with, for instance,
    Optuna to significantly accelerate the search for optimal parameters. All that’s
    needed is to move the model training in the `objective` function to the GPU device.
    When defining the objective function, we specify our Optuna parameter ranges as
    per usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create the model with the Optuna parameters and make sure to specify
    the device as `cuda` (or `gpu`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the objective function is to return the cross-validated scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run a parameter study as we would normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Importantly, we also set the `n_jobs` parameter to `1` here, as running parallel
    jobs leveraging the GPU could cause unnecessary contention and overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few noteworthy best practices for getting the best performance
    when training on a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: Always verify that the GPU is being used. LightGBM returns to CPU training if
    the GPU is unavailable despite setting `device=gpu`. A good way of checking is
    with a tool such as `nvidia-smi`, as shown previously, or comparing training times
    to reference benchmarks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a much smaller `max_bin` size. Large datasets reduce the impact of a smaller
    `max_bin` size, and the smaller number of bins benefits training on the GPU. Similarly,
    use single-precision floats for increased performance if your GPU supports it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU training works best for large, dense datasets. Data needs to be moved to
    the GPU’s VRAM for training, and if the dataset is too small, the overhead involved
    with moving the data is too significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid one-hot encoding of feature columns, as this leads to sparse feature matrices,
    which do not work well on the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our section on how to use the GPU with LightGBM. Although the
    setup might be more complex, GPUs offer a significant boost in training speed
    due to their ability to handle thousands of threads simultaneously, allowing for
    efficient parallel processing, especially with large datasets. The massive parallelism
    of GPUs is particularly beneficial for histogram-based algorithms in LightGBM,
    making operations such as building histograms more efficient and effective.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed two ways of accelerating computing with LightGBM.
    The first is large-scale distributed training across many machines using the Python
    library Dask. We showed how to set up a Dask cluster, how data can be distributed
    to the cluster using the Dask DataFrame, and how to run LightGBM on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we also looked at how to leverage the GPU with LightGBM. Notably, the
    GPU setup is complex, but significant speed-up can be achieved when it’s available.
    We also discussed some best practices for training LightGBM models on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *H. Zhang, S. Si, and C.-J. Hsieh, GPU-acceleration for Large-scale
    Tree* *Boosting, 2017.* |'
  prefs: []
  type: TYPE_TB
